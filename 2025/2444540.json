{
 "awd_id": "2444540",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CompCog: Generating Object Percepts in Peripheral Vision During Naturalistic Attention Tasks",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032924886",
 "po_email": "sfischer@nsf.gov",
 "po_sign_block_name": "Simon Fischer-Baum",
 "awd_eff_date": "2025-04-01",
 "awd_exp_date": "2028-03-31",
 "tot_intn_awd_amt": 684021.0,
 "awd_amount": 283832.0,
 "awd_min_amd_letter_date": "2025-03-25",
 "awd_max_amd_letter_date": "2025-03-25",
 "awd_abstract_narration": "Imagine doing some everyday activity, such as walking down a street to meet a friend at a restaurant. As you do this, your eyes are constantly moving\u2014darting from your phone to street signs to the people and cars around you. Yet, despite what should be a jittery visual mess, we perceive a relatively smooth and stable visual world. For decades, scientists have been theorizing about how the brain creates this illusion of visual stability\u2014recent advances in AI suggest an answer that can finally help crack this human perceptual code. \r\n\r\nThe latest AI-powered vision-language models are becoming very good at recognizing visual objects and understanding scenes, even imagining what should be there and filling in missing parts much like the human brain does. This begs the question of whether these models are making sense of the visual world the same as humans. This project puts these new AI models to the test by having them generate plausible visual scenes, in real time, using only the few input samples that correspond to where a person looked while freely viewing the scene. This person is then shown either the real or AI-generated scene and asked if it is the scene that they just viewed. By identifying generated scenes that people cannot tell from real, cognitive scientists learn more about the semantic variability that objects viewed in peripheral vision might take while still seeming plausible (i.e., stable), and computational neuroscientists learn more about the algorithms underlying human visual stability and scene perception. This knowledge may also contribute to the development of smarter AI systems capable of perceiving the world more like humans do, thereby potentially improving performance in time-critical applications such as self-driving cars and enhancing user experiences in augmented reality contexts.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Zelinsky",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory J Zelinsky",
   "pi_email_addr": "gregory.zelinsky@stonybrook.edu",
   "nsf_id": "000209271",
   "pi_start_date": "2025-03-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "perf_city_name": "STONY BROOK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117940001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002627DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002728DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 283832.0
  }
 ],
 "por": null
}