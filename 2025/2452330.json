{
 "awd_id": "2452330",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CIF: Information Theoretic Measures for Fairness-aware Supervised Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032920000",
 "po_email": "ahero@nsf.gov",
 "po_sign_block_name": "Alfred Hero",
 "awd_eff_date": "2024-10-01",
 "awd_exp_date": "2026-06-30",
 "tot_intn_awd_amt": 174850.0,
 "awd_amount": 89216.0,
 "awd_min_amd_letter_date": "2024-10-15",
 "awd_max_amd_letter_date": "2024-11-29",
 "awd_abstract_narration": "Despite the growing success of Machine Learning (ML) systems in accomplishing complex tasks, their increasing use in making or aiding consequential decisions that affect people\u2019s lives (e.g., university admission, healthcare, predictive policing) raises concerns about potential discriminatory practices. Unfair outcomes in ML systems result from historical biases in the data used to train them. A learning algorithm designed merely to minimize prediction error may inherit or even exacerbate such biases; particularly when observed attributes of individuals, critical for generating accurate decisions, are biased by their group identities (e.g., race or gender) due to existing social and cultural inequalities. Understanding and measuring these biases-- at the data level-- is a challenging yet crucial problem, leading to constructive insights and methodologies for debiasing the data and adapting the learning system to minimize discrimination, as well as raising the need for policy changes and infrastructural development. This project aims to establish a comprehensive framework for precisely quantifying the marginal impact of individuals\u2019 attributes on accuracy and unfairness of decisions, using tools from information and game theories and causal inference, along with legal and social science definitions of fairness. This multi-disciplinary effort will provide guidelines and design insights for practitioners in the field of fair data-driven automated systems and inform the public debate on social consequences of artificial intelligence.\r\n\r\nThe majority of previous work formulates the algorithmic fairness problem from the viewpoint of the learning algorithm by enforcing a statistical or counterfactual fairness constraint on the learner\u2019s outcome and designing a learner that meets it. As the considered fairness problem originates from biased data, merely adding constraints to the prediction task might not provide a holistic view of its fundamental limitations. This project looks at the fairness problem through different lens, where instead of asking \u201cfor a given learner, how can we achieve fairness\u201d?, it asks \u201cfor a given dataset, what are the inherent tradeoffs in the data, and based on these, what is the best learner we can design\u201d?. In supervised learning models, the challenge in the proposed problem lies in the complex structures of correlation/causation among individuals\u2019 attributes (covariates), their group identities (protected features), the target variable (label), and the prediction outcome (decision). In analyzing the dataset, the marginal impacts of covariates on the accuracy and discrimination of decisions are quantified from the data, via carefully designed measures accounting for the complex correlation/causation structures among variables and the inherent tension between accuracy and fairness objectives. Subsequently, methods to exploit the quantified impacts in guiding downstream ML systems to improve their achievable accuracy-fairness tradeoff will be investigated. Importantly, the proposed framework provides explainable solutions, where the inclusion of certain attributes in the learning system is explained by their importance for accurate as well as fair decisions.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mohamed",
   "pi_last_name": "Nafea",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Mohamed S Nafea",
   "pi_email_addr": "mnafea@mst.edu",
   "nsf_id": "000858726",
   "pi_start_date": "2024-10-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Missouri University of Science and Technology",
  "inst_street_address": "300 W. 12TH STREET",
  "inst_street_address_2": "202 CENTENNIAL HALL",
  "inst_city_name": "ROLLA",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "5733414134",
  "inst_zip_code": "654091330",
  "inst_country_name": "United States",
  "cong_dist_code": "08",
  "st_cong_dist_code": "MO08",
  "org_lgl_bus_name": "UNIVERSITY OF MISSOURI SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "Y6MGH342N169"
 },
 "perf_inst": {
  "perf_inst_name": "Missouri University of Science and Technology",
  "perf_str_addr": "300 W. 12TH STREET",
  "perf_city_name": "ROLLA",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "654091330",
  "perf_ctry_code": "US",
  "perf_cong_dist": "08",
  "perf_st_cong_dist": "MO08",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7797",
   "pgm_ref_txt": "COMM & INFORMATION FOUNDATIONS"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 89216.0
  }
 ],
 "por": null
}