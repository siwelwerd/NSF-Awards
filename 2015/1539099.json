{
 "awd_id": "1539099",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "VEC: Small: Collaborative Research: Scene Understanding from RGB-D Images",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 152000.0,
 "awd_amount": 160662.0,
 "awd_min_amd_letter_date": "2015-08-27",
 "awd_max_amd_letter_date": "2016-08-23",
 "awd_abstract_narration": "This project exploits the benefits of RGB-D (color and depth) image collections with extra depth information to significantly advance the state-of-the-art in visual scene understanding, and makes computer vision techniques become usable in practical applications. Recent advance in affordable depth sensors has made depth acquisition significantly easier for ordinary users. These depth cameras are becoming very common in digital devices and help automatic scene understanding. The research team develops technologies to take advantage of depth information. Besides the published research results, the research team plans to distribute source code and benchmark data sets that could benefit researchers in a variety of disciplines. This project is integrated with educational programs, such as interdisciplinary workshops and courses at the graduate, undergraduate, and professional levels and diversity enhancement programs that promote opportunities for disadvantaged groups. The research team is closely collaborating with the industrial partner (Intel), involving interns and technology transfer in real products. The project is also applying the developed algorithms to the assistive technology for the blind and visually impaired.\r\n\r\nThis research develops algorithms required to perform real-time segmentation, labeling, and recognition of RGB-D images, videos, and 3D scans of indoor environments. Specifically, the PIs develop methods to: (1) acquire large labeled RGB-D datasets for training and evaluation, (2) study algorithms to recognize objects and estimate detailed 3D knowledge about the scene, (3) exploit the object-to-object contextual relationships in 3D, and (4) demonstrate applications to benefit the general public, including household robotics and assistive technologies for the blind.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jitendra",
   "pi_last_name": "Malik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jitendra Malik",
   "pi_email_addr": "malik@cs.berkeley.edu",
   "nsf_id": "000447614",
   "pi_start_date": "2015-08-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alexei",
   "pi_last_name": "Efros",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Alexei A Efros",
   "pi_email_addr": "efros@eecs.berkeley.edu",
   "nsf_id": "000487848",
   "pi_start_date": "2015-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947045940",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "002Z",
   "pgm_ref_txt": "Intel/NSF VEC Partnership"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 152000.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 8662.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<div><span style=\"white-space: pre;\"> </span>This&nbsp;<span class=\"il\">project</span>&nbsp;studied building a 3D understanding of indoor environments with the end goal of developing systems that can assist the general public, including household robots and assistive technologies for the blind. This 3D information is important because it lets people and robots navigate through and understand how to interact with their environment. One of the main ways of obtaining this information is RGBD (Red, Green, Blue, Depth) sensors, which capture both an ordinary image and an image that tells the user how far everything is from the camera. These systems are powerful but limited and we aimed to better use and improve them. Towards achieving this, we worked on: building systems for acquiring more data, developing algorithms for using this data, and building technologies that can be used on top of these data and algorithms.</div>\n<div><span style=\"white-space: pre;\"> </span>At the base of the&nbsp;<span class=\"il\">project</span>&nbsp;was an effort to develop new techniques for getting 3D data. This is important because current systems in use require enormous amounts of data to learn how to do sensible things. Some of our work went towards improving existing sensors for 3D. While these sensors are powerful, they can't record data everywere, have limited field of view like a camera, and building a model of the environment from their observations requires new techniques. Our&nbsp;<span class=\"il\">project</span>&nbsp;tackled all of these issues, resulting in techniques that can greatly improve ordinary 3D sensors. To help provide supplemental data, our&nbsp;<span class=\"il\">project</span>&nbsp;also developed a number of realistic simulators that can help provide data for data-hungry learning systems.</div>\n<div><span style=\"white-space: pre;\"> </span>Building on top of our new data, we developed a number of different techniques for using 3D data. For example, both 3D sensors and ordinary cameras can only capture what they see and can't automatically infer the parts of the scene that are not immediately visible. For instance, all the legs of a table or backside of a bed are typically not in view. We developed techniques that can produce a full reconstruction of the room, including both the visible and invisible portions, using a single ordinary image or a single image from a depth sensor. This representation can then be used for tasks like understanding what a household robot could do. This was only such example, and we investigated a number of different tasks, including disassembling a 3D model into a set of simple parts and joining a set of 3D scans.</div>\n<div><span style=\"white-space: pre;\"> </span>Finally, we investigated the tasks of navigation and manipulation based on top of this 3D data. If we are to have assistive systems, they will need to be able to navigate through the world and interact with things. In navigation, we developed techniques for following directions such as following a path, going to a point at a given distance and direction, and finding a particular instance of an object (for instance: find me a chair). These navigation systems were built on top of the simulators built earlier in the&nbsp;<span class=\"il\">project</span>. In manipulation, we investigated how to use 3D data for better grasping and how to get demonstrations of humans interacting with the world. The manipulation system that we developed could infer how its gripper and suction grasper could interact the world based on an RGBD image.&nbsp;</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/13/2019<br>\n\t\t\t\t\tModified by: Alexei&nbsp;A&nbsp;Efros</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \n This project studied building a 3D understanding of indoor environments with the end goal of developing systems that can assist the general public, including household robots and assistive technologies for the blind. This 3D information is important because it lets people and robots navigate through and understand how to interact with their environment. One of the main ways of obtaining this information is RGBD (Red, Green, Blue, Depth) sensors, which capture both an ordinary image and an image that tells the user how far everything is from the camera. These systems are powerful but limited and we aimed to better use and improve them. Towards achieving this, we worked on: building systems for acquiring more data, developing algorithms for using this data, and building technologies that can be used on top of these data and algorithms.\n At the base of the project was an effort to develop new techniques for getting 3D data. This is important because current systems in use require enormous amounts of data to learn how to do sensible things. Some of our work went towards improving existing sensors for 3D. While these sensors are powerful, they can't record data everywere, have limited field of view like a camera, and building a model of the environment from their observations requires new techniques. Our project tackled all of these issues, resulting in techniques that can greatly improve ordinary 3D sensors. To help provide supplemental data, our project also developed a number of realistic simulators that can help provide data for data-hungry learning systems.\n Building on top of our new data, we developed a number of different techniques for using 3D data. For example, both 3D sensors and ordinary cameras can only capture what they see and can't automatically infer the parts of the scene that are not immediately visible. For instance, all the legs of a table or backside of a bed are typically not in view. We developed techniques that can produce a full reconstruction of the room, including both the visible and invisible portions, using a single ordinary image or a single image from a depth sensor. This representation can then be used for tasks like understanding what a household robot could do. This was only such example, and we investigated a number of different tasks, including disassembling a 3D model into a set of simple parts and joining a set of 3D scans.\n Finally, we investigated the tasks of navigation and manipulation based on top of this 3D data. If we are to have assistive systems, they will need to be able to navigate through the world and interact with things. In navigation, we developed techniques for following directions such as following a path, going to a point at a given distance and direction, and finding a particular instance of an object (for instance: find me a chair). These navigation systems were built on top of the simulators built earlier in the project. In manipulation, we investigated how to use 3D data for better grasping and how to get demonstrations of humans interacting with the world. The manipulation system that we developed could infer how its gripper and suction grasper could interact the world based on an RGBD image. \n\n \n\n\t\t\t\t\tLast Modified: 01/13/2019\n\n\t\t\t\t\tSubmitted by: Alexei A Efros"
 }
}