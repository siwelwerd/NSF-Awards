{
 "awd_id": "1520587",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Say What I Feel",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Glenn H. Larsen",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2015-12-31",
 "tot_intn_awd_amt": 149964.0,
 "awd_amount": 149964.0,
 "awd_min_amd_letter_date": "2015-05-19",
 "awd_max_amd_letter_date": "2015-05-19",
 "awd_abstract_narration": "This SBIR Phase I project aims to create an emotionally expressive software-based speech-generating communication system, which is designed for individuals with little or no verbal communication ability -- particularly those with autism spectrum disorder. Current alternative communication technology does not offer the ability to express emotional content: something that is crucial to effective, comprehensible communication at home, in the community, and at work. The ability to convey emotions -- sadness, anger, or happiness, for example -- through one's tone of voice or facial expressions has far-reaching educational and vocational benefits for individuals with communication challenges. In other words, emotional content helps clarify the communicative intent behind a spoken message. When non-verbal individuals can communicate thoroughly and effectively, they increase their ability to receive a free and appropriate public education, and they expand their vocational opportunities. When individuals can receive an education and find a job, they can contribute back to society. The proposed work supports progress in science and engineering, yet it also enhances the potential for current educational applications and future research studies. Finally, this project builds on Federal and State efforts to offer people with disabilities educational and vocational services.\r\n\r\n\t\r\nThis project will develop and combine novel software algorithms that target digital facial recognition patterns and speech-based waveforms, in order to enhance the emotional content of conversational communication for individuals who cannot speak volitionally. The ability to add emotional content onto synthesized speech is a new technology that could substantially benefit individuals with communication challenges, such as people with autism spectrum disorder. This project seeks to facilitate and augment emotional expression and, therefore, increase communicative competence for individuals who use software-based synthesized speech systems to express their ideas, feelings, and wishes. Measuring whether or not this project accurately conveys human emotion in speech and facial expressions requires a perceptual test involving neurotypical adults. Specifically, the perceptual test will include both trained and untrained judges. The purpose of this is to ensure that the facial expressions as well as the pitch contour and vocal emphasis align with the emotion labels (i.e., happy, sad, or mad). In order to accurately calculate the judges' inter-rater reliability, the study will use a method that determines the extent to which judges' ratings agree, relative to how much they would likely conform, if they were to randomly rate the same stimuli.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lois",
   "pi_last_name": "Brady",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Lois J Brady",
   "pi_email_addr": "loisjeanbrady@gmail.com",
   "nsf_id": "000668110",
   "pi_start_date": "2015-05-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "iTherapy LLC",
  "inst_street_address": "955 WALNUT AVE",
  "inst_street_address_2": "",
  "inst_city_name": "VALLEJO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9258120037",
  "inst_zip_code": "945921021",
  "inst_country_name": "United States",
  "cong_dist_code": "08",
  "st_cong_dist_code": "CA08",
  "org_lgl_bus_name": "ITHERAPY, LLC",
  "org_prnt_uei_num": "",
  "org_uei_num": "DHJNEXLBYXJ5"
 },
 "perf_inst": {
  "perf_inst_name": "iTherapy",
  "perf_str_addr": "649 Main Street #229",
  "perf_city_name": "Martinez",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "945531313",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "CA10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "118E",
   "pgm_ref_txt": "GRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "8031",
   "pgm_ref_txt": "Education Products"
  },
  {
   "pgm_ref_code": "8032",
   "pgm_ref_txt": "Software Services and Applications"
  },
  {
   "pgm_ref_code": "8039",
   "pgm_ref_txt": "Information, Communication & Computing"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 149964.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Project Outcomes Report</strong></p>\n<p><strong>SBIR-1520587 Say What I Feel</strong><strong>&nbsp;</strong></p>\n<div>\n<p>Introducing Emotional Content in Synthesized Speech via Facial Expression and Vocal Prosody</p>\n</div>\n<p>&nbsp;This SBIR Phase I project aims to create an emotionally expressive software-based speech-generating communication system, which is designed for individuals with little or no verbal communication ability -- particularly those with autism spectrum disorder. Current alternative communication technology does not offer the ability to express emotional content: something that is crucial to effective, comprehensible communication at home, in the community, and at work. The ability to convey emotions -- sadness, anger, or happiness, for example -- through one's tone of voice or facial expressions has far-reaching educational and vocational benefits for individuals with communication challenges. In other words, emotional content helps clarify the communicative intent behind a spoken message. When non-verbal individuals can communicate thoroughly and effectively, they increase their ability to receive a free and appropriate public education, and they expand their vocational opportunities. When individuals can receive an education and find a job, they can contribute back to society. The proposed work supports progress in science and engineering, yet it also enhances the potential for current educational applications and future research studies. Finally, this project builds on Federal and State efforts to offer people with disabilities educational and vocational services.&nbsp;</p>\n<p>This project developed and combine novel software algorithms that target digital facial recognition patterns and speech-based waveforms, in order to enhance the emotional content of conversational communication for individuals who cannot speak volitionally. The ability to add emotional content onto synthesized speech is a new technology that could substantially benefit individuals with communication challenges, such as people with autism spectrum disorder. This project developed a prototype to facilitate and augment emotional expression and, therefore, increase communicative competence for individuals who use software-based synthesized speech systems to express their ideas, feelings, and wishes. Measuring whether or not this project accurately conveys human emotion in speech and facial expressions requires a perceptual test involving neurotypical adults. The purpose of this is to ensure that the facial expressions as well as the pitch contour and vocal emphasis align with the emotion labels (i.e., happy, sad, or angry). In order to accurately calculate the judges' inter-rater reliability, the study will used a method that determines the extent to which judges' ratings agree, relative to how much they would likely conform, if they were to randomly rate the same stimuli.</p>\n<p>The results of Phase 1 research have shown that neurotypical adults most accurately receive and identify synthesized emotional communication content when those messages are sent using facial expressions and speech software, which specifically incorporates tone-of-voice algorithms (i.e., prosody, or the up-and-down pitch and volume contours that accompany emotional content during speech). In the area of communicative sciences, this is new &mdash; and possibly paradigm-shifting &mdash; information. Due to recent advances in mobile communication, facial recognition technology, and software development, communicative sciences have rapidly progressed past studying real-time face-to-face conversations and have advanced toward researching computer-mediated exchanges that involve a variety of media: texts, emails, pictures, short videos &mdash; and even symbolic representations of people, or avatars. Given the mobile technology industry&rsquo;s recent rush toward developing virtual reality produ...",
  "por_txt_cntn": "\nProject Outcomes Report\n\nSBIR-1520587 Say What I Feel \n\n\nIntroducing Emotional Content in Synthesized Speech via Facial Expression and Vocal Prosody\n\n\n This SBIR Phase I project aims to create an emotionally expressive software-based speech-generating communication system, which is designed for individuals with little or no verbal communication ability -- particularly those with autism spectrum disorder. Current alternative communication technology does not offer the ability to express emotional content: something that is crucial to effective, comprehensible communication at home, in the community, and at work. The ability to convey emotions -- sadness, anger, or happiness, for example -- through one's tone of voice or facial expressions has far-reaching educational and vocational benefits for individuals with communication challenges. In other words, emotional content helps clarify the communicative intent behind a spoken message. When non-verbal individuals can communicate thoroughly and effectively, they increase their ability to receive a free and appropriate public education, and they expand their vocational opportunities. When individuals can receive an education and find a job, they can contribute back to society. The proposed work supports progress in science and engineering, yet it also enhances the potential for current educational applications and future research studies. Finally, this project builds on Federal and State efforts to offer people with disabilities educational and vocational services. \n\nThis project developed and combine novel software algorithms that target digital facial recognition patterns and speech-based waveforms, in order to enhance the emotional content of conversational communication for individuals who cannot speak volitionally. The ability to add emotional content onto synthesized speech is a new technology that could substantially benefit individuals with communication challenges, such as people with autism spectrum disorder. This project developed a prototype to facilitate and augment emotional expression and, therefore, increase communicative competence for individuals who use software-based synthesized speech systems to express their ideas, feelings, and wishes. Measuring whether or not this project accurately conveys human emotion in speech and facial expressions requires a perceptual test involving neurotypical adults. The purpose of this is to ensure that the facial expressions as well as the pitch contour and vocal emphasis align with the emotion labels (i.e., happy, sad, or angry). In order to accurately calculate the judges' inter-rater reliability, the study will used a method that determines the extent to which judges' ratings agree, relative to how much they would likely conform, if they were to randomly rate the same stimuli.\n\nThe results of Phase 1 research have shown that neurotypical adults most accurately receive and identify synthesized emotional communication content when those messages are sent using facial expressions and speech software, which specifically incorporates tone-of-voice algorithms (i.e., prosody, or the up-and-down pitch and volume contours that accompany emotional content during speech). In the area of communicative sciences, this is new &mdash; and possibly paradigm-shifting &mdash; information. Due to recent advances in mobile communication, facial recognition technology, and software development, communicative sciences have rapidly progressed past studying real-time face-to-face conversations and have advanced toward researching computer-mediated exchanges that involve a variety of media: texts, emails, pictures, short videos &mdash; and even symbolic representations of people, or avatars. Given the mobile technology industry\u00c6s recent rush toward developing virtual reality products (CNN Money 2016), our team\u00c6s findings could contribute to new branches of communicative sciences &mdash; mainly the study of communication exchanges in computer-mediated c..."
 }
}