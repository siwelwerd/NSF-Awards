{
 "awd_id": "1544741",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Synergy: Collaborative Research: Learning control sharing strategies for assistive cyber-physical systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922568",
 "po_email": "wnilsen@nsf.gov",
 "po_sign_block_name": "Wendy Nilsen",
 "awd_eff_date": "2015-10-01",
 "awd_exp_date": "2021-02-28",
 "tot_intn_awd_amt": 363937.0,
 "awd_amount": 363937.0,
 "awd_min_amd_letter_date": "2015-09-14",
 "awd_max_amd_letter_date": "2020-09-03",
 "awd_abstract_narration": "CPS: Synergy: Collaborative Research: Learning control sharing strategies for assistive cyber-physical systems\r\n\r\nAssistive machines - like powered wheelchairs, myoelectric prostheses and robotic arms - promote independence and ability in those with severe motor impairments. As the state- of-the-art in these assistive Cyber-Physical Systems (CPSs) advances, more dexterous and capable machines hold the promise to revolutionize ways in which those with motor impairments can interact within society and with their loved ones, and to care for themselves with independence. However, as these machines become more capable, they often also become more complex. Which raises the question: how to control this added complexity? A new paradigm is proposed for controlling complex assistive Cyber-Physical Systems (CPSs), like robotic arms mounted on wheelchairs, via simple low-dimensional control interfaces that are accessible to persons with severe motor impairments, like 2-D joysticks or 1-D Sip-N-Puff interfaces. Traditional interfaces cover only a portion of the control space, and during teleoperation it is necessary to switch between different control modes to access the full control space. Robotics automation may be leveraged to anticipate when to switch between different control modes. This approach is a departure from the majority of control sharing approaches within assistive domains, which either partition the control space and allocate different portions to the robot and human, or augment the human's control signals to bridge the dimensionality gap. How to best share control within assistive domains remains an open question, and an appealing characteristic of this approach is that the user is kept maximally in control since their signals are not altered or augmented. The public health impact is significant, by increasing the independence of those with severe motor impairments and/or paralysis. Multiple efforts will facilitate large-scale deployment of our results, including a collaboration with Kinova, a manufacturer of assistive robotic arms, and a partnership with Rehabilitation Institute of Chicago. \r\n\r\nThe proposal introduces a formalism for assistive mode-switching that is grounded in hybrid dynamical systems theory, and aims to ease the burden of teleoperating high-dimensional assistive robots. By modeling this CPS as a hybrid dynamical system, assistance can be modeled as optimization over a desired cost function. The system's uncertainty over the user's goals can be modeled via a Partially Observable Markov Decision Processes. This model provides the natural scaffolding for learning user preferences. Through user studies, this project aims to address the following research questions: (Q1)  Expense: How expensive is mode-switching? (Q2)  Customization Need: Do we need to learn mode-switching from specific users? (Q3)  Learning Assistance: How can we learn mode-switching paradigms from a user? (Q4)  Goal Uncertainty: How should the assistance act under goal uncertainty? How will users respond? The proposal leverages the teams shared expertise in manipulation, algorithm development, and deploying real-world robotic systems. The proposal also leverages the teams complementary strengths on deploying advanced manipulation platforms, robotic motion planning and manipulation, and human-robot comanipulation, and on robot learning from human demonstration, control policy adaptation, and human rehabilitation. The proposed work targets the easier operation of robotic arms by severely paralyzed users. The need to control many degrees of freedom (DoF) gives rise to mode-switching during teleoperation. The switching itself can be cumbersome even with 2- and 3-axis joysticks, and becomes prohibitively so with more limited (1-D) interfaces. Easing the operation of switching not only lowers this burden on those already able to operate robotic arms, but may open use to populations to whom assistive robotic arms are currently inaccessible. This work is clearly synergistic: at the intersection of robotic manipulation, human rehabilitation, control theory, machine learning, human-robot interaction and clinical studies. The project addresses the science of CPS by developing new models of the interaction dynamics between the system and the user, the technology of CPS by developing new interfaces and interaction modalities with strong theoretical foundations, and the engineering of CPS by deploying our algorithms on real robot hardware and extensive studies with able-bodied and users with sprinal cord injuries.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brenna",
   "pi_last_name": "Argall",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brenna Argall",
   "pi_email_addr": "brenna.argall@northwestern.edu",
   "nsf_id": "000602007",
   "pi_start_date": "2015-09-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rehabilitation Institute of Chicago",
  "inst_street_address": "355 E ERIE ST",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3122385195",
  "inst_zip_code": "606113167",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "IL07",
  "org_lgl_bus_name": "REHABILITATION INSTITUTE OF CHICAGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "XAJWT43U55A3"
 },
 "perf_inst": {
  "perf_inst_name": "Rehabilitation Institute of Chicago",
  "perf_str_addr": "345 E. Superior St",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606112654",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "IL05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "8235",
   "pgm_ref_txt": "CPS-Synergy"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 363937.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project addresses the challenge of controlling complex assistive Cyber-Physical Systems, such as robotic arms mounted on wheelchairs, with simple low-dimensional control interfaces that are accessible to persons with severe motor impairments, such as 2-D joysticks or 1-D sip-and-puff interfaces. When the dimensionality of control signals issued by a human operator is smaller than the controllable degree-of-freedom (DoF) of the robot, often the control space is partitioned such that the human operates only a subset of the robot?s DoF at a given time (called a control mode). While one way for autonomy to play a role is to insert control that bridges this dimensionality gap, another way is to assist in switching between control modes. This project explores assistance in the form of the robot autonomy anticipating and performing for the user switches between control modes.</p>\n<p>&nbsp;</p>\n<p>The primary innovation of this project has been the idea of performing automated mode switching with the specific aim of <em>disambiguating</em> user intent. An assistive robotic system typically needs to infer what it is the human operator is aiming to do, and often using only information in the control signals of the human?s command. The general idea of our approach is one of \"help me to help you\"---whereby placing the system into a control mode which better disambiguates human intent during robot teleoperation, the autonomy is able to more quickly and more accurately step in to provide control assistance.</p>\n<p>&nbsp;</p>\n<p>Our algorithm identifies that disambiguating subspace, and selects the control mode which operates it. Our initial foundational algorithm utilized a heuristic metric, that characterized how the shape of the probability distribution over goals evolved as the user moved the robot in different spatial directions. Our later work cast this approach within the space of information theory, where the disambiguation metric characterized the information gain in regards to this distribution. We also introduced a novel intent inference scheme inspired by ideas from dynamic neural fields<em>, </em>in which the time evolution of the probability distribution over goals was specified as a dynamical system with constraints---because our intent disambiguation work revealed the disambiguation power of our algorithm to be intimately linked to the choice of intent inference mechanism. The evaluation study of the foundational algorithm, in which participants operated the robot using two different kinds of control interfaces (a 2-axis joystick and a switch-based headarray), revealed the disambiguation system to have more utility as the control interface becomes more limited and the task becomes more complex.</p>\n<p>&nbsp;</p>\n<p>Another activity under this project developed a framework for modeling human operation of the control interface during robot teleoperation and, critically, explicitly acknowledging differences between intended and issued control commands. Typically, models of human teleoperation treat the source of the human commands as a black box, and assumes that the human is always physically capable of issuing the intended commands---which is a strong assumption within the domain of physically assistive robots. The results of our subject study suggested that interface-aware assistance helped to significantly reduce task effort, and also to reduce cognitive workload and user frustration.</p>\n<p>&nbsp;</p>\n<p>This project also contributed a JavaScript software framework for conducting Human-Robot Interaction (HRI) experiments in a web browser. While no virtual environment can entirely replace the insights gained from having study participants interact with real hardware, there are many aspects of system design and human-autonomy interaction that can be assessed through a virtual system. Executing studies remotely, through a web browser, also has the potential advantage of reaching a broader base of participants---for those with motor impairments in particular, simply reaching the site of a study can be a challenge that limits participation. (Which of course is only exasperated by the current pandemic.) RemoteHRI comes with a flexible set of tools that allow for rapid prototyping and quick deployment of a wide-range of laboratory-like experiments that can be run online. This codebase has been released to the general public.</p>\n<p>&nbsp;</p>\n<p>The project's work was demoed annually at the Museum of Science and Industry in Chicago, one of the leading technology museums in the world, during National Robotics Week. This event brings in over 12,000 people---largely children---over the course of two days. A general software-based interface between ROS and CoppeliaSim furthermore was developed for an undergraduate Introduction to Robotics Laboratory course. This pipeline facilitated the porting of an in-person hardware robotics laboratory course to a virtual simulation environment; increasing access to robotics education by removing the constraint of hardware as a barrier to entry in a robotics laboratory-style course.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/29/2021<br>\n\t\t\t\t\tModified by: Brenna&nbsp;Argall</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1544741/1544741_10398893_1622297391167_21outcomes_fig0--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1544741/1544741_10398893_1622297391167_21outcomes_fig0--rgov-800width.jpg\" title=\"Assistive Robot Arm\"><img src=\"/por/images/Reports/POR/2021/1544741/1544741_10398893_1622297391167_21outcomes_fig0--rgov-66x44.jpg\" alt=\"Assistive Robot Arm\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Assistive robot arm performing an Activities of Daily Living (ADL) task during the evaluation of our intent disambiguation algorithm. For further details:D. Gopinath and B. Argall. Active Intent Disambiguation for Shared-Control Robots. TSNRE, 2020.</div>\n<div class=\"imageCredit\">argallab</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Brenna&nbsp;Argall</div>\n<div class=\"imageTitle\">Assistive Robot Arm</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1544741/1544741_10398893_1622297594831_21outcomes_fig2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1544741/1544741_10398893_1622297594831_21outcomes_fig2--rgov-800width.jpg\" title=\"RemoteHRI Active Stimulus Environments\"><img src=\"/por/images/Reports/POR/2021/1544741/1544741_10398893_1622297594831_21outcomes_fig2--rgov-66x44.jpg\" alt=\"RemoteHRI Active Stimulus Environments\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">RemoteHRI: Publicly-available software for conducting HRI experiments in a web browser. Further details:F. Lau*, D. Gopinath* and B. Argall. A JavaScript Framework for Crowdsourced Human-Robot Interaction Experiments: RemoteHRI. AAAI Fall Symposium, 2020.https://github.com/argallab/RemoteHRI</div>\n<div class=\"imageCredit\">argallab</div>\n<div class=\"imageSubmitted\">Brenna&nbsp;Argall</div>\n<div class=\"imageTitle\">RemoteHRI Active Stimulus Environments</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project addresses the challenge of controlling complex assistive Cyber-Physical Systems, such as robotic arms mounted on wheelchairs, with simple low-dimensional control interfaces that are accessible to persons with severe motor impairments, such as 2-D joysticks or 1-D sip-and-puff interfaces. When the dimensionality of control signals issued by a human operator is smaller than the controllable degree-of-freedom (DoF) of the robot, often the control space is partitioned such that the human operates only a subset of the robot?s DoF at a given time (called a control mode). While one way for autonomy to play a role is to insert control that bridges this dimensionality gap, another way is to assist in switching between control modes. This project explores assistance in the form of the robot autonomy anticipating and performing for the user switches between control modes.\n\n \n\nThe primary innovation of this project has been the idea of performing automated mode switching with the specific aim of disambiguating user intent. An assistive robotic system typically needs to infer what it is the human operator is aiming to do, and often using only information in the control signals of the human?s command. The general idea of our approach is one of \"help me to help you\"---whereby placing the system into a control mode which better disambiguates human intent during robot teleoperation, the autonomy is able to more quickly and more accurately step in to provide control assistance.\n\n \n\nOur algorithm identifies that disambiguating subspace, and selects the control mode which operates it. Our initial foundational algorithm utilized a heuristic metric, that characterized how the shape of the probability distribution over goals evolved as the user moved the robot in different spatial directions. Our later work cast this approach within the space of information theory, where the disambiguation metric characterized the information gain in regards to this distribution. We also introduced a novel intent inference scheme inspired by ideas from dynamic neural fields, in which the time evolution of the probability distribution over goals was specified as a dynamical system with constraints---because our intent disambiguation work revealed the disambiguation power of our algorithm to be intimately linked to the choice of intent inference mechanism. The evaluation study of the foundational algorithm, in which participants operated the robot using two different kinds of control interfaces (a 2-axis joystick and a switch-based headarray), revealed the disambiguation system to have more utility as the control interface becomes more limited and the task becomes more complex.\n\n \n\nAnother activity under this project developed a framework for modeling human operation of the control interface during robot teleoperation and, critically, explicitly acknowledging differences between intended and issued control commands. Typically, models of human teleoperation treat the source of the human commands as a black box, and assumes that the human is always physically capable of issuing the intended commands---which is a strong assumption within the domain of physically assistive robots. The results of our subject study suggested that interface-aware assistance helped to significantly reduce task effort, and also to reduce cognitive workload and user frustration.\n\n \n\nThis project also contributed a JavaScript software framework for conducting Human-Robot Interaction (HRI) experiments in a web browser. While no virtual environment can entirely replace the insights gained from having study participants interact with real hardware, there are many aspects of system design and human-autonomy interaction that can be assessed through a virtual system. Executing studies remotely, through a web browser, also has the potential advantage of reaching a broader base of participants---for those with motor impairments in particular, simply reaching the site of a study can be a challenge that limits participation. (Which of course is only exasperated by the current pandemic.) RemoteHRI comes with a flexible set of tools that allow for rapid prototyping and quick deployment of a wide-range of laboratory-like experiments that can be run online. This codebase has been released to the general public.\n\n \n\nThe project's work was demoed annually at the Museum of Science and Industry in Chicago, one of the leading technology museums in the world, during National Robotics Week. This event brings in over 12,000 people---largely children---over the course of two days. A general software-based interface between ROS and CoppeliaSim furthermore was developed for an undergraduate Introduction to Robotics Laboratory course. This pipeline facilitated the porting of an in-person hardware robotics laboratory course to a virtual simulation environment; increasing access to robotics education by removing the constraint of hardware as a barrier to entry in a robotics laboratory-style course.\n\n\t\t\t\t\tLast Modified: 05/29/2021\n\n\t\t\t\t\tSubmitted by: Brenna Argall"
 }
}