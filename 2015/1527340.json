{
 "awd_id": "1527340",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Object Detection, Pose Estimation, and Semantic Segmentation Using 3D Wireframe Models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 450192.0,
 "awd_amount": 458322.0,
 "awd_min_amd_letter_date": "2015-08-12",
 "awd_max_amd_letter_date": "2018-04-13",
 "awd_abstract_narration": "Humans can recognize a wide variety of object categories in an image, even in the midst of occlusion and clutter, and in spite of significant variability in the objects' shape, size, appearance, and pose. While automatic object recognition systems in computer vision have seen great progress over the past decade, state-of-the-art methods rely primarily on 2D representations of object categories, which cannot capture large variations in the objects' shape and pose. This project develops new 3D representations of objects that are general enough to describe the shape of a wide variety of object categories, yet simple enough so that they can be learned efficiently from a collection of 2D images, and used for detecting and segmenting objects in new images. This project can impact many research areas, including image search, autonomous navigation, medical diagnostic tools, surveillance and robotics.\r\n\r\nThis project develops a new class of 3D wireframe models for view-invariant object detection, fine-grained pose estimation, and semantic segmentation. A wireframe model is a sparse collection of 3D points, edges and surface normals defined only at a few points on the boundaries of the 3D object. The project develops methods for learning wireframe models from 2D images of multiple object categories, methods for learning deformable wireframe models that capture intra-class shape variations across object categories, methods for integrating appearance information into wireframe models, and methods for semantic segmentation based on wireframe models.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rene",
   "pi_last_name": "Vidal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rene Vidal",
   "pi_email_addr": "vidalr@upenn.edu",
   "nsf_id": "000486258",
   "pi_start_date": "2015-08-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N CHARLES ST",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 450192.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8130.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Overview:&nbsp;</strong></p>\n<p>Humans can recognize a wide variety of object categories in an image, even in the midst of occlusion and clutter, and in spite of significant variability in the objects' shape, size, appearance, and pose. While automatic object recognition systems in computer vision have seen great progress over the past decade, state-of-the-art methods rely primarily on 2D representations of object categories, which cannot capture large variations in the objects' shape and pose. This project developed new 3D representations of objects that are general enough to describe the shape of a wide variety of object categories, yet simple enough so that they can be learned efficiently from a collection of 2D images, and used them for detecting and segmenting objects in new images. This project also developed methods for estimating the 3D pose of an object in an image, as well as predicting its category. Applications of this research include autonomous navigation (detection and localization in 3D of vehicles and pedestrians for cars) and robotics (identifying and interacting with objects, locating obstacles and determining room layout for navigation).&nbsp;</p>\n<p><strong>Intellectual Merit :&nbsp;</strong></p>\n<p>This project developed two major approaches to 3D object pose estimation, object detection, segmentation and categorization. The first approach used a new class of 3D object models called 3D wireframe models, which consist of a sparse collection of 3D points, edges and surface normals defined only at a few points on the boundaries of the 3D object. The project developed methods for learning wireframe models from 2D images of multiple object categories, methods for learning deformable wireframe models that capture intra-class shape variations across object categories, methods for integrating appearance information into wireframe models, and methods for semantic segmentation based on wireframe models. The second approach leveraged recent advances on deep learning for object detection and classification in 2D images, by augmenting such approaches so they can also perform 3D pose estimation.&nbsp;Specifically, the project studied the following questions: (1) What is an appropriate representation for 3D object pose and correspondingly, what is the correct problem formulation for this task? (2) What are good loss functions to use while training CNNs for the pose estimation task? And (3) what should the network architecture be for these pose CNNs?&nbsp;Applications of this research include autonomous navigation (detection and localization in 3D of vehicles and pedestrians for cars) and robotics (identifying and interacting with objects, locating obstacles and determining room layout for navigation).</p>\n<p><strong>Broader Impacts :&nbsp;</strong></p>\n<p>This project significantly broadened the applicability of existing deep learning methods from 2D scenes to 3D scenes, which is needed for applications in autonomous navigation (detection and localization in 3D of vehicles and pedestrians for cars) and robotics (identifying and interacting with objects, locating obstacles and determining room layout for navigation). This project impacted many diversity outreach activities, including undergraduate students joining ongoing REU programs, students from the Society of Hispanic Professional Engineers (SHPE), and students from the STEM Achievement in Baltimore Elementary Schools (SABES) program. This project trained 1 PhD students and 1 undergraduate students at the intersection of machine learning, computer vision and optimization. This project produced 1 PhD thesis, 2 journal papers, and 4 conference papers. This project also impacted the research community through the organization of workshops in geometric learning, deep learning, and semantic information. Datasets and code will be made publicly accessible for research and educational purposes.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/11/2020<br>\n\t\t\t\t\tModified by: Rene&nbsp;Vidal</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1527340/1527340_10387006_1589238737171_Outcomes-NSF-ObjRec15--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1527340/1527340_10387006_1589238737171_Outcomes-NSF-ObjRec15--rgov-800width.jpg\" title=\"The Monocular Orientation Estimation problem\"><img src=\"/por/images/Reports/POR/2020/1527340/1527340_10387006_1589238737171_Outcomes-NSF-ObjRec15--rgov-66x44.jpg\" alt=\"The Monocular Orientation Estimation problem\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Monocular Orientation Estimation problem</div>\n<div class=\"imageCredit\">Siddharth Mahendran</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Rene&nbsp;Vidal</div>\n<div class=\"imageTitle\">The Monocular Orientation Estimation problem</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nOverview: \n\nHumans can recognize a wide variety of object categories in an image, even in the midst of occlusion and clutter, and in spite of significant variability in the objects' shape, size, appearance, and pose. While automatic object recognition systems in computer vision have seen great progress over the past decade, state-of-the-art methods rely primarily on 2D representations of object categories, which cannot capture large variations in the objects' shape and pose. This project developed new 3D representations of objects that are general enough to describe the shape of a wide variety of object categories, yet simple enough so that they can be learned efficiently from a collection of 2D images, and used them for detecting and segmenting objects in new images. This project also developed methods for estimating the 3D pose of an object in an image, as well as predicting its category. Applications of this research include autonomous navigation (detection and localization in 3D of vehicles and pedestrians for cars) and robotics (identifying and interacting with objects, locating obstacles and determining room layout for navigation). \n\nIntellectual Merit : \n\nThis project developed two major approaches to 3D object pose estimation, object detection, segmentation and categorization. The first approach used a new class of 3D object models called 3D wireframe models, which consist of a sparse collection of 3D points, edges and surface normals defined only at a few points on the boundaries of the 3D object. The project developed methods for learning wireframe models from 2D images of multiple object categories, methods for learning deformable wireframe models that capture intra-class shape variations across object categories, methods for integrating appearance information into wireframe models, and methods for semantic segmentation based on wireframe models. The second approach leveraged recent advances on deep learning for object detection and classification in 2D images, by augmenting such approaches so they can also perform 3D pose estimation. Specifically, the project studied the following questions: (1) What is an appropriate representation for 3D object pose and correspondingly, what is the correct problem formulation for this task? (2) What are good loss functions to use while training CNNs for the pose estimation task? And (3) what should the network architecture be for these pose CNNs? Applications of this research include autonomous navigation (detection and localization in 3D of vehicles and pedestrians for cars) and robotics (identifying and interacting with objects, locating obstacles and determining room layout for navigation).\n\nBroader Impacts : \n\nThis project significantly broadened the applicability of existing deep learning methods from 2D scenes to 3D scenes, which is needed for applications in autonomous navigation (detection and localization in 3D of vehicles and pedestrians for cars) and robotics (identifying and interacting with objects, locating obstacles and determining room layout for navigation). This project impacted many diversity outreach activities, including undergraduate students joining ongoing REU programs, students from the Society of Hispanic Professional Engineers (SHPE), and students from the STEM Achievement in Baltimore Elementary Schools (SABES) program. This project trained 1 PhD students and 1 undergraduate students at the intersection of machine learning, computer vision and optimization. This project produced 1 PhD thesis, 2 journal papers, and 4 conference papers. This project also impacted the research community through the organization of workshops in geometric learning, deep learning, and semantic information. Datasets and code will be made publicly accessible for research and educational purposes. \n\n \n\n\t\t\t\t\tLast Modified: 05/11/2020\n\n\t\t\t\t\tSubmitted by: Rene Vidal"
 }
}