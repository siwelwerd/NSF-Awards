{
 "awd_id": "1524371",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: A Data-Driven Framework to Sketch-to-Text Generation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2015-07-15",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 449999.0,
 "awd_amount": 449999.0,
 "awd_min_amd_letter_date": "2015-07-13",
 "awd_max_amd_letter_date": "2015-07-13",
 "awd_abstract_narration": "The project aims to address the limitations of the current natural language generation (NLG) systems by seeking new data-driven approaches to modeling the contextual and creative dimensions of text composition. By taking a large collection of online text as an unstructured database of rhetorical patterns and linguistic creativity, the project develops a statistical generation engine that is capable of composing text with a new level of linguistic creativity and sophistication than what has been previously possible. Formulating sketch-to-text generation as a conceptual framework, the project investigates automatic composition of image captions and product descriptions as application scenarios. The project also explores new possibilities of human-computer collaborative writing, by developing an interactive search-based editor that will assist student writers to learn from a large collection of other people's writings. The technical outcome of the project has the potential to benefit our society in two ways: first, by advancing automatic image captioning for a wide variety of everyday photographs, it can contribute toward equal web access for visually impaired. Second, by enabling interactive search channels over a large-corpus of online writings, it can create new education experiences for training students' writing skills. The project is instrumental for supporting the PI's ongoing efforts in attracting and educating students from underrepresented groups. \r\n\r\nThe proposed research is based on the premise that large-scale online writings, if used correctly, can be an enabling factor for sketch-to-text generation. The project consists of three fundamental research activities. First, the project proposes composition frames and elements as a new conceptual formalism to organize rhetorical patterns as building blocks, and develops unsupervised algorithms to extract them from a large-scale domain-specific corpus. Second, the project develops statistical approaches to differentiate literal language from figurative, with the specific goal of controlling the degree of literalness and creativity in the generated language. Finally, the proposed work designs scalable and robust inference algorithms for composition formulated as constrained optimization. Technical contributions include several unique resources to be shared with the research community, including a new large-scale corpus of image-caption pairs, and the database of learned composition frames and elements.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yejin",
   "pi_last_name": "Choi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yejin Choi",
   "pi_email_addr": "yejin@cs.washington.edu",
   "nsf_id": "000584106",
   "pi_start_date": "2015-07-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 449999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to seek new computational models of natural language generation (NLG) that can dramatically enhance the quality of machine text. Overall, we met the objectives of this project successfully on all fronts, often the results significantly exceeding the expected scope and the performance level of the proposal. In what follows, we summarize the key achievements of this project from seven different angles:<br /><strong></strong></p>\n<p><strong>1. \"Sketch-to-text generation\" as a conceptual framework</strong></p>\n<p>This project investigated a new conceptual framework of sketch-to-text generation. Applications under this framework require more open-ended text generation, for example, automatic sonnet generation given a topic phrase, automatic story continuation given a short story prompt, automatic conversation on random topics, automatic recipe generation from available ingredients, and more broadly, any long-form text generation applications such as document-to-document summarization. Common to all these application scenarios is that the input text provides only a rough sketch of what exactly needs to be generated in the output text. <strong></strong></p>\n<p><strong>2. Extreme-scale data-driven investigation</strong></p>\n<p><strong></strong>This project investigated data-driven approaches to robust text generation. For example, in our study on neural fake news detection, we trained neural language models based on the transformer architecture using documents that amount to 120 gigabytes to train a model with 1.5 billion parameters. Our study based on this extreme-scale data-driven regime resulted in several new insights in terms of both the advantages and limitations of neural text generation. More concretely, we found that the long-term coherence and the discourse quality can improve drastically---far beyond our expectations---from the purely data-driven scale-up. However, the quality of the tail distribution remains to be a major challenge, which leads to the non-negligible semantic inconsistencies and distributional watermarks that are unique in machine generated text compared to human text. <br /><br /><strong>3. Grounding generation with multimodal and commonsense knowledge</strong></p>\n<p>A major research goal of this project was to model the rich context of text generation. This project significantly enriched the scope of contextual text generation by investigating two types of contextual information: (1) multimodal context such as images, and (2) knowledge context such as relevant wikipedia articles or commonsense knowledge graphs. In particular, we have developed new computational models commonsense knowledge representations as well as neural architectures for effective knowledge-grounded text generation.<br /><br /><strong>4. Coorperative, multi-component neural architectures</strong></p>\n<p>A major technical challenge with sketch-to-text generation is long-term coherence. We have developed several new neural network architectures that are based on cooperative, multimodal components, to address the limitations of the monolithic architectures more commonly used for language modeling. For example, we have developed the \"learning-to-write\" framework, where a collection of discriminators that are inspired by Grice's maxims of communications work together with the base-generator to compose text of significantly higher discourse quality. We then extended this to \"Co-op-Net\" that generalizes and upgrades the individual components in the previous framework. We have also developed the \"Deep Communicating Agents\" framework where a collection of agents collaborate to compose a long-form document summarization with enhanced overall long-term coherence.<br /><br /><strong>5. New insights on neural text degeneration</strong></p>\n<p>In addition to the strong modeling contributions with major empirical advancements, we have also provided new insights on the neural text <em><strong>de</strong></em>generation: i.e., why even the best state-of-the-art neural networks assign higher probability scores to generic and repetitive text compared to more high-quality and human-like text. Based on the new insights, we have also proposed a new decoding algorithm, \"Nucleus Sampling\" that dramatically improves the quality of machine text for open-ended long-form text generation.<br /><br /><strong>6. Compelling NLG applications</strong></p>\n<p>This project spanned over a number of compelling NLG applications, including automatic sonnet generation given a topic phrase, automatic story continuation given a short story prompt, automatic conversation on random topics, automatic recipe generation from available ingredients, and more broadly, any long-form text generation applications such as document-to-document summarization. <br /><br /><strong>7. Broader Impacts</strong></p>\n<p>This project supported the training of diverse demographic backgrounds of students and postdocs, including undergraduate women students who advanced to their phd study, and several women phd students who advanced to top academic insitutiitions and research labs. In addition, we have addressed the emerging concerns about the potential misuse of neural language models for fake news generation by studying the threat modeling and the defense mechanisms with collaborations with the computer security experts. For dissemination, the PI has given numerous invited talks at conferences, workshops, and universities based on the research that resulted from this project. All papers, codes, datasets, and online demos are shared publicly online.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/28/2019<br>\n\t\t\t\t\tModified by: Yejin&nbsp;Choi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to seek new computational models of natural language generation (NLG) that can dramatically enhance the quality of machine text. Overall, we met the objectives of this project successfully on all fronts, often the results significantly exceeding the expected scope and the performance level of the proposal. In what follows, we summarize the key achievements of this project from seven different angles:\n\n\n1. \"Sketch-to-text generation\" as a conceptual framework\n\nThis project investigated a new conceptual framework of sketch-to-text generation. Applications under this framework require more open-ended text generation, for example, automatic sonnet generation given a topic phrase, automatic story continuation given a short story prompt, automatic conversation on random topics, automatic recipe generation from available ingredients, and more broadly, any long-form text generation applications such as document-to-document summarization. Common to all these application scenarios is that the input text provides only a rough sketch of what exactly needs to be generated in the output text. \n\n2. Extreme-scale data-driven investigation\n\nThis project investigated data-driven approaches to robust text generation. For example, in our study on neural fake news detection, we trained neural language models based on the transformer architecture using documents that amount to 120 gigabytes to train a model with 1.5 billion parameters. Our study based on this extreme-scale data-driven regime resulted in several new insights in terms of both the advantages and limitations of neural text generation. More concretely, we found that the long-term coherence and the discourse quality can improve drastically---far beyond our expectations---from the purely data-driven scale-up. However, the quality of the tail distribution remains to be a major challenge, which leads to the non-negligible semantic inconsistencies and distributional watermarks that are unique in machine generated text compared to human text. \n\n3. Grounding generation with multimodal and commonsense knowledge\n\nA major research goal of this project was to model the rich context of text generation. This project significantly enriched the scope of contextual text generation by investigating two types of contextual information: (1) multimodal context such as images, and (2) knowledge context such as relevant wikipedia articles or commonsense knowledge graphs. In particular, we have developed new computational models commonsense knowledge representations as well as neural architectures for effective knowledge-grounded text generation.\n\n4. Coorperative, multi-component neural architectures\n\nA major technical challenge with sketch-to-text generation is long-term coherence. We have developed several new neural network architectures that are based on cooperative, multimodal components, to address the limitations of the monolithic architectures more commonly used for language modeling. For example, we have developed the \"learning-to-write\" framework, where a collection of discriminators that are inspired by Grice's maxims of communications work together with the base-generator to compose text of significantly higher discourse quality. We then extended this to \"Co-op-Net\" that generalizes and upgrades the individual components in the previous framework. We have also developed the \"Deep Communicating Agents\" framework where a collection of agents collaborate to compose a long-form document summarization with enhanced overall long-term coherence.\n\n5. New insights on neural text degeneration\n\nIn addition to the strong modeling contributions with major empirical advancements, we have also provided new insights on the neural text degeneration: i.e., why even the best state-of-the-art neural networks assign higher probability scores to generic and repetitive text compared to more high-quality and human-like text. Based on the new insights, we have also proposed a new decoding algorithm, \"Nucleus Sampling\" that dramatically improves the quality of machine text for open-ended long-form text generation.\n\n6. Compelling NLG applications\n\nThis project spanned over a number of compelling NLG applications, including automatic sonnet generation given a topic phrase, automatic story continuation given a short story prompt, automatic conversation on random topics, automatic recipe generation from available ingredients, and more broadly, any long-form text generation applications such as document-to-document summarization. \n\n7. Broader Impacts\n\nThis project supported the training of diverse demographic backgrounds of students and postdocs, including undergraduate women students who advanced to their phd study, and several women phd students who advanced to top academic insitutiitions and research labs. In addition, we have addressed the emerging concerns about the potential misuse of neural language models for fake news generation by studying the threat modeling and the defense mechanisms with collaborations with the computer security experts. For dissemination, the PI has given numerous invited talks at conferences, workshops, and universities based on the research that resulted from this project. All papers, codes, datasets, and online demos are shared publicly online.\n\n\t\t\t\t\tLast Modified: 10/28/2019\n\n\t\t\t\t\tSubmitted by: Yejin Choi"
 }
}