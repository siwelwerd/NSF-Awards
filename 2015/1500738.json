{
 "awd_id": "1500738",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Contributions of Endangered Language Data for Advances in Technology-enhanced Speech Annotation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "D.  Langendoen",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2020-10-31",
 "tot_intn_awd_amt": 221505.0,
 "awd_amount": 286358.0,
 "awd_min_amd_letter_date": "2015-06-18",
 "awd_max_amd_letter_date": "2020-06-01",
 "awd_abstract_narration": "Linguists have increased efforts to collect authentic speech materials from endangered and little-studied languages to discover linguistic diversity. However, the challenge of transcribing these speech into written form to facilitate analysis is daunting. This is because of both the sheer quantity of digitally collected speech that needs to be transcribed and the difficulty of unpacking the sounds of spoken speech.  \r\n\r\nLinguist Andreas Kathol and computer scientist Vikramjit Mitra of SRI international and linguist Jonathan D. Amith of Gettysburg College will team up to create software that can substantially reduce the language transcription bottleneck. Using as a test case Yoloxochitl  Mixtec, an endangered language from the state of Guerrero, Mexico, the team will develop a software tool that will use previously transcribed Yoloxochitl Mixtec speech data to both train a new generation of native speakers in practical orthography and to develop automatic speech recognition software. The output of the recognition software will be used as preliminary transcription that native speakers will correct, as necessary, to create additional high-quality training data. This recursive method will create corpus of transcribed speech large enough so that software will be able to complete automatic transcription of newly collected speech materials.\r\n\r\nThe project will include the training of undergraduate and graduate students in software development and the analysis of the Yoloxochitl Mixtec sound system. The project will also train native speakers as documenters in an interactive fashion that systematically introduces them to the transcription conventions of their language. This software tool will help in establishing literacy in Yoloxochitl Mixtec among a broader base of speakers.\r\n\r\nThe results of this project will be available at the Archive of Indigenous Languages of Latin America (University of Texas, Austin), Kaipuleohone (University of Hawai'i Digital Language Archive),  and at the Linguistic Data Consortium (University of Pennsylvania).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andreas",
   "pi_last_name": "Kathol",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andreas Kathol",
   "pi_email_addr": "kathol@speech.sri.com",
   "nsf_id": "000653094",
   "pi_start_date": "2015-06-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vikramjit",
   "pi_last_name": "Mitra",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vikramjit Mitra",
   "pi_email_addr": "vmitra@speech.sri.com",
   "nsf_id": "000602622",
   "pi_start_date": "2015-06-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SRI International",
  "inst_street_address": "333 RAVENSWOOD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "MENLO PARK",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6097342285",
  "inst_zip_code": "940253493",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "SRI INTERNATIONAL",
  "org_prnt_uei_num": "SRG2J1WS9X63",
  "org_uei_num": "SRG2J1WS9X63"
 },
 "perf_inst": {
  "perf_inst_name": "SRI International",
  "perf_str_addr": "333 Ravenswood Ave.",
  "perf_city_name": "Menlo Park",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "940253493",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7298",
   "pgm_ref_txt": "COLLABORATIVE RESEARCH"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7719",
   "pgm_ref_txt": "DEL"
  },
  {
   "pgm_ref_code": "7791",
   "pgm_ref_txt": "ULAFOS SPECIAL PROGRAMS"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 237505.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 32853.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The documentation of endangered languages has become an increasing priority for researchers and those native speaker communities that wish to maintain and revitalize their language and culture. Estimates vary of the number of extant languages, the rate of language disappearance and cultural loss, and even the meaning of cultural loss and language death (versus change and shift, as when an Indigenous language is increasingly impacted by colonial languages and cultures). It appears certain that the majority of approximately 6,000-7,000 languages presently spoken will effectively disappear within this century. With this, the diversity of linguistic expression is diminished and the vast reservoir of human knowledge is impoverished.<br /><br />Yet our ability to document endangered languages and cultures is inhibited by a \"transcription bottleneck\": hundreds of hours of high quality recordings can be produced in a short amount of time but take months to process. For example, in one week in the village of Yoloxochitl (Guerrero, Mexico), Jonathan Amith, a project co-PI recorded 30 hours of conversations between a dozen native speakers. An excellent native speaker linguist, Rey Castillo Garcia, would require close to a year to produce an archival quality transcription and translation of this material. To address this issue researchers have looked to automatic speech recognition (ASR), the computer-generated production of a written transcription of an audio recording. This project has explored the possibility of using ASR to achieve a high level of accuracy in the transcription of Yoloxochitl Mixtec (YM), an endangered tonal language from west-central Mexico.<br /><br />The project's primary goal has been to address the \"transcription bottleneck\" mentioned above. This was accomplished in part by developing, under the direction of co-PI Andreas Kathol, a YM speech recognizer based on recent advances in ASR. First versions of this recognizer at the beginning of the project resulted in a word error rate (WER) of 30.1% for a held-out testset.&nbsp; By project end the WER had been reduced to 19%.<br /><br />A secondary set of goal has been to (1) TRAIN native speakers to write their language through a progression of increasingly challenging transcription tasks; (2) TEST native speakers on their success in learning as they progressed through increasingly difficult lessons; and (3) allow users to ANNOTATE, or correct, a transcription proposed by the ASR algorithm. To accomplish all these tasks a TTA (Training, Testing, Annotation) tool was created at SRI International and installed on a Chromebook laptop. The Training and Testing was evaluated with the participation of a native speaker, Esteban Guadalupe Sierra, who finished all lessons and then began to transcribe directly from audio. He has reached a level that is 97% in accord with the transcription of Rey Castillo Garcia, the expert native speaker linguist.<br /><br />A final goal was to implement ASR (along with the TTA tool) on Chromebook computers to be used in Yoloxochitl. Amith along with Castillo and Guadalupe would work with native speakers to obtain audio recordings. The speech would then be processed locally through the ASR system on the Chromebook and the transcription would be loaded into the TAA tool to be used in Annotation mode. Then the original speaker would correct the computer-generated transcription as best as possible. Correcting mistakes in tone, nasalization, or glottal stops is still challenging for all but the best transcribers. However, speakers can be expected to reliably eliminate words that ASR has mistakenly inserted or add words that the ASR program has failed to transcribe. The transcription so corrected by the narrator would then be given to Castillo to be finalized.<br /><br />We fully expect recognition accuracy to improve further with further advances in ASR technology. But even with the ASR accuracy achieved so far this project has shown the utility of an integrated approach to Training and Testing native speakers, recording and processing ASR locally, and then having the original speaker take the first step in reviewing his or her computer-generated transcription.<br /><br /><br /><br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/26/2021<br>\n\t\t\t\t\tModified by: Andreas&nbsp;Kathol</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe documentation of endangered languages has become an increasing priority for researchers and those native speaker communities that wish to maintain and revitalize their language and culture. Estimates vary of the number of extant languages, the rate of language disappearance and cultural loss, and even the meaning of cultural loss and language death (versus change and shift, as when an Indigenous language is increasingly impacted by colonial languages and cultures). It appears certain that the majority of approximately 6,000-7,000 languages presently spoken will effectively disappear within this century. With this, the diversity of linguistic expression is diminished and the vast reservoir of human knowledge is impoverished.\n\nYet our ability to document endangered languages and cultures is inhibited by a \"transcription bottleneck\": hundreds of hours of high quality recordings can be produced in a short amount of time but take months to process. For example, in one week in the village of Yoloxochitl (Guerrero, Mexico), Jonathan Amith, a project co-PI recorded 30 hours of conversations between a dozen native speakers. An excellent native speaker linguist, Rey Castillo Garcia, would require close to a year to produce an archival quality transcription and translation of this material. To address this issue researchers have looked to automatic speech recognition (ASR), the computer-generated production of a written transcription of an audio recording. This project has explored the possibility of using ASR to achieve a high level of accuracy in the transcription of Yoloxochitl Mixtec (YM), an endangered tonal language from west-central Mexico.\n\nThe project's primary goal has been to address the \"transcription bottleneck\" mentioned above. This was accomplished in part by developing, under the direction of co-PI Andreas Kathol, a YM speech recognizer based on recent advances in ASR. First versions of this recognizer at the beginning of the project resulted in a word error rate (WER) of 30.1% for a held-out testset.  By project end the WER had been reduced to 19%.\n\nA secondary set of goal has been to (1) TRAIN native speakers to write their language through a progression of increasingly challenging transcription tasks; (2) TEST native speakers on their success in learning as they progressed through increasingly difficult lessons; and (3) allow users to ANNOTATE, or correct, a transcription proposed by the ASR algorithm. To accomplish all these tasks a TTA (Training, Testing, Annotation) tool was created at SRI International and installed on a Chromebook laptop. The Training and Testing was evaluated with the participation of a native speaker, Esteban Guadalupe Sierra, who finished all lessons and then began to transcribe directly from audio. He has reached a level that is 97% in accord with the transcription of Rey Castillo Garcia, the expert native speaker linguist.\n\nA final goal was to implement ASR (along with the TTA tool) on Chromebook computers to be used in Yoloxochitl. Amith along with Castillo and Guadalupe would work with native speakers to obtain audio recordings. The speech would then be processed locally through the ASR system on the Chromebook and the transcription would be loaded into the TAA tool to be used in Annotation mode. Then the original speaker would correct the computer-generated transcription as best as possible. Correcting mistakes in tone, nasalization, or glottal stops is still challenging for all but the best transcribers. However, speakers can be expected to reliably eliminate words that ASR has mistakenly inserted or add words that the ASR program has failed to transcribe. The transcription so corrected by the narrator would then be given to Castillo to be finalized.\n\nWe fully expect recognition accuracy to improve further with further advances in ASR technology. But even with the ASR accuracy achieved so far this project has shown the utility of an integrated approach to Training and Testing native speakers, recording and processing ASR locally, and then having the original speaker take the first step in reviewing his or her computer-generated transcription.\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 02/26/2021\n\n\t\t\t\t\tSubmitted by: Andreas Kathol"
 }
}