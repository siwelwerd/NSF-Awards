{
 "awd_id": "1445604",
 "agcy_id": "NSF",
 "tran_type": "CoopAgrmnt",
 "awd_istr_txt": "Cooperative Agreement",
 "awd_titl_txt": "High Performance Computing System Acquisition: Jetstream - A Self-Provisioned, Scalable Science and Engineering Cloud Environment",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922247",
 "po_email": "rchadduc@nsf.gov",
 "po_sign_block_name": "Robert Chadduck",
 "awd_eff_date": "2014-12-01",
 "awd_exp_date": "2022-11-30",
 "tot_intn_awd_amt": 6576101.0,
 "awd_amount": 14496404.0,
 "awd_min_amd_letter_date": "2014-11-20",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "High Performance Computing System Acquisition: Jetstream - a self-provisioned, scalable science and engineering cloud environment\r\n\r\nJetstream will be a new type of computational research resource open for the national (nonclassified) research community - a data analysis and computational resource that US scientists and engineers will use interactively to conduct their research anytime, anywhere. Jetstream will complement current NSF-funded computational resources and bring a cloud-based system to the NSF computational resources incorporating the best elements of commercial cloud computing resources with some of the best software in existence for solving important scientific problems. This system will enable many US researchers and engineers to make new discoveries that are important to understanding the world around us and will help researchers make new discoveries that improve the quality of life of American citizens.\r\n\r\nIn terms of technical details, Jetstream will be a configurable large-scale computing resource that leverages both on-demand and persistent virtual machine technology to support a much wider array of software environments and services than current NSF resources can accommodate. As a fully configurable \"cloud\" resource, Jetstream bridges the obvious major gap in the current ecosystem, which has machines targeted at large-scale High-Performance Computing, high memory, large data, high-throughput, and visualization resources. As the open cloud for science, Jetstream will:\r\n \r\n*Provide \"self-serve\" academic cloud services, enabling researchers or students to select a VM image from a published library, or alternatively to create or customize their own virtual environment for discipline- or task-specific personalized research computing.\r\n\r\n*Host persistent VMs to provide services beyond the command line interface for science gateways and other science services. For example, Jetstream will become a primary host of the popular Galaxy scientific workbench and its main datasets, bringing many Galaxy users to the NSF ecosystem from day one.\r\n \r\n*Enable new modes of sharing computations, data, and reproducibility.\r\n \r\n*Expand access to the NSF XSEDE ecosystem by making virtual desktop services accessible from institutions with limited resources",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Craig",
   "pi_last_name": "Stewart",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Craig A Stewart",
   "pi_email_addr": "stewart@iu.edu",
   "nsf_id": "000188281",
   "pi_start_date": "2014-11-20",
   "pi_end_date": "2018-02-27"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Hancock",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "David Y Hancock",
   "pi_email_addr": "dyhancoc@iu.edu",
   "nsf_id": "000707272",
   "pi_start_date": "2018-02-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ian",
   "pi_last_name": "Foster",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ian Foster",
   "pi_email_addr": "foster@uchicago.edu",
   "nsf_id": "000234022",
   "pi_start_date": "2014-11-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Vaughn",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew W Vaughn",
   "pi_email_addr": "vaughn@tacc.utexas.edu",
   "nsf_id": "000300030",
   "pi_start_date": "2014-11-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Nirav",
   "pi_last_name": "Merchant",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Nirav C Merchant",
   "pi_email_addr": "nirav@email.arizona.edu",
   "nsf_id": "000485683",
   "pi_start_date": "2014-11-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Taylor",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "James Taylor",
   "pi_email_addr": "james@taylorlab.org",
   "nsf_id": "000212822",
   "pi_start_date": "2014-11-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University",
  "perf_str_addr": "509 E. 3rd St.",
  "perf_city_name": "Bloomington",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "474013654",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IN09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  },
  {
   "pgm_ele_code": "747600",
   "pgm_ele_name": "XD-Extreme Digital"
  },
  {
   "pgm_ele_code": "761900",
   "pgm_ele_name": "Innovative HPC"
  },
  {
   "pgm_ele_code": "772600",
   "pgm_ele_name": "Data Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7619",
   "pgm_ref_txt": "EQUIPMENT ACQUISITIONS"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "116E",
   "pgm_ref_txt": "RESEARCH EXP FOR UNDERGRADS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 6576101.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 5260880.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 503729.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 1363219.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 48000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 744475.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-fcd038d7-7fff-0bc5-627c-1d81d5180743\"> </span></p>\n<p dir=\"ltr\"><span>Indiana University and Jetstream partners fundamentally changed the landscape of the National Science Foundation cyberinfrastructure (CI) ecosystem through the Jetstream project. The Jetstream project served as a first-of-a-kind environment as a production cloud system, while simultaneously acting as a pilot deployment, positively influencing the NSF Advanced Computing Systems and Services (ACSS) program that followed. Jetstream provided on-demand computing and storage anytime, anywhere. By design and in practice, Jetstream was created as programmable cyberinfrastructure in that users could reconfigure the system to suit their needs; this took many forms, from multi-user platforms that leveraged Jetstream to configure their services, to single individuals who utilized Matlab interactively. Jetstream was a very different system from other XSEDE resources, a highly-usable system with a core focus on those in the long tail of science.&nbsp;</span></p>\n<p>Jetstream provided services directly to 18,714 researchers and educators, including 8,836 students from June 2016 through July 2022 as part of 1,220 projects in 69 fields of science for individuals at 399 institutions during this time period. The 63 science gateways that utilized Jetstream indirectly supported over 183,197 people over the life of the system, most without even knowing their use of the resource. As a training and instructional resource Jetstream was used seven times more than any other XSEDE resource in terms of Educational Allocations. Over the life of Jetstream, 216 courses and workshops utilized Jetstream as part of their instruction. These activities included multi-day workshops for early career researchers, continuing education for researchers, semester long courses for undergraduate and graduate students, and semester-long capstone projects for masters level students. Forty-six courses were conducted multiple times on Jetstream.</p>\n<p dir=\"ltr\"><span>The Jetstream environment was accepted under OpenStack Liberty and retired with the Rocky release on CentOS 7.8. Overall, the system went through seven major OpenStack releases without regularly scheduled downtime or unscheduled interruptions. Through the six years of operations Jetstream had an overall availability of 98.54% including planned and unplanned outages, and an uptime of 99.9967% where the system was operating in some form but at a reduced capacity. This was coupled with above average user satisfaction and importance, obtained via annual user surveys (1-5 scale) the \"Importance of Jetstream for Research or Education\" grew year over year, concluding with a rating &gt;4, along with the quality/speed to support question response averaging &gt;4.5.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The services pioneered on Jetstream or by the project team have also had a significant impact on currently funded Category I resources as well as within discipline-specific projects that leverage the environment including: dynamically scaling virtual clusters used by the Galaxy project, providing non-command-line access to CI resources by default through single sign-on (including providing resources to spawn a new OpenStack interface, Exosphere), providing the first virtual GPUs, providing the first object storage system in the NSF CI ecosystem, and the first cloud native orchestration interface to a production NSF resource (e.g. OpenStack Magnum used with Kubernetes and Docker Swarm).&nbsp;</span></p>\n<p dir=\"ltr\"><span>Jetstream's impact on student, researcher, and educator success took multiple forms:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>As was widely published in the mainstream media, a group of scientists produced the first image of a black hole using Event Horizon Telescope (EHT) observations of the center of the </span><a href=\"https://iopscience.iop.org/article/10.3847/2041-8213/ab0ec7\"><span>galaxy M87</span></a><span>. As part of this work, Jetstream was used to develop cloud-based data analysis pipelines that were critical for combining and sharing data taken from the geographically-distributed observatories. </span><span>It was noted that the cloud pipeline used for analysis could not have been developed without Jetstream.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Jetstream joined the COVID-19 HPC Consortium in March 2020 to contribute resources to the pandemic response. The team participated in joint reporting efforts to the NSF throughout the pandemic with numerous projects taking advantage of the resource. These COVID-19 related projects include gateways such as Galaxy (used for some of the earliest genomic analysis) and ChemCompute (used by students for remote learning), as well as for international medical records projects like OpenMRS, and regional information such as projections by University of Texas, San Antonio for Texas hospital bed and ICU bed usage.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Jetstream hosted an Research Experience for Undergraduates program for five consecutive years from 2017 through 2021 for 4-6 students per year (25 students slots total) that resulted in tangible outcomes for the students in the form of project papers, and opportunities to present at local events to their peers, as well as national forums such as the PEARC conference series and even at the SC Conference Series, the International Conference for High Performance Computing, Networking, Storage, and Analysis. The program focused recruitment on individuals from MSIs, HSI, and EPSCoR jurisdictions.&nbsp;</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/23/2023<br>\n\t\t\t\t\tModified by: David&nbsp;Y&nbsp;Hancock</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1445604/1445604_10344159_1679575716760_ucdhve4rur_actual--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1445604/1445604_10344159_1679575716760_ucdhve4rur_actual--rgov-800width.jpg\" title=\"Jetstream system at Indiana University\"><img src=\"/por/images/Reports/POR/2023/1445604/1445604_10344159_1679575716760_ucdhve4rur_actual--rgov-66x44.jpg\" alt=\"Jetstream system at Indiana University\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Picture of the Jetstream system located at Indiana University in Bloomington, Indiana.</div>\n<div class=\"imageCredit\">Indiana University</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">David&nbsp;Y&nbsp;Hancock</div>\n<div class=\"imageTitle\">Jetstream system at Indiana University</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1445604/1445604_10344159_1679575629428_Jetstream_topology_diagram--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1445604/1445604_10344159_1679575629428_Jetstream_topology_diagram--rgov-800width.jpg\" title=\"Jetstream Architecture Diagram\"><img src=\"/por/images/Reports/POR/2023/1445604/1445604_10344159_1679575629428_Jetstream_topology_diagram--rgov-66x44.jpg\" alt=\"Jetstream Architecture Diagram\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Jetstream architecture as originally deployed</div>\n<div class=\"imageCredit\">Indiana University</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">David&nbsp;Y&nbsp;Hancock</div>\n<div class=\"imageTitle\">Jetstream Architecture Diagram</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nIndiana University and Jetstream partners fundamentally changed the landscape of the National Science Foundation cyberinfrastructure (CI) ecosystem through the Jetstream project. The Jetstream project served as a first-of-a-kind environment as a production cloud system, while simultaneously acting as a pilot deployment, positively influencing the NSF Advanced Computing Systems and Services (ACSS) program that followed. Jetstream provided on-demand computing and storage anytime, anywhere. By design and in practice, Jetstream was created as programmable cyberinfrastructure in that users could reconfigure the system to suit their needs; this took many forms, from multi-user platforms that leveraged Jetstream to configure their services, to single individuals who utilized Matlab interactively. Jetstream was a very different system from other XSEDE resources, a highly-usable system with a core focus on those in the long tail of science. \n\nJetstream provided services directly to 18,714 researchers and educators, including 8,836 students from June 2016 through July 2022 as part of 1,220 projects in 69 fields of science for individuals at 399 institutions during this time period. The 63 science gateways that utilized Jetstream indirectly supported over 183,197 people over the life of the system, most without even knowing their use of the resource. As a training and instructional resource Jetstream was used seven times more than any other XSEDE resource in terms of Educational Allocations. Over the life of Jetstream, 216 courses and workshops utilized Jetstream as part of their instruction. These activities included multi-day workshops for early career researchers, continuing education for researchers, semester long courses for undergraduate and graduate students, and semester-long capstone projects for masters level students. Forty-six courses were conducted multiple times on Jetstream.\nThe Jetstream environment was accepted under OpenStack Liberty and retired with the Rocky release on CentOS 7.8. Overall, the system went through seven major OpenStack releases without regularly scheduled downtime or unscheduled interruptions. Through the six years of operations Jetstream had an overall availability of 98.54% including planned and unplanned outages, and an uptime of 99.9967% where the system was operating in some form but at a reduced capacity. This was coupled with above average user satisfaction and importance, obtained via annual user surveys (1-5 scale) the \"Importance of Jetstream for Research or Education\" grew year over year, concluding with a rating &gt;4, along with the quality/speed to support question response averaging &gt;4.5. \nThe services pioneered on Jetstream or by the project team have also had a significant impact on currently funded Category I resources as well as within discipline-specific projects that leverage the environment including: dynamically scaling virtual clusters used by the Galaxy project, providing non-command-line access to CI resources by default through single sign-on (including providing resources to spawn a new OpenStack interface, Exosphere), providing the first virtual GPUs, providing the first object storage system in the NSF CI ecosystem, and the first cloud native orchestration interface to a production NSF resource (e.g. OpenStack Magnum used with Kubernetes and Docker Swarm). \nJetstream's impact on student, researcher, and educator success took multiple forms:\n\n\nAs was widely published in the mainstream media, a group of scientists produced the first image of a black hole using Event Horizon Telescope (EHT) observations of the center of the galaxy M87. As part of this work, Jetstream was used to develop cloud-based data analysis pipelines that were critical for combining and sharing data taken from the geographically-distributed observatories. It was noted that the cloud pipeline used for analysis could not have been developed without Jetstream.\n\n\nJetstream joined the COVID-19 HPC Consortium in March 2020 to contribute resources to the pandemic response. The team participated in joint reporting efforts to the NSF throughout the pandemic with numerous projects taking advantage of the resource. These COVID-19 related projects include gateways such as Galaxy (used for some of the earliest genomic analysis) and ChemCompute (used by students for remote learning), as well as for international medical records projects like OpenMRS, and regional information such as projections by University of Texas, San Antonio for Texas hospital bed and ICU bed usage. \n\n\nJetstream hosted an Research Experience for Undergraduates program for five consecutive years from 2017 through 2021 for 4-6 students per year (25 students slots total) that resulted in tangible outcomes for the students in the form of project papers, and opportunities to present at local events to their peers, as well as national forums such as the PEARC conference series and even at the SC Conference Series, the International Conference for High Performance Computing, Networking, Storage, and Analysis. The program focused recruitment on individuals from MSIs, HSI, and EPSCoR jurisdictions. \n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 03/23/2023\n\n\t\t\t\t\tSubmitted by: David Y Hancock"
 }
}