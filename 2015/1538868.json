{
 "awd_id": "1538868",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Mobile Mid-Air Interactive Systems and Design Workflows for Creative 3D Shape Modeling",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rich Malak",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2015-08-23",
 "awd_max_amd_letter_date": "2015-08-23",
 "awd_abstract_narration": "Currently 3D modeling tools for product design and manufacturing are the purview of trained engineers and professional artists. Everyone has ideas but only a select few can bring them to reality. Although touch enabled devices such as tablets and smartphones are ubiquitous, the direct manipulation of content is inherently two-dimensional, and the plethora of tools commonly used today grew out of design metaphors based on windows, icons, menus, and pointers. They require extensive training, and inhibit the user's ability to create, manipulate, and modify virtual shapes in a straightforward manner. Research on mid-air interactions has shown that our bodies and their perceptually guided motions through the world do much of the work required to achieve our creative goals. This perceptual process, if carefully integrated into 3D modeling, can be instrumental in eliminating the barrier-to-entry for common users motivated to externalize their creative ideas.  This work aims at harnessing the potential of mobile interactions in mid-air to cater to the creative processes involved in design. In doing so, it takes an important step towards the computer-as-a-partner paradigm that transfers our cognition to the virtual environment through hand-held mobile devices.\r\n\r\nThe integration of a variety of sensors into smartphones provides new affordances for designing interactive processes towards creative on-the-fly shape modeling processes. However, existing research on mobile interactions has focused on 3D object manipulation, virtual scene navigation, and scientific visualization. The work will develop a framework for direct 3D shape conceptualization using mobile interactions. To do so, this work will leverage the unique advantage of using smartphones as an extension to our mind and body, to spatially create 2D and 3D sketches, and create, manipulate and modify virtual 3D objects. The ensuing simplicity of spatial interactions for easy, direct shape-modeling operations will transform our bodily motions and rotations of the phone and contextual sketches on the devices to desired configurations and geometries. By combining multi-touch interactions and device sensors, the work aims to leverage embodied cognition towards spatial interaction and develop new metaphors for spatial control, and develop processes for shape modeling to explore a new design space of intuitive interactions. This will be achieved by developing process flows for constrained and free-form 3D shapes, spatial configuration of 3D objects, and creative 3D compositions with shapes. These development cycles will be driven by rigorous user evaluations and iterations to derive the guidelines for design process flow and user experiences.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Karthik",
   "pi_last_name": "Ramani",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Karthik Ramani",
   "pi_email_addr": "ramani@purdue.edu",
   "nsf_id": "000284152",
   "pi_start_date": "2015-08-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "School of Mechanical Engineering",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072088",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "146400",
   "pgm_ele_name": "ESD-Eng & Systems Design"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "067E",
   "pgm_ref_txt": "DESIGN TOOLS"
  },
  {
   "pgm_ref_code": "068E",
   "pgm_ref_txt": "DESIGN THEORY"
  },
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Currently 3D modeling tools for product design and manufacturing are the purview of trained engineers and professional artists. Everyone has ideas but only a select few can bring them to reality. Although touch enabled devices such as tablets and smartphones are ubiquitous, the direct manipulation of content is inherently two-dimensional, and the plethora of tools commonly used today grew out of design metaphors based on windows, icons, menus, and pointers. They require extensive training, and inhibit the user's ability to create, manipulate, and modify virtual shapes in a straightforward manner. Research on mid-air interactions has shown that our bodies and their perceptually guided motions through the world do much of the work required to achieve our creative goals. We integrated this natural motions of humans into 3D modeling, in multiple scenarios and studied as well as understood the human-computer interaction oriented factors. We reduced substantially the barrier-to-entry for common users motivated to externalize their creative ideas. We harnessed the potential of mobile interactions in mid-air to cater to the creative processes involved in design. In doing so, we took an important step towards the computer-as-a-partner paradigm that transfers our cognition to the virtual environment through hand-held mobile devices. For example in our publications we converted multi-touch interactions to create and edit 3D compositions, and then assemble them. We also demonstrated direct creation of artifacts in mid-air using every day found objects using smart phones.</span></p>\n<p>We bridged the gap between the physical and digital worlds for creative design, we built a mobile maker-less MR environment using Google Tango tablet and merged sketch- and image-based modeling approaches within this MR environment to develop a new design workflows. Our approach allowed users to freely move around in the surrounding environment, seek for design inspiration, borrow visual and dimensional attributes from existing objects, and inspect the newly created shape within the augmented scene.&nbsp;</p>\n<p>We also further increased the users spatial awareness by developing a technique allows the mobile AR device spatial awareness of the physical world. We developed a method that enables instant discovery and localization of the surrounding smart things while also spatially registering the artifacts with a depth-senspr equiped mobile. By exploiting the spatial relationships betweenmobile AR systems and smart things, we foster&nbsp;in-situ interactions with connected devices.&nbsp;</p>\n<p><span>The ensuing simplicity of spatial interactions for easy, direct shape-modeling operations transforms our bodily motions and rotations of the phone and contextual sketches on the devices to desired configurations and geometries. We developed process flows for constrained and free-form 3D shapes, spatial configuration of 3D objects, and creative 3D compositions with shapes. These development cycles were driven by rigorous user evaluations and iterations to derive the guidelines for design process flow and user experiences.&nbsp; We also&nbsp;</span>validated the localization accuracy as well as the performance of theenabled spatial aware interactions using a depth sensor based mobile interaction tablet.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/28/2018<br>\n\t\t\t\t\tModified by: Karthik&nbsp;Ramani</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nCurrently 3D modeling tools for product design and manufacturing are the purview of trained engineers and professional artists. Everyone has ideas but only a select few can bring them to reality. Although touch enabled devices such as tablets and smartphones are ubiquitous, the direct manipulation of content is inherently two-dimensional, and the plethora of tools commonly used today grew out of design metaphors based on windows, icons, menus, and pointers. They require extensive training, and inhibit the user's ability to create, manipulate, and modify virtual shapes in a straightforward manner. Research on mid-air interactions has shown that our bodies and their perceptually guided motions through the world do much of the work required to achieve our creative goals. We integrated this natural motions of humans into 3D modeling, in multiple scenarios and studied as well as understood the human-computer interaction oriented factors. We reduced substantially the barrier-to-entry for common users motivated to externalize their creative ideas. We harnessed the potential of mobile interactions in mid-air to cater to the creative processes involved in design. In doing so, we took an important step towards the computer-as-a-partner paradigm that transfers our cognition to the virtual environment through hand-held mobile devices. For example in our publications we converted multi-touch interactions to create and edit 3D compositions, and then assemble them. We also demonstrated direct creation of artifacts in mid-air using every day found objects using smart phones.\n\nWe bridged the gap between the physical and digital worlds for creative design, we built a mobile maker-less MR environment using Google Tango tablet and merged sketch- and image-based modeling approaches within this MR environment to develop a new design workflows. Our approach allowed users to freely move around in the surrounding environment, seek for design inspiration, borrow visual and dimensional attributes from existing objects, and inspect the newly created shape within the augmented scene. \n\nWe also further increased the users spatial awareness by developing a technique allows the mobile AR device spatial awareness of the physical world. We developed a method that enables instant discovery and localization of the surrounding smart things while also spatially registering the artifacts with a depth-senspr equiped mobile. By exploiting the spatial relationships betweenmobile AR systems and smart things, we foster in-situ interactions with connected devices. \n\nThe ensuing simplicity of spatial interactions for easy, direct shape-modeling operations transforms our bodily motions and rotations of the phone and contextual sketches on the devices to desired configurations and geometries. We developed process flows for constrained and free-form 3D shapes, spatial configuration of 3D objects, and creative 3D compositions with shapes. These development cycles were driven by rigorous user evaluations and iterations to derive the guidelines for design process flow and user experiences.  We also validated the localization accuracy as well as the performance of theenabled spatial aware interactions using a depth sensor based mobile interaction tablet.\n\n\t\t\t\t\tLast Modified: 02/28/2018\n\n\t\t\t\t\tSubmitted by: Karthik Ramani"
 }
}