{
 "awd_id": "1453781",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Advanced Knowledge Extraction of Affective Behaviors During Natural Human Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 495853.0,
 "awd_amount": 535853.0,
 "awd_min_amd_letter_date": "2015-08-06",
 "awd_max_amd_letter_date": "2020-03-23",
 "awd_abstract_narration": "Identifying and characterizing emotional behaviors are challenging but very important research topics for enriched speech-derived analytics and human-computer interaction. This CAREER project aims to create novel algorithms to recognize spontaneous affective behaviors from speech that capture the underlying externalization process of emotions and generalize to recordings of human interactions collected under real-world conditions. The lack of generalization of current speech emotion algorithms to recognize expressive behaviors during natural human interaction is the key barrier to deploying affective-aware technology in real-life applications. Under a theoretical framework grounded in the nonuniform externalization of expressive behaviors, the project brings transformative solutions to address this problem. The proposed models and algorithms promise insights to explore and extend theories in linguistic&#8232; and paralinguistic human behaviors. Several new scientific avenues can emerge that serve as truly innovative advancements that will impact applications in security and defense, next generation of advanced user interfaces, health behavior informatics, and education. The role of human centered technologies, especially contextualized in applications of direct societal relevance, can inspire young &#8232;scholars into computing and engineering: from creating robust technologies for sensing, to &#8232;actually incorporating such information as a part of advanced analytics and enhanced user experiences. As a Hispanic faculty, the PI serves as a mentor and role model for high school, undergraduate &#8232;and graduate students involved in the Minority Scholars Symposium, Diversity Scholarship Program and Graduate Student Mentoring Program at the University of Texas at Dallas. Through lab open houses, demonstrations, and active online and social media presence, the PI is reaching out to non-traditional students, as well &#8232;as the broader, non-technical audience interested in human behavior science.\r\n\r\nThe project evaluates &#8232;the powerful, scalable and appealing concept of using neutral reference models to contrast deviations in speech characteristics associated with emotions. The study proposes flexible, integrative and discriminative frameworks that capture the underlying encoding process of expressive behaviors including of emotion salient regions in the speech stream, intrinsic reliability of features,&#8232; and dynamic evolution of emotions. The study considers binary and rank-based classifiers to recognize and rank-order specific expressive behaviors. The project presents speaker and lexical compensation schemes, and model adaptation strategies to increase the robustness of the proposed models. All these theoretical and algorithmic advances are carefully evaluated with naturalistic data, in which emotional content will be annotated &#8232;with a novel crowdsourcing scheme that tracks in real time the performance of the evaluators.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Carlos",
   "pi_last_name": "Busso",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Carlos Busso",
   "pi_email_addr": "cbusso@andrew.cmu.edu",
   "nsf_id": "000544291",
   "pi_start_date": "2015-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Dallas",
  "inst_street_address": "800 WEST CAMPBELL RD.",
  "inst_street_address_2": "SP2.25",
  "inst_city_name": "RICHARDSON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9728832313",
  "inst_zip_code": "750803021",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "TX24",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT DALLAS",
  "org_prnt_uei_num": "",
  "org_uei_num": "EJCVPNN1WFS5"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Texas At Dallas",
  "perf_str_addr": "800 W. Campbell Rd",
  "perf_city_name": "Richardson",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "750803021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "TX24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 94242.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 104633.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 107097.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 109633.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 112248.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Research Objective and Significance:&nbsp;</p>\n<p>&nbsp;This CAREER&nbsp;project created novel algorithms to recognize spontaneous affective behaviors from speech that capture the underlying externalization process of emotions and generalize to recordings of human interactions collected under real-world conditions. The lack of generalization of current speech emotion algorithms to recognize expressive behaviors during natural human interaction is the key barrier to deploying affective-aware technology in real-life applications. Under a theoretical framework grounded in the nonuniform externalization of expressive behaviors, the project explored transformative solutions to address this problem. The developed models can serve as truly innovative advancements that will impact applications in security and defense, next generation of advanced user interfaces, health behavior informatics, and education. The research goals of the project were: (1) to analyze and understand the heterogeneous, dynamic nonuniform encoding process of expressive behaviors in speech for guiding the design of algorithms to uncover localized (&ldquo;salient&rdquo;) emotional speech segments, (2) to investigate speech emotion recognizers tailored to specific expressive behaviors, and (3) to explore robust adaptation and compensation schemes to generalize an emotion classifier to naturalistic interactions.&nbsp;</p>\n<p>&nbsp;The project&rsquo;s outcomes were published in top venues: 16 journals and 31 conference papers. Five graduate students involved in this project obtained their PhD degree. Other four PhD students, one master student and eight undergraduate students were also involved in the project.</p>\n<p>&nbsp;</p>\n<p>Research Project and Findings:</p>\n<p>&nbsp;We highlight some of the methods developed during this project.</p>\n<p>&nbsp;(a) curriculum learning for speech emotion recognition</p>\n<p>&nbsp;We proposed a method to design a curriculum for machine-learning to maximize the efficiency during the training process of DNNs for speech emotion recognition. Previous studies in other machine-learning problems have shown the benefits of training a classifier following a curriculum where samples are gradually presented in increasing level of difficulty. For speech emotion recognition, the challenge is to establish a natural order of difficulty in the training set to create the curriculum. We addressed this problem by assuming that ambiguous samples for humans are also ambiguous for computers. We proposed to use the disagreement between evaluators as a measure of difficulty for the classification task. We proposed metrics that quantify the inter-evaluation agreement to define the curriculum for regression problems and binary and multi-class classification problems.&nbsp;</p>\n<p>&nbsp;</p>\n<p>(b) Neutral reference model with synthetic speech&nbsp;</p>\n<p>We explored the idea of creating neutral reference model from synthetic speech to contrast the emotional content of a speech signal. Having a reference signal with known emotional content (e.g., neutral emotion) to compare the target sentence may produce more reliable metrics to identify emotional segments. Ideally, we would like to have an emotionally neutral sentence with the same lexical content as the target sentence. In this fictitious scenario, we would be able to identify localized emotional cues by contrasting frame-by-frame the acoustic features of the target and reference sentences. We explored the idea of building these reference sentences leveraging the advances on speech synthesis. We built a synthetic speech signal that conveys the same lexical information and is timely aligned with the target sentence. We built multiple synthetic sentences using various voices and text-to-speech (TTS) approaches.&nbsp;</p>\n<p>&nbsp;</p>\n<p>(c) Semi-supervised emotion recognition with ladder networks&nbsp;</p>\n<p>A major drawback of the SER systems is their lack of generalization across different conditions. An effective way to increase the generalization of the models is by regularizing the models through multitask learning, where auxiliary tasks are learned along with the primary task. We proposed the use of ladder networks for emotion recognition, which utilizes an unsupervised auxiliary task. The primary task is a regression problem to predict emotional attributes. The auxiliary task is the reconstruction of intermediate feature representations using a denoising autoencoder. This auxiliary task does not require labels so it is possible to train the framework in a semi-supervised fashion with abundant unlabeled data from the target domain. This study showed that the proposed approach creates a powerful framework for SER, achieving superior performance than fully supervised baselines.&nbsp;</p>\n<p>&nbsp;</p>\n<p>d) Domain adversarial for acoustic emotion recognition&nbsp;</p>\n<p>&nbsp;The performance of speech emotion recognition is affected by the differences in data distributions between train (source domain) and test (target domain) sets used to build and evaluate the models. We explored the use of adversarial multitask training to extract a common representation between train and test domains. The primary task is to predict emotional attribute-based descriptors for arousal, valence, or dominance. The secondary task is to learn a common representation where the train and test domains cannot be distinguished. This method effectively uses unlabeled data for speech emotion recognition.&nbsp;</p>\n<p>&nbsp;</p>\n<p>(e) The protocol for the MSP-Podcast corpus</p>\n<p>We proposed a novel approach to effectively build a large, naturalistic emotional database with balanced emotional content, reduced cost and reduced manual labor. This database is the MSP-Podcast corpus, which relies on existing spontaneous recordings obtained from audio-sharing websites. This project started the data collection effort.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/14/2021<br>\n\t\t\t\t\tModified by: Carlos&nbsp;Busso</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639524062798_teaser-Lotfian_201x--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639524062798_teaser-Lotfian_201x--rgov-800width.jpg\" title=\"Outcome (e) MSP-Podcast\"><img src=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639524062798_teaser-Lotfian_201x--rgov-66x44.jpg\" alt=\"Outcome (e) MSP-Podcast\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The protocol for the MSP-Podcast corpus</div>\n<div class=\"imageCredit\">Reza Lotfian</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Carlos&nbsp;Busso</div>\n<div class=\"imageTitle\">Outcome (e) MSP-Podcast</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523556105_Ladder_networks_teaser--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523556105_Ladder_networks_teaser--rgov-800width.jpg\" title=\"Outcome (c) ladder network\"><img src=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523556105_Ladder_networks_teaser--rgov-66x44.jpg\" alt=\"Outcome (c) ladder network\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Semi-supervised emotion recognition with ladder networks</div>\n<div class=\"imageCredit\">Srinivas Parthasarathy</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Carlos&nbsp;Busso</div>\n<div class=\"imageTitle\">Outcome (c) ladder network</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523712500_Slide1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523712500_Slide1--rgov-800width.jpg\" title=\"Outcome (d) Domain adversarial\"><img src=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523712500_Slide1--rgov-66x44.jpg\" alt=\"Outcome (d) Domain adversarial\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Domain adversarial for acoustic emotion recognition</div>\n<div class=\"imageCredit\">Mohammed AbdelWahab</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Carlos&nbsp;Busso</div>\n<div class=\"imageTitle\">Outcome (d) Domain adversarial</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523854641_teaser--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523854641_teaser--rgov-800width.jpg\" title=\"Outcome (a) ladder network\"><img src=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523854641_teaser--rgov-66x44.jpg\" alt=\"Outcome (a) ladder network\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">curriculum learning for speech emotion recognition</div>\n<div class=\"imageCredit\">Reza Lotfian</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Carlos&nbsp;Busso</div>\n<div class=\"imageTitle\">Outcome (a) ladder network</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523965841_Slide1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523965841_Slide1--rgov-800width.jpg\" title=\"Outcome (b) Synthetic speech reference\"><img src=\"/por/images/Reports/POR/2021/1453781/1453781_10384371_1639523965841_Slide1--rgov-66x44.jpg\" alt=\"Outcome (b) Synthetic speech reference\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Neutral reference model with synthetic speech</div>\n<div class=\"imageCredit\">Reza Lotfian</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Carlos&nbsp;Busso</div>\n<div class=\"imageTitle\">Outcome (b) Synthetic speech reference</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nResearch Objective and Significance: \n\n This CAREER project created novel algorithms to recognize spontaneous affective behaviors from speech that capture the underlying externalization process of emotions and generalize to recordings of human interactions collected under real-world conditions. The lack of generalization of current speech emotion algorithms to recognize expressive behaviors during natural human interaction is the key barrier to deploying affective-aware technology in real-life applications. Under a theoretical framework grounded in the nonuniform externalization of expressive behaviors, the project explored transformative solutions to address this problem. The developed models can serve as truly innovative advancements that will impact applications in security and defense, next generation of advanced user interfaces, health behavior informatics, and education. The research goals of the project were: (1) to analyze and understand the heterogeneous, dynamic nonuniform encoding process of expressive behaviors in speech for guiding the design of algorithms to uncover localized (\"salient\") emotional speech segments, (2) to investigate speech emotion recognizers tailored to specific expressive behaviors, and (3) to explore robust adaptation and compensation schemes to generalize an emotion classifier to naturalistic interactions. \n\n The project\u2019s outcomes were published in top venues: 16 journals and 31 conference papers. Five graduate students involved in this project obtained their PhD degree. Other four PhD students, one master student and eight undergraduate students were also involved in the project.\n\n \n\nResearch Project and Findings:\n\n We highlight some of the methods developed during this project.\n\n (a) curriculum learning for speech emotion recognition\n\n We proposed a method to design a curriculum for machine-learning to maximize the efficiency during the training process of DNNs for speech emotion recognition. Previous studies in other machine-learning problems have shown the benefits of training a classifier following a curriculum where samples are gradually presented in increasing level of difficulty. For speech emotion recognition, the challenge is to establish a natural order of difficulty in the training set to create the curriculum. We addressed this problem by assuming that ambiguous samples for humans are also ambiguous for computers. We proposed to use the disagreement between evaluators as a measure of difficulty for the classification task. We proposed metrics that quantify the inter-evaluation agreement to define the curriculum for regression problems and binary and multi-class classification problems. \n\n \n\n(b) Neutral reference model with synthetic speech \n\nWe explored the idea of creating neutral reference model from synthetic speech to contrast the emotional content of a speech signal. Having a reference signal with known emotional content (e.g., neutral emotion) to compare the target sentence may produce more reliable metrics to identify emotional segments. Ideally, we would like to have an emotionally neutral sentence with the same lexical content as the target sentence. In this fictitious scenario, we would be able to identify localized emotional cues by contrasting frame-by-frame the acoustic features of the target and reference sentences. We explored the idea of building these reference sentences leveraging the advances on speech synthesis. We built a synthetic speech signal that conveys the same lexical information and is timely aligned with the target sentence. We built multiple synthetic sentences using various voices and text-to-speech (TTS) approaches. \n\n \n\n(c) Semi-supervised emotion recognition with ladder networks \n\nA major drawback of the SER systems is their lack of generalization across different conditions. An effective way to increase the generalization of the models is by regularizing the models through multitask learning, where auxiliary tasks are learned along with the primary task. We proposed the use of ladder networks for emotion recognition, which utilizes an unsupervised auxiliary task. The primary task is a regression problem to predict emotional attributes. The auxiliary task is the reconstruction of intermediate feature representations using a denoising autoencoder. This auxiliary task does not require labels so it is possible to train the framework in a semi-supervised fashion with abundant unlabeled data from the target domain. This study showed that the proposed approach creates a powerful framework for SER, achieving superior performance than fully supervised baselines. \n\n \n\nd) Domain adversarial for acoustic emotion recognition \n\n The performance of speech emotion recognition is affected by the differences in data distributions between train (source domain) and test (target domain) sets used to build and evaluate the models. We explored the use of adversarial multitask training to extract a common representation between train and test domains. The primary task is to predict emotional attribute-based descriptors for arousal, valence, or dominance. The secondary task is to learn a common representation where the train and test domains cannot be distinguished. This method effectively uses unlabeled data for speech emotion recognition. \n\n \n\n(e) The protocol for the MSP-Podcast corpus\n\nWe proposed a novel approach to effectively build a large, naturalistic emotional database with balanced emotional content, reduced cost and reduced manual labor. This database is the MSP-Podcast corpus, which relies on existing spontaneous recordings obtained from audio-sharing websites. This project started the data collection effort.\n\n \n\n\t\t\t\t\tLast Modified: 12/14/2021\n\n\t\t\t\t\tSubmitted by: Carlos Busso"
 }
}