{
 "awd_id": "1513051",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CI: NEW: Collaborative Research: Computer System Failure Data Repository to Enable Data-Driven Dependability",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 370905.0,
 "awd_amount": 370905.0,
 "awd_min_amd_letter_date": "2015-06-29",
 "awd_max_amd_letter_date": "2015-06-29",
 "awd_abstract_narration": "Dependability has become a necessary requisite property for many of the computer systems that surround us or work behind the scenes to support our personal and professional lives. Heroic progress has been made by computer systems researchers and practitioners working together to build and deploy dependable systems. However, an overwhelming majority of this work is not based on real publicly available failure data. Unfortunately, an open failure data repository for any recent computing infrastructure that is large enough, diverse enough and with enough information about the infrastructure and the applications that run on them does not exist. \r\n\r\nThis project will address this pressing need. The research team appreciates that this effort is challenging on many levels. Failure data are considered sensitive and are usually unveiled only before trusting eyes of a small subset of the people at the organization. As part of a current one-year planning grant, this team has collected specific requirements for the repository from a wide audience, collected failure and usage data from the largest centrally managed computing cluster at Purdue and performed preliminary analysis to reveal the workload usage patterns. The goal of this full-scale project is to collect data from a variety of computational infrastructure at the two participating universities, and from several of the NSF-funded large cyberinfrastructure projects.\r\n\r\nThe project will collect, curate, and present public failure data of large-scale computing systems in a repository called FRESCO. The data sets will include static information, dynamic information about the workloads, and failure information for both planned and unplanned outages. The data collection from production machines will have to obey several practical constraints -- no changes to the workload, little performance perturbation, and minimal changes to the operating system. Further, the data have to be sanitized for removing sensitive information and processed to make it interpretable by a broad group of researchers. This project will also provide analysis tools to answer certain commonly occurring questions, such as the correlation between workload and failure and the performance implications of using one library over another, as well as an intuitive graphical front-end which will allow people to explore the data sets and download the relevant ones.\r\n\r\nWidespread use of the data and the associated analysis tools will give computer systems researchers an unprecedented ability to do data-driven research and offer computing infrastructure providers an analytic-driven capability to run more efficient reliable infrastructures.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ravishankar",
   "pi_last_name": "Iyer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ravishankar Iyer",
   "pi_email_addr": "rkiyer@illinois.edu",
   "nsf_id": "000444242",
   "pi_start_date": "2015-06-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Zbigniew",
   "pi_last_name": "Kalbarczyk",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zbigniew Kalbarczyk",
   "pi_email_addr": "kalbarcz@illinois.edu",
   "nsf_id": "000296509",
   "pi_start_date": "2015-06-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618207473",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 370905.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Large-scale high performance computing (HPC) systems have become common in academic, industrial, and government for compute intensive applications, including large-scale parallel applications. These HPC systems solve problems that would take millennia on personal computers, but managing such large shared resources can be challenging and requires administrators to balance requirements from a diverse set of users. Large, focused organizations can afford to buy centralized resources, and choose to manage and operate it at academic organizations through a central IT organization. These are funded by federal funding agencies (like the National Science Foundation in the US) and individual researchers write grant proposals to get access to compute time on these systems. Examples of such systems include Comet at the University of California San Diego, BlueWaters at the University of Illinois at Urbana-Champaign, and Frontera at the University of Texas at Austin.</p>\n<p>Progress in building dependable systems can be made faster if researchers working on dependability challenges can be exposed to problems through quantitative data. Theories in the labs and small demonstrations in prototypes can be transitioned to the demanding realities of large computer systems if they could validate their inventions with real system failure and attack data. There is an astonishing lack of such publicly available data for researchers and as a result many productive avenues of work in dependable system building are lying hidden. A comparison may fruitfully be drawn to the widespread use of benchmarks and reference data sets in performance analysis of computer systems, such as, those put out by SPEC or TPC. We therefore proposed to solve this problem by collecting, cleaning, annotating, and presenting data from the production compute infrastructures at several public universities.&nbsp;&nbsp;</p>\n<div>\n<p>We were not Pollyannaish in our effort and appreciated that this effort is challenging, both due to technological and psychological reasons. The technological reason referred to the need to keep the production infrastructure relatively undisturbed as the monitoring and the data collection happens. The psychological reason referred to the fact that such data is considered sensitive by many, to be unveiled only before trusting eyes of a subset of the people at the organization. We mitigated the two factors in our project. The technological factor was mitigated by carefully deciding which monitoring tool to use, when that should be activated, and how to store the data, both online and offline. We sidestepped the psychological factor by focusing on computational infrastructure at public universities (including ours), rather than private commercial organizations, and then working closely with the production IT unit to enable the data collection and data understanding.</p>\n<p><strong>Intellectual Merit:</strong></p>\n<p>We performed 4 different categories of analysis on production compute data collected from central computing clusters at three large public universities?Purdue University, University of Texas at Austin, and University of Illinois at Urbana-Champaign. In the first, we showed the breakdown of node failures into different categories and reasoned about the corresponding up times and recovery times. In the second, we considered the failures of individual jobs and reasoned about their root cause through examination of their exit codes. The third analysis sheds light on the relation between resource usage and job failure rates. We consider the 5 primary kinds of resources, local and remote, memory on a node, local IO, remote IO to the parallel file system, network, and runtime of a job. For the runtime, we considered both the spatial component (i.e., number of nodes) and the temporal component (i.e., the execution time on each node). Finally, we developed a job failure prediction model which can help minimize resource wastage corresponding to job failures due to system related issues.</p>\n<p>We collected or released datasets from production systems: (i) dataset from network performance counters, (ii) dataset on filesystem performance and reliability, and (iii) dataset from fault injection experiments for assessing system and application resiliency. We also developed a software tools: (i) Kaleidoscope, a tool for data-driven characterization of failures and performance degradation of file system, (ii) Monet, a framework for characterizing reliability and performance of supercomputing interconnects, and (iii) HPCArrow, a fault injection framework to support automatic fault injection campaigns that target network links, compute nodes, and compute blades.</p>\n<p><strong>Broader Impacts: </strong></p>\n<p>The project has created a large public workload and failure data repository from production computing clusters in a university setting, Fresco (for Purdue and UT Austin) and Monet (University of Illinois at Urbana-Champaign). Widespread use of the data and the associated analysis tools will give computer systems researchers an unprecedented ability to do data-driven research and computing infrastructure providers an analytics capability for running the infrastructures more efficiently and more reliably. University researchers will benefit from more available centralized computing clusters. Broad societal impact will result from the development of more efficient and more reliable large-scale computing clusters that can run societal critical applications reliably and efficiently and at large scales.</p>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2019<br>\n\t\t\t\t\tModified by: Zbigniew&nbsp;Kalbarczyk</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nLarge-scale high performance computing (HPC) systems have become common in academic, industrial, and government for compute intensive applications, including large-scale parallel applications. These HPC systems solve problems that would take millennia on personal computers, but managing such large shared resources can be challenging and requires administrators to balance requirements from a diverse set of users. Large, focused organizations can afford to buy centralized resources, and choose to manage and operate it at academic organizations through a central IT organization. These are funded by federal funding agencies (like the National Science Foundation in the US) and individual researchers write grant proposals to get access to compute time on these systems. Examples of such systems include Comet at the University of California San Diego, BlueWaters at the University of Illinois at Urbana-Champaign, and Frontera at the University of Texas at Austin.\n\nProgress in building dependable systems can be made faster if researchers working on dependability challenges can be exposed to problems through quantitative data. Theories in the labs and small demonstrations in prototypes can be transitioned to the demanding realities of large computer systems if they could validate their inventions with real system failure and attack data. There is an astonishing lack of such publicly available data for researchers and as a result many productive avenues of work in dependable system building are lying hidden. A comparison may fruitfully be drawn to the widespread use of benchmarks and reference data sets in performance analysis of computer systems, such as, those put out by SPEC or TPC. We therefore proposed to solve this problem by collecting, cleaning, annotating, and presenting data from the production compute infrastructures at several public universities.  \n\n\nWe were not Pollyannaish in our effort and appreciated that this effort is challenging, both due to technological and psychological reasons. The technological reason referred to the need to keep the production infrastructure relatively undisturbed as the monitoring and the data collection happens. The psychological reason referred to the fact that such data is considered sensitive by many, to be unveiled only before trusting eyes of a subset of the people at the organization. We mitigated the two factors in our project. The technological factor was mitigated by carefully deciding which monitoring tool to use, when that should be activated, and how to store the data, both online and offline. We sidestepped the psychological factor by focusing on computational infrastructure at public universities (including ours), rather than private commercial organizations, and then working closely with the production IT unit to enable the data collection and data understanding.\n\nIntellectual Merit:\n\nWe performed 4 different categories of analysis on production compute data collected from central computing clusters at three large public universities?Purdue University, University of Texas at Austin, and University of Illinois at Urbana-Champaign. In the first, we showed the breakdown of node failures into different categories and reasoned about the corresponding up times and recovery times. In the second, we considered the failures of individual jobs and reasoned about their root cause through examination of their exit codes. The third analysis sheds light on the relation between resource usage and job failure rates. We consider the 5 primary kinds of resources, local and remote, memory on a node, local IO, remote IO to the parallel file system, network, and runtime of a job. For the runtime, we considered both the spatial component (i.e., number of nodes) and the temporal component (i.e., the execution time on each node). Finally, we developed a job failure prediction model which can help minimize resource wastage corresponding to job failures due to system related issues.\n\nWe collected or released datasets from production systems: (i) dataset from network performance counters, (ii) dataset on filesystem performance and reliability, and (iii) dataset from fault injection experiments for assessing system and application resiliency. We also developed a software tools: (i) Kaleidoscope, a tool for data-driven characterization of failures and performance degradation of file system, (ii) Monet, a framework for characterizing reliability and performance of supercomputing interconnects, and (iii) HPCArrow, a fault injection framework to support automatic fault injection campaigns that target network links, compute nodes, and compute blades.\n\nBroader Impacts: \n\nThe project has created a large public workload and failure data repository from production computing clusters in a university setting, Fresco (for Purdue and UT Austin) and Monet (University of Illinois at Urbana-Champaign). Widespread use of the data and the associated analysis tools will give computer systems researchers an unprecedented ability to do data-driven research and computing infrastructure providers an analytics capability for running the infrastructures more efficiently and more reliably. University researchers will benefit from more available centralized computing clusters. Broad societal impact will result from the development of more efficient and more reliable large-scale computing clusters that can run societal critical applications reliably and efficiently and at large scales.\n\n\n \n\n\t\t\t\t\tLast Modified: 10/30/2019\n\n\t\t\t\t\tSubmitted by: Zbigniew Kalbarczyk"
 }
}