{
 "awd_id": "1522334",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Priorconditioned Krylov Subspace Methods for Inverse Problems",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Leland Jameson",
 "awd_eff_date": "2015-08-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 220002.0,
 "awd_amount": 220002.0,
 "awd_min_amd_letter_date": "2015-07-24",
 "awd_max_amd_letter_date": "2019-08-14",
 "awd_abstract_narration": "Inverse problems are gaining importance in a wide variety of applications; they play an important role in medical imaging because of the push towards non-invasive diagnostic techniques. In some applications, e.g., in the investigation of the brain activity from the measurement of the induced magnetic field in the space outside the skull, the relation between the unknown causes and the observed effects can be expressed as a linear function. In other cases, when the relationship is more complicated, the solution of linear inverse problems may have to be addressed as part of a more general solution scheme. While in principle easy to state, the solution of a linear system of equations arising from inverse problems can be extremely challenging, in particular when there is a mismatch between the number of observations and the degrees of freedom and when the dimensions of the problems are very large. When data collection is problematic because of the associated costs, technical difficulties, or health risks, the number of unknowns in the resulting linear system exceeds the number of equations. In order to produce a meaningful solution for such systems it is necessary to augment standard techniques with qualitative knowledge about the problem. This project concerns the design and analysis of computational methods for the solution of linear ill-posed problems that naturally translate qualitative information or belief about the data and the solution in quantitative terms. In particular, by formulating the problem within the framework of Bayesian inference, the project will develop mathematically sound and computationally efficient schemes for large scale problems where the disturbance in the data may be rather substantial and may have a statistics rather different from white noise. The Bayesian framework is the natural setting for expressing the a priori beliefs about the solution. The prior beliefs may vary widely from one time instance to another, or from one point in space to another, and it may be necessary to express them in hierarchical layers. Since this approach very closely resembles the way in which people formulate what they know and how knowledge is updated as new evidence arrives, it is expected that the methodology will be widely utilized. \r\n\r\nThe increasing popularity of complex models in inverse problems comes with an increase in associated computational costs. The methodology developed as part of this project addresses the need for computational efficiency by combining Bayesian inference with the Krylov subspace iterative methods, the natural choice for the solution of large scale linear systems. In this manner the philosophical appeal of the Bayesian framework is transformed in a very powerful Bayes-meets-Krylov computational scheme of wide applicability. The project provides an important connection between numerical linear algebra and Bayesian inference and will shed some light on how to link spectral properties of linear operators with statistical features of the unknown solution. Krylov subspace methods for inverse and ill-posed problems and the Bayesian solution of inverse problems are two very rich research areas which have received much interest, individually and jointly, in the last decade. There is experimental evidence that their symbiotic cooperation can be very advantageous in a variety of applications, but a solid understanding of the changes in the subspaces where the approximate solutions are sought and in approximation of the relevant eigenvalues in the associated Lanczos processes is still largely missing. The combination of theoretical and computational tools will fill this intellectual gap and open the way for the use of state-of-the-art iterative numerical solvers for very large ill-posed systems in the context of sequential Monte Carlo methods. This will reduce the gap between statistical uncertainty quantification and numerical linear algebra, to great advantage for both fields. In fact, the success of the Krylov-meets-Bayes approach, confirmed in a number of different settings and particularly in the solution of underdetermined problems, relies on left and right preconditioners to augment the quantitative data with additional qualitative information. Understanding the changes in the Krylov subspaces and in the associated Lanczos process induced by the statistically inspired preconditioners in discrete linear inverse problems is one of the aims of this project. In particular, the powerful tools of numerical linear algebra and the connection between Krylov subspace iterative solvers, the Lanczos process, and the associated orthogonal polynomials will be utilized to enlighten the connections and differences with classical schemes, including Tikhonov regularization. In the first part of the project, the analysis will be first carried out in the case of Gaussian prior and noise, and will be subsequently extended to the case of conditionally Gaussian prior, whose covariance matrix depends on unknown parameters, which are estimated via a nonlinear step as we learn more about the unknown of primary interest. In the latter case, the ensuing prior conditioners will be a parametrized family of matrices. Understanding how the spectral properties of the preconditioned systems change as functions of the parameters of the prior covariance will be part of the project; here, the connections with Gauss-type quadrature rules and moments may turn out be crucial.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniela",
   "pi_last_name": "Calvetti",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniela Calvetti",
   "pi_email_addr": "dxc57@case.edu",
   "nsf_id": "000253080",
   "pi_start_date": "2015-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Case Western Reserve University",
  "inst_street_address": "10900 EUCLID AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CLEVELAND",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "2163684510",
  "inst_zip_code": "441064901",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "OH11",
  "org_lgl_bus_name": "CASE WESTERN RESERVE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJMKEF7EJW69"
 },
 "perf_inst": {
  "perf_inst_name": "Case Western Reserve University",
  "perf_str_addr": "",
  "perf_city_name": "Cleveland",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "441064901",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "OH11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 220002.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The PI has completed her project \"Priorconditioned Krylov Subspace Methods for Inverse Problems\" and the outcomes include 1. the publication of several papers in top ranked journals; 2. the research training and mentoring of graduate and undergraduate students; 3. the interpretation of sparsity in inverse problems as an a priori belief in the Bayesian framework; 4. the promotion of a probabilistic interpretation of numerical linear algebra and, in general, scientific computing, in the classroom.</p>\n<p>The project, and the dissemination of its results, has played a major role in promoting the idea that preconditioners are effective and computationally efficient ways to encode a priori belief about the sought solutions of inverse problems, and that the computational advantages are particularly striking when used in conjunction with Krylov iterative solvers.</p>\n<p>During the period of the project, Bayesian scientific computing had become much more accepted and mainstream, and the number of publications where Krylov iterative solvers are paired with prior-inspired preconditioners has grown at very high rate. Recently the PI has been invited to present the results of the project in plenary presentations at international workshops on both linear algebra and imagining science, in addition to inverse problems meetings.</p>\n<p>The project has motivated and partially supported the research training of PhD students (Prezioso, Strang and Nakkireddy), MS students (Johnson and Devathi) and undergraduates. Some results from the project have been included as topics of the courses taught by the PI and her colleagues.</p>\n<p>The complete analysis of a class of sparsity promoting iterative solvers, and the convergence and sparsity results proven as part of the project are getting some attention as computationally efficient alternatives to L1 penalties. The natural predisposition of priorconditioned Krylov iterative solvers for inverse problems to the quantification of the uncertainty in the solutions makes them particularly attractive for real applications.</p>\n<p>The results of the project have been applied to the investigation of the changes occurring in brain during mediation (still ongoing), to the design of efficient and accurate meshes for the finite elements solution of inverse problems, and to data science problems, including, but not limited to, dictionary learning.</p>\n<!--  /* Font Definitions */ @font-face \t{font-family:\"Cambria Math\"; \tpanose-1:0 0 0 0 0 0 0 0 0 0; \tmso-font-charset:1; \tmso-generic-font-family:roman; \tmso-font-format:other; \tmso-font-pitch:variable; \tmso-font-signature:0 0 0 0 0 0;} @font-face \t{font-family:Calibri; \tpanose-1:2 15 5 2 2 2 4 3 2 4; \tmso-font-charset:0; \tmso-generic-font-family:auto; \tmso-font-pitch:variable; \tmso-font-signature:-536870145 1073786111 1 0 415 0;}  /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal \t{mso-style-unhide:no; \tmso-style-qformat:yes; \tmso-style-parent:\"\"; \tmargin:0in; \tmargin-bottom:.0001pt; \tmso-pagination:widow-orphan; \tfont-size:12.0pt; \tfont-family:Calibri; \tmso-ascii-font-family:Calibri; \tmso-ascii-theme-font:minor-latin; \tmso-fareast-font-family:Calibri; \tmso-fareast-theme-font:minor-latin; \tmso-hansi-font-family:Calibri; \tmso-hansi-theme-font:minor-latin; \tmso-bidi-font-family:\"Times New Roman\"; \tmso-bidi-theme-font:minor-bidi;} p \t{mso-style-noshow:yes; \tmso-style-priority:99; \tmso-margin-top-alt:auto; \tmargin-right:0in; \tmso-margin-bottom-alt:auto; \tmargin-left:0in; \tmso-pagination:widow-orphan; \tfont-size:12.0pt; \tfont-family:\"Times New Roman\"; \tmso-fareast-font-family:Calibri; \tmso-fareast-theme-font:minor-latin;} .MsoChpDefault \t{mso-style-type:export-only; \tmso-default-props:yes; \tfont-family:Calibri; \tmso-ascii-font-family:Calibri; \tmso-ascii-theme-font:minor-latin; \tmso-fareast-font-family:Calibri; \tmso-fareast-theme-font:minor-latin; \tmso-hansi-font-family:Calibri; \tmso-hansi-theme-font:minor-latin; \tmso-bidi-font-family:\"Times New Roman\"; \tmso-bidi-theme-font:minor-bidi;}size:8.5in 11.0in; \tmargin:1.0in 1.0in 1.0in 1.0in; \tmso-header-margin:.5in; \tmso-footer-margin:.5in; \tmso-paper-source:0;} div.WordSection1 \t{page:WordSection1;} --><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/16/2020<br>\n\t\t\t\t\tModified by: Daniela&nbsp;Calvetti</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe PI has completed her project \"Priorconditioned Krylov Subspace Methods for Inverse Problems\" and the outcomes include 1. the publication of several papers in top ranked journals; 2. the research training and mentoring of graduate and undergraduate students; 3. the interpretation of sparsity in inverse problems as an a priori belief in the Bayesian framework; 4. the promotion of a probabilistic interpretation of numerical linear algebra and, in general, scientific computing, in the classroom.\n\nThe project, and the dissemination of its results, has played a major role in promoting the idea that preconditioners are effective and computationally efficient ways to encode a priori belief about the sought solutions of inverse problems, and that the computational advantages are particularly striking when used in conjunction with Krylov iterative solvers.\n\nDuring the period of the project, Bayesian scientific computing had become much more accepted and mainstream, and the number of publications where Krylov iterative solvers are paired with prior-inspired preconditioners has grown at very high rate. Recently the PI has been invited to present the results of the project in plenary presentations at international workshops on both linear algebra and imagining science, in addition to inverse problems meetings.\n\nThe project has motivated and partially supported the research training of PhD students (Prezioso, Strang and Nakkireddy), MS students (Johnson and Devathi) and undergraduates. Some results from the project have been included as topics of the courses taught by the PI and her colleagues.\n\nThe complete analysis of a class of sparsity promoting iterative solvers, and the convergence and sparsity results proven as part of the project are getting some attention as computationally efficient alternatives to L1 penalties. The natural predisposition of priorconditioned Krylov iterative solvers for inverse problems to the quantification of the uncertainty in the solutions makes them particularly attractive for real applications.\n\nThe results of the project have been applied to the investigation of the changes occurring in brain during mediation (still ongoing), to the design of efficient and accurate meshes for the finite elements solution of inverse problems, and to data science problems, including, but not limited to, dictionary learning.\n\n\n\t\t\t\t\tLast Modified: 11/16/2020\n\n\t\t\t\t\tSubmitted by: Daniela Calvetti"
 }
}