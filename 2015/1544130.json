{
 "awd_id": "1544130",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: The Role of Instructor and Peer Feedback in Improving the Cognitive, Interpersonal, and Intrapersonal Competencies of Student Writers in STEM Courses",
 "cfda_num": "47.076",
 "org_code": "11010000",
 "po_phone": "7032925309",
 "po_email": "cdellapi@nsf.gov",
 "po_sign_block_name": "Connie Della-Piana",
 "awd_eff_date": "2015-09-15",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 73500.0,
 "awd_amount": 73500.0,
 "awd_min_amd_letter_date": "2015-09-16",
 "awd_max_amd_letter_date": "2015-09-16",
 "awd_abstract_narration": "The Promoting Research and Innovation in Methodologies for Evaluation (PRIME) program seeks to support research on evaluation with special emphasis on: (1) exploring innovative approaches for determining the impacts and usefulness of STEM education projects and programs; (2) building on and expanding the theoretical foundations for evaluating STEM education and workforce development initiatives, including translating and adapting approaches from other fields; and (3) growing the capacity and infrastructure of the evaluation field.\r\n\r\nThis project will have critical significance for Science, Technology, Engineering, and Mathematics (STEM) educators by increasing writing and collaboration skills in students, areas of importance to economics, science, and national security. This study focuses on teacher and peer interactions and writing quality and improvement in the context of undergraduate STEM courses. Specifically, the project will map the development of three competency domains (cognitive, interpersonal and intrapersonal) by researching the effects of teacher and peer response on writing improvement and knowledge adaptation in STEM courses. The project utilizes a web-based assessment tool called My Reviewers (MyR). The tool will be piloted by STEM faculty in college-level Introductory Biology or Chemistry on the campuses of University of South Florida (USF), North Carolina State University (NCSU), Dartmouth, Massachusetts Institute of Technology (MIT), and University of Pennsylvania (UPenn). Research domains include both academic performance and inter/intra-personal competencies. Project deliverables will provide new tools and procedures to assist in the assessment of students' knowledge, skills, and attitudes for project and program evaluation.\r\n\r\nApproximately 10,000 students enrolled in STEM courses at USF, NCSU, Dartmouth, MIT, and UPenn will upload their course-based writing to My Reviewers, an assessment tool, and use the tool to conduct peer reviews and team projects.  This information is supplemented by surveys of demographics and dispositions along with click patterns within the toolset. Researchers will subsequently analyze this wealth of data using predictive modeling of student writing ability and improvement, including text-based methods to identify useful features of comments, papers, peer reviews, student evaluations of other peers? reviews, and instructor and student meta-reflections. Outcome goals are to (1) demonstrate ways the assessment community can use real-time assessment tools to create valid measures of writing development; (2) provide quantitative evidence regarding the likely effects of particular commenting and scoring patterns on cohorts of students; (3) offer a domain map to help STEM educators better understand student success in the STEM curriculum; and (4) inform STEM faculty regarding the efficacy of peer review.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DGE",
 "org_div_long_name": "Division Of Graduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Valerie",
   "pi_last_name": "Ross",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Valerie Ross",
   "pi_email_addr": "vross@writing.upenn.edu",
   "nsf_id": "000695214",
   "pi_start_date": "2015-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": "3808 Walnut Street",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "726100",
   "pgm_ele_name": "Project & Program Evaluation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "009Z",
   "pgm_ref_txt": "PRIME - Promoting Research and Innovatio"
  }
 ],
 "app_fund": [
  {
   "app_code": "0415",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001516DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 73500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The Role of Instructor and Peer Feedback in Improving the Cognitive, Interpersonal, and Intrapersonal Competencies of Student Writers in STEM Courses</span></p>\n<p><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our project set out to explore the competency domains defined by the National Research Council as determining and predicting student success in the STEM curriculum. Our aim was to identify the development of these core competencies by studying the impact of teacher and peer responses to students' writing on writing revisions and knowledge adaptation. A second goal was to develop and refine digital assessment tools that would help us use this information to facilitate targeted, effective responses to student writing. We used My Reviewers (MyR), a web-assessment tool, to collect students' drafts along with the instructor and peer scores and comments each draft received.&nbsp; We sought to add to the literature on the effectiveness of instructor versus peer feedback, as well as the types of comments that prompted writing improvement, and whether these were related to any particular demographic factors such as gender, race, ethnicity, or socioeconomic class.</span></p>\n<p><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Launching one of the first big-data projects in writing analytics, we were unaware of the implications of designing such an ambitious project, involving over 120GB of data--roughly 180,000 student drafts--an exponential number of instructor and peer comments and scores students received on each of these drafts throughout the semester, representing 290 courses taught by faculty from across the disciplines, and then relating each of these to various other factors, including demographic and assignment data. The greatest obstacle proved to be data extraction. Arguably, our biggest achievement was figuring out how to improve upon project design so that data could be readily extracted, organized, and retrievable. Another significant achievement was the creation of a massive corpus of student writing, drawn from across the disciplines. This corpus is, as one linguist put it, a \"gold mine\" for researchers interested in human and machine learning, artificial intelligence, corpus linguistics, natural language processing, and the learning and teaching of writing in STEM and other fields.</span></p>\n<p><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Collecting and cleaning the data took the entire three years of the study. Our corpus analysis is thus at an early stage, but quite promising. For example, we have identified what appears to be a distinct pattern of difference in the comments high-performing versus lower-performing student writers receive from instructors and peers, illuminating how struggling and successful writers are perhaps being motivated or dispirited by the responses they are receiving.&nbsp; Another major finding for us was that our junior and senior STEM students were able to transfer knowledge from their first-year writing seminars to their third-and-fourth-year STEM classes, suggesting how first-year writing classes can serve as a foundation for writing in other disciplines. The study also advanced our understanding of how to assess STEM and other discipline-based writing.</span></p>\n<p><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our multidisciplinary research team and participants, along with the data collected, promise to contribute both to the creation of digital tools to advance student writing, and to the emergent field of big-data study of peer and instructor feedback in STEM and non-STEM student writing.</span></p>\n<p><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">What most surprised us throughout this study was the enthusiasm of STEM students for the kinds of writing opportunities that we were able to design in collaboration with their STEM faculty, and the quality of writing that these students produced. This taught us that STEM students' lack of confidence and motivation as writers is malleable.</span></p>\n<p><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Publications as a result of this study:</span></p>\n<ul>\n<li><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Branham, Cassandra, Joe Moxley, and Valerie Ross. 2015. \"My Reviewers: Participatory Design &amp; Crowd-Sourced Usability Processes.\" SIGDOC '15. Article No. 26. doi:</span><a id=\"docs-internal-guid-a9c12271-7fff-3b72-2336-b2561697c85e\" style=\"text-decoration: none;\" href=\"https://doi.org/10.1145/2775441.2775482\"><span style=\"font-size: 11pt; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;\">10.1145/2775441.2775482</span></a></li>\n</ul>\n<ul>\n<li><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">McMurtrie, Beth. 2018. \"How Artificial Intelligence Is Changing Teaching.\" Interview. The Chronicle of Higher Education. Volume 64. Issue 40. </span><a href=\"https://www.chronicle.com/article/How-Artificial-Intelligence-Is/244231\"><span style=\"text-decoration: none;\"><span style=\"font-size: 11pt; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;\">https://www.chronicle.com/article/How-Artificial-Intelligence-Is/244231</span></span></a><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span></li>\n</ul>\n<ul>\n<li><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Moxley, Joe and Valerie Ross. 2015. \"Using Digital Tools to Foster Writing Research and Students Success in STEM Courses.\" 2015 IEEE International Professional Communication Conference (IPCC). <a href=\"https://doi.org/10.1109/IPCC.2015.7235790\">doi:10.1109/IPCC.2015.7235790.</a></span></li>\n</ul>\n<ul>\n<li><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Ross, Valerie et al. 2016. \"Weighted Log-odds-ratio, Informative Dirichlet Prior Method to Enhance Peer Review Feedback for Low- and High-scoring College Students in a Required First-year Writing Program.\" <a id=\"docs-internal-guid-a616333e-7fff-5191-aa9a-7ac884839133\" style=\"text-decoration: none;\" href=\"http://toolsforwriters.com/edm-workshop-writing-analytics-data-mining-writing-studies/\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Writing Analytics, Data Mining, &amp; Writing Studies</span></a><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. </span></span><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Conference Proceedings, Educational Data Mining. <a href=\"http://ceur-ws.org/Vol-1633/ws2-paper4.pdf\">http://ceur-ws.org/Vol-1633/ws2-paper4.pdf</a>.</span></li>\n</ul>\n<ul>\n<li><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Ross, Valerie &amp; Rodger LeGrand. 2017. \"Assessing Writing Constructs: Toward an Expanded View of Inter-Rater Reliability.\" Journal of Writing Analytics. Volume 1. <a href=\"https://journals.colostate.edu/index.php/analytics/article/view/141\">https://journals.colostate.edu/index.php/analytics/article/view/141</a>.</span></li>\n</ul><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/03/2019<br>\n\t\t\t\t\tModified by: Valerie&nbsp;Ross</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe Role of Instructor and Peer Feedback in Improving the Cognitive, Interpersonal, and Intrapersonal Competencies of Student Writers in STEM Courses\n\nOur project set out to explore the competency domains defined by the National Research Council as determining and predicting student success in the STEM curriculum. Our aim was to identify the development of these core competencies by studying the impact of teacher and peer responses to students' writing on writing revisions and knowledge adaptation. A second goal was to develop and refine digital assessment tools that would help us use this information to facilitate targeted, effective responses to student writing. We used My Reviewers (MyR), a web-assessment tool, to collect students' drafts along with the instructor and peer scores and comments each draft received.  We sought to add to the literature on the effectiveness of instructor versus peer feedback, as well as the types of comments that prompted writing improvement, and whether these were related to any particular demographic factors such as gender, race, ethnicity, or socioeconomic class.\n\nLaunching one of the first big-data projects in writing analytics, we were unaware of the implications of designing such an ambitious project, involving over 120GB of data--roughly 180,000 student drafts--an exponential number of instructor and peer comments and scores students received on each of these drafts throughout the semester, representing 290 courses taught by faculty from across the disciplines, and then relating each of these to various other factors, including demographic and assignment data. The greatest obstacle proved to be data extraction. Arguably, our biggest achievement was figuring out how to improve upon project design so that data could be readily extracted, organized, and retrievable. Another significant achievement was the creation of a massive corpus of student writing, drawn from across the disciplines. This corpus is, as one linguist put it, a \"gold mine\" for researchers interested in human and machine learning, artificial intelligence, corpus linguistics, natural language processing, and the learning and teaching of writing in STEM and other fields.\n\nCollecting and cleaning the data took the entire three years of the study. Our corpus analysis is thus at an early stage, but quite promising. For example, we have identified what appears to be a distinct pattern of difference in the comments high-performing versus lower-performing student writers receive from instructors and peers, illuminating how struggling and successful writers are perhaps being motivated or dispirited by the responses they are receiving.  Another major finding for us was that our junior and senior STEM students were able to transfer knowledge from their first-year writing seminars to their third-and-fourth-year STEM classes, suggesting how first-year writing classes can serve as a foundation for writing in other disciplines. The study also advanced our understanding of how to assess STEM and other discipline-based writing.\n\nOur multidisciplinary research team and participants, along with the data collected, promise to contribute both to the creation of digital tools to advance student writing, and to the emergent field of big-data study of peer and instructor feedback in STEM and non-STEM student writing.\n\nWhat most surprised us throughout this study was the enthusiasm of STEM students for the kinds of writing opportunities that we were able to design in collaboration with their STEM faculty, and the quality of writing that these students produced. This taught us that STEM students' lack of confidence and motivation as writers is malleable.\n\nPublications as a result of this study:\n\nBranham, Cassandra, Joe Moxley, and Valerie Ross. 2015. \"My Reviewers: Participatory Design &amp; Crowd-Sourced Usability Processes.\" SIGDOC '15. Article No. 26. doi:10.1145/2775441.2775482\n\n\nMcMurtrie, Beth. 2018. \"How Artificial Intelligence Is Changing Teaching.\" Interview. The Chronicle of Higher Education. Volume 64. Issue 40. https://www.chronicle.com/article/How-Artificial-Intelligence-Is/244231.\n\n\nMoxley, Joe and Valerie Ross. 2015. \"Using Digital Tools to Foster Writing Research and Students Success in STEM Courses.\" 2015 IEEE International Professional Communication Conference (IPCC). doi:10.1109/IPCC.2015.7235790.\n\n\nRoss, Valerie et al. 2016. \"Weighted Log-odds-ratio, Informative Dirichlet Prior Method to Enhance Peer Review Feedback for Low- and High-scoring College Students in a Required First-year Writing Program.\" Writing Analytics, Data Mining, &amp; Writing Studies. Conference Proceedings, Educational Data Mining. http://ceur-ws.org/Vol-1633/ws2-paper4.pdf.\n\n\nRoss, Valerie &amp; Rodger LeGrand. 2017. \"Assessing Writing Constructs: Toward an Expanded View of Inter-Rater Reliability.\" Journal of Writing Analytics. Volume 1. https://journals.colostate.edu/index.php/analytics/article/view/141.\n\n\n\t\t\t\t\tLast Modified: 12/03/2019\n\n\t\t\t\t\tSubmitted by: Valerie Ross"
 }
}