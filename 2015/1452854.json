{
 "awd_id": "1452854",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Finding Levers for Privacy and Security by Design in Mobile Development",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 499136.0,
 "awd_amount": 499136.0,
 "awd_min_amd_letter_date": "2015-04-22",
 "awd_max_amd_letter_date": "2018-06-27",
 "awd_abstract_narration": "Mobile data are one of the fastest emerging forms of personal data. Ensuring the privacy and security of these data are critical challenges for the mobile device ecosystem. Mobile applications are easy to build and distribute, and can collect a large variety of sensitive personal data. Current approaches to protecting this data rely on security and privacy by design: encouraging developers to proactively implement security and privacy features to protect sensitive data. Although there are many technical innovations available to help developers protect user data, adoption of these innovations is low. Reasons for low adoption range from a lack of training in privacy or security design to the fact that privacy-enhancing features and best-practice data security measures are often expensive to implement, or even counter to business models that require user profiling or monitoring. Low adoption of privacy and security protection mechanisms is a social problem that inhibits a secure and trustworthy mobile ecosystem. It is unknown what factors can motivate developers to implement privacy or data security features when faced with disincentives such as longer development timelines, markets for personal data, and tensions between data protection and data-enabled services. Understanding developer decision-making is central to addressing this problem: the decisions made by developers are fundamental to enabling privacy and security by design to succeed. \r\n\r\nThis project  1) studies developers to discover work practices that encourage privacy and data security by design; and 2) builds tools to encourage such work practices. This project uses surveys and field experiments to determine factors that motivate privacy and security by design. It develops and test evidence-based toolkits for mobile developers to improve privacy and data security in the mobile data ecosystem. The project asks the following research questions: \r\n1. How do mobile application developers define privacy and security by design?\r\n2. What practices in mobile application development encourage developers to prioritize data protection? \r\n3. How can development tools encourage developers to prioritize data protection during design?\r\n\r\nBy answering these questions, the project illuminates development culture, illustrates how developers understand privacy and security needs, and discovers practices that  prioritize privacy and security. The project explains the impact of development practices on privacy and data security outcomes, advancing knowledge in software engineering and secure and trustworthy computing. Findings and products from this project support the rapidly developing mobile technology sector in enabling privacy and data security by design. Finding development practices that encourage privacy and data security by design improves technology transfer of technical innovations from the secure and trustworthy cyberspace research community, and bolsters protections for this sensitive data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Katherine",
   "pi_last_name": "Shilton",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Katherine Shilton",
   "pi_email_addr": "kshilton@umd.edu",
   "nsf_id": "000597876",
   "pi_start_date": "2015-04-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425141",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 128140.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 106890.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 159673.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 104433.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-91598496-7fff-20e6-c227-9dc7b08ae020\"> </span></p>\n<p dir=\"ltr\"><span>Digital technologies too frequently harvest vast amounts of user data, fail to properly protect that data, and use that data to make biased and untrusted decisions. This project studied developers and created interventions to scaffold design so that privacy, security and fairness are more central requirements. The project has both discovered practices that can encourage reflection on privacy, security, and trust during design, and created new educational interventions to teach reflective design practices.&nbsp;</span></p>\n<p dir=\"ltr\"><strong>Intellectual merit:</strong></p>\n<p dir=\"ltr\"><span>The project discovered development practices associated with secure and trustworthy design among two groups: mobile application developers and machine learning engineers. Mobile applications can (and often do) collect vast amounts of users' personal data. Machine learning (ML) tools use such data to make recommendations, decisions, or new predictions.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Using ethnographic work and content analysis, the project identified how mobile developers consider and define privacy. Analysis revealed that app developers rely almost entirely on notice and consent definitions, which are direct interpretations of the ways platforms define privacy in their developer policies. The qualitative work also discovered practices within application development that encourage developers to prioritize data protection and trustworthy systems. This phase identified design practices such as navigating app store policies and interacting with users as critical to mobile application developers' reflections on privacy. Publications sharing these project findings are regularly taught and cited in information science, business ethics, and computer science, and have been discussed with industry stakeholders in invited talks and meetings.&nbsp;&nbsp;</span></p>\n<p dir=\"ltr\"><span>Next, the project identified practices to increase reflection about bias, fairness, and privacy in machine learning. ML development challenges many of our current values-oriented design practices, as systems learn and change over time without transparency. The project used talk-aloud studies with ML engineers to discover that context documents called datasheets successfully scaffolded ethical reflection and sensitivity for machine learning engineers. </span><span>Findings are now being extended into tools to improve context documentation for machine learning.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Over the course of the project, we also made a significant theoretical contribution to secure and trustworthy computing by adapting and extending the theoretical construct of ethical sensitivity for technology development. We believe this construct will be useful for people studying many forms of ethical challenges in technology development, and have shared this theoretical development with technology ethics researchers in publications and workshops.&nbsp;</span></p>\n<p dir=\"ltr\"><strong>Broader Impacts:</strong></p>\n<p dir=\"ltr\"><span>The project used its discoveries to create two game-based interventions for experiential learning in secure and trustworthy computing. We built on our empirical findings using the Values-at-Play framework to give computer and information science students experience in navigating real-world decisions about privacy</span><span>, security, and trust. Privacy By Design is a cooperative game in which players take roles within a mobile app startup. A board guides decisions about what data to collect from users. The team gains and loses company resources and user trust as they draw cards that illustrate the real-world consequences of their data decisions. Content Moderation by Design is a cooperative game in which participants play the role of both policymakers and moderators at a startup social platform. In these roles, participants experience the challenges associated with moderating user-generated online content in a way that balances values such as free expression and community safety. The games were tested and evaluated with students, developers, and the broader public, and have been played in courses in both the U.S. and internationally. Both games, along with instructions for play and curricular materials to suppo</span><span>rt their integration into courses, are available for download under creative commons licenses. We also used the Content Moderation by Design game as a launching pad for increasing democratic participation in regulating the privacy, security, and trustworthiness of emerging technologies. The game served as a central feature in a citizen's panel devoted to advising Congress on platform liability for content. The resulting citizen's panel report was shared with Congressional lawmakers and is available on the project website.</span></p>\n<p dir=\"ltr\"><span>In addition to peer-reviewed publications and openly-available games, the project contributed to the professional development of 6 students, 3 of whom graduated with doctoral degrees. These students engaged in qualitative research methods, values-sensitive design, and developing evidence-based interventions for secure and trustworthy computing. Among the lasting impacts from the project are new knowledge of the link between design practices and implementation of secure and trustworthy computing best practices; new theoretical resources for research and evaluating technology ethics work; and new experience-based educational resources for teaching secure and trustworthy computing.&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/19/2021<br>\n\t\t\t\t\tModified by: Katherine&nbsp;Shilton</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nDigital technologies too frequently harvest vast amounts of user data, fail to properly protect that data, and use that data to make biased and untrusted decisions. This project studied developers and created interventions to scaffold design so that privacy, security and fairness are more central requirements. The project has both discovered practices that can encourage reflection on privacy, security, and trust during design, and created new educational interventions to teach reflective design practices. \nIntellectual merit:\nThe project discovered development practices associated with secure and trustworthy design among two groups: mobile application developers and machine learning engineers. Mobile applications can (and often do) collect vast amounts of users' personal data. Machine learning (ML) tools use such data to make recommendations, decisions, or new predictions. \nUsing ethnographic work and content analysis, the project identified how mobile developers consider and define privacy. Analysis revealed that app developers rely almost entirely on notice and consent definitions, which are direct interpretations of the ways platforms define privacy in their developer policies. The qualitative work also discovered practices within application development that encourage developers to prioritize data protection and trustworthy systems. This phase identified design practices such as navigating app store policies and interacting with users as critical to mobile application developers' reflections on privacy. Publications sharing these project findings are regularly taught and cited in information science, business ethics, and computer science, and have been discussed with industry stakeholders in invited talks and meetings.  \nNext, the project identified practices to increase reflection about bias, fairness, and privacy in machine learning. ML development challenges many of our current values-oriented design practices, as systems learn and change over time without transparency. The project used talk-aloud studies with ML engineers to discover that context documents called datasheets successfully scaffolded ethical reflection and sensitivity for machine learning engineers. Findings are now being extended into tools to improve context documentation for machine learning. \nOver the course of the project, we also made a significant theoretical contribution to secure and trustworthy computing by adapting and extending the theoretical construct of ethical sensitivity for technology development. We believe this construct will be useful for people studying many forms of ethical challenges in technology development, and have shared this theoretical development with technology ethics researchers in publications and workshops. \nBroader Impacts:\nThe project used its discoveries to create two game-based interventions for experiential learning in secure and trustworthy computing. We built on our empirical findings using the Values-at-Play framework to give computer and information science students experience in navigating real-world decisions about privacy, security, and trust. Privacy By Design is a cooperative game in which players take roles within a mobile app startup. A board guides decisions about what data to collect from users. The team gains and loses company resources and user trust as they draw cards that illustrate the real-world consequences of their data decisions. Content Moderation by Design is a cooperative game in which participants play the role of both policymakers and moderators at a startup social platform. In these roles, participants experience the challenges associated with moderating user-generated online content in a way that balances values such as free expression and community safety. The games were tested and evaluated with students, developers, and the broader public, and have been played in courses in both the U.S. and internationally. Both games, along with instructions for play and curricular materials to support their integration into courses, are available for download under creative commons licenses. We also used the Content Moderation by Design game as a launching pad for increasing democratic participation in regulating the privacy, security, and trustworthiness of emerging technologies. The game served as a central feature in a citizen's panel devoted to advising Congress on platform liability for content. The resulting citizen's panel report was shared with Congressional lawmakers and is available on the project website.\nIn addition to peer-reviewed publications and openly-available games, the project contributed to the professional development of 6 students, 3 of whom graduated with doctoral degrees. These students engaged in qualitative research methods, values-sensitive design, and developing evidence-based interventions for secure and trustworthy computing. Among the lasting impacts from the project are new knowledge of the link between design practices and implementation of secure and trustworthy computing best practices; new theoretical resources for research and evaluating technology ethics work; and new experience-based educational resources for teaching secure and trustworthy computing. \n\n \n\n\t\t\t\t\tLast Modified: 11/19/2021\n\n\t\t\t\t\tSubmitted by: Katherine Shilton"
 }
}