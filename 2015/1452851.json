{
 "awd_id": "1452851",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Situated Recognition: Learning to understand our local visual environment",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2015-03-01",
 "awd_exp_date": "2020-02-29",
 "tot_intn_awd_amt": 509965.0,
 "awd_amount": 509965.0,
 "awd_min_amd_letter_date": "2015-03-03",
 "awd_max_amd_letter_date": "2015-03-03",
 "awd_abstract_narration": "This project develops computer vision technologies for recognizing objects in our daily lives.  For recognizing visual content around us, where cameras can record multiple images over a period of time, there is an opportunity to take advantage of context that is not available for internet images.  This project pursues new representations and computational strategies exploiting this context efficiently to achieve high-quality visual recognition in our environment.  Balanced against the opportunity of using this context is the challenge of making recognition work in any particular environment, in the face of clutter, occlusion, non-canonical views, and idiosyncratic appearance variation.  The methods developed can be a core part of developing technology to help computer vision systems scale to recognize everything in our daily world. The research leads to automated systems for better understanding and monitoring of our daily environment, improved human-computer interaction, and encourages more research in this area.\r\n\r\nThis research direction is different from the majority of work in recognition that has focused on internet images collected from the web. The biases of such web-collected may lead to models that do not generalize to a particular environment. Situated recognition allows exploiting local context, including human interaction and spoken language, to build models specific to an environment and furthermore to the parts of an environment that are important to people. The project collects multiple datasets stressing multi-view imagery and long-term observation of environments while sampling a wide variety of settings. The research team develops algorithms to parse and detect objects by exploiting context, efficient re-use, and context-dependent saliency; and uses situated natural language to drive automatic learning of visual recognition models.\r\n\r\nProject Webpage: http://acberg.com",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Berg",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander C Berg",
   "pi_email_addr": "bergac@uci.edu",
   "nsf_id": "000569050",
   "pi_start_date": "2015-03-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Chapel Hill",
  "perf_str_addr": "201 S. Columbia St",
  "perf_city_name": "Chapel Hill",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275993175",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 509965.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project developed computer vision technologies for recognizing objects in our daily lives.&nbsp; Toward the long-term goal of recognizing visual content around us from multiple images we developed a series of datasets, analyses, and new approaches.&nbsp; Some of the most notable were: 1) Our work on \"single-shot detection\" that significantly advanced the speed vs accuracy trade-off for fast detection by predicting object detections on a coarse grid and then \"snapping\" them to the exact location, and was quickly picked up for embedded applications especially work toward autonomous vehicles; 2) Our work on datasets including AVD, A Dataset for Developing and Benchmarking Active Vision, collected using a robot platform that captured a dense array of images from real world environments that could be sampled to simulate robots taking paths through the real-world environment with real imagery; 3) Our work on training networks to synthesize novel views of objects in \"Transformation-Grounded Image Generation\" as a step toward dealing with very limited training data; and 4) our work on learning to track robustly and learning to associate object detections without explicit tracking to better use multiple images of objects when available.&nbsp; Together this work made object detection faster and more accurate, and improved the ability to use multiple images of an object to achieve better accuracy for detection at low computational cost.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/16/2021<br>\n\t\t\t\t\tModified by: Alexander&nbsp;C&nbsp;Berg</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project developed computer vision technologies for recognizing objects in our daily lives.  Toward the long-term goal of recognizing visual content around us from multiple images we developed a series of datasets, analyses, and new approaches.  Some of the most notable were: 1) Our work on \"single-shot detection\" that significantly advanced the speed vs accuracy trade-off for fast detection by predicting object detections on a coarse grid and then \"snapping\" them to the exact location, and was quickly picked up for embedded applications especially work toward autonomous vehicles; 2) Our work on datasets including AVD, A Dataset for Developing and Benchmarking Active Vision, collected using a robot platform that captured a dense array of images from real world environments that could be sampled to simulate robots taking paths through the real-world environment with real imagery; 3) Our work on training networks to synthesize novel views of objects in \"Transformation-Grounded Image Generation\" as a step toward dealing with very limited training data; and 4) our work on learning to track robustly and learning to associate object detections without explicit tracking to better use multiple images of objects when available.  Together this work made object detection faster and more accurate, and improved the ability to use multiple images of an object to achieve better accuracy for detection at low computational cost.\n\n\t\t\t\t\tLast Modified: 04/16/2021\n\n\t\t\t\t\tSubmitted by: Alexander C Berg"
 }
}