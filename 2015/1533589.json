{
 "awd_id": "1533589",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative: NCS-FO: Integrating neural interfaces and machine intelligence for advanced neural prosthetics",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Shubhra Gangopadhyay",
 "awd_eff_date": "2015-08-01",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 341984.0,
 "awd_amount": 341984.0,
 "awd_min_amd_letter_date": "2015-08-10",
 "awd_max_amd_letter_date": "2015-08-10",
 "awd_abstract_narration": "Brain-machine interfaces (BMI) read signals directly from the brain to control external devices such as robotic limbs.  While this technology has great potential to benefit people who are paralyzed, BMIs often have poor performance because they use noisy, low-level signals to simultaneously control many aspects of the robotic limb's movements.  In contrast, this project will address this shortcoming by reading high-level intents from the brain in order to control an intelligent robotic system.  These changes reflect cutting-edge advances in neuroscience and machine intelligence and will require close cooperation between scientists, engineers, and physicians.  The project aims to leverage expertise across these diverse fields in order to generate significant improvements in BMI technology to advance the national health, increase scientific understanding of the brain, and lead to dramatic improvements in the quality of life for these severely disabled persons.\r\n\r\nThis collaborative project will decode high-level cognitive actions from neural signals recorded in the parietal cortex of a tetraplegic human, then carry out those intents using a smart robotic prosthesis.  Persons with tetraplegia who have multielectrode arrays (MEA) implanted in reach and grasp areas of the posterior parietal cortex (PPC), will participate in experiments to explore the neural representation of cognitive intentions in human PPC including object selection, action intention, and neural control of robotic limbs.  Experimental results will be used to construct BMI control algorithms optimized to decode these cognitive signals.  In parallel, a modular, semi-autonomous robotic prosthesis will be developed that can identify household objects and plan reach-and-grasp movements to manipulate or transport the objects.  These scientific and technological efforts will be supported by continued clinical care of the tetraplegic participants. The study will explore increasingly capable iterations of the BMI system, culminating in testing of the fully developed BMI system in the participants' own home environment where they will practice activities of daily living. The resulting system will leverage deep insights in cognitive neuroscience and advanced capabilities in machine sensing and robotic control systems to substantially improve the ease of use and capability of brain-machine interfaces.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Andersen",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Richard A Andersen",
   "pi_email_addr": "andersen@vis.caltech.edu",
   "nsf_id": "000471356",
   "pi_start_date": "2015-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "California Institute of Technology",
  "inst_street_address": "1200 E CALIFORNIA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "PASADENA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6263956219",
  "inst_zip_code": "911250001",
  "inst_country_name": "United States",
  "cong_dist_code": "28",
  "st_cong_dist_code": "CA28",
  "org_lgl_bus_name": "CALIFORNIA INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "U2JMKHNS5TG4"
 },
 "perf_inst": {
  "perf_inst_name": "California Institute of Technology",
  "perf_str_addr": "1200 E. California Blvd.",
  "perf_city_name": "Pasadena",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "911250001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "28",
  "perf_st_cong_dist": "CA28",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "534500",
   "pgm_ele_name": "Engineering of Biomed Systems"
  },
  {
   "pgm_ele_code": "763300",
   "pgm_ele_name": "EFRI Research Projects"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "004E",
   "pgm_ref_txt": "BIOMEDICAL ENG AND DIAGNOSTICS"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "8551",
   "pgm_ref_txt": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 341984.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In spinal cord injury (SCI), the brain remains functional despite the damaged communication pathway between the brain and the rest of the body. As one promising solution to provide increased independence to persons with SCI, brain-machine interfaces (BMI) read information directly from the brain to allow the user to control assistive devices, such as a robot arm or computer cursor. Traditionally, these systems directly correlate neural activity to the individual degrees of freedom (DOFs) in the assistive device. Although relatively good performance is possible, modern neural recording technology yields fewer DOFs than are available in an anthropomorphic robotic prosthesis. Furthermore, as the number of DOFs increases, the mental burden of controlling multiple things simultaneously also increases. This project was designed to decode high-level cognitive intent from the parietal cortex, an area of the brain involved in making movement plans, for execution by a sensorized, machine-intelligent robotic system. This collaborative project incorporated activities across multiple tracks including Cognitive Neural Control, Advanced Robotics, and Clinical areas of research.</p>\n<p>We worked with two quadriplegic human subjects during this work. With both subjects, a central theme was to develop the capability to consciously modulate neural activity as a control signal for the machine-intelligent robotic systems. The subjects quickly developed mental strategies to reliably distinguish between \"go\" and \"no-go\" states (Figure 1). We then tested larger sets of actions, finding robust tuning in both single neuron activity and local field potentials (Figure 2). We observed co-modulation between mental imagery of actions and objects in the subject's field of view; for example, firing rates were higher when a tool was shown while the subject imagined reaching out to grasp it than when an unrelated image was shown (Figure 3).&nbsp; Even as the array implant lifetimes increased to several years, we continued to find features with directional preference that could be used to decode movement (Figure 4), including conscious modulation to specific firing rates, not just \"on\" or \"off\" (Figure 5). The subjects were able to discriminate between five different grasp shapes with mental imagery (Figure 6).</p>\n<p>In addition to working toward neuroscientific achievements, we worked closely with JHU/APL to continue developing the shared control framework between neural decoding and machine intelligence. We implemented a Hidden Markov Model-based classifier to classify states online and integrated an eye-tracking solution into the shared control framework. We also added multi-object detection and multi-action selection, modular action sequences that can be updated mid-stream, and abstract representations of objects and actions to allow for easier configuration in new environments.</p>\n<p>The field of neural engineering, the principle discipline of this project, is expanding at an enormous rate. Many efforts are underway globally to tackle the issue of decoding behavior from the brain to control prosthetic devices. The goals completed under this project have provided evidence to demonstrate improvements in brain-machine interfaces (BMI) that can provide better clinical and rehabilitative capabilities to the device and increased independence to people living with tetraplegia or other disorders of motor function. The results of this project will have a profound impact on the quality of life and care of those suffering from upper limb motor dysfunction.</p>\n<p>In conjunction with our NSF sponsored grant we received an NSF participant support grant for patient outreach. This participant support grant allowed us to work closely with clinicians and patients at Rancho Los Amigos National Rehabilitation Center to hold a patient-centered outreach exposition on July 27, 2019. The purpose of this event was to share with disabled persons the perspectives of the research participants in ours and other similar projects. By all measures, the event was hugely successful: approximately 130 people attended, with 55% patients and 45% caregivers, nurses, occupational and physical therapists, physicians and other interested community members. We invited several other research projects working with Rancho, and the patient participants in these projects, to give short presentations describing the research in an accessible way and giving the participants an opportunity to talk and answer audience questions about their experience in the research. These additional projects included: stem cell therapy, spinal cord stimulation, ECoG-based BMI for walking, and an exoskeleton demonstration. After the event we provided lunch for the attendees and staffed booths where researchers could engage with patients one-on-one. We received a great deal of positive feedback from patients, clinicians, and the leadership of Rancho Los Amigos National Rehabilitation Center and Rancho Research Foundation, which was instrumental in assisting with the logistics of the event. It is likely, given the success of the event, that the 2019 Rancho Patient Expo will be the first of a now annual event intended to bring researchers and patients together as we work to build solutions that meet the needs of persons with disability.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/25/2019<br>\n\t\t\t\t\tModified by: Richard&nbsp;A&nbsp;Andersen</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1571959680809_Figure1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1571959680809_Figure1--rgov-800width.jpg\" title=\"Figure 1: Example neuronal firing rates.\"><img src=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1571959680809_Figure1--rgov-66x44.jpg\" alt=\"Figure 1: Example neuronal firing rates.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Neuronal firing rates modulate when the subject imagines grasp vs. arithmetic. Grasp was linked to \"go\" in the top trace, and to \"stop\" in the bottom trace. In both, blue corresponds to \"go\" and red corresponds to \"stop\". In both cases, the neuronal modulation reliably encoded the same responses.</div>\n<div class=\"imageCredit\">Richard Andersen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Richard&nbsp;A&nbsp;Andersen</div>\n<div class=\"imageTitle\">Figure 1: Example neuronal firing rates.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032572371_Figure2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032572371_Figure2--rgov-800width.jpg\" title=\"Figure 2. Action preference in single units and local field potential.\"><img src=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032572371_Figure2--rgov-66x44.jpg\" alt=\"Figure 2. Action preference in single units and local field potential.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Neuronal firing rates (top) and power in 130-170 Hz of the local field potential (bottom) modulate during imagined actions (different channels). Both responded preferentially to the \"windmill\" action.</div>\n<div class=\"imageCredit\">Richard Andersen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Richard&nbsp;A&nbsp;Andersen</div>\n<div class=\"imageTitle\">Figure 2. Action preference in single units and local field potential.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032628291_Figure3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032628291_Figure3--rgov-800width.jpg\" title=\"Figure 3. Action and object modulated firing rate for subject P3.\"><img src=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032628291_Figure3--rgov-66x44.jpg\" alt=\"Figure 3. Action and object modulated firing rate for subject P3.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">In neurons tuned for actions, higher firing rates were observed when a task relevant object was displayed than when an irrelevant object was displayed or there was nothing on the screen.</div>\n<div class=\"imageCredit\">Richard Andersen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Richard&nbsp;A&nbsp;Andersen</div>\n<div class=\"imageTitle\">Figure 3. Action and object modulated firing rate for subject P3.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032682300_Figure4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032682300_Figure4--rgov-800width.jpg\" title=\"Figure 4. Average endpoint cursor movements during a center out reaching task.\"><img src=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032682300_Figure4--rgov-66x44.jpg\" alt=\"Figure 4. Average endpoint cursor movements during a center out reaching task.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Shaded regions represent the movements? standard error. Each panel shows the average for a single experimental session, around 3 minutes in duration, with 40% computer assistance.</div>\n<div class=\"imageCredit\">Richard Andersen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Richard&nbsp;A&nbsp;Andersen</div>\n<div class=\"imageTitle\">Figure 4. Average endpoint cursor movements during a center out reaching task.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032730804_Figure5--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032730804_Figure5--rgov-800width.jpg\" title=\"Figure 5. Difference in population firing rates during 1D endpoint control.\"><img src=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032730804_Figure5--rgov-66x44.jpg\" alt=\"Figure 5. Difference in population firing rates during 1D endpoint control.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">As the target location moved from left to right, the average difference in population firing rates also moved from its minimum to maximum, showing the subject?s ability to volitionally control population firing rates.</div>\n<div class=\"imageCredit\">Richard Andersen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Richard&nbsp;A&nbsp;Andersen</div>\n<div class=\"imageTitle\">Figure 5. Difference in population firing rates during 1D endpoint control.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032778419_Figure6--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032778419_Figure6--rgov-800width.jpg\" title=\"Figure 6. Percentage of neurons tuned to grasp.\"><img src=\"/por/images/Reports/POR/2019/1533589/1533589_10385997_1572032778419_Figure6--rgov-66x44.jpg\" alt=\"Figure 6. Percentage of neurons tuned to grasp.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Sorted single units exhibited grasp-related tuning during the different phases of the task. During the Image Cue phase, an image of a human hand forming one of the five grasps was shown on the screen. Then, after a brief delay, the subject was instructed to begin imagining the cued grasp (Action).</div>\n<div class=\"imageCredit\">Richard Andersen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Richard&nbsp;A&nbsp;Andersen</div>\n<div class=\"imageTitle\">Figure 6. Percentage of neurons tuned to grasp.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIn spinal cord injury (SCI), the brain remains functional despite the damaged communication pathway between the brain and the rest of the body. As one promising solution to provide increased independence to persons with SCI, brain-machine interfaces (BMI) read information directly from the brain to allow the user to control assistive devices, such as a robot arm or computer cursor. Traditionally, these systems directly correlate neural activity to the individual degrees of freedom (DOFs) in the assistive device. Although relatively good performance is possible, modern neural recording technology yields fewer DOFs than are available in an anthropomorphic robotic prosthesis. Furthermore, as the number of DOFs increases, the mental burden of controlling multiple things simultaneously also increases. This project was designed to decode high-level cognitive intent from the parietal cortex, an area of the brain involved in making movement plans, for execution by a sensorized, machine-intelligent robotic system. This collaborative project incorporated activities across multiple tracks including Cognitive Neural Control, Advanced Robotics, and Clinical areas of research.\n\nWe worked with two quadriplegic human subjects during this work. With both subjects, a central theme was to develop the capability to consciously modulate neural activity as a control signal for the machine-intelligent robotic systems. The subjects quickly developed mental strategies to reliably distinguish between \"go\" and \"no-go\" states (Figure 1). We then tested larger sets of actions, finding robust tuning in both single neuron activity and local field potentials (Figure 2). We observed co-modulation between mental imagery of actions and objects in the subject's field of view; for example, firing rates were higher when a tool was shown while the subject imagined reaching out to grasp it than when an unrelated image was shown (Figure 3).  Even as the array implant lifetimes increased to several years, we continued to find features with directional preference that could be used to decode movement (Figure 4), including conscious modulation to specific firing rates, not just \"on\" or \"off\" (Figure 5). The subjects were able to discriminate between five different grasp shapes with mental imagery (Figure 6).\n\nIn addition to working toward neuroscientific achievements, we worked closely with JHU/APL to continue developing the shared control framework between neural decoding and machine intelligence. We implemented a Hidden Markov Model-based classifier to classify states online and integrated an eye-tracking solution into the shared control framework. We also added multi-object detection and multi-action selection, modular action sequences that can be updated mid-stream, and abstract representations of objects and actions to allow for easier configuration in new environments.\n\nThe field of neural engineering, the principle discipline of this project, is expanding at an enormous rate. Many efforts are underway globally to tackle the issue of decoding behavior from the brain to control prosthetic devices. The goals completed under this project have provided evidence to demonstrate improvements in brain-machine interfaces (BMI) that can provide better clinical and rehabilitative capabilities to the device and increased independence to people living with tetraplegia or other disorders of motor function. The results of this project will have a profound impact on the quality of life and care of those suffering from upper limb motor dysfunction.\n\nIn conjunction with our NSF sponsored grant we received an NSF participant support grant for patient outreach. This participant support grant allowed us to work closely with clinicians and patients at Rancho Los Amigos National Rehabilitation Center to hold a patient-centered outreach exposition on July 27, 2019. The purpose of this event was to share with disabled persons the perspectives of the research participants in ours and other similar projects. By all measures, the event was hugely successful: approximately 130 people attended, with 55% patients and 45% caregivers, nurses, occupational and physical therapists, physicians and other interested community members. We invited several other research projects working with Rancho, and the patient participants in these projects, to give short presentations describing the research in an accessible way and giving the participants an opportunity to talk and answer audience questions about their experience in the research. These additional projects included: stem cell therapy, spinal cord stimulation, ECoG-based BMI for walking, and an exoskeleton demonstration. After the event we provided lunch for the attendees and staffed booths where researchers could engage with patients one-on-one. We received a great deal of positive feedback from patients, clinicians, and the leadership of Rancho Los Amigos National Rehabilitation Center and Rancho Research Foundation, which was instrumental in assisting with the logistics of the event. It is likely, given the success of the event, that the 2019 Rancho Patient Expo will be the first of a now annual event intended to bring researchers and patients together as we work to build solutions that meet the needs of persons with disability.\n\n\t\t\t\t\tLast Modified: 10/25/2019\n\n\t\t\t\t\tSubmitted by: Richard A Andersen"
 }
}