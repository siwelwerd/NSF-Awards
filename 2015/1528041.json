{
 "awd_id": "1528041",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Low-Cost Deduplication and Search for Versioned Datasets",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Wei-Shinn Ku",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 499998.0,
 "awd_amount": 515998.0,
 "awd_min_amd_letter_date": "2015-08-20",
 "awd_max_amd_letter_date": "2016-08-01",
 "awd_abstract_narration": "Organizations and companies often archive high volumes of versioned digital datasets. There are research challenges and opportunities for developing  integrated  archival and search support needed for data preservation, electronic discovery, and  regulatory compliance. Since versioned datasets contain highly repetitive content, deduplication can reduce the storage demand by an order of magnitude or more; however such an optimization is resource-intensive. After deduplication, the structure of an inverted index for versioned data becomes complex and  it is expensive to search relevant results.  This project will study low-cost solutions for compact archiving and indexing and develop efficient algorithms and systems techniques for searching versioned datasets. It will also consider that the archived data can be stored in an untrusted server environment and investigate tradeoffs in efficiency and privacy-preservation for search. The developed solutions will bring significant computing and storage cost advantages for application users involving large-scale versioned data management and search. The developed software will be made public for research communities. The research effort will be integrated with an educational plan containing research mentoring, instruction improvement, and outreach activities.\r\n\r\nThis project will be focused on studying key challenges and cost-sensitive technical aspects in integrated archival and search support for managing large versioned datasets. The main tasks include efficient software architecture and optimization for detecting duplicated content on a cloud cluster architecture, fast multi-phase search with a hybrid index structure to exploit content similarity and query characteristics, and an efficient privacy-preserving framework with top result ranking.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tao",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tao Yang",
   "pi_email_addr": "tyang@cs.ucsb.edu",
   "nsf_id": "000202471",
   "pi_start_date": "2015-08-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Stefano",
   "pi_last_name": "Tessaro",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefano Tessaro",
   "pi_email_addr": "tessaro@cs.washington.edu",
   "nsf_id": "000660734",
   "pi_start_date": "2015-08-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "",
  "perf_city_name": "Santa Babara",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931065110",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 499998.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has studied key challenges and cost-sensitive technical aspects in archival and search support for managing large versioned datasets. The main activities include efficient software architecture and optimization in detecting duplicated content on a cloud cluster architecture, fast multi-phase search with a hybrid index structure to exploit content similarity and query characteristics, and an efficient privacy-preserving framework with top result ranking.</p>\n<p>For cluster-based cloud backup, we have proposed&nbsp;a source-side backup scheme with low-resource usage through collaborative deduplication and approximated lazy deletion when frequent virtual machine snapshot backup is required in a large-scale cloud cluster. The key ideas are to orchestrate multi-round duplicate detection batches among machines in a partitioned asynchronous manner and remove most unreferenced content chunks with approximated snapshot deletion.&nbsp;For the tested datasets, network cost is reduced by 255% and storage cost is reduced by 355% times compared to a pure dirty-bit-based source-side deduplication. The new scheme is an order of magnitude faster than a synchronous scheme.</p>\n<p>For versioned-data search,&nbsp;we&nbsp;proposed an approach called representative-guided two-phase search (RTP) that uses cluster-based retrieval to quickly narrow the search scope guided by version representatives at Phase 1 and developed&nbsp;a hybrid index structure with adaptive runtime data traversal to conduct fast Phase 2 search and ranking. The hybrid scheme exploits the advantages of forward index and inverted index based on the term characteristics to minimize the time in extracting positional and other features during runtime search.&nbsp;The evaluation shows that &nbsp;RTP can be up-to&nbsp;4x as fast as the previously-developed two-phase method for the tested datasets. The RTP prototype software is accessible from the project website.<strong>&nbsp;</strong></p>\n<p>To improve query-time efficiency for search, we have proposed an approximation of interaction-based neural ranking algorithms based on locality-sensitive hashing. This scheme accelerates query-document interaction computation by using a query-time cache with precomputed term vectors, and speeds up kernel calculation by taking advantages of limited integer similarity values. This scheme preserves semantic similarity of search words with a small error while yielding a significant efficiency gain. For example,&nbsp;our approximation solution for neural ranking yields up-to 106x scoring time speedup compared to the previous work in the tested datasets while accomplishing competitive relevance.&nbsp;We have also&nbsp;developed cache-aware runtime optimization to speed up the scoring of tree ensembles in ranking a large number of feature vectors.&nbsp;</p>\n<p>We have further addressed several open problems in privacy-aware top result ranking based on linear and nonlinear ranking algorithms.&nbsp;The main challenge for privacy protection is that letting a cloud server perform ranking computation may unsafely reveal privacy-sensitive information.&nbsp;&nbsp;For&nbsp;linear additive scoring used for searching modest-sized cloud datasets, we have developed a privacy-aware scheme&nbsp;with single-round client-server collaboration and server-side partial ranking based on blinded feature weights and random masks.&nbsp;&nbsp;This scheme strikes for a tradeoff between privacy and efficiency and client-side preprocessing includes query decomposition with chunked postings to facilitate earlier range intersection and fast access of server-side key-value stores. Server-side query processing can efficiently deal with feature vector sparsity through optional feature matching and enable result filtering with query-dependent chunk-wide random masks for queries that yield too many matched documents.&nbsp;</p>\n<p>For nonlinear result ranking with tree-based ensembles,&nbsp;we have developed a privacy-aware server-side query processing scheme based on comparison-preserving mapping that&nbsp;hides feature values and tree thresholds. This solution scales well for large datasets since a server does not involve time-consuming heavyweight cryptography or additional client involvement after receiving encrypted search words from a client. Our solution restricts the computation complexity of feature composition which represents a trade-off of relevancy for privacy. The evaluation shows that decision trees relying more on raw features can still perform well with a hybrid model trained for each query length and our approach only yields a small relevance degradation compared to a traditional tree algorithm for the tested dataset.&nbsp;For ranking with interaction-based neural ranking, we have&nbsp;analyzed the critical leakages during score calculation and studied countermeasures to mitigate such a leakage. We&nbsp;have developed a privacy-aware scheme and its&nbsp;key techniques include the replacement of the exact kernel with a tree ensemble, a soft match map using obfuscated kernel values and term closures, and adaptive clustering for term occurrence obfuscation and storage optimization. Our design for privacy enhancement is to prevent the leakage of two critical text signals in terms of term frequency and occurrence needed for the attacks shown in the previous work and our analysis. We have also investigated the feasibility of using trusted processors to enhance privacy for search.</p>\n<p>During the project period, the PI and co-PI have taught graduate and&nbsp;undergraduate courses on information retrieval, high performance computing, cryptography, and security, and have&nbsp;&#8232;advised a number of&nbsp;&nbsp;PhD, master, and undergraduate students including under-represented students,&nbsp;&nbsp;working on related research projects.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/24/2020<br>\n\t\t\t\t\tModified by: Tao&nbsp;Yang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project has studied key challenges and cost-sensitive technical aspects in archival and search support for managing large versioned datasets. The main activities include efficient software architecture and optimization in detecting duplicated content on a cloud cluster architecture, fast multi-phase search with a hybrid index structure to exploit content similarity and query characteristics, and an efficient privacy-preserving framework with top result ranking.\n\nFor cluster-based cloud backup, we have proposed a source-side backup scheme with low-resource usage through collaborative deduplication and approximated lazy deletion when frequent virtual machine snapshot backup is required in a large-scale cloud cluster. The key ideas are to orchestrate multi-round duplicate detection batches among machines in a partitioned asynchronous manner and remove most unreferenced content chunks with approximated snapshot deletion. For the tested datasets, network cost is reduced by 255% and storage cost is reduced by 355% times compared to a pure dirty-bit-based source-side deduplication. The new scheme is an order of magnitude faster than a synchronous scheme.\n\nFor versioned-data search, we proposed an approach called representative-guided two-phase search (RTP) that uses cluster-based retrieval to quickly narrow the search scope guided by version representatives at Phase 1 and developed a hybrid index structure with adaptive runtime data traversal to conduct fast Phase 2 search and ranking. The hybrid scheme exploits the advantages of forward index and inverted index based on the term characteristics to minimize the time in extracting positional and other features during runtime search. The evaluation shows that  RTP can be up-to 4x as fast as the previously-developed two-phase method for the tested datasets. The RTP prototype software is accessible from the project website. \n\nTo improve query-time efficiency for search, we have proposed an approximation of interaction-based neural ranking algorithms based on locality-sensitive hashing. This scheme accelerates query-document interaction computation by using a query-time cache with precomputed term vectors, and speeds up kernel calculation by taking advantages of limited integer similarity values. This scheme preserves semantic similarity of search words with a small error while yielding a significant efficiency gain. For example, our approximation solution for neural ranking yields up-to 106x scoring time speedup compared to the previous work in the tested datasets while accomplishing competitive relevance. We have also developed cache-aware runtime optimization to speed up the scoring of tree ensembles in ranking a large number of feature vectors. \n\nWe have further addressed several open problems in privacy-aware top result ranking based on linear and nonlinear ranking algorithms. The main challenge for privacy protection is that letting a cloud server perform ranking computation may unsafely reveal privacy-sensitive information.  For linear additive scoring used for searching modest-sized cloud datasets, we have developed a privacy-aware scheme with single-round client-server collaboration and server-side partial ranking based on blinded feature weights and random masks.  This scheme strikes for a tradeoff between privacy and efficiency and client-side preprocessing includes query decomposition with chunked postings to facilitate earlier range intersection and fast access of server-side key-value stores. Server-side query processing can efficiently deal with feature vector sparsity through optional feature matching and enable result filtering with query-dependent chunk-wide random masks for queries that yield too many matched documents. \n\nFor nonlinear result ranking with tree-based ensembles, we have developed a privacy-aware server-side query processing scheme based on comparison-preserving mapping that hides feature values and tree thresholds. This solution scales well for large datasets since a server does not involve time-consuming heavyweight cryptography or additional client involvement after receiving encrypted search words from a client. Our solution restricts the computation complexity of feature composition which represents a trade-off of relevancy for privacy. The evaluation shows that decision trees relying more on raw features can still perform well with a hybrid model trained for each query length and our approach only yields a small relevance degradation compared to a traditional tree algorithm for the tested dataset. For ranking with interaction-based neural ranking, we have analyzed the critical leakages during score calculation and studied countermeasures to mitigate such a leakage. We have developed a privacy-aware scheme and its key techniques include the replacement of the exact kernel with a tree ensemble, a soft match map using obfuscated kernel values and term closures, and adaptive clustering for term occurrence obfuscation and storage optimization. Our design for privacy enhancement is to prevent the leakage of two critical text signals in terms of term frequency and occurrence needed for the attacks shown in the previous work and our analysis. We have also investigated the feasibility of using trusted processors to enhance privacy for search.\n\nDuring the project period, the PI and co-PI have taught graduate and undergraduate courses on information retrieval, high performance computing, cryptography, and security, and have &#8232;advised a number of  PhD, master, and undergraduate students including under-represented students,  working on related research projects. \n\n \n\n\t\t\t\t\tLast Modified: 07/24/2020\n\n\t\t\t\t\tSubmitted by: Tao Yang"
 }
}