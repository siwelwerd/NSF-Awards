{
 "awd_id": "1545089",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Synergy: Collaborative Research: Adaptive Intelligence for Cyber-Physical Automotive Active Safety - System Design and Evaluation",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928950",
 "po_email": "rwachter@nsf.gov",
 "po_sign_block_name": "Ralph Wachter",
 "awd_eff_date": "2015-09-15",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 240000.0,
 "awd_amount": 240000.0,
 "awd_min_amd_letter_date": "2015-09-16",
 "awd_max_amd_letter_date": "2015-09-16",
 "awd_abstract_narration": "The automotive industry finds itself at a cross-roads. Current advances in MEMS sensor technology, the emergence of embedded control software, the rapid progress in computer technology, digital image processing, machine learning and control algorithms, along with an ever increasing investment in vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) technologies, are about to revolutionize the way we use vehicles and commute in everyday life. Automotive active safety systems, in particular, have been used with enormous success in the past 50 years and have helped keep traffic accidents in check. Still, more than 30,000 deaths and 2,000,000 injuries occur each year in the US alone, and many more worldwide. The impact of traffic accidents on the economy is estimated to be as high as $300B/yr in the US alone. Further improvement in terms of driving safety (and comfort) necessitates that the next generation of active safety systems are more proactive (as opposed to reactive) and can comprehend and interpret driver intent. Future active safety systems will have to account for the diversity of drivers' skills, the behavior of drivers in traffic, and the overall traffic conditions.\r\n\r\nThis research aims at improving the current capabilities of automotive active safety control systems (ASCS) by taking into account the interactions between the driver, the vehicle, the ASCS and the environment. Beyond solving a fundamental problem in automotive industry, this research will have ramifications in other cyber-physical domains, where humans manually control vehicles or equipment including: flying, operation of heavy machinery, mining, tele-robotics, and robotic medicine. Making autonomous/automated systems that feel and behave \"naturally\" to human operators is not always easy. As these systems and machines participate more in everyday interactions with humans, the need to make them operate in a predictable manner is more urgent than ever.\r\n\r\nTo achieve the goals of the proposed research, this project will use the estimation of the driver's cognitive state to adapt the ASCS accordingly, in order to achieve a seamless operation with the driver. Specifically, new methodologies will be developed to infer long-term and short-term behavior of drivers via the use of Bayesian networks and neuromorphic algorithms to estimate the driver's skills and current state of attention from eye movement data, together with dynamic motion cues obtained from steering and pedal inputs. This information will be injected into the ASCS operation in order to enhance its performance by taking advantage of recent results from the theory of adaptive and real-time, model-predictive optimal control. The correct level of autonomy and workload distribution between the driver and ASCS will ensure that no conflicts arise between the driver and the control system, and the safety and passenger comfort are not compromised. A comprehensive plan will be used to test and validate the developed theory by collecting measurements from several human subjects while operating a virtual reality-driving simulator.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Laurent",
   "pi_last_name": "Itti",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Laurent Itti",
   "pi_email_addr": "itti@pollux.usc.edu",
   "nsf_id": "000487896",
   "pi_start_date": "2015-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S. Flower St.",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900890001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "CA37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "8235",
   "pgm_ref_txt": "CPS-Synergy"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 240000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we have focused on developing a new software and hardware framework to record video, vehicle dynamics, eye movements, and limb/body movements behavior, new algorithms for visual attention, new algorithms for time-series analysis (including data streamed from inertial sensors), and new algorithms and datasets for visual object recognition in images and for simultaneous localization and mapping of (SLAM) of a vehicle using scanning lasers (LIDAR) sensors.&nbsp;</p>\n<p>Together, these achievements have established the basis for our main hypothesis: eye movements and motions of a person provide a window onto their mind, intentions, and cognitive state. We have shown that one can use the same principles to develop an early fault detection for machines, by monitoring the subtle vibrations a machine makes while it operates. We have shown, using a 3D printer as test machine, how one can detect different classes of faults (loose belt, bent guide rods, etc) from inexpensive accelerometers placed on the machine.</p>\n<p>Overall, our new discoveries from this project are, we believe, far reaching and provide deep new insights for the design of intelligent machines and vehicles. In particular, our research combines behavioral measurements from human or machine participants with neuro-inspired computational modeling. In addition, we have extended the traditional experiment paradigm of measuring eye movements to using inertial sensors as a potential new source of data. We have shown in several applications that indeed inertial sensors can be used to decode some of the internal functioning of an individual or of a machine. Finally, our work has been recognized and published at top venues, including Nature Communications, PNAS, IEEE CVPR, ICML, WACV, IEEE TIP, and more. We are very grateful for the support of the National Science Foundation.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/15/2018<br>\n\t\t\t\t\tModified by: Laurent&nbsp;Itti</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this project, we have focused on developing a new software and hardware framework to record video, vehicle dynamics, eye movements, and limb/body movements behavior, new algorithms for visual attention, new algorithms for time-series analysis (including data streamed from inertial sensors), and new algorithms and datasets for visual object recognition in images and for simultaneous localization and mapping of (SLAM) of a vehicle using scanning lasers (LIDAR) sensors. \n\nTogether, these achievements have established the basis for our main hypothesis: eye movements and motions of a person provide a window onto their mind, intentions, and cognitive state. We have shown that one can use the same principles to develop an early fault detection for machines, by monitoring the subtle vibrations a machine makes while it operates. We have shown, using a 3D printer as test machine, how one can detect different classes of faults (loose belt, bent guide rods, etc) from inexpensive accelerometers placed on the machine.\n\nOverall, our new discoveries from this project are, we believe, far reaching and provide deep new insights for the design of intelligent machines and vehicles. In particular, our research combines behavioral measurements from human or machine participants with neuro-inspired computational modeling. In addition, we have extended the traditional experiment paradigm of measuring eye movements to using inertial sensors as a potential new source of data. We have shown in several applications that indeed inertial sensors can be used to decode some of the internal functioning of an individual or of a machine. Finally, our work has been recognized and published at top venues, including Nature Communications, PNAS, IEEE CVPR, ICML, WACV, IEEE TIP, and more. We are very grateful for the support of the National Science Foundation.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/15/2018\n\n\t\t\t\t\tSubmitted by: Laurent Itti"
 }
}