{
 "awd_id": "1453430",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Enabling Distributed and In-Situ Analysis for Multidimensional Structured Data",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Alan Sussman",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 412969.0,
 "awd_amount": 412969.0,
 "awd_min_amd_letter_date": "2015-03-03",
 "awd_max_amd_letter_date": "2015-03-03",
 "awd_abstract_narration": "Advances in modern science have led to explosions of data across all science, technology, engineering, and mathematics (STEM) disciplines. Extracting meaningful knowledge from this large pool of information has become both complicated and costly. In fields like genomics and astronomy, where very large volumes of data are produced daily, it is necessary to store repositories throughout multiple, geographically distinct locations. This type of data allocation results in expensive computations and incomplete analyses. For health informatics and finances, data is typically isolated between research centers due to privacy, security, or cost issues. Again, the inability to have a global view of the data yields inaccurate outcomes at computation time. The classic centralized approach to analyzing data no longer produces optimal results; it has become a major bottleneck, hindering the advantages Big Data has to offer. Current solutions for distributed analysis still lack generality, scalability, or accuracy.  \r\n\r\nThis project aims to ameliorate problems in the management of distributed data while enabling scalable and accurate analyses.  The project provides a comprehensive approach to handle data-to-knowledge extraction, representation, and learning at scale. Products of this research include: (1) an algorithmic suite of semantic projections and scalable learning methods for efficient data dimensionality reduction, pattern recognition, anomaly detection, and clustering, and (2) an open source middleware for coupling distributed data acquisition processes with in-situ analytics and crowd sourcing. These products will be made available through a GitHub repository at https://github.com/distributedreasoningatunm.  Moreover, the crowd sourcing extension doubles as an educational platform, which aims to attract interest in the STEM fields.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Trilce",
   "pi_last_name": "Estrada",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Trilce Estrada",
   "pi_email_addr": "estrada@cs.unm.edu",
   "nsf_id": "000656205",
   "pi_start_date": "2015-03-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of New Mexico",
  "inst_street_address": "1 UNIVERSITY OF NEW MEXICO",
  "inst_street_address_2": "",
  "inst_city_name": "ALBUQUERQUE",
  "inst_state_code": "NM",
  "inst_state_name": "New Mexico",
  "inst_phone_num": "5052774186",
  "inst_zip_code": "871310001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NM01",
  "org_lgl_bus_name": "UNIVERSITY OF NEW MEXICO",
  "org_prnt_uei_num": "",
  "org_uei_num": "F6XLTRUQJEN4"
 },
 "perf_inst": {
  "perf_inst_name": "University of New Mexico",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NM",
  "perf_st_name": "New Mexico",
  "perf_zip_code": "871310001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NM01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "104500",
   "pgm_ele_name": "CAREER: FACULTY EARLY CAR DEV"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 412969.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-60a80a57-7fff-e6b1-1cba-c5611f773245\"> </span></p>\n<p dir=\"ltr\"><span>Within the last decade, it became possible to accelerate scientific simulations by orders of magnitude. Information processing is now computationally cheap, and it promotes the generation of exorbitant amounts of data. With this deluge, new research questions arise at different stages of the data management pipeline: When and where should analysis be performed? How can these analysis be used to provide insight at runtime? How do we expose insightful information from complex data sources? With these questions in mind, this work focuses on the challenges of scalable distributed learning: how do we build lightweight and effective models when data is distributed over multiple locations, when data arrives at such high rate that cannot be stored and processed later, and how do we express complex information in a way that is effective for this type of analysis.&nbsp;</span></p>\n<p dir=\"ltr\"><strong>Intellectual merit. </strong><span>Modern data acquisition techniques, the need for high performance analysis, and constraints regarding data management are slowly rendering centralized or offline data analysis obsolete. In domains like climate simulations, high-energy physics, astronomy, and remote sensing, data transfer represents a major bottleneck. In medical or financial domains, policies regarding security and privacy impede data openness, even when volume is not a concern. Our work through this grant consisted of developing novel algorithms for scalable analysis of very large dimensional data. Each of our contributions tackled an important scalability problem: distributed data (i.e, the algorithm has only a partial view of the problem), in-situ analysis (i.e., very lightweight analysis that can be coupled with other heavy computations and is performed on the fly), and stream analysis (data arrives at high rates and every data point needs to be processed only once). For example, our distributed clustering method, KeyBin2, scales horizontally, that is, every data sample can be processed in parallel without requiring any direct knowledge of other data samples, and to a certain extent, it scales vertically, every feature, or dimension, can be processed independently.&nbsp; Another example is PASCAL-G a probabilistic method to detect community formation in graphs (e.g., social networks), which works on streams. PASCAL-G is able to extract structure from large distributed networks in a single pass and can be used to determine how communities evolve over time. We successfully implemented these and other approaches on heterogeneous HPC platforms, and applied them to in-situ analysis of protein folding trajectories, real-time social community formation during hurricanes, vaccination networks, and medical image analysis. The long term goal of this research is to provide a foundation for the next generation of very high throughput analysis of biomedical, social, and scientific applications.</span><span>&nbsp;</span></p>\n<p dir=\"ltr\"><strong>Broader impact. </strong><span>The project has released open source algorithms, datasets, and middleware for the free use of the research community. The applications that motivated our different algorithms are socially and scientifically relevant. In addition, we released a middleware prototype for a new learning paradigm that we call crowdlearning. Where students are upgraded from passive content consumers to primary content creators and curators. Crowdlearning is all about allowing learners to share their particular vision of the world, understanding how different users learn, and putting in place mechanisms to safely, scalable, and accurately share and consume learning materials online. The grant supported two undergraduate students, four master graduates and one PhD graduate. The UNM course CS 467/567: Principles and Applications of Big Data uses extensive materials derived from this project, including an in-class Kaggle competition.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/29/2021<br>\n\t\t\t\t\tModified by: Trilce&nbsp;Estrada-Piedra</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nWithin the last decade, it became possible to accelerate scientific simulations by orders of magnitude. Information processing is now computationally cheap, and it promotes the generation of exorbitant amounts of data. With this deluge, new research questions arise at different stages of the data management pipeline: When and where should analysis be performed? How can these analysis be used to provide insight at runtime? How do we expose insightful information from complex data sources? With these questions in mind, this work focuses on the challenges of scalable distributed learning: how do we build lightweight and effective models when data is distributed over multiple locations, when data arrives at such high rate that cannot be stored and processed later, and how do we express complex information in a way that is effective for this type of analysis. \nIntellectual merit. Modern data acquisition techniques, the need for high performance analysis, and constraints regarding data management are slowly rendering centralized or offline data analysis obsolete. In domains like climate simulations, high-energy physics, astronomy, and remote sensing, data transfer represents a major bottleneck. In medical or financial domains, policies regarding security and privacy impede data openness, even when volume is not a concern. Our work through this grant consisted of developing novel algorithms for scalable analysis of very large dimensional data. Each of our contributions tackled an important scalability problem: distributed data (i.e, the algorithm has only a partial view of the problem), in-situ analysis (i.e., very lightweight analysis that can be coupled with other heavy computations and is performed on the fly), and stream analysis (data arrives at high rates and every data point needs to be processed only once). For example, our distributed clustering method, KeyBin2, scales horizontally, that is, every data sample can be processed in parallel without requiring any direct knowledge of other data samples, and to a certain extent, it scales vertically, every feature, or dimension, can be processed independently.  Another example is PASCAL-G a probabilistic method to detect community formation in graphs (e.g., social networks), which works on streams. PASCAL-G is able to extract structure from large distributed networks in a single pass and can be used to determine how communities evolve over time. We successfully implemented these and other approaches on heterogeneous HPC platforms, and applied them to in-situ analysis of protein folding trajectories, real-time social community formation during hurricanes, vaccination networks, and medical image analysis. The long term goal of this research is to provide a foundation for the next generation of very high throughput analysis of biomedical, social, and scientific applications. \nBroader impact. The project has released open source algorithms, datasets, and middleware for the free use of the research community. The applications that motivated our different algorithms are socially and scientifically relevant. In addition, we released a middleware prototype for a new learning paradigm that we call crowdlearning. Where students are upgraded from passive content consumers to primary content creators and curators. Crowdlearning is all about allowing learners to share their particular vision of the world, understanding how different users learn, and putting in place mechanisms to safely, scalable, and accurately share and consume learning materials online. The grant supported two undergraduate students, four master graduates and one PhD graduate. The UNM course CS 467/567: Principles and Applications of Big Data uses extensive materials derived from this project, including an in-class Kaggle competition.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/29/2021\n\n\t\t\t\t\tSubmitted by: Trilce Estrada-Piedra"
 }
}