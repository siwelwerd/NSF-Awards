{
 "awd_id": "1456822",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Visual Memory Mechanisms in Transsaccadic Integration and Overt Search",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Michael Hout",
 "awd_eff_date": "2015-03-15",
 "awd_exp_date": "2020-02-29",
 "tot_intn_awd_amt": 424834.0,
 "awd_amount": 424834.0,
 "awd_min_amd_letter_date": "2015-02-27",
 "awd_max_amd_letter_date": "2015-02-27",
 "awd_abstract_narration": "A fundamental question in vision science concerns how people perceive a continuous visual environment before them when visual information enters the visual system through a series of brief glances interrupted by frequent rapid eye movements. Part of the answer may be that the visual system relies on a form of visual memory to allow for continuity between glances. In the present work, the research team will investigate how this type of visual memory operates. Achieving a better understanding of the basic operation of visual memory across eye movements could potentially lead to practical applications, such as optimized procedures for radiologists, baggage screeners, satellite image analysts, and others whose occupations require them to search for critical pieces of visual information during a visual search process. Another potential application could be optimized design of artificial vision systems.  \r\n\r\nThe proposed work aims to answer two basic questions regarding the operation of visual memory between glances. First, the research aims to investigate the capacity of such visual memory: How much visual information can this type of memory hold? Second, the research also aims to investigate how the capacity limitations of the visual memory mechanism constrain human performance in visual search tasks. For example, when the capacity of the visual memory is exceeded, how does this impact a person's ability to search for a specific piece of visual information that may be hidden among lots of other visual items? A set of rigorous computational and experimental techniques will be used to characterize the information contained within visual memory during visual search as well as to assess the impact of visual memory on visual search performance. The techniques will allow for detailed and novel quantitative predictions about how visual sensitivity and visual memory capacity interact to determine human performance in visual search tasks. Model predictions will be tested through psychophysical experiments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Melchi",
   "pi_last_name": "Michel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Melchi Michel",
   "pi_email_addr": "melchi.michel@rutgers.edu",
   "nsf_id": "000649480",
   "pi_start_date": "2015-02-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 424834.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Natural visual tasks, such as visual search, typically involve at least several saccadic eye movements, with visual information collected during the intervening fixations. Integrating this visual information across eye movements requires memory (called <em>trans-saccadic </em>memory). However, very little is known about this trans-saccadic memory and how it constrains visual search performance. In the projects described below, we have started to answer these questions. In particular, we measure the limited capacity of trans-saccadic memory, we show that this limited capacity can substantially degrade search performance, and we explore how search performance is impacted by various alternative memory allocation strategies.</p>\n<p>The primary goal of the current project was to investigate this problem. This work had four aims: (1) to derive normative (ideal observer) models of memory-limited visual integration in overt seach, (2) to use these models, along with human psychophysics to estimate trans-saccadic memory capacity, (3) to determine how this measured trans-saccadic memory capacity limits visual search performance, and (4) to characterize the specific memory allocation strategies used by human observers in trans-saccadic integration.&nbsp; Over the term of the project, we have made considerable progress on these aims. In particular, we derived a memory-limited ideal searcher and devised a pair of tasks that allowed us to estimate the capacities of both visual short-term memory (VSTM) and trans-saccadic memory (TSM) in task-independent units (bits). Our results suggest that TSM plays an important role in visual search tasks, that the effective capacity of TSM is greater than that of VSTM (a novel result), and that the TSM capacity of human observers significantly limits performance in multiple-fixation visual search tasks (Kleene &amp; Michel, 2018).&nbsp;</p>\n<p>We have also been exploring both normative (i.e., what people ought to do) versus actual human dynamic memory allocation strategies. Using the memory-limited ideal searcher, we showed how the optimal memory allocation strategy changes systematically as a function of available capacity, with <em>equitable</em> allocation strategies (in which available capacity is allocated equally among potential targets) becoming less optimal and <em>foreclosing</em> allocation strategies (in which available capacity is only allocated to the most likely target) becoming more optimal as capacity decreases (Kleene &amp; Michel, 2018). In other experiments, we used temporal reverse correlation analyses to determine how human observers actually allocate TSM across time. Our results suggest that observers exhibit a <em>recency</em> effect, allocating more capacity to later than to earlier information, and that the magnitude of this recency effect increases with increasing memory load (Bittner &amp; Michel, 2018).&nbsp;</p>\n<p>In addition to the primary goal (of characterizing trans-saccadic memory) outlined above, this project also investigated closely related questions regarding how visual information is integrated across fixations and in visual search more broadly.</p>\n<p>In one of these investigations (Wilmott &amp; Michel, 2021) we used a reverse-correlation technique to characterize how the perceptual computations made in the interval preceding an eye movement differs from those made during stable fixation. &nbsp;Our results reveal a pattern of spatial integration in the pre-saccadic interval that is consistent with updating based on the anticipated direction and magnitude of the impending eye movement. We found that this updating is attention-linked, spatially precise, and can occur simultaneously for multiple, spatially distinct attended objects.</p>\n<p>In another set of experiments (Semizer &amp; Michel, 2017; 2019; Semizer, Michel, Evans, &amp; Wolfe, 2018), we investigated how various types of uncertainty limit human performance in visual search tasks. Our major finding in these experiments was that, across a variety of search tasks, including search for specific or categorical targets in synthetic or natural scenes, intrinsic position uncertainty (i.e., uncertainty about object positions in the visual periphery) interacts with visual clutter to degrade search performance in a manner that is independent of the search set size (e.g., the number or area of locations where the search target can appear).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/04/2022<br>\n\t\t\t\t\tModified by: Melchi&nbsp;Michel</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nNatural visual tasks, such as visual search, typically involve at least several saccadic eye movements, with visual information collected during the intervening fixations. Integrating this visual information across eye movements requires memory (called trans-saccadic memory). However, very little is known about this trans-saccadic memory and how it constrains visual search performance. In the projects described below, we have started to answer these questions. In particular, we measure the limited capacity of trans-saccadic memory, we show that this limited capacity can substantially degrade search performance, and we explore how search performance is impacted by various alternative memory allocation strategies.\n\nThe primary goal of the current project was to investigate this problem. This work had four aims: (1) to derive normative (ideal observer) models of memory-limited visual integration in overt seach, (2) to use these models, along with human psychophysics to estimate trans-saccadic memory capacity, (3) to determine how this measured trans-saccadic memory capacity limits visual search performance, and (4) to characterize the specific memory allocation strategies used by human observers in trans-saccadic integration.  Over the term of the project, we have made considerable progress on these aims. In particular, we derived a memory-limited ideal searcher and devised a pair of tasks that allowed us to estimate the capacities of both visual short-term memory (VSTM) and trans-saccadic memory (TSM) in task-independent units (bits). Our results suggest that TSM plays an important role in visual search tasks, that the effective capacity of TSM is greater than that of VSTM (a novel result), and that the TSM capacity of human observers significantly limits performance in multiple-fixation visual search tasks (Kleene &amp; Michel, 2018). \n\nWe have also been exploring both normative (i.e., what people ought to do) versus actual human dynamic memory allocation strategies. Using the memory-limited ideal searcher, we showed how the optimal memory allocation strategy changes systematically as a function of available capacity, with equitable allocation strategies (in which available capacity is allocated equally among potential targets) becoming less optimal and foreclosing allocation strategies (in which available capacity is only allocated to the most likely target) becoming more optimal as capacity decreases (Kleene &amp; Michel, 2018). In other experiments, we used temporal reverse correlation analyses to determine how human observers actually allocate TSM across time. Our results suggest that observers exhibit a recency effect, allocating more capacity to later than to earlier information, and that the magnitude of this recency effect increases with increasing memory load (Bittner &amp; Michel, 2018). \n\nIn addition to the primary goal (of characterizing trans-saccadic memory) outlined above, this project also investigated closely related questions regarding how visual information is integrated across fixations and in visual search more broadly.\n\nIn one of these investigations (Wilmott &amp; Michel, 2021) we used a reverse-correlation technique to characterize how the perceptual computations made in the interval preceding an eye movement differs from those made during stable fixation.  Our results reveal a pattern of spatial integration in the pre-saccadic interval that is consistent with updating based on the anticipated direction and magnitude of the impending eye movement. We found that this updating is attention-linked, spatially precise, and can occur simultaneously for multiple, spatially distinct attended objects.\n\nIn another set of experiments (Semizer &amp; Michel, 2017; 2019; Semizer, Michel, Evans, &amp; Wolfe, 2018), we investigated how various types of uncertainty limit human performance in visual search tasks. Our major finding in these experiments was that, across a variety of search tasks, including search for specific or categorical targets in synthetic or natural scenes, intrinsic position uncertainty (i.e., uncertainty about object positions in the visual periphery) interacts with visual clutter to degrade search performance in a manner that is independent of the search set size (e.g., the number or area of locations where the search target can appear).\n\n\t\t\t\t\tLast Modified: 02/04/2022\n\n\t\t\t\t\tSubmitted by: Melchi Michel"
 }
}