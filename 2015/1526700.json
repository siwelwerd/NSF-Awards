{
 "awd_id": "1526700",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Increasing the Value of Existing Web Archives",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 481780.0,
 "awd_amount": 481780.0,
 "awd_min_amd_letter_date": "2015-08-19",
 "awd_max_amd_letter_date": "2015-08-19",
 "awd_abstract_narration": "Web archiving is a thriving activity, but remains at the fringes of the larger web community.  Web archiving often runs into two opinions: (1) who cares about the past?  and (2) hasn't the Internet Archive solved this already?  While the Internet Archive is the cornerstone of web archiving, there remains much work to be done to align archiving with the larger web community.  The PIs will investigate a collection of methods and concepts to accelerate the adoption and utility of web archives.  While there are more than a dozen publicly available web archives (including the Internet Archive) that are simultaneously accessible via the Memento protocol, these archives are mostly underutilized because they lack the APIs and services to be of greater immediate use to the live web.  For example, rather than returning \"HTTP 404\" responses for pages that are not archived, the archives can introspect on their collections for replacement or similar pages.  This project will research: (1) extended APIs for archives; (2) models and methods for archival quality; and (3) user tools and techniques for exploring and understanding temporality on the web.  The broader impacts of this research will include increasing the ability of archives to record today's social discourse, which primarily occurs on the web, oftentimes with print or TV as secondary. The ability to publish data on the web far outstrips the ability to archive it for posterity. There are a number of public web archives that are doing yeoman's work saving as much material as they can, but saving is only a precondition for later use. Mostly these archived web pages are being underutilized, only because the tools for extracting the value from these archives are lacking. This project will research and build the tools, infrastructure, and methods to better utilize, understand, and interact with the archived materials that we already have.\r\n\r\nAside from their crawling, archives are mostly passive collections of content that offer little in the way of services other than answering \"yes\" or \"no\" to a request for an archived page. Even with the increased rate of archiving (and a greater number of active web archives), there is little analysis on the web archives to provide better services for incoming requests.  The PIs will build on their prior API work to explore recommendation services for web pages, where even if an archive does not have the requested web page it can make recommendations for a replacement page based on content and link analysis.  This will prevent the web archives from being a dead end if they do not have the requested page.  The PIs will also perform fundamental research on the issue of the quality of the reconstructed page, a topic that has been mostly ignored. In particular the PIs are concerned with detecting and resolving \"temporal violations,\" combinations of HTML pages with embedded resources that are presented to the user as an historical page but in fact they never existed in that combination on the live web. This occurs in at least 5% of the pages replayed through the Internet Archive. The other aspect of quality research deals with automatically assessing how damaged an archived page is with respect to its missing embedded resources.  Straight percentages (e.g., this page is missing 3 of 57 embedded resources) do not tell the whole tale, but there are automated methods that can be used to estimate how important the resource was (even though you do not have it) to the rendered page. This will allow large-scale assessment not only of pages, but of archive-wide performance for comparable time periods. Lastly, the PIs will focus on tools and methods for allowing users to better understand and interact with the archived web and temporal concepts in general.  Users' understanding of temporal concepts is not well advanced, in part because the tools are not in place to allow them to better understand and build models for interaction. For further information see the web site at: http://ws-dl.cs.odu.edu/.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Nelson",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Nelson",
   "pi_email_addr": "mln@cs.odu.edu",
   "nsf_id": "000245189",
   "pi_start_date": "2015-08-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michele",
   "pi_last_name": "Weigle",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Michele C Weigle",
   "pi_email_addr": "mweigle@cs.odu.edu",
   "nsf_id": "000357685",
   "pi_start_date": "2015-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Old Dominion University Research Foundation",
  "inst_street_address": "4111 MONARCH WAY STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "NORFOLK",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "7576834293",
  "inst_zip_code": "235082561",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "VA03",
  "org_lgl_bus_name": "OLD DOMINION UNIVERSITY RESEARCH FOUNDATION",
  "org_prnt_uei_num": "DSLXBD7UWRV6",
  "org_uei_num": "DSLXBD7UWRV6"
 },
 "perf_inst": {
  "perf_inst_name": "Old Dominion University",
  "perf_str_addr": "Hampton Boulevard",
  "perf_city_name": "Norfolk",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "235290001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "VA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 481780.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-29344799-7fff-7a69-2eca-d2c85fa578a4\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Project Outcomes for \"Increasing the Value of Existing Web Archives\"</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">For \"intellectual merit\" we demonstrated:</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Extending the APIs of web archives (Task 1):</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* How archives could enhance the quality of their response by not just sending 404 \"not found\" responses, but by also recommending similar pages they do have archived. Traffic to web archives is driven largely by existence on the live web, so archived pages that no longer exist on the live web will be less likely to be discovered, even if they are a suitable replacement for the user's query.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* How private and public web archives can be combined in a single, cascading framework. The Internet Archive is a tremendous resource and archives many of the pages we seek to replay, but as much as two-thirds of all web traffic is not archivable by the Internet Archive and other web archives, so we much create private web archives so we can save our bank statements, facebook posts, and other private web resources.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Understanding archival quality (Task 2):</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* We delivered a suite of tools, libraries, and other code that has directly advanced the state of web archiving.&nbsp; For example, cnn.com has not be replayable from the Internet Archive since November 1, 2016, but through our software libraries we have enabled the successful replay of those pages (they were archived, just not replayable).&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* We have demonstrated that blockchain and other conventional fixity techniques are not suitable for third party audit of replayed web pages. We are developing an \"archiving-aware\" hashing function that will be flexible with well-known issues regarding replay of archived pages while still detecting tampering (without a corresponding large number of false positives). We have also investigated different methods for including fixity information about archived web pages in web archives themselves, and not in separate infrastructure such as blockchain.&nbsp;&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Interacting with temporality (Task 3):</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* We explored different methods and techniques for archival \"banners\" that are often displayed during the replay of archived web pages. Currently the banners are inserted directly into the HTML by the archives themselves, but this can lead to security problems where JavaScript can either \"attack\" a legitimate archival banner or impersonate an archival banner with \"fake\" metadata (for example, for which live web page this is an archived version of, as well as when the archiving took place).&nbsp; By generating having client-side generation of archival banners based, we have finer grained control, more security, and can accomodate replay which draws from multiple archives (cf. \"private vs. public web archives\" in Task 1).</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* We developed \"MementoMap\", a method for compact summarization of the holdings of different web archives. Web archives are like basements, attics, and garages: over time they fill up, but even those responsible for filling them can no longer remember exactly what, how, and when resources were collected. MementoMap facilitates many applications, such as routing of URL lookups (e.g., \"is this archive likely to have this URL? if not, don't bother asking it\") and visualizing and comparing the holdings and collection development policies of web archives (e.g., archiving two or three pages of nasa.gov once a year is not the same as archiving all of it on a continuous basis; it's not sufficient to say \"we both archive nasa.gov\", and MementoMap provides a method for describing the two archives' depth and frequency of coverage).&nbsp;&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">For \"broader impacts\":</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* We made archives more immediately useful, by demonstrating and evaluating how archives can quickly recommend pages they do contain even when they don't have the requested page.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* Made archives more personally accessible, by using the same tools and techniques that the large, public web archives (e.g., Internet Archive) use and integrating them with private web archives. Since the majority of our web traffic is private (e.g., requires a personal login that you will never share with the Internet Archive), we must make web archives suitable for archiving private pages.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* Investigated how public trust in web archives can be increased, from being the first to document well-known limitations of replaying archived twitter.com pages, to better UIs for explaining the provenance of archived web pages, to quantifying why conventional hashing techniques (such as those used by current blockchain-based solutions) are not suitable for third party audit of replayed web pages.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* We have also greatly expanded the number of experts in web archiving entering the workforce with graduate degrees.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">All software, data, slides, publications, etc. from this and all projects from the ODU Web Science and Digital Libraries Group can be found on our:</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* Twitter: https://twitter.com/WebSciDL</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* Blog: https://ws-dl.blogspot.com/</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">* GitHub: https://github.com/oduwsdl/</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/06/2020<br>\n\t\t\t\t\tModified by: Michael&nbsp;L&nbsp;Nelson</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578349828253_nsf-outcomes-2002-5--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578349828253_nsf-outcomes-2002-5--rgov-800width.jpg\" title=\"Integrating private and public web archives.\"><img src=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578349828253_nsf-outcomes-2002-5--rgov-66x44.jpg\" alt=\"Integrating private and public web archives.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Given the proliferation of web archives, we must be able to integrate them in a meaningful manner. In this image, Alice's request for an archived page goes to her archive plus Bob's and Carol's personal archives, and only if all 3 archives return a 404 does the request go to the Internet Archive.</div>\n<div class=\"imageCredit\">Mat Kelly</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Nelson</div>\n<div class=\"imageTitle\">Integrating private and public web archives.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578351200437_nsf-outcomes-2020-2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578351200437_nsf-outcomes-2020-2--rgov-800width.jpg\" title=\"The discrepancy between archived and accessed resources in Arquivo.pt.\"><img src=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578351200437_nsf-outcomes-2020-2--rgov-66x44.jpg\" alt=\"The discrepancy between archived and accessed resources in Arquivo.pt.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">In bins of power of 10, the x-axis show the frequency of archive request (e.g., 3.2M pages are requested 1-9 times, 19k pages requested 10-99 times) vs. the archives actual holdings on the y-axis (e.g., 2B pages archived just once). What archives archive and what people request are different.</div>\n<div class=\"imageCredit\">Sawood Alam</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Nelson</div>\n<div class=\"imageTitle\">The discrepancy between archived and accessed resources in Arquivo.pt.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578351273286_nsf-outcomes-2020-3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578351273286_nsf-outcomes-2020-3--rgov-800width.jpg\" title=\"Use web archives to store fixity information about archived web pages.\"><img src=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578351273286_nsf-outcomes-2020-3--rgov-66x44.jpg\" alt=\"Use web archives to store fixity information about archived web pages.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Since there is a proliferation of independent web archives, we can use these untrusted web archives (assuming they're independent) to store the fixity information of pages in other web archives. We do not need to separate systems such as blockchain.</div>\n<div class=\"imageCredit\">Mohamed Aturban</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Nelson</div>\n<div class=\"imageTitle\">Use web archives to store fixity information about archived web pages.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578351538080_nsf-outcomes-2020-1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578351538080_nsf-outcomes-2020-1--rgov-800width.jpg\" title=\"Replayed twitter.com from web archives is problematic\"><img src=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578351538080_nsf-outcomes-2020-1--rgov-66x44.jpg\" alt=\"Replayed twitter.com from web archives is problematic\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Cookies and JavaScript can combine to replay Twitter pages from the web archive which never existed. The UI/replay that does not match people's expectation is a problem that undermines people's trust in web archives.</div>\n<div class=\"imageCredit\">Sawood Alam</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Nelson</div>\n<div class=\"imageTitle\">Replayed twitter.com from web archives is problematic</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578352290240_nsf-outcomes-2020-4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578352290240_nsf-outcomes-2020-4--rgov-800width.jpg\" title=\"How much of an archived page do you hash: just the HTML, or all of the embedded resources?\"><img src=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578352290240_nsf-outcomes-2020-4--rgov-66x44.jpg\" alt=\"How much of an archived page do you hash: just the HTML, or all of the embedded resources?\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Modern web pages typically have 100+ embedded resources. If we monitor (hash) just the archived HTML page, the images, stylesheets, javascript, etc. can be modified without detection. However, if we hash all those resources we're likely to get false positives about changes.</div>\n<div class=\"imageCredit\">Mohamed Aturban</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Nelson</div>\n<div class=\"imageTitle\">How much of an archived page do you hash: just the HTML, or all of the embedded resources?</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578349734848_nsf-outcomes-2020-6--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578349734848_nsf-outcomes-2020-6--rgov-800width.jpg\" title=\"Instead of sending a 404, the archive recommends a similar page.\"><img src=\"/por/images/Reports/POR/2020/1526700/1526700_10389368_1578349734848_nsf-outcomes-2020-6--rgov-66x44.jpg\" alt=\"Instead of sending a 404, the archive recommends a similar page.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Instead of the archive sending a 404 when the user requests a page it does not have archived, the archive recommends similar pages based on analyzing the URL only. In this case tripadvisor.com/ where_to_travel is 404, but archive has travelassist.com/mag/mag_home.html</div>\n<div class=\"imageCredit\">Lulwah Alwkai</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Nelson</div>\n<div class=\"imageTitle\">Instead of sending a 404, the archive recommends a similar page.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "Project Outcomes for \"Increasing the Value of Existing Web Archives\"\n\n \nFor \"intellectual merit\" we demonstrated:\n\n \nExtending the APIs of web archives (Task 1):\n\n \n* How archives could enhance the quality of their response by not just sending 404 \"not found\" responses, but by also recommending similar pages they do have archived. Traffic to web archives is driven largely by existence on the live web, so archived pages that no longer exist on the live web will be less likely to be discovered, even if they are a suitable replacement for the user's query. \n\n \n* How private and public web archives can be combined in a single, cascading framework. The Internet Archive is a tremendous resource and archives many of the pages we seek to replay, but as much as two-thirds of all web traffic is not archivable by the Internet Archive and other web archives, so we much create private web archives so we can save our bank statements, facebook posts, and other private web resources.\n\n \nUnderstanding archival quality (Task 2):\n\n \n* We delivered a suite of tools, libraries, and other code that has directly advanced the state of web archiving.  For example, cnn.com has not be replayable from the Internet Archive since November 1, 2016, but through our software libraries we have enabled the successful replay of those pages (they were archived, just not replayable). \n\n \n* We have demonstrated that blockchain and other conventional fixity techniques are not suitable for third party audit of replayed web pages. We are developing an \"archiving-aware\" hashing function that will be flexible with well-known issues regarding replay of archived pages while still detecting tampering (without a corresponding large number of false positives). We have also investigated different methods for including fixity information about archived web pages in web archives themselves, and not in separate infrastructure such as blockchain.  \n\n \nInteracting with temporality (Task 3):\n\n \n* We explored different methods and techniques for archival \"banners\" that are often displayed during the replay of archived web pages. Currently the banners are inserted directly into the HTML by the archives themselves, but this can lead to security problems where JavaScript can either \"attack\" a legitimate archival banner or impersonate an archival banner with \"fake\" metadata (for example, for which live web page this is an archived version of, as well as when the archiving took place).  By generating having client-side generation of archival banners based, we have finer grained control, more security, and can accomodate replay which draws from multiple archives (cf. \"private vs. public web archives\" in Task 1).\n\n \n* We developed \"MementoMap\", a method for compact summarization of the holdings of different web archives. Web archives are like basements, attics, and garages: over time they fill up, but even those responsible for filling them can no longer remember exactly what, how, and when resources were collected. MementoMap facilitates many applications, such as routing of URL lookups (e.g., \"is this archive likely to have this URL? if not, don't bother asking it\") and visualizing and comparing the holdings and collection development policies of web archives (e.g., archiving two or three pages of nasa.gov once a year is not the same as archiving all of it on a continuous basis; it's not sufficient to say \"we both archive nasa.gov\", and MementoMap provides a method for describing the two archives' depth and frequency of coverage).  \n\n \nFor \"broader impacts\":\n\n \n* We made archives more immediately useful, by demonstrating and evaluating how archives can quickly recommend pages they do contain even when they don't have the requested page.\n\n \n* Made archives more personally accessible, by using the same tools and techniques that the large, public web archives (e.g., Internet Archive) use and integrating them with private web archives. Since the majority of our web traffic is private (e.g., requires a personal login that you will never share with the Internet Archive), we must make web archives suitable for archiving private pages.\n\n \n* Investigated how public trust in web archives can be increased, from being the first to document well-known limitations of replaying archived twitter.com pages, to better UIs for explaining the provenance of archived web pages, to quantifying why conventional hashing techniques (such as those used by current blockchain-based solutions) are not suitable for third party audit of replayed web pages. \n\n \n* We have also greatly expanded the number of experts in web archiving entering the workforce with graduate degrees.\n\n \nAll software, data, slides, publications, etc. from this and all projects from the ODU Web Science and Digital Libraries Group can be found on our:\n\n \n* Twitter: https://twitter.com/WebSciDL\n* Blog: https://ws-dl.blogspot.com/\n* GitHub: https://github.com/oduwsdl/\n\n\t\t\t\t\tLast Modified: 01/06/2020\n\n\t\t\t\t\tSubmitted by: Michael L Nelson"
 }
}