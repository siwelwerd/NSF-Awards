{
 "awd_id": "1508880",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Recurrent Deep Learning Machines for Robust, Adaptive, or Accommodative Filtering",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Aranya Chakrabortty",
 "awd_eff_date": "2015-07-15",
 "awd_exp_date": "2020-06-30",
 "tot_intn_awd_amt": 340736.0,
 "awd_amount": 340736.0,
 "awd_min_amd_letter_date": "2015-06-29",
 "awd_max_amd_letter_date": "2015-06-29",
 "awd_abstract_narration": "Prediction and estimation of a process, called a signal process, given a relevant process, called a measurement process, both of which usually involve randomness, is a fundamental problem in a broad range of fields. As the signal evolves and measurements keep coming in, an algorithm is needed to predict or estimate the signal at each time instant using the measurement at the same instant to update the prediction or estimate without requiring the use of the preceding  measurements. Such an algorithm is called a filter. When the signal or measurement process is affected by an uncertain or changing environment, a filter that adapts to the environment is called an adaptive filter. In many applications, whether an uncertain or changing environment is involved, large individual errors in estimation or prediction may cause undesirable or even disastrous consequences and are to be avoided. A filter that can reduce large errors is called a robust filter. A robust filter must balance filtering accuracy and robustness. Optimal Filtering for nonlinear signal or measurement processes was a long-standing notorious problem until neural filters were proposed in 1992. Although neural filtering has many advantages over its main competitor, the particle filter, the local-minimum problem in training neural filters plagued the approach until now. The local-minimum problem has finally been overcome by a technique called the gradual deconvexification method developed under a recent NSF grant. Neural filters are now ready for application. The purpose of this project is to develop adaptive and robust neural filters.\r\n\r\nIn particular, the following filters will be developed:\r\n\r\n(1)\tAccommodative neural filters. Properly trained RNNs (recurrent neural networks) with fixed weights are proven to have adaptive ability and are called accommodative neural networks. They are not adjusted online. This is an important advantage because the signal process is usually unavailable online for weight adjustment. An adaptive filter that is an accommodative neural network is called an accommodative neural filter.\r\n\r\n(2)\tAdaptive neural filters with long- and short-term memories. If the nonlinear and linear weights of an RNN, which affect the RNN's outputs in a nonlinear and linear manner respectively, are used as long- and short-term memories (LASTMs) respectively, it has been proven that the long-term memory can be trained offline for different environments and only the short-term memory needs to be adjusted online to adapt to the environment. An adaptive neural filter that has LASTMs is called an adaptive neural filter (with LASTMs). Such filters are expected to have better generalization capability than accommodative neural filters.\r\n\r\n(3)\tRobust neural filters. The risk-sensitivity index in the normalized risk-sensitive error (NRSE) criterion for training a neural network determines its degree of robustness. Depending on whether; being positive or negative, the NRSE averts larger \"risks\" or ignores \"outliers\" to induce robust engineering or robust statistical performance respectively. Existence of robust neural filters has been proven. It is also proven that as the risk-sensitivity index grows without bound, the NRSE approaches the minimax criterion.\r\n\r\n(4)\tRobust accommodative neural filters. If both adaptive and robust performances are required of a filter and online adjustment of the filter is undesirable, then a robust accommodative filter can be used.\r\n\r\n(5)\tRobust adaptive neural filters with long- and short-term memories. If both adaptive and robust performances are required of a filter and better generalization ability of the filter is desirable, then a robust adaptive filter with LASTMs can be used.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Lo",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "James T Lo",
   "pi_email_addr": "jameslo@umbc.edu",
   "nsf_id": "000415714",
   "pi_start_date": "2015-06-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland Baltimore County",
  "inst_street_address": "1000 HILLTOP CIR",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4104553140",
  "inst_zip_code": "212500001",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND BALTIMORE COUNTY",
  "org_prnt_uei_num": "",
  "org_uei_num": "RNKYWXURFRL5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland Baltimore County",
  "perf_str_addr": "1000 Hilltop Circle",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212500001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760700",
   "pgm_ele_name": "EPCN-Energy-Power-Ctrl-Netwrks"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "030E",
   "pgm_ref_txt": "CONTROL SYSTEMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 340736.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>1. Predicting the future of a process from the history of its measurements is called time series prediction or signal filtering. The neural filter computes recursively the prediction even if the math model of the time series or signal and their measurements is not given. The mean squared error of prediction is minimized for the training data that is used to synthesize the neural filter is minimized. If a mathematical model of the signal and measurement processes is available, it can be used to generate as much measurement data as needed. Therefore, the performance of the synthesized neural filter can be made as close that of the minimum-variance filter as possible. Filtering is defined in such a general manner that it includes system identification and control as special cases and requires no conventional assumptions such as linearity, markov property, additive noise, Gaussian distribution, etc. In this project, we developed adaptive neural filtering for predicting or filtering a signal process or time series with time varying parameters. An idea called observability in the conventional system theory is generalized to observability of a nonlinear stochastic process from measurements.</p>\n<p>2. The nonconvexity of the error criterion for training learning machines causes the local-minimum problem, which has plagued the development of neural networks or machine learning since its inception. Many techniques have been developed especially in the past 10 years to circumvent the problem. To overcome the problem thoroughly at its root without causing other complications or additional computation is to convexify the training error criterion. This was achieved and comparison numerical results showed the superiority of the convexification method in not only accuracy but also computation reduction. Above all, multiple training sessions for selecting resulting learning machine with the best accuracy are no longer needed.</p>\n<p>3. The convexification method removes underfitting of the DLM to training data. Cross validation was used to stop the training process to avoid overfitting. To further reduce overfitting, we need to devise a method to remove overfitting of the convexification method. Such a method was conceived, but not yet developed in the project.</p>\n<p>4. The temporal hierarchical probabilistic associative memory (THPAM) that explains how the cortex encodes, learns, memorizes, recalls and generalizes and has potentially the capabilities of online, photographic, unsupervised, and hierarchical learning was published in 2011. It is an explainable learning machine, which learns by building cross correlation matrices and recalls by simple matrix multiplication (or decorrelation). A method to enable THPAM to process real-valued quantities was developed and successfully tested in this project.</p>\n<p>5. A real-time unsupervised learning machine for hierarchical clustering and retrieving was conceived and constructed for computational efficiency. It is applicable to a wide range of data clustering applications including text clustering. Testing of the unsupervised learning on large data sets remains to be done.</p>\n<p>------------------------------------------------</p>\n<p>*This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/04/2021<br>\n\t\t\t\t\tModified by: James&nbsp;T&nbsp;Lo</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n1. Predicting the future of a process from the history of its measurements is called time series prediction or signal filtering. The neural filter computes recursively the prediction even if the math model of the time series or signal and their measurements is not given. The mean squared error of prediction is minimized for the training data that is used to synthesize the neural filter is minimized. If a mathematical model of the signal and measurement processes is available, it can be used to generate as much measurement data as needed. Therefore, the performance of the synthesized neural filter can be made as close that of the minimum-variance filter as possible. Filtering is defined in such a general manner that it includes system identification and control as special cases and requires no conventional assumptions such as linearity, markov property, additive noise, Gaussian distribution, etc. In this project, we developed adaptive neural filtering for predicting or filtering a signal process or time series with time varying parameters. An idea called observability in the conventional system theory is generalized to observability of a nonlinear stochastic process from measurements.\n\n2. The nonconvexity of the error criterion for training learning machines causes the local-minimum problem, which has plagued the development of neural networks or machine learning since its inception. Many techniques have been developed especially in the past 10 years to circumvent the problem. To overcome the problem thoroughly at its root without causing other complications or additional computation is to convexify the training error criterion. This was achieved and comparison numerical results showed the superiority of the convexification method in not only accuracy but also computation reduction. Above all, multiple training sessions for selecting resulting learning machine with the best accuracy are no longer needed.\n\n3. The convexification method removes underfitting of the DLM to training data. Cross validation was used to stop the training process to avoid overfitting. To further reduce overfitting, we need to devise a method to remove overfitting of the convexification method. Such a method was conceived, but not yet developed in the project.\n\n4. The temporal hierarchical probabilistic associative memory (THPAM) that explains how the cortex encodes, learns, memorizes, recalls and generalizes and has potentially the capabilities of online, photographic, unsupervised, and hierarchical learning was published in 2011. It is an explainable learning machine, which learns by building cross correlation matrices and recalls by simple matrix multiplication (or decorrelation). A method to enable THPAM to process real-valued quantities was developed and successfully tested in this project.\n\n5. A real-time unsupervised learning machine for hierarchical clustering and retrieving was conceived and constructed for computational efficiency. It is applicable to a wide range of data clustering applications including text clustering. Testing of the unsupervised learning on large data sets remains to be done.\n\n------------------------------------------------\n\n*This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.\n\n\t\t\t\t\tLast Modified: 01/04/2021\n\n\t\t\t\t\tSubmitted by: James T Lo"
 }
}