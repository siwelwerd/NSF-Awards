{
 "awd_id": "1522954",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: RobotSLANG: Simultaneous Localization, Mapping, and Language Acquisition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 650000.0,
 "awd_amount": 650000.0,
 "awd_min_amd_letter_date": "2015-08-06",
 "awd_max_amd_letter_date": "2020-06-22",
 "awd_abstract_narration": "Humans and robots alike have a critical need to navigate through new environments to carry out everyday tasks.  A parent and child may be touring a college campus; a robot may be searching for survivors after a building has collapsed.  In this collaboration by faculty at two institutions, the PIs envision human and robotic partners sharing common perceptual-linguistic experiences and cooperating in mundane tasks like janitorial work and home care as well as in critical tasks like emergency response or search-and-rescue.  But while mapping and navigation are now commonplace for mobile robots, when considering human-robot collaboration for even simple tasks one is confronted by a critical barrier: robots and people do not share a common language.  Human language is rich in linguistic elements for describing our spatial environment, the objects and places within it, and navigable paths through it (e.g., \"go down the hallway and enter the third door on the right.\").  Robots, on the other hand, inhabit a metric world of occupied and unoccupied discretized grid cells, wherein most objects are devoid of meaning (semantics).  The PIs' goal in this project is to overcome this limitation by conjoining the well understood problem of simultaneous localization and mapping (SLAM) with that of language acquisition, in order to enable robots to learn to communicate with people in English about navigation tasks.  The PIs will spur interest in this novel research area within the scientific community by means of an Amazing Race challenge problem modeled after the reality television show of the same name, which will place robots and human-robot teams in unknown environments and charge them with completing a specific task as quickly as possible.  Other outreach activities will include visits to K-12 schools with demonstrations.  \r\n\r\nThis work will focus on simultaneous localization, mapping, and language acquisition, a field of inquiry that remains untouched.  The crucial principles are that semantics are formulated as a cost function, which in turn specifies a joint distribution over many variables including those capturing sensory input, language, the environment map, and robot motor control.  The cost function and joint distribution support standard inference of many forms, such as command following.  More importantly, they support multidirectional inference over multiple variable sets jointly, such as simultaneous mapping and language interpretation.  Within this innovative multivariate optimization-based framework, the PIs plan a thorough experimental regimen including both synthetic and real-world datasets of challenging environments, grounding the semantics of natural language in spatial maps of the realistic visual world and robot motor control, while navigating along particular paths or to arrive at particular destinations in (possibly novel) environments that are mapped not only in a geometric sense but also with linguistic underpinning to these particular paths and destinations.  The language approach is compositional and uses spatially-grounded representations of nouns (objects/places) and prepositions (relations between them).  These spatially-grounded representations will be modeled in the context of mapping.  Furthermore, the PIs will consider realistic environments and adapt visual models thereof according to the joint model.  The PIs are aware of no other work that jointly models mapping, vision, and language acquisition.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Siskind",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey M Siskind",
   "pi_email_addr": "qobi@purdue.edu",
   "nsf_id": "000169494",
   "pi_start_date": "2015-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "465 Northwestern Avenue",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072035",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 650000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>People regularly ask other people for directions. Whether they are driving or walking, they often don't know how to get to where they want to and ask for help from other people. Even with GPS, situations still arise where assistance is needed from knowledgable humans. We anticipate that when self-driving cars eventually become available, situations will still arise where these autoonomous vehicles will need to ask anonymous human pedestrians for directions. Or their passengers will give spoken driving directions to vehicles. These autonomous vehicles will need to understand and follow these directions. Moreover, if autonomous robots become commonplace in human homes, offices, workplaces, and public spaces like shopping malls, airports, and factories, to perform delivery tasks, or other tasks to assist humans, they will also need to converse with humans to get directions to their destinations and then follow those directions autonomously.</p>\n<p>Our research has focussed on this problem. It falls within a growing community of researchers investigating a problem that has come to be known as vision-language navigation (VLN). The objective is for an antonomous entity to obtain and follow navigation instructions specified in English in the context of sensor input like cameras. Most existing VLN research has operated offline in simulation: large datasets have been collected with images of indoor environments taken every few feet annotated with pathways between images. Written text directions describe navigational instructions along these pathways.</p>\n<p>Our work has gone beyond this work in several key ways. First, our work operates on real robots navigating in real environments, not just in simulation. Our robots have been evaluated navigating on 4 floors of 6 different buildings on our college campus, even taking the elevator between floors. Second, our work involves spoken dialog between the robot and passersby. The robot wanders around the environment looking for people to ask for help. It assesses which people it can approach for help by seeing which people seem to be willing to help by walking towards the robot or standing still close to and in front of the robot. It avoids people who express disinterest by walking away. It operates safely in the environment taking care not to harm people or property. It finds people to engage in conversation. It then holds a spoken conversation, asking for help, and understaning the directions obtained. This conversation can take several turns. If the robot doesn't understand the person, or the directions given are incomplete or inconsistent, the robot asks for clarification. After receiving a complete set of directions, the robot then follows the directions to find the goal. Along the way it reads the door tags to help find the goal. In short, it operates much like a human asking for directions in a new unknown environment.</p>\n<p>This is a very difficult problem. Yet, we have obtained modest success. Our robot can correctly understand spoken English navigation instructions about 75% of the time. And it can correctly follow the navigation plan that it produces about 75% of the time in novel environments that it has never seen before. Overall, on a single trial, it can both understand and follow directions correctly about 50% of the time. But since our robot is flexible, robust, and has fallback options, much like humans, like searching for the goal itself without asking for help and searching for and asking new people for help if earlier directions did not allow it to find its goal, its overall sucess rate in reaching its goal&nbsp; is even higher, about 75%.</p>\n<p>We have collected a dataset of navigation sensor data and English instructions that we call Office-to-Office to support our research. We plan to release this dataset to allow others to build on our results.</p>\n<p>Three PhDs have been graduated under this project whose thesis research primarily was focused on this effort. One was an active duty officer in the US Army and a veteran of the Iraq War during his PhD studies.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/22/2021<br>\n\t\t\t\t\tModified by: Jeffrey&nbsp;M&nbsp;Siskind</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nPeople regularly ask other people for directions. Whether they are driving or walking, they often don't know how to get to where they want to and ask for help from other people. Even with GPS, situations still arise where assistance is needed from knowledgable humans. We anticipate that when self-driving cars eventually become available, situations will still arise where these autoonomous vehicles will need to ask anonymous human pedestrians for directions. Or their passengers will give spoken driving directions to vehicles. These autonomous vehicles will need to understand and follow these directions. Moreover, if autonomous robots become commonplace in human homes, offices, workplaces, and public spaces like shopping malls, airports, and factories, to perform delivery tasks, or other tasks to assist humans, they will also need to converse with humans to get directions to their destinations and then follow those directions autonomously.\n\nOur research has focussed on this problem. It falls within a growing community of researchers investigating a problem that has come to be known as vision-language navigation (VLN). The objective is for an antonomous entity to obtain and follow navigation instructions specified in English in the context of sensor input like cameras. Most existing VLN research has operated offline in simulation: large datasets have been collected with images of indoor environments taken every few feet annotated with pathways between images. Written text directions describe navigational instructions along these pathways.\n\nOur work has gone beyond this work in several key ways. First, our work operates on real robots navigating in real environments, not just in simulation. Our robots have been evaluated navigating on 4 floors of 6 different buildings on our college campus, even taking the elevator between floors. Second, our work involves spoken dialog between the robot and passersby. The robot wanders around the environment looking for people to ask for help. It assesses which people it can approach for help by seeing which people seem to be willing to help by walking towards the robot or standing still close to and in front of the robot. It avoids people who express disinterest by walking away. It operates safely in the environment taking care not to harm people or property. It finds people to engage in conversation. It then holds a spoken conversation, asking for help, and understaning the directions obtained. This conversation can take several turns. If the robot doesn't understand the person, or the directions given are incomplete or inconsistent, the robot asks for clarification. After receiving a complete set of directions, the robot then follows the directions to find the goal. Along the way it reads the door tags to help find the goal. In short, it operates much like a human asking for directions in a new unknown environment.\n\nThis is a very difficult problem. Yet, we have obtained modest success. Our robot can correctly understand spoken English navigation instructions about 75% of the time. And it can correctly follow the navigation plan that it produces about 75% of the time in novel environments that it has never seen before. Overall, on a single trial, it can both understand and follow directions correctly about 50% of the time. But since our robot is flexible, robust, and has fallback options, much like humans, like searching for the goal itself without asking for help and searching for and asking new people for help if earlier directions did not allow it to find its goal, its overall sucess rate in reaching its goal  is even higher, about 75%.\n\nWe have collected a dataset of navigation sensor data and English instructions that we call Office-to-Office to support our research. We plan to release this dataset to allow others to build on our results.\n\nThree PhDs have been graduated under this project whose thesis research primarily was focused on this effort. One was an active duty officer in the US Army and a veteran of the Iraq War during his PhD studies.\n\n\t\t\t\t\tLast Modified: 10/22/2021\n\n\t\t\t\t\tSubmitted by: Jeffrey M Siskind"
 }
}