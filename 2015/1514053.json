{
 "awd_id": "1514053",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Medium: Constructing Knowledge Bases by Extracting Entity-Relations and Meanings from Natural Language via \"Universal Schema\"",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 1000000.0,
 "awd_amount": 1000000.0,
 "awd_min_amd_letter_date": "2015-08-24",
 "awd_max_amd_letter_date": "2015-09-17",
 "awd_abstract_narration": "Automated knowledge base (KB) construction from natural language is of fundamental importance to (a) scientists (for example, there has been long-standing interest in building KBs of genes and proteins), (b) social scientists (for example, building social networks from textual data), and (c) national defense (where network analysis of criminals and terrorists have proven useful). The core of a knowledge base is its objects (\"entities\", such as proteins, people, organizations and locations) and its connections between these objects (\"relations\", such as one protein increasing production of another, or a person working for an organization). This project aims to greatly increase the accuracy with which entity-relations can be extracted from text, as well as increase the fidelity which many subtle distinctions among types of relations can be represented. The project's technical approach -- which we call \"universal schema\" -- is a markedly novel departure from traditional methods, based on representing all of the input relation expressions as positions in a common multi-dimensional space, with nearby relations having similar meanings. Broader impacts will include collaboration with industry on applications of economic importance, collaboration with academic non-computer-scientists on a multidisciplinary application, creating and publicly releasing new data sets for benchmark evaluation by ourselves and others (enabling scientific progress through improved performance comparisons), creating and publicly releasing an open-source implementation of our methods (enabling further scientific research, easy large-scale use, rapid commercialization and third-party enhancements). Education impacts include creating and teaching a new course on knowledge base construction for the sciences, organizing a research workshop on embeddings, extraction and knowledge representation, and training multiple undergraduates and graduate students. \r\n\r\nMost previous research in relation extraction falls into one of two categories. In the first, one must define a pre-fixed schema of relation types (such as lives-in, employed-by and a handful of others), which limits expressivity and hides language ambiguities. Training machine learning models here either relies on labeled training data (which is scarce and expensive), or uses lightly-supervised self-training procedures (which are often brittle and wander farther from the truth with additional iterations). In the second category, one extracts into an \"open\" schema based on language strings themselves (lacking ability to generalize among them), or attempts to gain generalization with unsupervised clustering of these strings (suffering from clusters that fail to capture reliable synonyms, or even find the desired semantics at all). This project proposes research in relation extraction of \"universal schema\", where we learn a generalizing model of the union of all input schemas, including multiple available pre-structured KBs as well as all the observed natural language surface forms. The approach thus embraces the diversity and ambiguity of original language surface forms (not trying to force relations into pre-defined boxes), yet also successfully generalizes by learning non-symmetric implicature among explicit and implicit relations using new extensions to the probabilistic matrix factorization and vector embedding methods that were so successful in the NetFlix prize competition. Universal schema provide for a nearly limitless diversity of relation types (due to surface forms), and support convenient semi-supervised learning through integration with existing structured data (i.e., the relation types of existing databases). In preliminary experiments, the approach already surpassed by a wide margin the previous state-of-the-art relation extraction methods on a benchmark task. New proposed research includes new training processes, new representations that include multiple-senses for the same surface form as well as embeddings with variances, new methods of incorporating constraints, joint inference between entity- and relation-types, new models of non-binary and higher-order relations, and scalability through parallel distribution. The project web site (http://www.iesl.cs.umass.edu/projects/NSF_USchema.html) will include information on the project and provide access to data sets, source code and documentation, teaching and workshop materials, and publications. In addition, datasets will be disseminated via UCI Machine Learning Repository (or other similar archive location for machine learning data) to facilitate sharing with other researchers and ensure long-term availability, and GitHub will be used to facilitate release, sharing, and archiving of code.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrew",
   "pi_last_name": "McCallum",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Andrew K McCallum",
   "pi_email_addr": "mccallum@cs.umass.edu",
   "nsf_id": "000253651",
   "pi_start_date": "2015-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039242",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 1000000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p dir=\"ltr\">A knowledge base (KB) is a structured, navigable map of knowledge about objects (\"entities\", such as proteins, people, organizations and locations) and the connections between them (\"relations\", such as one protein increasing production of another, or a person working for an organization).&nbsp; KBs provide a queryable, browsable, and machine-readable view on knowledge that usually otherwise would be spread across many natural language textual documents.&nbsp; For example, if given only a document collection, biomedical scientist can merely search for documents and read them; however, if given a KB, biomedical scientists can browse a navigable map from gene to protein to disease to symptom---a map that has integrated disparate information from across many scientific documents about any one gene, etc.&nbsp; KBs are of fundamental importance to accelerating scientific progress, national defense, business, and many other disciplines.&nbsp;&nbsp;</p>\n<p dir=\"ltr\">Automated knowledge base construction (AKBC) is the process of using natural language processing (NLP) and other artificial intelligence (AI) techniques to build the KB from massive document collections without requiring humans to do data-entry.&nbsp; It is a tremendously challenging task that requires subtle interpretation, resolving ambiguities, and flexible knowledge representation.</p>\n<p dir=\"ltr\">This project has developed new methods in machine learning (ML), NLP and AI that greatly increase the accuracy with which entity-relations can be extracted from text, as well as increase the fidelity with which many subtle distinctions among types of relations can be represented. The project's technical approach---which we call \"universal schema\"---is based on representing all of the input relation expressions as vector positions in a common multi-dimensional space, with nearby relations having similar meanings.&nbsp;&nbsp;</p>\n<p dir=\"ltr\">Prior to this project, most research in relation extraction fell into one of two categories. In the first, one must define a pre-fixed schema of relation types (such as &ldquo;lives-in&rdquo;, &ldquo;employed-by&rdquo;, and a handful of others), which limits expressivity and hides language ambiguities. In the second category, one extracts into an &lsquo;&lsquo;open&rsquo;&rsquo; schema based on language strings themselves (lacking ability to generalize among them), or attempts to gain generalization with unsupervised clustering of these strings (suffering from clusters that fail to capture reliable synonyms, or even find the desired semantics at all).</p>\n<p dir=\"ltr\">In our &ldquo;universal schema&rdquo; we avoid this dichotomy, using a vector space to learn a model of the union of all input schemas, including multiple available pre-structured KBs as well as all the observed natural language surface forms. Our approach thus embraces the diversity and ambiguity of original language surface forms (not trying to force relations into predefined categories), yet also successfully generalizes by learning non-symmetric implicature among explicit and implicit relations using new extensions to the probabilistic matrix factorization and vector embedding methods that have been so successful in recommender systems and deep neural networks.</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\">Intellectual Merit</p>\n<p dir=\"ltr\">Our approach is a markedly novel departure from previous, traditional methods. Our latent embedding vectors for entities and relations offer an intriguing flexible approach to semantics, relational implicature, and semi-supervised training. Universal schema provide for a nearly limitless diversity of relation types (coming from the subtleties of natural language), and also support convenient semi-supervised learning through integration with existing structured data (i.e. the relation types and schema of existing databases).&nbsp;</p>\n<p dir=\"ltr\">The work supported by this grant bore out our optimism about our approach.&nbsp; We surpassed previous state-of-the-art results in many areas, including the NIST TAC-Knowledge Base Population task.&nbsp; We developed new machine learning methods and natural language processing techniques, publishing over 20 peer-reviewed research papers.&nbsp; We developed intellectual connections between &ldquo;universal schema&rdquo; and other areas of ML and NLP, including joint inference, deep neural networks, parsing, coreference, and question answering.</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\">Broader Impacts</p>\n<p dir=\"ltr\">Information overload has become an increasingly burdensome problem across many fields of high national priority, including biomedicine, material science, national defense, business decision-making, and many other areas.&nbsp; Improved methods for building and maintaining knowledge bases are a key ingredient to help decision-makers navigate high-information domains.&nbsp; The work supported by this grant has yielded new methods that not only have merit through intellectual novelty, but also yielded broad practical impact.&nbsp;&nbsp;</p>\n<p dir=\"ltr\">Our &ldquo;universal schema&rdquo; method and its successors are now widely used in industry, in both large companies such as IBM, Oracle and Google, and startup companies such as Lexalytics.&nbsp; External deployments of our ideas have been employed in a wide variety of fields, including biomedicine, business decision-making, and national defense.</p>\n<p dir=\"ltr\">We have made more than five open-source software releases implementing our methods, enabling others to apply and extend our work.&nbsp; We have released multiple datasets allowing others to test their&nbsp; methods and compare them to ours.&nbsp;&nbsp;</p>\n<p dir=\"ltr\">The work of this project has also supported the creation of new scientific communities.&nbsp; In 2019 the PI launched (and served as the General Chair of) the first international conference on &ldquo;Automated Knowledge Base Construction&rdquo; which is now in its third successful year, bringing together researchers from ML, NLP, semantic web, and databases.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/11/2021<br>\n\t\t\t\t\tModified by: Andrew&nbsp;K&nbsp;Mccallum</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "A knowledge base (KB) is a structured, navigable map of knowledge about objects (\"entities\", such as proteins, people, organizations and locations) and the connections between them (\"relations\", such as one protein increasing production of another, or a person working for an organization).  KBs provide a queryable, browsable, and machine-readable view on knowledge that usually otherwise would be spread across many natural language textual documents.  For example, if given only a document collection, biomedical scientist can merely search for documents and read them; however, if given a KB, biomedical scientists can browse a navigable map from gene to protein to disease to symptom---a map that has integrated disparate information from across many scientific documents about any one gene, etc.  KBs are of fundamental importance to accelerating scientific progress, national defense, business, and many other disciplines.  \nAutomated knowledge base construction (AKBC) is the process of using natural language processing (NLP) and other artificial intelligence (AI) techniques to build the KB from massive document collections without requiring humans to do data-entry.  It is a tremendously challenging task that requires subtle interpretation, resolving ambiguities, and flexible knowledge representation.\nThis project has developed new methods in machine learning (ML), NLP and AI that greatly increase the accuracy with which entity-relations can be extracted from text, as well as increase the fidelity with which many subtle distinctions among types of relations can be represented. The project's technical approach---which we call \"universal schema\"---is based on representing all of the input relation expressions as vector positions in a common multi-dimensional space, with nearby relations having similar meanings.  \nPrior to this project, most research in relation extraction fell into one of two categories. In the first, one must define a pre-fixed schema of relation types (such as \"lives-in\", \"employed-by\", and a handful of others), which limits expressivity and hides language ambiguities. In the second category, one extracts into an \u2018\u2018open\u2019\u2019 schema based on language strings themselves (lacking ability to generalize among them), or attempts to gain generalization with unsupervised clustering of these strings (suffering from clusters that fail to capture reliable synonyms, or even find the desired semantics at all).\nIn our \"universal schema\" we avoid this dichotomy, using a vector space to learn a model of the union of all input schemas, including multiple available pre-structured KBs as well as all the observed natural language surface forms. Our approach thus embraces the diversity and ambiguity of original language surface forms (not trying to force relations into predefined categories), yet also successfully generalizes by learning non-symmetric implicature among explicit and implicit relations using new extensions to the probabilistic matrix factorization and vector embedding methods that have been so successful in recommender systems and deep neural networks.\n\n \nIntellectual Merit\nOur approach is a markedly novel departure from previous, traditional methods. Our latent embedding vectors for entities and relations offer an intriguing flexible approach to semantics, relational implicature, and semi-supervised training. Universal schema provide for a nearly limitless diversity of relation types (coming from the subtleties of natural language), and also support convenient semi-supervised learning through integration with existing structured data (i.e. the relation types and schema of existing databases). \nThe work supported by this grant bore out our optimism about our approach.  We surpassed previous state-of-the-art results in many areas, including the NIST TAC-Knowledge Base Population task.  We developed new machine learning methods and natural language processing techniques, publishing over 20 peer-reviewed research papers.  We developed intellectual connections between \"universal schema\" and other areas of ML and NLP, including joint inference, deep neural networks, parsing, coreference, and question answering.\n\n \nBroader Impacts\nInformation overload has become an increasingly burdensome problem across many fields of high national priority, including biomedicine, material science, national defense, business decision-making, and many other areas.  Improved methods for building and maintaining knowledge bases are a key ingredient to help decision-makers navigate high-information domains.  The work supported by this grant has yielded new methods that not only have merit through intellectual novelty, but also yielded broad practical impact.  \nOur \"universal schema\" method and its successors are now widely used in industry, in both large companies such as IBM, Oracle and Google, and startup companies such as Lexalytics.  External deployments of our ideas have been employed in a wide variety of fields, including biomedicine, business decision-making, and national defense.\nWe have made more than five open-source software releases implementing our methods, enabling others to apply and extend our work.  We have released multiple datasets allowing others to test their  methods and compare them to ours.  \nThe work of this project has also supported the creation of new scientific communities.  In 2019 the PI launched (and served as the General Chair of) the first international conference on \"Automated Knowledge Base Construction\" which is now in its third successful year, bringing together researchers from ML, NLP, semantic web, and databases.\n\n \n\n\t\t\t\t\tLast Modified: 02/11/2021\n\n\t\t\t\t\tSubmitted by: Andrew K Mccallum"
 }
}