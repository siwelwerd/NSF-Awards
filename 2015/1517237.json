{
 "awd_id": "1517237",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "New Probabilistic Methods for Observational Cosmology",
 "cfda_num": "47.049",
 "org_code": "03020000",
 "po_phone": "7032924905",
 "po_email": "nsharp@nsf.gov",
 "po_sign_block_name": "Nigel Sharp",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 328312.0,
 "awd_amount": 328312.0,
 "awd_min_amd_letter_date": "2015-08-20",
 "awd_max_amd_letter_date": "2015-08-20",
 "awd_abstract_narration": "Future astronomical surveys will be producing such enormous quantities of data that new analysis methods are becoming imperative.  Especially when large quantities of information must be distilled into theoretical insights, and the individual items are themselves actually of less certainty, a probability-based approach offers impressive advantages.  This project will generate toolkits to implement such novel approaches to the extraction of knowledge from overwhelmingly large data sets.  With the potential to transform the way science is done both in cosmology and in other areas under threat of being swamped with data, the impact of this work cannot be overestimated.  The methods will both use and propagate collaborations with applied mathematics that make the impossible possible.\r\n\r\nNew cosmological surveys will require measurement of smaller signals using larger numbers of galaxies which are individually observed at lower confidence.  This will require data analyses that are as information-preserving as possible.  This project will create new methods for cosmological data analysis that permit inferences using not lossy, derived data products, such as galaxy catalogs, best-fit redshifts, or correlation function point estimates, but something much closer to the original imaging and spectroscopic data.  These techniques will be informed by the principles of probabilistic inference and applied-mathematics technology.  The work creates three related toolsets.  Toolset 1 is for reconstruction and marginalization of cosmological density fields, which could be the mass, galaxy, or neutral-gas density field, or the two-dimensional projected mass density.  Toolset 2 is for cosmological inference that makes proper use of probabilistic information about galaxy and quasar redshifts, improving probabilistic redshift information, and providing informative imputation of missing redshifts, thereby producing predictions and tools for smaller-scale scientific questions.  Toolset 3 is for propagation of probabilistic image-level quantities such as galaxy shapes and the point-spread function, into weak-lensing studies of large-scale structure.  This will permit inference from survey data conditional priors over galaxy shapes, and use them in a justified forward-modeling measurement of the shear field and cosmological parameters.  The toolsets from this project will be the first practical methods for cosmological inference and large-scale structure measurement that can make full and proper, justified, use of probabilistic outputs.  For the first time, it will be possible to perform simultaneous inference or refinement of catalog-level properties along with large-scale structure and cosmological inferences.  Simultaneous inference will significantly reduce statistical biases in cosmological measurements, and also reduce variance in catalog-level quantities.\r\n\r\nThe toolsets will be papers and methods but also open-source codebases, with benefits beyond cosmology.  These developments will help create standards for generating and delivering probabilistic outputs.  The research will reach populations inside and outside academia by producing pedagogical papers on inference, data analysis, and computational statistics in the physical sciences.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "AST",
 "org_div_long_name": "Division Of Astronomical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Hogg",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "David W Hogg",
   "pi_email_addr": "david.hogg@nyu.edu",
   "nsf_id": "000186667",
   "pi_start_date": "2015-08-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "4 Washington Place",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100036603",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "121700",
   "pgm_ele_name": "EXTRAGALACTIC ASTRON & COSMOLO"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1206",
   "pgm_ref_txt": "THEORETICAL & COMPUTATIONAL ASTROPHYSICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 328312.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Intellectual merit: We contributed tools, ideas, and publications to the field of precision cosmology. We helped prepare the community for the new data and capabilities of new mission but most especially the NSF-funded LSST project. In particular, we determined how to use probabilistic redshift outputs (and other kinds of posterior pdf outputs) from cosmology surveys in cosmological inferences. And we worked on how to produce and report them as well. These are core technologies for LSST and other projects, including the ESA Gaia Mission. We showed that many of the current thoughts about how to produce and how to use these kinds of outputs are flawed, and that with small changes, they could be dramatically improved. We delivered working code that performs correct analyses with probabilistic outputs, and also code that can be used to create and deliver probabilistic outputs. One of our principal findings is that it is likelihood information, not posterior information, that is the critical output of a survey or project, if that project is trying to support new data analyses by its users.</p>\n<p>Broader impacts: We created educational environments and educational materials, and workshopped them at workshops and hack weeks. We published written educational materials on the arXiv and we led sessions at many events. In particular, we ran and participated in the AstroHackWeek and GaiaSprint projects in which we educated scientists from a wide range of domains, across a wide range of career stages, in interdisciplinary and methodological work related to precision measurement, inference, and data analysis.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/02/2020<br>\n\t\t\t\t\tModified by: David&nbsp;W&nbsp;Hogg</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIntellectual merit: We contributed tools, ideas, and publications to the field of precision cosmology. We helped prepare the community for the new data and capabilities of new mission but most especially the NSF-funded LSST project. In particular, we determined how to use probabilistic redshift outputs (and other kinds of posterior pdf outputs) from cosmology surveys in cosmological inferences. And we worked on how to produce and report them as well. These are core technologies for LSST and other projects, including the ESA Gaia Mission. We showed that many of the current thoughts about how to produce and how to use these kinds of outputs are flawed, and that with small changes, they could be dramatically improved. We delivered working code that performs correct analyses with probabilistic outputs, and also code that can be used to create and deliver probabilistic outputs. One of our principal findings is that it is likelihood information, not posterior information, that is the critical output of a survey or project, if that project is trying to support new data analyses by its users.\n\nBroader impacts: We created educational environments and educational materials, and workshopped them at workshops and hack weeks. We published written educational materials on the arXiv and we led sessions at many events. In particular, we ran and participated in the AstroHackWeek and GaiaSprint projects in which we educated scientists from a wide range of domains, across a wide range of career stages, in interdisciplinary and methodological work related to precision measurement, inference, and data analysis.\n\n\t\t\t\t\tLast Modified: 01/02/2020\n\n\t\t\t\t\tSubmitted by: David W Hogg"
 }
}