{
 "awd_id": "1452068",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: New methods for multivariate analysis in high dimensions",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2015-02-05",
 "awd_max_amd_letter_date": "2019-06-13",
 "awd_abstract_narration": "Datasets from imaging, gene microarray experiments, and many other fields often have more measured characteristics than subjects.  Analyzing these data with standard statistical methods is either impossible or inadequate.  The investigator addresses this problem by developing new statistical methods that are appropriate for such datasets.  The investigator develops theoretical justifications for these new methods and fast computational algorithms for their application.  Software that implements these algorithms will be made available to the public.  These new products will help practitioners in industry create better predictive models and will also help advance research in many other fields.  The investigator will also develop new curricula, including the creation of an undergraduate statistical computing course, an undergraduate statistical machine learning course, a Ph.D.-level topics course, and a new track within the undergraduate statistics major.\r\n\r\nBuilding statistical models when the number of explanatory variables exceeds the sample size is an exciting area at the forefront of multivariate analysis.  Fitting these models with classical techniques is typically impossible and some constraints or penalties must be imposed.  Penalties that encourage zeros in parameter estimates have received substantial attention.  These penalties are useful because they lead to interpretable parameter estimates, but assuming that these zeros exist may be inappropriate in some applications.  The investigator develops and analyzes new methods to fit models in high dimensions that do not require that zeros are present in the parameters of interest, but still allow the practitioner to make simple interpretations of the fit in terms of the measured variables.  This includes the development of new methods to fit multiple response regression models and multinomial logistic regression models, as well as the development of new methods to shrink characteristics of inverse covariance estimates that are needed to fit predictive models.  The investigator will develop new curricula and involve Ph.D. students in the research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Adam",
   "pi_last_name": "Rothman",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Adam J Rothman",
   "pi_email_addr": "arothman@umn.edu",
   "nsf_id": "000547003",
   "pi_start_date": "2015-02-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota-Twin Cities",
  "perf_str_addr": "383 Ford Hall, 224 Church St. SE",
  "perf_city_name": "Minneapolis",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554552070",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  },
  {
   "pgm_ele_code": "804800",
   "pgm_ele_name": "Division Co-Funding: CAREER"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 69801.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 67549.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 77409.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 91062.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 94179.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-fc3bd086-7fff-edc7-c4ce-020f14f32202\"> </span></p>\n<p><span id=\"docs-internal-guid-7d2748ec-7fff-74a5-a747-b5e673ee91b2\"> </span></p>\n<p><span id=\"docs-internal-guid-c25254dd-7fff-ba12-744e-75debcdaedfa\">&nbsp;</span></p>\n<p dir=\"ltr\"><span>The PI and his coauthors developed and theoretically justified several new data analysis methods.&nbsp; In some regression problems, the response (or output) is multivariate.&nbsp; We developed three new methods to fit multivariate-response regression models to data.&nbsp; We also developed two new methods for categorical-response regression (also called classification) as well as a method for estimating inverse covariance matrices that are needed to fit predictive models.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Fitting the multivariate-response linear regression model to data is difficult when the dimension of the response or predictor is large (a setting that we call high dimensional). Most of the recently proposed high-dimensional fitting methods assume that the regression coefficient matrix has many entries equal to zero. Zeros in the estimate are easy to interpret, but this assumption may be unreasonable in some applications. We developed a new method to fit this model that can estimate a non-sparse regression coefficient matrix in high dimensions and produce an interpretable fitted model. Our method indirectly estimates the regression coefficient matrix through shrinkage or sparse estimation of the parameters of the inverse regression. In some applications, the inverse regression's coefficient matrix could have many entries equal to zero when the (forward) regression coefficient matrix has none. A special case of our method takes advantage of this scenario and allows the practitioner to interpret the estimated zeros in the inverse regression's coefficient matrix. We also developed a method to fit the multivariate-response linear regression model by exploiting a parametric connection between the regression coefficient matrix and the error covariance matrix. This parametric connection is that the correlations between entries in the multivariate error random vector are proportional to the cosines of the angles between their corresponding regression coefficient matrix columns, so as the angle between two regression coefficient matrix columns decreases, the correlation between the corresponding errors increases.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Categorical-response regression methods (also called classification methods) include multinomial logistic regression and linear discriminant analysis.&nbsp; Fitting a multinomial logistic regression model to data is difficult when there are many response categories.&nbsp; We developed a method that automatically combines response categories in multinomial logistic regression.&nbsp; This allows practitioners to discover easy-to-interpret models with fewer response categories.&nbsp; We also developed a method to predict a categorical response variable using a matrix-valued explanatory variable through a parsimonious parameterization of the linear discriminant analysis model.</span></p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\"><span>&nbsp;</span>Multivariate-categorical-response regression methods are designed to outperform the use of separate univariate-categorical-response regression methods.&nbsp; We developed a new method for multivariate-categorical-response regression that allows practitioners to estimate which explanatory variables are irrelevant, which only affect the marginal distributions of the response, and which affect both the marginal distributions and odds ratios.</p>\n<p dir=\"ltr\"><span><br /></span></p>\n<p dir=\"ltr\"><span>We also developed a precision matrix estimation framework that aims to improve prediction performance by shrinking the characteristic of the precision matrix estimator that is needed for prediction.&nbsp; &nbsp;</span></p>\n<p dir=\"ltr\"><span><br /></span></p>\n<p dir=\"ltr\"><span>The PI developed two new courses at the University of Minnesota and supervised PhD students.&nbsp; The PI also gave invited talks at conferences and department seminars.</span></p>\n<p dir=\"ltr\"><span><br /></span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/19/2021<br>\n\t\t\t\t\tModified by: Adam&nbsp;J&nbsp;Rothman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \n\n \n\n \nThe PI and his coauthors developed and theoretically justified several new data analysis methods.  In some regression problems, the response (or output) is multivariate.  We developed three new methods to fit multivariate-response regression models to data.  We also developed two new methods for categorical-response regression (also called classification) as well as a method for estimating inverse covariance matrices that are needed to fit predictive models.\n\n \nFitting the multivariate-response linear regression model to data is difficult when the dimension of the response or predictor is large (a setting that we call high dimensional). Most of the recently proposed high-dimensional fitting methods assume that the regression coefficient matrix has many entries equal to zero. Zeros in the estimate are easy to interpret, but this assumption may be unreasonable in some applications. We developed a new method to fit this model that can estimate a non-sparse regression coefficient matrix in high dimensions and produce an interpretable fitted model. Our method indirectly estimates the regression coefficient matrix through shrinkage or sparse estimation of the parameters of the inverse regression. In some applications, the inverse regression's coefficient matrix could have many entries equal to zero when the (forward) regression coefficient matrix has none. A special case of our method takes advantage of this scenario and allows the practitioner to interpret the estimated zeros in the inverse regression's coefficient matrix. We also developed a method to fit the multivariate-response linear regression model by exploiting a parametric connection between the regression coefficient matrix and the error covariance matrix. This parametric connection is that the correlations between entries in the multivariate error random vector are proportional to the cosines of the angles between their corresponding regression coefficient matrix columns, so as the angle between two regression coefficient matrix columns decreases, the correlation between the corresponding errors increases.\n\n \nCategorical-response regression methods (also called classification methods) include multinomial logistic regression and linear discriminant analysis.  Fitting a multinomial logistic regression model to data is difficult when there are many response categories.  We developed a method that automatically combines response categories in multinomial logistic regression.  This allows practitioners to discover easy-to-interpret models with fewer response categories.  We also developed a method to predict a categorical response variable using a matrix-valued explanatory variable through a parsimonious parameterization of the linear discriminant analysis model.\n \n Multivariate-categorical-response regression methods are designed to outperform the use of separate univariate-categorical-response regression methods.  We developed a new method for multivariate-categorical-response regression that allows practitioners to estimate which explanatory variables are irrelevant, which only affect the marginal distributions of the response, and which affect both the marginal distributions and odds ratios.\n\n\nWe also developed a precision matrix estimation framework that aims to improve prediction performance by shrinking the characteristic of the precision matrix estimator that is needed for prediction.   \n\n\nThe PI developed two new courses at the University of Minnesota and supervised PhD students.  The PI also gave invited talks at conferences and department seminars.\n\n\n\n\t\t\t\t\tLast Modified: 10/19/2021\n\n\t\t\t\t\tSubmitted by: Adam J Rothman"
 }
}