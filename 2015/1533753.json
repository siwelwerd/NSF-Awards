{
 "awd_id": "1533753",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: FULL: DSD: Scalable High Performance with Halide and Simit Domain Specific Languages",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927885",
 "po_email": "abanerje@nsf.gov",
 "po_sign_block_name": "Anindya Banerjee",
 "awd_eff_date": "2015-08-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 845000.0,
 "awd_amount": 845000.0,
 "awd_min_amd_letter_date": "2015-07-20",
 "awd_max_amd_letter_date": "2015-07-20",
 "awd_abstract_narration": "Title: XPS: FULL: DSD: Scalable High Performance with Halide and Simit Domain Specific Languages\r\n\r\nToday, getting scalable parallel performance requires heroic programming by experts. In this proposal we are developing a methodology based on Domain Specific Languages (DSLs) to simplify the programmer effort required to harness the power of scalable parallelism. The intellectual merits of this proposal are to show that DSLs can provide a way for programmers to take advantage of scalable performance without a heroic effort. Having a simple path for scalable parallel performance will have a broader significance and importance on areas such as climate modeling and other simulations of large-scale science, by enabling them to efficiently utilize large-scale machines and the cloud.\r\n\r\nThis proposal aims to radically simplify high performance DSL construction. First, it will introduce a unified transformation framework where complex program transformations are described by example. Using synthesis technology, combinations of localized rewriting rules will be extracted and applied, simplifying the implementation while providing correctness guarantees. Second, it will build a unified parallel low-level intermediate representation by extending LLVM. With the new parallel backend, DSLs only have to expose algorithmic parallelism and the backend will do all architecture-specific mapping of parallelism to vector, non-uniform memory access (NUMA), graphics processing unit (GPU) and distributed parallel components. Third, it will develop a unified auto-tuning framework. Effectiveness of frontend transformations depends on the ability of backends to exploit them. The unified auto-tuning framework will completely eliminate this complexity by offloading transformation selection to the auto-tuner which will use sophisticated machine learning techniques to empirically select transformations that yield the best scalable parallel performance.  The ideas introduced will be demonstrated through two important domain-specific languages ? the Halide DSL for image processing pipelines and the Simit DSL for physical simulations.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Saman",
   "pi_last_name": "Amarasinghe",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Saman P Amarasinghe",
   "pi_email_addr": "saman@mit.edu",
   "nsf_id": "000184884",
   "pi_start_date": "2015-07-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Fredo",
   "pi_last_name": "Durand",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fredo Durand",
   "pi_email_addr": "fredo@graphics.lcs.mit.edu",
   "nsf_id": "000107364",
   "pi_start_date": "2015-07-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Armando",
   "pi_last_name": "Solar-Lezama",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Armando Solar-Lezama",
   "pi_email_addr": "asolar@csail.mit.edu",
   "nsf_id": "000541631",
   "pi_start_date": "2015-07-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Wojciech",
   "pi_last_name": "Matusik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wojciech Matusik",
   "pi_email_addr": "wojciech@csail.mit.edu",
   "nsf_id": "000579272",
   "pi_start_date": "2015-07-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenu",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 845000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project created and extended three compiler frameworks Halide, TACO, and Tiramisu.&nbsp;</p>\n<p>The Halide language is designed to make it easier to write high-performance data-parallel programs on the multidimensional array and dense tensors. Halide&rsquo;s key idea is to think of a program, not as two distinct parts: a declaratively specified algorithm to be performed and a complementary schedule that maps the algorithm to physical hardware. This allows simple, portable, and composable algorithm code. The schedule language models the choices and tradeoffs fundamental to multidimensional array processing, providing a powerful representation to express and explore countless ways to exploit locality, parallelism, and specialized hardware at all scales of the resulting implementation. The resulting programs deliver performance matching or exceeding the best prior expert-optimized implementations across a wide range of architectures while requiring code often orders of magnitude simpler. As a result, Halide is now widely used throughout the industry: it runs on billions of devices, comprises a measurable percentage of all of Google&rsquo;s data center compute, and is the target of new specialized accelerators.</p>\n<p>TACO is the first compiler that generates sparse code for any tensor expression on most existing sparse tensor representations. The generated code matches or exceeds the performance of all hand-optimized libraries while generalizing to any expression and many user-specified irregular data structures. This project developed a sparse tensor algebra theory, which combines tensor expressions with specifications of irregular data structures and optimizations to produce efficient parallel sparse code. The key was to break it into three subproblems that he independently solved and then composed. The subproblems were how to represent any tensor data structure, how to characterize the multidimensional sparse iteration spaces of expressions, and how to generate code to co-iterate over irregular data structures.&nbsp;</p>\n<p>Tiramisu is a polyhedral compiler designed to simplify code generation for hardware architectures. It achieves state-of-the-art performance for deep learning in general and outperforms existing frameworks for the class of recurrent neural networks in particular. Tiramisu explicitly divides the intermediate representation into four layers. These layers are designed to hide the complexity and large variety of execution platforms by separating the architecture-independent algorithm from code transformations, data layout, and communication. The four layers are represented using the polyhedral model, a unified mathematical model used to represent code and transformations on that code. This model provides many advantages such as the ability to apply1complex loop and data layout transformations, the ability to check the correctness of transformations, and the ability to express and optimize programs that have cycles in their data flow graphs (such as RNNs).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2020<br>\n\t\t\t\t\tModified by: Saman&nbsp;P&nbsp;Amarasinghe</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project created and extended three compiler frameworks Halide, TACO, and Tiramisu. \n\nThe Halide language is designed to make it easier to write high-performance data-parallel programs on the multidimensional array and dense tensors. Halide\u2019s key idea is to think of a program, not as two distinct parts: a declaratively specified algorithm to be performed and a complementary schedule that maps the algorithm to physical hardware. This allows simple, portable, and composable algorithm code. The schedule language models the choices and tradeoffs fundamental to multidimensional array processing, providing a powerful representation to express and explore countless ways to exploit locality, parallelism, and specialized hardware at all scales of the resulting implementation. The resulting programs deliver performance matching or exceeding the best prior expert-optimized implementations across a wide range of architectures while requiring code often orders of magnitude simpler. As a result, Halide is now widely used throughout the industry: it runs on billions of devices, comprises a measurable percentage of all of Google\u2019s data center compute, and is the target of new specialized accelerators.\n\nTACO is the first compiler that generates sparse code for any tensor expression on most existing sparse tensor representations. The generated code matches or exceeds the performance of all hand-optimized libraries while generalizing to any expression and many user-specified irregular data structures. This project developed a sparse tensor algebra theory, which combines tensor expressions with specifications of irregular data structures and optimizations to produce efficient parallel sparse code. The key was to break it into three subproblems that he independently solved and then composed. The subproblems were how to represent any tensor data structure, how to characterize the multidimensional sparse iteration spaces of expressions, and how to generate code to co-iterate over irregular data structures. \n\nTiramisu is a polyhedral compiler designed to simplify code generation for hardware architectures. It achieves state-of-the-art performance for deep learning in general and outperforms existing frameworks for the class of recurrent neural networks in particular. Tiramisu explicitly divides the intermediate representation into four layers. These layers are designed to hide the complexity and large variety of execution platforms by separating the architecture-independent algorithm from code transformations, data layout, and communication. The four layers are represented using the polyhedral model, a unified mathematical model used to represent code and transformations on that code. This model provides many advantages such as the ability to apply1complex loop and data layout transformations, the ability to check the correctness of transformations, and the ability to express and optimize programs that have cycles in their data flow graphs (such as RNNs).\n\n\t\t\t\t\tLast Modified: 10/29/2020\n\n\t\t\t\t\tSubmitted by: Saman P Amarasinghe"
 }
}