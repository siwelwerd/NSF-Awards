{
 "awd_id": "1463988",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: III: Scaling up Distance Metric Learning for Large-scale Ultrahigh-dimensional Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Maria Zemankova",
 "awd_eff_date": "2015-03-01",
 "awd_exp_date": "2018-02-28",
 "tot_intn_awd_amt": 174576.0,
 "awd_amount": 174576.0,
 "awd_min_amd_letter_date": "2015-03-03",
 "awd_max_amd_letter_date": "2015-03-03",
 "awd_abstract_narration": "This project is to research and develop highly scalable stochastic optimization algorithms for distance metric learning (DML) for large-scale ultrahigh-dimensional (LSUD) data. DML is a fundamental problem in machine learning aiming to learn a distance metric such that intra-class variation is small and inter-class variation is large.  When the scale and dimensionality of data is very large, the computational cost of DML is prohibitive. Domains utilizing machine learning techniques such as computer vision, natural language processing and bioinformatics will be directly impacted by this research. For example, one application is fine-grained image classification, e.g., categorizing different types of flowers or models of vehicles from pictures (this application will be used as one criteria to evaluate success of the research.) The research will enable data scientists to extract more knowledge from massive high-dimensional data complementing the White House BIG DATA Initiative to analyze large and complex data sets. Beyond its research impact, this project will facilitate the development of a new machine learning course at the University of Iowa (UI), and contribute to training future professionals  in big data analytics. Broader impact will be further affected by dissemination of results through publications, open-sourced software, etc.\r\n\r\nThis project addresses the computational challenges of LSUD-DML by scaling up the state of the art stochastic gradient descent (SGD) methods. A key computational bottleneck in applying SGD to DML is to project the updated solution into a complicated feasible domain at each iteration. The innovative proposed ideas lie at reducing the total cost of projections by (i) constructing and exploring a low-rank structured stochastic gradient to reduce the cost of projection, and (ii) dividing iterations into epochs and performing a projection-efficient SGD at each epoch to reduce the number of projections. Investigating data-dependent sampling strategies (i.e., selective sampling, importance sampling, and a combination of both) for LSUD-DML will further scale up the proposed methods. This research will provide experimental evidence regarding the scalability of the proposed algorithms while revealing insights into the proposed techniques and various analytical tradeoffs.\r\n\r\nFor further information see the project web site at: \r\nhttp://homepage.cs.uiowa.edu/~tyng/dml.html.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tianbao",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tianbao Yang",
   "pi_email_addr": "tianbao-yang@tamu.edu",
   "nsf_id": "000678293",
   "pi_start_date": "2015-03-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Iowa",
  "inst_street_address": "105 JESSUP HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IOWA CITY",
  "inst_state_code": "IA",
  "inst_state_name": "Iowa",
  "inst_phone_num": "3193352123",
  "inst_zip_code": "522421316",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IA01",
  "org_lgl_bus_name": "THE UNIVERSITY OF IOWA",
  "org_prnt_uei_num": "",
  "org_uei_num": "Z1H9VJS8NG16"
 },
 "perf_inst": {
  "perf_inst_name": "University of Iowa",
  "perf_str_addr": "2 Gilmore Hall",
  "perf_city_name": "Iowa",
  "perf_st_code": "IA",
  "perf_st_name": "Iowa",
  "perf_zip_code": "522421320",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IA01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 174576.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project developed efficient and effective randomized reduction algorithms and stochastic optimization algorithms for solving large-scale high-dimensional problems in machine learning, in particular distance metric learning problems.&nbsp;</p>\n<p>&nbsp;</p>\n<p>The new randomized reduction methods have strong guarantee on the recovery error under milder conditions than previous work. In traditional random projection based methods, lower dimensional models are simply learned in the low dimensional space. The theoretical guarantee of these methods only holds when the data has some nice properties (e.g., linearly separable and low rank).&nbsp;&nbsp;The proposed methodology is to learn a recovered model in the original high-dimensional space by only using dimensionality-reduced data. By modifying the optimization formulations, the obtained solution has much stronger guarantee under mild conditions.&nbsp;</p>\n<p>&nbsp;</p>\n<p>The new stochastic optimization algorithms can enjoy much faster convergence than existing algorithms by leveraging the problem&rsquo;s local geometric properties. Existing theories of stochastic optimization algorithms rely on stronger conditions such as smoothness and strong convexity for deriving faster convergence. However, many practical problems do not have such strong conditions. We propose new algorithms to leverage generic conditions for mostly seen problems in machine learning, namely local growth conditions, and establish faster convergence of these new algorithms. In addition, we developed new algorithms with a reduced number of projections and with faster rates for solving complex constrained problems under local growth conditions.&nbsp;</p>\n<p>&nbsp;</p>\n<p>These results have been disseminated to the community through publications, tutorials, seminars, workshops, codes and course materials. Twenty-four papers were published on top machine learning venues including ICML, NIPS, AAAI, IJCAI, KDD, AISTATS, COLT, ALT. Two tutorials were successfully delivered to SIAM Knowledge Discovery and Data Mining Conference and Asian Conference on Machine Learning. Workshops on optimization with local growth conditions were organized on INFORMS and SIAM Optimization Conference. Codes were made available on the project website.&nbsp;<span>One graduate student used this project as his thesis topic. Two high-school students were involved in this project during two summers.&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/22/2018<br>\n\t\t\t\t\tModified by: Tianbao&nbsp;Yang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project developed efficient and effective randomized reduction algorithms and stochastic optimization algorithms for solving large-scale high-dimensional problems in machine learning, in particular distance metric learning problems. \n\n \n\nThe new randomized reduction methods have strong guarantee on the recovery error under milder conditions than previous work. In traditional random projection based methods, lower dimensional models are simply learned in the low dimensional space. The theoretical guarantee of these methods only holds when the data has some nice properties (e.g., linearly separable and low rank).  The proposed methodology is to learn a recovered model in the original high-dimensional space by only using dimensionality-reduced data. By modifying the optimization formulations, the obtained solution has much stronger guarantee under mild conditions. \n\n \n\nThe new stochastic optimization algorithms can enjoy much faster convergence than existing algorithms by leveraging the problem?s local geometric properties. Existing theories of stochastic optimization algorithms rely on stronger conditions such as smoothness and strong convexity for deriving faster convergence. However, many practical problems do not have such strong conditions. We propose new algorithms to leverage generic conditions for mostly seen problems in machine learning, namely local growth conditions, and establish faster convergence of these new algorithms. In addition, we developed new algorithms with a reduced number of projections and with faster rates for solving complex constrained problems under local growth conditions. \n\n \n\nThese results have been disseminated to the community through publications, tutorials, seminars, workshops, codes and course materials. Twenty-four papers were published on top machine learning venues including ICML, NIPS, AAAI, IJCAI, KDD, AISTATS, COLT, ALT. Two tutorials were successfully delivered to SIAM Knowledge Discovery and Data Mining Conference and Asian Conference on Machine Learning. Workshops on optimization with local growth conditions were organized on INFORMS and SIAM Optimization Conference. Codes were made available on the project website. One graduate student used this project as his thesis topic. Two high-school students were involved in this project during two summers. \n\n \n\n\t\t\t\t\tLast Modified: 04/22/2018\n\n\t\t\t\t\tSubmitted by: Tianbao Yang"
 }
}