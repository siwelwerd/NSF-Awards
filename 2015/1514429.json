{
 "awd_id": "1514429",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Augmented Reality for Multiple People, Perspectives, Platforms, and Tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 802286.0,
 "awd_amount": 802286.0,
 "awd_min_amd_letter_date": "2015-08-19",
 "awd_max_amd_letter_date": "2017-08-14",
 "awd_abstract_narration": "This is a project to understand and improve how augmented reality can facilitate task performance by superimposing information in users' visual fields to help identify objects and features, direct actions, and provide spatial-temporal overviews and other perspectives. Augmented reality is a rapidly developing information display technology, using either head-mounted displays or handhelds incorporating cameras, to add information to the visual field of the user. The project will develop and test the best ways to facilitate performance with multiple people, perspectives, platforms, and tasks to convey the what, where, when, and how of action. The research will be conducted in the laboratory and in the field, including the historic El Barrio section of East Harlem.  Project software is expected to significantly improve the ability of residents and visitors, alone and in groups, to learn about, plan visits to, and explore the neighborhood.  In their personal and professional lives, people need to perform an enormous range of complex tasks, including navigation, maintenance, and assembly. Technology can make these tasks easier, and the use of augmented reality to interactively assist users by overlaying crucial missing information directly on a user's view is especially promising.\r\n\r\nEach of the tasks studied in this research entails an organized sequence of actions with respect to features or objects in the world. The features or objects may be hard to find or even be occluded. The actions may be complex. Work is increasingly collaborative, requiring coordination with others who have different perspectives. Each action can depend on previous and subsequent actions. The research will integrate work in cognitive science and computer science to develop principled approaches for using the transformative technology of augmented reality to assist people in navigation, maintenance, assembly, and related tasks. It will expand our abilities in understanding how people represent, transform, and communicate space and the actions in it; in designing instructions; in developing systems to assist users in assembly, maintenance, and navigation; and in designing stationary, mobile, and wearable user interfaces that use graphics, multimedia, and augmented reality, both indoors and outdoors. The scientific results will enlighten the study of the communication of objects, actions, and data, instructional design, and user interface design. The techniques and systems developed will inform the design of future systems that can aid the general public for educational and recreational ends, as well as systems that can assist people with auditory, visual, or physical impairments. Navigation, maintenance, and assembly are representative of many important daily tasks. Software created by the project will be adaptable to different situations and displays, and made available to the public as open source. The project will train students, and will be carried out in part through collaborative interdisciplinary projects in courses in computer science and cognitive science.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Feiner",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Steven K Feiner",
   "pi_email_addr": "feiner@cs.columbia.edu",
   "nsf_id": "000121899",
   "pi_start_date": "2015-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "500 West 120th St., 450 CS Bldg.",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 260275.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 267351.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 274660.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Tools that augment what we are seeing and doing are increasingly evident on the job, in museums, in schools, on the streets, and in homes and stores. These tools can enrich our experience by adding information. They can also guide our behavior, making whatever tasks we are doing easier and more fluent. We have selected for our collaborative research project tasks that are common both in everyday life and in work: understanding and navigating an environment, and assembling, maintaining, or repairing a device. Some of our research has been devoted to understanding the ways people perform those tasks unaided, carefully observing how gesture, language, diagrams, movements of the eyes, hands, and head, and action sequences work together to accomplish the tasks. The larger part of our work has been to design, develop, and test augmented reality research tools to make tasks more fluent and accurate.</p>\n<p><em>Augmented reality</em> (AR) combines virtual graphics and other media with our perception of the physical world, using devices ranging from smartphones to head-worn displays that will someday become commonplace. Our tool building has focused on visual perception, augmented through head-worn displays. The research tools that we have created are designed to help users to understand, formulate, and direct actions, facilitating performance involving multiple people, multiple perspectives, and multiple platforms. Our emphasis has been on conveying the what, where, when, and how of action, helping users plan and perform a range of tasks individually and collaboratively, whether the users are in the same location or different locations.</p>\n<p>Significant outcomes of this project have included ones that address collaborative AR for remote task assistance, AR to assist in manually orienting 3D objects, improved map navigation, an open-source software framework for developing collaborative AR user interfaces, and collaborative AR for visualizing urban data, each described briefly below. Our work has also given insight into the ways that actions themselves communicate and explain, as well as gestures and visualizations. These under-researched aspects of communication play critical roles in collaborative work</p>\n<p><strong>Collaborative AR for remote task assistance.</strong> We developed, evaluated, and demonstrated live at <em>SIGGRAPH 2017</em>, new approaches to allow a local maintenance technician in AR to be advised by a remote expert in AR or virtual reality (VR). Unlike commercial AR systems, the expert can create and precisely manipulate virtual copies of physical objects in the local environment, to refer to parts of those physical objects and to indicate complex actions on them.</p>\n<p><strong>AR to assist in manually orienting 3D objects.</strong> We developed and evaluated four 3D AR visualizations for guiding a user in rotating a physical object to a desired orientation, a crucial task in many assembly and repair scenarios. Our formal user study showed that a novel visualization technique we developed made it possible for users to perform rotation tasks more efficiently than the best known current visualizations.</p>\n<p><strong>Improved map navigation.</strong> We developed and evaluated approaches, presented at <em>CHI 2016</em> and <em>2018</em>, that significantly improve user navigation in interactive maps. One uses a personalized set of points of interest to help users better understand the distance and direction to a currently unseen target location. Another allows users to directly manipulate locations themselves to perform tasks better than current commercial software.&nbsp;</p>\n<p><strong>An open-source software framework for developing collaborative AR user interfaces.</strong> We developed and released for free public access Mercury Messaging (<a href=\"https://github.com/ColumbiaCGUI/MercuryMessaging\">https://github.com/ColumbiaCGUI/MercuryMessaging</a>), an open-source software framework. Mercury Messaging is implemented in Unity 3D and allows programmers to create AR (and VR) user interfaces that support rich interconnections among their components on a single computer or across multiple networked computers with significantly less effort than other approaches. We used Mercury Messaging in most of the research software implemented for this grant.</p>\n<p><strong>Collaborative AR for exploring urban data.</strong> We created, and demonstrated live at <em>SIGGRAPH 2018,</em> a novel AR research system that allows multiple users to collaboratively interact with live data from social media (e,g., Twitter and Yelp) or municipal databases (e.g., <span style=\"text-decoration: underline;\"><a href=\"https://opendata.cityofnewyork.us\">https://opendata.cityofnewyork.us</a></span>)&nbsp; in context of an immersive scale model of an urban environment.</p>\n<p>Broader impacts of this collaborative research project have included training eight PhD students (seven of whom graduated during the grant period) and over a dozen MS and undergrad students (two of whom have formed AR startups after participating in grant research: <a href=\"https://www.echoar.xyz/\">https://www.echoar.xyz/</a> and <a href=\"https://www.ravn.com/\">https://www.ravn.com/</a>), giving numerous presentations of our work to the general public and to researchers in other domains, and experimentally applying our research to other domains, including education, design, rehabilitation, and medicine.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/01/2020<br>\n\t\t\t\t\tModified by: Steven&nbsp;K&nbsp;Feiner</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2019/1514429/1514429_10389772_1577816149343_collaborativeARVRimageForNSF_POR_1514429-2019--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1514429/1514429_10389772_1577816149343_collaborativeARVRimageForNSF_POR_1514429-2019--rgov-800width.jpg\" title=\"Collaborative AR and VR for Remote Task Assistance\"><img src=\"/por/images/Reports/POR/2019/1514429/1514429_10389772_1577816149343_collaborativeARVRimageForNSF_POR_1514429-2019--rgov-66x44.jpg\" alt=\"Collaborative AR and VR for Remote Task Assistance\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Live demo of remote task assistance at SIGGRAPH 2017. Remote expert in VR instructs local technician in AR. Objects in technician\ufffds physical environment are tracked in 3D to create expert\ufffds virtual environment. Transparent virtual gear and connection to physical gear at left were placed by expert.</div>\n<div class=\"imageCredit\">\ufffd 2017 C. Elvezio, M. Sukan, O. Oda, B. Tversky, S. Feiner, Columbia University</div>\n<div class=\"imageSubmitted\">Steven&nbsp;K&nbsp;Feiner</div>\n<div class=\"imageTitle\">Collaborative AR and VR for Remote Task Assistance</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1514429/1514429_10389772_1577822331096_CroppedOrienting3D-FigureForNSF_POR_2019--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1514429/1514429_10389772_1577822331096_CroppedOrienting3D-FigureForNSF_POR_2019--rgov-800width.jpg\" title=\"AR visualization to assist in manually orienting 3D objects\"><img src=\"/por/images/Reports/POR/2019/1514429/1514429_10389772_1577822331096_CroppedOrienting3D-FigureForNSF_POR_2019--rgov-66x44.jpg\" alt=\"AR visualization to assist in manually orienting 3D objects\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A novel free-hand rotation-guidance AR visualization. The user must rotate the object such the two \ufffdhandles\ufffd rotate into their corresponding \ufffdrings\ufffd.  A formal study showed that this visualization helps users rotate an object to a target orientation more efficiently than previously known techniques.</div>\n<div class=\"imageCredit\">\ufffd 2016 M. Sukan, C. Elvezio, B. Tversky, S. Feiner, Columbia University</div>\n<div class=\"imageSubmitted\">Steven&nbsp;K&nbsp;Feiner</div>\n<div class=\"imageTitle\">AR visualization to assist in manually orienting 3D objects</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1514429/1514429_10389772_1577854141107_orientationVisualizationGlassNSF_POR_1514429-2019--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1514429/1514429_10389772_1577854141107_orientationVisualizationGlassNSF_POR_1514429-2019--rgov-800width.jpg\" title=\"Task assistance for lightweight single-display eyewear\"><img src=\"/por/images/Reports/POR/2019/1514429/1514429_10389772_1577854141107_orientationVisualizationGlassNSF_POR_1514429-2019--rgov-66x44.jpg\" alt=\"Task assistance for lightweight single-display eyewear\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A novel visualization designed for lightweight, small field-of-view, single-display eyewear (Google Glass) to show the user to rotate a tracked hand-held object to a target orientation. The inset shows the perceived size of the display relative to the user?s hand.</div>\n<div class=\"imageCredit\">(c) 2016 M. Sukan, C. Elvezio, B. Tversky, S. Feiner, Columbia University</div>\n<div class=\"imageSubmitted\">Steven&nbsp;K&nbsp;Feiner</div>\n<div class=\"imageTitle\">Task assistance for lightweight single-display eyewear</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1514429/1514429_10389772_1577855191338_collaborativeUrbanVisualization_NSF_POR_1514429--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1514429/1514429_10389772_1577855191338_collaborativeUrbanVisualization_NSF_POR_1514429--rgov-800width.jpg\" title=\"Collaborative urban visualization in AR\"><img src=\"/por/images/Reports/POR/2020/1514429/1514429_10389772_1577855191338_collaborativeUrbanVisualization_NSF_POR_1514429--rgov-66x44.jpg\" alt=\"Collaborative urban visualization in AR\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Users explore live geotagged data from social media (Yelp) and urban datasets (311 nonemergency call records from NYC Open Data) in context of an immersive scale model of the city. Each user has a copy of a shared pinboard (at right) on which they can copy and arrange records to perform tasks.</div>\n<div class=\"imageCredit\">(c) 2019 C. Elvezio, F. Ling, J.-S. Liu, B. Tversky, S. Feiner, Columbia University</div>\n<div class=\"imageSubmitted\">Steven&nbsp;K&nbsp;Feiner</div>\n<div class=\"imageTitle\">Collaborative urban visualization in AR</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nTools that augment what we are seeing and doing are increasingly evident on the job, in museums, in schools, on the streets, and in homes and stores. These tools can enrich our experience by adding information. They can also guide our behavior, making whatever tasks we are doing easier and more fluent. We have selected for our collaborative research project tasks that are common both in everyday life and in work: understanding and navigating an environment, and assembling, maintaining, or repairing a device. Some of our research has been devoted to understanding the ways people perform those tasks unaided, carefully observing how gesture, language, diagrams, movements of the eyes, hands, and head, and action sequences work together to accomplish the tasks. The larger part of our work has been to design, develop, and test augmented reality research tools to make tasks more fluent and accurate.\n\nAugmented reality (AR) combines virtual graphics and other media with our perception of the physical world, using devices ranging from smartphones to head-worn displays that will someday become commonplace. Our tool building has focused on visual perception, augmented through head-worn displays. The research tools that we have created are designed to help users to understand, formulate, and direct actions, facilitating performance involving multiple people, multiple perspectives, and multiple platforms. Our emphasis has been on conveying the what, where, when, and how of action, helping users plan and perform a range of tasks individually and collaboratively, whether the users are in the same location or different locations.\n\nSignificant outcomes of this project have included ones that address collaborative AR for remote task assistance, AR to assist in manually orienting 3D objects, improved map navigation, an open-source software framework for developing collaborative AR user interfaces, and collaborative AR for visualizing urban data, each described briefly below. Our work has also given insight into the ways that actions themselves communicate and explain, as well as gestures and visualizations. These under-researched aspects of communication play critical roles in collaborative work\n\nCollaborative AR for remote task assistance. We developed, evaluated, and demonstrated live at SIGGRAPH 2017, new approaches to allow a local maintenance technician in AR to be advised by a remote expert in AR or virtual reality (VR). Unlike commercial AR systems, the expert can create and precisely manipulate virtual copies of physical objects in the local environment, to refer to parts of those physical objects and to indicate complex actions on them.\n\nAR to assist in manually orienting 3D objects. We developed and evaluated four 3D AR visualizations for guiding a user in rotating a physical object to a desired orientation, a crucial task in many assembly and repair scenarios. Our formal user study showed that a novel visualization technique we developed made it possible for users to perform rotation tasks more efficiently than the best known current visualizations.\n\nImproved map navigation. We developed and evaluated approaches, presented at CHI 2016 and 2018, that significantly improve user navigation in interactive maps. One uses a personalized set of points of interest to help users better understand the distance and direction to a currently unseen target location. Another allows users to directly manipulate locations themselves to perform tasks better than current commercial software. \n\nAn open-source software framework for developing collaborative AR user interfaces. We developed and released for free public access Mercury Messaging (https://github.com/ColumbiaCGUI/MercuryMessaging), an open-source software framework. Mercury Messaging is implemented in Unity 3D and allows programmers to create AR (and VR) user interfaces that support rich interconnections among their components on a single computer or across multiple networked computers with significantly less effort than other approaches. We used Mercury Messaging in most of the research software implemented for this grant.\n\nCollaborative AR for exploring urban data. We created, and demonstrated live at SIGGRAPH 2018, a novel AR research system that allows multiple users to collaboratively interact with live data from social media (e,g., Twitter and Yelp) or municipal databases (e.g., https://opendata.cityofnewyork.us)  in context of an immersive scale model of an urban environment.\n\nBroader impacts of this collaborative research project have included training eight PhD students (seven of whom graduated during the grant period) and over a dozen MS and undergrad students (two of whom have formed AR startups after participating in grant research: https://www.echoar.xyz/ and https://www.ravn.com/), giving numerous presentations of our work to the general public and to researchers in other domains, and experimentally applying our research to other domains, including education, design, rehabilitation, and medicine.\n\n \n\n\t\t\t\t\tLast Modified: 01/01/2020\n\n\t\t\t\t\tSubmitted by: Steven K Feiner"
 }
}