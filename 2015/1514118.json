{
 "awd_id": "1514118",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Learning to Summarize User-Generated Video",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 547005.0,
 "awd_amount": 547005.0,
 "awd_min_amd_letter_date": "2015-06-09",
 "awd_max_amd_letter_date": "2021-06-02",
 "awd_abstract_narration": "Today there is far more video being captured - by consumers, scientists, defense analysts, and others - than can ever be watched.  With this explosion of video data comes a pressing need to develop automatic video summarization algorithms.  Video summarization takes a long video as input and produces a short video as output, while preserving its information content as much as possible.   As such, summarization techniques have great potential to make large video collections substantially more efficient to browse, search, disseminate, and facilitate communication.  Such increased efficiency will play a vital role in many important application areas.  For example, with reliable summarization systems, a primatologist gathering long videos of her animal subjects could quickly browse a week's worth of their activity before deciding where to inspect the data most closely.  A young student searching YouTube to learn about Yellowstone National Park could see at a glance what content exists, much better than today's simple thumbnail images can depict.  An intelligence agent could rapidly sift through reams of aerial video, reducing the resources required to analyze surveillance data to identify suspicious activities.\r\n\r\nThis project develops new machine learning and computer vision algorithms for video summarization.  Unsupervised methods, which are the cornerstone of nearly all existing approaches, have become increasingly limiting due to their reliance on hand-crafted heuristics.  By instead posing video summarization as a supervised learning problem, this project investigates a markedly different formulation of the task.  The research team is investigating four key new ideas: powerful probabilistic models for learning to select the optimal subset of video frames for summarization, semi-supervised learning models and co-summarization algorithms for leveraging the abundance of multiple related videos, algorithms for exploiting photos on the Web to improve summarization, and evaluation protocols that assess summaries in a way that aligns well with human comprehension.  The broader impact of the proposed research includes practical tools for video summarization, scientific advances that appeal broadly to several communities, publicly disseminated research results, inter-disciplinarily trained graduate students, and outreach activities to engage young students in STEM education and career paths.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kristen",
   "pi_last_name": "Grauman",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Kristen L Grauman",
   "pi_email_addr": "grauman@cs.utexas.edu",
   "nsf_id": "000282504",
   "pi_start_date": "2015-06-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Texas at Austin",
  "perf_str_addr": "101 E. 27th Street, Stop A9000",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121532",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 137404.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 133661.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 275940.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Research outcomes:&nbsp;</strong></p>\n<p>This project investigated how to automatically summarize videos: given a long video as input, can computer vision models learn to extract a concise visual summary?&nbsp; The research outcomes are new machine learning models that can create summaries for in-the-wild user-generated video.&nbsp; In particular, we addressed two sides of the summarization problem: ``when to look\" and ``where to look\".&nbsp; For the former, we devised algorithms to automatically determine what frames or clips can be dropped to reduce the original video's length without harming its content for a human observer.&nbsp; For the latter, we showed how to identify the most informative spatial sections in an omnidirectional (360) video, to allow pointing a virtual camera in the direction most useful for a human observer to watch.&nbsp; To enable these outcomes, we made technical advances on probabilistic models to select an optimal subset from a larger set (introducing formulations with determinantal point processes (DPP), non-parametric supervised models, and structured prediction sequence-to-sequence recurrent neural networks); novel models to discover highlights and informative directions from noisy user-generated videos; and learning-based approaches to accommodate 360 degree videos with convolutional neural networks pre-trained for perspective imagery, greatly improving scalability for summarization, compression, and other visual tasks.&nbsp;</p>\n<p>Later in the project we expanded our scope to address fine-grained visual recognition challenges.&nbsp; Our main outcomes on this front were visual question answering and assistive shopping systems for visually impaired or blind users, a large-scale anthropology study exploring how tourism intersects with heritage regulations and social media, and discovery of clothing trends over decades of in-the-wild photos.&nbsp; Those efforts were enabled by our new perception models to detect subtle differences and find repeated style patterns in large photo collections.</p>\n<p>&nbsp;</p>\n<p><strong>Intellectual merit:</strong></p>\n<p>Video summarization has shifted into new territory, owing to the explosion of video data of many different types. From user-generated videos to professionally edited content, the complexity and heterogeneity poses many new challenges. Unsupervised methods, which were the cornerstone of nearly all prior approaches, have become increasingly limiting due to their reliance on hand-crafted heuristics.&nbsp; By posing video summarization as a supervised and semi-supervised learning problem, this project provided a markedly different formulation of the summarization task. Our research advanced the state of the art in machine learning and computer vision, as demonstrated in the various top-tier publications resulting from this project.&nbsp; Our contributions include several new machine learning models that are applicable both for the desired task of user-generated video summarization, as well as more broadly for other sequential subset selection problems.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Broader impact:</strong></p>\n<p>The project promoted the training and growth of Ph.D. students, undergraduate students, and high school students, and the outreach components contributed to widening participation in computer science, with particular emphasis on reaching female students.</p>\n<p>In our research, there is broader impact on three fronts.&nbsp; First, our work addressed the urgent need to develop methods for summarizing user-generated videos -- often mobile phone or wearable camera video -- which have become the most common video genre on public sharing sites and private exchanges. Our research provides practical tools for video summarization, which will greatly facilitate (video) information browsing, search, dissemination, and communication.&nbsp; Applications of reliable summarization systems with societal impact are abundant. Examples include a primatologist gathering long videos of her animal subjects, who could quickly browse a week's&nbsp; worth of their activity before deciding where to inspect the data most closely. A young student searching YouTube to learn about Yellowstone National Park could see at a glance what content exists, much better than today's simple thumbnail images can depict. A grandparent could navigate endless videos of the grandchildren, while an intelligent agent could rapidly sift through reams of aerial video.&nbsp; Second, our efforts building assistive technology with computer vision for blind and visually impaired users can have impact on how users access important visual data online.&nbsp; The VizWiz question answering and BrowseWithMe fashion shopping systems we prototyped are steps towards enabling visually impaired and blind users to have fuller perceptual experiences in their daily lives.&nbsp; Third, our work modeling the influence of world events on what people choose to wear has potential impact on anthropology and social studies.&nbsp; Our work shows how to automatically discover the links between cultural events and clothing, a step towards a computational, scalable, and easily refreshable way to understand how external factors affect the clothes we wear.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/15/2023<br>\n\t\t\t\t\tModified by: Kristen&nbsp;L&nbsp;Grauman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nResearch outcomes: \n\nThis project investigated how to automatically summarize videos: given a long video as input, can computer vision models learn to extract a concise visual summary?  The research outcomes are new machine learning models that can create summaries for in-the-wild user-generated video.  In particular, we addressed two sides of the summarization problem: ``when to look\" and ``where to look\".  For the former, we devised algorithms to automatically determine what frames or clips can be dropped to reduce the original video's length without harming its content for a human observer.  For the latter, we showed how to identify the most informative spatial sections in an omnidirectional (360) video, to allow pointing a virtual camera in the direction most useful for a human observer to watch.  To enable these outcomes, we made technical advances on probabilistic models to select an optimal subset from a larger set (introducing formulations with determinantal point processes (DPP), non-parametric supervised models, and structured prediction sequence-to-sequence recurrent neural networks); novel models to discover highlights and informative directions from noisy user-generated videos; and learning-based approaches to accommodate 360 degree videos with convolutional neural networks pre-trained for perspective imagery, greatly improving scalability for summarization, compression, and other visual tasks. \n\nLater in the project we expanded our scope to address fine-grained visual recognition challenges.  Our main outcomes on this front were visual question answering and assistive shopping systems for visually impaired or blind users, a large-scale anthropology study exploring how tourism intersects with heritage regulations and social media, and discovery of clothing trends over decades of in-the-wild photos.  Those efforts were enabled by our new perception models to detect subtle differences and find repeated style patterns in large photo collections.\n\n \n\nIntellectual merit:\n\nVideo summarization has shifted into new territory, owing to the explosion of video data of many different types. From user-generated videos to professionally edited content, the complexity and heterogeneity poses many new challenges. Unsupervised methods, which were the cornerstone of nearly all prior approaches, have become increasingly limiting due to their reliance on hand-crafted heuristics.  By posing video summarization as a supervised and semi-supervised learning problem, this project provided a markedly different formulation of the summarization task. Our research advanced the state of the art in machine learning and computer vision, as demonstrated in the various top-tier publications resulting from this project.  Our contributions include several new machine learning models that are applicable both for the desired task of user-generated video summarization, as well as more broadly for other sequential subset selection problems. \n\n \n\nBroader impact:\n\nThe project promoted the training and growth of Ph.D. students, undergraduate students, and high school students, and the outreach components contributed to widening participation in computer science, with particular emphasis on reaching female students.\n\nIn our research, there is broader impact on three fronts.  First, our work addressed the urgent need to develop methods for summarizing user-generated videos -- often mobile phone or wearable camera video -- which have become the most common video genre on public sharing sites and private exchanges. Our research provides practical tools for video summarization, which will greatly facilitate (video) information browsing, search, dissemination, and communication.  Applications of reliable summarization systems with societal impact are abundant. Examples include a primatologist gathering long videos of her animal subjects, who could quickly browse a week's  worth of their activity before deciding where to inspect the data most closely. A young student searching YouTube to learn about Yellowstone National Park could see at a glance what content exists, much better than today's simple thumbnail images can depict. A grandparent could navigate endless videos of the grandchildren, while an intelligent agent could rapidly sift through reams of aerial video.  Second, our efforts building assistive technology with computer vision for blind and visually impaired users can have impact on how users access important visual data online.  The VizWiz question answering and BrowseWithMe fashion shopping systems we prototyped are steps towards enabling visually impaired and blind users to have fuller perceptual experiences in their daily lives.  Third, our work modeling the influence of world events on what people choose to wear has potential impact on anthropology and social studies.  Our work shows how to automatically discover the links between cultural events and clothing, a step towards a computational, scalable, and easily refreshable way to understand how external factors affect the clothes we wear. \n\n\t\t\t\t\tLast Modified: 02/15/2023\n\n\t\t\t\t\tSubmitted by: Kristen L Grauman"
 }
}