{
 "awd_id": "1540216",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative: Secure and Efficient Data Provenance",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Deborah Shands",
 "awd_eff_date": "2014-08-15",
 "awd_exp_date": "2017-03-31",
 "tot_intn_awd_amt": 110075.0,
 "awd_amount": 110075.0,
 "awd_min_amd_letter_date": "2015-05-18",
 "awd_max_amd_letter_date": "2015-05-18",
 "awd_abstract_narration": "Data provenance involves determining the conditions under which information was originally generated, as well as all subsequent modifications to that information and the conditions under which those modifications were performed. As systems become increasingly distributed and organizations become reliant on cloud computing for processing their data, the need to securely manage and validate the provenance of that data becomes critical.\r\n\r\nThis project develops new frameworks for evaluating secure fine-grained provenance collection and management in hosts.  The research activities examine the architectures and algorithms required to make the collection and management of trustworthy provenance feasible at scale. We provide a general architecture for collecting provenance at the kernel level and consider methods of reducing the amount of provenance generated and managed while maintaining high-fidelity provenance records.  \r\n\r\nThe project introduces a number of optimizations to enable scalable and performant provenance collection, including policy-based log reduction and provenance deduplication.  The project's main scientific contributions include (1) the development of Linux Provenance Modules that provide complete provenance mediation within the Linux kernel; (2) the design of policy-reduced provenance through the use of mandatory access control policies to reduce the number of subjects and actions that are provenance-generating events to those of interest in a system; and (3) techniques for deduplication of provenance such that minimal records for commonly occurring events can be stored and later fully reconstructed.  We will demonstrate that our approach makes provenance collection practical at scale, enabling more secure and trustworthy computing environments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Butler",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kevin Butler",
   "pi_email_addr": "butler@cise.ufl.edu",
   "nsf_id": "000573521",
   "pi_start_date": "2015-05-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Florida",
  "inst_street_address": "1523 UNION RD RM 207",
  "inst_street_address_2": "",
  "inst_city_name": "GAINESVILLE",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "3523923516",
  "inst_zip_code": "326111941",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "FL03",
  "org_lgl_bus_name": "UNIVERSITY OF FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NNFQH1JAPEP3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Florida",
  "perf_str_addr": "1 UNIVERSITY OF FLORIDA",
  "perf_city_name": "GAINESVILLE",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "326112002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "FL03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 110075.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this work is to design and develop new frameworks for evaluating secure fine-grained provenance collection and management in hosts. The research activities examine the architectures and algorithms required to make the collection and management of trustworthy provenance feasible at scale. We plan to provide a general architecture for collecting provenance at the kernel level and consider methods of reducing the amount of provenance generated and managed while maintaining high-fidelity provenance records. The goal is to demonstrate that our proposed techniques makes provenance collection practical at scale, enabling more secure and trustworthy computing environments.</p>\n<p>One of the main outcomes of this work was the development of the Linux Provenance Modules (LPM) framework for fine-grained provenance collection within the Linux kernel. We found important operations that we instrumented to be able to better understand the operations inside a computer, as our system allows the tracing of data as well as the lineage of that data. We have designed a complete set of LPM hooks and as a proof of concept, and integrated many other provenance systems to demonstrate its flexibility. &nbsp;Additionally, we developed a data-loss prevention application to show how provenance can improve system security.</p>\n<p>Collecting fine-grained provenance automatically incurs a substantial amount of storage overhead. We found that a 10-minute long kernel compilation recorded with LPM garnered almost 5 GB of data. This is clearly unsustainable in order for provenance to be manageable. Past approaches to provenance compression and even de-duplication have ignored a root cause of storage overhead, that being that many provenance events are simply&nbsp;<em>uninteresting</em>. Regular output from well-defined processes do not inform insight in the system, and there are other ways to support their security, e.g., through type enforcement. We therefore looked at the idea of using mandatory access control policies to define a&nbsp;<em>provenance wall</em>&nbsp;for the system so that only processes capturing information that we would need to track and reconstitute would be subject to provenance collection. We developed new formal mechanisms for defining the notion of selective and minimal completeness with regards to the collection of provenance, where selectively completeness represents a subgraph of a complete provenance graph defined as all nodes reachable from the nodes of interest, while a minimally complete provenance graph is a subgraph of the complete graph that has full provenance for all nodes of interest and no provenance for all remaining nodes that are not of interest. We provide guarantees about completeness in the presence of mandatory access control policies and notions of tamperproofness based on the use of such policies and a trusted computing base. Our results show that based on the application being instrumented, we can reduce provenance overheads by up to 90%. &nbsp;Additionally, we instrumented web service application workflows with provenance mechanisms allowing us to audit and better determine the root causes of remote exploits. This work can help make cloud computing safer by allowing tracing of activities within that environment.&nbsp;</p>\n<p>Other efforts have led to developing provenance techniques that can attest to the authenticity of HCI data. We demonstrated how garbled speech could be constructed such that it could be understood by machine speech recognition systems but not by humans. Expanding on these early results, we studied mechanisms to determine the provenance (i.e., originators) of voice commands issued to personal digital assistants (e.g., Siri and Google Now) and other devices (e.g., wearables, Amazon Echo, etc.). We showed that defenses against our \"hidden voice commands\" are possible; specifically, we introduce techniques based on machine learning for determining the true provenance of interpreted commands.</p>\n<p>We considered provenance in the context of enforcing policy within cyber-physical environments, specifically the electrical smart grid. Access control policies become more complicated when interaction of different operations cannot be detected until runtime, as is the case in these systems. To address this problem, we built a policy engine to support controlling policy based on the provenance of the request. We leverage the time, location, events, and provenance of the data to check for permissions and enforce these in a Prolog-based logic system. This mechasnim was designed as part of a system for securing the smart grid against attacks.</p>\n<p>The Linux Provenance Modules work and our other efforts have garnered significant attention within the provenance community, with the Hidden Voice Commands work garnering significant media attention. National laboratories are using LPM and the PIs are working with industry to transition their efforts into other areas such as securing infrastructure and the Internet of Things. This work supported a PhD thesis and other graduate student efforts. It resulted in 13 peer-reviewed publications through workshops, conferences, journal articles, and a book chapter. These effort have substantially moved the bar forward in building systems to better understand and track data through systems as well as understanding limitations of these approaches.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/12/2017<br>\n\t\t\t\t\tModified by: Kevin&nbsp;Butler</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this work is to design and develop new frameworks for evaluating secure fine-grained provenance collection and management in hosts. The research activities examine the architectures and algorithms required to make the collection and management of trustworthy provenance feasible at scale. We plan to provide a general architecture for collecting provenance at the kernel level and consider methods of reducing the amount of provenance generated and managed while maintaining high-fidelity provenance records. The goal is to demonstrate that our proposed techniques makes provenance collection practical at scale, enabling more secure and trustworthy computing environments.\n\nOne of the main outcomes of this work was the development of the Linux Provenance Modules (LPM) framework for fine-grained provenance collection within the Linux kernel. We found important operations that we instrumented to be able to better understand the operations inside a computer, as our system allows the tracing of data as well as the lineage of that data. We have designed a complete set of LPM hooks and as a proof of concept, and integrated many other provenance systems to demonstrate its flexibility.  Additionally, we developed a data-loss prevention application to show how provenance can improve system security.\n\nCollecting fine-grained provenance automatically incurs a substantial amount of storage overhead. We found that a 10-minute long kernel compilation recorded with LPM garnered almost 5 GB of data. This is clearly unsustainable in order for provenance to be manageable. Past approaches to provenance compression and even de-duplication have ignored a root cause of storage overhead, that being that many provenance events are simply uninteresting. Regular output from well-defined processes do not inform insight in the system, and there are other ways to support their security, e.g., through type enforcement. We therefore looked at the idea of using mandatory access control policies to define a provenance wall for the system so that only processes capturing information that we would need to track and reconstitute would be subject to provenance collection. We developed new formal mechanisms for defining the notion of selective and minimal completeness with regards to the collection of provenance, where selectively completeness represents a subgraph of a complete provenance graph defined as all nodes reachable from the nodes of interest, while a minimally complete provenance graph is a subgraph of the complete graph that has full provenance for all nodes of interest and no provenance for all remaining nodes that are not of interest. We provide guarantees about completeness in the presence of mandatory access control policies and notions of tamperproofness based on the use of such policies and a trusted computing base. Our results show that based on the application being instrumented, we can reduce provenance overheads by up to 90%.  Additionally, we instrumented web service application workflows with provenance mechanisms allowing us to audit and better determine the root causes of remote exploits. This work can help make cloud computing safer by allowing tracing of activities within that environment. \n\nOther efforts have led to developing provenance techniques that can attest to the authenticity of HCI data. We demonstrated how garbled speech could be constructed such that it could be understood by machine speech recognition systems but not by humans. Expanding on these early results, we studied mechanisms to determine the provenance (i.e., originators) of voice commands issued to personal digital assistants (e.g., Siri and Google Now) and other devices (e.g., wearables, Amazon Echo, etc.). We showed that defenses against our \"hidden voice commands\" are possible; specifically, we introduce techniques based on machine learning for determining the true provenance of interpreted commands.\n\nWe considered provenance in the context of enforcing policy within cyber-physical environments, specifically the electrical smart grid. Access control policies become more complicated when interaction of different operations cannot be detected until runtime, as is the case in these systems. To address this problem, we built a policy engine to support controlling policy based on the provenance of the request. We leverage the time, location, events, and provenance of the data to check for permissions and enforce these in a Prolog-based logic system. This mechasnim was designed as part of a system for securing the smart grid against attacks.\n\nThe Linux Provenance Modules work and our other efforts have garnered significant attention within the provenance community, with the Hidden Voice Commands work garnering significant media attention. National laboratories are using LPM and the PIs are working with industry to transition their efforts into other areas such as securing infrastructure and the Internet of Things. This work supported a PhD thesis and other graduate student efforts. It resulted in 13 peer-reviewed publications through workshops, conferences, journal articles, and a book chapter. These effort have substantially moved the bar forward in building systems to better understand and track data through systems as well as understanding limitations of these approaches.\n\n\t\t\t\t\tLast Modified: 07/12/2017\n\n\t\t\t\t\tSubmitted by: Kevin Butler"
 }
}