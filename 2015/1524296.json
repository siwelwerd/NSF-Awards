{
 "awd_id": "1524296",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TWC: Small: Designing a Coordination Mechanism for Managing Privacy as a Common-Pool Resource",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 463107.0,
 "awd_amount": 463107.0,
 "awd_min_amd_letter_date": "2015-08-14",
 "awd_max_amd_letter_date": "2020-06-23",
 "awd_abstract_narration": "Ubiquitous computing technologies such as \"smart\" door locks, thermostats, fitness trackers and video monitors can help make users' lives safer and more efficient. These devices automatically collect data about users and their activities within their homes, which are then combined and processed by algorithms on a cloud server owned by the service provider. This enables beneficial system functionality that would not be possible from the devices in isolation. However, aggregating data from different points in time and about many different devices and users can also produce potentially invasive insights and inferences about individuals and households that can be surprising, unsettling or harmful when used for purposes users do not expect. This project develops a coordination mechanism for users to jointly manage derived data as a common pool resource. This shifts privacy problems from the current up-front decisions about what to disclose toward a collective governance model that supports updating disclosure decisions as technology and social norms evolve.\r\n\r\nThis project will investigate norms for acceptable uses of derived data, as well as develop and evaluate tools to support collective privacy management decisions. The social norm studies include semi-structured interviews to identify norms, and validation experiments involving simulated norms violation and responses.  Building on frameworks for analyzing social-ecological common pool resource systems, the project will perform iterative design and prototyping of a privacy coordination mechanism based on home automation systems. The system will be installed and evaluated in a real-world test to evaluate effectiveness and usability, as well as qualitative analysis of unexpected events.\r\n\r\nMore information is available on the project website at: https://bitlab.cas.msu.edu/privacy/",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Emilee",
   "pi_last_name": "Rader",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Emilee J Rader",
   "pi_email_addr": "ejrader2@wisc.edu",
   "nsf_id": "000580969",
   "pi_start_date": "2015-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Michigan State University",
  "inst_street_address": "426 AUDITORIUM RD RM 2",
  "inst_street_address_2": "",
  "inst_city_name": "EAST LANSING",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "5173555040",
  "inst_zip_code": "488242600",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MI07",
  "org_lgl_bus_name": "MICHIGAN STATE UNIVERSITY",
  "org_prnt_uei_num": "VJKZC4D1JN36",
  "org_uei_num": "R28EKN92ZTZ9"
 },
 "perf_inst": {
  "perf_inst_name": "Michigan State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "488242600",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MI07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 463107.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Data collected by sensors in technologies like mobile phones, \"internet of things\" (IoT) devices and automobiles can be analyzed to produce \"derived data\": new insights and inferences about people that can be surprising, unsettling or harmful when used for unexpected purposes. The purpose of this project was to identify social norms for the use of derived data in sensor-enabled computing systems, and to design and evaluate ways to help people coordinate to manage derived data according to their privacy preferences. We identified three things that need to happen in order for collective privacy management to be viable.</p>\n<p>1) People who use sensor-based technologies need to know they are not alone in their privacy concerns.</p>\n<p>We discovered that social norms exist for collection and use of derived data. For example, people had strong negative reactions to hypothetical scenarios involving a mobile device that can infer how many times they went to the bathroom based on location and movement data, or how healthy they are based on the recipes stored in a cookbook app. These reactions were rooted in norms about how it is rude to discuss bathroom behavior and the need to appear to make healthy food choices to avoid negative judgments from others. We also found evidence of norms related to privacy-preserving behaviors: many people we interviewed across multiple studies commented that taking steps to limit one's use of technologies is something that only \"crazy\" or \"paranoid\" people do. This suggests that a phenomenon called \"pluralistic ignorance\" may be happening with respect to digital privacy. Pluralistic ignorance happens when people engage in behaviors they privately do not believe in, but they do it anyway because it appears that everyone else does it. Our research suggests that pluralistic ignorance may be a significant barrier to collective privacy management, as the pervasive use of sensor-enabled technologies is highly visible but information about people's privacy-related concerns and motivations is hidden.</p>\n<p>2) People need more information about how inferences and derived data are actually being used and shared.</p>\n<p>Another barrier to collective privacy management is that constraints limit people's interpretation of what inferences about them mean and how they could be used. Participants in our studies were actually often aware of the kinds of data that can be collected about their behavior by the systems they use. But, the inferences they thought were plausible were limited to what they remembered about their own past experiences and behavior, and what the user interface of the system showed them about the data collection. For example, ad inferences categories assigned by Google made the most sense to people when those inferences were clearly related to past web searches they remembered doing. In another study, participants interpreted an inference about their driving style based on driving data from their cars to reflect positively on their driving behavior, regardless of what the underlying data looked like in comparison to other drivers. \"Defensive\" driving was interpreted to mean driving safely and avoiding endangering others, whereas \"assertive\" driving meant taking taking quick, decisive action and being proactive in avoiding potential accidents. These constraints mean that inferences that are more easily anticipated and understood are often likely to be viewed as obvious and benign, making it unlikely that users of sensor-enabled systems would try to protect their privacy when using these systems.</p>\n<p>3) People need help speculating about and anticipating the potential harms to themselves and others from the uses of derived data.</p>\n<p>Finally, this project identified an underexplored problem with the assumption that people behave in a self-contradictory manner when they say privacy is important to them, while at the same time choosing to use technologies that collect data about them. In our research we identified six fundamental human values that underlie people's technology use. Three of the values (self-direction, conformity, and face) present in their reactions to hypothetical scenarios in which inferences were made supported goals related to controlling access to information about oneself. But, while people could envision specific and concrete ways the system would help them achieve success (achievement value) or avoid threats (security value) they had only vague ideas about what harmful consequences could result from losing control over the information. This means that even when privacy is a desirable outcome consistent with people's values, it is difficult for them to imagine that organizations would use the data they collect in ways that contradict people's goals for using the technologies in the first place.</p>\n<p>This project also trained 1 postdoc, 5 graduate students, and 5 undergraduate students, including both computer science students and social science students, most of whom are from under-represented groups. Our training includes helping the students learn how to communicate across disciplines and work together on a interdisciplinary team.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/09/2021<br>\n\t\t\t\t\tModified by: Emilee&nbsp;J&nbsp;Rader</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nData collected by sensors in technologies like mobile phones, \"internet of things\" (IoT) devices and automobiles can be analyzed to produce \"derived data\": new insights and inferences about people that can be surprising, unsettling or harmful when used for unexpected purposes. The purpose of this project was to identify social norms for the use of derived data in sensor-enabled computing systems, and to design and evaluate ways to help people coordinate to manage derived data according to their privacy preferences. We identified three things that need to happen in order for collective privacy management to be viable.\n\n1) People who use sensor-based technologies need to know they are not alone in their privacy concerns.\n\nWe discovered that social norms exist for collection and use of derived data. For example, people had strong negative reactions to hypothetical scenarios involving a mobile device that can infer how many times they went to the bathroom based on location and movement data, or how healthy they are based on the recipes stored in a cookbook app. These reactions were rooted in norms about how it is rude to discuss bathroom behavior and the need to appear to make healthy food choices to avoid negative judgments from others. We also found evidence of norms related to privacy-preserving behaviors: many people we interviewed across multiple studies commented that taking steps to limit one's use of technologies is something that only \"crazy\" or \"paranoid\" people do. This suggests that a phenomenon called \"pluralistic ignorance\" may be happening with respect to digital privacy. Pluralistic ignorance happens when people engage in behaviors they privately do not believe in, but they do it anyway because it appears that everyone else does it. Our research suggests that pluralistic ignorance may be a significant barrier to collective privacy management, as the pervasive use of sensor-enabled technologies is highly visible but information about people's privacy-related concerns and motivations is hidden.\n\n2) People need more information about how inferences and derived data are actually being used and shared.\n\nAnother barrier to collective privacy management is that constraints limit people's interpretation of what inferences about them mean and how they could be used. Participants in our studies were actually often aware of the kinds of data that can be collected about their behavior by the systems they use. But, the inferences they thought were plausible were limited to what they remembered about their own past experiences and behavior, and what the user interface of the system showed them about the data collection. For example, ad inferences categories assigned by Google made the most sense to people when those inferences were clearly related to past web searches they remembered doing. In another study, participants interpreted an inference about their driving style based on driving data from their cars to reflect positively on their driving behavior, regardless of what the underlying data looked like in comparison to other drivers. \"Defensive\" driving was interpreted to mean driving safely and avoiding endangering others, whereas \"assertive\" driving meant taking taking quick, decisive action and being proactive in avoiding potential accidents. These constraints mean that inferences that are more easily anticipated and understood are often likely to be viewed as obvious and benign, making it unlikely that users of sensor-enabled systems would try to protect their privacy when using these systems.\n\n3) People need help speculating about and anticipating the potential harms to themselves and others from the uses of derived data.\n\nFinally, this project identified an underexplored problem with the assumption that people behave in a self-contradictory manner when they say privacy is important to them, while at the same time choosing to use technologies that collect data about them. In our research we identified six fundamental human values that underlie people's technology use. Three of the values (self-direction, conformity, and face) present in their reactions to hypothetical scenarios in which inferences were made supported goals related to controlling access to information about oneself. But, while people could envision specific and concrete ways the system would help them achieve success (achievement value) or avoid threats (security value) they had only vague ideas about what harmful consequences could result from losing control over the information. This means that even when privacy is a desirable outcome consistent with people's values, it is difficult for them to imagine that organizations would use the data they collect in ways that contradict people's goals for using the technologies in the first place.\n\nThis project also trained 1 postdoc, 5 graduate students, and 5 undergraduate students, including both computer science students and social science students, most of whom are from under-represented groups. Our training includes helping the students learn how to communicate across disciplines and work together on a interdisciplinary team.\n\n\t\t\t\t\tLast Modified: 12/09/2021\n\n\t\t\t\t\tSubmitted by: Emilee J Rader"
 }
}