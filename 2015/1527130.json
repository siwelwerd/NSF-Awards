{
 "awd_id": "1527130",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Collaborative Research:Synchronization and Deduplication of Distributed Coded Data: Fundamental Limits and Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2015-06-29",
 "awd_max_amd_letter_date": "2015-06-29",
 "awd_abstract_narration": "Part 1: Coding for distributed storage systems has garnered significant attention in the past few years due to the rapid development of information technologies and the emergence of Big Data formats that need to be stored and disseminated across large-scale networks. As typical distributed systems need to ensure low-latency data access and store a large number of files over a set of nodes connected through a communication network, it is imperative to develop new distributed coding schemes that protect the systems from undesired component failures. The two key functionalities of codes used in distributed systems, namely the reconstruction of files via access to a subset of the nodes and repair of failed nodes, need to be retained when the files are accessed and processed by the users via symbol/block insertion, deletion, or substitution edits. Deletions frequently arise due to system-level data deduplication: when parts of files are deduplicated or edited, the changes in the information content need to be communicated to the redundant storage nodes with minimum communication cost. Current solutions for synchronizing data that underwent edits assume that data is uncoded and they do not fully exploit the distributed nature of information. Furthermore, they mostly ignore the presence of deduplication protocols. This makes distributed storage architectures inefficient in terms of storage, user access times, and error protection. Hence, the goals of the proposed research program are to develop a new set of protocols and coding schemes that will  support a new generation of versatile and updatable coded distributed storage systems. \r\n\r\nPart 2: Building on the preliminary work of the investigators, this proposal aims to set the foundations of the new field of coded synchronization and deduplication, with the goal of deriving fundamental performance limits, developing efficient algorithmic solutions for the two families of problems, and constructing new distributed storage codes that enable synchronization of coded data and coded deduplication. In particular, the proposal addresses the following comprehensive issues: \r\n1) Characterizing the communication rate limits of known and new (un)coded synchronization schemes, trade-offs between deduplication and data repair performance for different structured or encoded data formats and different types of communication channels.\r\n2) Introducing and analyzing the communication rate-distortion (CRD) function for approximate synchronization and deduplication of structured/encoded data, with a special focus on delay-sensitive applications.\r\n3) Developing dynamically updatable synchronization and deduplication algorithms cognizant of the network topology and of different prioritization needs of the users, as encountered in image and video data coding.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lara",
   "pi_last_name": "Dolecek",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lara Dolecek",
   "pi_email_addr": "dolecek@ee.ucla.edu",
   "nsf_id": "000541683",
   "pi_start_date": "2015-06-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900952000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Data is growing at a staggering pace, yet the tools for efficiently managing seemingly similar (but not identical) data files fall short in the presence of edits. Edits are changes in data files, and include insertions and deletions. Edits are fundamental operations&nbsp;on data and can be man-made (e.g., when multiple users work on a software code at the same time, and make changes to it across different versions), or nature-made (mutations in DNA sequences). Despite their pervasiveness, edit errors are difficult to study since the changes they cause in the data set are, mathematically speaking, non-linear. This situation is in contrast to substitution errors and erasures of data that are linear operations, and for which, a comprehensive mathematical machinery has been developed and successfully implemented in practice.&nbsp;</p>\n<p>Recovery from edit errors is needed in data synchronization where distant users need to efficiently synchronized their versions of the data sets (e.g., in version control) , in data deduplication where redundant data needs to be identified and omitted (e.g., to keep data storage compact), and in data reconstruction where the original but unknown data needs to be reconstructed based on its available, edited copies (e.g. in phylogenomics). &nbsp;</p>\n<p>In this research project, we established fundamental performance limits and invented practical algorithms for synchronization, deduplication and reconstruction schemes that recover from a potentially large number of edits. Our results utilize the concept of interactive communication as well as advanced tools from information theory and combinatorics. We successfully demonstrated the utility of our algorithms on practical data sets including images, text translation, and DNA sequences. Our results will help pave the way for efficient management of large-scale data across many existing and emerging applications.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/15/2019<br>\n\t\t\t\t\tModified by: Lara&nbsp;Dolecek</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nData is growing at a staggering pace, yet the tools for efficiently managing seemingly similar (but not identical) data files fall short in the presence of edits. Edits are changes in data files, and include insertions and deletions. Edits are fundamental operations on data and can be man-made (e.g., when multiple users work on a software code at the same time, and make changes to it across different versions), or nature-made (mutations in DNA sequences). Despite their pervasiveness, edit errors are difficult to study since the changes they cause in the data set are, mathematically speaking, non-linear. This situation is in contrast to substitution errors and erasures of data that are linear operations, and for which, a comprehensive mathematical machinery has been developed and successfully implemented in practice. \n\nRecovery from edit errors is needed in data synchronization where distant users need to efficiently synchronized their versions of the data sets (e.g., in version control) , in data deduplication where redundant data needs to be identified and omitted (e.g., to keep data storage compact), and in data reconstruction where the original but unknown data needs to be reconstructed based on its available, edited copies (e.g. in phylogenomics).  \n\nIn this research project, we established fundamental performance limits and invented practical algorithms for synchronization, deduplication and reconstruction schemes that recover from a potentially large number of edits. Our results utilize the concept of interactive communication as well as advanced tools from information theory and combinatorics. We successfully demonstrated the utility of our algorithms on practical data sets including images, text translation, and DNA sequences. Our results will help pave the way for efficient management of large-scale data across many existing and emerging applications.\n\n \n\n\t\t\t\t\tLast Modified: 10/15/2019\n\n\t\t\t\t\tSubmitted by: Lara Dolecek"
 }
}