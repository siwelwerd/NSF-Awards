{
 "awd_id": "1453651",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: New Directions in Deep Representation Learning from Complex Multimodal Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 488617.0,
 "awd_amount": 488617.0,
 "awd_min_amd_letter_date": "2015-08-20",
 "awd_max_amd_letter_date": "2022-07-27",
 "awd_abstract_narration": "The goal of deep learning is to learn an abstract representation of data with a hierarchical and compositional structure. Deep learning methods can effectively learn discriminative features from high-dimensional input data (e.g., for classification), and have been successfully applied to many real-world problems, such as image classification, speech recognition, and text modeling. Despite these successes, there still remains a challenging open question: how can we learn a robust deep representation that allows for holistic understanding and high-level reasoning from complex data?  This CAREER project aims to address this question and is expected to result in novel deep architectures, graphical models, and algorithmic advances for inference, learning, and optimization in deep representation learning.  The research outcomes will be disseminated through publications, talks, and tutorials. In addition to advancing the state of the art in deep learning and the many applications it entails, the project will integrate research and education through 1) developing courses in machine learning that include deep learning as a key topic; 2) mentoring significant graduate and undergraduate research activities; and 3) reaching out to K-12 students via hosting demo sessions and mentoring for science fair/research projects. \r\n \r\nThis project investigates the following closely interrelated and complementary thrusts:  First, it develops deep learning algorithms to disentangle factors of variation from complex data. This is done by modeling higher-order interactions between multiple groups of latent variables with a deep generative model (e.g., modeling face images via interaction of latent factors that correspond to identity, viewpoint, and emotion). In addition to better generalization, this approach is amenable to high-level reasoning, such as making analogies. Modeling higher-order interaction will be approached by learning a sub-manifold for each factor of variation, where correspondence information is used for regularizing the latent representation. The project will also develop weakly-supervised and semi-supervised disentangling algorithms that automatically establish correspondences without manual supervision. Second, the project develops deep representation learning methods for structured prediction problems. Specifically, it will develop a graphical model with deep representations that can model complex dependencies between output variables. This framework can be also viewed as data-driven modeling of higher-order prior on structured data, and can be used for modeling higher-order conditional random fields that permit efficient inference and learning. In addition, the project develops stochastic conditional generative models for structured prediction problems that involve uncertainty (i.e., one-to-many mappings).  Third, the project develops novel deep learning algorithms for constructing shared representations from multiple heterogeneous input modalities, such as image and text, audio and video, and multiple sensor streams. The main idea is to separately model conditional distribution of each input modality given other modalities. This approach addresses the well-known difficulty of modeling a joint distribution across heterogeneous multimodal input, and provides a theoretical analysis on conditions under which the approach can recover a consistent generative model. This formulation allows for robust recognition and high-level reasoning from heterogeneous multimodal data. Overall, these three thrusts are complementary and are expected to play synergistic roles in tackling a broader range of AI problems and moving beyond the current state-of-the-art in deep learning.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Honglak",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Honglak Lee",
   "pi_email_addr": "honglak@eecs.umich.edu",
   "nsf_id": "000583664",
   "pi_start_date": "2015-08-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "2260 Hayward, 3726 Beyster Bldg.",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 352639.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 51716.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 84262.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-8ed629a8-7fff-d3b5-e448-3f6b29795ecd\"> </span></p>\n<p><span id=\"docs-internal-guid-1b56fb3d-7fff-6bf9-193f-5c93e1161eff\"> </span></p>\n<p><span id=\"docs-internal-guid-a483c9cc-7fff-8277-7daf-3c360c3895a9\"> </span></p>\n<p><span id=\"docs-internal-guid-a2a31772-7fff-a86a-89dc-94dc3b7e135e\"> </span></p>\n<p><span id=\"docs-internal-guid-850562d9-7fff-1c30-8d68-f134a805bf0f\"> </span></p>\n<p dir=\"ltr\"><span>This project has achieved substantial breakthroughs in deep learning by developing robust representations that enhance holistic understanding and reasoning across complex data from diverse modalities. These advancements are encapsulated in three primary areas:</span></p>\n<ol>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Disentangled Representations from Complex Sensory Data: We innovated in algorithms to disentangle factors of variation, crucial for improving generalization and conditional generation. For visual data such as 2D images, 3D objects, and videos, the project made pioneering contributions as follows:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>2D Images: By leveraging disentanglement properties, we developed methods enhancing image classification and photo-realistic image synthesis. In addition, we developed techniques to identify object landmarks and manipulate object appearances without explicit supervision.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>3D Objects: The project advanced 3D object manipulation from 2D images without requiring 3D supervision data, demonstrating significant potential for practical applications.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Videos: We developed novel algorithms to separate motion from content, enabling long-term video generation with more than 100 frames, a feat previously deemed extremely challenging due to inherent uncertainties and compounding error accumulation over time.</span></p>\n</li>\n</ul>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Structured Output Prediction: The project extended the frontiers of image and video processing by leveraging structured representations as follows:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Image Segmentation and Generation: We developed advanced methods for weakly supervised segmentation and for manipulating and generating images based on structural predictions, and we advanced deep learning methods for contour detection and image localization using natural language.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Video and Motion Generation with Structures: Unsupervised techniques for motion retargeting and synthesis were developed, pushing the frontiers in generating dynamic and diverse visual content.</span></p>\n</li>\n</ul>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Shared Representations across Modalities: We explored joint representation learning, particularly between image and text, which facilitated advancements in zero-shot image recognition and text-to-image generation. Specifically, our contributions include:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Text-to-Image Synthesis: The project developed advanced methods for generating images from textual descriptions, enhancing zero-shot recognition and enabling sophisticated image generation from text and visual attributes, facilitating richer and more accurate visual content creation.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Cross-modal 3D Generations: The project innovated in translating and generating content across text, 3D objects, and code, demonstrating the versatile applications of learning shared representations.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Language and Text Understanding: Novel methods for sentence and document modeling were developed, improving the representation of language for more accurate style transfers and better understanding of complex document structures.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Reinforcement Learning in Complex Environments: The research extended to reinforcement learning, where models were trained to interact in various multimodal simulation environments (e.g., 3d world). These methods prioritize efficient learning from complex dynamics and sparse rewards, demonstrating significant advancements in learning control strategies and environmental interactions.</span></p>\n</li>\n</ul>\n</li>\n</ol>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>In conclusion, this project has had a large impact on the field, amassing 41 publications in top conferences, journals, and workshops in machine learning and AI, including NeurIPS, ICML, ICLR, CVPR, ECCV, ICCV, AAAI, IJCAI, NAACL-HLT, TMLR, as well as several invited talks at top venues. Overall, these works have been cited over 16,700 times as of the completion of this project report. The methodologies developed have not only advanced academic research but also promise significant transformative applications in real-world scenarios such as creative content generation and robotics.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/25/2024<br>\nModified by: Honglak&nbsp;Lee</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\nThis project has achieved substantial breakthroughs in deep learning by developing robust representations that enhance holistic understanding and reasoning across complex data from diverse modalities. These advancements are encapsulated in three primary areas:\n\n\n\n\nDisentangled Representations from Complex Sensory Data: We innovated in algorithms to disentangle factors of variation, crucial for improving generalization and conditional generation. For visual data such as 2D images, 3D objects, and videos, the project made pioneering contributions as follows:\n\n\n\n\n2D Images: By leveraging disentanglement properties, we developed methods enhancing image classification and photo-realistic image synthesis. In addition, we developed techniques to identify object landmarks and manipulate object appearances without explicit supervision.\n\n\n\n\n3D Objects: The project advanced 3D object manipulation from 2D images without requiring 3D supervision data, demonstrating significant potential for practical applications.\n\n\n\n\nVideos: We developed novel algorithms to separate motion from content, enabling long-term video generation with more than 100 frames, a feat previously deemed extremely challenging due to inherent uncertainties and compounding error accumulation over time.\n\n\n\n\n\n\nStructured Output Prediction: The project extended the frontiers of image and video processing by leveraging structured representations as follows:\n\n\n\n\nImage Segmentation and Generation: We developed advanced methods for weakly supervised segmentation and for manipulating and generating images based on structural predictions, and we advanced deep learning methods for contour detection and image localization using natural language.\n\n\n\n\nVideo and Motion Generation with Structures: Unsupervised techniques for motion retargeting and synthesis were developed, pushing the frontiers in generating dynamic and diverse visual content.\n\n\n\n\n\n\nShared Representations across Modalities: We explored joint representation learning, particularly between image and text, which facilitated advancements in zero-shot image recognition and text-to-image generation. Specifically, our contributions include:\n\n\n\n\nText-to-Image Synthesis: The project developed advanced methods for generating images from textual descriptions, enhancing zero-shot recognition and enabling sophisticated image generation from text and visual attributes, facilitating richer and more accurate visual content creation.\n\n\n\n\nCross-modal 3D Generations: The project innovated in translating and generating content across text, 3D objects, and code, demonstrating the versatile applications of learning shared representations.\n\n\n\n\nLanguage and Text Understanding: Novel methods for sentence and document modeling were developed, improving the representation of language for more accurate style transfers and better understanding of complex document structures.\n\n\n\n\nReinforcement Learning in Complex Environments: The research extended to reinforcement learning, where models were trained to interact in various multimodal simulation environments (e.g., 3d world). These methods prioritize efficient learning from complex dynamics and sparse rewards, demonstrating significant advancements in learning control strategies and environmental interactions.\n\n\n\n\n\n\n\n\n\nIn conclusion, this project has had a large impact on the field, amassing 41 publications in top conferences, journals, and workshops in machine learning and AI, including NeurIPS, ICML, ICLR, CVPR, ECCV, ICCV, AAAI, IJCAI, NAACL-HLT, TMLR, as well as several invited talks at top venues. Overall, these works have been cited over 16,700 times as of the completion of this project report. The methodologies developed have not only advanced academic research but also promise significant transformative applications in real-world scenarios such as creative content generation and robotics.\n\n\n\t\t\t\t\tLast Modified: 04/25/2024\n\n\t\t\t\t\tSubmitted by: HonglakLee\n"
 }
}