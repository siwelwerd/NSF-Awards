{
 "awd_id": "1545974",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Conference: Hardware and Algorithms for Learning On-a-chip; November 5, 2015; Austin, TX",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2015-07-15",
 "awd_exp_date": "2016-06-30",
 "tot_intn_awd_amt": 15000.0,
 "awd_amount": 15000.0,
 "awd_min_amd_letter_date": "2015-07-14",
 "awd_max_amd_letter_date": "2015-07-14",
 "awd_abstract_narration": "Support for student participation in a workshop to establish a forum for discussion of the current practices, as well as future research needs in three interrelated subareas of brain related sciences is provided through this grant. The three specific subareas that the workshop will discuss are: hardware acceleration of on-chip learning; machine learning algorithms and neuro-inspired information processing; and nano-electronic design for learning, with the focus on performance and energy efficiency. The workshop will consist of three sessions and will be followed by a panel discussion, and will end with a poster session to present student work. The conference will not only provide a venue for researchers to address problems but also a platform for graduate students and others to learn about cutting-edge research by attending invited talks, presentations, tutorials in the general area.\r\n\r\nThe outcome of this workshop include a webpage hosted by the Arizona State University to disseminate the program and discussions, as well as a report that clearly identifies major research challenges and needs in computational neurosciences, machine learning, and hardware design. The papers and posters will be published through the workshop proceedings. These results will illustrate new and exciting inter-disciplinary research problems of interest to the related communities, and articulate a vision of conducting further research in this area. The final workshop report will also be submitted to NSF as an annual report and will  be released online for sharing with the research community in general.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yu",
   "pi_last_name": "Cao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yu Cao",
   "pi_email_addr": "yucao@umn.edu",
   "nsf_id": "000488324",
   "pi_start_date": "2015-07-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "PO Box 6011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852876011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 15000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Machine learning algorithms are being developed and will fundamentally alter the way individuals and organizations live, work, and interact with each other. However their computational complexity still challenges the state-of-the-art computing platforms, especially when the application of interest is tightly constrained by the requirements of low power, high throughput, small latency, etc. In recent years, there have been enormous advances in implementing machine learning algorithms with application-specific hardware (e.g., FPGA, ASIC, etc.). There is a timely need to map the latest learning algorithms to physical hardware, in order to achieve orders of magnitude improvement in performance, energy efficiency and compactness. Recent progress in computational neurosciences and nanoelectronic technology, such as resistive memory devices, will further help shed light on future hardware-software platforms for learning on-a-chip.</p>\n<p>The workshop on Hardware and Algorithms for Learning On-a-chip (HALO) is organized to explore the potential of on-chip machine learning, to reveal emerging algorithms and design needs, and to promote novel applications for learning. It aims to establish a forum to discuss the current practices, as well as future research needs in the fields. HALO is collocated with International Conference on Computer-Aided Design (ICCAD) 2016. As the premium conference on IC design automation, ICCAD attracts many experts from industry and academia.</p>\n<p>The HALO Workshop was successfully held at Doubletree Hotel in Austin on November 5th, 2015. Because this is the very first time, the workshop organizers decided to invite eleven speakers from universities and leading companies. In addition, a poster session was organized with 31 posters by university students and researchers. There were 73 attendees participated the workshop. Four sessions were presented on the hardware acceleration, learning algorithms and neuromorphic hardware design. The detailed agenda was posted at <a href=\"http://nimo.asu.edu/halo\">http://nimo.asu.edu/halo</a>. &nbsp;</p>\n<p>The speakers presented a comprehensive view on hardware acceleration, learning algorithms, and neuromorphic computing. The development of deep learning algorithms, such as convolutional neural network (CNN), have led to the breakthroughs in the accuracy of many computer vision and information processing tasks. However, their multi-layer structure is enormous and complex, requiring substantial computing resources to train and evaluate. Indeed, machine learning with big data has become one of the most important workloads in both data centers and mobile platforms. Even with state-of-the-art CPUs/GPUs, an acceleration factor of 10<sup>2</sup>-10<sup>4</sup> is critically needed to perform real-time video analytics. While advanced hardware solutions, such as IBM TrueNorth, help bring expensive learning and classification to a low-power processor, they are still much less efficient compared to the human brain. In this context, we need to fundamentally re-think the learning algorithm and hardware architecture, inspired by mathematical and neuromorphic principles at multiple levels. &nbsp; &nbsp;</p>\n<p>During the workshop, several presenters discussed acceleration of large-scale learning algorithms (e.g., deep CNNs, logistic regression, and specialized image recognition) on FPGAs and ASICs. Heterogeneous hardware offers a promising path towards major improvement in processing capability while achieving high energy efficiency. It was demonstrated that by co-optimizing the algorithm and the architecture, these implementations effectively reduce the latency in model training and evaluation. The results enable a broad range of applications, including human-like visual capability, driving assistance, and human-machine symbiotic data management.</p>\n<p>In addition to hardware architecture, the development of next-generati...",
  "por_txt_cntn": "\nMachine learning algorithms are being developed and will fundamentally alter the way individuals and organizations live, work, and interact with each other. However their computational complexity still challenges the state-of-the-art computing platforms, especially when the application of interest is tightly constrained by the requirements of low power, high throughput, small latency, etc. In recent years, there have been enormous advances in implementing machine learning algorithms with application-specific hardware (e.g., FPGA, ASIC, etc.). There is a timely need to map the latest learning algorithms to physical hardware, in order to achieve orders of magnitude improvement in performance, energy efficiency and compactness. Recent progress in computational neurosciences and nanoelectronic technology, such as resistive memory devices, will further help shed light on future hardware-software platforms for learning on-a-chip.\n\nThe workshop on Hardware and Algorithms for Learning On-a-chip (HALO) is organized to explore the potential of on-chip machine learning, to reveal emerging algorithms and design needs, and to promote novel applications for learning. It aims to establish a forum to discuss the current practices, as well as future research needs in the fields. HALO is collocated with International Conference on Computer-Aided Design (ICCAD) 2016. As the premium conference on IC design automation, ICCAD attracts many experts from industry and academia.\n\nThe HALO Workshop was successfully held at Doubletree Hotel in Austin on November 5th, 2015. Because this is the very first time, the workshop organizers decided to invite eleven speakers from universities and leading companies. In addition, a poster session was organized with 31 posters by university students and researchers. There were 73 attendees participated the workshop. Four sessions were presented on the hardware acceleration, learning algorithms and neuromorphic hardware design. The detailed agenda was posted at http://nimo.asu.edu/halo.  \n\nThe speakers presented a comprehensive view on hardware acceleration, learning algorithms, and neuromorphic computing. The development of deep learning algorithms, such as convolutional neural network (CNN), have led to the breakthroughs in the accuracy of many computer vision and information processing tasks. However, their multi-layer structure is enormous and complex, requiring substantial computing resources to train and evaluate. Indeed, machine learning with big data has become one of the most important workloads in both data centers and mobile platforms. Even with state-of-the-art CPUs/GPUs, an acceleration factor of 102-104 is critically needed to perform real-time video analytics. While advanced hardware solutions, such as IBM TrueNorth, help bring expensive learning and classification to a low-power processor, they are still much less efficient compared to the human brain. In this context, we need to fundamentally re-think the learning algorithm and hardware architecture, inspired by mathematical and neuromorphic principles at multiple levels.    \n\nDuring the workshop, several presenters discussed acceleration of large-scale learning algorithms (e.g., deep CNNs, logistic regression, and specialized image recognition) on FPGAs and ASICs. Heterogeneous hardware offers a promising path towards major improvement in processing capability while achieving high energy efficiency. It was demonstrated that by co-optimizing the algorithm and the architecture, these implementations effectively reduce the latency in model training and evaluation. The results enable a broad range of applications, including human-like visual capability, driving assistance, and human-machine symbiotic data management.\n\nIn addition to hardware architecture, the development of next-generation learning on-a-chip requires novel algorithms that are able to organize the objects in a hierarchy, transfer previous knowledge, and train the model dynamically and o..."
 }
}