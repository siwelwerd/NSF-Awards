{
 "awd_id": "1541364",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CC*DNI Networking Infrastructure: Washington University Research Network (WURN)",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924220",
 "po_email": "kthompso@nsf.gov",
 "po_sign_block_name": "Kevin Thompson",
 "awd_eff_date": "2015-10-01",
 "awd_exp_date": "2017-09-30",
 "tot_intn_awd_amt": 475590.0,
 "awd_amount": 475590.0,
 "awd_min_amd_letter_date": "2015-08-07",
 "awd_max_amd_letter_date": "2015-10-07",
 "awd_abstract_narration": "New discoveries in science and engineering research in the 21st century arise from the creation, analysis and communication of staggering amounts of data.  Researchers at Washington University in St. Louis (WUSTL) are at the forefront of Big Data research and thus continually pushing the university's network infrastructure.  The creation of the Washington University Research Network is a critical step forward in providing the necessary hardware and software capabilities to enable Petabytes of data to move into and out of university systems.  However, the capability of genomics, imaging and planetary science (to name a few) instruments to generate new data is increasing exponentially and can quickly outpace the capacity of any network. \r\n\r\nThe project creates a Software Defined Network (SDN) based distributed Science DMZ network on the WUSTL campus across three existing data centers linked to each other and research and education wide area network connections via dedicated 40Gbps hardware in the existing campus core. The key contribution of this project is to deploy new techniques in SDN that optimize the way existing and future capacity is utilized, while also ensuring the necessary monitoring oversight needed for predictable and secure operation.  By scheduling large data transfers and intelligently partitioning and sharing network capacity, the project's goal is to ensure near optimal utilization of resources and provide a cost effective solution.   Network programmability is also leveraged to dynamically instantiate monitoring resources that oversee the security of those transfers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Roch",
   "pi_last_name": "Guerin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Roch Guerin",
   "pi_email_addr": "guerin@wustl.edu",
   "nsf_id": "000444512",
   "pi_start_date": "2015-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Rohit",
   "pi_last_name": "Pappu",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Rohit V Pappu",
   "pi_email_addr": "pappu@wustl.edu",
   "nsf_id": "000093653",
   "pi_start_date": "2015-10-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Fred",
   "pi_last_name": "Prior",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fred Prior",
   "pi_email_addr": "priorf@mir.wustl.edu",
   "nsf_id": "000644568",
   "pi_start_date": "2015-08-07",
   "pi_end_date": "2015-10-07"
  }
 ],
 "inst": {
  "inst_name": "Washington University",
  "inst_street_address": "1 BROOKINGS DR",
  "inst_street_address_2": "",
  "inst_city_name": "SAINT LOUIS",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "3147474134",
  "inst_zip_code": "631304862",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MO01",
  "org_lgl_bus_name": "WASHINGTON UNIVERSITY, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "L6NFUM28LQM5"
 },
 "perf_inst": {
  "perf_inst_name": "Washington University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "631304899",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808000",
   "pgm_ele_name": "Campus Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 475590.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span style=\"text-decoration: underline;\"><strong>Overview</strong></span></p>\n<p>The project&rsquo;s main goal was to enhance the ability of Washington University&rsquo;s network to efficiently support its research mission.&nbsp; The focus was on creating a parallel network to the existing campus network; one that would be dedicated to carrying research traffic and would be shielded from interference by non-research users.&nbsp; Additionally, the new network was expected to deliver a greater level of performance in support of the many big data applications behind much of today's research.&nbsp; Specifically, there is a growing reliance on large-scale data and computational resources as the main drivers behind many of today&rsquo;s research projects.&nbsp; In other words, successful research increasingly involves access to enormous data repositories (often produced by others), and applying massive amounts of computation to that data to unlock the research value it holds.&nbsp; For example, Washington University&rsquo;s McDonnell Genome Institute routinely needs to transfer or retrieve tens of terabytes (one terabyte is roughly equivalent to 1,000 Encyclopedia Britannica) to research partners across the world.&nbsp; The combination of massive data sets and enormous computational resources calls for the availability of high-speed networks to connect them.&nbsp; Realizing such a network was the motivation behind the Washington University Research Network (WURN).</p>\n<p><strong><span style=\"text-decoration: underline;\">The WURN as a Research Enabler</span></strong></p>\n<p>The project was successful in designing and deploying a dedicated pan campus research network, the WURN, which operates at a speed of 40 Gbps (40 billions bits per second) with access ramps to the network operating at 10 Gbps.&nbsp; The WURN&rsquo;s first users included the Center for High Performance Computing (CHPC), the Earth and Planetary Remote Sensing Lab (EPRSL), the WashU Center for Cellular Imaging (WUCCI), the Center for Biological Systems Engineering (CBSE), and the McDonnell Genome Institute (MGI).&nbsp; Three new sites from neurobiology, genetics, and neuro-imaging and radiology have since been added.&nbsp; The WURN&rsquo;s success, however, goes beyond connecting those users to the new network, and is really evident in what this new connectivity enables.&nbsp; This is best illustrated through early testimonials provided by those users.&nbsp; For example, WashU&rsquo;s Center for High Performance Computing that is used by 600 research users across 30 departments had seen access to its local storage facility (the facility used to stage data for compute-intensive processing) become a major bottleneck and pain-point for its users.&nbsp; This was due to the growing size of the data sets on which computations needed to be performed, which resulted in increasing long transfer times into local storage from the equipment producing the data.&nbsp; &nbsp;The introduction of the WURN produced a nearly ten-fold increase in data throughput, and correspondingly a decrease in latency in loading data into the HPC local storage store.&nbsp; This is illustrated in the accompanying figure that displays a comparison of the before and after data transfer rates into the center&rsquo;s storage facility.&nbsp; As the scale of the figure&rsquo;s vertical axis indicates, connection to the WURN resulted in a ten-fold improvement. A similar outcome was realized by the McDonnell Genome Center that distributes genome sequencing data to partners around the world working on key medical research projects such as the Alzheimer Disease Sequencing Project.&nbsp; After connecting to the WURN, the McDonnell Genome Institute saw download times of large genome data sets to several of its partners decrease by an order of magnitude or more.</p>\n<p><strong><span style=\"text-decoration: underline;\">The WURN as an Operational Platform</span></strong></p>\n<p>In addition to delivering an infrastructure, the WURN, that significantly improves how research users can access and transfer the large data set their research relies on, the project also successfully transitioned the WURN from its initial experimental status to a full-fledged component of the WashU IT operational offering.&nbsp; This includes migrating its operation and oversight to the WashU IT System Operations Center (SOC), as well as creating a web interface to both introduce the WURN to the WashU Research community, and for providing a standard service request form to apply for WURN connectivity.&nbsp; Requests for connection to the WURN can currently select from three onboarding options.&nbsp; Standard onboarding is provided to research users located in one of the research data centers already connected to the WURN, and willing to utilize the standard data transfer appliances offered and supported by WashU IT.&nbsp; The second tier extends this option to allow research users located in those research data centers to connect their own data transfer solution, albeit without support from WashU IT.&nbsp; The third tier is for users requiring an expansion of the WURN access to their location.&nbsp; This combination of three tiers with different configurations and levels of flexibility has been successful in helping expand the number of WURN users beyond the initial set of pilot users.&nbsp; A formal review process has also been put in place to evaluate and prioritize tier 3 connection requests that call for extending the current set of WURN access points to other locations on campus.</p>\n<p><strong><span style=\"text-decoration: underline;\"><br /></span></strong></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/17/2017<br>\n\t\t\t\t\tModified by: Roch&nbsp;Guerin</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1541364/1541364_10384792_1508197459542_HPC_transfers_2017--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1541364/1541364_10384792_1508197459542_HPC_transfers_2017--rgov-800width.jpg\" title=\"HPC data transfer rates before &amp; after the WURN\"><img src=\"/por/images/Reports/POR/2017/1541364/1541364_10384792_1508197459542_HPC_transfers_2017--rgov-66x44.jpg\" alt=\"HPC data transfer rates before &amp; after the WURN\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Data transfers rate in and out of the Center for High Performance Computing before (top) and after (bottom) connecting to the WURN.</div>\n<div class=\"imageCredit\">Malcom Tobias-  Director, Center for High Performance Computing</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Roch&nbsp;Guerin</div>\n<div class=\"imageTitle\">HPC data transfer rates before & after the WURN</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nOverview\n\nThe project?s main goal was to enhance the ability of Washington University?s network to efficiently support its research mission.  The focus was on creating a parallel network to the existing campus network; one that would be dedicated to carrying research traffic and would be shielded from interference by non-research users.  Additionally, the new network was expected to deliver a greater level of performance in support of the many big data applications behind much of today's research.  Specifically, there is a growing reliance on large-scale data and computational resources as the main drivers behind many of today?s research projects.  In other words, successful research increasingly involves access to enormous data repositories (often produced by others), and applying massive amounts of computation to that data to unlock the research value it holds.  For example, Washington University?s McDonnell Genome Institute routinely needs to transfer or retrieve tens of terabytes (one terabyte is roughly equivalent to 1,000 Encyclopedia Britannica) to research partners across the world.  The combination of massive data sets and enormous computational resources calls for the availability of high-speed networks to connect them.  Realizing such a network was the motivation behind the Washington University Research Network (WURN).\n\nThe WURN as a Research Enabler\n\nThe project was successful in designing and deploying a dedicated pan campus research network, the WURN, which operates at a speed of 40 Gbps (40 billions bits per second) with access ramps to the network operating at 10 Gbps.  The WURN?s first users included the Center for High Performance Computing (CHPC), the Earth and Planetary Remote Sensing Lab (EPRSL), the WashU Center for Cellular Imaging (WUCCI), the Center for Biological Systems Engineering (CBSE), and the McDonnell Genome Institute (MGI).  Three new sites from neurobiology, genetics, and neuro-imaging and radiology have since been added.  The WURN?s success, however, goes beyond connecting those users to the new network, and is really evident in what this new connectivity enables.  This is best illustrated through early testimonials provided by those users.  For example, WashU?s Center for High Performance Computing that is used by 600 research users across 30 departments had seen access to its local storage facility (the facility used to stage data for compute-intensive processing) become a major bottleneck and pain-point for its users.  This was due to the growing size of the data sets on which computations needed to be performed, which resulted in increasing long transfer times into local storage from the equipment producing the data.   The introduction of the WURN produced a nearly ten-fold increase in data throughput, and correspondingly a decrease in latency in loading data into the HPC local storage store.  This is illustrated in the accompanying figure that displays a comparison of the before and after data transfer rates into the center?s storage facility.  As the scale of the figure?s vertical axis indicates, connection to the WURN resulted in a ten-fold improvement. A similar outcome was realized by the McDonnell Genome Center that distributes genome sequencing data to partners around the world working on key medical research projects such as the Alzheimer Disease Sequencing Project.  After connecting to the WURN, the McDonnell Genome Institute saw download times of large genome data sets to several of its partners decrease by an order of magnitude or more.\n\nThe WURN as an Operational Platform\n\nIn addition to delivering an infrastructure, the WURN, that significantly improves how research users can access and transfer the large data set their research relies on, the project also successfully transitioned the WURN from its initial experimental status to a full-fledged component of the WashU IT operational offering.  This includes migrating its operation and oversight to the WashU IT System Operations Center (SOC), as well as creating a web interface to both introduce the WURN to the WashU Research community, and for providing a standard service request form to apply for WURN connectivity.  Requests for connection to the WURN can currently select from three onboarding options.  Standard onboarding is provided to research users located in one of the research data centers already connected to the WURN, and willing to utilize the standard data transfer appliances offered and supported by WashU IT.  The second tier extends this option to allow research users located in those research data centers to connect their own data transfer solution, albeit without support from WashU IT.  The third tier is for users requiring an expansion of the WURN access to their location.  This combination of three tiers with different configurations and levels of flexibility has been successful in helping expand the number of WURN users beyond the initial set of pilot users.  A formal review process has also been put in place to evaluate and prioritize tier 3 connection requests that call for extending the current set of WURN access points to other locations on campus.\n\n\n\n\n\t\t\t\t\tLast Modified: 10/17/2017\n\n\t\t\t\t\tSubmitted by: Roch Guerin"
 }
}