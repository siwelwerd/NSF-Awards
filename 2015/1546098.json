{
 "awd_id": "1546098",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Learning Big Bayesian Networks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-10-01",
 "awd_exp_date": "2020-09-30",
 "tot_intn_awd_amt": 919305.0,
 "awd_amount": 919305.0,
 "awd_min_amd_letter_date": "2015-09-15",
 "awd_max_amd_letter_date": "2015-09-15",
 "awd_abstract_narration": "A fundamental problem in analyzing big data is to extract and represent the relations among the huge number of variables in a dataset. For example, in a genomic dataset, one may want to find out the dependence among a large number of genetic variations and various disease states. The Bayesian network is a commonly used class of mathematical models to represent such complex relations among a collection of variables, with wide applications in many scientific fields, ranging from the biomedical sciences to the social sciences. The goal of this project is to develop statistical and machine learning methods to construct Bayesian networks from big data, where the datasets may contain thousands to millions of variables. This is a challenging problem, particularly for large networks, as seen from the fact that state-of-the-art methods can barely handle thousands of variables. In this project, a novel divide-and-conquer approach will be developed and implemented as open-source packages for public use. The PIs will also study the theoretical properties of key components of this approach.  Through seminar organization and educational activities in both graduate and undergraduate training, the cutting-edge research in this project will be communicated immediately to a much broader audience.\r\n\r\nThe proposed approach consists of three main components: Partition, Estimation and Fusion (PEF). In the partition stage, spectral clustering will be embedded into an iterative subsampling approach to efficiently group variables into clusters. In the estimation stage, a few new methods will be developed to estimate the structure of a Bayesian network for each cluster of nodes, which serves as a subgraph of the big network. These methods include convex relaxations for permutations, fast algorithms for large-scale regularized estimation of the parameters of a Bayesian network, and novel formulations for discrete data. The final fusion stage will merge subgraphs into one big Bayesian network via a new method based on multiple-response sparse regression. Rigorous analysis of the PEF learning strategy for Bayesian networks under high-dimensional scaling will be conducted to provide theoretical guarantees for the methods and the algorithms.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Qing",
   "pi_last_name": "Zhou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qing Zhou",
   "pi_email_addr": "zhou@stat.ucla.edu",
   "nsf_id": "000005427",
   "pi_start_date": "2015-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Arash",
   "pi_last_name": "Amini",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Arash A Amini",
   "pi_email_addr": "arash.amini@stat.ucla.edu",
   "nsf_id": "000685553",
   "pi_start_date": "2015-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "8125 Math Sciences Bldg",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951554",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 919305.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Bayesian networks are a class of graphical models with wide applications in many scientific fields, ranging from the biomedical sciences to the social sciences. However, structure learning of Bayesian networks from data has always been challenging, particularly for large networks. In this project, we developed novel statistical and machine learning methods and released open-source software packages for structure learning of big Bayesian networks. Through educational activities, the cutting-edge research ideas in this project have been communicated to a much broader audience.</p>\n<p>First, we developed a divide-and-conquer strategy, called partition-estimation-fusion (PEF), for learning Bayesian networks from big data which contain a large number of variables. Our method first partitions nodes into clusters, then learns a subgraph on each cluster of nodes, and finally fuses all learned subgraphs into one Bayesian network. Extensive numerical experiments demonstrated the competitive performance of our PEF method, in terms of both speed and accuracy compared to existing methods. Our method can improve the accuracy of structure learning, while reducing running time up to two orders-of-magnitude.</p>\n<p>Second, we developed a novel method, annealing on regularized Cholesky score (ARCS), to search over topological sorts, or permutations of nodes, for a high-scoring Bayesian network. Our scoring function is derived from regularizing Gaussian DAG likelihood, and its optimization gives an alternative formulation of the sparse Cholesky factorization problem from a statistical viewpoint. We combine simulated annealing over permutation space with a fast proximal gradient algorithm to compute the score of any permutation. Combined, the two approaches allow us to quickly and effectively search over the space of DAGs without the need to verify the acyclicity constraint or to enumerate possible parent sets given a candidate topological sort. Through extensive numerical comparisons, we showed that ARCS outperformed existing methods by a substantial margin, demonstrating its great advantage in structure learning of Bayesian networks from both observational and experimental data. We also established the consistency of our scoring function in estimating topological sorts and DAG structures in the large-sample limit.</p>\n<p>Third, we have developed theory to quantify the estimation accuracy of learning Bayesian networks from high-dimensional data. We analyzed a popular score-based estimator that has been the subject of extensive empirical inquiry in recent years and is known to achieve state-of-the-art results. The approach we study does not require strong assumptions such as faithfulness that existing theory for score-based learning crucially relies on. The resulting estimator is based around a difficult nonconvex optimization problem, and its analysis may be of independent interest given recent developments in nonconvex optimization in machine learning.</p>\n<p>We have published papers, given talks and presented posters to disseminate the research results in this project to researchers in statistics, machine learning and data science. Moreover, a few software packages that implement the above methods have been developed and released for public use, such as the sparsebn R package.</p>\n<p>The research of this project has been integrated into educational activities to enhance its broader impact. The PI has incorporated some of the above new methods into graduate and undergraduate courses on statistical modeling and machine learning. This project also provided STEM training opportunities for women and minorities.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/05/2021<br>\n\t\t\t\t\tModified by: Qing&nbsp;Zhou</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nBayesian networks are a class of graphical models with wide applications in many scientific fields, ranging from the biomedical sciences to the social sciences. However, structure learning of Bayesian networks from data has always been challenging, particularly for large networks. In this project, we developed novel statistical and machine learning methods and released open-source software packages for structure learning of big Bayesian networks. Through educational activities, the cutting-edge research ideas in this project have been communicated to a much broader audience.\n\nFirst, we developed a divide-and-conquer strategy, called partition-estimation-fusion (PEF), for learning Bayesian networks from big data which contain a large number of variables. Our method first partitions nodes into clusters, then learns a subgraph on each cluster of nodes, and finally fuses all learned subgraphs into one Bayesian network. Extensive numerical experiments demonstrated the competitive performance of our PEF method, in terms of both speed and accuracy compared to existing methods. Our method can improve the accuracy of structure learning, while reducing running time up to two orders-of-magnitude.\n\nSecond, we developed a novel method, annealing on regularized Cholesky score (ARCS), to search over topological sorts, or permutations of nodes, for a high-scoring Bayesian network. Our scoring function is derived from regularizing Gaussian DAG likelihood, and its optimization gives an alternative formulation of the sparse Cholesky factorization problem from a statistical viewpoint. We combine simulated annealing over permutation space with a fast proximal gradient algorithm to compute the score of any permutation. Combined, the two approaches allow us to quickly and effectively search over the space of DAGs without the need to verify the acyclicity constraint or to enumerate possible parent sets given a candidate topological sort. Through extensive numerical comparisons, we showed that ARCS outperformed existing methods by a substantial margin, demonstrating its great advantage in structure learning of Bayesian networks from both observational and experimental data. We also established the consistency of our scoring function in estimating topological sorts and DAG structures in the large-sample limit.\n\nThird, we have developed theory to quantify the estimation accuracy of learning Bayesian networks from high-dimensional data. We analyzed a popular score-based estimator that has been the subject of extensive empirical inquiry in recent years and is known to achieve state-of-the-art results. The approach we study does not require strong assumptions such as faithfulness that existing theory for score-based learning crucially relies on. The resulting estimator is based around a difficult nonconvex optimization problem, and its analysis may be of independent interest given recent developments in nonconvex optimization in machine learning.\n\nWe have published papers, given talks and presented posters to disseminate the research results in this project to researchers in statistics, machine learning and data science. Moreover, a few software packages that implement the above methods have been developed and released for public use, such as the sparsebn R package.\n\nThe research of this project has been integrated into educational activities to enhance its broader impact. The PI has incorporated some of the above new methods into graduate and undergraduate courses on statistical modeling and machine learning. This project also provided STEM training opportunities for women and minorities.\n\n \n\n\t\t\t\t\tLast Modified: 01/05/2021\n\n\t\t\t\t\tSubmitted by: Qing Zhou"
 }
}