{
 "awd_id": "1551131",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research: Models of Child Speech",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2018-02-28",
 "tot_intn_awd_amt": 160000.0,
 "awd_amount": 160000.0,
 "awd_min_amd_letter_date": "2015-07-23",
 "awd_max_amd_letter_date": "2015-07-23",
 "awd_abstract_narration": "In contrast to the production, modeling, and machine recognition of adult speech, which have been studied for decades, the production, acoustic modeling and recognition of child speech have not received the same level of attention. The lack of scholarly resources for dealing with children's speech is problematic as new applications for child speech and language development become increasingly important and commonplace. This is especially true for elementary school children. As children grow, their articulators grow as well, resulting in variations in their speech sounds. For example, the waveform of the word 'sunny' spoken by a 6-year old can be quite different than that of the same child when she is 9 years old. This is why current machine recognition of children's speech does not perform well for young children and does not scale up as the child grows. That is, these systems tend to be age dependent. Understanding and modeling child speech as children grow is important not only to developing better recognition systems but also for better understanding and diagnosis of speech-language pathology (SLP). As the negative long-term ramifications of deficits in early childhood development gain increasingly broad recognition, the opportunities and the need for social and health services and technological applications targeted toward young children are growing. In particular, it is now understood that early deficits in language development and literacy persist into adulthood, and the demand for SLP services in public schools is significantly outpacing supply. As a result, it is no longer feasible for clinicians and teachers to provide the most effective treatments or the necessary attention to every child. Better speech recognition systems would provide an opportunity for improved diagnosis and more intense computer-based therapy.  This Early Grant for Exploratory Research project aims to model how speech and language develop during elementary school and how children with speech disorders differ in their articulation of speech sounds.  Models of child speech will lead to the development of computer programs which can be used for educational as well as therapeutic purposes.\r\n\r\nScientifically, the exploratory project will 1) reveal processes of speech production development in 20-26 elementary school-aged children through a unique combination of articulatory and acoustic analyses, and 2) develop acoustic models and eventually automatic speech recognition systems for children's speech which can be scalable with age (as opposed to being age-dependent systems). This can only be achieved by understanding how the articulation and corresponding acoustics develop with age. The different aspects of the project are therefore synergistic: findings from articulation and acoustic experiments will inform the development of algorithms essential to automatic speech recognition.  Production data will include real-time 3D ultrasound recordings of the tongue, video recordings of the lips, palate impressions, microphone recordings, and accelerometer recordings of neck skin vibrations which have been shown to be beneficial in automatic speech and speaker recognition applications. The causal relationship between articulatory and acoustic variability will be explored, as will their relationship to misrecognition of child speech. Articulatory features will be incorporated into new automatic speech recognition systems along with acoustic features. The exploratory project will contribute to knowledge of variability between children, as well as variability over time as children grow and will provide, for the first time, normative data and scientific models. These can lead to robust child speech recognition systems as well as tools that will be useful for a variety of applications such as educational games, training of speech-language pathologists, automatic or semi-automatic transcription systems, and speech articulation visualization systems. It will train undergraduate and graduate students in important cross-disciplinary activities of technological and scientific significance. We believe that the proposed project is transformative in its advancement of the scientific and technological state of the art related to child speech.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Lulich",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Steven M Lulich",
   "pi_email_addr": "slulich@indiana.edu",
   "nsf_id": "000436930",
   "pi_start_date": "2015-07-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University",
  "perf_str_addr": "200 South Jordan Avenue",
  "perf_city_name": "Bloomington",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "474057000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IN09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 160000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project resulted in the creation of a three-year longitudinal dataset consisting of acoustic signals and articulatory measurements from a total of 16 male and female children in the 1st, 2nd, and 3rd grades. &nbsp;These data were used in multiple ways.&nbsp; The study focused on the production and perception of R-sounds, and the data were anlayzed in several ways.</p>\n<p>First, three-dimensional tongue surface shapes were extracted from articulatory data to show how cihldren shape the tongue to produce both good and poor R-sounds. The tongue surface shapes were quantified using standard metrics.</p>\n<p>Second, perceptual ratings of the children's R-sounds were obtained from graduate student speech-language pathologists, and the relationship between perceptual ratings and tongue shapes was investigated.&nbsp; Clear differences in tongue shape were observed for good and poor R-sounds, but the quantitative tongue shape metrics failed to capture these differences.</p>\n<p>Third, a speech recognition application was developed to automatically detect children's poorly produced R-sounds, with the goal of applying this new technology to the diagnosis of speech disorders.</p>\n<p>The project involved the development of procedures and software for recording and analyzing 3D tongue images using cutting-edge ultrasound technology. The free software toolbox called \"WASL\" was developed for this purpose.</p>\n<p>Analysis of the data from 1st-grade children showed that children exhibit a range of articulatory strategies to produce R-sounds, with common features (for good R-sounds) including a deep groove in the back of the tongue, and bunching of the tongue body near the back of the upper teeth.&nbsp; Few R-sounds were produced with a retroflex tongue tip.&nbsp; Children who were unable to produce reliably good R-sounds tended to omit the groove in the back of the tongue, and bunched the tongue body closer to the soft palate instead of the teeth.</p>\n<p>The study resulted in a large corpus with a rich array of data, including audio, ultrasound images, MRI imaging, video, palate impressions, and lung acoustics.&nbsp; This corpus will feed future research projects for years, includings studies of speech development, disordered speech, relationships between speech and lung acoustics, articulatory-acoustic relations, and speech recognition technology for children's typical and disordered speech. Although only R-sounds were focused on in the present research, all consonants and vowels of American English are represented in the corpus in clinically relevant contexts.</p>\n<p>This work supported the research assistantships of five graduate student speech-language pathology clinicians, who received formal training in research and speech communication, speech disorders, linguistics, articulatory and acoustic phonetics, speech perception, and speech recognition.</p>\n<p>The outcomes of this research have been shared with the research community through conference/meeting presentations, posters, and the \"WASL\" software. The data produced in this work will be shared with the research community through the Indiana University IUScholarWorks online repository.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/23/2018<br>\n\t\t\t\t\tModified by: Steven&nbsp;M&nbsp;Lulich</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project resulted in the creation of a three-year longitudinal dataset consisting of acoustic signals and articulatory measurements from a total of 16 male and female children in the 1st, 2nd, and 3rd grades.  These data were used in multiple ways.  The study focused on the production and perception of R-sounds, and the data were anlayzed in several ways.\n\nFirst, three-dimensional tongue surface shapes were extracted from articulatory data to show how cihldren shape the tongue to produce both good and poor R-sounds. The tongue surface shapes were quantified using standard metrics.\n\nSecond, perceptual ratings of the children's R-sounds were obtained from graduate student speech-language pathologists, and the relationship between perceptual ratings and tongue shapes was investigated.  Clear differences in tongue shape were observed for good and poor R-sounds, but the quantitative tongue shape metrics failed to capture these differences.\n\nThird, a speech recognition application was developed to automatically detect children's poorly produced R-sounds, with the goal of applying this new technology to the diagnosis of speech disorders.\n\nThe project involved the development of procedures and software for recording and analyzing 3D tongue images using cutting-edge ultrasound technology. The free software toolbox called \"WASL\" was developed for this purpose.\n\nAnalysis of the data from 1st-grade children showed that children exhibit a range of articulatory strategies to produce R-sounds, with common features (for good R-sounds) including a deep groove in the back of the tongue, and bunching of the tongue body near the back of the upper teeth.  Few R-sounds were produced with a retroflex tongue tip.  Children who were unable to produce reliably good R-sounds tended to omit the groove in the back of the tongue, and bunched the tongue body closer to the soft palate instead of the teeth.\n\nThe study resulted in a large corpus with a rich array of data, including audio, ultrasound images, MRI imaging, video, palate impressions, and lung acoustics.  This corpus will feed future research projects for years, includings studies of speech development, disordered speech, relationships between speech and lung acoustics, articulatory-acoustic relations, and speech recognition technology for children's typical and disordered speech. Although only R-sounds were focused on in the present research, all consonants and vowels of American English are represented in the corpus in clinically relevant contexts.\n\nThis work supported the research assistantships of five graduate student speech-language pathology clinicians, who received formal training in research and speech communication, speech disorders, linguistics, articulatory and acoustic phonetics, speech perception, and speech recognition.\n\nThe outcomes of this research have been shared with the research community through conference/meeting presentations, posters, and the \"WASL\" software. The data produced in this work will be shared with the research community through the Indiana University IUScholarWorks online repository.\n\n\t\t\t\t\tLast Modified: 05/23/2018\n\n\t\t\t\t\tSubmitted by: Steven M Lulich"
 }
}