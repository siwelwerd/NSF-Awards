{
 "awd_id": "1546480",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Collaborative Research: F: Streaming Architecture for Continuous Entity Linking in Social Media",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-01-01",
 "awd_exp_date": "2020-12-31",
 "tot_intn_awd_amt": 783339.0,
 "awd_amount": 799339.0,
 "awd_min_amd_letter_date": "2015-09-03",
 "awd_max_amd_letter_date": "2018-05-04",
 "awd_abstract_narration": "A large fraction of the ever-growing internet content is found in social media such as (micro)blogs. Users access it to both form and share their opinions about events and people, election preferences, product and brand recommendations. This situation provides opportunities to create added layers of data mining and analysis regarding users' views on developing events, products, services, or government actions; at the same time, it raises challenges for Entity Linking (EL) in social media. EL is the task of linking an extracted mention to a specific definition of the entity. The definition of an entity is usually a pointer to a Web page that defines the entity. Information extraction from social media generally faces many challenging issues due to: message volume, message speed (Twitter alone generates over 500 million messages per day), variety, free-form language, lack of context, large reference variation and language diversity. Hashtags are an essential part of the ethos of social networks. They are used to denote brands, events, people, social rallies, etc. The hashtag disambiguation problem is to detect synonymous hashtags and recognize the polysemic ones. For example, the hashtag '#BHaram' refers to the entity 'Boko Haram', defined at Wikipedia page en.wikipedia.org/wiki/Boko_Haram or at National Counterterrorism Center Web web page www.nctc.gov/site/groups/boko_haram.html. The purpose of this project is to perform EL in social media. This work will benefit multiple segments of society that rely on applications using data from microblog systems, such as targeted monitoring of Twitter and Facebook to collect and understand users' opinions about a recent product or a world event; data aggregation (e.g., reviews about products and services); and data mining for early crisis detection and response as well as national security. This project is one more step towards addressing the government's latest initiative of fighting crime using big data.\r\n\r\nThe goals of this project are to research algorithms to detect in near real-time those pieces of text in messages that reference entities, Web pages that describe entities, and to link entity references to Web pages and across microblog systems so that together a broad, more complete characterization of each entity can be automatically generated. The proposed approaches are based on innovative techniques that include: incremental, iterative message analysis; smart indexing techniques with live updates to support fast incremental entity reference detection; computationally light soft-clustering of messages to improve entity reference detection; and fast incremental K-partite graph clustering. The resulting artifacts (e.g., software tools) will be made available to benefit researchers in academe and industry. Distribution of free, open-source software for implementing the techniques developed will enhance existing research infrastructure. The project will support and train at least three PhD students, as well as involve undergraduate students in research at Temple University and Binghampton University. The project web site (http://cis.temple.edu/~edragut/projects/nimel.htm) includes more information on the project, software, datasets, educational materials, and publications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eduard",
   "pi_last_name": "Dragut",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eduard Dragut",
   "pi_email_addr": "edragut@temple.edu",
   "nsf_id": "000583882",
   "pi_start_date": "2015-09-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yuhong",
   "pi_last_name": "Guo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuhong Guo",
   "pi_email_addr": "yuhong@temple.edu",
   "nsf_id": "000537682",
   "pi_start_date": "2015-09-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Temple University",
  "inst_street_address": "1805 N BROAD ST",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2157077547",
  "inst_zip_code": "191226104",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "PA02",
  "org_lgl_bus_name": "TEMPLE UNIVERSITY-OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "QD4MGHFDJKU1",
  "org_uei_num": "QD4MGHFDJKU1"
 },
 "perf_inst": {
  "perf_inst_name": "Temple University",
  "perf_str_addr": "1805 North Broad Street",
  "perf_city_name": "324 Wachman Hall, Phila.",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191226003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "PA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 783339.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goals of this project were to research algorithms to detect in near real-time those pieces of text in microblog streams that reference entities, Web pages that describe entities, and to link entity references to Web pages and across microblog systems so that together a broad, more complete characterization of each entity can be automatically generated.</p>\n<p>This project produced the following main outcomes:</p>\n<ul>\n</ul>\n<ul>\n<li>A new approach for detecting entity and entity mentions from microblog streams in near real-time. Experiments show that this approach achieves an average effectiveness improvement of 14.6% while maintaining at least 2.64 times higher throughput when compared to state-of-the-art systems. Many downstream applications can benefit from detected entity mentions. The source code of the system (TwiCS) that implements this approach and several annotated tweet datasets are share with the community at large (<a href=\"https://github.com/dalakada/TwiCSv2\">https://github.com/dalakada/TwiCSv2</a>).</li>\n<li>A comprehensive framework that includes a large number of strategies to systematically consolidate information from different sources about the same entity. Such a consolidation leads to more uniform entity representations that can be more effectively used by other applications. The source code and the data sets of this work is available at <a href=\"https://github.com/tomdyq/RecordNormalization\">https://github.com/tomdyq/RecordNormalization</a>. </li>\n<li>An approach to segment tweets with URLs into different classes. We showed that such a segmentation benefited downstream applications that use tweets for various analysis. For example, we gave an empirical study showing that the segmentation improved the accuracy of sentiment analysis.</li>\n<li>A human-in-the-loop machine learning framework for entity extraction<strong>. </strong>The proposed solution combines the power of regular expressions, the ability of deep learning to learn from massive data, and human-in-the-loop active learning into a new integrated framework for entity recognition. </li>\n<li>A new method that leverages social media signals for record linkage<strong>.</strong> This method uses user-generated contents that accompany many of the entities on the Web. It uses document-based distances, with an emphasis on word embedding document distances, to determine if two entities match.</li>\n<li>A flexible query-time record linkage and fusion framework, called ORLF. Web applications often require duplicate-free data and error-free representation of entities. The former goal is achieved through record linkage while the latter is achieved through data fusion. ORLF allows seamless application of record linkage and fusion over live data coming from multiple Web sources. </li>\n<li><strong>A new method for result merging for structured queries on the Deep. </strong>Data integration systems on the Deep Web offer a transparent means to query multiple data sources at once. Result merging ? the generation of an overall ranked list of results from different sources in response to a query ? is a key component of a data integration system. Different from the existing techniques for result merging, the new method estimates the relevance of a data source in answering a query at query time. </li>\n<li>A novel unsupervised heterogeneous domain adaptation (HAD) approach is created to bridge the representation gap between the source and target domains. HDA aims to adapt information across domains with different input feature spaces. This novel approach learns a sparse feature transformation function based on the data in both the source and target domains and a small number of existing parallel instances. </li>\n<li>8 PhD students, 2 MS students, and 6 undergraduate students have participated in this project. Out of these, 5 PhD students, all MS students and 5 undergraduate students have graduated. 5 of the students were female students.</li>\n<li>The project has produced 14 peer-reviewed publications, 2 of which are publications by undergraduate students.</li>\n</ul><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/25/2021<br>\n\t\t\t\t\tModified by: Eduard&nbsp;Dragut</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goals of this project were to research algorithms to detect in near real-time those pieces of text in microblog streams that reference entities, Web pages that describe entities, and to link entity references to Web pages and across microblog systems so that together a broad, more complete characterization of each entity can be automatically generated.\n\nThis project produced the following main outcomes:\n\n\n\nA new approach for detecting entity and entity mentions from microblog streams in near real-time. Experiments show that this approach achieves an average effectiveness improvement of 14.6% while maintaining at least 2.64 times higher throughput when compared to state-of-the-art systems. Many downstream applications can benefit from detected entity mentions. The source code of the system (TwiCS) that implements this approach and several annotated tweet datasets are share with the community at large (https://github.com/dalakada/TwiCSv2).\nA comprehensive framework that includes a large number of strategies to systematically consolidate information from different sources about the same entity. Such a consolidation leads to more uniform entity representations that can be more effectively used by other applications. The source code and the data sets of this work is available at https://github.com/tomdyq/RecordNormalization. \nAn approach to segment tweets with URLs into different classes. We showed that such a segmentation benefited downstream applications that use tweets for various analysis. For example, we gave an empirical study showing that the segmentation improved the accuracy of sentiment analysis.\nA human-in-the-loop machine learning framework for entity extraction. The proposed solution combines the power of regular expressions, the ability of deep learning to learn from massive data, and human-in-the-loop active learning into a new integrated framework for entity recognition. \nA new method that leverages social media signals for record linkage. This method uses user-generated contents that accompany many of the entities on the Web. It uses document-based distances, with an emphasis on word embedding document distances, to determine if two entities match.\nA flexible query-time record linkage and fusion framework, called ORLF. Web applications often require duplicate-free data and error-free representation of entities. The former goal is achieved through record linkage while the latter is achieved through data fusion. ORLF allows seamless application of record linkage and fusion over live data coming from multiple Web sources. \nA new method for result merging for structured queries on the Deep. Data integration systems on the Deep Web offer a transparent means to query multiple data sources at once. Result merging ? the generation of an overall ranked list of results from different sources in response to a query ? is a key component of a data integration system. Different from the existing techniques for result merging, the new method estimates the relevance of a data source in answering a query at query time. \nA novel unsupervised heterogeneous domain adaptation (HAD) approach is created to bridge the representation gap between the source and target domains. HDA aims to adapt information across domains with different input feature spaces. This novel approach learns a sparse feature transformation function based on the data in both the source and target domains and a small number of existing parallel instances. \n8 PhD students, 2 MS students, and 6 undergraduate students have participated in this project. Out of these, 5 PhD students, all MS students and 5 undergraduate students have graduated. 5 of the students were female students.\nThe project has produced 14 peer-reviewed publications, 2 of which are publications by undergraduate students.\n\n\n\t\t\t\t\tLast Modified: 06/25/2021\n\n\t\t\t\t\tSubmitted by: Eduard Dragut"
 }
}