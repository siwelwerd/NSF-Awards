{
 "awd_id": "1528037",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Fast, Scalable Joint Inference for NLP using Markov Logic",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 360348.0,
 "awd_amount": 360348.0,
 "awd_min_amd_letter_date": "2015-07-29",
 "awd_max_amd_letter_date": "2020-08-31",
 "awd_abstract_narration": "Many fundamental tasks in natural language processing (NLP) such as coreference resolution and event extraction involve complex output constraints. Markov Logic Networks (MLNs), a joint inference framework that combines logical and probabilistic representations, enable manual specification of such constraints in a compact manner, effectively allowing easy incorporation of background knowledge into NLP systems to improve their performance. While theoretically appealing, MLNs have been relatively underused in NLP applications.  Owing to issues of scalability, researchers have largely restricted themselves to simple MLNs that either make simplifying, sometimes unreasonable assumptions or ignore complex output constraints. This project seeks to bring transformative changes to the way joint inference is applied in NLP. The idea is to develop fast, scalable learning and inference techniques for MLNs so that rich models (i.e., models with high-dimensional features and/or complex output constraints) can be efficiently trained and applied to large data sets. A key component of the project is the formulation and evaluation of rich MLN-based models for important and complex NLP tasks such as coreference resolution. These rich models, especially when trained on large data sets, can potentially yield breakthrough results in NLP, which in turn can have profound societal impact. For example, improvements in coreference technologies stand to benefit essentially all NLP applications the general public relies on every day, such as search, information extraction, and question answering.\r\n\r\nSuccessful application of rich MLN-based models to complex NLP tasks requires the development of fast, scalable learning and inference techniques. To scale up weight learning in MLNs, this project develops approaches that leverage advanced algorithms from the constraint satisfaction literature for fast, approximate solution counting. To scale up probabilistic inference, it employs lifted inference algorithms to reduce the domain size of variables in MLNs by exploiting exact as well as approximate symmetries (e.g., paraphrases). The core NLP tasks it focuses on, such as coreference resolution and temporal relation extraction, are sufficiently complex that they provide convincing testbeds for evaluating the scalability of these learning and inference techniques. Equally importantly, as approximate language is a phenomenon that occurs across NLP tasks, these advances are likely to impact a wide swath of tasks in NLP.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vincent",
   "pi_last_name": "Ng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vincent Ng",
   "pi_email_addr": "vince@hlt.utdallas.edu",
   "nsf_id": "000182032",
   "pi_start_date": "2015-07-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vibhav",
   "pi_last_name": "Gogate",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vibhav Gogate",
   "pi_email_addr": "Vibhav.Gogate@utdallas.edu",
   "nsf_id": "000624381",
   "pi_start_date": "2015-07-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Dallas",
  "inst_street_address": "800 WEST CAMPBELL RD.",
  "inst_street_address_2": "SP2.25",
  "inst_city_name": "RICHARDSON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9728832313",
  "inst_zip_code": "750803021",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "TX24",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT DALLAS",
  "org_prnt_uei_num": "",
  "org_uei_num": "EJCVPNN1WFS5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Dallas",
  "perf_str_addr": "800 W. Campbell Rd.",
  "perf_city_name": "Richardson",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "750803021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "TX24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 360348.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project developed novel inference and learning algorithms, based on the Markov logic networks (MLNs) representation, for solving joint inference tasks in natural language processing (NLP). MLNs are a unifying framework for representing and reasoning about real-world domains that have relational structure and uncertainty. They use weighted first-order logic rules for generating statistical models where each rule describes patterns that are typically true in the domain and the weight specifies the strength or confidence in the rule; the higher the weight the higher the probability that the rule is true in the real world. A major issue with existing MLN methods is that although they perform well in domains having small number of variables or features, they perform poorly when the number of features is large. This issue is especially prevalent when MLNs are used to solve hard joint inference tasks in NLP, many of which have hundreds of thousands of features. At a high level, a hard joint inference task is a task which can be divided into sub-tasks such that the sub-tasks share information and are related to each other. As a result, rather than solving the inference task by solving the sub-tasks one by one (in a pipeline), it is beneficial to simultaneously (or jointly) reason about sub-tasks and their relationship.</p>\n<p>The project focused on scaling up existing Markov logic technology enabling it to accurately and efficiently solve hard joint inference tasks. Specifically, it focused on improving weight learning algorithms, which learn the weights of a given set of rules based on data, and developing novel algorithms for important query types, including combinatorial optimization problems (which find the best assignment given data and observations) and constrained optimization problems (which find the best assignment given data, constraints and observations). For weight learning, the project developed novel counting methods that reduce the number of rules one needs to consider; tractable architectures that enable efficient, polynomial time processing of rules; and size-aware robust methods that can be trained on datasets having a small number of variables and applied to datasets having a large number of variables with minor degradation in performance. For solving combinatorial optimization problems (both unconstrained and constrained versions), the project developed novel lower and upper bounding schemes using advanced sampling and Lagrange relaxation techniques; lifted inference algorithms that process a large number of rules in one go without explicitly enumerating them; and new specialized tractable architectures on which optimization problems can be solved accurately and in polynomial time.</p>\n<p>We tested our methods by applying them to build rich MLNs for complex NLP tasks. Applying MLNs to such NLP tasks has two key advantages. First, the use of first-order logic in MLNs compactly captures complex linguistic structure and background knowledge. Second, MLNs allow easy incorporation of feature combinations as constraints. We built MLNs for several challenging tasks in information extraction (IE), including event extraction and event coreference resolution.&nbsp;</p>\n<p>Event extraction is the task of extracting and labeling all instances in a text document that correspond to pre-defined event types. An event can have an arbitrary number of arguments (e.g., the times, places, and people involved in the event) that correspond to pre-defined argument types and is identified by a keyword called the trigger. The task is complicated by the fact that an event may serve as an argument of another event. We designed an MLN for biomedical event extaction that is composed of soft formulas (each of which encodes a soft constraint whose associated weight indicates how important it is to satisfy the constraint) and hard formulas (constraints that always need to be satisfied) to capture the relational dependencies between triggers and arguments. The resulting MLN achieved state-of-the-art results on standard biomedical event exraction datasets.</p>\n<p>Event coreference resolution involves determining which event mentions in a text refer to the same real-world event. For two event mentions to be coreferent, both their triggers and their corresponding arguments have to be compatible. However, identifying potential arguments (which is provided by an entity extraction system), linking arguments to their event mentions (which is performed by an event extraction system), and determining the compatibility between two event arguments (which is the job of an entity coreference resolver), are all non-trivial tasks. Worse still, the errors made by any of these upstream components in the IE pipeline can propagate to the event coreference resolver. To address this error propagation problem, we performed MLN-based joint inference over entity extraction, entity coreference, event extraction, and event coreference. Specifically, we encoded hard intra-task constraints (e.g., transitivity in the coreference tasks), hard inter-task constraints (e.g.,two event mentions cannot be coreferent if their types are not compatible), and soft inter-task constraints (e.g., two event mentions can be coreferent even if their corresponding arguments are different).&nbsp;The resulting MLN achieved state-of-the-art results on standard event coreference resolution datasets.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/21/2022<br>\n\t\t\t\t\tModified by: Vincent&nbsp;Ng</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project developed novel inference and learning algorithms, based on the Markov logic networks (MLNs) representation, for solving joint inference tasks in natural language processing (NLP). MLNs are a unifying framework for representing and reasoning about real-world domains that have relational structure and uncertainty. They use weighted first-order logic rules for generating statistical models where each rule describes patterns that are typically true in the domain and the weight specifies the strength or confidence in the rule; the higher the weight the higher the probability that the rule is true in the real world. A major issue with existing MLN methods is that although they perform well in domains having small number of variables or features, they perform poorly when the number of features is large. This issue is especially prevalent when MLNs are used to solve hard joint inference tasks in NLP, many of which have hundreds of thousands of features. At a high level, a hard joint inference task is a task which can be divided into sub-tasks such that the sub-tasks share information and are related to each other. As a result, rather than solving the inference task by solving the sub-tasks one by one (in a pipeline), it is beneficial to simultaneously (or jointly) reason about sub-tasks and their relationship.\n\nThe project focused on scaling up existing Markov logic technology enabling it to accurately and efficiently solve hard joint inference tasks. Specifically, it focused on improving weight learning algorithms, which learn the weights of a given set of rules based on data, and developing novel algorithms for important query types, including combinatorial optimization problems (which find the best assignment given data and observations) and constrained optimization problems (which find the best assignment given data, constraints and observations). For weight learning, the project developed novel counting methods that reduce the number of rules one needs to consider; tractable architectures that enable efficient, polynomial time processing of rules; and size-aware robust methods that can be trained on datasets having a small number of variables and applied to datasets having a large number of variables with minor degradation in performance. For solving combinatorial optimization problems (both unconstrained and constrained versions), the project developed novel lower and upper bounding schemes using advanced sampling and Lagrange relaxation techniques; lifted inference algorithms that process a large number of rules in one go without explicitly enumerating them; and new specialized tractable architectures on which optimization problems can be solved accurately and in polynomial time.\n\nWe tested our methods by applying them to build rich MLNs for complex NLP tasks. Applying MLNs to such NLP tasks has two key advantages. First, the use of first-order logic in MLNs compactly captures complex linguistic structure and background knowledge. Second, MLNs allow easy incorporation of feature combinations as constraints. We built MLNs for several challenging tasks in information extraction (IE), including event extraction and event coreference resolution. \n\nEvent extraction is the task of extracting and labeling all instances in a text document that correspond to pre-defined event types. An event can have an arbitrary number of arguments (e.g., the times, places, and people involved in the event) that correspond to pre-defined argument types and is identified by a keyword called the trigger. The task is complicated by the fact that an event may serve as an argument of another event. We designed an MLN for biomedical event extaction that is composed of soft formulas (each of which encodes a soft constraint whose associated weight indicates how important it is to satisfy the constraint) and hard formulas (constraints that always need to be satisfied) to capture the relational dependencies between triggers and arguments. The resulting MLN achieved state-of-the-art results on standard biomedical event exraction datasets.\n\nEvent coreference resolution involves determining which event mentions in a text refer to the same real-world event. For two event mentions to be coreferent, both their triggers and their corresponding arguments have to be compatible. However, identifying potential arguments (which is provided by an entity extraction system), linking arguments to their event mentions (which is performed by an event extraction system), and determining the compatibility between two event arguments (which is the job of an entity coreference resolver), are all non-trivial tasks. Worse still, the errors made by any of these upstream components in the IE pipeline can propagate to the event coreference resolver. To address this error propagation problem, we performed MLN-based joint inference over entity extraction, entity coreference, event extraction, and event coreference. Specifically, we encoded hard intra-task constraints (e.g., transitivity in the coreference tasks), hard inter-task constraints (e.g.,two event mentions cannot be coreferent if their types are not compatible), and soft inter-task constraints (e.g., two event mentions can be coreferent even if their corresponding arguments are different). The resulting MLN achieved state-of-the-art results on standard event coreference resolution datasets.\n\n\t\t\t\t\tLast Modified: 07/21/2022\n\n\t\t\t\t\tSubmitted by: Vincent Ng"
 }
}