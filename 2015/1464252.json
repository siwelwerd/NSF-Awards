{
 "awd_id": "1464252",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: What do you mean? -- Automatic identification of inferences drawn from text",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "D.  Langendoen",
 "awd_eff_date": "2015-02-01",
 "awd_exp_date": "2019-01-31",
 "tot_intn_awd_amt": 143374.0,
 "awd_amount": 143374.0,
 "awd_min_amd_letter_date": "2015-02-04",
 "awd_max_amd_letter_date": "2015-02-04",
 "awd_abstract_narration": "When dealing with language, readers/hearers understand far more than only the literal meaning of the words they read/hear. For instance, if your son's teacher tells you \"I doubt your son will pass his exam\", you will infer that your son will probably fail the exam. This project investigates how to automatically derive systematic inferences that people commonly draw. To approximate human understanding, it is essential for natural language processing (NLP) systems to develop broad-coverage models that capture what is conveyed in language without being explicitly said. The project focuses on how events are perceived: in the example above, do people believe that the son will pass? Accurately identifying events that are agreed upon and taken as facts has implications for any NLP task that require an accurate inference process, such as in information extraction.  The outcomes of the project consist of a better grasp of how linguistic insights can be used to automatically approximate human-level understanding, and publicly available data that will serve to develop robust, broad-coverage NLP systems as well as to evaluate and sharpen linguistic theories.\r\n \r\nThe goal of automatically deriving common inferences is pursued by developing classifiers that bring in, as features, linguistic insights studied in semantics and pragmatics, and by constructing a dataset of naturally occurring examples, from different genres, annotated with humans' intuitions via crowdsourcing techniques. A large body of work in NLP is recently focusing on the power of \"surface\" features for NLP tasks. But such features are reaching a limit. This project demonstrates how specialized linguistic features go beyond what can be approximated with surface-level information given available data and leads to fundamental advances in NLP systems. Independently of performance on NLP tasks, semantic and pragmatic features are of interest from a theoretical linguistics perspective. By quantitatively studying the interactions of linguistic features on a large amount of naturally-occurring examples, this project has an impact not only for NLP but also for semantic and pragmatic theories.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marie-Catherin",
   "pi_last_name": "de Marneffe",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Marie-Catherin H de Marneffe",
   "pi_email_addr": "demarneffe.1@osu.edu",
   "nsf_id": "000676570",
   "pi_start_date": "2015-02-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "The Ohio State University",
  "perf_str_addr": "Department of Linguistics",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101219",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 143374.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>When faced with a piece of text, humans understand far more than just the literal meaning of the words in the text. In even our most mundane interactions, much of what we communicate is not said explicitly but rather inferred. For instance, if I ask a friend to lunch and she replies, \"I had a very large breakfast\", I might infer that she won't go, even though she (perhaps deliberately) avoided saying so directly. If we want Natural Language Processing (NLP) systems capable of providing text understanding comparable to what humans achieve, it is important to automatically capture the kind of information that a reader will reliably extract from an utterance within a discourse, going beyond the literal meaning of words. The project focused on two kinds of inference, for improving NLP systems.<br /><br />The first case study aimed at improving classifiers of agree-disagreement in dialogue. For instance in (1), people infer that speaker B disagrees with speaker A.<br /><br />(1) A: God doesn't take away sinful desires. You've never had sinful desires? I know I have.<span> <br /></span>&nbsp; &nbsp; &nbsp;B: Yes, God does take away sinful desires. (If you ask Him.) I'm not saying that it doesn't take any work on your part, though.<br /><br />Most NLP systems rely on surface features and would learn that in general \"yes\" is a good cue of agreement. But in (1), to adequately infer that B disagrees with A despite the presence of \"yes\", it is necessary to capture the polarity mismatch between the speaker A's first sentence and speaker B's first sentence (\"God doesn't take away sinful desires\" vs. \"Yes, God does take away sinful desires\"). We showed that linguistic features go beyond what can be approximated with surface-level information. We introduced semantic environment features derived by comparing speakers' sentences which align well and demonstrated that adding such features improves classifier accuracy relative to baseline methods.<br /><br />The second case study illustrated how looking at pragmatic information of what speakers are committed to can improve NLP applications, and also provide insights to questions in theoretical linguistics. Our understanding of an utterance depends on assessing the extent to which the speaker stands by the event she describes. An unadorned declarative like \"The cancer has spread\" conveys firm speaker commitment of the cancer having spread, whereas \"There are some indicators that the cancer has spread\" imbues the claim with uncertainty. It is not only the absence vs. presence of embedding material that determines whether or not a speaker is committed to the event described: from (2) we will infer that the speaker is committed to there <strong>being</strong> war, whereas in (3) we will infer the speaker is committed to relocating species <strong>not being</strong> a panacea, even though the clauses that describe the events in (2) and (3) are both embedded under \"(s)he doesn't believe\".<br /><br />(2) The problem, I'm afraid, with my colleague here, he really doesn't believe that it's war.<br /><br />(3)&nbsp; Transplanting an ecosystem can be risky, as history shows. Hellmann doesn't believe that relocating species threatened by climate change is a panacea.<br /><br />We focused first on an application-driven perspective. Previous work has tried to predict the outcome of contests (such as the Oscars or elections) from tweets. We showed that by distinguishing tweets that convey firm speaker commitment toward a given outcome (e.g., \"Dunkirk will win Best Picture in 2018\") from ones that only suggest the outcome (e.g., \"Dunkirk might have a shot at the 2018 Oscars\") or tweets that convey the negation of the event (\"Dunkirk is good but not academy level good for the Oscars\"), we can outperform previous methods. From a theoretical perspective, we constructed a corpus of naturally-occurring sentences (such as (2) and (3)) in context, rather than focusing only on the sentence level (as it is the case in tweets) annotated for different linguistic factors (verb tense, subject of the verb, plausibility of the event) and showed the potential that the corpus offers to complement existing experimental work in deepening our understanding of the factors at play in identifying speaker commitment, as well as in evaluating state-of-the-art computational systems of speaker commitment.</p>\n<p>Overall, the project showed that linguistic insights can be used to automatically approximate human-level understanding and provided publicly available data that will serve to develop robust, broad-coverage NLP systems as well as to evaluate and sharpen linguistic theories. The project also helped build a bridge between theoretical and computational linguistics, focusing on a better synergy between both fields.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/07/2019<br>\n\t\t\t\t\tModified by: Marie-Catherin&nbsp;H&nbsp;De Marneffe</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWhen faced with a piece of text, humans understand far more than just the literal meaning of the words in the text. In even our most mundane interactions, much of what we communicate is not said explicitly but rather inferred. For instance, if I ask a friend to lunch and she replies, \"I had a very large breakfast\", I might infer that she won't go, even though she (perhaps deliberately) avoided saying so directly. If we want Natural Language Processing (NLP) systems capable of providing text understanding comparable to what humans achieve, it is important to automatically capture the kind of information that a reader will reliably extract from an utterance within a discourse, going beyond the literal meaning of words. The project focused on two kinds of inference, for improving NLP systems.\n\nThe first case study aimed at improving classifiers of agree-disagreement in dialogue. For instance in (1), people infer that speaker B disagrees with speaker A.\n\n(1) A: God doesn't take away sinful desires. You've never had sinful desires? I know I have. \n     B: Yes, God does take away sinful desires. (If you ask Him.) I'm not saying that it doesn't take any work on your part, though.\n\nMost NLP systems rely on surface features and would learn that in general \"yes\" is a good cue of agreement. But in (1), to adequately infer that B disagrees with A despite the presence of \"yes\", it is necessary to capture the polarity mismatch between the speaker A's first sentence and speaker B's first sentence (\"God doesn't take away sinful desires\" vs. \"Yes, God does take away sinful desires\"). We showed that linguistic features go beyond what can be approximated with surface-level information. We introduced semantic environment features derived by comparing speakers' sentences which align well and demonstrated that adding such features improves classifier accuracy relative to baseline methods.\n\nThe second case study illustrated how looking at pragmatic information of what speakers are committed to can improve NLP applications, and also provide insights to questions in theoretical linguistics. Our understanding of an utterance depends on assessing the extent to which the speaker stands by the event she describes. An unadorned declarative like \"The cancer has spread\" conveys firm speaker commitment of the cancer having spread, whereas \"There are some indicators that the cancer has spread\" imbues the claim with uncertainty. It is not only the absence vs. presence of embedding material that determines whether or not a speaker is committed to the event described: from (2) we will infer that the speaker is committed to there being war, whereas in (3) we will infer the speaker is committed to relocating species not being a panacea, even though the clauses that describe the events in (2) and (3) are both embedded under \"(s)he doesn't believe\".\n\n(2) The problem, I'm afraid, with my colleague here, he really doesn't believe that it's war.\n\n(3)  Transplanting an ecosystem can be risky, as history shows. Hellmann doesn't believe that relocating species threatened by climate change is a panacea.\n\nWe focused first on an application-driven perspective. Previous work has tried to predict the outcome of contests (such as the Oscars or elections) from tweets. We showed that by distinguishing tweets that convey firm speaker commitment toward a given outcome (e.g., \"Dunkirk will win Best Picture in 2018\") from ones that only suggest the outcome (e.g., \"Dunkirk might have a shot at the 2018 Oscars\") or tweets that convey the negation of the event (\"Dunkirk is good but not academy level good for the Oscars\"), we can outperform previous methods. From a theoretical perspective, we constructed a corpus of naturally-occurring sentences (such as (2) and (3)) in context, rather than focusing only on the sentence level (as it is the case in tweets) annotated for different linguistic factors (verb tense, subject of the verb, plausibility of the event) and showed the potential that the corpus offers to complement existing experimental work in deepening our understanding of the factors at play in identifying speaker commitment, as well as in evaluating state-of-the-art computational systems of speaker commitment.\n\nOverall, the project showed that linguistic insights can be used to automatically approximate human-level understanding and provided publicly available data that will serve to develop robust, broad-coverage NLP systems as well as to evaluate and sharpen linguistic theories. The project also helped build a bridge between theoretical and computational linguistics, focusing on a better synergy between both fields.\n\n\t\t\t\t\tLast Modified: 05/07/2019\n\n\t\t\t\t\tSubmitted by: Marie-Catherin H De Marneffe"
 }
}