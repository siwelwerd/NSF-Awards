{
 "awd_id": "1544895",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: TTP Option: Synergy: Collaborative Research: Nested Control of Assistive Robots through Human Intent Inference",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-10-01",
 "awd_exp_date": "2020-09-30",
 "tot_intn_awd_amt": 602999.0,
 "awd_amount": 602999.0,
 "awd_min_amd_letter_date": "2015-09-14",
 "awd_max_amd_letter_date": "2016-02-26",
 "awd_abstract_narration": "Part 1: Upper-limb motor impairments arise from a wide range of clinical conditions including amputations, spinal cord injury, or stroke. Addressing lost hand function, therefore, is a major focus of rehabilitation interventions; and research in robotic hands and hand exoskeletons aimed at restoring fine motor control functions gained significant speed recently. Integration of these robots with neural control mechanisms is also an ongoing research direction. We will develop prosthetic and wearable hands controlled via nested control that seamlessly blends neural control based on human brain activity and dynamic control based on sensors on robots. These Hand Augmentation using Nested Decision (HAND) systems  will also provide rudimentary tactile feedback to the user. The HAND design framework will contribute to the assistive and augmentative robotics field. The resulting technology will improve the quality of life for individuals with lost limb function. The project will help train engineers skilled in addressing multidisciplinary challenges. Through outreach activities, STEM careers will be promoted at the K-12 level, individuals from underrepresented groups in engineering will be recruited to engage in this research project, which will contribute to the diversity of the STEM workforce.\r\n\r\nPart 2: The team previously introduced the concept of human-in-the-loop cyber-physical systems (HILCPS). Using the HILCPS hardware-software co-design and automatic synthesis infrastructure, we will develop prosthetic and wearable HAND systems that are robust to uncertainty in human intent inference from physiological signals. One challenge arises from the fact that the human and the cyber system jointly operate on the same physical element. Synthesis of networked real-time applications from algorithm design environments poses a framework challenge. These will be addressed by a tightly coupled optimal nested control strategy that relies on EEG-EMG-context fusion for human intent inference. Custom distributed embedded computational and robotic platforms will be built and iteratively refined. This work will enhance the HILCPS design framework, while simultaneously making novel contributions to body/brain interface technology and assistive/augmentative robot technology. Specifically we will (1) develop a theoretical EEG-EMG-context fusion framework for agile HILCPS application domains; (2) develop theory for and design novel control theoretic solutions to handle uncertainty, blend motion/force planning with high-level human intent and ambient intelligence to robustly execute daily manipulation activities; (3) further develop and refine the HILCPS domain-specific design framework to enable rapid deployment of HILCPS algorithms onto distributed embedded systems, empowering a new class of real-time algorithms that achieve distributed embedded sensing, analysis, and decision making; (4) develop new paradigms to replace, retrain or augment hand function via the prosthetic/wearable HAND by optimizing performance on a subject-by-subject basis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Deniz",
   "pi_last_name": "Erdogmus",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Deniz Erdogmus",
   "pi_email_addr": "erdogmus@ece.neu.edu",
   "nsf_id": "000483728",
   "pi_start_date": "2015-09-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Taskin",
   "pi_last_name": "Padir",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Taskin Padir",
   "pi_email_addr": "t.padir@northeastern.edu",
   "nsf_id": "000531489",
   "pi_start_date": "2016-02-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gunar",
   "pi_last_name": "Schirner",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gunar Schirner",
   "pi_email_addr": "schirner@ece.neu.edu",
   "nsf_id": "000552705",
   "pi_start_date": "2015-09-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "8235",
   "pgm_ref_txt": "CPS-Synergy"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 602999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The primary goal of this project was to develop techniques for (a) estimating the intended hand grasp shape of a human as the arm approaches an object with electrophysiological activity of muscles from the forearm and cameras on the hand or at the eye level; (b) shared control of a light-weight but grasp-capable robotic hand. The intended applications included prosthetics, but methodologies and designs could benefit exoskeletons used in rehabilitation after stroke, or for teleoperation.</p>\n<p>In the course of the project we developed multiple versions of the system, each time making changes and improvements based on lessons learned. One of the most surprising lessons we learned was that, counter to our initial expectation, cameras located on the robotic hand are not as useful as a camera that provides the human/user point-of-view looking at the object and the scene from a perspective removed from the trajectory of the hand. The main issue with camera(s) on the hand was that, despite carious attempts, even including up to 4 cameras, even though we were able to get good visual coverage around the hand, the visual evidence from cameras on the hand were useful only too late in the reach-to-grasp process. By the time cameras saw the object for computer vision to provide potential grasp options that are appropriate/anticipated, the electromyography (EMG) signals from the forearm was already highly informative about the human's intended grasp type.</p>\n<p>The final design involved fusing probabilistically visual evidence from a point-of-view camera and multiple channels of EMG capturing various muscle activity from the forearm. The setup is illustrated in Figure 1 below. On the robotic hand side, after multiple iterations, we have converged on a design that weighs 333grams (11 oz), costs les than $175 in parts, and is able to lift over 500grams (18oz) of weight.</p>\n<p>This research project supported the education of multiple doctoral and bachelor degree-seeking students. These students have graduated and joined the national workforce in technology research and development divisions of industry or other universities, or went to pursue higher degrees in artificial intelligence, medicine, or miomedical engineering.</p>\n<p>Two patents were awarded and a spinoff company emerged from the team, and some of these outcomes are now available commercially through these technology transfer efforts.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/21/2021<br>\n\t\t\t\t\tModified by: Deniz&nbsp;Erdogmus</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1544895/1544895_10399074_1621631715615_Fig1_InferenceSystem--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1544895/1544895_10399074_1621631715615_Fig1_InferenceSystem--rgov-800width.jpg\" title=\"Figure 1: EMG-Vision fusion for intended-grasp inference\"><img src=\"/por/images/Reports/POR/2021/1544895/1544895_10399074_1621631715615_Fig1_InferenceSystem--rgov-66x44.jpg\" alt=\"Figure 1: EMG-Vision fusion for intended-grasp inference\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 1: EMG-Vision fusion for intended-grasp inference</div>\n<div class=\"imageCredit\">?</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Deniz&nbsp;Erdogmus</div>\n<div class=\"imageTitle\">Figure 1: EMG-Vision fusion for intended-grasp inference</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe primary goal of this project was to develop techniques for (a) estimating the intended hand grasp shape of a human as the arm approaches an object with electrophysiological activity of muscles from the forearm and cameras on the hand or at the eye level; (b) shared control of a light-weight but grasp-capable robotic hand. The intended applications included prosthetics, but methodologies and designs could benefit exoskeletons used in rehabilitation after stroke, or for teleoperation.\n\nIn the course of the project we developed multiple versions of the system, each time making changes and improvements based on lessons learned. One of the most surprising lessons we learned was that, counter to our initial expectation, cameras located on the robotic hand are not as useful as a camera that provides the human/user point-of-view looking at the object and the scene from a perspective removed from the trajectory of the hand. The main issue with camera(s) on the hand was that, despite carious attempts, even including up to 4 cameras, even though we were able to get good visual coverage around the hand, the visual evidence from cameras on the hand were useful only too late in the reach-to-grasp process. By the time cameras saw the object for computer vision to provide potential grasp options that are appropriate/anticipated, the electromyography (EMG) signals from the forearm was already highly informative about the human's intended grasp type.\n\nThe final design involved fusing probabilistically visual evidence from a point-of-view camera and multiple channels of EMG capturing various muscle activity from the forearm. The setup is illustrated in Figure 1 below. On the robotic hand side, after multiple iterations, we have converged on a design that weighs 333grams (11 oz), costs les than $175 in parts, and is able to lift over 500grams (18oz) of weight.\n\nThis research project supported the education of multiple doctoral and bachelor degree-seeking students. These students have graduated and joined the national workforce in technology research and development divisions of industry or other universities, or went to pursue higher degrees in artificial intelligence, medicine, or miomedical engineering.\n\nTwo patents were awarded and a spinoff company emerged from the team, and some of these outcomes are now available commercially through these technology transfer efforts.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/21/2021\n\n\t\t\t\t\tSubmitted by: Deniz Erdogmus"
 }
}