{
 "awd_id": "1506786",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "HCC: Medium: Collaborative Research: Generating Accurate, Understandable Sign Language Animations Based on Analysis of Human Signing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2014-08-20",
 "awd_exp_date": "2016-06-30",
 "tot_intn_awd_amt": 59964.0,
 "awd_amount": 59964.0,
 "awd_min_amd_letter_date": "2014-11-13",
 "awd_max_amd_letter_date": "2014-11-13",
 "awd_abstract_narration": "American Sign Language (ASL) animations have the potential to make information accessible to many deaf adults in the United States who possess only limited English literacy. In this research, which involves collaboration across three institutions, the PIs' goal is to gain a better understanding of ASL linguistics through computational techniques while advancing the state of the art in the generation of ASL animations for accessibility applications for people who are deaf.  To these ends, the PIs will develop linguistically based models of two aspects of ASL production: movements required for head gestures and facial expressions that carry essential grammatical information and frequently extend over domains larger than a single sign, and the timing and coordination of manual and non-manual elements of ASL signing.  Preliminary work has shown that these issues significantly affect how well signers understand ASL animations, and that these aspects of current ASL animation technologies require improvement.  How should the face of a human or animated character be articulated to perform, with accuracy, the linguistically meaningful facial expressions that are part of ASL grammar?  How should the onsets, offsets, and transitions of these movements be produced?  How should the facial expressions and hand movements be temporally coordinated so that the ASL production is as grammatically correct and understandable as possible?  To answer open questions such as these, the PIs' novel approach will apply techniques from computer vision to linguistically annotated video data collected from human signers, in order to produce models for use in animation-production. The PIs will expand their existing annotated video ASL corpora through new data collection and annotation, and will analyze these data to study the use, timing, and synchronization of manual and non-manual components of ASL production.  The annotated videos will be used to train high quality computer vision models for recognition of linguistically significant facial expressions and timing subtleties.  Parameters of these computer vision models will be used to hypothesize computational models of ASL timing and facial movements, to be incorporated into ASL-animation generation software and evaluated by native signers.   The models will be iteratively refined in cycles of user-based studies and incorporated into ASL animation technologies to more accurately mimic human signing.  Project outcomes will include high quality models of the movement of virtual human characters for animations of ASL performance.  The analysis of video corpora of ASL will produce new linguistic insights into the micro-facial expressions and the temporal coordination of the face and hands in ASL production, while advances in the analysis of ASL prosody will contribute to an understanding of the fundamental commonalities and modality-specific differences between signed and spoken languages that is essential to a full understanding of the human language faculty. The creation of new modeling approaches and recognition techniques will advance the field of computer vision, by benefiting the identification and tracking of the human face and body in video during the rapid and complex movements of ASL (and other forms of human movement).\r\n\r\nBroader Impacts:  This research will lead to significant improvements to technology for generating linguistically accurate ASL animations, which will make information, applications, websites, and services more accessible to the large number of deaf individuals with relatively low English literacy.  Advances in computer vision techniques for recognizing ASL in videos of humans will have general applicability in human-computer interaction, recognition and animation of facial expressions, and computer vision.  The corpora created in this project will enable students and researchers in both linguistics and computer science (including those without access to the requisite technological and human resources to carry out their own data collection from native signers and time-intensive linguistic annotations) to engage in research on ASL.  The techniques to be developed will also enable partial automation of the time-consuming creation of annotated ASL video corpora.  As in the PIs' earlier work, the proposed research will create opportunities for people who are deaf and members of other underrepresented groups to participate in scientific research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matt",
   "pi_last_name": "Huenerfauth",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matt Huenerfauth",
   "pi_email_addr": "matt.huenerfauth@rit.edu",
   "nsf_id": "000220138",
   "pi_start_date": "2014-11-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rochester Institute of Tech",
  "inst_street_address": "1 LOMB MEMORIAL DR",
  "inst_street_address_2": "",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5854757987",
  "inst_zip_code": "146235603",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "ROCHESTER INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "J6TWTRKC1X14"
 },
 "perf_inst": {
  "perf_inst_name": "Rochester Institute of Tech",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146235603",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 59964.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Signed languages make essential use of nonmanual expressions (facial expressions and gestures of the head and upper body) to convey grammatical information, in addition to expressing various types of attitudes and emotions.&nbsp; For example, critical information about sentence type (affirmative or negative, statement or specific type of question, e.g.) is often conveyed solely or principally through such nonmanual gestures. Thus, computer-based sign language recognition must take the nonmanual channel into account; likewise, computer-based sign language generation, e.g. via avatars, needs to be able to reproduce the appropriate expressions in appropriate synchrony with manual signing. These are difficult challenges, and the limitations of current systems in these respects adversely affects the accuracy with which sign language can be interpreted by computer and the ability to animate natural-looking signing avatars expressing appropriate linguistic information.</p>\n<p>The goal of this collaborative project has been to advance the state of the art with respect to our linguistic understanding of the use of these expressions and the technology to enable sign language recognition and generation to incorporate appropriate linguistic modeling of these phenomena. The potential benefits of such improvements are great, with applicability to a wide range of tools and technologies that would facilitate human and machine communication with the nearly half a million people in the United States for whom American Sign Language (ASL) is the primary means of communication. Although many naively assume that Deaf signers can simply read and write English to communicate, in reality standardized testing has revealed lower levels of English literacy among many deaf adults. This project has investigated how human ASL signers produce such facial expressions and how computer software can generate these facial expressions automatically for a wide variety of ASL sentences.</p>\n<p>This has been a collaborative research effort involving three interdisciplinary teams: linguistic researchers at Boston University, computer vision and machine learning researchers at Rutgers University, and researchers on accessibility technology for people who are deaf at the Rochester Institute of Technology (RIT).&nbsp; At Boston University, recordings of human signers producing a variety of ASL sentences were collected, and the BU research team carefully annotated the linguistic properties of each video, while also investigating fundamental linguistic issues in ASL.&nbsp; Rutgers University developed new computational techniques for tracking and recognizing the movements of the face in video, and then analyzed the recordings from BU to model how the human face moves in various ASL grammatical constructions in a range of sentences; they also made possible visual display of the results from the tracking of facial and head movements. &nbsp;RIT created software for generating computer animations of a virtual human signer, capable of performing a wide variety of ASL sentences and making subtle face movements. Based on the collection of video recordings and analysis data, RIT used statistical machine-learning techniques to learn how various parts of the face move over time, when performing each type of ASL facial expression, in a variety of sentences or linguistic contexts. &nbsp;This software can predict how the face should move during a particular ASL sentence, including new sentences that were never previously recorded during the project.&nbsp; This technology can therefore enable automatic generation of animations of ASL sentences, including correct facial expressions.</p>\n<p>To evaluate the accuracy of this software, computer animations of ASL were generated using these models of face movement, and experimental studies were conducted in which deaf participants evaluated the understandability of the resulting ASL animations.&nbsp; In this way, multiple versions of the software could be compared during the course of the project, to determine the most accurate way to produce facial expressions, so that ASL animations would be as linguistically accurate and understandable as possible.</p>\n<p>The linguistically annotated and computationally analyzed video data from this project will be shared publicly via the Data Access Interface (DAI) now in the final stages of development (with funding from NSF grant #CNS-1059218). This will provide significant new data for both linguistic and computer science research.</p>\n<p>In addition to creating technology that can help to produce a more inclusive and accessible society, this project also created new research opportunities for deaf and hearing students at all levels (high school students as well as undergraduate and graduate students at the partner universities). Students have been involved in all aspects of the project and have participated in discussions of the interdisciplinary research. In many cases, those who have participated in the project have gone (or will go) on to higher-level educational training and/or careers in related areas, including education, interpretation, linguistics, and computer science.</p>\n<p>The scientific results of the research on ASL linguistics, ASL animation, and computer vision technology have been published in a variety of high-quality scientific journals and conferences. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/28/2016<br>\n\t\t\t\t\tModified by: Matt&nbsp;Huenerfauth</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSigned languages make essential use of nonmanual expressions (facial expressions and gestures of the head and upper body) to convey grammatical information, in addition to expressing various types of attitudes and emotions.  For example, critical information about sentence type (affirmative or negative, statement or specific type of question, e.g.) is often conveyed solely or principally through such nonmanual gestures. Thus, computer-based sign language recognition must take the nonmanual channel into account; likewise, computer-based sign language generation, e.g. via avatars, needs to be able to reproduce the appropriate expressions in appropriate synchrony with manual signing. These are difficult challenges, and the limitations of current systems in these respects adversely affects the accuracy with which sign language can be interpreted by computer and the ability to animate natural-looking signing avatars expressing appropriate linguistic information.\n\nThe goal of this collaborative project has been to advance the state of the art with respect to our linguistic understanding of the use of these expressions and the technology to enable sign language recognition and generation to incorporate appropriate linguistic modeling of these phenomena. The potential benefits of such improvements are great, with applicability to a wide range of tools and technologies that would facilitate human and machine communication with the nearly half a million people in the United States for whom American Sign Language (ASL) is the primary means of communication. Although many naively assume that Deaf signers can simply read and write English to communicate, in reality standardized testing has revealed lower levels of English literacy among many deaf adults. This project has investigated how human ASL signers produce such facial expressions and how computer software can generate these facial expressions automatically for a wide variety of ASL sentences.\n\nThis has been a collaborative research effort involving three interdisciplinary teams: linguistic researchers at Boston University, computer vision and machine learning researchers at Rutgers University, and researchers on accessibility technology for people who are deaf at the Rochester Institute of Technology (RIT).  At Boston University, recordings of human signers producing a variety of ASL sentences were collected, and the BU research team carefully annotated the linguistic properties of each video, while also investigating fundamental linguistic issues in ASL.  Rutgers University developed new computational techniques for tracking and recognizing the movements of the face in video, and then analyzed the recordings from BU to model how the human face moves in various ASL grammatical constructions in a range of sentences; they also made possible visual display of the results from the tracking of facial and head movements.  RIT created software for generating computer animations of a virtual human signer, capable of performing a wide variety of ASL sentences and making subtle face movements. Based on the collection of video recordings and analysis data, RIT used statistical machine-learning techniques to learn how various parts of the face move over time, when performing each type of ASL facial expression, in a variety of sentences or linguistic contexts.  This software can predict how the face should move during a particular ASL sentence, including new sentences that were never previously recorded during the project.  This technology can therefore enable automatic generation of animations of ASL sentences, including correct facial expressions.\n\nTo evaluate the accuracy of this software, computer animations of ASL were generated using these models of face movement, and experimental studies were conducted in which deaf participants evaluated the understandability of the resulting ASL animations.  In this way, multiple versions of the software could be compared during the course of the project, to determine the most accurate way to produce facial expressions, so that ASL animations would be as linguistically accurate and understandable as possible.\n\nThe linguistically annotated and computationally analyzed video data from this project will be shared publicly via the Data Access Interface (DAI) now in the final stages of development (with funding from NSF grant #CNS-1059218). This will provide significant new data for both linguistic and computer science research.\n\nIn addition to creating technology that can help to produce a more inclusive and accessible society, this project also created new research opportunities for deaf and hearing students at all levels (high school students as well as undergraduate and graduate students at the partner universities). Students have been involved in all aspects of the project and have participated in discussions of the interdisciplinary research. In many cases, those who have participated in the project have gone (or will go) on to higher-level educational training and/or careers in related areas, including education, interpretation, linguistics, and computer science.\n\nThe scientific results of the research on ASL linguistics, ASL animation, and computer vision technology have been published in a variety of high-quality scientific journals and conferences.  \n\n \n\n\t\t\t\t\tLast Modified: 09/28/2016\n\n\t\t\t\t\tSubmitted by: Matt Huenerfauth"
 }
}