{
 "awd_id": "1463960",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI-Small: Context-Driven Haptic Inquiry of Objects Based on Task Requirements for Artificial Grasp and Manipulation",
 "cfda_num": "47.041",
 "org_code": "07020000",
 "po_phone": "7032922895",
 "po_email": "cpayne@nsf.gov",
 "po_sign_block_name": "Christina Payne",
 "awd_eff_date": "2014-07-01",
 "awd_exp_date": "2018-09-30",
 "tot_intn_awd_amt": 454632.0,
 "awd_amount": 454632.0,
 "awd_min_amd_letter_date": "2014-10-14",
 "awd_max_amd_letter_date": "2014-10-14",
 "awd_abstract_narration": "PI: Santos, Veronica\r\nProposal Number: 1208519\r\n\r\nIntellectual Merit: Human-like dexterous manipulation is featured prominently as a grand challenge in the 2009 Roadmap for U.S. Robotics' report. Human dexterity relies heavily on tactile sensation and is influenced by proprioceptive and visual feedback. The proposed work aims to advance artificial manipulators by integrating a new class of multimodal tactile sensors with anthropomorphic artificial hands and developing generalizable routines for context-driven haptic inquiry of objects based on task requirements for artificial grasp and manipulation. A primary goal is the development of capabilities for a robot hand to efficiently learn about objects in its unstructured environment through touch, specifically for cases where computer vision would fail to provide critical information about the physical hand-object interactions. While computer vision provides preliminary information about an object and its environment, vision alone cannot provide all essential information necessary for successful physical hand-object interactions. This is especially true when digits are occluded by the grasped object, and when the hand-object interaction is completely out of view. Inspiration for the haptic inquiry framework will be drawn from a suite of human haptic exploration procedures. In contrast to haptic exploration, haptic inquiry will require that the order and time spent on each exploratory procedure depend on task goals. The order and type of questions to be asked haptically will be context-dependent and designed to yield high-level, task-directed information at a low cost of inquiry. The weight given to each mode of tactile sensing (force, vibration, temperature) will also be tuned according to the context of the task.\r\nThis proposal aims to strengthen the robustness of co-robot systems by developing a framework for context-driven, task-directed haptic inquiry that integrates multi-digit tactile and proprioception data in a task-appropriate manner. The framework will be developed and deployed on an anthropomorphic robot hand outfitted with a new class of commercially-available multimodal tactile sensors. The work is transformative because it will enable co-robot systems to remain functional even in the absence of visual feedback, which is typically the primary form of feedback for robotic systems. The long-term research objective of this proposal is to reduce the cognitive burden on the user of an artificial manipulator. \r\n\r\nBroader Impacts: The proposed translational research could enhance the functional capabilities of co-robot systems in which humans use artificial manipulators to work in unstructured, unsafe, or limited access environments (prosthetic, rehabilitative, assistive, space, underwater, military, rescue, surgery).  The proposed work could benefit the human user of a co-robot system by empowering the robot with the ability to control low-level perception-action loops autonomously without burdening the human. The ROS operating system may be used to simulate and control an anthropomorphic robot hand outfitted with commercially-available tactile sensors using commercially-available actuators. Custom source code (C, MATLAB, ROS) and an open source haptic library for a commercially-available tactile sensor (suitable for data mining) will be made publicly available for the benefit and advancement of the robotics community.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CBET",
 "org_div_long_name": "Division of Chemical, Bioengineering, Environmental, and Transport Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Veronica",
   "pi_last_name": "Santos",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Veronica J Santos",
   "pi_email_addr": "vjsantos@ucla.edu",
   "nsf_id": "000521933",
   "pi_start_date": "2014-10-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900952000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 454632.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Intellectual Merit: </strong>The long-term research objective was to enhance the robustness of artificial manipulators for human-robot systems that will enable grasp and manipulation in the absence of visual feedback. Research Goals: 1) Establish relationships between multi-digit tactile flow and task-relevant features of an object, 2) Develop algorithms for task-appropriate weighting and integration of multimodal sensor data for haptic inquiry, 3) Develop a framework for context-driven, task-directed haptic inquiry that determines an efficient line of questioning based on prior information.</p>\n<p><span style=\"text-decoration: underline;\">Toward Goal #1:</span> We developed a highly sensorized robot hand testbed called the 'BairClaw.' The multi-articulating, anthropomorphic robot testbed can be used to study proprioceptive and tactile sensory stimuli during finger-object interactions. A remote actuation system enables the modular control of tendon-driven hands. The artificial proprioception system enables direct measurement of joint angles and tendon tensions while temperature, vibration, and skin deformation are provided by a multimodal tactile sensor. Conceived for artificial grasp, manipulation, and haptic exploration, the BairClaw could also be used for future studies on the neurorehabilitation of somatosensory disorders due to upper limb impairment or loss.</p>\n<p><span style=\"text-decoration: underline;\">Toward Goal #2:</span> Haptic perception of salient geometric features such as edges would be useful when vision is obstructed or when proprioceptive feedback is inadequate. We developed an approach for the artificial haptic perception of edge orientation. A robot hand outfitted with a deformable, bladder-type, multimodal tactile sensor was used to replay human-inspired haptic 'exploratory procedures.' Electrode impedance signals provided the most useful inputs by encoding spatially asymmetric skin deformation across the entire fingertip. Interestingly, sensor regions that were not in direct contact with the stimulus provided particularly useful information. We also used exploratory procedures to haptically perceive fingertip-sized geometric features. The geometric features varied by type (bump, pit), curvature (planar, conical, spherical), and footprint dimension (1.25 - 20 mm). Tactile signals generated by fingertip motions were used to extract parameters for use as inputs to supervised learning models. A support vector classifier was developed to perceive local shape while support vector regression models were developed to perceive size.</p>\n<p><span style=\"text-decoration: underline;\">Toward Goal #3:</span> We developed an approach for real-time haptic perception and decision-making for a haptics-driven, functional contour-following task: the closure of a ziplock bag. This task is challenging for robots because the bag is deformable, transparent, and visually occluded by artificial fingertip sensors that are also compliant. A deep neural net classifier was trained to estimate the state of a zipper within a robot's pinch grasp. A Contextual Multi-Armed Bandit (C-MAB) reinforcement learning algorithm was implemented to maximize cumulative rewards by balancing exploration versus exploitation of the state-action space. The C-MAB learner outperformed a benchmark Q-learner by more efficiently exploring the state-action space while learning a hard-to-code task. Importantly, this work contributes to the development of reinforcement learning approaches that account for limited resources such as hardware life and researcher time. As robots are used to perform complex, physically interactive tasks in unstructured or unmodeled environments, it becomes important to develop methods that enable efficient and effective learning with physical testbeds.</p>\n<p><strong>Broader Impacts:</strong> Our work contributes to the development of 'haptic intelligence' capabilities for co-robot systems that will enable robust performance when faced with situations in which complementary sensing modalities such as computer vision and proprioception are unavailable or inadequate. Tactile sensors that provide rich, dynamic information can complement computer vision-based approaches and are indispensable when line-of-sight is unavailable. Artificial haptic perception can be used to advance semi-autonomous robots and/or provide actionable information to remote teleoperators of robotic manipulators intended for unstructured and/or limited-access environments (assistive, space, underwater, military, rescue, nuclear decommissioning, telesurgery).</p>\n<p>Our work at Arizona State University and the University of California, Los Angeles strengthened collaborations across countries, universities, disciplines, and between academia and industry. The project produced six journal articles, one edited volume, two conference proceedings articles, two doctoral dissertations, two undergraduate honors theses, and one U.S. patent. Lab tours, exhibits, and school visits benefited hundreds of students from elementary to graduate levels, teachers, and members of the public. We developed low-cost, hands-on instructional robotics modules for elementary school students. Free slides, videos, and activities are provided at <a href=\"https://uclabiomechatronics.wordpress.com/outreach/\">https://uclabiomechatronics.wordpress.com/outreach/</a>. A graduate-level course on the 'Control of Robotic Systems' was developed, which featured research supported by this grant.</p>\n<p>This project provided opportunities to educate the public on scientific approaches and the benefits of publicly funded research. Our work was featured in articles by Science News, PCMag, and Newsmax, among others. Our work was featured in video segments for PBS NewsHour hosted by Miles O'Brien, an educational show called 'STEM Journals' hosted by Geoff Notkin, Voice of America (bilingual), NSF Science Nation (with podcast), an NSF robotics video, a documentary/visual art piece on humanity and technology called 'NEO,' and an episode of History Channel's longest-running nonfiction series 'Ancient Aliens.'</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/19/2018<br>\n\t\t\t\t\tModified by: Veronica&nbsp;J&nbsp;Santos</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545196215425_Figure1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545196215425_Figure1--rgov-800width.jpg\" title=\"'BairClaw' sensorized robot hand testbed\"><img src=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545196215425_Figure1--rgov-66x44.jpg\" alt=\"'BairClaw' sensorized robot hand testbed\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We developed a highly sensorized robot hand testbed called the 'BairClaw.' (A) Tap-and-hold experiment against an instrumented plate. (B) Joint angles, (C) tactile sensor internal fluid pressure and microvibration, (D) tactile sensor skin deformation, and (E) normal contact force data are shown.</div>\n<div class=\"imageCredit\">Hellman, R.B., Chang, E., Tanner, J., Helms Tillery, S.I., and Santos, V.J., 'A robot hand testbed designed for enhancing embodiment and functional neurorehabilitation of body schema in subjects with upper limb impairment or loss.' Front Hum Neurosci 2015:9(26):1-10.</div>\n<div class=\"imageSubmitted\">Veronica&nbsp;J&nbsp;Santos</div>\n<div class=\"imageTitle\">'BairClaw' sensorized robot hand testbed</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545196639313_Figure2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545196639313_Figure2--rgov-800width.jpg\" title=\"Artificial haptic perception of edges\"><img src=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545196639313_Figure2--rgov-66x44.jpg\" alt=\"Artificial haptic perception of edges\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Using a commercially available tactile sensor, we developed a method for the haptic perception of edges. Human-inspired, 'exploratory procedures' (EPs) were performed. (a) A constant fingertip contact angle was used. (b) static contact, (c, d) distal to proximal strokes, (e) radial to ulnar stroke.</div>\n<div class=\"imageCredit\">Ponce Wong, R.D., Hellman, R.B., and Santos, V.J. 'Spatial asymmetry in tactile sensor skin deformation aids perception of edge orientation during haptic exploration,' IEEE Trans on Haptics, Special Issue on 'Haptics in Rehabilitation and Neural Engineering,' 2014:7(2):191-202.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Veronica&nbsp;J&nbsp;Santos</div>\n<div class=\"imageTitle\">Artificial haptic perception of edges</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545197211019_Figure3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545197211019_Figure3--rgov-800width.jpg\" title=\"Artificial haptic perception of fingertip-sized geometric features\"><img src=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545197211019_Figure3--rgov-66x44.jpg\" alt=\"Artificial haptic perception of fingertip-sized geometric features\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Using a commercially available tactile sensor, we developed a method for the haptic perception of fingertip-sized geometric features. Human-inspired, 'exploratory procedures' were performed: (a) static contact, (b) distal-proximal and (c) radial-ulnar linear stroke, (d) roll of the fingertip.</div>\n<div class=\"imageCredit\">Ponce Wong, R.D., Hellman, R.B. and Santos, V.J. 'Haptic exploration of fingertip-sized geometric features using a multimodal tactile sensor,' in Proc SPIE Defense, Security and Sensing / Sensing Tech and Applications 'Sensors for Next-Gen Robotics' Conf, 2014, vol. 9116, pp. 911605-1 to 911605-15.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Veronica&nbsp;J&nbsp;Santos</div>\n<div class=\"imageTitle\">Artificial haptic perception of fingertip-sized geometric features</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545197335181_Figure4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545197335181_Figure4--rgov-800width.jpg\" title=\"Real-time haptic perception and decision-making for a functional contour-following task\"><img src=\"/por/images/Reports/POR/2018/1463960/1463960_10200542_1545197335181_Figure4--rgov-66x44.jpg\" alt=\"Real-time haptic perception and decision-making for a functional contour-following task\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We developed a method for real-time haptic perception and decision-making for a functional contour-following task: closing a ziplock bag. Collaborating with reinforcement learning experts, a Contextual Multi-Armed Bandit policy was tested on (a) empty bag, and (b,c) with cereal.</div>\n<div class=\"imageCredit\">Hellman, R.B., Tekin, C., van der Schaar, M., and Santos, V.J. 'Functional contour-following via haptic perception and reinforcement learning,' IEEE Trans on Haptics, 2018:11(1):61-72.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Veronica&nbsp;J&nbsp;Santos</div>\n<div class=\"imageTitle\">Real-time haptic perception and decision-making for a functional contour-following task</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIntellectual Merit: The long-term research objective was to enhance the robustness of artificial manipulators for human-robot systems that will enable grasp and manipulation in the absence of visual feedback. Research Goals: 1) Establish relationships between multi-digit tactile flow and task-relevant features of an object, 2) Develop algorithms for task-appropriate weighting and integration of multimodal sensor data for haptic inquiry, 3) Develop a framework for context-driven, task-directed haptic inquiry that determines an efficient line of questioning based on prior information.\n\nToward Goal #1: We developed a highly sensorized robot hand testbed called the 'BairClaw.' The multi-articulating, anthropomorphic robot testbed can be used to study proprioceptive and tactile sensory stimuli during finger-object interactions. A remote actuation system enables the modular control of tendon-driven hands. The artificial proprioception system enables direct measurement of joint angles and tendon tensions while temperature, vibration, and skin deformation are provided by a multimodal tactile sensor. Conceived for artificial grasp, manipulation, and haptic exploration, the BairClaw could also be used for future studies on the neurorehabilitation of somatosensory disorders due to upper limb impairment or loss.\n\nToward Goal #2: Haptic perception of salient geometric features such as edges would be useful when vision is obstructed or when proprioceptive feedback is inadequate. We developed an approach for the artificial haptic perception of edge orientation. A robot hand outfitted with a deformable, bladder-type, multimodal tactile sensor was used to replay human-inspired haptic 'exploratory procedures.' Electrode impedance signals provided the most useful inputs by encoding spatially asymmetric skin deformation across the entire fingertip. Interestingly, sensor regions that were not in direct contact with the stimulus provided particularly useful information. We also used exploratory procedures to haptically perceive fingertip-sized geometric features. The geometric features varied by type (bump, pit), curvature (planar, conical, spherical), and footprint dimension (1.25 - 20 mm). Tactile signals generated by fingertip motions were used to extract parameters for use as inputs to supervised learning models. A support vector classifier was developed to perceive local shape while support vector regression models were developed to perceive size.\n\nToward Goal #3: We developed an approach for real-time haptic perception and decision-making for a haptics-driven, functional contour-following task: the closure of a ziplock bag. This task is challenging for robots because the bag is deformable, transparent, and visually occluded by artificial fingertip sensors that are also compliant. A deep neural net classifier was trained to estimate the state of a zipper within a robot's pinch grasp. A Contextual Multi-Armed Bandit (C-MAB) reinforcement learning algorithm was implemented to maximize cumulative rewards by balancing exploration versus exploitation of the state-action space. The C-MAB learner outperformed a benchmark Q-learner by more efficiently exploring the state-action space while learning a hard-to-code task. Importantly, this work contributes to the development of reinforcement learning approaches that account for limited resources such as hardware life and researcher time. As robots are used to perform complex, physically interactive tasks in unstructured or unmodeled environments, it becomes important to develop methods that enable efficient and effective learning with physical testbeds.\n\nBroader Impacts: Our work contributes to the development of 'haptic intelligence' capabilities for co-robot systems that will enable robust performance when faced with situations in which complementary sensing modalities such as computer vision and proprioception are unavailable or inadequate. Tactile sensors that provide rich, dynamic information can complement computer vision-based approaches and are indispensable when line-of-sight is unavailable. Artificial haptic perception can be used to advance semi-autonomous robots and/or provide actionable information to remote teleoperators of robotic manipulators intended for unstructured and/or limited-access environments (assistive, space, underwater, military, rescue, nuclear decommissioning, telesurgery).\n\nOur work at Arizona State University and the University of California, Los Angeles strengthened collaborations across countries, universities, disciplines, and between academia and industry. The project produced six journal articles, one edited volume, two conference proceedings articles, two doctoral dissertations, two undergraduate honors theses, and one U.S. patent. Lab tours, exhibits, and school visits benefited hundreds of students from elementary to graduate levels, teachers, and members of the public. We developed low-cost, hands-on instructional robotics modules for elementary school students. Free slides, videos, and activities are provided at https://uclabiomechatronics.wordpress.com/outreach/. A graduate-level course on the 'Control of Robotic Systems' was developed, which featured research supported by this grant.\n\nThis project provided opportunities to educate the public on scientific approaches and the benefits of publicly funded research. Our work was featured in articles by Science News, PCMag, and Newsmax, among others. Our work was featured in video segments for PBS NewsHour hosted by Miles O'Brien, an educational show called 'STEM Journals' hosted by Geoff Notkin, Voice of America (bilingual), NSF Science Nation (with podcast), an NSF robotics video, a documentary/visual art piece on humanity and technology called 'NEO,' and an episode of History Channel's longest-running nonfiction series 'Ancient Aliens.'\n\n\t\t\t\t\tLast Modified: 12/19/2018\n\n\t\t\t\t\tSubmitted by: Veronica J Santos"
 }
}