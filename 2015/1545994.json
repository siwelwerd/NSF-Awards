{
 "awd_id": "1545994",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Collaborative Research: F: IA: Statistical Learning for Big Data with Random Projections",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Nandini Kannan",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 100577.0,
 "awd_amount": 100577.0,
 "awd_min_amd_letter_date": "2015-09-01",
 "awd_max_amd_letter_date": "2015-09-01",
 "awd_abstract_narration": "Contemporary data-driven science and engineering problems require the development of statistical methods that do not compromise statistical accuracy, yet are computationally feasible.  Data quality, particularly the heterogeneity in data measurements, is a critical factor that affects statistical accuracy in the analysis of large datasets.  This project will explore and demonstrate the impact and feasibility of improving computational and statistical performances simultaneously for Big Data problems with massive datasets.  The research will advance the state of knowledge in predictive statistical learning with Big Data, and be extremely valuable in applications related to financial risk management or commercial operations employing recommender systems, biology, and image analysis. \r\n\r\n A key phenomenon motivating this project is the notion that some refined ensemble methods combined with random projections can simultaneously enable the fast analysis of massive data while enhancing statistical performance.  Specifically, the aims of the project are: (1) Develop new classification methods based on random projections and the random forest.   By defining appropriate projections, the proposed method is shown to improve statistical accuracy for massive datasets with a large number of irrelevant noisy measurements.   The theoretical properties of this method will be analyzed, and an adaptive version of the algorithm developed to optimize the computational and statistical efficiency gains; (2) Propose boosting algorithms with random projections.  The statistical properties, practical performance, and implementation of the proposed random projected boosting algorithms will be investigated; (3) Develop classification methods with heterogeneities.  A classification method that involves the weighted bootstrap and ensemble learning to handle heterogeneity or covariate shifts in measurements in large datasets will be developed.  The random projection method will be applied to improve the proposed method for high-dimensional datasets.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Wen",
   "pi_last_name": "Zhou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wen Zhou",
   "pi_email_addr": "w.zhou@nyu.edu",
   "nsf_id": "000679403",
   "pi_start_date": "2015-09-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Colorado State University",
  "inst_street_address": "601 S HOWES ST",
  "inst_street_address_2": "",
  "inst_city_name": "FORT COLLINS",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "9704916355",
  "inst_zip_code": "805212807",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "COLORADO STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "LT9CXX8L19G1"
 },
 "perf_inst": {
  "perf_inst_name": "Colorado State University",
  "perf_str_addr": "200 W. Lake St",
  "perf_city_name": "Fort Collins",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "805214593",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 100577.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Contemporary data-driven science and engineering problems require the development of statistical methods that do not compromise statistical accuracy, yet are computationally feasible. Dimensionality, heteroskedasticity, tailedness are all critical factors that affects statistical accuracy in the analysis of large datasets. This project explored and demonstrated the impact and feasibility of improving computational and statistical performances simultaneously for Big Data problems with massive datasets. The research will advance the state of knowledge in predictive statistical learning and high dimensional inference with Big Data, and be extremely valuable in applications related to financial risk management or commercial operations employing recommender systems, biology, and image analysis.&nbsp;</p>\n<p>This fruitful project provide advancement in four aspects: developing powerful and computationally efficient high dimensional hypothesis tests (bootstrap-based), proposing novel clustering methods for data with growing dimension (projection-motivated), robust estimation and inference procedure for data with heavy tailedness; and multiple testing procedure for data of sophisticated design. For example, in one paper supported by the project,&nbsp;we study the problem of testing the mean vectors of high dimensional data in both one-sample and two-sample cases and propose testing procedures using L_infty statistics. We adopt the wild bootstrap techniques to derive the critical values. Different from existing tests that heavily rely on the structural conditions on unknown covariance matrices, the proposed tests allow general covariance structures and therefore enjoy wide scope of applicability in practice. To enhance powers of the tests against sparse alternatives, we further propose two-step procedures with a preliminary feature screening step. In another paper from the project, we propose a computationally fast procedure for testing the equality of two large covariance matrices when the data dimension is much larger than the sample size. Again,a distinguishing feature of the new procedure is that it imposes no structural assumptions on the unknown covariance matrices. Hence, the test is robust with respect to various complex dependence structures. As an interesting application, we develope a new gene clustering algorithm which shares the same nice property of avoiding restrictive structural assumptions for high-dimensional data. In a recent paper out of the project, we propose a new omnibus test for multivariate whitenoise using the maximum absolute auto-correlations and cross-correlations of the component series. Based on an approximation by the L_infty norm of a normal random vector, the critical value of the test can be evaluated by bootstrapping from a multivariate Gaussian. In contrast to the conventional tests,the new method is proved to be valid for testing departure from white noise that is not independent&nbsp;and identically distributed. The new test outperforms several commonly used methods, especially when the dimension of the time series is high in relation to the sample size. We also show that the new test can be further enhanced when it is applied to pre-transformed data obtained via the time series principal component analysis. The proposed test is implemented in the R-package HDtest. The outcomes of this project provide a series of computationally efficient statistical methods, either for inference or estimation, of data with high dimensionality and complex structures without compromising the statistical guarantees. The methods not only provide theoretical sophistication but also prove to be useful in practice. Most of them have been implemented in R-packages for easy accessibility.&nbsp;</p>\n<p>This project has supported five graduate students during its entire life, including a Ph.D. thesis successfully defended in Fall 2016. Students were supported by the project to participate conference like JSM, ICSA, ENAR, IISA, CoDA etc to present their research and communicate with the community. The summer support from the project on graduate students substantially help them on the progress toward thesis. A few good statistical publications, as well as interdisplinary papers, have been produced from the project, including:</p>\n<p>J. Chang, C. Zheng, W.-X. Zhou, and W. Zhou (2017) . Simulation-based hypothesis testing ofhigh dimensional means under covariance heterogeneity. Biometrics. 73(4), 1300-1310.</p>\n<p>J. Chang, Q. Yao, andW. Zhou (2017). Testing for high-dimensional white noise using maximumcross correlations. Biometrika. 104(1), 111-127.</p>\n<p>J. Chang, W. Zhou, W.-X. Zhou, and L. Wang (2017). Comparing large covariance matricesunder weak conditions on the dependence structure and its application to gene clustering. Biometrics. 73(1), 31-41.9.</p>\n<p>In addiiton, two R-packages, HDtest and PARSE, were developed out of the project. From the conferences participated by graduate students and PI, new collaborations on both statistics and biology have been established. A large number of seminar and invited talks were given during the life of the award.</p>\n<p>Overall, the project is fruitful and successful with interesting and promising statisitcal methods developed and delievered to both statistical community and disciplinary fields such as bioinformatics or genomics. Graduate students have been benefited from the project as well.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/07/2018<br>\n\t\t\t\t\tModified by: Wen&nbsp;Zhou</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nContemporary data-driven science and engineering problems require the development of statistical methods that do not compromise statistical accuracy, yet are computationally feasible. Dimensionality, heteroskedasticity, tailedness are all critical factors that affects statistical accuracy in the analysis of large datasets. This project explored and demonstrated the impact and feasibility of improving computational and statistical performances simultaneously for Big Data problems with massive datasets. The research will advance the state of knowledge in predictive statistical learning and high dimensional inference with Big Data, and be extremely valuable in applications related to financial risk management or commercial operations employing recommender systems, biology, and image analysis. \n\nThis fruitful project provide advancement in four aspects: developing powerful and computationally efficient high dimensional hypothesis tests (bootstrap-based), proposing novel clustering methods for data with growing dimension (projection-motivated), robust estimation and inference procedure for data with heavy tailedness; and multiple testing procedure for data of sophisticated design. For example, in one paper supported by the project, we study the problem of testing the mean vectors of high dimensional data in both one-sample and two-sample cases and propose testing procedures using L_infty statistics. We adopt the wild bootstrap techniques to derive the critical values. Different from existing tests that heavily rely on the structural conditions on unknown covariance matrices, the proposed tests allow general covariance structures and therefore enjoy wide scope of applicability in practice. To enhance powers of the tests against sparse alternatives, we further propose two-step procedures with a preliminary feature screening step. In another paper from the project, we propose a computationally fast procedure for testing the equality of two large covariance matrices when the data dimension is much larger than the sample size. Again,a distinguishing feature of the new procedure is that it imposes no structural assumptions on the unknown covariance matrices. Hence, the test is robust with respect to various complex dependence structures. As an interesting application, we develope a new gene clustering algorithm which shares the same nice property of avoiding restrictive structural assumptions for high-dimensional data. In a recent paper out of the project, we propose a new omnibus test for multivariate whitenoise using the maximum absolute auto-correlations and cross-correlations of the component series. Based on an approximation by the L_infty norm of a normal random vector, the critical value of the test can be evaluated by bootstrapping from a multivariate Gaussian. In contrast to the conventional tests,the new method is proved to be valid for testing departure from white noise that is not independent and identically distributed. The new test outperforms several commonly used methods, especially when the dimension of the time series is high in relation to the sample size. We also show that the new test can be further enhanced when it is applied to pre-transformed data obtained via the time series principal component analysis. The proposed test is implemented in the R-package HDtest. The outcomes of this project provide a series of computationally efficient statistical methods, either for inference or estimation, of data with high dimensionality and complex structures without compromising the statistical guarantees. The methods not only provide theoretical sophistication but also prove to be useful in practice. Most of them have been implemented in R-packages for easy accessibility. \n\nThis project has supported five graduate students during its entire life, including a Ph.D. thesis successfully defended in Fall 2016. Students were supported by the project to participate conference like JSM, ICSA, ENAR, IISA, CoDA etc to present their research and communicate with the community. The summer support from the project on graduate students substantially help them on the progress toward thesis. A few good statistical publications, as well as interdisplinary papers, have been produced from the project, including:\n\nJ. Chang, C. Zheng, W.-X. Zhou, and W. Zhou (2017) . Simulation-based hypothesis testing ofhigh dimensional means under covariance heterogeneity. Biometrics. 73(4), 1300-1310.\n\nJ. Chang, Q. Yao, andW. Zhou (2017). Testing for high-dimensional white noise using maximumcross correlations. Biometrika. 104(1), 111-127.\n\nJ. Chang, W. Zhou, W.-X. Zhou, and L. Wang (2017). Comparing large covariance matricesunder weak conditions on the dependence structure and its application to gene clustering. Biometrics. 73(1), 31-41.9.\n\nIn addiiton, two R-packages, HDtest and PARSE, were developed out of the project. From the conferences participated by graduate students and PI, new collaborations on both statistics and biology have been established. A large number of seminar and invited talks were given during the life of the award.\n\nOverall, the project is fruitful and successful with interesting and promising statisitcal methods developed and delievered to both statistical community and disciplinary fields such as bioinformatics or genomics. Graduate students have been benefited from the project as well. \n\n \n\n\t\t\t\t\tLast Modified: 11/07/2018\n\n\t\t\t\t\tSubmitted by: Wen Zhou"
 }
}