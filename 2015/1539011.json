{
 "awd_id": "1539011",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "VEC: Medium: Large-Scale Visual Recognition: From Cloud Data Centers to Wearable Devices",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 960000.0,
 "awd_amount": 960000.0,
 "awd_min_amd_letter_date": "2015-09-15",
 "awd_max_amd_letter_date": "2020-09-14",
 "awd_abstract_narration": "Advances in computer hardware and software promise to revolutionize the ways in which society interacts with visual information. However, visual recognition systems are limited by the lack of a practical means to classify the millions of concepts that arise in visual scenes and thus efficiently recognize when a small number of these concepts appear in a given scene. Furthermore, while real-time processing of visual data could significantly expand our perception of our surroundings, state-of-the-art vision systems cannot currently be implemented on wearable devices such as smartphones due to the limited heat dissipation (e.g., no fans or liquid cooling) and power such devices can provide. This research will overcome these challenges by developing artificial intelligence (AI) systems that efficiently manage the resources most crucial for high-performance wearable-based visual recognition, including the wearable device's real-time power consumption and computation. These systems will be empowered to initiate bursts of intense computation that are thermally managed by materials within the wearable device which are engineered to melt during heavy heating and solidify between bursts. Moreover, the AI systems will govern the communication between the device and external (cloud-based) computation resources as well as large-scale visual concept databases housed in data centers, thus providing extreme performance in a wearable form factor. Central concepts of this work will be integrated in undergraduate and graduate coursework, and a demonstration system will be made available to the research community and used in educational modules for high school students.\r\n\r\nThis effort seeks to advance the core capabilities of large-scale visual recognition by co-designing visual models and computing infrastructure. The goal is to enable encyclopedic, real-time visual recognition through seamless integration of visual computing on wearable devices and in the cloud. The PIs envision a wearable visual recognition system that continuously captures live video input while providing intelligent, real-time assistance through automatic or on-demand visual recognition by means of a combination of computation at the device and offloading to the cloud. Such a system is not currently feasible due to a number of fundamental challenges. First, the severe energy and thermal constraints of wearable devices render them incapable of performing the intensive computation necessary for visual recognition. Second,  it remains an open question how to support encyclopedic recognition in terms of both visual models and data center infrastructure. In particular, it remains unclear how current visual models, although highly successful at recognizing 1,000 object categories, can scale to millions or more distinct visual concepts. Moreover, such an encyclopedic visual model must be supported through data center infrastructure, but little progress has been made on how to build such infrastructure. This project addresses these fundamental challenges through an interdisciplinary approach integrating computer vision, hardware architecture, VLSI design, and heat transfer. The PIs will investigate three research thrusts. In Thrust 1, the PIs will develop a new type of deep neural networks that allow resource-efficient execution of modules. This new framework provide a unified way to design, learn, and run scalable visual models that can maximize the utility of recognition subject to resource constraints, such as latency, energy, or thermal dissipation of a wearable device. In Thrust 2, the PIs will design and fabricate a visual processing chip capable of computational sprinting (bursts of extreme computation well above steady-state thermal dissipation capabilities), leveraging the new framework developed in Thrust 1. In Thrust 3, the PIs will design datacenter infrastructure that supports large-scale hierarchical indexing of visual concepts for encyclopedic recognition, with a focus on latency, throughput, and energy efficiency. Finally, the PIs will build a demonstration system to evaluate the proposed algorithms, software, and hardware components and to assess the overall performance of an end-to-end system. The project web site (http://mivec.eecs.umich.edu/) will provide access to the results of this research including technical reports, datasets, and source code.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Wenisch",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas Wenisch",
   "pi_email_addr": "twenisch@umich.edu",
   "nsf_id": "000110966",
   "pi_start_date": "2019-02-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Jia",
   "pi_last_name": "Deng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jia Deng",
   "pi_email_addr": "jiadeng@princeton.edu",
   "nsf_id": "000662553",
   "pi_start_date": "2015-09-15",
   "pi_end_date": "2019-02-26"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Pipe",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Kevin P Pipe",
   "pi_email_addr": "pipe@umich.edu",
   "nsf_id": "000319442",
   "pi_start_date": "2015-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Wenisch",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas Wenisch",
   "pi_email_addr": "twenisch@umich.edu",
   "nsf_id": "000110966",
   "pi_start_date": "2015-09-15",
   "pi_end_date": "2019-02-26"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Lingjia",
   "pi_last_name": "Tang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lingjia Tang",
   "pi_email_addr": "lingjia@umich.edu",
   "nsf_id": "000628990",
   "pi_start_date": "2015-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Mars",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jason Mars",
   "pi_email_addr": "profmars@umich.edu",
   "nsf_id": "000629579",
   "pi_start_date": "2015-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "2260 Hayward, 3640 Beyster Bldg.",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "002Z",
   "pgm_ref_txt": "Intel/NSF VEC Partnership"
  },
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 320000.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 390000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With the proliferation of high-speed internet connectivity and the widespread availability of cheap, high-resolution cameras, large-scale visual recognition is becoming ever more important to enable advanced applications like self-driving vehicles. This project advanced the core capabilities of large-scale visual recognition by co-designing and co-managing visual models and computing infrastructure. The long-term goal is to enable encyclopedic, real-time visual recognition through seamless integration of visual computing on wearable devices and the cloud.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>\n<p>This effort comprised three main research thrusts. Thrust 1 developed Dynamic Deep Neural Networks (D2NN), a new deep learning framework that introduces triage modules---special DNN modules that dynamically control the execution of other DNN units and optimize recognition performance under the resource constraints of data centers and wearable devices. These triage modules enable a trade-off between energy efficiency and how much of the DNN module is activated, allowing less energy to be expent for inference instances that are easier to recognize.&nbsp; Thrust 2 leveraged computational sprinting, a computing paradigm to design custom hardware to provide D2NN the triage option to &ldquo;sprint'' on a wearable device, i.e., performing a burst of computation exceeding the sustainable power limit. Thrust 3 designed data center infrastructure to support D2NN-based encyclopedic recognition (such as recognizing specific car models) with a focus on latency and energy efficiency.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/26/2022<br>\n\t\t\t\t\tModified by: Thomas&nbsp;Wenisch</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWith the proliferation of high-speed internet connectivity and the widespread availability of cheap, high-resolution cameras, large-scale visual recognition is becoming ever more important to enable advanced applications like self-driving vehicles. This project advanced the core capabilities of large-scale visual recognition by co-designing and co-managing visual models and computing infrastructure. The long-term goal is to enable encyclopedic, real-time visual recognition through seamless integration of visual computing on wearable devices and the cloud.                                            \n\nThis effort comprised three main research thrusts. Thrust 1 developed Dynamic Deep Neural Networks (D2NN), a new deep learning framework that introduces triage modules---special DNN modules that dynamically control the execution of other DNN units and optimize recognition performance under the resource constraints of data centers and wearable devices. These triage modules enable a trade-off between energy efficiency and how much of the DNN module is activated, allowing less energy to be expent for inference instances that are easier to recognize.  Thrust 2 leveraged computational sprinting, a computing paradigm to design custom hardware to provide D2NN the triage option to \"sprint'' on a wearable device, i.e., performing a burst of computation exceeding the sustainable power limit. Thrust 3 designed data center infrastructure to support D2NN-based encyclopedic recognition (such as recognizing specific car models) with a focus on latency and energy efficiency. \n\n \n\n\t\t\t\t\tLast Modified: 06/26/2022\n\n\t\t\t\t\tSubmitted by: Thomas Wenisch"
 }
}