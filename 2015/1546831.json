{
 "awd_id": "1546831",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "I-Corps:  Turning a Mobile Device into a Mouse in the Air",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "lydia mcclure",
 "awd_eff_date": "2015-07-15",
 "awd_exp_date": "2016-12-31",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2015-07-03",
 "awd_max_amd_letter_date": "2015-07-03",
 "awd_abstract_narration": "A mouse has been one of the most successful technologies for controlling the graphic user interface due to its ease of use. Its attraction penetrates well beyond just computers. There already have been mice designed for game consoles and smart TVs. A smart TV allows a user to run popular computer programs and smartphone applications. For example, a smart TV user may want to use a Web browser and click on a certain URL or some part of a map using a mouse. A traditional remote controller, which uses buttons for user input, is no longer sufficient to exploit full functionalities offered by the smart TV. More and more devices in the future, such as Google Glasses, baby monitors, and a new generation of home appliances, all desire mouse functionalities, which allow users to choose from a wide variety of options and easily click on different parts of the view. On the other hand, a traditional mouse, which requires a flat, smooth surface to operate, cannot satisfy many new usage scenarios. This I-Corps team has developed an accurate and easy way of tracking hand movement, and believes this technology has a wide range of potential applications in smart glasses and the Internet of Things.\r\n\r\nThis team has developed an Air Mouse that accurately tracks device movement in real time. It enables any mobile device with a microphone, such as a smart phone and smart watch, to serve as a mouse to control an electronic device with speakers. A unique feature of the proposed approach is that it achieves high tracking accuracy using existing hardware in mobile and electronic devices. Applications of the proposed technology include controlling smart TVs, smart glasses, and home appliances. The proposed approach sends inaudible sound pulses at a few selected frequencies, and uses the frequency shifts to estimate the speed and distance traveled. Techniques have also been developed to quickly calibrate the distance between speakers and narrow down the device's initial position using its movement trajectory. Based on the information, the device's new position in real time can be continuously tracked.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lili",
   "pi_last_name": "Qiu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lili Qiu",
   "pi_email_addr": "lili@cs.utexas.edu",
   "nsf_id": "000112738",
   "pi_start_date": "2015-07-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121532",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802300",
   "pgm_ele_name": "I-Corps"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Video games, Virtual Reality (VR), Augmented Reality (AR), and Smartappliances (e.g., smart TVs) all call for a new way for users tointeract and control them. Motivated by these needs, this NSF project develops a novel motion tracking technology that accurately tracks themovement of a mobile device in real time so that a user can control a nearby device by simply moving his hand while holding the mobiledevice. &nbsp;Our motion tracking is based on acoustic signals. Audio signals are attractive for tracking due to its slow propagation speed and easy to generate by the speakers and microphones on commodity devices. Achieving high tracking accuracy is essential to provide enjoyable user experience. Our approach can achieve high tracking accuracy using an existing mobile device without special hardware. We have developed several interesting demos to showcase the utility ofour motion tracking, such as drawing in the air and playing motion-based games. We demoed at SXSW'16 and MobiCom'16 as well as to several companies, and got great responses.</p>\n<p>In addition to developing novel technology and demos, we have conducted customer discovery by interviewing 100+ game players and developers, and identified several important applications for our technology. &nbsp;Moreover, several graduate students are trained in research, development, presentation during the course of the project.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/31/2017<br>\n\t\t\t\t\tModified by: Lili&nbsp;Qiu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nVideo games, Virtual Reality (VR), Augmented Reality (AR), and Smartappliances (e.g., smart TVs) all call for a new way for users tointeract and control them. Motivated by these needs, this NSF project develops a novel motion tracking technology that accurately tracks themovement of a mobile device in real time so that a user can control a nearby device by simply moving his hand while holding the mobiledevice.  Our motion tracking is based on acoustic signals. Audio signals are attractive for tracking due to its slow propagation speed and easy to generate by the speakers and microphones on commodity devices. Achieving high tracking accuracy is essential to provide enjoyable user experience. Our approach can achieve high tracking accuracy using an existing mobile device without special hardware. We have developed several interesting demos to showcase the utility ofour motion tracking, such as drawing in the air and playing motion-based games. We demoed at SXSW'16 and MobiCom'16 as well as to several companies, and got great responses.\n\nIn addition to developing novel technology and demos, we have conducted customer discovery by interviewing 100+ game players and developers, and identified several important applications for our technology.  Moreover, several graduate students are trained in research, development, presentation during the course of the project.\n\n \n\n\t\t\t\t\tLast Modified: 03/31/2017\n\n\t\t\t\t\tSubmitted by: Lili Qiu"
 }
}