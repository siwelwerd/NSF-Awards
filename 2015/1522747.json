{
 "awd_id": "1522747",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Algorithms for Nonlinear Nonconvex Optimization under Uncertainty",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Leland Jameson",
 "awd_eff_date": "2015-09-15",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 210000.0,
 "awd_amount": 210000.0,
 "awd_min_amd_letter_date": "2015-09-15",
 "awd_max_amd_letter_date": "2015-09-15",
 "awd_abstract_narration": "The investigator will develop computational methods that seek to find optimal decisions when not all information is known exactly.  This situation arises when future events cannot be predicted with high accuracy or when quantities can only be estimated from limited data.  A similar setting occurs when computer programs are employed to simulate processes. In that case, the computer output can be noisy, due to limited precision of the underlying numerical algorithms or due to randomness inherent in the simulation. An illustrative example is the deployment of solar or wind energy: Electricity demand estimates based on past observations are uncertain, and weather forecast models are started from random perturbations of the initial conditions. Computational optimization algorithms exist that address data uncertainty, but current methods are not able to handle difficult nonlinear relationships in the mathematical optimization model. This research will overcome this limitation on two fronts. The first research project will result in optimization algorithms for problems in which constraints need to be satisfied only with a given probability. These methods will permit a much wider range of these constraints than the present state-of-the-art. The second project will produce methods that optimize computer simulations by explicitly addressing the nature of the output noise. In contrast to existing approaches, these algorithms will not stagnate at spurious solutions induced by the noise.  All new methods will be implemented in software and evaluated on real-life problems, and new mathematical theory will be developed that proves the convergence of these methods.\r\n\r\nIn this project, new algorithms for continuous chance-constrained optimization will be developed. In current approaches, the objective and constraint functions are required to be linear or convex, and the nonconvexity induced by the chance-constraints is handled either by conservative convex approximations or by the global solution of discrete formulations via combinatorial branch-and-bound enumeration.  The new methods will permit problem statements that involve nonlinear and nonconvex objective functions and include joint chance constraints with nonconvex probabilistic constraints. This is made possible by seeking only local optima, which can be found more easily than global minima but are still highly valuable in practice. As a result, established techniques from nonconvex nonlinear optimization can be built upon and extended. The new sequential quadratic chance-constrained programming framework requires the introduction of new chance-constrained trust-region subproblem solvers and convergence theory for chance-constrained penalty functions which will be developed in this project. The PI will also develop a derivative-free optimization method for objective functions with deterministic noise caused by numerical error in computer simulations. The approach is based on a smoothed objective function obtained via convolution with a Gaussian kernel.  The integral in the new objective is approximated by Monte-Carlo sample average approximation. Adaptive multiple importance sampling permits the reuse of the expensive function evaluations computed in all previous iterations.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andreas",
   "pi_last_name": "Waechter",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andreas Waechter",
   "pi_email_addr": "waechter@iems.northwestern.edu",
   "nsf_id": "000609240",
   "pi_start_date": "2015-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "2145 Sheridan Road",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602083100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "006Y00",
   "pgm_ele_name": "OE Operations Engineering"
  },
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "072E",
   "pgm_ref_txt": "NETWORKS & QUEUING SYSTEMS"
  },
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "077E",
   "pgm_ref_txt": "SIMULATION MODELS"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 210000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Under this grant we developed efficient and practical numerical algorithms for solving optimization problems that take data uncertainty into account.&nbsp; The distinguishing aspect of this research was that established methods from deterministic nonlinear optimization could be transferred into the field of stochastic programming.&nbsp; This made it possible to solve some classes of nonlinear optimization problems with uncertain data for the first time.</p>\n<p><br />Intellectual merit:</p>\n<p><br />It is well known that optimization under uncertainty constitutes a challenging mathematical task. Extending the scope of stochastic optimization to more general nonconvex problems adds significant generality and value, but leads to challenging open questions. The intellectual merits of the performed research lie both in the difficulty of integrating computational advances from diverse areas of optimization in order to handle nonconvexities effectively, as well as in the efficient use of computer simulations.&nbsp; This integration of ideas from stochastic optimization, nonlinear programming, and simulation formed the intellectual crux of the proposed project.<br />More specifically, this grant funded two separate research thrusts.<br />The first thrust considered the minimization of objective functions that can be evaluated only with significant noise.&nbsp; This can occur when function values are computed by complicated computer programs and small numerical errors or approximations accumulate.&nbsp; In this project we developed a new smoothed formulation that makes such an optimization well defined.&nbsp; The specific intellectual merit in this project lies in the invention of a new optimization algorithm that is able to make use of all function evaluation encountered during the optimization until the current iteration.&nbsp; This was accomplished by utilizing the technique of adaptive multiple importance sampling for the first time in an optimization algorithm.&nbsp; We prove that the algorithm is convergent and reported promising initial numerical results.<br />The second research thrust evolved into two research projects.&nbsp; Both considered settings in which constraints have to be satisfied only with a given probability; such constraints are called chance constraints.&nbsp; One of the key goals we set was that the algorithms must be able to handle joint chance constraints in which many constraints much be satisfied with high probability as the same time.&nbsp; This is considerably more difficult than solving problems with single chance constraints.&nbsp; The first project produced the first numerical algorithm with theoretical convergence guarantees that was able solve instances with nonlinear and nonconvex joint chance constraints; previous state-of-the-art methods only permitted linear or convex constraints.&nbsp; This method was able to solve these problems efficiently, but we decided to developed a different solution methodology in the second project to improve the computational speed.&nbsp;<br />The main innovation of the second project of the second thrust is a new smoothing technique that approximates the original chance constraint, which can be computed to high accuracy only in rare cases.&nbsp; The key feature of the approximation is that it produces constraint functions that are differentiable.&nbsp; As a consequence, established and robust algorithms and techniques from gradient-based nonlinear optimization could be exploited in this new setting and resulted in numerical methods that significantly outperformed previous state-of-the-art methods for continuous chance-constrained optimization.</p>\n<p><br />Broader Impact:</p>\n<p><br />Nonconvex stochastic optimization problems with uncertain data arise naturally in numerous scientific, economic, and industrial applications. These range from the design of a chemical plant under changing environmental conditions to the management of hydro-power reservoirs given uncertain electricity prices. This project has impact through the resulting general-purpose algorithms.&nbsp; Specifically the project concerning the smoothing technique of chance constraints is already being used in research of electrical power systems, such as the optimal operation of electrical power grids that need to remain operational under uncertain demand and withstand the intermittent electricity production of solar and wind farms.&nbsp; One female Ph.D. student was trained under this grant.&nbsp; The research conducted resulted in two articles in the leading journal on numerical optimization and one conference paper. Two other journal manuscripts are currently under review.&nbsp; The findings of these projects were reported at twelve international conferences and workshops.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/04/2019<br>\n\t\t\t\t\tModified by: Andreas&nbsp;Waechter</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nUnder this grant we developed efficient and practical numerical algorithms for solving optimization problems that take data uncertainty into account.  The distinguishing aspect of this research was that established methods from deterministic nonlinear optimization could be transferred into the field of stochastic programming.  This made it possible to solve some classes of nonlinear optimization problems with uncertain data for the first time.\n\n\nIntellectual merit:\n\n\nIt is well known that optimization under uncertainty constitutes a challenging mathematical task. Extending the scope of stochastic optimization to more general nonconvex problems adds significant generality and value, but leads to challenging open questions. The intellectual merits of the performed research lie both in the difficulty of integrating computational advances from diverse areas of optimization in order to handle nonconvexities effectively, as well as in the efficient use of computer simulations.  This integration of ideas from stochastic optimization, nonlinear programming, and simulation formed the intellectual crux of the proposed project.\nMore specifically, this grant funded two separate research thrusts.\nThe first thrust considered the minimization of objective functions that can be evaluated only with significant noise.  This can occur when function values are computed by complicated computer programs and small numerical errors or approximations accumulate.  In this project we developed a new smoothed formulation that makes such an optimization well defined.  The specific intellectual merit in this project lies in the invention of a new optimization algorithm that is able to make use of all function evaluation encountered during the optimization until the current iteration.  This was accomplished by utilizing the technique of adaptive multiple importance sampling for the first time in an optimization algorithm.  We prove that the algorithm is convergent and reported promising initial numerical results.\nThe second research thrust evolved into two research projects.  Both considered settings in which constraints have to be satisfied only with a given probability; such constraints are called chance constraints.  One of the key goals we set was that the algorithms must be able to handle joint chance constraints in which many constraints much be satisfied with high probability as the same time.  This is considerably more difficult than solving problems with single chance constraints.  The first project produced the first numerical algorithm with theoretical convergence guarantees that was able solve instances with nonlinear and nonconvex joint chance constraints; previous state-of-the-art methods only permitted linear or convex constraints.  This method was able to solve these problems efficiently, but we decided to developed a different solution methodology in the second project to improve the computational speed. \nThe main innovation of the second project of the second thrust is a new smoothing technique that approximates the original chance constraint, which can be computed to high accuracy only in rare cases.  The key feature of the approximation is that it produces constraint functions that are differentiable.  As a consequence, established and robust algorithms and techniques from gradient-based nonlinear optimization could be exploited in this new setting and resulted in numerical methods that significantly outperformed previous state-of-the-art methods for continuous chance-constrained optimization.\n\n\nBroader Impact:\n\n\nNonconvex stochastic optimization problems with uncertain data arise naturally in numerous scientific, economic, and industrial applications. These range from the design of a chemical plant under changing environmental conditions to the management of hydro-power reservoirs given uncertain electricity prices. This project has impact through the resulting general-purpose algorithms.  Specifically the project concerning the smoothing technique of chance constraints is already being used in research of electrical power systems, such as the optimal operation of electrical power grids that need to remain operational under uncertain demand and withstand the intermittent electricity production of solar and wind farms.  One female Ph.D. student was trained under this grant.  The research conducted resulted in two articles in the leading journal on numerical optimization and one conference paper. Two other journal manuscripts are currently under review.  The findings of these projects were reported at twelve international conferences and workshops.\n\n \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/04/2019\n\n\t\t\t\t\tSubmitted by: Andreas Waechter"
 }
}