{
 "awd_id": "1520942",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Particle Tracking at High Luminosity on Heterogeneous, Parallel Processor Architectures",
 "cfda_num": "47.049",
 "org_code": "03010000",
 "po_phone": "7032928235",
 "po_email": "bmihaila@nsf.gov",
 "po_sign_block_name": "Bogdan Mihaila",
 "awd_eff_date": "2015-08-01",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 424228.0,
 "awd_amount": 424228.0,
 "awd_min_amd_letter_date": "2015-07-31",
 "awd_max_amd_letter_date": "2017-07-12",
 "awd_abstract_narration": "Particle physics experiments at the Large Hadron Collider (LHC) at CERN seek to explore fundamental questions in modern physics, such as how particles attain mass, why gravity is weak, and the nature of dark matter. The large quantity of data produced at experimental facilities such as the LHC requires the development of complex pattern recognition algorithms and software techniques to achieve the scientific goals of these physics programs. This project will investigate new algorithms and techniques for data analysis using emerging computing processor architectures. These activities will enable the LHC experiments to take data more efficiently and improve the quality of the data that is recorded in order to extend the reach of the next generation of discoveries from planned hardware upgrades at the LHC over the next decade. The results of this research will significantly reduce the cost of computing for all LHC experiments. Software source code tools will be made available to the particle physics community. The investigators will host workshops to train post-doctoral fellows and graduate students from all areas of particle physics on how to use these advanced techniques.  This training is valuable preparation for dealing with big data science in general.\r\n\r\nThis project will support research focused on novel compute architectures for parallelized and vectorized charged particle track reconstruction. This research will improve the reach of energy-frontier particle physics experiments, such as the ATLAS, CMS, LHCb and ALICE experiments at the LHC and any other fields where studying the passage of charged particles is of critical importance.  The science targeted in this project includes studying the properties of the Higgs boson, probing dark matter by searching for supersymmetry, and exploring the unknown by looking for such proposed effects as large extra space-time dimensions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "PHY",
 "org_div_long_name": "Division Of Physics",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Avi",
   "pi_last_name": "Yagil",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Avi Yagil",
   "pi_email_addr": "ayagil@ucsd.edu",
   "nsf_id": "000585499",
   "pi_start_date": "2015-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Wuerthwein",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Frank Wuerthwein",
   "pi_email_addr": "fkw@ucsd.edu",
   "nsf_id": "000144338",
   "pi_start_date": "2015-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Dr.",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930319",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "122100",
   "pgm_ele_name": "HEP-High Energy Physics"
  },
  {
   "pgm_ele_code": "724400",
   "pgm_ele_name": "COMPUTATIONAL PHYSICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7569",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE/SCIENCE"
  },
  {
   "pgm_ref_code": "8084",
   "pgm_ref_txt": "CDS&E"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 139148.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 138031.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 147049.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Charged-particle reconstruction (both finding and fitting) is one of the most important features of a collider detector experiment, like the Compact Muon Solenoid (CMS) at the Large Hadron Collider (LHC), CERN, Switzerland. Efficient and accurate reconstruction of charged-particles is critically important for the physics performance of such experiments. It is also the most complex, time-consuming, and costly part of a collision event reconstruction. This task gets more complex (and expensive) with increased occupancy in the detector.</p>\n<p>The next major upgrade of the LHC accelerator aims at unprecedented brightness of beams that will result in much more physics data collected but also ?dirtier? conditions; each collision of interest will be accompanied by ~200 parasitic collisions. This will pose major challenges to all detector components, but especially to charge-particle detection and reconstruction.</p>\n<p>The famous \"Moore's law\" states that chip transistor counts increases exponentially over time for the same cost. Over four decades, these decreases in transistor cost turned into exponential gains in the performance of software applications like those used in particle physics. Around 2005, the computing processor market reached an epochal turning point: power density limitations in chips ended this trend, and our applications no longer immediately run exponentially faster on subsequent generations of processors. This is true even though the underlying transistor count continues to increase per Moore?s law. What changed is the types of processors. Due to this change, the performance improvements are now more on the order of 20-25% per year, and many of the improvements in FLOP counts come from addition of specialized hardware such as vector units and many cores. Over the next decade, this corresponds to a factor of six growth instead of a factor of &sim; 30 as one would expect from Moore?s Law, and not enough to keep up with the exponential growth in the reconstruction time due to the harsher conditions expected during the High-Luminosity (HL) run of the LHC. A change is required to move from the sequential applications of today to vectorized, parallelized applications of tomorrow.</p>\n<p>This was exactly the challenge and goal of this PIF project. We aimed to find a way to deploy the traditional sequential and serial charged-particle reconstruction on modern architectures by parallelizing and vectorizing them to fit to the modern computer architectures. It required redesign of data structures, and finding ways to avoid bottlenecks due to specific local challenges invariably present in different parts of the reconstructed event.</p>\n<p>In results shown in multiple international conferences (CHEP, ACAT, CTD) as well as in internal CMS collaboration meetings we have demonstrated charged-particle reconstruction that is very similar (but not quite as good yet) physics performance (efficiency and fake rates) to the traditional one but approximately 8 times faster.&nbsp; The project will lead to a deployment of new software code to address the challenges of the high luminosity LHC running in the near future. Work will be followed-up under the auspices of the IRIS-HEP project.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/19/2019<br>\n\t\t\t\t\tModified by: Avi&nbsp;Yagil</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nCharged-particle reconstruction (both finding and fitting) is one of the most important features of a collider detector experiment, like the Compact Muon Solenoid (CMS) at the Large Hadron Collider (LHC), CERN, Switzerland. Efficient and accurate reconstruction of charged-particles is critically important for the physics performance of such experiments. It is also the most complex, time-consuming, and costly part of a collision event reconstruction. This task gets more complex (and expensive) with increased occupancy in the detector.\n\nThe next major upgrade of the LHC accelerator aims at unprecedented brightness of beams that will result in much more physics data collected but also ?dirtier? conditions; each collision of interest will be accompanied by ~200 parasitic collisions. This will pose major challenges to all detector components, but especially to charge-particle detection and reconstruction.\n\nThe famous \"Moore's law\" states that chip transistor counts increases exponentially over time for the same cost. Over four decades, these decreases in transistor cost turned into exponential gains in the performance of software applications like those used in particle physics. Around 2005, the computing processor market reached an epochal turning point: power density limitations in chips ended this trend, and our applications no longer immediately run exponentially faster on subsequent generations of processors. This is true even though the underlying transistor count continues to increase per Moore?s law. What changed is the types of processors. Due to this change, the performance improvements are now more on the order of 20-25% per year, and many of the improvements in FLOP counts come from addition of specialized hardware such as vector units and many cores. Over the next decade, this corresponds to a factor of six growth instead of a factor of &sim; 30 as one would expect from Moore?s Law, and not enough to keep up with the exponential growth in the reconstruction time due to the harsher conditions expected during the High-Luminosity (HL) run of the LHC. A change is required to move from the sequential applications of today to vectorized, parallelized applications of tomorrow.\n\nThis was exactly the challenge and goal of this PIF project. We aimed to find a way to deploy the traditional sequential and serial charged-particle reconstruction on modern architectures by parallelizing and vectorizing them to fit to the modern computer architectures. It required redesign of data structures, and finding ways to avoid bottlenecks due to specific local challenges invariably present in different parts of the reconstructed event.\n\nIn results shown in multiple international conferences (CHEP, ACAT, CTD) as well as in internal CMS collaboration meetings we have demonstrated charged-particle reconstruction that is very similar (but not quite as good yet) physics performance (efficiency and fake rates) to the traditional one but approximately 8 times faster.  The project will lead to a deployment of new software code to address the challenges of the high luminosity LHC running in the near future. Work will be followed-up under the auspices of the IRIS-HEP project.\n\n \n\n\t\t\t\t\tLast Modified: 08/19/2019\n\n\t\t\t\t\tSubmitted by: Avi Yagil"
 }
}