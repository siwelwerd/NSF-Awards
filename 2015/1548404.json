{
 "awd_id": "1548404",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER/Collaborative Research: A New Science of Visual Experience",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922443",
 "po_email": "gaklutke@nsf.gov",
 "po_sign_block_name": "Georgia-Ann Klutke",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 104124.0,
 "awd_amount": 104124.0,
 "awd_min_amd_letter_date": "2015-09-04",
 "awd_max_amd_letter_date": "2015-09-04",
 "awd_abstract_narration": "The essence of human experience is interacting with the natural and man-made environments through the five human senses, and through vision in particular. The objective of this EArly-concept Grant for Exploratory Research (EAGER) project is to build analytical foundations for a new science of visual experience that will bridge basic approaches from cognitive science and systems engineering. Specifically, the research will build mathematical and computational models of a human navigating a three dimensional space such as a factory, museum, or retail store.  The idea is to gain insights into the limits on observability and controllability in human-technology systems, and to improve the user's situational awareness.  If the research is successful, researchers will be able to describe situations in terms of possibilities for action and access to information.  Such quantitative tools will allow engineers to design environments to achieve outcomes such as increased focus, improved safety, better wayfinding, and improved experience. In time, it might be possible to engineer interactive environments that adapt to the personal attributes and identities of the humans that inhabit them. The results of the research have the potential to be used by many disciplines such as engineering, business, architecture, psychology, cognition, and human factors.\r\n\r\nThe multidisciplinary research team consisting of academics (engineering, psychology, and computer science) and two industry personnel suggests moving visual experience of a three dimensional environment from the realm of intuition and experience to analytical science. The specific focus of this grant will be on developing a general set of analytical models that emerge from the analysis of a variety of context-specific human-environment interactions (e.g., nurse in CCU, shopper in a retail store). With a solid grounding in the basic psychology of Perception-Action, the analytical models will integrate three-dimensional spatial relationships with the human eye's field of vision and the physical attributes of a human. Most significantly, the research will consider complex dynamics resulting from human movement in the space, with all the attendant changes in visual angles, and the appearance and disappearance of visual obstacles. Human performance will be empirically examined in a Virtual Environment in order to validate the analytical metrics of visual experience.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Gue",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Kevin R Gue",
   "pi_email_addr": "kevin.gue@louisville.edu",
   "nsf_id": "000262064",
   "pi_start_date": "2015-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Louisville Research Foundation Inc",
  "inst_street_address": "2301 S 3RD ST",
  "inst_street_address_2": "",
  "inst_city_name": "LOUISVILLE",
  "inst_state_code": "KY",
  "inst_state_name": "Kentucky",
  "inst_phone_num": "5028523788",
  "inst_zip_code": "402081838",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "KY03",
  "org_lgl_bus_name": "UNIVERSITY OF LOUISVILLE",
  "org_prnt_uei_num": "",
  "org_uei_num": "E1KJM4T54MK6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Louisville",
  "perf_str_addr": "2301 South Third Street",
  "perf_city_name": "Louisville",
  "perf_st_code": "KY",
  "perf_st_name": "Kentucky",
  "perf_zip_code": "402920001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "KY03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "763300",
   "pgm_ele_name": "EFRI Research Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "030E",
   "pgm_ref_txt": "CONTROL SYSTEMS"
  },
  {
   "pgm_ref_code": "031E",
   "pgm_ref_txt": "MECHATRONICS"
  },
  {
   "pgm_ref_code": "032E",
   "pgm_ref_txt": "SENSORS AND ACTUATORS"
  },
  {
   "pgm_ref_code": "033E",
   "pgm_ref_txt": "Smart and responsive structures"
  },
  {
   "pgm_ref_code": "034E",
   "pgm_ref_txt": "Dynamical systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8025",
   "pgm_ref_txt": "Advanced Materials Processing"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 104124.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This is a collaborative research project with Professor Pratik Parikh and his team at Wright State University. The objective of the project was to build mathematical and computer-based foundations for a new &ldquo;science of visual experience&rdquo; that quantifies what humans can see in a physical space such as a factory, retail store, library, or museum. The ability to model environments in this way gives spatial designers such as industrial engineers, architects, interior designers and others, the ability to know how their designs will affect the people inhabiting those spaces. For example, retailers will have a better understanding of which products can be seen from where (and for how long) as shoppers roam the aisles, and factory designers will know which safety signage can be read and understood from which locations in a plant.&nbsp;</p>\n<p>The research was divided between teams at Wright State University and the University of Louisville. Researchers at Wright State focused on problems of exposure and intensity (what can be&nbsp;<em>seen&nbsp;</em>and for how long?) in two- and three-dimensional models. The team at the University of Louisville focused on the subjects of legibility (what can be&nbsp;<em>read&nbsp;</em>or interpreted?) and cross-visibility (what locations can be seen from a specific point?).</p>\n<p>At the University of Louisville, the research supported Mina Shekari, a Ph.D. student in industrial engineering. Mina conducted extensive literature reviews in legibility and cross-visibility&mdash;areas very new to her and to her advisor. She then used established and new methods to model the details of legibility, which records not just what can be seen but what can be read. For example, a label in 24-point font can be read from 10 feet away, but only if the label is &ldquo;sufficiently perpendicular&rdquo; to the line of sight; otherwise, the letters are visually compressed and indecipherable. Mina integrated existing models of these phenomena with new computational methods developed by the team at Wright State to produce new models of legibility in&nbsp;<em>dynamic</em>environments, in which people move through a space.</p>\n<p>The most significant result of her experiments was a surprising difference between retail store layouts designed to maximize exposure or intensity and layouts designed to maximize legibility. In general, designs that maximize exposure and intensity tend to align racks in such a way that people in the environment easily look &ldquo;down&rdquo; the aisles, and therefore the racks are oriented at extreme angles (close to 0 or 180 degrees) with respect to the eye. The point of these designs is only to allow the person to see that the location is there. Layouts that maximize legibility, by contrast, seek to orient racks nearer 90 degrees with respect to the eye in order to allow the human to read. Therefore, the design of racks and aisles in a retail setting should depend on the type of product on display.</p>\n<p>The results of this research comprise a collection of modeling tools with which researchers and developers can quantify what can be seen and read in environments in which people travel. These tools might be integrated into interactive software design packages that would allow designers to understand the implications of design choices on the humans that inhabit the designed spaces. In the long term, this research could be used to understand and design adaptive environments in which fonts change size or color, or perhaps objects reorient themselves to accommodate the specific physical and visual characteristics of people in the space.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/16/2018<br>\n\t\t\t\t\tModified by: Kevin&nbsp;R&nbsp;Gue</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis is a collaborative research project with Professor Pratik Parikh and his team at Wright State University. The objective of the project was to build mathematical and computer-based foundations for a new \"science of visual experience\" that quantifies what humans can see in a physical space such as a factory, retail store, library, or museum. The ability to model environments in this way gives spatial designers such as industrial engineers, architects, interior designers and others, the ability to know how their designs will affect the people inhabiting those spaces. For example, retailers will have a better understanding of which products can be seen from where (and for how long) as shoppers roam the aisles, and factory designers will know which safety signage can be read and understood from which locations in a plant. \n\nThe research was divided between teams at Wright State University and the University of Louisville. Researchers at Wright State focused on problems of exposure and intensity (what can be seen and for how long?) in two- and three-dimensional models. The team at the University of Louisville focused on the subjects of legibility (what can be read or interpreted?) and cross-visibility (what locations can be seen from a specific point?).\n\nAt the University of Louisville, the research supported Mina Shekari, a Ph.D. student in industrial engineering. Mina conducted extensive literature reviews in legibility and cross-visibility&mdash;areas very new to her and to her advisor. She then used established and new methods to model the details of legibility, which records not just what can be seen but what can be read. For example, a label in 24-point font can be read from 10 feet away, but only if the label is \"sufficiently perpendicular\" to the line of sight; otherwise, the letters are visually compressed and indecipherable. Mina integrated existing models of these phenomena with new computational methods developed by the team at Wright State to produce new models of legibility in dynamicenvironments, in which people move through a space.\n\nThe most significant result of her experiments was a surprising difference between retail store layouts designed to maximize exposure or intensity and layouts designed to maximize legibility. In general, designs that maximize exposure and intensity tend to align racks in such a way that people in the environment easily look \"down\" the aisles, and therefore the racks are oriented at extreme angles (close to 0 or 180 degrees) with respect to the eye. The point of these designs is only to allow the person to see that the location is there. Layouts that maximize legibility, by contrast, seek to orient racks nearer 90 degrees with respect to the eye in order to allow the human to read. Therefore, the design of racks and aisles in a retail setting should depend on the type of product on display.\n\nThe results of this research comprise a collection of modeling tools with which researchers and developers can quantify what can be seen and read in environments in which people travel. These tools might be integrated into interactive software design packages that would allow designers to understand the implications of design choices on the humans that inhabit the designed spaces. In the long term, this research could be used to understand and design adaptive environments in which fonts change size or color, or perhaps objects reorient themselves to accommodate the specific physical and visual characteristics of people in the space.\n\n \n\n\t\t\t\t\tLast Modified: 07/16/2018\n\n\t\t\t\t\tSubmitted by: Kevin R Gue"
 }
}