{
 "awd_id": "1534745",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase II:  Three-Dimensional Computational Optical Imaging Sensor",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Muralidharan Nair",
 "awd_eff_date": "2015-09-15",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 733549.0,
 "awd_amount": 941437.0,
 "awd_min_amd_letter_date": "2015-09-14",
 "awd_max_amd_letter_date": "2018-08-03",
 "awd_abstract_narration": "The broader impact/commercial potential of this project stems from the possibility of\r\nobtaining real-time precise 3D depth information using a miniature robust sensor system\r\nopening up a myriad of opportunities for commercial application. Imaging sensors are\r\nnow widespread and inexpensive, as is computing power, already an integral part of\r\nmost cameras. The 3D imaging sensor to be developed enables disruptive applications\r\nfor manufacturing, robotics, human-machine interfaces, unmanned aerial vehicles, and\r\nemerging 3D scanners.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase 2 project is focused on the\r\ndesign, development, and testing of a miniature, robust, low-cost optical imaging sensor\r\nsystem capable of acquiring three-dimensional (3D) information from a scene with high\r\nprecision and accuracy, overcoming current state-of-the-art technologies. The sensor\r\nprovides, from a single shot, an image and a depth map; associating each object\r\nfeature with its precise 3D location. With the advances in sensing technology, 3D\r\ninformation is increasingly incorporated into real-world applications?from\r\nmanufacturing to entertainment and security. The proposed sensor provides\r\nimprovements in depth resolution while being fast, compact, lightweight, and amenable\r\nfor mass production at low cost.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anurag",
   "pi_last_name": "Agrawal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anurag Agrawal",
   "pi_email_addr": "anurag@doublehelixoptics.com",
   "nsf_id": "000653762",
   "pi_start_date": "2015-09-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Double Helix Optics Inc",
  "inst_street_address": "3415 COLORADO AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOULDER",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3037354900",
  "inst_zip_code": "803031904",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "DOUBLE HELIX OPTICS, INC.",
  "org_prnt_uei_num": "UV6KRAFL8LJ2",
  "org_uei_num": "UV6KRAFL8LJ2"
 },
 "perf_inst": {
  "perf_inst_name": "Double Helix LLC",
  "perf_str_addr": "3415 Colorado Ave",
  "perf_city_name": "Boulder",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "803031904",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1185",
   "pgm_ref_txt": "SENSORY SYSTEMS"
  },
  {
   "pgm_ref_code": "165E",
   "pgm_ref_txt": "SBIR Phase IIB"
  },
  {
   "pgm_ref_code": "5373",
   "pgm_ref_txt": "SMALL BUSINESS PHASE II"
  },
  {
   "pgm_ref_code": "8035",
   "pgm_ref_txt": "Hardware Devices"
  },
  {
   "pgm_ref_code": "8240",
   "pgm_ref_txt": "SBIR/STTR CAP"
  },
  {
   "pgm_ref_code": "9139",
   "pgm_ref_txt": "INFORMATION INFRASTRUCTURE & TECH APPL"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 733549.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 207888.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With the advances in sensing technology, 3D information is increasingly incorporated into real-world applications?from architecture to entertainment, manufacturing, to security. The extraction of 3D information has been studied for decades, but remains a challenging problem, in particular under unconstrained environments that can include variable lighting, specular and deforming scene surfaces, and occluded objects, among other things. A major challenge to date has been integrating depth information with increased resolution and depth range. There are several existing architectures for estimating depth that rely on methods like triangulation (capture two images from different viewpoints), time of flight (measure amount of time that light takes to bounce back from an object), light-field (map the direction and position of light rays from each object point), etc. To date these techniques ? each with their own relative strengths - have required users to trade off precision of the image captured with the depth range. These different techniques are additionally constrained by hardware complexity, size, power consumption or cost to be applicable for broad adoption. In the face of these challenges, a new approach to 3D object capture is inevitable, one that will extend capabilities and enable improvements in both depth resolution and precision in areas such as 3D machine vision, gesture recognition and robotics.</p>\n<p>This SBIR Phase II project successfully advanced the feasibility work performed in Phase I towards an optical imaging sensor capable of acquiring extended-depth three-dimensional (3D) information from a scene with high precision and accuracy. The system simultaneously provides, from a single shot, a brightness map (an image) as well as distance information (a depth map); so that each object feature within a scene is associated with its precise location in 3D space. The system is capable of working in active (structured light) or passive (ambient light) modes with scalable imaging volume and resolution based on the imaging system and conditions. The data provided by the system can be used for 3D rendering, refocus at multiple depths, and other higher-level operations. Furthermore, the system can also be used for extended depth 2D imaging with negligible resolution loss for applications that do not require depth information but have a limited light budget.</p>\n<p>The sensor system is based on a novel integrated computational optical imaging paradigm. In particular, it utilizes a unique diffractive optical element (DOE) design capable of encoding depth information with enhanced resolution. These DOEs implement system responses that are dramatically depth variant and hence provide enhanced depth sensitivity combined with reconstruction algorithms to provide the final 3D information. The proposed sensor system is fast, parallel, compact, and scalable.</p>\n<p>The accomplishments of Phase II have laid a firm foundation for a new technology that can be advanced and refined to take advantage of significant commercial potential in the fields of industrial inspection, materials science, and other commercial applications by enabling conventional 2D imaging systems to simultaneously capture high-resolution depth and intensity information. Moreover, the sensor is amenable to mass production at low cost, enabling applications in areas such as robotics, 3D scanners, advanced manufacturing, and human-machine interfaces.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/17/2019<br>\n\t\t\t\t\tModified by: Anurag&nbsp;Agrawal</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWith the advances in sensing technology, 3D information is increasingly incorporated into real-world applications?from architecture to entertainment, manufacturing, to security. The extraction of 3D information has been studied for decades, but remains a challenging problem, in particular under unconstrained environments that can include variable lighting, specular and deforming scene surfaces, and occluded objects, among other things. A major challenge to date has been integrating depth information with increased resolution and depth range. There are several existing architectures for estimating depth that rely on methods like triangulation (capture two images from different viewpoints), time of flight (measure amount of time that light takes to bounce back from an object), light-field (map the direction and position of light rays from each object point), etc. To date these techniques ? each with their own relative strengths - have required users to trade off precision of the image captured with the depth range. These different techniques are additionally constrained by hardware complexity, size, power consumption or cost to be applicable for broad adoption. In the face of these challenges, a new approach to 3D object capture is inevitable, one that will extend capabilities and enable improvements in both depth resolution and precision in areas such as 3D machine vision, gesture recognition and robotics.\n\nThis SBIR Phase II project successfully advanced the feasibility work performed in Phase I towards an optical imaging sensor capable of acquiring extended-depth three-dimensional (3D) information from a scene with high precision and accuracy. The system simultaneously provides, from a single shot, a brightness map (an image) as well as distance information (a depth map); so that each object feature within a scene is associated with its precise location in 3D space. The system is capable of working in active (structured light) or passive (ambient light) modes with scalable imaging volume and resolution based on the imaging system and conditions. The data provided by the system can be used for 3D rendering, refocus at multiple depths, and other higher-level operations. Furthermore, the system can also be used for extended depth 2D imaging with negligible resolution loss for applications that do not require depth information but have a limited light budget.\n\nThe sensor system is based on a novel integrated computational optical imaging paradigm. In particular, it utilizes a unique diffractive optical element (DOE) design capable of encoding depth information with enhanced resolution. These DOEs implement system responses that are dramatically depth variant and hence provide enhanced depth sensitivity combined with reconstruction algorithms to provide the final 3D information. The proposed sensor system is fast, parallel, compact, and scalable.\n\nThe accomplishments of Phase II have laid a firm foundation for a new technology that can be advanced and refined to take advantage of significant commercial potential in the fields of industrial inspection, materials science, and other commercial applications by enabling conventional 2D imaging systems to simultaneously capture high-resolution depth and intensity information. Moreover, the sensor is amenable to mass production at low cost, enabling applications in areas such as robotics, 3D scanners, advanced manufacturing, and human-machine interfaces.\n\n\t\t\t\t\tLast Modified: 10/17/2019\n\n\t\t\t\t\tSubmitted by: Anurag Agrawal"
 }
}