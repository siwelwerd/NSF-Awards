{
 "awd_id": "1527208",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: Task Dependent Semantic Modeling for Robot Perception",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 267486.0,
 "awd_amount": 267486.0,
 "awd_min_amd_letter_date": "2015-08-12",
 "awd_max_amd_letter_date": "2015-08-12",
 "awd_abstract_narration": "The research in this project enables robots to better deal with the complex cluttered environments around us, ranging from open scenes to cluttered table-top settings and to perform the basic mapping, navigation, object search so as to enable fetch and delivery tasks most commonly required in service co-robotics applications.  The key contribution of the project is to develop visual perception systems for robots that can understand the semantic labels of the visual world at multiple levels of specificity as required by particular robot tasks or human-robot interaction.  In addition, the project enables robot perception systems to better understand new, previously unseen, environments through automatically adapting existing learned models, and by actively choosing how to best explore and recognize novel visual spaces and objects.  The datasets and benchmarks, as well as the developed models, form basis for more rapid progress on semantic visual perception for robotics.\r\n\r\nThe development of methodologies for learning compositional representations which enable active learning and efficient inference is a long standing problem in computer vision and robot perception. Guided by the constraints of indoors and outdoors environments, we plan to exploit large amounts of data, strong geometric and semantic priors and develop novel representations of objects and scenes. The developed representations are captured by compositional structured probabilistic models including deep convolutional networks. Doing this rapidly is required to support active visual exploration to improve semantic parsing of a space.  Furthermore the project team collects and disseminates a large dataset of densely sampled RGBD imagery to support offline evaluation and benchmarking of active vision for semantic parsing.  The project can result in advances in active hierarchical semantic vision for robot tasks including exploration, search, manipulation, programming by example, and generally for human-robot interaction.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jana",
   "pi_last_name": "Kosecka",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jana Kosecka",
   "pi_email_addr": "kosecka@gmu.edu",
   "nsf_id": "000207363",
   "pi_start_date": "2015-08-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "George Mason University",
  "inst_street_address": "4400 UNIVERSITY DR",
  "inst_street_address_2": "",
  "inst_city_name": "FAIRFAX",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "7039932295",
  "inst_zip_code": "220304422",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "VA11",
  "org_lgl_bus_name": "GEORGE MASON UNIVERSITY",
  "org_prnt_uei_num": "H4NRWLFCDF43",
  "org_uei_num": "EADLFP7Z72E5"
 },
 "perf_inst": {
  "perf_inst_name": "George Mason University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "220304422",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "VA11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 267486.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; min-height: 14.0px} -->\n<p class=\"p1\">Consider mobile robots engaged in fetch and delivery tasks in indoor and outdoor scenes.&nbsp; Simple task specified by instructions as \"Bring me a coke can from the refrigerator\", require mobile agent to&nbsp; recognize objects of interest, navigate to specific locations and understand the context of the environment to search for objects in case the objects are not visible.&nbsp;</p>\n<p class=\"p2\"><span>&nbsp;</span>The first research thrust focused on object&nbsp; instance detection, semantic segmentation and 3D pose recovery of objects. Previous approaches for object detection and recognition developed in computer vision focus on object categories (cars, pedestrians) and use large annotated images for training highly performant models based on Deep Convolutional Neural Networks (DCNNs). This approach is not scalable to hundreds of object instances. To overcome the need for tedious annotations required for training supervised object detection models, we have developed novel methods for automating the process of data generation by synthesizing the training data for object instance detection and object instance pose recovery. Using repositories of 3D textured models of object instances we have demonstrated how to synthesize images and annotations and train effective and scalable object detectors. The idea of bypassing the tedious annotation stage was extended to 3D object pose recovery, where we used untextured 3D CAD models of objects that were exploited for training networks.&nbsp;</p>\n<p class=\"p1\">The second research thrust focused on mobile robot navigation. For navigation, the traditional approaches typically rely on the availability of metric and semantic maps of the environments.<span>&nbsp;</span>The goal was to develop methods for learning navigation strategies for searching and reaching the targets in the absence of prior map of the environment.&nbsp; We have shown how to exploit semantic information about objects for learning target driven navigation policies in reinforcement learning framework. The models were trained using mixture of real and sythetic environments, showing agent's ability to reach the targets effectively in previously unseen environments.&nbsp;</p>\n<p class=\"p2\">Overall we have developed class of novel machine learning approaches and models for effective object detection and target driven navigation without an explicit map.&nbsp; We have created two datasets for benchmarking and evaluation of object instance detection and target driven navigation and established novel baselines for both of these tasks.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2019<br>\n\t\t\t\t\tModified by: Jana&nbsp;Kosecka</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nConsider mobile robots engaged in fetch and delivery tasks in indoor and outdoor scenes.  Simple task specified by instructions as \"Bring me a coke can from the refrigerator\", require mobile agent to  recognize objects of interest, navigate to specific locations and understand the context of the environment to search for objects in case the objects are not visible. \n The first research thrust focused on object  instance detection, semantic segmentation and 3D pose recovery of objects. Previous approaches for object detection and recognition developed in computer vision focus on object categories (cars, pedestrians) and use large annotated images for training highly performant models based on Deep Convolutional Neural Networks (DCNNs). This approach is not scalable to hundreds of object instances. To overcome the need for tedious annotations required for training supervised object detection models, we have developed novel methods for automating the process of data generation by synthesizing the training data for object instance detection and object instance pose recovery. Using repositories of 3D textured models of object instances we have demonstrated how to synthesize images and annotations and train effective and scalable object detectors. The idea of bypassing the tedious annotation stage was extended to 3D object pose recovery, where we used untextured 3D CAD models of objects that were exploited for training networks. \nThe second research thrust focused on mobile robot navigation. For navigation, the traditional approaches typically rely on the availability of metric and semantic maps of the environments. The goal was to develop methods for learning navigation strategies for searching and reaching the targets in the absence of prior map of the environment.  We have shown how to exploit semantic information about objects for learning target driven navigation policies in reinforcement learning framework. The models were trained using mixture of real and sythetic environments, showing agent's ability to reach the targets effectively in previously unseen environments. \nOverall we have developed class of novel machine learning approaches and models for effective object detection and target driven navigation without an explicit map.  We have created two datasets for benchmarking and evaluation of object instance detection and target driven navigation and established novel baselines for both of these tasks. \n\n \n\n\t\t\t\t\tLast Modified: 12/30/2019\n\n\t\t\t\t\tSubmitted by: Jana Kosecka"
 }
}