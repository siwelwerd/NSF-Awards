{
 "awd_id": "1526386",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Techniques and Frameworks for Exploiting Recent SIMD Architectural Advances",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2020-06-30",
 "tot_intn_awd_amt": 449999.0,
 "awd_amount": 449999.0,
 "awd_min_amd_letter_date": "2015-06-30",
 "awd_max_amd_letter_date": "2020-03-24",
 "awd_abstract_narration": "Single Instruction Multiple Data (SIMD) parallelism has been available in common processors for several decades now, and has been widely exploited for dense matrix and other regular problems. Recent architectural trends, such as increasing width of SIMD lanes coupled with instruction sets, provide significantly enhanced functionality.  There is a need to effectively use the new architectural features for dynamic or irregular applications, which have not been easy to perform on parallel on SIMD processors in the past.   \r\n\r\nThe PI proposes comprehensive research program on mapping various classes of unstructured and/or irregular applications to modern SIMD novel architectural features and developing optimized compiler transformations from programs written in CUDA and OpenCL. This research proposal proposes to address the challenge of developing and executing unstructured and/or irregular applications using novel SIMD processors' instructions such as scatter and gather. The PI plans to design compiler transformation methods from CUDA and OpenCL programs to increase the number of contiguous accesses  and decrease the number of cache lines from which data needs to be accessed. These novel transformation methods are targeting applications such as unstructured grid kernels, sparse matrix computations, and graph and tree traversals.  The PI plans to continue development of an Operator Overloading based library.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Gagan",
   "pi_last_name": "Agrawal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gagan Agrawal",
   "pi_email_addr": "gagrawal@uga.edu",
   "nsf_id": "000230838",
   "pi_start_date": "2015-06-30",
   "pi_end_date": "2020-01-07"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rajiv",
   "pi_last_name": "Ramnath",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rajiv Ramnath",
   "pi_email_addr": "ramnath.6@osu.edu",
   "nsf_id": "000272508",
   "pi_start_date": "2020-01-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101277",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 449999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project researched how Single Instruction Multiple Data (SIMD) parallelism can be applied successfully to dynamic or \"irregular\" applications (such as multiplying matrices that are a mix of sparse and dense components) by using new and emerging system architectures which have specialized components, such as GPUs, thereby providing a range of capabilities. These dynamic or irregular applications have not been easy to parallelize well on older SIMD architectures which contain only standard components such as CPUs, perhaps with multiple cores, because these applications are not uniform in their computational needs. Major activities have been 1) a development of a sampling and reconstruction method to parallelize loops with dependencies (while using SIMD parallelism to gain efficiency), 2) extension of this method to execute efficiently on GPGPUs, 3) development of sampling methods that can help predict execution time of irregular applications with SIMD parallelism 4) development of compression techniques to make GPU calculations more efficient 5) out-of-core implementation of sparse matrix multiplications and 6) optimization of large-scale graph learning algorithms. Significant intellectual merit contributions include new methods to (a) merge partial sparse matrix multiplications (on samples) in parallel on GPUs and thereby achieve almost linear scalability, (b) parallelize sequential loops and achieve linear scalability across multiple cores, specifically, types of loops that could not be successfully parallelized by current state-of-the-art techniques, (c) improve data locality for and scale multiplications of sparse and dense matrices, (d) reduce data movement by summarizing and thereby compressing data as well as by localizing computations, all without loss of accuracy. &nbsp;<br /><br />The indirect broader impacts of this work are in the improved efficiency of important types of computations that are very common in scientific applications, thus increasing the scalability of these applications. The direct broader impacts have been in the education and research training of ten graduate students, two of whom have gone on to academic careers.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/20/2020<br>\n\t\t\t\t\tModified by: Rajiv&nbsp;Ramnath</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project researched how Single Instruction Multiple Data (SIMD) parallelism can be applied successfully to dynamic or \"irregular\" applications (such as multiplying matrices that are a mix of sparse and dense components) by using new and emerging system architectures which have specialized components, such as GPUs, thereby providing a range of capabilities. These dynamic or irregular applications have not been easy to parallelize well on older SIMD architectures which contain only standard components such as CPUs, perhaps with multiple cores, because these applications are not uniform in their computational needs. Major activities have been 1) a development of a sampling and reconstruction method to parallelize loops with dependencies (while using SIMD parallelism to gain efficiency), 2) extension of this method to execute efficiently on GPGPUs, 3) development of sampling methods that can help predict execution time of irregular applications with SIMD parallelism 4) development of compression techniques to make GPU calculations more efficient 5) out-of-core implementation of sparse matrix multiplications and 6) optimization of large-scale graph learning algorithms. Significant intellectual merit contributions include new methods to (a) merge partial sparse matrix multiplications (on samples) in parallel on GPUs and thereby achieve almost linear scalability, (b) parallelize sequential loops and achieve linear scalability across multiple cores, specifically, types of loops that could not be successfully parallelized by current state-of-the-art techniques, (c) improve data locality for and scale multiplications of sparse and dense matrices, (d) reduce data movement by summarizing and thereby compressing data as well as by localizing computations, all without loss of accuracy.  \n\nThe indirect broader impacts of this work are in the improved efficiency of important types of computations that are very common in scientific applications, thus increasing the scalability of these applications. The direct broader impacts have been in the education and research training of ten graduate students, two of whom have gone on to academic careers.\n\n\t\t\t\t\tLast Modified: 10/20/2020\n\n\t\t\t\t\tSubmitted by: Rajiv Ramnath"
 }
}