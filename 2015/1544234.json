{
 "awd_id": "1544234",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: The Role of Instructor and Peer Feedback in Improving the Cognitive, Interpersonal, and Intrapersonal Competencies of Student Writers in STEM Courses",
 "cfda_num": "47.076",
 "org_code": "11010000",
 "po_phone": "7032925309",
 "po_email": "cdellapi@nsf.gov",
 "po_sign_block_name": "Connie Della-Piana",
 "awd_eff_date": "2015-09-15",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 69520.0,
 "awd_amount": 69520.0,
 "awd_min_amd_letter_date": "2015-09-16",
 "awd_max_amd_letter_date": "2015-09-16",
 "awd_abstract_narration": "The Promoting Research and Innovation in Methodologies for Evaluation (PRIME) program seeks to support research on evaluation with special emphasis on: (1) exploring innovative approaches for determining the impacts and usefulness of STEM education projects and programs; (2) building on and expanding the theoretical foundations for evaluating STEM education and workforce development initiatives, including translating and adapting approaches from other fields; and (3) growing the capacity and infrastructure of the evaluation field.\r\n\r\nThis project will have critical significance for Science, Technology, Engineering, and Mathematics (STEM) educators by increasing writing and collaboration skills in students, areas of importance to economics, science, and national security. This study focuses on teacher and peer interactions and writing quality and improvement in the context of undergraduate STEM courses. Specifically, the project will map the development of three competency domains (cognitive, interpersonal and intrapersonal) by researching the effects of teacher and peer response on writing improvement and knowledge adaptation in STEM courses. The project utilizes a web-based assessment tool called My Reviewers (MyR). The tool will be piloted by STEM faculty in college-level Introductory Biology or Chemistry on the campuses of University of South Florida (USF), North Carolina State University (NCSU), Dartmouth, Massachusetts Institute of Technology (MIT), and University of Pennsylvania (UPenn). Research domains include both academic performance and inter/intra-personal competencies. Project deliverables will provide new tools and procedures to assist in the assessment of students' knowledge, skills, and attitudes for project and program evaluation.\r\n\r\nApproximately 10,000 students enrolled in STEM courses at USF, NCSU, Dartmouth, MIT, and UPenn will upload their course-based writing to My Reviewers, an assessment tool, and use the tool to conduct peer reviews and team projects.  This information is supplemented by surveys of demographics and dispositions along with click patterns within the toolset. Researchers will subsequently analyze this wealth of data using predictive modeling of student writing ability and improvement, including text-based methods to identify useful features of comments, papers, peer reviews, student evaluations of other peers? reviews, and instructor and student meta-reflections. Outcome goals are to (1) demonstrate ways the assessment community can use real-time assessment tools to create valid measures of writing development; (2) provide quantitative evidence regarding the likely effects of particular commenting and scoring patterns on cohorts of students; (3) offer a domain map to help STEM educators better understand student success in the STEM curriculum; and (4) inform STEM faculty regarding the efficacy of peer review.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DGE",
 "org_div_long_name": "Division Of Graduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Suzanne",
   "pi_last_name": "Lane",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Suzanne T Lane",
   "pi_email_addr": "stlane@mit.edu",
   "nsf_id": "000667632",
   "pi_start_date": "2015-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "726100",
   "pgm_ele_name": "Project & Program Evaluation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "009Z",
   "pgm_ref_txt": "PRIME - Promoting Research and Innovatio"
  }
 ],
 "app_fund": [
  {
   "app_code": "0415",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001516DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 69520.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aimed to support, and research the effects of, introducing student peer review in STEM subjects at five universities.&nbsp; Researchers at MIT, the University of South Florida, the University of Pennsylvania, North Carolina State University, and Dartmouth College used an on online peer-review platform called MyReviewers to gather data from student and instructor feedback on written drafts of assignments in STEM subjects, as well as survey data, in order to assess how providing written feedback to peers? drafts affected students? cognitive, interpersonal, and intra-personal (e.g., self-awareness, self-regulation) abilities.</p>\n<p>&nbsp;</p>\n<p>This project had a number of specific goals: to develop methods and materials for teaching writing and peer review in STEM subjects, through collaborating with faculty in various STEM disciplines; to understand how the act of giving and receiving comments on drafts in STEM subjects affects student?s performance; to consider the relationship between various domains in peer comments; and to provide an evidence base for future, big-data approaches to analyzing peer review.</p>\n<p>&nbsp;</p>\n<p>The co-PIs at each of the five participating institutions developed their own instructional material, as well as their own research design.&nbsp; At MIT, we began from a theoretical framework of integrating knowledge domains developed by Anne Beaufort in <em>College Writing and Beyond: A New Framework for University Writing Instruction</em> (Utah State Press, 2007). This framework focuses on the work students must do to synthesize and operationalize knowledge of subject matter, rhetorical situation, genre, discourse community, and writing process, in order to communicate effectively in a discipline.&nbsp; Our primary research focus was on the extent to which students are able to synthesize these domains in their peer feedback, and on which type of instruction and rubrics might best promote this synthesis, thus laying the foundation for best practices for faculty in STEM disciplines when including peer review.&nbsp;</p>\n<p>&nbsp;</p>\n<p>To address these questions, we worked with STEM subjects in Mathematics, Biology, Computer Science and Electrical Engineering, and Materials Science and Engineering. &nbsp;In some of these subjects, we were able to create an integrated curriculum that synthesized instruction in the various knowledge domains; in others, the domains were taught independently.&nbsp; For each subject, we created peer review rubrics aligned with the instructional approach and goals.</p>\n<p>&nbsp;</p>\n<p>To analyze students? peer comments, we created a coding framework that allowed us to quantify comments on two scales.&nbsp; First, we coded each comment as either evaluating a strength or weakness in a peer?s draft, or offering a suggestion for revision.&nbsp; Prior research strongly documents that providing suggestions correlates with greater improvement, for both the reviewer and the reviewee.&nbsp; Next, we coded each comment for the knowledge domains it addressed, which also allowed us to identify the extent to which students integrate knowledge domains in their comments.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Significant differences were found between the different subjects and instructional approaches. Across the different classes, the percentage of comments that offered a revision suggestion ranged from 34% to 82%.&nbsp; Similarly, the percentage of comments that integrated knowledge domains ranged from 13% to 53%. These two scales showed a high degree of correlation, and also correlated to the degree of domain integration in the instruction, though our methodology did not allow us to draw causal conclusions.</p>\n<p>&nbsp;</p>\n<p>Percentage of comments does not tell the whole story, however, since the rubrics differed in the number of questions they asked and thus invited students to offer more or fewer comments. While students were always able, with the tools built into the MyReviewers system, to provide as many in-text comments as they liked, in addition to completing rubric items, most students approached the task as one of answering the rubric questions.&nbsp; Thus we also calculated the average number of comments in each category that students gave and received. &nbsp;The average number of comments offered ranged from 2.8 to 14.9; average suggestions from 2.1 to 8.9; and average comments combing knowledge domains from .82 to 4.8. Even when fewer comments overall were offered, due largely to differences in rubric design, integrated instruction and rubrics still led to more suggestions and more integrated comments offered overall.</p>\n<p>&nbsp;</p>\n<p>We can draw the following implications from this research:</p>\n<p>&nbsp;</p>\n<p>Even novice reviewers (first semester sophomores in their first or second subject in a STEM discipline) can offer extensive, integrated feedback to peers, including making many suggestions for revision, when they receive instruction that integrates all five knowledge domains.</p>\n<p>&nbsp;</p>\n<p>Integrated instruction and integrated rubrics are likely to help students both integrate knowledge domains in their comments, and to provide more revision suggestions to their peers.</p>\n<p>&nbsp;</p>\n<p>Rubrics that ask students to comment separately on the technical content and on aspects of the writing are much less likely to lead students to make useful revision suggestions to their peers.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/02/2019<br>\n\t\t\t\t\tModified by: Suzanne&nbsp;T&nbsp;Lane</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project aimed to support, and research the effects of, introducing student peer review in STEM subjects at five universities.  Researchers at MIT, the University of South Florida, the University of Pennsylvania, North Carolina State University, and Dartmouth College used an on online peer-review platform called MyReviewers to gather data from student and instructor feedback on written drafts of assignments in STEM subjects, as well as survey data, in order to assess how providing written feedback to peers? drafts affected students? cognitive, interpersonal, and intra-personal (e.g., self-awareness, self-regulation) abilities.\n\n \n\nThis project had a number of specific goals: to develop methods and materials for teaching writing and peer review in STEM subjects, through collaborating with faculty in various STEM disciplines; to understand how the act of giving and receiving comments on drafts in STEM subjects affects student?s performance; to consider the relationship between various domains in peer comments; and to provide an evidence base for future, big-data approaches to analyzing peer review.\n\n \n\nThe co-PIs at each of the five participating institutions developed their own instructional material, as well as their own research design.  At MIT, we began from a theoretical framework of integrating knowledge domains developed by Anne Beaufort in College Writing and Beyond: A New Framework for University Writing Instruction (Utah State Press, 2007). This framework focuses on the work students must do to synthesize and operationalize knowledge of subject matter, rhetorical situation, genre, discourse community, and writing process, in order to communicate effectively in a discipline.  Our primary research focus was on the extent to which students are able to synthesize these domains in their peer feedback, and on which type of instruction and rubrics might best promote this synthesis, thus laying the foundation for best practices for faculty in STEM disciplines when including peer review. \n\n \n\nTo address these questions, we worked with STEM subjects in Mathematics, Biology, Computer Science and Electrical Engineering, and Materials Science and Engineering.  In some of these subjects, we were able to create an integrated curriculum that synthesized instruction in the various knowledge domains; in others, the domains were taught independently.  For each subject, we created peer review rubrics aligned with the instructional approach and goals.\n\n \n\nTo analyze students? peer comments, we created a coding framework that allowed us to quantify comments on two scales.  First, we coded each comment as either evaluating a strength or weakness in a peer?s draft, or offering a suggestion for revision.  Prior research strongly documents that providing suggestions correlates with greater improvement, for both the reviewer and the reviewee.  Next, we coded each comment for the knowledge domains it addressed, which also allowed us to identify the extent to which students integrate knowledge domains in their comments. \n\n \n\nSignificant differences were found between the different subjects and instructional approaches. Across the different classes, the percentage of comments that offered a revision suggestion ranged from 34% to 82%.  Similarly, the percentage of comments that integrated knowledge domains ranged from 13% to 53%. These two scales showed a high degree of correlation, and also correlated to the degree of domain integration in the instruction, though our methodology did not allow us to draw causal conclusions.\n\n \n\nPercentage of comments does not tell the whole story, however, since the rubrics differed in the number of questions they asked and thus invited students to offer more or fewer comments. While students were always able, with the tools built into the MyReviewers system, to provide as many in-text comments as they liked, in addition to completing rubric items, most students approached the task as one of answering the rubric questions.  Thus we also calculated the average number of comments in each category that students gave and received.  The average number of comments offered ranged from 2.8 to 14.9; average suggestions from 2.1 to 8.9; and average comments combing knowledge domains from .82 to 4.8. Even when fewer comments overall were offered, due largely to differences in rubric design, integrated instruction and rubrics still led to more suggestions and more integrated comments offered overall.\n\n \n\nWe can draw the following implications from this research:\n\n \n\nEven novice reviewers (first semester sophomores in their first or second subject in a STEM discipline) can offer extensive, integrated feedback to peers, including making many suggestions for revision, when they receive instruction that integrates all five knowledge domains.\n\n \n\nIntegrated instruction and integrated rubrics are likely to help students both integrate knowledge domains in their comments, and to provide more revision suggestions to their peers.\n\n \n\nRubrics that ask students to comment separately on the technical content and on aspects of the writing are much less likely to lead students to make useful revision suggestions to their peers.\n\n\t\t\t\t\tLast Modified: 12/02/2019\n\n\t\t\t\t\tSubmitted by: Suzanne T Lane"
 }
}