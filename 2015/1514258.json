{
 "awd_id": "1514258",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Robotic Assistance with Dressing using Simulation-Based Optimization",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 1199987.0,
 "awd_amount": 1199987.0,
 "awd_min_amd_letter_date": "2015-07-08",
 "awd_max_amd_letter_date": "2019-10-24",
 "awd_abstract_narration": "The aging population, rising healthcare costs, and shortage of healthcare workers in the United States create a pressing need for affordable and effective personalized care. Physical disabilities due to illness, injury, or aging can result in people having difficulty dressing themselves, and the healthcare community has found that dressing is an important task for independent living.  The goal of this research is to develop techniques that enable robots to assist people with putting on clothing, which is a challenging task for robots due to the complexities of cloth, the human body, and robots. A key aspect of this research is that robots will discover how they can help people by quickly trying out many options in a computer simulation.  Success in this research would make progress towards robots capable of giving millions of people greater independence and a higher quality of life. In addition to healthcare applications, this research will result in better computer tools for fruitful collaborations between robots and humans in other scenarios.\r\n\r\nThis research uses efficient physics simulation and optimization tools to substantially automate the design of assistive robots for dressing. The approach considers the robot to be an assistive device that a human learns to use. The system optimizes the assistive robot based on what a particular human with impairments is capable of doing comfortably, rather than what he/she typically does. This approach automatically optimizes personalized assistive controllers for a particular user and article of clothing via simulation. Due to frequent line-of-sight occlusion and the importance of controlling forces applied to the user's body, controllers that use data-driven haptic perception are trained using simulation-generated data. These capabilities critically depend on advancements in the efficient physical simulation of cloth, robots, and humans, as well as the discovery of appropriate human motions for a given assistive robot. This work advances the state of the art in assistive robotics, haptic perception, human modeling, optimization and efficient physical simulation. Evaluation of the system is in simulation and in the real world with test rigs that model aspects of dressing, a PR2 robot dressing a humanoid robot, and a PR2 dressing able-bodied participants with restricted motion.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Karen",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Karen Liu",
   "pi_email_addr": "karenliu@cs.stanford.edu",
   "nsf_id": "000430108",
   "pi_start_date": "2015-07-08",
   "pi_end_date": "2019-08-15"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "Kemp",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Charles C Kemp",
   "pi_email_addr": "charlie.kemp@bme.gatech.edu",
   "nsf_id": "000367160",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Greg",
   "pi_last_name": "Turk",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Greg Turk",
   "pi_email_addr": "turk@cc.gatech.edu",
   "nsf_id": "000469303",
   "pi_start_date": "2015-07-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "Kemp",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Charles C Kemp",
   "pi_email_addr": "charlie.kemp@bme.gatech.edu",
   "nsf_id": "000367160",
   "pi_start_date": "2015-07-08",
   "pi_end_date": "2019-08-15"
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 1199987.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-3febfd85-7fff-81a4-5044-d4bc59c902e3\"> </span></p>\n<p dir=\"ltr\"><span>Millions of people require physical assistance on a daily basis due to disabilities resulting from illness, injury, aging, and other causes. Dressing is an important daily task with which many people require physical assistance. For example, more older adults receive physical assistance with dressing than with feeding or walking.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Robot-assisted dressing has the potential to increase independence and improve quality of life for people with disabilities. However, dressing is a complex task involving physical interactions between a garment, a robot, and a person. The task also entails risk. For example, taut fabrics can apply dangerously high force to a person's body and cut off circulation.&nbsp;</span></p>\n<p dir=\"ltr\"><span>This award supported pioneering research on robot-assisted dressing. The team enabled robots to learn how to assist with dressing using computer simulations of robots, people, and clothing (see Image 3). Robots also learned how to assist people in personalized ways by modeling an individual's capabilities. For example, a robot successfully provided personalized dressing assistance to people with disabilities in a study supported by this award (see Image 1).&nbsp;</span></p>\n<p dir=\"ltr\"><span>Since dressing involves visual occlusion of the person's body, the team also made novel contributions to robotic sensing. For example, the team enabled robots to use force sensing to infer the pressure applied to the person's body while assisting them (see Image 4). The team also enabled robots to predict the pressure that would result from different actions in order to select better actions and thus provide better assistance. The team also created novel capacitive sensing methods that enable robots to sense the human body without touching it (see Image 2).&nbsp;</span></p>\n<p dir=\"ltr\"><span>In addition to these contributions, the team produced peer-reviewed publications, open source software, and open hardware. The award also supported education by inspiring classes, helping with the training of undergraduate and graduate students, and creating compelling demonstrations for local junior high and high school students during tours for National Robotics Week.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/18/2022<br>\n\t\t\t\t\tModified by: Charles&nbsp;C&nbsp;Kemp</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652894089837_auro_dressing_2019--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652894089837_auro_dressing_2019--rgov-800width.jpg\" title=\"Image 1: robot-assisted dressing\"><img src=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652894089837_auro_dressing_2019--rgov-66x44.jpg\" alt=\"Image 1: robot-assisted dressing\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A person with a disability receives personalized dressing assistance from a robot.</div>\n<div class=\"imageCredit\">Georgia Institute of Technology</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Charles&nbsp;C&nbsp;Kemp</div>\n<div class=\"imageTitle\">Image 1: robot-assisted dressing</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652894239847_capacitive_servoing--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652894239847_capacitive_servoing--rgov-800width.jpg\" title=\"Image 2: capacitive sensing for robot-assisted dressing\"><img src=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652894239847_capacitive_servoing--rgov-66x44.jpg\" alt=\"Image 2: capacitive sensing for robot-assisted dressing\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A robot uses a novel capacitive sensor to provide dressing assistance to an able-bodied participant .</div>\n<div class=\"imageCredit\">Georgia Institute of Technology</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Charles&nbsp;C&nbsp;Kemp</div>\n<div class=\"imageTitle\">Image 2: capacitive sensing for robot-assisted dressing</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652894582614_Screenshot-from-2019-10-13-15-33-14--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652894582614_Screenshot-from-2019-10-13-15-33-14--rgov-800width.jpg\" title=\"Image 3: learning from physics simulations\"><img src=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652894582614_Screenshot-from-2019-10-13-15-33-14--rgov-66x44.jpg\" alt=\"Image 3: learning from physics simulations\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Simulated robots learn to provide effective dressing assistance by collaborating with simulated people.</div>\n<div class=\"imageCredit\">Georgia Institute of Technology</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Charles&nbsp;C&nbsp;Kemp</div>\n<div class=\"imageTitle\">Image 3: learning from physics simulations</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652898094896_inferred_pressure--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652898094896_inferred_pressure--rgov-800width.jpg\" title=\"Image 4: robot infers applied pressure\"><img src=\"/por/images/Reports/POR/2022/1514258/1514258_10374771_1652898094896_inferred_pressure--rgov-66x44.jpg\" alt=\"Image 4: robot infers applied pressure\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The robot infers the pressure applied to the person's arm.</div>\n<div class=\"imageCredit\">Georgia Institute of Technology</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Charles&nbsp;C&nbsp;Kemp</div>\n<div class=\"imageTitle\">Image 4: robot infers applied pressure</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nMillions of people require physical assistance on a daily basis due to disabilities resulting from illness, injury, aging, and other causes. Dressing is an important daily task with which many people require physical assistance. For example, more older adults receive physical assistance with dressing than with feeding or walking. \nRobot-assisted dressing has the potential to increase independence and improve quality of life for people with disabilities. However, dressing is a complex task involving physical interactions between a garment, a robot, and a person. The task also entails risk. For example, taut fabrics can apply dangerously high force to a person's body and cut off circulation. \nThis award supported pioneering research on robot-assisted dressing. The team enabled robots to learn how to assist with dressing using computer simulations of robots, people, and clothing (see Image 3). Robots also learned how to assist people in personalized ways by modeling an individual's capabilities. For example, a robot successfully provided personalized dressing assistance to people with disabilities in a study supported by this award (see Image 1). \nSince dressing involves visual occlusion of the person's body, the team also made novel contributions to robotic sensing. For example, the team enabled robots to use force sensing to infer the pressure applied to the person's body while assisting them (see Image 4). The team also enabled robots to predict the pressure that would result from different actions in order to select better actions and thus provide better assistance. The team also created novel capacitive sensing methods that enable robots to sense the human body without touching it (see Image 2). \nIn addition to these contributions, the team produced peer-reviewed publications, open source software, and open hardware. The award also supported education by inspiring classes, helping with the training of undergraduate and graduate students, and creating compelling demonstrations for local junior high and high school students during tours for National Robotics Week.\n\n \n\n\t\t\t\t\tLast Modified: 05/18/2022\n\n\t\t\t\t\tSubmitted by: Charles C Kemp"
 }
}