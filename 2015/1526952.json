{
 "awd_id": "1526952",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Fundamental Connections in Randomness and Complexity",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2015-08-12",
 "awd_max_amd_letter_date": "2015-08-12",
 "awd_abstract_narration": "Major advances in computer science have come from finding connections between different areas and then exploiting them in nontrivial ways. In this project, the PI seeks new connections and plans to capitalize on these and known connections among several areas of randomness and computational complexity. Computational complexity explores which problems are computationally intractable and why, as well as tradeoffs between computational resources such as time, space, and randomness.\r\n\r\nRandomness is extremely useful in computer science and widely used in practice.  When simulating complex phenomena, such as the weather or the economy, it is standard to include random components.  Computer security is impossible without randomness.  Yet while randomness is provably necessary for computer security, it is not known whether it is provably necessary for algorithms. A major question in computing is to understand the power of randomness and whether it is really necessary for algorithms. Research addressing this question often focuses on two fundamental objects: pseudorandom generators and randomness extractors. A pseudorandom generator is a deterministic algorithm that expands a small number of random bits into a large number of pseudorandom bits, where algorithms using these pseudorandom bits behave similarly to algorithms using perfectly random bits. A randomness extractor is a deterministic algorithm that converts a large amount of low-quality randomness into a smaller, but still large, amount of high-quality randomness.\r\n\r\nThis project has several themes. How do pseudorandom generators and randomness extractors relate to lower bounds for computational problems? How do pseudorandom generators and randomness extractors relate to cryptography, the mathematical foundations of computer security? How do randomness extractors relate to error-correcting codes, which enable reliable transmission over noisy media? How do codes relate to machine learning? How do pseudorandom generators relate to computational biology? By finding and exploiting these connections, we can greatly advance knowledge in the underlying areas and increase the chances of breakthroughs. Several application areas are important to society. Understanding molecular structure could impact biology, medicine, and drug design. Improvements in cryptography could lead to improved computer security. Machine learning addresses the omnipresent big data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Zuckerman",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "David I Zuckerman",
   "pi_email_addr": "diz@utexas.edu",
   "nsf_id": "000181714",
   "pi_start_date": "2015-08-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "2317 Speedway, Stop D9500",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121757",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Randomness is amazingly useful in many areas.&nbsp; For example, the gold standard for medical research involves randomized controlled trials.</p>\n<p>My research focuses on randomness, computing, and algorithms, which are like recipes for computing something. Many randomized algorithms are faster than algorithms that don't use randomness. Scientists use random numbers to simulate complex systems such as the climate. Computer security is impossible without random numbers.</p>\n<p>On the other hand, generating random numbers seems difficult for a computer.&nbsp; This is because a computer appears the opposite of random, executing one instruction after the other in a predetermined way.&nbsp; For that reason, computers use ad hoc sources of randomness, such as timing intervals between keystrokes. However, such ad hoc random sources are really only a little bit random.&nbsp; It is therefore essential to improve the quality of the random source.&nbsp; Unfortunately, this is impossible with just one, general, low-quality random source.</p>\n<p>For this NSF project, my former PhD student Eshan Chattopadhyay and I solved a longstanding open problem by introducing an algorithm that combines two low-quality random sources to create one high-quality random source.&nbsp; Previous attempts needed at least one of the two input sources to be of moderately high-quality.&nbsp; Our algorithm, called a two-source extractor, allows us to produce high-quality randomness starting with exponentially lower quality random numbers than known previously.</p>\n<p>Theoreticians consider our two-source extractor a breakthrough. It also gives a major improvement to an important mathematical problem in a field called Ramsey Theory, which seeks to find structure even in random-looking objects.</p>\n<p>It will take time before our extractor is made practical, but already several researchers have built and improved on our ideas.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/26/2018<br>\n\t\t\t\t\tModified by: David&nbsp;I&nbsp;Zuckerman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nRandomness is amazingly useful in many areas.  For example, the gold standard for medical research involves randomized controlled trials.\n\nMy research focuses on randomness, computing, and algorithms, which are like recipes for computing something. Many randomized algorithms are faster than algorithms that don't use randomness. Scientists use random numbers to simulate complex systems such as the climate. Computer security is impossible without random numbers.\n\nOn the other hand, generating random numbers seems difficult for a computer.  This is because a computer appears the opposite of random, executing one instruction after the other in a predetermined way.  For that reason, computers use ad hoc sources of randomness, such as timing intervals between keystrokes. However, such ad hoc random sources are really only a little bit random.  It is therefore essential to improve the quality of the random source.  Unfortunately, this is impossible with just one, general, low-quality random source.\n\nFor this NSF project, my former PhD student Eshan Chattopadhyay and I solved a longstanding open problem by introducing an algorithm that combines two low-quality random sources to create one high-quality random source.  Previous attempts needed at least one of the two input sources to be of moderately high-quality.  Our algorithm, called a two-source extractor, allows us to produce high-quality randomness starting with exponentially lower quality random numbers than known previously.\n\nTheoreticians consider our two-source extractor a breakthrough. It also gives a major improvement to an important mathematical problem in a field called Ramsey Theory, which seeks to find structure even in random-looking objects.\n\nIt will take time before our extractor is made practical, but already several researchers have built and improved on our ideas.\n\n \n\n\t\t\t\t\tLast Modified: 11/26/2018\n\n\t\t\t\t\tSubmitted by: David I Zuckerman"
 }
}