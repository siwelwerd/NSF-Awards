{
 "awd_id": "1527536",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Linking and Resolving Entities in Big Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 499984.0,
 "awd_amount": 499984.0,
 "awd_min_amd_letter_date": "2015-08-19",
 "awd_max_amd_letter_date": "2015-08-19",
 "awd_abstract_narration": "This project will explore the challenge of cleaning data in the context of analysis pipelines over big data. Data cleaning has traditionally been designed to improve data quality in ETL systems where enterprise data is collected, prepared, staged, transformed, and loaded into a data warehouse to support offline data analysis. In the era of big data, such back-end processes are quickly giving way to interactive exploratory data analysis where analysts immerse themselves in data (possibly collected from heterogeneous data sources) in order to drive online (near-) real-time decision making. Existing systems do not scale to the volume, velocity, or the variability of the dynamically generated data (e.g., social media streams) and the offline architecture is unsuited for the online real-time nature of analysis. The market is abuzz with innovations in data transformation technologies, e.g., TriFacta allows analysts to visually manipulate data to generate complex analytical transformations and Data Tamer is exploring scalable data curation from diverse sources. Data quality (and hence data cleaning technologies) remain at the core of big-data analytics. Many popular media (as well as academic) articles have highlighted challenges such as entity linking and resolution as among the most important and immediate roadblocks for big data analytics. The key insight on which this project is based is that data cleaning to support analytics over big data is not simply a matter of scaling up known approaches to larger data sets by exploiting more hardware. While scale up is important, big data analytics in streaming, real-time, and interactive settings requires a paradigm shift in how data cleaning is performed. This project will significantly impact and change the modern practices of data cleaning and the way cleaning is integrated in the Big Data analysis pipeline and will explore broader impact through: (a) technology transfer opportunities with a relevant industrial partner whose existing products could benefit from the proposed research; and (b) open source effort in the context of the ongoing social media analytics system (SoDAS), currently under development, in which the proposed research algorithms will be integrated.\r\n\r\nThis research will explore two new innovations that will help advance data cleaning to enable Big Data analysis. The first innovation explores a progressive approach to entity resolution to support progressive analysis. The research will explore an approach where progressiveness is pervasive spanning all the phases of the cleaning process especially in scenarios when cleaning is based on complex logic possibly requiring dynamic acquisition of additional contextual information. The second innovation is the analysis-aware data cleaning that is developed for structured queries (e.g., Hive and SQL) for both one-time and continuous query scenarios that are issued on top of static and streaming data. The project will address these methodologies at the higher conceptual level as well as implement them on modern highly-parallel computing platforms and frameworks that run on a cluster of machines. The project will exploit two concrete contexts to guide the research exploration: (a) supporting analytical queries over structured web data sources such as fusion tables; and (b) online analysis of social media data. These application contexts will serve as vehicles for testing and demonstrating the research. The planned research, system development, and educational activities (e.g., curriculum changes to incorporate projects related to big data and data quality in the CS curriculum at UCI) will significantly enhance the educational experience of students, preparing them for a brighter future in the today?s knowledge driven society. More information about the project can be found at http://sherlock.ics.uci.edu.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sharad",
   "pi_last_name": "Mehrotra",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sharad Mehrotra",
   "pi_email_addr": "sharad@ics.uci.edu",
   "nsf_id": "000491471",
   "pi_start_date": "2015-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "Bren Hall, Room 2082",
  "perf_city_name": "Irvine",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926972725",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 499984.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-064760e8-7fff-21bb-5de7-9c87b4025be8\">\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\"><span>&nbsp;</span><span>The goal of this project was to explore the challenge of cleaning data in the context of analysis pipelines over big data. Data cleaning has traditionally been designed to improve data quality in extract-transforn-load&nbsp; (ETL) systems wherein enterprise data is collected, prepared, staged, transformed, and loaded into a data warehouse to support online data analysis. In the era of big data, such back-end processes are quickly giving way to interactive exploratory analysis wherein analysts immerse themselves in data (possibly collected from heterogeneous data sources) in order to drive online (near-) real-time decision making. Existing systems do not scale to the volume, velocity, or the variability of the dynamically generated data (e.g., social media streams) and the existing architecture is unsuited for the online real-time nature of analysis.</span></p>\n<p dir=\"ltr\"><span>The project led to a large number of significant outcomes.</span></p>\n<p dir=\"ltr\"><span>First, Yasser Altowin wrote a dissertation on a new paradigm for progressive&nbsp; entity resolution (ER).. Progressive ER aims to resolve the dataset in such a way that maximizes the rate at which the data quality improves. This approach can help in substantially reducing the resolution cost since the ER process can be prematurely terminated whenever a satisfying level of quality is achieved. In this thesis, he explored two aspects of the ER problem and proposed a progressive approach to each of them. In particular, he first explored a progressive approach to relational ER, wherein the input dataset consists of multiple entity-sets and relationships among them. He then developed&nbsp; a parallel approach to entity resolution using the MapReduce (MR) framework.&nbsp;</span></p>\n<p dir=\"ltr\"><span>In an ongoing dissertation Yiming Lin explored approaches to&nbsp; cleaning WiFi connectivity data to locate users to semantic indoor locations such as buildings, regions, rooms. WiFi connectivity data consists of sporadic connections between devices and nearby WiFi access points (APs), each of which may cover a relatively large area within a building. &nbsp; He designed a system, entitled semantic LOCATion cleanER (LOCATER), that postulates semantic localization as a series of data cleaning tasks&nbsp; which are computed progressively.</span></p>\n<p dir=\"ltr\"><span>The project further resulted in new approaches to&nbsp; integrating data cleaning/enrichment with query processing that supports progressive query results and a system implementation that demonstrates efficacy of the techniques developed (ongoing Ph.D. thesis of Dhrubajyoti Ghosh).&nbsp; The project also resulted in a design of a new class of data management system optimized for sensor data management and smart space applications that supports autonomous enrichment/cleaning of sensor data (ongoing Ph.D. thesis of Peeyush Gupta). &nbsp; Finally, a new framework for Top-k queries that require expensive entity linking operations got designed (ongoing Ph.D. dissertation Abdulrahman Alsaudi). The </span><span>framework optimizes the&nbsp; set of mentions that need to be linked&nbsp; in order to answer a top-k query in the least amount of time both accurately, as well as, approximately with probabilistic bounds on quality.&nbsp;</span></p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/17/2020<br>\n\t\t\t\t\tModified by: Sharad&nbsp;Mehrotra</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n \n The goal of this project was to explore the challenge of cleaning data in the context of analysis pipelines over big data. Data cleaning has traditionally been designed to improve data quality in extract-transforn-load  (ETL) systems wherein enterprise data is collected, prepared, staged, transformed, and loaded into a data warehouse to support online data analysis. In the era of big data, such back-end processes are quickly giving way to interactive exploratory analysis wherein analysts immerse themselves in data (possibly collected from heterogeneous data sources) in order to drive online (near-) real-time decision making. Existing systems do not scale to the volume, velocity, or the variability of the dynamically generated data (e.g., social media streams) and the existing architecture is unsuited for the online real-time nature of analysis.\nThe project led to a large number of significant outcomes.\nFirst, Yasser Altowin wrote a dissertation on a new paradigm for progressive  entity resolution (ER).. Progressive ER aims to resolve the dataset in such a way that maximizes the rate at which the data quality improves. This approach can help in substantially reducing the resolution cost since the ER process can be prematurely terminated whenever a satisfying level of quality is achieved. In this thesis, he explored two aspects of the ER problem and proposed a progressive approach to each of them. In particular, he first explored a progressive approach to relational ER, wherein the input dataset consists of multiple entity-sets and relationships among them. He then developed  a parallel approach to entity resolution using the MapReduce (MR) framework. \nIn an ongoing dissertation Yiming Lin explored approaches to  cleaning WiFi connectivity data to locate users to semantic indoor locations such as buildings, regions, rooms. WiFi connectivity data consists of sporadic connections between devices and nearby WiFi access points (APs), each of which may cover a relatively large area within a building.   He designed a system, entitled semantic LOCATion cleanER (LOCATER), that postulates semantic localization as a series of data cleaning tasks  which are computed progressively.\nThe project further resulted in new approaches to  integrating data cleaning/enrichment with query processing that supports progressive query results and a system implementation that demonstrates efficacy of the techniques developed (ongoing Ph.D. thesis of Dhrubajyoti Ghosh).  The project also resulted in a design of a new class of data management system optimized for sensor data management and smart space applications that supports autonomous enrichment/cleaning of sensor data (ongoing Ph.D. thesis of Peeyush Gupta).   Finally, a new framework for Top-k queries that require expensive entity linking operations got designed (ongoing Ph.D. dissertation Abdulrahman Alsaudi). The framework optimizes the  set of mentions that need to be linked  in order to answer a top-k query in the least amount of time both accurately, as well as, approximately with probabilistic bounds on quality. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\t\t\t\t\tLast Modified: 12/17/2020\n\n\t\t\t\t\tSubmitted by: Sharad Mehrotra"
 }
}