{
 "awd_id": "1548409",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Human centered robotic system design",
 "cfda_num": "47.041",
 "org_code": "07020000",
 "po_phone": "7032922191",
 "po_email": "asimonia@nsf.gov",
 "po_sign_block_name": "Aleksandr Simonian",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2015-09-02",
 "awd_max_amd_letter_date": "2015-09-02",
 "awd_abstract_narration": "1548409(Hovakimyan)\r\n\r\nTechnological advances today have the potential to transform the life of everybody within the next 20-30 years in a dramatic way. The society of non-distant future assumes co-existence of humans, pilotless planes, driverless cars, and robots, in which machines and human beings share the airspace, the roads, and populate indoor environments. This idea poses significant challenges, falling within the scope of engineering, psychology, social science, politics, ethics, and economics. Inspired by this vision, the project proposes to lay down the foundations for the world of the future, in which cars, drones, and robotic machines autonomously co-habit and work with humans. Hence, it is important to build a scientific community, where psychologists and engineers work together in order to develop socially trustable autonomous mobile robots. More specifically, we propose to formulate rigorously relevant scientific and human centered psychological concerns (namely, actual safety, humans' perceived safety, and humans' comfort) that arise in such systems, where humans and robots are required to safely interact in a shared space. The overarching objective is to provide a general design framework and control architecture for human interaction with autonomous robots that is flexible and capable of safe operations.\r\n\r\nThis proposal focuses on the problem of human centered robotic design and control. The goal of this project is to provide the foundations to address human related concerns that arise in multiple human-robot systems, where robots have to perform tasks in the presence of (and in cooperation with) humans. In particular, this proposal targets the fundamental understanding of two issues that are crucial in the integration of robotic systems into real-life human populated environments: first, how humans perceive autonomous mobile robots as a function of robots' appearance and behavior; second, how to design and control mobile robots so that to improve the level of comfort and perceived safety of the people present in the environment. The research here assumes a collaborative study, involving Mechanical Engineering, Psychology and Computer Science Departments at University of Illinois at Urbana-Champaign, in which human perception of robotic appearance and behavior is the main subject that has to define what it means for a robot to be socially accepted and to generate empathy between humans and robotic machines. Social etiquette in human-robot systems will be sought through the study of human behavior. At the initial stage of this ambitious research project, the virtual reality cave at Beckman Institute and the motion capture suite at CSL Intelligent Robotic Lab will serve as testing and validation environment to derive robots' design features and behavioral protocols toward the development of these robots. The outcome of this first step will provide the basis for further research in the field and will contribute to the sociotechnical society of the future.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CBET",
 "org_div_long_name": "Division of Chemical, Bioengineering, Environmental, and Transport Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Naira",
   "pi_last_name": "Hovakimyan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Naira Hovakimyan",
   "pi_email_addr": "nhovakim@illinois.edu",
   "nsf_id": "000494200",
   "pi_start_date": "2015-09-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ranxiao",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ranxiao Wang",
   "pi_email_addr": "wang18@illinois.edu",
   "nsf_id": "000166426",
   "pi_start_date": "2015-09-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Camille",
   "pi_last_name": "Goudeseune",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Camille M Goudeseune",
   "pi_email_addr": "cog@uiuc.edu",
   "nsf_id": "000431398",
   "pi_start_date": "2015-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618207473",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "763300",
   "pgm_ele_name": "EFRI Research Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "010E",
   "pgm_ref_txt": "DISABILITY RES & HOMECARE TECH"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project produced algorithms for modeling human's safety perception in close proximity to flying vehicles. Sixty-two subjects have been analyzed in Virtual Reality environment using three sensors: skin conductance, heart rate and head tilt. The collected data was analyzed with methods from machine learning to build a safety perception model. This model was subsequently used for optimal path generation for flying vehicles. The project has led to a number of publications in conferences and archival&nbsp; journals.</p>\n<p>&nbsp;</p>\n<p>Here are the key accomplishments:</p>\n<p><br />(1) A subject test was designed and performed to study human's safety perception of the flying robot using a virtual reality environment, Figure 2.</p>\n<p><br />(2) The test data were analyzed;&nbsp; we found that there is significant difference in human's arousal sensor signal for varying speed, height, and the sound level of the flying robot.&nbsp;&nbsp;</p>\n<p><br />(3) The arousal model as a function robot's position and velocity was estimated using a hidden Markov model to disregard irrelevant arousal signals, Figure 5.</p>\n<p><br />(4) Optimal trajectory generation uses the humans' arousal model to consider perceived safety, while reaching to the target position, Figure 4. We implemented the optimal trajectories in VR environment to test whether the optimal trajectories improve the acceptance of the human users.</p>\n<p><br />(5) A flying manipulator was built and configured in our lab, Figure 1.</p>\n<p><br />(6) A pick and place task was performed in the lab with a flying manipulator, Figure 3.</p>\n<p>&nbsp;</p>\n<p>Finally, the results of this project were used for writing a new NRI proposal on drone delivery networks, which is recommended for award. This project is NSF #</p>\n<table id=\"gappsDashboardDisplayTable\" cellspacing=\"0\" cellpadding=\"0\">\n<tbody>\n<tr class=\"odd\">\n<td class=\"hyphenate\"><a href=\"desktop?_nfpb=true&amp;_windowLabel=gappsDashboard_1&amp;wsrp-urlType=blockingAction&amp;wsrp-url=&amp;wsrp-requiresRewrite=&amp;wsrp-navigationalState=eJyLL07OL0i1zc1PSQ0GsVLU0nPykxJzbAF*5QmO&amp;wsrp-interactionState=wlpgappsDashboard_1_gappsDetailRequest.grantsGovId%3D%26wlpgappsDashboard_1_gappsDetailRequest.agencyId%3DNSF%26wlpgappsDashboard_1_gappsDetailRequest.applicationId%3D1830639%26wlpgappsDashboard_1_action%3DselectGrantApplicationAction&amp;wsrp-mode=&amp;wsrp-windowState=\">1830639</a><br /><br /></td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/02/2018<br>\n\t\t\t\t\tModified by: Naira&nbsp;Hovakimyan</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535925681014_FlyingRobot_with_RobotArm--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535925681014_FlyingRobot_with_RobotArm--rgov-800width.jpg\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535925681014_FlyingRobot_with_RobotArm--rgov-66x44.jpg\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Flying Manipulator</div>\n<div class=\"imageCredit\">Gabriel Haberfeld Barsi</div>\n<div class=\"imagePermisssions\">Royalty-free (restricted use - cannot be shared)</div>\n<div class=\"imageSubmitted\">Naira&nbsp;Hovakimyan</div>\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535925888912_Fig_VR_Experiment--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535925888912_Fig_VR_Experiment--rgov-800width.jpg\" title=\"Figure 2\"><img src=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535925888912_Fig_VR_Experiment--rgov-66x44.jpg\" alt=\"Figure 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Virtual Reality setup</div>\n<div class=\"imageCredit\">Chris Widdowson</div>\n<div class=\"imagePermisssions\">Royalty-free (restricted use - cannot be shared)</div>\n<div class=\"imageSubmitted\">Naira&nbsp;Hovakimyan</div>\n<div class=\"imageTitle\">Figure 2</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535926011036_FlyingRobot_in_IRL--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535926011036_FlyingRobot_in_IRL--rgov-800width.jpg\" title=\"Figure 3\"><img src=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535926011036_FlyingRobot_in_IRL--rgov-66x44.jpg\" alt=\"Figure 3\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Flying manipulator performing pick and place task</div>\n<div class=\"imageCredit\">Gabriel Barsi</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Naira&nbsp;Hovakimyan</div>\n<div class=\"imageTitle\">Figure 3</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535926932052_Fig_Optimal_Paths_Both--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535926932052_Fig_Optimal_Paths_Both--rgov-800width.jpg\" title=\"Figure 4\"><img src=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535926932052_Fig_Optimal_Paths_Both--rgov-66x44.jpg\" alt=\"Figure 4\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Optimal paths obtained with different methods.</div>\n<div class=\"imageCredit\">Hyung-Jin Yoon</div>\n<div class=\"imagePermisssions\">Royalty-free (restricted use - cannot be shared)</div>\n<div class=\"imageSubmitted\">Naira&nbsp;Hovakimyan</div>\n<div class=\"imageTitle\">Figure 4</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535927222526_Fig_Prediction_V2copy--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535927222526_Fig_Prediction_V2copy--rgov-800width.jpg\" title=\"Figure 5\"><img src=\"/por/images/Reports/POR/2018/1548409/1548409_10395075_1535927222526_Fig_Prediction_V2copy--rgov-66x44.jpg\" alt=\"Figure 5\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Prediction of human arousal</div>\n<div class=\"imageCredit\">Hyung-Jin Yoon</div>\n<div class=\"imagePermisssions\">Royalty-free (restricted use - cannot be shared)</div>\n<div class=\"imageSubmitted\">Naira&nbsp;Hovakimyan</div>\n<div class=\"imageTitle\">Figure 5</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe project produced algorithms for modeling human's safety perception in close proximity to flying vehicles. Sixty-two subjects have been analyzed in Virtual Reality environment using three sensors: skin conductance, heart rate and head tilt. The collected data was analyzed with methods from machine learning to build a safety perception model. This model was subsequently used for optimal path generation for flying vehicles. The project has led to a number of publications in conferences and archival  journals.\n\n \n\nHere are the key accomplishments:\n\n\n(1) A subject test was designed and performed to study human's safety perception of the flying robot using a virtual reality environment, Figure 2.\n\n\n(2) The test data were analyzed;  we found that there is significant difference in human's arousal sensor signal for varying speed, height, and the sound level of the flying robot.  \n\n\n(3) The arousal model as a function robot's position and velocity was estimated using a hidden Markov model to disregard irrelevant arousal signals, Figure 5.\n\n\n(4) Optimal trajectory generation uses the humans' arousal model to consider perceived safety, while reaching to the target position, Figure 4. We implemented the optimal trajectories in VR environment to test whether the optimal trajectories improve the acceptance of the human users.\n\n\n(5) A flying manipulator was built and configured in our lab, Figure 1.\n\n\n(6) A pick and place task was performed in the lab with a flying manipulator, Figure 3.\n\n \n\nFinally, the results of this project were used for writing a new NRI proposal on drone delivery networks, which is recommended for award. This project is NSF #\n\n\n\n1830639\n\n\n\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 09/02/2018\n\n\t\t\t\t\tSubmitted by: Naira Hovakimyan"
 }
}