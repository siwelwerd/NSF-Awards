{
 "awd_id": "1514495",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Fellowship Award",
 "awd_titl_txt": "Integrating New Technologies to Assess Visual and Attentional Influences on Movement and Imitative Behavior in Autism",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Josie S. Welkom",
 "awd_eff_date": "2015-06-15",
 "awd_exp_date": "2018-05-31",
 "tot_intn_awd_amt": 52020.0,
 "awd_amount": 52020.0,
 "awd_min_amd_letter_date": "2015-06-09",
 "awd_max_amd_letter_date": "2015-06-09",
 "awd_abstract_narration": "This award supports a rising interdisciplinary scholar investigating visuomotor integration in typical development and Autism Spectrum Disorder (ASD). Effective navigation and action requires accurate visuomotor integration, or the use of visual information to guide movement. Visuomotor integration also requires attentional filtering to ensure that relevant information is processed, while irrelevant information is disregarded or suppressed. Few studies have examined visuomotor integration in naturalistic settings that allow measurement of both full-body motion and eye movement. The proposed project combines new technologies to investigate visuomotor integration in typical development and Autism Spectrum Disorder (ASD). ASD is a clinical population with known visual and motor differences in the brain regions that support these systems and in functional performance. At present, most treatment approaches focus on social communication in a broad, qualitative manner without specific attention to the role of visual and motor functioning. This project quantifies differences in eye and body movements during imitative gesturing with a robot partner, as well as during other movements such as walking. By studying how visuomotor integration impacts movement and interaction with real and virtual objects, researchers gain a better understanding of the impact this skill has on higher-order features of ASD. This, in turn, aids in developing and delivering more targeted, evidence-based treatments. Quantitative approaches to measuring visuomotor integration skills also serve as an effective biomarker of ASD for early diagnosis, since visual and motor skills can be precisely measured earlier than social communication skills.\r\n\t\r\nRecent innovations in eye tracking, robotics, and virtual reality have yielded technologies that can be integrated to study the interaction between visual, motor, and attentional systems in real-time. This project investigates visual, motor, and attentional processes in ASD and typical development to determine their relative contributions to accurate perception and action using virtual environments and human-robot interaction tasks that test visual and motor responses to object motion and imitative gesturing. The project aims include refining software used to analyze motion and eye data together by calculating gaze vector in a manner that accounts for head and body movement. This enables researchers to examine the strategies used by individuals with and without ASD when locating and tracking moving objects (e.g., preference for moving the head versus shifting gaze) or gestures (e.g., waving hand). In addition to head-eye integration, the project aims include measuring full-body motor responses to object motion. In order to comprehensively investigate imitative gesturing in the human-robot interaction tasks, the researchers use a new technique, Dynamic Time Warping, which allows examination of both spatial and temporal synchrony between a gesture modeled by the robot and the imitative movement generated by a participant. This novel approach to analysis may reveal important biomarkers of ASD related to visuomotor integration, which would not be evident in studies that examine only the spatial properties of imitative gesturing. The proposed project also advances methodological approaches via the development of new tools for data collection and analysis that are specifically suited to investigating perception and action in a naturalistic environment.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Haylie",
   "pi_last_name": "Miller",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Haylie L Miller",
   "pi_email_addr": "millerhl@umich.edu",
   "nsf_id": "000656081",
   "pi_start_date": "2015-06-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Texas Health Science Center at Fort Worth",
  "inst_street_address": "3500 CAMP BOWIE BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "FORT WORTH",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "8177355073",
  "inst_zip_code": "761072644",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "TX12",
  "org_lgl_bus_name": "THE UNIVERSITY OF NORTH TEXAS HEALTH SCIENCE CENTER AT FORT WORTH",
  "org_prnt_uei_num": "",
  "org_uei_num": "JE8AKPCR2KA4"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Texas Health Science Center",
  "perf_str_addr": "3500 Camp Bowie Blvd.",
  "perf_city_name": "Fort Worth",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "761072699",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "TX12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "820900",
   "pgm_ele_name": "SPRF-IBSS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 52020.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>For a person to effectively engage with his or her environment, s/he must (1) identify and look at relevant objects, people, or locations in the world, (2) correctly and efficiently process information about the shape, size, location, and motion of those targets, and (3) integrate that information into a behavioral or motor response.&nbsp; This process, visuomotor integration, is the means by which we use visual information to guide movement. In some clinical populations, like Autism Spectrum Disorder (ASD), visual behaviors and motor skills have been shown to differ from typical development. Not many studies have looked at vision and movement together in ASD, especially in settings that allow measurement of both full-body motion and eye movement. In this project, we combined three technologies (virtual reality, eye-tracking, motion-capture, robotics) to investigate visuomotor integration in typical development and ASD.</p>\n<p>Intellectual Merit:&nbsp;</p>\n<p>We set out to design new tasks that could be used to test visuomotor integration in real-time. We wanted these tasks to be engaging and naturalistic in terms of their demands, but we also wanted the results to be quantifiable. To achieve this goal, we used a high-resolution full-body motion capture system that allowed us to capture the precise position of a participant's body. We used integrated motion-capture and virtual reality so that participants could be immersed in the tasks, playing them like life-sized video games. We also designed the tasks with traditional eye-tracking literature in mind, with varying degrees of visual complexity and difficulty. We effectively developed and deployed a suite of 13 visuomotor integration tasks that allowed us to monitor, precisely and in real-time, where participants were looking, and how that related to their motor responses to tasks in the virtual environent. We also refined and deployed a protocol for monitoring, precisely and in real-time, where participants were looking and how that related to their imitative gesturing ability during interaction with a robot partner.&nbsp;</p>\n<p>Our team collaborated with external partners in the fields of robotics, computer science, and engineering to integrate data from the motion-capture, eye-tracking, and virtual reality systems. Integration of these data is a novel approach to analysis that allows us to examine spatial and temporal synchrony between a target (e.g., an object in the virtual environment or a movement generated by the robot) and the eye and body movements made by a participant in response.</p>\n<p>Broader Impacts:</p>\n<p>Over 30 undergraduate and graduate students at UNTHSC, UT Arlington, and University of Louisville have been involved in data collection, data processing/analysis, and dissemination of preliminary results from this project over the life of the award. Many of these students were from underrepresented minority groups or low-resource communities. Through observation and hands-on experience, students received interdisciplinary training and exposure to new STEM fields outside of their primary discipline (e.g., robotics students learning about ASD; physical therapy students learning about eye-tracking). A significant number of these students (specifically, those training to be physical therapists, public health professionals, and physicians) were also trained in best practices related to advocacy and patient-centered care, and on the specific visuomotor issues inherent to ASD.</p>\n<p>Preliminary results from this project have been disseminated through a number of avenues, including cross-disciplinary presentations at the International Society for Posture &amp; Gait Research, the International Conference on Virtual Rehabilitation, the International Meeting for Autism Research, the annual meeting of the Society for Research in Child Development, and the Combined Sections Meeting of the Association for Physical Therapy. Preliminary results have also been disseminated through publications in journals such as IEEE Proceedings of ICVR, Proceedings of SPIE; and most recently, Gait &amp; Posture. In addition, Drs. Miller and Bugnariu published a related paper on the role of level of immersion in effectiveness of ASD interventions.</p>\n<p>Our findings to date suggest that individuals with ASD rely heavily on visual information to generate motor responses, but that their eye movements are not accurate or efficient, thus introducing error into the corresponding body movements.&nbsp;Analysis of the data generated from this project is ongoing, and is expected to yield several additional publications and key findings related to ASD and visuomotor integration.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/30/2018<br>\n\t\t\t\t\tModified by: Haylie&nbsp;Miller</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1514495/1514495_10368454_1535638803096_EYETRACKER001--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1514495/1514495_10368454_1535638803096_EYETRACKER001--rgov-800width.jpg\" title=\"Child Participating in Visuomotor Integration Study\"><img src=\"/por/images/Reports/POR/2018/1514495/1514495_10368454_1535638803096_EYETRACKER001--rgov-66x44.jpg\" alt=\"Child Participating in Visuomotor Integration Study\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This image shows a child participating in a visuomotor integration task in the Human Movement Performance Laboratory at UNT Health Science Center in Fort Worth, TX, USA. The child is wearing eye-tracking glasses and markers for full-body motion-capture.</div>\n<div class=\"imageCredit\">Jill Johnson, UNTHSC</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Haylie&nbsp;Miller</div>\n<div class=\"imageTitle\">Child Participating in Visuomotor Integration Study</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1514495/1514495_10368454_1535639409671_zeno_subject3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1514495/1514495_10368454_1535639409671_zeno_subject3--rgov-800width.jpg\" title=\"Body Models for Robot Interaction Phase of Visuomotor Integration Study\"><img src=\"/por/images/Reports/POR/2018/1514495/1514495_10368454_1535639409671_zeno_subject3--rgov-66x44.jpg\" alt=\"Body Models for Robot Interaction Phase of Visuomotor Integration Study\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This image shows two body models generated in the Cortex (Motion Analysis Corp.) software suite from the motion-capture system. These models were used to track the head, trunk, pelvis, and limbs of the participant, and corresponding head, trunk, and arm segments on Zeno the robot (Hanson Robokind).</div>\n<div class=\"imageCredit\">Haylie Miller, UNTHSC</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Haylie&nbsp;Miller</div>\n<div class=\"imageTitle\">Body Models for Robot Interaction Phase of Visuomotor Integration Study</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1514495/1514495_10368454_1535639753021_Zeno_Front.Side_Markers--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1514495/1514495_10368454_1535639753021_Zeno_Front.Side_Markers--rgov-800width.jpg\" title=\"Robot used in Visuomotor Integration Study\"><img src=\"/por/images/Reports/POR/2018/1514495/1514495_10368454_1535639753021_Zeno_Front.Side_Markers--rgov-66x44.jpg\" alt=\"Robot used in Visuomotor Integration Study\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This image shows Zeno the robot (Hanson Robokind) used in the visuomotor integration study. This robot is instrumented with markers in locations similar to those on the human participant. Markers were used to track joint angles during imitative gesturing.</div>\n<div class=\"imageCredit\">Haylie Miller</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Haylie&nbsp;Miller</div>\n<div class=\"imageTitle\">Robot used in Visuomotor Integration Study</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nFor a person to effectively engage with his or her environment, s/he must (1) identify and look at relevant objects, people, or locations in the world, (2) correctly and efficiently process information about the shape, size, location, and motion of those targets, and (3) integrate that information into a behavioral or motor response.  This process, visuomotor integration, is the means by which we use visual information to guide movement. In some clinical populations, like Autism Spectrum Disorder (ASD), visual behaviors and motor skills have been shown to differ from typical development. Not many studies have looked at vision and movement together in ASD, especially in settings that allow measurement of both full-body motion and eye movement. In this project, we combined three technologies (virtual reality, eye-tracking, motion-capture, robotics) to investigate visuomotor integration in typical development and ASD.\n\nIntellectual Merit: \n\nWe set out to design new tasks that could be used to test visuomotor integration in real-time. We wanted these tasks to be engaging and naturalistic in terms of their demands, but we also wanted the results to be quantifiable. To achieve this goal, we used a high-resolution full-body motion capture system that allowed us to capture the precise position of a participant's body. We used integrated motion-capture and virtual reality so that participants could be immersed in the tasks, playing them like life-sized video games. We also designed the tasks with traditional eye-tracking literature in mind, with varying degrees of visual complexity and difficulty. We effectively developed and deployed a suite of 13 visuomotor integration tasks that allowed us to monitor, precisely and in real-time, where participants were looking, and how that related to their motor responses to tasks in the virtual environent. We also refined and deployed a protocol for monitoring, precisely and in real-time, where participants were looking and how that related to their imitative gesturing ability during interaction with a robot partner. \n\nOur team collaborated with external partners in the fields of robotics, computer science, and engineering to integrate data from the motion-capture, eye-tracking, and virtual reality systems. Integration of these data is a novel approach to analysis that allows us to examine spatial and temporal synchrony between a target (e.g., an object in the virtual environment or a movement generated by the robot) and the eye and body movements made by a participant in response.\n\nBroader Impacts:\n\nOver 30 undergraduate and graduate students at UNTHSC, UT Arlington, and University of Louisville have been involved in data collection, data processing/analysis, and dissemination of preliminary results from this project over the life of the award. Many of these students were from underrepresented minority groups or low-resource communities. Through observation and hands-on experience, students received interdisciplinary training and exposure to new STEM fields outside of their primary discipline (e.g., robotics students learning about ASD; physical therapy students learning about eye-tracking). A significant number of these students (specifically, those training to be physical therapists, public health professionals, and physicians) were also trained in best practices related to advocacy and patient-centered care, and on the specific visuomotor issues inherent to ASD.\n\nPreliminary results from this project have been disseminated through a number of avenues, including cross-disciplinary presentations at the International Society for Posture &amp; Gait Research, the International Conference on Virtual Rehabilitation, the International Meeting for Autism Research, the annual meeting of the Society for Research in Child Development, and the Combined Sections Meeting of the Association for Physical Therapy. Preliminary results have also been disseminated through publications in journals such as IEEE Proceedings of ICVR, Proceedings of SPIE; and most recently, Gait &amp; Posture. In addition, Drs. Miller and Bugnariu published a related paper on the role of level of immersion in effectiveness of ASD interventions.\n\nOur findings to date suggest that individuals with ASD rely heavily on visual information to generate motor responses, but that their eye movements are not accurate or efficient, thus introducing error into the corresponding body movements. Analysis of the data generated from this project is ongoing, and is expected to yield several additional publications and key findings related to ASD and visuomotor integration. \n\n \n\n\t\t\t\t\tLast Modified: 08/30/2018\n\n\t\t\t\t\tSubmitted by: Haylie Miller"
 }
}