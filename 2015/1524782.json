{
 "awd_id": "1524782",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Digitally Mediated Multi-party Communication: Acquisition, Modeling, and Evaluation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 397560.0,
 "awd_amount": 413560.0,
 "awd_min_amd_letter_date": "2015-08-19",
 "awd_max_amd_letter_date": "2016-05-10",
 "awd_abstract_narration": "Online persistent and shared multi-user virtual environments (MUVEs), with thousands or even millions of users, constitute an emerging and rapidly growing field that is likely to dramatically impact higher education in the near future.  Direct player-to-player interaction, and the networks that players develop in the virtual world, are central to the unique experience and success of these MUVEs.  However, despite their increased visual realism, the immersive \"social functionality\" in current MUVEs is still rudimentary at best, since real-world conversations and social interactions have not been mimicked and modeled.  This is because it is technically challenging to extend existing one-to-one conversation modeling approaches to digitally mediated multi-party conversations and interactions in virtual worlds, due to the significant differences in nonverbal behavior and interaction patterns.  The automated generation of digitally mediated multi-party communication and interaction has thus become a major technical barrier that restricts the depth and usefulness of various online virtual worlds and virtual reality applications.  In this research, the PI will tackle this issue by designing new algorithms and systems driven by live speech from users in different locations, which can automatically generate synchronized multi-modal conversational gestures on embodied avatars, including head/eye movement, lip movement, hand gesture, and body posture.  Project outcomes will facilitate the widespread adoption of useful avatar and tele-immersion technology in applications where computer-mediated communication plays a role, including education, commerce, health and engineering.   The PI will make the acquired high-fidelity multi-modal multi-party conversational behavior datasets available to the scientific community at large, so they can be used in future research. \r\n\r\nThis ambitious project will focus on three inter-related research thrusts that are aligned with the PI's research expertise in computer animation, virtual humans, and human computer interaction.  Automated generation of realistic talking avatars based on live speech input alone; the PI will design efficient and automated schemes to generate on-the-fly talking avatars based on live speech input, by fusing established social exchange rules with data-driven statistical modeling.  Automated generation of believable listening avatars with immersive social exchanges; based on in-depth statistical analysis of real life multiparty conversation data, the PI will design data-driven schemes for generating tightly coordinated gazes, head movements, and body posture shifts on listening avatars, as well as social gaze exchanges between listening peers.  Comparative evaluation of the proposed avatar-mediated multi-party conversation and interaction approach in an in-house built research testbed; the robustness and effectiveness of the proposed framework will be evaluated by integrating it into an in-house built research testbed (i.e., a simplified MUVE prototype).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhigang",
   "pi_last_name": "Deng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhigang Deng",
   "pi_email_addr": "zhigang.deng@gmail.com",
   "nsf_id": "000489047",
   "pi_start_date": "2015-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Houston",
  "inst_street_address": "4300 MARTIN LUTHER KING BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "HOUSTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7137435773",
  "inst_zip_code": "772043067",
  "inst_country_name": "United States",
  "cong_dist_code": "18",
  "st_cong_dist_code": "TX18",
  "org_lgl_bus_name": "UNIVERSITY OF HOUSTON SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "QKWEF8XLMTT3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Houston",
  "perf_str_addr": "4800 Calhoun Road",
  "perf_city_name": "Houston",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "772043010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "18",
  "perf_st_cong_dist": "TX18",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 397560.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As the main research outcomes of this project, the PI and the team have designed the following novel algorithms and systems: (i) to investigate the contribution of eye contact to the identification of the speaker in three-party conversations, a data-driven framework was proposed to model the occurrence of eye contact during uttering and further distinguish the speaker from the listeners. The study also provides fresh quantitative evidence that eye contact provides an objective cue for reliable identification of the speakers in three-party conversations. (ii) to investigate how head motion contributes to the perception of emotion in an utterance, intra-related objective analysis and perceptual experiments were conducted to quantify the link between the perception of emotion and various static/dynamic head movement features. The study shows that humans are unable to reliably perceive emotion from head motion alone, and that humans are sensitive to the static feature (in reference to the averaged up-down rotation angle) and the dynamic features (which reflect the fluidity and speed of movement). (iii) A novel hierarchical method was developed to reconstruct high resolution facial geometry and appearance in real-time by capturing an individual-specific face model with fine-scale details, based on monocular RGB video input. (iv) A novel deep learning based framework was developed to generate realistic three-party head and eye motions based on novel acoustic speech input together with speaker marking (i.e., speaking time for each interlocutor). (v) A novel real-time end-to-end system was developed for facial expression transformation, without the need of any driving source. It can be directly used for transforming the expression of a given monocular face video to a new user-specified expression. (vi) A live speech driven, avatarized, three-party telepresence system was developed and evaluated, through three remote users, embodied as avatars in a shared 3D immersive virtual world, can perform natural three-party telecommunication.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Through the research participation in this project, more than 8 PhD students, postDocs, and &nbsp;undergraduate student have been trained. Most of them have been working in major IT companies and universities in US and around the world. &nbsp;More than 30 peer-reviewed research articles have been published on major journals and conferences in computer graphics and human computer interaction. &nbsp;In addition, many local high school students have been provided summer interns or lab tours in the PI&rsquo;s group.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/07/2020<br>\n\t\t\t\t\tModified by: Zhigang&nbsp;Deng</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAs the main research outcomes of this project, the PI and the team have designed the following novel algorithms and systems: (i) to investigate the contribution of eye contact to the identification of the speaker in three-party conversations, a data-driven framework was proposed to model the occurrence of eye contact during uttering and further distinguish the speaker from the listeners. The study also provides fresh quantitative evidence that eye contact provides an objective cue for reliable identification of the speakers in three-party conversations. (ii) to investigate how head motion contributes to the perception of emotion in an utterance, intra-related objective analysis and perceptual experiments were conducted to quantify the link between the perception of emotion and various static/dynamic head movement features. The study shows that humans are unable to reliably perceive emotion from head motion alone, and that humans are sensitive to the static feature (in reference to the averaged up-down rotation angle) and the dynamic features (which reflect the fluidity and speed of movement). (iii) A novel hierarchical method was developed to reconstruct high resolution facial geometry and appearance in real-time by capturing an individual-specific face model with fine-scale details, based on monocular RGB video input. (iv) A novel deep learning based framework was developed to generate realistic three-party head and eye motions based on novel acoustic speech input together with speaker marking (i.e., speaking time for each interlocutor). (v) A novel real-time end-to-end system was developed for facial expression transformation, without the need of any driving source. It can be directly used for transforming the expression of a given monocular face video to a new user-specified expression. (vi) A live speech driven, avatarized, three-party telepresence system was developed and evaluated, through three remote users, embodied as avatars in a shared 3D immersive virtual world, can perform natural three-party telecommunication.\n\n       Through the research participation in this project, more than 8 PhD students, postDocs, and  undergraduate student have been trained. Most of them have been working in major IT companies and universities in US and around the world.  More than 30 peer-reviewed research articles have been published on major journals and conferences in computer graphics and human computer interaction.  In addition, many local high school students have been provided summer interns or lab tours in the PI\u2019s group.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 09/07/2020\n\n\t\t\t\t\tSubmitted by: Zhigang Deng"
 }
}