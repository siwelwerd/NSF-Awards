{
 "awd_id": "1526015",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Rethinking System Software for Overprovisioned, High-Performance Computing Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2015-10-01",
 "awd_exp_date": "2020-09-30",
 "tot_intn_awd_amt": 489996.0,
 "awd_amount": 505882.0,
 "awd_min_amd_letter_date": "2015-08-26",
 "awd_max_amd_letter_date": "2018-07-18",
 "awd_abstract_narration": "Currently, the high-performance computing (HPC) community is focused on achieving exaflop performance, which is about a 30-fold improvement from the performance of the best supercomputer in the world today. Because of practical, financial, and environmental concerns, the Department of Energy is setting a power limit for achieving an exaflop at 20 megawatts.  As today's top machines generally consume between five and 20 megawatts---and yet are an order of magnitude or more away from the exaflop performance target, significant hardware and software advances in HPC systems are necessary.  One way to improve hardware is to use overprovisioned systems, which contain more machines than can be fully powered simultaneously.  While overprovisioned systems have the potential to significantly improve power and performance, software will need to be redesigned to support such systems.\r\n\r\nThe focus of this proposal is to design and implement software infrastructure that will support overprovisioned systems.  The key advance in the infrastructure is support of system-wide optimizations, i.e., optimizations that span multiple applications. This is in stark contrast to the current focus in HPC systems of optimizing on a per-application basis.  The developed software will consist of a job profiler, a scheduler that performs analysis on multiple jobs at a time, and a cluster-wide run-time system that jointly optimizes multiple applications based on the output of the scheduler analysis.\r\n\r\nAchieving exascale computing is an important national priority and will impact many critical application domains, such as climate/weather, renewable energy, nuclear energy, materials science, and national security.  The work described here will improve whole-system performance on power-constrained HPC systems, which is one important step towards the exascale goal.  The project plans to transfer technology resulting from this research in the form of the proposed software stack via longstanding collaborations with several national laboratories.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Lowenthal",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "David K Lowenthal",
   "pi_email_addr": "dkl@cs.arizona.edu",
   "nsf_id": "000435738",
   "pi_start_date": "2015-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Arizona",
  "inst_street_address": "845 N PARK AVE RM 538",
  "inst_street_address_2": "",
  "inst_city_name": "TUCSON",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "5206266000",
  "inst_zip_code": "85721",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "AZ07",
  "org_lgl_bus_name": "UNIVERSITY OF ARIZONA",
  "org_prnt_uei_num": "",
  "org_uei_num": "ED44Y3W6P7B9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Arizona",
  "perf_str_addr": "888 N Euclid Ave",
  "perf_city_name": "Tucson",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "857210001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "AZ07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 489996.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 15886.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Currently, the high-performance computing (HPC) community is focused<br />on achieving practical exascale computing, where nontrivial programs<br />can reach a sustained exaflop.&nbsp; One of the key barriers to exascale is<br />solving the power problem.&nbsp; Hardware overprovisioning---where the<br />supercomputer center has more nodes than can be fully powered<br />simultaneously---has been proposed as one way to help alleviate the<br />power problem.&nbsp; However, while overprovisioning has shown potential<br />for performance improvement, there is currently no support for it from<br />HPC system software.<br /><br />The focus of this proposal was to design and implement software<br />infrastructure that will support overprovisioned systems.&nbsp; The key<br />advance in our infrastructure was that we will support system-wide<br />optimizations, i.e., optimizations that span multiple applications.<br />This is in stark contrast to the current focus in HPC systems of<br />optimizing on a per-application basis.<br /><br />We carried out three primary projects in this proposal.&nbsp; The first was<br />on the what we term inter-job power sharing problem.&nbsp; The key<br />observation was that applications assigned a fixed power bound may be<br />forced to slow down during high-power computation phases, but may not<br />consume their full power allocation during low-power I/O phases.&nbsp; This<br />project explored algorithms that leverage application<br />semantics---phase frequency, duration and power needs---to shift<br />unused power from applications in I/O phases to applications in<br />computation phases, thus improving system-wide performance.&nbsp; We<br />designed novel techniques that include explicit staggering of<br />applications to improve power shifting.&nbsp; Compared to executing without<br />power shifting, our algorithms improved average performance by up to<br />8% or a single, high-priority application by up to 32%.&nbsp; The results<br />were published in IPDPS 2016.</p>\n<p>The second was analyzing and mitigating network interference on<br />different types of modern interconnects.&nbsp; One project consisted of a<br />performance study and re-routing strategy for inter-job interference<br />on a fat-tree architecture (the \"Cab\" cluster, located at Lawrence<br />Livermore National Lab).&nbsp; Our results revealed two insights.&nbsp; First,<br />when multiple jobs compete for the interconnect, there is slowdown due<br />to a few localized hot spots.&nbsp; This was known for one job, but not for<br />multiple jobs.&nbsp; Second, we developed a re-routing strategy that<br />mitigates most of this slowdown by shifting load to lesser-loaded<br />areas of the interconnect.&nbsp; Performance results showed nearly a 50%<br />reduction in execution time with our technique.&nbsp; The results were<br />published in SC 2018 in a paper that was nominated for a Best Student<br />Paper award.&nbsp; The second project on understanding network variation<br />due to interference on Dragonfly interconnects was published in IPDPS<br />2020.<br /><br />The third project was developing an HPC job scheduler for fat-tree<br />interconnects that provides each job with an *interference-free node<br />allocation*---and still achieves high node utilization.&nbsp; There are<br />known techniques for fat trees for each of these goals separately, but<br />our job scheduler is the first to achieve both simultaneously.&nbsp; In<br />addition, we proved that our technique allows each allocation to<br />achieve the same full bandwidth guaranteed by fat trees.&nbsp; We are<br />preparing a submission to HPDC 2021 that describes these results.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/29/2020<br>\n\t\t\t\t\tModified by: David&nbsp;Lowenthal</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nCurrently, the high-performance computing (HPC) community is focused\non achieving practical exascale computing, where nontrivial programs\ncan reach a sustained exaflop.  One of the key barriers to exascale is\nsolving the power problem.  Hardware overprovisioning---where the\nsupercomputer center has more nodes than can be fully powered\nsimultaneously---has been proposed as one way to help alleviate the\npower problem.  However, while overprovisioning has shown potential\nfor performance improvement, there is currently no support for it from\nHPC system software.\n\nThe focus of this proposal was to design and implement software\ninfrastructure that will support overprovisioned systems.  The key\nadvance in our infrastructure was that we will support system-wide\noptimizations, i.e., optimizations that span multiple applications.\nThis is in stark contrast to the current focus in HPC systems of\noptimizing on a per-application basis.\n\nWe carried out three primary projects in this proposal.  The first was\non the what we term inter-job power sharing problem.  The key\nobservation was that applications assigned a fixed power bound may be\nforced to slow down during high-power computation phases, but may not\nconsume their full power allocation during low-power I/O phases.  This\nproject explored algorithms that leverage application\nsemantics---phase frequency, duration and power needs---to shift\nunused power from applications in I/O phases to applications in\ncomputation phases, thus improving system-wide performance.  We\ndesigned novel techniques that include explicit staggering of\napplications to improve power shifting.  Compared to executing without\npower shifting, our algorithms improved average performance by up to\n8% or a single, high-priority application by up to 32%.  The results\nwere published in IPDPS 2016.\n\nThe second was analyzing and mitigating network interference on\ndifferent types of modern interconnects.  One project consisted of a\nperformance study and re-routing strategy for inter-job interference\non a fat-tree architecture (the \"Cab\" cluster, located at Lawrence\nLivermore National Lab).  Our results revealed two insights.  First,\nwhen multiple jobs compete for the interconnect, there is slowdown due\nto a few localized hot spots.  This was known for one job, but not for\nmultiple jobs.  Second, we developed a re-routing strategy that\nmitigates most of this slowdown by shifting load to lesser-loaded\nareas of the interconnect.  Performance results showed nearly a 50%\nreduction in execution time with our technique.  The results were\npublished in SC 2018 in a paper that was nominated for a Best Student\nPaper award.  The second project on understanding network variation\ndue to interference on Dragonfly interconnects was published in IPDPS\n2020.\n\nThe third project was developing an HPC job scheduler for fat-tree\ninterconnects that provides each job with an *interference-free node\nallocation*---and still achieves high node utilization.  There are\nknown techniques for fat trees for each of these goals separately, but\nour job scheduler is the first to achieve both simultaneously.  In\naddition, we proved that our technique allows each allocation to\nachieve the same full bandwidth guaranteed by fat trees.  We are\npreparing a submission to HPDC 2021 that describes these results.\n\n\t\t\t\t\tLast Modified: 12/29/2020\n\n\t\t\t\t\tSubmitted by: David Lowenthal"
 }
}