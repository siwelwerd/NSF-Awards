{
 "awd_id": "1544636",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: TTP Option: Synergy: Collaborative Research: Nested Control of Assistive Robots through Human Intent Inference",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-10-01",
 "awd_exp_date": "2020-09-30",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2015-09-14",
 "awd_max_amd_letter_date": "2015-10-14",
 "awd_abstract_narration": "Part 1: Upper-limb motor impairments arise from a wide range of clinical conditions including amputations, spinal cord injury, or stroke. Addressing lost hand function, therefore, is a major focus of rehabilitation interventions; and research in robotic hands and hand exoskeletons aimed at restoring fine motor control functions gained significant speed recently. Integration of these robots with neural control mechanisms is also an ongoing research direction. We will develop prosthetic and wearable hands controlled via nested control that seamlessly blends neural control based on human brain activity and dynamic control based on sensors on robots. These Hand Augmentation using Nested Decision (HAND) systems  will also provide rudimentary tactile feedback to the user. The HAND design framework will contribute to the assistive and augmentative robotics field. The resulting technology will improve the quality of life for individuals with lost limb function. The project will help train engineers skilled in addressing multidisciplinary challenges. Through outreach activities, STEM careers will be promoted at the K-12 level, individuals from underrepresented groups in engineering will be recruited to engage in this research project, which will contribute to the diversity of the STEM workforce.\r\n\r\nPart 2: The team previously introduced the concept of human-in-the-loop cyber-physical systems (HILCPS). Using the HILCPS hardware-software co-design and automatic synthesis infrastructure, we will develop prosthetic and wearable HAND systems that are robust to uncertainty in human intent inference from physiological signals. One challenge arises from the fact that the human and the cyber system jointly operate on the same physical element. Synthesis of networked real-time applications from algorithm design environments poses a framework challenge. These will be addressed by a tightly coupled optimal nested control strategy that relies on EEG-EMG-context fusion for human intent inference. Custom distributed embedded computational and robotic platforms will be built and iteratively refined. This work will enhance the HILCPS design framework, while simultaneously making novel contributions to body/brain interface technology and assistive/augmentative robot technology. Specifically we will (1) develop a theoretical EEG-EMG-context fusion framework for agile HILCPS application domains; (2) develop theory for and design novel control theoretic solutions to handle uncertainty, blend motion/force planning with high-level human intent and ambient intelligence to robustly execute daily manipulation activities; (3) further develop and refine the HILCPS domain-specific design framework to enable rapid deployment of HILCPS algorithms onto distributed embedded systems, empowering a new class of real-time algorithms that achieve distributed embedded sensing, analysis, and decision making; (4) develop new paradigms to replace, retrain or augment hand function via the prosthetic/wearable HAND by optimizing performance on a subject-by-subject basis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Taskin",
   "pi_last_name": "Padir",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Taskin Padir",
   "pi_email_addr": "t.padir@northeastern.edu",
   "nsf_id": "000531489",
   "pi_start_date": "2015-09-14",
   "pi_end_date": "2015-10-14"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Cagdas",
   "pi_last_name": "Onal",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Cagdas D Onal",
   "pi_email_addr": "cdonal@wpi.edu",
   "nsf_id": "000637331",
   "pi_start_date": "2015-10-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Cagdas",
   "pi_last_name": "Onal",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Cagdas D Onal",
   "pi_email_addr": "cdonal@wpi.edu",
   "nsf_id": "000637331",
   "pi_start_date": "2015-09-14",
   "pi_end_date": "2015-10-14"
  }
 ],
 "inst": {
  "inst_name": "Worcester Polytechnic Institute",
  "inst_street_address": "100 INSTITUTE RD",
  "inst_street_address_2": "",
  "inst_city_name": "WORCESTER",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "5088315000",
  "inst_zip_code": "016092280",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "WORCESTER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJNQME41NBU4"
 },
 "perf_inst": {
  "perf_inst_name": "Worcester Polytechnic Institute",
  "perf_str_addr": "100 Institute Rd",
  "perf_city_name": "Worcester",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "016092247",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "8235",
   "pgm_ref_txt": "CPS-Synergy"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-c9c90dcb-7fff-b48b-2293-67ab074189b6\"> </span></p>\n<p dir=\"ltr\"><span>The primary goal of this project was to develop techniques for (a) estimating the intended hand grasp shape of a human as the arm approaches an object with electrophysiological activity of muscles from the forearm and cameras on the hand or at the eye level; (b) shared control of a light-weight but grasp-capable robotic hand. The intended applications included prosthetics, but methodologies and designs could benefit exoskeletons used in rehabilitation after stroke, or for teleoperation.</span></p>\n<p dir=\"ltr\"><span>In the course of the project we developed multiple versions of the system, each time making changes and improvements based on lessons learned. One of the most surprising lessons we learned was that, counter to our initial expectation, cameras located on the robotic hand are not as useful as a camera that provides the human/user point-of-view looking at the object and the scene from a perspective removed from the trajectory of the hand. The main issue with camera(s) on the hand was that, despite various attempts, even including up to 4 cameras, even though we were able to get good visual coverage around the hand, the visual evidence from cameras on the hand were useful only too late in the reach-to-grasp process. By the time cameras saw the object for computer vision to provide potential grasp options that are appropriate/anticipated, the electromyography (EMG) signals from the forearm was already highly informative about the human's intended grasp type.&nbsp;</span>The final system involved fusing probabilistically visual evidence from a point-of-view camera and multiple channels of EMG capturing various muscle activity from the forearm.&nbsp;</p>\n<p dir=\"ltr\"><span>On the robotic hand side, after multiple iterations</span>, we converged on a kinematically&nbsp;optimized underactuated anthropomorphic prosthetic hand that is inspired by the open source robotic hands we have developed in the past, addressing their shortcomings, and improving their functionality such as an active opposable thumb and human inspired positioning of each finger on a circular arc on the palm to maximize the range of objects (from approximately 1 mm to 13 cm diameter) that can be grasped both with precision and power grasps. In addition, the proposed hand provides fingertip normal and shear force measurements via our integrated custom magnetic force sensors. The hand weighs 333 g and costs less than 175 USD in parts, while being strong enough to lift over 1.1 lb (500 g) repeatably. We demonstrated the performance of our hand by employing a classification-based user intent decision system which predicts the grasp type using real-time electromyographic (EMG) activity patterns.</p>\n<p><span id=\"docs-internal-guid-3876ba60-7fff-a25a-ab52-53c2424c237b\"> </span></p>\n<div>This research project supported the education of multiple doctoral, master, and bachelor degree-seeking students. These students have graduated and joined the national workforce in technology research and development divisions of industry or other universities, or went to pursue higher degrees in artificial intelligence, medicine, or biomedical engineering.&nbsp;</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/21/2021<br>\n\t\t\t\t\tModified by: Cagdas&nbsp;D&nbsp;Onal</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe primary goal of this project was to develop techniques for (a) estimating the intended hand grasp shape of a human as the arm approaches an object with electrophysiological activity of muscles from the forearm and cameras on the hand or at the eye level; (b) shared control of a light-weight but grasp-capable robotic hand. The intended applications included prosthetics, but methodologies and designs could benefit exoskeletons used in rehabilitation after stroke, or for teleoperation.\nIn the course of the project we developed multiple versions of the system, each time making changes and improvements based on lessons learned. One of the most surprising lessons we learned was that, counter to our initial expectation, cameras located on the robotic hand are not as useful as a camera that provides the human/user point-of-view looking at the object and the scene from a perspective removed from the trajectory of the hand. The main issue with camera(s) on the hand was that, despite various attempts, even including up to 4 cameras, even though we were able to get good visual coverage around the hand, the visual evidence from cameras on the hand were useful only too late in the reach-to-grasp process. By the time cameras saw the object for computer vision to provide potential grasp options that are appropriate/anticipated, the electromyography (EMG) signals from the forearm was already highly informative about the human's intended grasp type. The final system involved fusing probabilistically visual evidence from a point-of-view camera and multiple channels of EMG capturing various muscle activity from the forearm. \nOn the robotic hand side, after multiple iterations, we converged on a kinematically optimized underactuated anthropomorphic prosthetic hand that is inspired by the open source robotic hands we have developed in the past, addressing their shortcomings, and improving their functionality such as an active opposable thumb and human inspired positioning of each finger on a circular arc on the palm to maximize the range of objects (from approximately 1 mm to 13 cm diameter) that can be grasped both with precision and power grasps. In addition, the proposed hand provides fingertip normal and shear force measurements via our integrated custom magnetic force sensors. The hand weighs 333 g and costs less than 175 USD in parts, while being strong enough to lift over 1.1 lb (500 g) repeatably. We demonstrated the performance of our hand by employing a classification-based user intent decision system which predicts the grasp type using real-time electromyographic (EMG) activity patterns.\n\n \nThis research project supported the education of multiple doctoral, master, and bachelor degree-seeking students. These students have graduated and joined the national workforce in technology research and development divisions of industry or other universities, or went to pursue higher degrees in artificial intelligence, medicine, or biomedical engineering. \n\n\t\t\t\t\tLast Modified: 05/21/2021\n\n\t\t\t\t\tSubmitted by: Cagdas D Onal"
 }
}