{
 "awd_id": "1539014",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "VEC: Small: Collaborative Research: Scene Understanding from RGB-D Images",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 135000.0,
 "awd_amount": 135000.0,
 "awd_min_amd_letter_date": "2015-08-27",
 "awd_max_amd_letter_date": "2016-05-26",
 "awd_abstract_narration": "This project exploits the benefits of RGB-D (color and depth) image collections with extra depth information to significantly advance the state-of-the-art in visual scene understanding, and makes computer vision techniques become usable in practical applications. Recent advance in affordable depth sensors has made depth acquisition significantly easier for ordinary users. These depth cameras are becoming very common in digital devices and help automatic scene understanding. The research team develops technologies to take advantage of depth information. Besides the published research results, the research team plans to distribute source code and benchmark data sets that could benefit researchers in a variety of disciplines. This project is integrated with educational programs, such as interdisciplinary workshops and courses at the graduate, undergraduate, and professional levels and diversity enhancement programs that promote opportunities for disadvantaged groups. The research team is closely collaborating with the industrial partner (Intel), involving interns and technology transfer in real products. The project is also applying the developed algorithms to the assistive technology for the blind and visually impaired.\r\n\r\nThis research develops algorithms required to perform real-time segmentation, labeling, and recognition of RGB-D images, videos, and 3D scans of indoor environments. Specifically, the PIs develop methods to: (1) acquire large labeled RGB-D datasets for training and evaluation, (2) study algorithms to recognize objects and estimate detailed 3D knowledge about the scene, (3) exploit the object-to-object contextual relationships in 3D, and (4) demonstrate applications to benefit the general public, including household robotics and assistive technologies for the blind.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Funkhouser",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas A Funkhouser",
   "pi_email_addr": "funk@cs.princeton.edu",
   "nsf_id": "000092134",
   "pi_start_date": "2016-05-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Jianxiong",
   "pi_last_name": "Xiao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jianxiong Xiao",
   "pi_email_addr": "xj@princeton.edu",
   "nsf_id": "000654589",
   "pi_start_date": "2015-08-27",
   "pi_end_date": "2016-05-26"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Funkhouser",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas A Funkhouser",
   "pi_email_addr": "funk@cs.princeton.edu",
   "nsf_id": "000092134",
   "pi_start_date": "2015-08-27",
   "pi_end_date": "2016-05-26"
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "87 Prospect Avenue",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "002Z",
   "pgm_ref_txt": "Intel/NSF VEC Partnership"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 135000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main goal of the project was to study how to exploit the depth information in RGB-D data to boost the performance of general-purpose visual scene understanding.&nbsp; Specifically, we investigated: 1) how to acquire large RGB-D datasets with labels, 2) how to design or learn RGB-D specific features using 3D representation, 3) how to reason about scenes in 3D, and 4) how to demonstrate the power of RGB-D recognition for applications in robotics.</p>\n<p>Over the course of the project, we addressed these goals with several collaborative research projects.&nbsp; First, we developed benchmark datasets for RGB-D scene understanding.&nbsp; Second, we developed algorithms for estimating missing depths using models trained on color images, estimating camera poses by aligning semantic and structural features, learning shape descriptors with self-supervion from prior surface reconstructions, completing partially observed scenes based on semantic segmentation, and parsing scenes based on learned contextual priors, in addition to several other challenging problems.&nbsp; We have used these algorithms in robotics applications that require navigation through complex scenes and&nbsp;manipulating objects in complex cluttered environments.&nbsp; &nbsp;We have disseminated all the research through peer-reviewed publications, in addition to publicly available datasets, benchmarks, and code.</p>\n<p>The educational goals of the project have been addressed through teaching and mentoring at several levels.&nbsp; &nbsp;The PIs have taught undergraduate courseson computer vision, graduate courses on 3D deep learning, and advanced graduate seminars on 3D scene understanding.&nbsp; The project has provided mentoring for dozens of undergraduate and graduate students, including several that received Ph.Ds and went on to faculty positions at top schools.</p>\n<p>Overall, the project has contributed to a significant advance in the technologies required to understand 3D scenes using RGB-D sensors.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/07/2019<br>\n\t\t\t\t\tModified by: Thomas&nbsp;A&nbsp;Funkhouser</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe main goal of the project was to study how to exploit the depth information in RGB-D data to boost the performance of general-purpose visual scene understanding.  Specifically, we investigated: 1) how to acquire large RGB-D datasets with labels, 2) how to design or learn RGB-D specific features using 3D representation, 3) how to reason about scenes in 3D, and 4) how to demonstrate the power of RGB-D recognition for applications in robotics.\n\nOver the course of the project, we addressed these goals with several collaborative research projects.  First, we developed benchmark datasets for RGB-D scene understanding.  Second, we developed algorithms for estimating missing depths using models trained on color images, estimating camera poses by aligning semantic and structural features, learning shape descriptors with self-supervion from prior surface reconstructions, completing partially observed scenes based on semantic segmentation, and parsing scenes based on learned contextual priors, in addition to several other challenging problems.  We have used these algorithms in robotics applications that require navigation through complex scenes and manipulating objects in complex cluttered environments.   We have disseminated all the research through peer-reviewed publications, in addition to publicly available datasets, benchmarks, and code.\n\nThe educational goals of the project have been addressed through teaching and mentoring at several levels.   The PIs have taught undergraduate courseson computer vision, graduate courses on 3D deep learning, and advanced graduate seminars on 3D scene understanding.  The project has provided mentoring for dozens of undergraduate and graduate students, including several that received Ph.Ds and went on to faculty positions at top schools.\n\nOverall, the project has contributed to a significant advance in the technologies required to understand 3D scenes using RGB-D sensors.\n\n\t\t\t\t\tLast Modified: 12/07/2019\n\n\t\t\t\t\tSubmitted by: Thomas A Funkhouser"
 }
}