{
 "awd_id": "1533850",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: FULL:  FP: Tools and Algorithms for Resilient, Power-efficient ExaScale Computing Using the GNU-CAF Compiler",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tevfik Kosar",
 "awd_eff_date": "2015-10-01",
 "awd_exp_date": "2020-09-30",
 "tot_intn_awd_amt": 750929.0,
 "awd_amount": 755595.0,
 "awd_min_amd_letter_date": "2015-08-17",
 "awd_max_amd_letter_date": "2019-05-06",
 "awd_abstract_narration": "Various problems of technological significance as well as of deep scientific interest require the use of parallel computers with the highest levels of parallelism. Thus, it is well-recognized in the scientific community that this research needs to be performed on Exascale computers, i.e. computers that have a thousand-fold greater computing power than computers today. But the technological endeavour of scaling-up comes with several attendant problems, such as being able to reuse existing code and giving the ability to ordinary developers to write scalable software that is also efficient with respect to power consumption, and is fault tolerant. This project addresses these needs by providing a open-source (GNU licensed), free, FORTRAN compiler that can make existing code Exascale ready, and which allows a programmer who only needs to be familiar with FORTRAN (and there are many of those) to develop scalable, power-efficient and fault-tolerant code, without having to learn an inordinate amount of new programming skills. The project will also contribute to a textbook on scientific computation that is being written by the Principal Investigator, as well as to a textbook on high performance computing that is being written by 2 other researchers on the team. The project website already exists that will freely distribute such knowledge to the public. This website already gets more than 50,000 hits per year. \r\n\r\n\r\nThe proposal will make the following transformative advances:\r\n1) Develop a full-fledged, open-source, Exascale-Ready GNU compiler that implements novel parallel features of the Fortran 2008 standard. These Fortran features go under the name of Coarray Fortran (CAF). Recent work has shown that CAF is either competitive or outperforms the recent MPI-3 standard while allowing the end-user to express Petascale-class parallelism much more simply. Future architectures should make the one-sided CAF-style messaging much less power-hungry compared to alternative styles of messaging.\r\n2) Exascale computers will need to support billion-way concurrency among cores, with the result that nodes might fail quite frequently. Resiliency to failure will have to be built into the compiler technologies and end-user application. An early implementation of failure-resiliency within the GNU compiler will be made and it will be used to explore how those features work within a large class of Computational Fluid Dynamics (CFD) algorithms.\r\n3) Exascale applications will also have to use power very parsimoniously. This can only be done by deciding when to focus resources on communication and when to focus them on computation. New algorithms are needed that intersperse relatively modest amounts of communication with large amounts of computation. Furthermore, the expert-level algorithm developer should be able to communicate these different resource needs to the run-time system via compiler directives. The proposed work will develop a class of high-accuracy CFD algorithms that can communicate their resource needs to the run-time system via compiler extensions in the GNU compiler.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dinshaw",
   "pi_last_name": "Balsara",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Dinshaw S Balsara",
   "pi_email_addr": "dbalsara@nd.edu",
   "nsf_id": "000304329",
   "pi_start_date": "2015-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Hart",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Hart",
   "pi_email_addr": "dhart@ucar.edu",
   "nsf_id": "000644652",
   "pi_start_date": "2015-08-17",
   "pi_end_date": "2016-09-23"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Nagle",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Nagle",
   "pi_email_addr": "dnagle@ucar.edu",
   "nsf_id": "000728882",
   "pi_start_date": "2016-08-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Notre Dame",
  "inst_street_address": "940 GRACE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "NOTRE DAME",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "5746317432",
  "inst_zip_code": "465565708",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "IN02",
  "org_lgl_bus_name": "UNIVERSITY OF NOTRE DAME DU LAC",
  "org_prnt_uei_num": "FPU6XGFXMBE9",
  "org_uei_num": "FPU6XGFXMBE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Notre Dame",
  "perf_str_addr": "940 Grace Hall",
  "perf_city_name": "Notre Dame",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "465565708",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "IN02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "778100",
   "pgm_ele_name": "Leadership-Class Computing"
  },
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7781",
   "pgm_ref_txt": "PETASCALE - TRACK 1"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 750929.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 4666.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>High performance computation is crucial for retaining the technological leadership of the United States. Such computers are increasingly made up of collections of tens of thousands of processors; and in the future they could be made up of millions of processors. The problem with having so many processors, is that once in a while processors can fail. When that happens, the entire calculation that is running on such a supercomputer also fails.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Now if the supercomputer is made up of a few hundred to a few thousand processors, the failure of any one processor will be a rather infrequent occurance. However, the modern Peta- and ExaScale supercomputers will be made up of millions of processors. At that point, the failure of any one processor becomes a rather frequent occurance. Being able to safeguard against that, and being able to recover from a processor failure, carries the name of \"resiliency\". The goal of this proposal was to develop resilient computational capabilities.</p>\n<p>&nbsp;</p>\n<p>One of the really interesting capabilities that emerged from this funded work was that we were indeed able to show that meaningful computational algorithms can indeed be made resilient. In other words, by simulating a point failure of one or two processors, we were able to show that the computation could indeed recover without crashing even when the individual processors were made to crash (unexpectedly).</p>\n<p>&nbsp;</p>\n<p>Another development was to build parsimonious computational algorithms which minimize messaging and the heat associated with message passing. We were able to develop Discontinuous Galerkin algorithms that do exactly that.</p>\n<p>&nbsp;</p>\n<p>A third, and rather unexpected development, was the emergence of very powerful GPUs. We showed that such GPUs can give performance acceleration that is significantly larger than the expected number. This minimizes the time to solution and exposure to failure.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/02/2021<br>\n\t\t\t\t\tModified by: Dinshaw&nbsp;S&nbsp;Balsara</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHigh performance computation is crucial for retaining the technological leadership of the United States. Such computers are increasingly made up of collections of tens of thousands of processors; and in the future they could be made up of millions of processors. The problem with having so many processors, is that once in a while processors can fail. When that happens, the entire calculation that is running on such a supercomputer also fails. \n\n \n\nNow if the supercomputer is made up of a few hundred to a few thousand processors, the failure of any one processor will be a rather infrequent occurance. However, the modern Peta- and ExaScale supercomputers will be made up of millions of processors. At that point, the failure of any one processor becomes a rather frequent occurance. Being able to safeguard against that, and being able to recover from a processor failure, carries the name of \"resiliency\". The goal of this proposal was to develop resilient computational capabilities.\n\n \n\nOne of the really interesting capabilities that emerged from this funded work was that we were indeed able to show that meaningful computational algorithms can indeed be made resilient. In other words, by simulating a point failure of one or two processors, we were able to show that the computation could indeed recover without crashing even when the individual processors were made to crash (unexpectedly).\n\n \n\nAnother development was to build parsimonious computational algorithms which minimize messaging and the heat associated with message passing. We were able to develop Discontinuous Galerkin algorithms that do exactly that.\n\n \n\nA third, and rather unexpected development, was the emergence of very powerful GPUs. We showed that such GPUs can give performance acceleration that is significantly larger than the expected number. This minimizes the time to solution and exposure to failure. \n\n\t\t\t\t\tLast Modified: 09/02/2021\n\n\t\t\t\t\tSubmitted by: Dinshaw S Balsara"
 }
}