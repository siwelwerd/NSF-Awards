{
 "awd_id": "1533885",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: FULL: CCA: NUMB: Exploiting Non-Uniform Memory Bandwidth  for Computational Science",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2019-02-28",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 859934.0,
 "awd_min_amd_letter_date": "2015-08-17",
 "awd_max_amd_letter_date": "2017-09-11",
 "awd_abstract_narration": "This research seeks to maximize the benefit that modern and upcoming trends in computer hardware systems can confer on science and engineering disciplines that use computation to catalyze discovery and innovation. Computer architecture and hardware systems have been experiencing disruptive transformations, and are certain to continue on this evolutionary path as platforms highlighted by massive parallelism and heterogeneous composition of processing units are becoming the norm. Such advances have triggered a realignment of market segments and computing capabilities from what was previously known: Complex fluid simulations that were previously in the purview of enterprise-grade computing may now be accommodated in modestly sized clusters. Virtual prototyping tasks previously handled by clusters can now be performed using a single workstation. This newfound availability of advanced computer capabilities, however, poses challenges for traditional practices in scalable software engineering, and even reaches the limitations of theory and algorithms that were designed with a less-parallel, homogeneous computing platform in mind. This research activity combines one of the most prominent ongoing trends in computer architecture, namely the fact that memory access and bandwidth are both non-uniform in heterogeneous CPU/GPU systems, with driving applications from the domain of computational science and engineering. This project will lead to the coordinated development of hardware innovations, scalable development practices, heterogeneity-friendly distributed computing algorithms and parallelism oriented numerical methods to extract optimal performance from emerging platforms on scientific computing workloads. \r\n\r\nThe expected systems and computer architecture advances resulting from this activity include: (i) Defining appropriate consistency models for systems with Non-Uniform Memory Bandwidth (NUMB), developing and refining Heterogeneous Race Free (HRF) consistency models for overlapping scopes and NUMB platforms. (ii) Improving the coordination of memory bandwidth utilization, by developing interfaces and mechanisms to manage interstage temporal locality, and assessing such mechanisms and policies in the context of our driving applications. (iii) Exploring enhanced mechanisms for synchronization, both among GPUs as well as between the CPU and GPU, by exploiting hardware-assisted reduction models in heterogeneous system and provide fine-grain data handling and synchronization semantics without fine-scale locks and barriers. (iv) Designing and implementing Operating System extensions to better manage heterogeneous memory (e.g., various levels of caches, accelerator memory or non-volatile memory (NVM) data stores). Applications research will emphasize (a) workloads in Adaptive Computational Fluid Dynamics (CFD), an area that has traditionally fostered co-development of theoretical and systems aspects and is well represented in the joint expertise of the collaborating investigators, and (b) instances of interactive Virtual Anatomical Modeling and Simulation, as an emerging exemplar of a host of computer-aided training tools for medical and surgical education that has only been made possible due to the enhanced capacity of modern computing platforms. This synergistic research endeavor will facilitate important emerging applications in medicine, computer-aided design and engineering research that are on the verge of attaining practical utility. It will also reveal new options for enabling more energy-efficient systems, provide a blueprint for optimized performance in throughput-sensitive applications beyond numerical computing, and generate opportunities for development of academic courses that highlight the merits of a conscious co-evolution of systems and theory principles.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Wood",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "David A Wood",
   "pi_email_addr": "david@cs.wisc.edu",
   "nsf_id": "000442514",
   "pi_start_date": "2015-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Hill",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Mark D Hill",
   "pi_email_addr": "markhill@cs.wisc.edu",
   "nsf_id": "000328470",
   "pi_start_date": "2015-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Swift",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Michael M Swift",
   "pi_email_addr": "swift@cs.wisc.edu",
   "nsf_id": "000103907",
   "pi_start_date": "2015-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Eftychios",
   "pi_last_name": "Sifakis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eftychios Sifakis",
   "pi_email_addr": "sifakis@wisc.edu",
   "nsf_id": "000581486",
   "pi_start_date": "2015-08-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "Computer Science Department",
  "perf_str_addr": "1210 W. Dayton Street",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061685",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "689200",
   "pgm_ele_name": "CI REUSE"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7933",
   "pgm_ref_txt": "NUM, SYMBOL, & ALGEBRA COMPUT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 750000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 109934.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; min-height: 14.0px} --> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; min-height: 14.0px} --> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; min-height: 14.0px} --> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; min-height: 14.0px} --> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; min-height: 14.0px} --> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; min-height: 14.0px} -->\n<p class=\"p1\">Computational science has been a critical influence in the design decisions of high-performance computer systems, and one of the key areas where advances in hardware are celebrated for their impact. There are many reasons for this interdependence. First, applications in physics, chemistry, energy, engineering, medicine, and elsewhere, which rely on scientific computing methodologies to replicate the function of natural materials on a computer, carry significant intrinsic value for science at large. Second, scientific applications impose strict demands on computer systems, such as mandating high numerical computation or memory bandwidth rates;<span>&nbsp; </span>such requirements act as a forcing function to expand the capabilities of the underlying computing platform in ways that not only benefit the application that inspired such innovations, but also impact a significantly broader spectrum of applications.<span>&nbsp; </span>In commerce, for example, the growing utilization of machine learning techniques and (big) data analytics tends to accentuate computational traits similar to computational science, which are gradually overtaking online transaction processing in terms of presence and net cost.&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">This project advanced a new perspective on how transformative advances in computational sciences might be facilitated, in light of current and emerging trends in computer hardware, and how closely algorithms and software engineering practices should adapt to and track the idiosyncrasies of the evolving underlying computing platform. Due to the breakdown of Dennard scaling, performance gains are no longer to be expected merely as an outcome of increased transistor counts, as the associated increases in power demands impose severe practical limits on the growth of such paradigm. A sustainable model of growing computational capacity is bound to be centered around an energy-efficient design, in which platforms with an increasingly heterogeneous composition (synergistic assemblies of CPUs and GPUs; deep memory hierarchies with distinct sizes and speeds) are the norm. A central tenant of this paradigm is the non-uniformity of memory bandwidth depending on the origin and destination of a data transfer, as well as its proximity to the computational unit that produces or consumes it.<span>&nbsp;</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">In light of these considerations, this project pursued two synergistic threads of investigation. On one hand, systems innovations were sought that facilitated performance of computing platforms with pervasive characteristics of heterogeneity and memory non-uniformity of bandwidth, while emphasizing the needs of workloads of central importance to computational science workloads. On the complementary side of this investigation, innovations on core numerical algorithms, data structures and software engineering practices were pursued, that optimize computational density and efficiency by embracing the idiosyncratic nature of heterogeneous platforms as a trait that theory may adapt to, as opposed to a nuisance that should be circumvented or even ignored.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">This project delivered a number of technical conquests related to hardware methods and design innovations. A new coherence interface (Crossing Guard) was proposed between the host system and accelerator modules in a heterogeneous platform. The Crossing Guard interface provides the accelerator designer with a standardized set of coherence messages that are simple enough to aid in design of bug-free coherent caches, but enable performance comparable to using the host protocol. A high-level performance model (LogCA) was proposed for hardware accelerators, which helps both programmers and architects identify performance bounds and design bottlenecks early in the design cycle, and provide insight into which optimizations may alleviate these bottlenecks. De-virtualized Memory (DVM) was proposed as a methodology that combines the protection properties of Virtual Memory, with the performance benefits of direct access to Physical Memory. By allocating memory such that physical and virtual addresses are almost always identical DVM mostly replaces page-level address translation with faster region-level De-virtualized Access Validation (DAV). Methods for improving the efficiency of System-On-Chip devices were developed, that distribute work effectively across distinct accelerators according to a new performance model. Finally, with the focus on Persistent Memory (PD) devices that combine the size and durability of solid-state memory with latency that competes with DRAM, we introduced minimally ordered durable (MOD) data structures which significantly lower the overhead of memory flushing by overlapping long-latency operations to PM.<span>&nbsp;</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">These hardware innovations were accompanied by targeted interventions on core numerical algorithms and data structures that are specifically tailored to heterogeneous platforms. The SPGrid data structure introduced a highly innovative new storage paradigm for sparse, grid-embedded data, that leverages the virtual memory system for delivering sparse storage embedded in a much larger Cartesian index set. Numerical techniques based on Domain Decomposition and Schur Complement methods were used to craft a simulator for fluid flow that accommodated highly irregular domains with billions of voxels, with the domain residing on (large) main system memory, with effective offloading of computation on multiple GPUs. Finally, a method for high-resolution topology optimization on sparse domains was presented (see picture) that achieved resolutions (in excess of one billion degrees of freedom) on a single server, that were previously only demonstrated on clusters with thousands of CPUs.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/01/2019<br>\n\t\t\t\t\tModified by: Eftychios&nbsp;Sifakis</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2019/1533885/1533885_10388390_1561957749035_frontmatter--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1533885/1533885_10388390_1561957749035_frontmatter--rgov-800width.jpg\" title=\"Computational estimation of the interior structure supporting the shell of a bird beak, generated using a very high resolution topology optimization process.\"><img src=\"/por/images/Reports/POR/2019/1533885/1533885_10388390_1561957749035_frontmatter--rgov-66x44.jpg\" alt=\"Computational estimation of the interior structure supporting the shell of a bird beak, generated using a very high resolution topology optimization process.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Structure generated using a novel large-scale solver for topology optimization; the result incorporates more than a billion degrees of freedom, in a grid of resolution 3000 x 2400 x 1600 voxels -- on a single computer. Image featured on cover of ACM Trans. on Graphics 37(6) [Proc. SIGGRAPH Asia '18]</div>\n<div class=\"imageCredit\">Liu et al, \"Narrow Band Topology Optimization on a Sparsely Populated Grid\", ACM Trans. on Graphics 37(6), 2018.</div>\n<div class=\"imageSubmitted\">Eftychios&nbsp;Sifakis</div>\n<div class=\"imageTitle\">Computational estimation of the interior structure supporting the shell of a bird beak, generated using a very high resolution topology optimization process.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nComputational science has been a critical influence in the design decisions of high-performance computer systems, and one of the key areas where advances in hardware are celebrated for their impact. There are many reasons for this interdependence. First, applications in physics, chemistry, energy, engineering, medicine, and elsewhere, which rely on scientific computing methodologies to replicate the function of natural materials on a computer, carry significant intrinsic value for science at large. Second, scientific applications impose strict demands on computer systems, such as mandating high numerical computation or memory bandwidth rates;  such requirements act as a forcing function to expand the capabilities of the underlying computing platform in ways that not only benefit the application that inspired such innovations, but also impact a significantly broader spectrum of applications.  In commerce, for example, the growing utilization of machine learning techniques and (big) data analytics tends to accentuate computational traits similar to computational science, which are gradually overtaking online transaction processing in terms of presence and net cost. \n \nThis project advanced a new perspective on how transformative advances in computational sciences might be facilitated, in light of current and emerging trends in computer hardware, and how closely algorithms and software engineering practices should adapt to and track the idiosyncrasies of the evolving underlying computing platform. Due to the breakdown of Dennard scaling, performance gains are no longer to be expected merely as an outcome of increased transistor counts, as the associated increases in power demands impose severe practical limits on the growth of such paradigm. A sustainable model of growing computational capacity is bound to be centered around an energy-efficient design, in which platforms with an increasingly heterogeneous composition (synergistic assemblies of CPUs and GPUs; deep memory hierarchies with distinct sizes and speeds) are the norm. A central tenant of this paradigm is the non-uniformity of memory bandwidth depending on the origin and destination of a data transfer, as well as its proximity to the computational unit that produces or consumes it. \n \nIn light of these considerations, this project pursued two synergistic threads of investigation. On one hand, systems innovations were sought that facilitated performance of computing platforms with pervasive characteristics of heterogeneity and memory non-uniformity of bandwidth, while emphasizing the needs of workloads of central importance to computational science workloads. On the complementary side of this investigation, innovations on core numerical algorithms, data structures and software engineering practices were pursued, that optimize computational density and efficiency by embracing the idiosyncratic nature of heterogeneous platforms as a trait that theory may adapt to, as opposed to a nuisance that should be circumvented or even ignored.\n \nThis project delivered a number of technical conquests related to hardware methods and design innovations. A new coherence interface (Crossing Guard) was proposed between the host system and accelerator modules in a heterogeneous platform. The Crossing Guard interface provides the accelerator designer with a standardized set of coherence messages that are simple enough to aid in design of bug-free coherent caches, but enable performance comparable to using the host protocol. A high-level performance model (LogCA) was proposed for hardware accelerators, which helps both programmers and architects identify performance bounds and design bottlenecks early in the design cycle, and provide insight into which optimizations may alleviate these bottlenecks. De-virtualized Memory (DVM) was proposed as a methodology that combines the protection properties of Virtual Memory, with the performance benefits of direct access to Physical Memory. By allocating memory such that physical and virtual addresses are almost always identical DVM mostly replaces page-level address translation with faster region-level De-virtualized Access Validation (DAV). Methods for improving the efficiency of System-On-Chip devices were developed, that distribute work effectively across distinct accelerators according to a new performance model. Finally, with the focus on Persistent Memory (PD) devices that combine the size and durability of solid-state memory with latency that competes with DRAM, we introduced minimally ordered durable (MOD) data structures which significantly lower the overhead of memory flushing by overlapping long-latency operations to PM. \n \nThese hardware innovations were accompanied by targeted interventions on core numerical algorithms and data structures that are specifically tailored to heterogeneous platforms. The SPGrid data structure introduced a highly innovative new storage paradigm for sparse, grid-embedded data, that leverages the virtual memory system for delivering sparse storage embedded in a much larger Cartesian index set. Numerical techniques based on Domain Decomposition and Schur Complement methods were used to craft a simulator for fluid flow that accommodated highly irregular domains with billions of voxels, with the domain residing on (large) main system memory, with effective offloading of computation on multiple GPUs. Finally, a method for high-resolution topology optimization on sparse domains was presented (see picture) that achieved resolutions (in excess of one billion degrees of freedom) on a single server, that were previously only demonstrated on clusters with thousands of CPUs.\n \n \n \n \n \n \n \n \n \n \n \n\n\t\t\t\t\tLast Modified: 07/01/2019\n\n\t\t\t\t\tSubmitted by: Eftychios Sifakis"
 }
}