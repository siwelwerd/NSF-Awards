{
 "awd_id": "1538217",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Perfect Simulation of Stochastic Networks",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922443",
 "po_email": "gaklutke@nsf.gov",
 "po_sign_block_name": "Georgia-Ann Klutke",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 127588.0,
 "awd_amount": 127588.0,
 "awd_min_amd_letter_date": "2015-08-25",
 "awd_max_amd_letter_date": "2015-08-25",
 "awd_abstract_narration": "Stochastic networks are a general class of time-varying probabilistic models where there is competition for limited resources.  They are used in a wide range of engineering applications such as communication networks, call centers, and manufacturing systems. Operators of these types of systems are often interested in achieving a high level of performance over the long run, i.e., in steady state. Thus, it is important to devise efficient computational methods for steady-state analysis of stochastic networks.  Simulation is one of the most commonly used methods for estimating steady-state performance but straightforward application results in an initial-transient bias. This award provides a comprehensive set of tools that will enable exact (i.e. with no initial-transient bias) steady-state stochastic simulation of a wide range of complex stochastic networks of interest. This characteristic (complete bias deletion) is what defines a perfect simulation algorithm. This research will therefore enable accurate steady-state analysis in a wide range of areas of societal impact, thereby allowing operators to improve efficiency and performance. Because steady-state analysis arises in a wide variety of areas, including Bayesian Statistics, the award will also be impactful beyond the types of applications mentioned earlier. \r\n\r\nSteady-state performance analysis of stochastic networks (including general queueing networks) is of great importance in operations research. Stochastic simulation has been a traditional tool used by modelers and researchers to perform steady-state computations. The key challenge in steady-state simulation is the quantification of the bias caused by the initial transient behavior associated to any direct stochastic simulation procedure. This award's focus is on algorithms that fully eliminate the initial transient bias in a non-asymptotic sense; these are known as perfect simulation algorithms. This research will produce the first class of perfect simulation algorithms for general stochastic networks with features such as non-Markovian input, time-inhomogeneous (periodic) characteristics, long-range dependence traffic (e.g. fractional Brownian motion), and multidimensional networks with and without capacity constraints (such as generalized Jackson networks).  This research combines techniques from areas such as rare-event simulation and steady-state simulation, which have not been connected for the purpose of developing computational methods. The project has important implications for other scientific areas of great relevance, such as Bayesian Statistics, due to the connection between steady-state simulation through Markov chain Monte Carlo method.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jose",
   "pi_last_name": "Blanchet",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Jose H Blanchet",
   "pi_email_addr": "jose.blanchet@stanford.edu",
   "nsf_id": "000489034",
   "pi_start_date": "2015-08-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "500 W 120th St.",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "006Y00",
   "pgm_ele_name": "OE Operations Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "072E",
   "pgm_ref_txt": "NETWORKS & QUEUING SYSTEMS"
  },
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "077E",
   "pgm_ref_txt": "SIMULATION MODELS"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 127588.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Many applications are concerned with the design of engineering systems which exhibit a desirable long term average performance. One may think, for example, in applications such as health care operations or manufacturing systems, in which managers and engineers are interested in implementing and evaluating policies which minimize the long term average cost per unit of time of running such systems. These types of systems are complex, high dimensional and non-stationary in time, so even evaluating the performance of a policy is a non-trivial undertaking. So, stochastic simulation is a natural approach to performance analysis and optimization in these types of engineering systems, which are commonly known in the Operations Research literature as stochastic networks.</p>\n<p>In turn, stochastic simulation consists in generating scenarios of the system in consideration, based on random inputs which are inherently part of the system's evolution. For instance, the flow of patients and the type of care that they require when arriving to the hospital or the demand and supply fluctuations in the setting of a manufacturing operation. The basic premise in stochastic simulation, which is based on the statistical Law of Large Numbers, is that by averaging many independent scenarios one can compute the expected performance&nbsp;of the system in consideration. The problem with the approach in the setting of steady-state cost analysis is that when computing the long term average cost of running a system, each scenario needs to be simulated for long period of time. In fact, in principle, strictly speaking, an infinite amount of time if one genuinely wishes to estimate the steady-state cost per unit of time of running the system. Naturally, this can be potentially computationally very expensive. One can, of course, truncate the simulation to a fix time horizon, but such truncation if not done carefully may introduce biases.</p>\n<p>One of the main outcomes of this project is the construction of a class of algorithms that allow to estimate without any bias the steady-state cost of complex stochastic networks by running each scenario only for a finite (but random) amount of time. This types of estimators of steady-state costs are known in the community as \"perfect\" because they do not possess biases. The advantage of having perfect or unbiased samples is that they can be easily replicated in parallel, distributing the computation easily among many computers. Thus, obtaining unbiased estimates of steady-state costs with errors that can be reduced by increasing the amount of independent replications. It is important to note that each replication or scenario now requires only a finite window of time to be implemented.&nbsp;</p>\n<p>The methodological elements of the proposal are abstracted and applied to other applications in which bias plays a key role. For instance, in the context of machine learning and data-analysis of massive data sets, often very large optimization problems are solved for model fitting. The sets may be so massive that a subsample may be needed to implement fitting procedures. Unfortunately, optimizing over a subsample will introduce a bias in the fitting procedure. The tools studied and proposed in this proposal are also applied to the problem of finding unbiased estimators of solutions of massive optimization problems, by solving small problems, each with a size which is independent of the size of the original data set (which can be arbitrarily massive in size).</p>\n<p>In sum, the tools studied in this project enable the evaluation of quantities of interest, such as steady-state costs and optimization of massive data sets, without any bias. In principle, some of these quantities may take arbitrarily long time to be computed, yet, the algorithms produced by this project produce unbiased estimators of these quantities in a finite amount of time. These estimators can be averaged to improve the accuracy of the estimates in a timely manner. Moreover, the estimators can be easily produced in parallel, making the approach scalable and versatile.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/23/2019<br>\n\t\t\t\t\tModified by: Jose&nbsp;Blanchet</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany applications are concerned with the design of engineering systems which exhibit a desirable long term average performance. One may think, for example, in applications such as health care operations or manufacturing systems, in which managers and engineers are interested in implementing and evaluating policies which minimize the long term average cost per unit of time of running such systems. These types of systems are complex, high dimensional and non-stationary in time, so even evaluating the performance of a policy is a non-trivial undertaking. So, stochastic simulation is a natural approach to performance analysis and optimization in these types of engineering systems, which are commonly known in the Operations Research literature as stochastic networks.\n\nIn turn, stochastic simulation consists in generating scenarios of the system in consideration, based on random inputs which are inherently part of the system's evolution. For instance, the flow of patients and the type of care that they require when arriving to the hospital or the demand and supply fluctuations in the setting of a manufacturing operation. The basic premise in stochastic simulation, which is based on the statistical Law of Large Numbers, is that by averaging many independent scenarios one can compute the expected performance of the system in consideration. The problem with the approach in the setting of steady-state cost analysis is that when computing the long term average cost of running a system, each scenario needs to be simulated for long period of time. In fact, in principle, strictly speaking, an infinite amount of time if one genuinely wishes to estimate the steady-state cost per unit of time of running the system. Naturally, this can be potentially computationally very expensive. One can, of course, truncate the simulation to a fix time horizon, but such truncation if not done carefully may introduce biases.\n\nOne of the main outcomes of this project is the construction of a class of algorithms that allow to estimate without any bias the steady-state cost of complex stochastic networks by running each scenario only for a finite (but random) amount of time. This types of estimators of steady-state costs are known in the community as \"perfect\" because they do not possess biases. The advantage of having perfect or unbiased samples is that they can be easily replicated in parallel, distributing the computation easily among many computers. Thus, obtaining unbiased estimates of steady-state costs with errors that can be reduced by increasing the amount of independent replications. It is important to note that each replication or scenario now requires only a finite window of time to be implemented. \n\nThe methodological elements of the proposal are abstracted and applied to other applications in which bias plays a key role. For instance, in the context of machine learning and data-analysis of massive data sets, often very large optimization problems are solved for model fitting. The sets may be so massive that a subsample may be needed to implement fitting procedures. Unfortunately, optimizing over a subsample will introduce a bias in the fitting procedure. The tools studied and proposed in this proposal are also applied to the problem of finding unbiased estimators of solutions of massive optimization problems, by solving small problems, each with a size which is independent of the size of the original data set (which can be arbitrarily massive in size).\n\nIn sum, the tools studied in this project enable the evaluation of quantities of interest, such as steady-state costs and optimization of massive data sets, without any bias. In principle, some of these quantities may take arbitrarily long time to be computed, yet, the algorithms produced by this project produce unbiased estimators of these quantities in a finite amount of time. These estimators can be averaged to improve the accuracy of the estimates in a timely manner. Moreover, the estimators can be easily produced in parallel, making the approach scalable and versatile.\n\n\t\t\t\t\tLast Modified: 04/23/2019\n\n\t\t\t\t\tSubmitted by: Jose Blanchet"
 }
}