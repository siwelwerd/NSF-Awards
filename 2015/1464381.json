{
 "awd_id": "1464381",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: III: Declarative array processing for large-scale scientific analyses",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Aidong Zhang",
 "awd_eff_date": "2015-04-15",
 "awd_exp_date": "2018-03-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2015-04-13",
 "awd_max_amd_letter_date": "2015-04-13",
 "awd_abstract_narration": "Scientists understand complex natural phenomena through data-intensive analyses that run on hundreds of thousands of processing cores. Quickly exploring very large datasets in parallel for insights, however, is challenging. Analyzing larger-than-memory datasets exposes the intricacies of the storage hierarchy and necessitates different implementations based on the anticipated data volume and the system architecture. Domain scientists using large-scale computing facilities are therefore faced with a dilemma: they need to either perpetually maintain and tune their data processing codes to the evolving system infrastructure, or limit their investigation to analyses that can be completed in a reasonable time as datasets continue to grow in size. \r\n\r\nDeclarative data processing techniques can alleviate scientists from the burden of managing how scientific data are accessed or stored. Although many declarative data management systems are actively used by scientists, these systems require time-consuming data transformations, such as loading, chunking and repartitioning, before answering any scientific query. In addition, many data management systems assume complete control of the underlying hardware, and are oblivious to optimizations and scaling opportunities that are offered through the batch execution paradigm of large-scale computing facilities. To address this gap in research, we will investigate techniques in the intersection of data management and scientific computing on how to allocate resources, and how to proactively manage parallel I/O and distributed memory. To impact scientific practice, we will develop a prototype runtime  system that will augment an established scientific file format library with declarative querying capabilities for data analysis in leadership computing facilities.\r\n\r\nFor further information see the project web site at: http://go.osu.edu/insitu_analysis",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Spyros",
   "pi_last_name": "Blanas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Spyros Blanas",
   "pi_email_addr": "blanas.2@osu.edu",
   "nsf_id": "000652382",
   "pi_start_date": "2015-04-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p { margin-bottom: 0.1in; line-height: 120%; } -->\n<p style=\"margin-bottom: 0in; line-height: 100%;\">Scientists understand complex natural phenomena by analyzing massive array datasets. Quickly exploring very large datasets in parallel for insights, however, is challenging. Analyzing larger-than-memory datasets exposes the intricacies of the storage hierarchy and necessitates different implementations based on the anticipated data volume and the system architecture. Declarative data processing techniques can alleviate scientists from the burden of managing how scientific data are accessed or stored. Although many declarative data management systems are actively used by scientists, these systems often require a time-consuming data ingestion step. This project bridges the gap between declarative array processing techniques on pre-loaded datasets and imperative array processing on raw data.</p>\n<p style=\"margin-bottom: 0in; line-height: 100%;\">The first outcome of this project is the design and development of ArrayBridge, an I/O middleware that queries scientific data in the HDF5 format directly from the open-source SciDB array processing system. A detailed performance evaluation in a large scientific computing facility (NERSC at the Lawrence Berkeley National Lab) shows that ArrayBridge can query TB-sized datasets in parallel in minutes, whereas the same dataset would have taken days to load into a system like SciDB. ArrayBridge exhibits statistically indistinguishable performance and I/O scalability to the native SciDB storage engine. ArrayBridge also supports the TileDB array format for sparse array storage and the TensorFlow processing runtime for analyses based on machine learning.</p>\n<p style=\"margin-bottom: 0in; line-height: 100%;\">In addition to fast data ingestion, scientists also want to efficiently materialize array objects in the HDF5 format. This is important for interoperability with existing HDF5-based tools. ArrayBridge brings efficient parallel writing and deduplication for versioned datasets in a manner that does not break backwards compatibility with existing applications. ArrayBridge controls the I/O concurrency independently of the compute concurrency by writing through a non-materialized array view. The non-materialized view partitions I/O into independent I/O streams on the write path and transparently reconstructs objects on the read path to remain fully-compatible with existing applications. The array view mechanism is leveraged to permit the efficient storage of versioned datasets. This project contributes two solutions, Chunk Mosaic and Delta Encoding, that offer different trade-offs between the storage space and reconstruction time for older versions. These techniques allow scientists to offload version management to a runtime instead of performing version management manually through ad-hoc metadata and flat files.</p>\n<p style=\"margin-bottom: 0in; line-height: 100%;\">The opportunity of managing data using a declarative runtime is that it permits automated optimizations that scientists would find too time-consuming to perform manually. This project has also investigated how to improve the network performance during parallel data processing---a common bottleneck in many scientific analyses. Alas, many MPI-based programs fail to transfer data at line rate during query processing. The question is how to use the remote direct memory access (RDMA) capability of fast networks for parallel data processing, in particular considering (1) the number of open connections, (2) the contention for the shared network interface and (3) the communication primitive, and (4) how much memory should be reserved to shuffle data between compute nodes during query processing. This project contributed an RDMA-aware shuffling algorithm that improves throughput by as much as 4&times; over the state of the art.</p>\n<p style=\"margin-bottom: 0in; line-height: 100%;\">The ArrayBridge technology has been transferred to the SciDB open-source array data processing system. The source code of the SciDB branch that incorporates ArrayBridge is publicly available at: https://code.osu.edu/arraybridge/scidb</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/22/2018<br>\n\t\t\t\t\tModified by: Spyros&nbsp;Blanas</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nScientists understand complex natural phenomena by analyzing massive array datasets. Quickly exploring very large datasets in parallel for insights, however, is challenging. Analyzing larger-than-memory datasets exposes the intricacies of the storage hierarchy and necessitates different implementations based on the anticipated data volume and the system architecture. Declarative data processing techniques can alleviate scientists from the burden of managing how scientific data are accessed or stored. Although many declarative data management systems are actively used by scientists, these systems often require a time-consuming data ingestion step. This project bridges the gap between declarative array processing techniques on pre-loaded datasets and imperative array processing on raw data.\nThe first outcome of this project is the design and development of ArrayBridge, an I/O middleware that queries scientific data in the HDF5 format directly from the open-source SciDB array processing system. A detailed performance evaluation in a large scientific computing facility (NERSC at the Lawrence Berkeley National Lab) shows that ArrayBridge can query TB-sized datasets in parallel in minutes, whereas the same dataset would have taken days to load into a system like SciDB. ArrayBridge exhibits statistically indistinguishable performance and I/O scalability to the native SciDB storage engine. ArrayBridge also supports the TileDB array format for sparse array storage and the TensorFlow processing runtime for analyses based on machine learning.\nIn addition to fast data ingestion, scientists also want to efficiently materialize array objects in the HDF5 format. This is important for interoperability with existing HDF5-based tools. ArrayBridge brings efficient parallel writing and deduplication for versioned datasets in a manner that does not break backwards compatibility with existing applications. ArrayBridge controls the I/O concurrency independently of the compute concurrency by writing through a non-materialized array view. The non-materialized view partitions I/O into independent I/O streams on the write path and transparently reconstructs objects on the read path to remain fully-compatible with existing applications. The array view mechanism is leveraged to permit the efficient storage of versioned datasets. This project contributes two solutions, Chunk Mosaic and Delta Encoding, that offer different trade-offs between the storage space and reconstruction time for older versions. These techniques allow scientists to offload version management to a runtime instead of performing version management manually through ad-hoc metadata and flat files.\nThe opportunity of managing data using a declarative runtime is that it permits automated optimizations that scientists would find too time-consuming to perform manually. This project has also investigated how to improve the network performance during parallel data processing---a common bottleneck in many scientific analyses. Alas, many MPI-based programs fail to transfer data at line rate during query processing. The question is how to use the remote direct memory access (RDMA) capability of fast networks for parallel data processing, in particular considering (1) the number of open connections, (2) the contention for the shared network interface and (3) the communication primitive, and (4) how much memory should be reserved to shuffle data between compute nodes during query processing. This project contributed an RDMA-aware shuffling algorithm that improves throughput by as much as 4&times; over the state of the art.\nThe ArrayBridge technology has been transferred to the SciDB open-source array data processing system. The source code of the SciDB branch that incorporates ArrayBridge is publicly available at: https://code.osu.edu/arraybridge/scidb\n\n\t\t\t\t\tLast Modified: 06/22/2018\n\n\t\t\t\t\tSubmitted by: Spyros Blanas"
 }
}