{
 "awd_id": "1546482",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Collaborative Research: F: Stochastic Approximation for Subspace and Multiview Representation Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 704672.0,
 "awd_amount": 704672.0,
 "awd_min_amd_letter_date": "2015-09-03",
 "awd_max_amd_letter_date": "2015-09-03",
 "awd_abstract_narration": "Unsupervised learning of useful features, or representations, is one of the most basic challenges of machine learning. Unsupervised representation learning techniques capitalize on unlabeled data which is often cheap and abundant and sometimes virtually unlimited. The goal of these ubiquitous techniques is to learn a representation that reveals intrinsic low-dimensional structure in data, disentangles underlying factors of variation by incorporating universal AI priors such as smoothness and sparsity, and is useful across multiple tasks and domains. \r\n\r\nThis project aims to develop new theory and methods for representation learning that can easily scale to large datasets. In particular, this project is concerned with methods for large-scale unsupervised feature learning, including Principal Component Analysis (PCA) and Partial Least Squares (PLS). To capitalize on massive amounts of unlabeled data, this project will develop appropriate computational approaches and study them in the \"data-laden\" regime. Therefore, instead of viewing representation learning as dimensionality reduction techniques and focusing on an empirical objective on finite data, these methods are studied with the goal of optimizing a population objective based on sample. This view suggests using Stochastic Approximation approaches, such as Stochastic Gradient Descent (SGD) and Stochastic Mirror Descent, that are incremental in nature and process each new sample with a computationally cheap update. Furthermore, this view enables a rigorous analysis of benefits of stochastic approximation algorithms over traditional finite-data methods. The project aims to develop stochastic approximation approaches to PCA and PLS and related problems and extensions, including deep, and sparse variants, and analyze these problems in the data-laden regime.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Raman",
   "pi_last_name": "Arora",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Raman Arora",
   "pi_email_addr": "arora@cs.jhu.edu",
   "nsf_id": "000656458",
   "pi_start_date": "2015-09-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N CHARLES ST",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 704672.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Overview:</strong> Learning useful representations of data is one of the most basic challenges in machine learning and data science. In this project, we developed general-purpose unsupervised representation learning techniques (see Figure 1) that capitalize on unlabeled data, which is often cheap and abundant and sometimes virtually unlimited. The goal of these ubiquitous techniques is to learn a representation that reveals intrinsic low-dimensional structure in data and is useful across multiple tasks and domains.</p>\n<p><strong>Research Outcomes:</strong> Our main contributions include the following.</p>\n<p>First, given that we aim to capitalize on massive amounts of unlabeled data for representation learning, we develop appropriate computational approaches and study them in a \"data-laden\" regime. The algorithms we give are provably optimal and perform well in practice.&nbsp;</p>\n<p>Second, we identify a useful universal prior for learning representations. We show that high-level factors measured across different views, or modalities, or different times and places, have high mutual information, i.e., can be predicted from each other and contain many bits of information. The set of techniques developed using this principle are referred to as multiview representation learning and capitalize on multiview/multimodal data.&nbsp;</p>\n<p>Third, we develop robust and nonlinear variants that leverage advances in deep learning.&nbsp;</p>\n<p>Finally, as a true testament to the broad applicability of our ideas, our methods yield state-of-the-art results on various tasks in different domains, including speech, language processing, multimedia analysis, computational healthcare, and social media analytics, and has been the subject of hundreds of follow-up works.</p>\n<p><strong>Educational Outcomes:</strong>&nbsp;The project provided educational training and professional development for project participants. A new interdisciplinary course, titled \"representation learning,\" was designed and taught as part of the curriculum development activities. Several graduate students and postdocs who contributed to the project have accepted full-time positions in industrial research labs and faculty positions in academia. The project also supported outreach to local high school students through Stem Achievement in Elementary Schools (SABES) and Women in Science and Engineering (WISE) programs.&nbsp;</p>\n<p>The results from the project were published at top peer-reviewed conferences in machine learning research. The application-focused publications appeared in speech and natural language processing conferences and signal processing journals. The preliminary versions of the conference publications and expanded journal versions were released on arXiv for the timely and broad dissemination of ideas. The PI and students gave several talks and tutorials to further the timely dissemination of main results and ideas and enable academic discourse. The codebase and datasets generated for the project have been posted on public repositories.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/19/2022<br>\n\t\t\t\t\tModified by: Raman&nbsp;Arora</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1546482/1546482_10395735_1642622898137_replearn-landscape--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1546482/1546482_10395735_1642622898137_replearn-landscape--rgov-800width.jpg\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2022/1546482/1546482_10395735_1642622898137_replearn-landscape--rgov-66x44.jpg\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Representation learning landscape: Unsupervised learning criteria and function classes in representation learning.</div>\n<div class=\"imageCredit\">Raman Arora</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Raman&nbsp;Arora</div>\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nOverview: Learning useful representations of data is one of the most basic challenges in machine learning and data science. In this project, we developed general-purpose unsupervised representation learning techniques (see Figure 1) that capitalize on unlabeled data, which is often cheap and abundant and sometimes virtually unlimited. The goal of these ubiquitous techniques is to learn a representation that reveals intrinsic low-dimensional structure in data and is useful across multiple tasks and domains.\n\nResearch Outcomes: Our main contributions include the following.\n\nFirst, given that we aim to capitalize on massive amounts of unlabeled data for representation learning, we develop appropriate computational approaches and study them in a \"data-laden\" regime. The algorithms we give are provably optimal and perform well in practice. \n\nSecond, we identify a useful universal prior for learning representations. We show that high-level factors measured across different views, or modalities, or different times and places, have high mutual information, i.e., can be predicted from each other and contain many bits of information. The set of techniques developed using this principle are referred to as multiview representation learning and capitalize on multiview/multimodal data. \n\nThird, we develop robust and nonlinear variants that leverage advances in deep learning. \n\nFinally, as a true testament to the broad applicability of our ideas, our methods yield state-of-the-art results on various tasks in different domains, including speech, language processing, multimedia analysis, computational healthcare, and social media analytics, and has been the subject of hundreds of follow-up works.\n\nEducational Outcomes: The project provided educational training and professional development for project participants. A new interdisciplinary course, titled \"representation learning,\" was designed and taught as part of the curriculum development activities. Several graduate students and postdocs who contributed to the project have accepted full-time positions in industrial research labs and faculty positions in academia. The project also supported outreach to local high school students through Stem Achievement in Elementary Schools (SABES) and Women in Science and Engineering (WISE) programs. \n\nThe results from the project were published at top peer-reviewed conferences in machine learning research. The application-focused publications appeared in speech and natural language processing conferences and signal processing journals. The preliminary versions of the conference publications and expanded journal versions were released on arXiv for the timely and broad dissemination of ideas. The PI and students gave several talks and tutorials to further the timely dissemination of main results and ideas and enable academic discourse. The codebase and datasets generated for the project have been posted on public repositories. \n\n\t\t\t\t\tLast Modified: 01/19/2022\n\n\t\t\t\t\tSubmitted by: Raman Arora"
 }
}