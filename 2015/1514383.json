{
 "awd_id": "1514383",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF:  Medium: Collaborative Research: Hardness in Polynomial Time",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Joseph Maurice Rojas",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 599945.0,
 "awd_amount": 599945.0,
 "awd_min_amd_letter_date": "2015-06-09",
 "awd_max_amd_letter_date": "2017-08-02",
 "awd_abstract_narration": "A central endeavor of theoretical computer science is to classify computational problems according to the resources (such as running time and storage space) needed to solve them.  Although the field of algorithm design has been highly successful in discovering efficient, polynomial-time algorithms for problems of practical interest, little evidence has been shown for the optimality of most algorithms.  The goal of this project is to build a useful complexity theory for the class of polynomial-time solvable problems (called P), by proving equivalences between problems and proving conditional lower bounds on specific problems, assuming the validity of certain plausible mathematical conjectures. \r\n\r\nKnown lower bounds for specific problems in P are conditioned on some complexity-theoretic assumption such as the (Strong) Exponential Time Hypothesis (concerning the complexity of k-CNF-SAT), the conjecture that dense all-pairs shortest paths (APSP) requires cubic time, or that 3SUM requires quadratic time.  The goals of this project are threefold.  The first goal is to establish conditional lower bounds on problems in diverse areas (such as graph optimization, string matching, geometry, and dynamic data structures) using standard hardness conjectures. The second goal is to search for better hardness conjectures that are both plausible and versatile, and to discover relationships (implications or equivalences) between nominally unrelated conjectures.  The last goal is to investigate the plausibility of these conjectures by attempting to disprove them. \r\n\r\nThe curricular portion of this project involves developing lecture material suitable for introductory algorithms and complexity courses at both the undergraduate and graduate level.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Seth",
   "pi_last_name": "Pettie",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Seth Pettie",
   "pi_email_addr": "pettie@umich.edu",
   "nsf_id": "000477111",
   "pi_start_date": "2015-06-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan",
  "perf_str_addr": "2260 Hayward st.",
  "perf_city_name": "Ann ARbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 385213.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 214732.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>One of the great successes of theoretical computer science is classifying problems as tractable (in the complexity class P) or intractable (typically NP-hard).&nbsp; The aim of this project is to develop a theory of fine-grained complexity, differentiating problems whose optimal running times grow as n, n^2, n^3, etc. and understanding the relationship between problems via reductions.&nbsp;</p>\n<p>This grant supported the organization of two workshops on this topic, <em>Structure and Hardness in P</em> in 2016 and and <em>Fine-grained Approximation Algorithms and Complexity</em> in 2019, which helped popularize and accelerate research in this exciting field.</p>\n<p>Many lower bounds in fine-grained complexity are based on plausible, but unproven, assumptions on the difficulty of archetypical problems such as 3SUM, or Boolean Satisfiability.&nbsp; It was long conjectured that the 3SUM problem required at least n^2 time, and also required n^2 numerical comparisons.&nbsp; We proved (<em>J. ACM</em> 2018) that this conjecture is false: 3SUM can be solved with n^1.5 numerical comparisons, and in slightly sub-quadratic time.&nbsp; Based on a weaker version of this 3SUM conjecture, we proved that many combinatorial algorithms are optimal or near optimal, for generalized pattern matching problems relevant to virus detection, subgraph enumeration, dynamic graph connectivity, and dynamic matchings.&nbsp; Most notably, we proved that the inverted index (the data structure used by search engines to support keyword search) cannot be substantially improved.</p>\n<p>Based on a different conjecture concerning the complexity of Boolean Satisfiability (the Strong Exponential Time Hypothesis), we proved lower bounds on the complexity of deciding the winner in a classic full-information 2-player game played on graphs, called <em>Cops and Robbers</em>.&nbsp; Deciding the winner in a game with g cops and one robber requires at least n^g time, and deciding the minimum number of cops needed to win requires 2^sqrt{n} time.</p>\n<p>A large part of this project focused on understanding the time hierarchy of the distributed LOCAL model in a fine-grained way.&nbsp; We proved that the LOCAL time hierarchy supports an infinite number of natural problem complexities, like the Turing Machine time hierarchy, but in other respects the two models are qualitatively different.&nbsp; The LOCAL model has a small constant number of gaps (empty regions) in its time hierarchy.&nbsp; As a byproduct of identifying these gaps, we proved that the so-called &ldquo;graph-shattering&rdquo; approach to solving problems in the LOCAL model is unavoidable, that for some class of problems there is an exponential separation between deterministic complexity and randomized complexity, and the constructive Lovasz Local Lemma is complete for randomized sublogarithmic time. &nbsp;We designed new algorithms for classical symmetry breaking problems in the LOCAL model, such as vertex coloring and edge coloring, and algorithms for the distributed Lovasz Local Lemma.</p>\n<p>This grant supported a postdoctoral researcher and a Ph.D. student, who are now a professor at Bar-Ilan University and a postdoctoral fellow at ETH Z&uuml;rich.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/06/2020<br>\n\t\t\t\t\tModified by: Seth&nbsp;Pettie</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOne of the great successes of theoretical computer science is classifying problems as tractable (in the complexity class P) or intractable (typically NP-hard).  The aim of this project is to develop a theory of fine-grained complexity, differentiating problems whose optimal running times grow as n, n^2, n^3, etc. and understanding the relationship between problems via reductions. \n\nThis grant supported the organization of two workshops on this topic, Structure and Hardness in P in 2016 and and Fine-grained Approximation Algorithms and Complexity in 2019, which helped popularize and accelerate research in this exciting field.\n\nMany lower bounds in fine-grained complexity are based on plausible, but unproven, assumptions on the difficulty of archetypical problems such as 3SUM, or Boolean Satisfiability.  It was long conjectured that the 3SUM problem required at least n^2 time, and also required n^2 numerical comparisons.  We proved (J. ACM 2018) that this conjecture is false: 3SUM can be solved with n^1.5 numerical comparisons, and in slightly sub-quadratic time.  Based on a weaker version of this 3SUM conjecture, we proved that many combinatorial algorithms are optimal or near optimal, for generalized pattern matching problems relevant to virus detection, subgraph enumeration, dynamic graph connectivity, and dynamic matchings.  Most notably, we proved that the inverted index (the data structure used by search engines to support keyword search) cannot be substantially improved.\n\nBased on a different conjecture concerning the complexity of Boolean Satisfiability (the Strong Exponential Time Hypothesis), we proved lower bounds on the complexity of deciding the winner in a classic full-information 2-player game played on graphs, called Cops and Robbers.  Deciding the winner in a game with g cops and one robber requires at least n^g time, and deciding the minimum number of cops needed to win requires 2^sqrt{n} time.\n\nA large part of this project focused on understanding the time hierarchy of the distributed LOCAL model in a fine-grained way.  We proved that the LOCAL time hierarchy supports an infinite number of natural problem complexities, like the Turing Machine time hierarchy, but in other respects the two models are qualitatively different.  The LOCAL model has a small constant number of gaps (empty regions) in its time hierarchy.  As a byproduct of identifying these gaps, we proved that the so-called \"graph-shattering\" approach to solving problems in the LOCAL model is unavoidable, that for some class of problems there is an exponential separation between deterministic complexity and randomized complexity, and the constructive Lovasz Local Lemma is complete for randomized sublogarithmic time.  We designed new algorithms for classical symmetry breaking problems in the LOCAL model, such as vertex coloring and edge coloring, and algorithms for the distributed Lovasz Local Lemma.\n\nThis grant supported a postdoctoral researcher and a Ph.D. student, who are now a professor at Bar-Ilan University and a postdoctoral fellow at ETH Z&uuml;rich.\n\n \n\n\t\t\t\t\tLast Modified: 01/06/2020\n\n\t\t\t\t\tSubmitted by: Seth Pettie"
 }
}