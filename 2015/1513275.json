{
 "awd_id": "1513275",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CI-NEW: Collaborative Research: A Modular Platform for Enabling Computing Research in Intelligent Human-Robot Interaction",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2015-06-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 324334.0,
 "awd_amount": 336334.0,
 "awd_min_amd_letter_date": "2015-06-05",
 "awd_max_amd_letter_date": "2020-04-29",
 "awd_abstract_narration": "This project is to design, develop, and freely distribute novel, affordable, modular hardware and accompanying software platforms for enabling non-contact human-robot interaction (HRI) research. Such research is a significant portion of HRI today, and encompasses a broad spectrum of computing challenges and compelling application domains, including education, training, rehabilitation, and health. The goal of this project is to significantly increase access to hardware to a large body of researchers, so that computing advances can be applied to physical systems and evaluated in real-world environments, in order to drive progress in the computing community.\r\n\r\nAdvances in sensor and communication technologies have facilitated progress in computing research on physical platforms. The field of human-robot interaction in particular has grown significantly and actively brings together an interdisciplinary community of researchers across computing, robotics, and social science. However, progress has been limited by the lack of affordable, general-purpose, modular hardware platforms with available low-level software that would enable large numbers of computing researchers to enter the field and develop and test algorithms, as well as conduct statistically significant user studies by deploying systems in the real world and collecting user data to inform further computational research in HRI.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Maja",
   "pi_last_name": "Matari\u0107",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Maja J Matari\u0107",
   "pi_email_addr": "mataric@usc.edu",
   "nsf_id": "000410606",
   "pi_start_date": "2015-06-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S. Flower St.",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900890001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "CA37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 324334.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 12000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-16466f89-7fff-4a81-0e55-881c41863868\">&nbsp;</span></p>\n<p dir=\"ltr\"><span>This NSF Computing Research Infrastructure (CRI) project involved the design, development, and deployment of an affordable, general-purpose, and modular face-to-face human-robot interaction (HRI) robot platform called Quori, as well as open-source social behavior application programming interfaces (APIs) and developer tools for computing researchers to develop and test algorithms and conduct user studies in the real world, using Quori or other suitable HRI platforms.&nbsp; This CRI project involved a collaboration with an industry partner, Semio (</span><a href=\"https://semio.ai\"><span>https://semio.ai</span></a><span>). The project team used an iterative, community-driven design process for determining the design of the hardware, software, physical appearance, and cost of the robot. By continually engaging communities of interest through surveys, symposia, and workshops, this CRI project addressed the needs and desires of potential platform users.&nbsp; The developed robot has multiple novel design features, including a projected face and a waist joint, allowing for new directions of HRI research.&nbsp; The developed software, in particular the social behavior APIs and developer tools are general and platform-agnostic, helping researchers to explore topics in HRI without having to write and maintain basic software in each individual lab and institution.&nbsp; The project included a phase that solicited applications from interested institutions to be awarded a Quori platform for research; eight research institutions across the US were awarded Quori robots, along with the two partnered leading institutions on the project (U Penn and USC), and the industry partner (Semio).</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The rest of this summary summarizes the software developed for this project.&nbsp; (U Penn?s summary will describe the robot hardware design.) This CRI project developed software packages that enable </span><span>verbal HRI</span><span> (using speech recognition and generation), and </span><span>nonverbal HRI</span><span> (using pointing gesture recognition, and attention recognition and generation). In addition, the project provided intuitive software tools to help research teams to rapidly create and deploy multimodal conversational content for HRI. The project contributions include:</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>1. </span><span>Speech Generation and Recognition:</span><span> The community survey informed the team that speech generation and speech recognition were the most desired software modules. While these are not novel technologies, they are essential for social HRI and their integration was therefore a key result of this project.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>2. </span><span>Attention Generation and Recognition:</span><span> The real-time </span><span>attention generation</span><span> software module uses three models of human visual attention to produce robot eye gaze behaviors for HRI. These models include (1) a </span><span>neurobiological model</span><span> of the human visual attention system, (2) a </span><span>conversational gaze model</span><span> for multi-party interactions, and (3) a </span><span>functional gaze model</span><span> governed by the need for the robot to track body features of the human user to conduct a successful interaction. The </span><span>attention recognition</span><span> estimates what the human user is looking at, based on an anatomically informed estimate of human attention.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>3. </span><span>Pointing Gesture Recognition:</span><span> The referential </span><span>pointing gesture recognition</span><span> software module considers human reaching and visual attention.&nbsp;</span></p>\n<p><br /><span>4. </span><span>Animation and Dialog Tools:</span><span> To enable teams of programmers and non-technical users to develop socially interactive robot applications, two web-based software tools were developed: (1) a </span><span>animation tool</span><span> for creating expressive robot movements; and (2) a </span><span>dialog tool</span><span> for authoring conversational human-robot interactions. These tools allow content creators to animate socially interactive robot behaviors and synchronize them with speech without needing to program, making robot application development more broadly accessible. To support replicability in HRI research, these tools are publicly available on the Quori website (https://www.quori.org), and all applications developed for the Quori platform are available for public use, viewing, copying, and modifying in a Wikipedia of conversational content.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/05/2021<br>\n\t\t\t\t\tModified by: Maja&nbsp;J&nbsp;Mataric</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175570569_Semio-BehaviorGeneration-Attention--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175570569_Semio-BehaviorGeneration-Attention--rgov-800width.jpg\" title=\"Robot Attention Generation\"><img src=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175570569_Semio-BehaviorGeneration-Attention--rgov-66x44.jpg\" alt=\"Robot Attention Generation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Visualized output from Quori\ufffds attention system that selects where the robot should look based on three models of eye gaze behavior: (1) a neurobiological gaze model of the human visual attention system; (2) a conversational gaze model for multiparty interactions; and (3) a functional gaze model gov</div>\n<div class=\"imageCredit\">Ross Mead</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Maja&nbsp;J&nbsp;Mataric</div>\n<div class=\"imageTitle\">Robot Attention Generation</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175458338_Semio-Tools-Animation--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175458338_Semio-Tools-Animation--rgov-800width.jpg\" title=\"Robot Animation Tool\"><img src=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175458338_Semio-Tools-Animation--rgov-66x44.jpg\" alt=\"Robot Animation Tool\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The web-based tool for creating expressive robot movements for Quori.</div>\n<div class=\"imageCredit\">Ross Mead</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Maja&nbsp;J&nbsp;Mataric</div>\n<div class=\"imageTitle\">Robot Animation Tool</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175509513_Semio-Tools-Dialog--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175509513_Semio-Tools-Dialog--rgov-800width.jpg\" title=\"Human-Robot Dialog Authoring Tool\"><img src=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175509513_Semio-Tools-Dialog--rgov-66x44.jpg\" alt=\"Human-Robot Dialog Authoring Tool\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A web-based tool for writing scripted multimodal conversational interactions between Quori and one, few, or many users.</div>\n<div class=\"imageCredit\">Ross Mead</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Maja&nbsp;J&nbsp;Mataric</div>\n<div class=\"imageTitle\">Human-Robot Dialog Authoring Tool</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175387254_Semio-BehaviorRecognition-AttentionandPointing--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175387254_Semio-BehaviorRecognition-AttentionandPointing--rgov-800width.jpg\" title=\"Human Social Behavior Recognition\"><img src=\"/por/images/Reports/POR/2021/1513275/1513275_10367537_1628175387254_Semio-BehaviorRecognition-AttentionandPointing--rgov-66x44.jpg\" alt=\"Human Social Behavior Recognition\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Visualized output from Quori\ufffds human attention recognition (left) and human pointing recognition (right) systems; each arrow indicates the target entity (e.g., a person, place, or thing) attended to by the associated social behavior recognition system.</div>\n<div class=\"imageCredit\">Ross Mead</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Maja&nbsp;J&nbsp;Mataric</div>\n<div class=\"imageTitle\">Human Social Behavior Recognition</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nThis NSF Computing Research Infrastructure (CRI) project involved the design, development, and deployment of an affordable, general-purpose, and modular face-to-face human-robot interaction (HRI) robot platform called Quori, as well as open-source social behavior application programming interfaces (APIs) and developer tools for computing researchers to develop and test algorithms and conduct user studies in the real world, using Quori or other suitable HRI platforms.  This CRI project involved a collaboration with an industry partner, Semio (https://semio.ai). The project team used an iterative, community-driven design process for determining the design of the hardware, software, physical appearance, and cost of the robot. By continually engaging communities of interest through surveys, symposia, and workshops, this CRI project addressed the needs and desires of potential platform users.  The developed robot has multiple novel design features, including a projected face and a waist joint, allowing for new directions of HRI research.  The developed software, in particular the social behavior APIs and developer tools are general and platform-agnostic, helping researchers to explore topics in HRI without having to write and maintain basic software in each individual lab and institution.  The project included a phase that solicited applications from interested institutions to be awarded a Quori platform for research; eight research institutions across the US were awarded Quori robots, along with the two partnered leading institutions on the project (U Penn and USC), and the industry partner (Semio).\n\n \nThe rest of this summary summarizes the software developed for this project.  (U Penn?s summary will describe the robot hardware design.) This CRI project developed software packages that enable verbal HRI (using speech recognition and generation), and nonverbal HRI (using pointing gesture recognition, and attention recognition and generation). In addition, the project provided intuitive software tools to help research teams to rapidly create and deploy multimodal conversational content for HRI. The project contributions include:\n\n \n1. Speech Generation and Recognition: The community survey informed the team that speech generation and speech recognition were the most desired software modules. While these are not novel technologies, they are essential for social HRI and their integration was therefore a key result of this project.\n\n \n2. Attention Generation and Recognition: The real-time attention generation software module uses three models of human visual attention to produce robot eye gaze behaviors for HRI. These models include (1) a neurobiological model of the human visual attention system, (2) a conversational gaze model for multi-party interactions, and (3) a functional gaze model governed by the need for the robot to track body features of the human user to conduct a successful interaction. The attention recognition estimates what the human user is looking at, based on an anatomically informed estimate of human attention. \n\n \n3. Pointing Gesture Recognition: The referential pointing gesture recognition software module considers human reaching and visual attention. \n\n\n4. Animation and Dialog Tools: To enable teams of programmers and non-technical users to develop socially interactive robot applications, two web-based software tools were developed: (1) a animation tool for creating expressive robot movements; and (2) a dialog tool for authoring conversational human-robot interactions. These tools allow content creators to animate socially interactive robot behaviors and synchronize them with speech without needing to program, making robot application development more broadly accessible. To support replicability in HRI research, these tools are publicly available on the Quori website (https://www.quori.org), and all applications developed for the Quori platform are available for public use, viewing, copying, and modifying in a Wikipedia of conversational content.\n\n\t\t\t\t\tLast Modified: 08/05/2021\n\n\t\t\t\t\tSubmitted by: Maja J Mataric"
 }
}