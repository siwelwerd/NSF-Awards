{
 "awd_id": "1464421",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CRII: NeTS: Data-Driven QoE for Mobile Videos",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Darleen Fisher",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2015-06-02",
 "awd_max_amd_letter_date": "2016-07-12",
 "awd_abstract_narration": "Mobile data traffic was around 18 Exabytes in 2013 and is expected to double every year. Of this, 67% is video traffic currently and this percentage is estimated to increase. This necessitates development of strategies to map subjective human scores into on-line deployable methods for Quality of user Experience (QoE) assessment, which can in turn control the underlying service architecture used by network and application providers.\r\n\r\nThe goal of this CISE Research Initiation Initiative project is to develop data-driven mobile video QoE models which can be used for real-time estimation of mobile-video quality in smart devices. The researchers envision collecting large volume of actual video-watch data using a customized application and processing the data to mine important patterns and trends. This work will build a data-driven video QoE model from the user's viewing experience to assess and improve the performance of mobile video applications in contrast to existing work, which has used distortion-specific metrics or full-reference approaches or have data-driven models to model user engagement. The research entails building mobile applications to measure video quality in mobile devices and collect subjective user-experience scores. The anticipated outcomes of this two-year project include answers to key research questions in video delivery for mobile devices: Can we define new objective models for QoE of mobile-multimedia to incorporate the loss in quality caused by freezing, distortions and other factors? Is it possible to develop simpler, more reliable and cost-effective methods for objective evaluation of video quality in battery constraint mobile devices? What is the impact of device aesthetics, network resources and user preferences, and can this impact be accurately modeled? What kinds of guarantees can be made in different application scenarios and what cannot? How can the new metrics be used to design better video delivery system with a focus to enhance user's quality of experience? \r\n\r\nThe researchers will first generate a mobile application to a generate pool of video data obtaining user's subjective video assessment as well as content/network/device/visual quality metadata while allowing a user to play videos from a project-generated dataset or content from popular websites such as Youtube or Netflix. This will be accompanied by development of metrics for quality assessment to capture delivery losses and their proportional impact on video quality. A machine learning model will be developed to model distortions as well as factors such as screen resolution, video resolution, freeze frequency and intensity which impact video quality perception and their proportional impact on video QoE. \r\n\r\nThe development of proposed quality-assessment tools can be used by content and network providers to identify the bottlenecks in ensuring video quality and rectify them with QoE-aware adaptive streaming, caching, transcoding and optimizations. The PI is also committed to including a strong educational plan that has both a conventional component using the project results to drive coursework and involvement of graduate and undergraduates students in research and in building a QoE application for smart phones to involve undergraduate and graduate students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Amit",
   "pi_last_name": "Pande",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Amit Pande",
   "pi_email_addr": "pande@ucdavis.edu",
   "nsf_id": "000632098",
   "pi_start_date": "2015-06-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "One Shields Ave",
  "perf_city_name": "Davis",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956165270",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736300",
   "pgm_ele_name": "Networking Technology and Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7363",
   "pgm_ref_txt": "RES IN NETWORKING TECH & SYS"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 85897.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 89103.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this Research Initiation Initiative was to investigate data-driven mobile video QoE models which can be used for real-time estimation of mobile-video quality in smart devices. A proof of concept was developed using (a) data-driven visual acuity model, and (b) data-driven real-time video transcoding framework for mobile video, (c) data-driven motion prediction for 360 videos. The works were published in reputed conferences.</p>\n<p>(a) We present a visual acuity framework which makes fast online computations in a mobile device and provide an accurate estimate of mobile video QoE. We identify and study the three main causes that impact visual acuity in mobile videos: spatial distortions, types of buffering and resolution changes. Each of them can be accurately modeled using our framework. We use machine learning techniques to build a prediction model for visual acuity, which depicts more than 78% accuracy. We present an experimental implementation on iPhone 4 and 5s to show that the proposed visual acuity framework is feasible to deploy in mobile devices. Using a data corpus of over 2852 mobile video clips for the experiments, we validate the proposed framework.</p>\n<p>(b) We introduce Vsync, a framework for cloud based video synchronization for mobile devices. A video content is streamed using a cloud-based real-time transcoding&nbsp; and transmission framework to provide smooth video quality. It is built over prediction models for video transcoding sessions and a QoE based adaptive video streaming protocol. QoE based real-time adaptation and transcoding allows Vsync to obtain the improvements of 37 &sim; 80% than other compared schemes. The dataset and evaluation was done on a pool of 220K video clips.</p>\n<p>&nbsp;</p>\n<p>(c) 360-degree video is on the cusp of going mainstream. We present a scheme to optimize the network bandwidth using motion-prediction-based multicast to serve concurrent viewers.We present a data-driven scheme for temporal prediction of viewer motion from previous states, and hence optimize the multicast bandwidth consumption by sending only the portion likely to be watched by a group of viewers. Our evaluations with real viewer motion traces show a bandwidth saving of over 50%, compared to full frame video multicast, and significant bandwidth reduction compared to unicast.</p>\n<p>Other efforts involve network allocation using data-driven approaches and mobile device authentication using implicit signatures of motion sensors. We also explored the usage of reinforcement learning based techniques for network allocation/ video resource allocation. However, this work did not mature.</p>\n<p>&nbsp;</p>\n<p>REFERENCES:</p>\n<p>&nbsp;</p>\n<p>[A] E. Baik, A. Pande, Z. Zheng, P. Mohapatra, &ldquo;VSync: Cloud Based Video Streaming Service for Mobile Devices&rdquo; IEEE International Conference on Computer Communications INFOCOM, 2016.</p>\n<p>[B] E. Baik, A. Pande, C. Stover, P. Mohapatra, &ldquo;Video Acuity Assessment in Mobile Devices&rdquo; IEEE International Conference on Computer Communications INFOCOM, 2015.</p>\n<p>[C] Yanan Bao, Tianxao Zhang, Amit Pande, Huasen Wu, Xin Liu &ldquo;Motion-Prediction-Based Multicast for 360-Degree Video Transmissions&rdquo; IEEE SECON 2017</p>\n<p>[D] Yunze Zheng, Amit Pande, Jindan Zhu, Prasant Mohapatra, &ldquo;WearIA: Wearable Device Implicit Authentication based on Activity Information&rdquo; IEEE WOWMOM 2017</p>\n<p>[E] Yanan Bao, Xin Liu, and Amit Pande, &ldquo;Data-Guided Approach for Learning and Improving User Experience in Computer Networks,&rdquo; the 7th Asian Conference on Machine Learning (ACML), Oct. 2015.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/04/2019<br>\n\t\t\t\t\tModified by: Amit&nbsp;Pande</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this Research Initiation Initiative was to investigate data-driven mobile video QoE models which can be used for real-time estimation of mobile-video quality in smart devices. A proof of concept was developed using (a) data-driven visual acuity model, and (b) data-driven real-time video transcoding framework for mobile video, (c) data-driven motion prediction for 360 videos. The works were published in reputed conferences.\n\n(a) We present a visual acuity framework which makes fast online computations in a mobile device and provide an accurate estimate of mobile video QoE. We identify and study the three main causes that impact visual acuity in mobile videos: spatial distortions, types of buffering and resolution changes. Each of them can be accurately modeled using our framework. We use machine learning techniques to build a prediction model for visual acuity, which depicts more than 78% accuracy. We present an experimental implementation on iPhone 4 and 5s to show that the proposed visual acuity framework is feasible to deploy in mobile devices. Using a data corpus of over 2852 mobile video clips for the experiments, we validate the proposed framework.\n\n(b) We introduce Vsync, a framework for cloud based video synchronization for mobile devices. A video content is streamed using a cloud-based real-time transcoding  and transmission framework to provide smooth video quality. It is built over prediction models for video transcoding sessions and a QoE based adaptive video streaming protocol. QoE based real-time adaptation and transcoding allows Vsync to obtain the improvements of 37 &sim; 80% than other compared schemes. The dataset and evaluation was done on a pool of 220K video clips.\n\n \n\n(c) 360-degree video is on the cusp of going mainstream. We present a scheme to optimize the network bandwidth using motion-prediction-based multicast to serve concurrent viewers.We present a data-driven scheme for temporal prediction of viewer motion from previous states, and hence optimize the multicast bandwidth consumption by sending only the portion likely to be watched by a group of viewers. Our evaluations with real viewer motion traces show a bandwidth saving of over 50%, compared to full frame video multicast, and significant bandwidth reduction compared to unicast.\n\nOther efforts involve network allocation using data-driven approaches and mobile device authentication using implicit signatures of motion sensors. We also explored the usage of reinforcement learning based techniques for network allocation/ video resource allocation. However, this work did not mature.\n\n \n\nREFERENCES:\n\n \n\n[A] E. Baik, A. Pande, Z. Zheng, P. Mohapatra, \"VSync: Cloud Based Video Streaming Service for Mobile Devices\" IEEE International Conference on Computer Communications INFOCOM, 2016.\n\n[B] E. Baik, A. Pande, C. Stover, P. Mohapatra, \"Video Acuity Assessment in Mobile Devices\" IEEE International Conference on Computer Communications INFOCOM, 2015.\n\n[C] Yanan Bao, Tianxao Zhang, Amit Pande, Huasen Wu, Xin Liu \"Motion-Prediction-Based Multicast for 360-Degree Video Transmissions\" IEEE SECON 2017\n\n[D] Yunze Zheng, Amit Pande, Jindan Zhu, Prasant Mohapatra, \"WearIA: Wearable Device Implicit Authentication based on Activity Information\" IEEE WOWMOM 2017\n\n[E] Yanan Bao, Xin Liu, and Amit Pande, \"Data-Guided Approach for Learning and Improving User Experience in Computer Networks,\" the 7th Asian Conference on Machine Learning (ACML), Oct. 2015.\n\n\t\t\t\t\tLast Modified: 09/04/2019\n\n\t\t\t\t\tSubmitted by: Amit Pande"
 }
}