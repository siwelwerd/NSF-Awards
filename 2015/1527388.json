{
 "awd_id": "1527388",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Learning Signal Representations for Multiple Inference Tasks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2015-08-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2015-07-27",
 "awd_max_amd_letter_date": "2015-07-27",
 "awd_abstract_narration": "Rapid advances in high-performance computing and widespread availability of massive datasets are bringing about a paradigm shift in the theory and practice of signal representations, geared towards inference and learning. A signal representation is a compressed summary that only retains those features of the signal that are salient for a class of inference tasks. This project provides a comprehensive theoretical and algorithmic framework for signal representations, which is sufficiently broad to cover both the traditional types of signal representations, such as vector quantization and sparse codes, and the more modern types, inspired by recent advances in machine learning and signal processing for Big Data. \r\n\r\nUnder this framework, the statistical performance and the computational complexity of signal representations are addressed in a unified manner by imposing structural constraints on the encoding map, the decoding map, and the model space of the representation, while simultaneously tailoring these objects to the class of tasks of interest. This unification leads to new theoretical and algorithmic insights into highly structured internal representations that are a key factor in recent spectacular success of deep neural networks on challenging tasks in visual, audio, and speech analytics.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Maxim",
   "pi_last_name": "Raginsky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Maxim Raginsky",
   "pi_email_addr": "maxim@illinois.edu",
   "nsf_id": "000551233",
   "pi_start_date": "2015-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Pierre",
   "pi_last_name": "Moulin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Pierre Moulin",
   "pi_email_addr": "moulin@ifp.uiuc.edu",
   "nsf_id": "000189624",
   "pi_start_date": "2015-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618014052",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Rapid advances in high-performance computing and widespread availability of massive datasets are bringing about a paradigm shift in the theory and practice of signal representations &mdash; we are steadily moving away from operational criteria revolving signal reconstruction fidelity after digital transmission and storage, and towards operational criteria linked to inference and learning. In particular, recent spectacular successes of deep neural networks in a number of challenging speech analysis and computer vision tasks can be attributed in large part to the fact that these architectures crucially rely on extracting intermediate &ldquo;compressed&rdquo; representations of the input signals. Such architectures are inspired by the human&rsquo;s ability to summarize sensory information into parsimonious, salient features that are rapidly and efficiently encoded, communicated to, and processed in the cerebral cortex.<br /><br />A signal representation is a compressed summary that only retains those features of the signal that are salient for a given inference task or class of tasks (e.g., reconstruction; classification; estimation; etc.). The major objective of this project was to develop a comprehensive theoretical and algorithmic framework for signal representations, which is sufficiently broad to cover both the traditional types of signal representations, such as vector quantization and sparse codes, and the more modern types, inspired by recent advances in machine learning and signal processing for Big Data. The statistical performance and the computational complexity of signal representations must be addressed in a unified manner by imposing structural constraints on the encoding map, the decoding map, and the model space of the representation, while simultaneously tailoring these objects to the class of tasks of interest.<br /><br />The following was accomplished under this project: <br /><br />1. A key problem pertaining to signal representations for inference has to do with domain adaptation: in practice, one increasingly encounters the situation when the training data are sampled from one distribution, whereas the learned predictor is applied to data with a different distribution. Existing approaches to this problem are primarily ad hoc, and depend on fragile assumptions about the relationship between the training (or source) distribution and the test (or target) distribution. A quantitative framework was developed to capture the impact of signal representation on the performance of classifiers and initiated the study of domain adaptation through the lens of common signal representations for a class of tasks.<br /><br />2. An emerging problem in machine learning is to quantify the performance of a classifier under adversarial attacks, where an adversary slightly modifies an input, producing a forgery that is misclassified. Such attacks exploit weaknesses in the signal representations used by the classifier and have the potential to be devastating for driverless transportation systems, for instance. By exploiting the connection between the creation of such forgeries and the field of watermarking, a framework was developed for forgery detection that thwarts such attacks.<br /><br />3. A comprehensive theoretical approach was introduced for learning finite-dimensional representations of high-dimensional signals. A representation consists of a codebook and a decoder, i.e., a mapping from the codebook into the original signal space. The encoder maps each signal to that element of the codebook whose image under the decoder is the closest to the signal. Classical methods, such as VQ or PCA, can be represented in this way by fixing a class of linear decoders. For example, if the codebook consists of K orthonormal basis vectors, then any linear mapping induces a K-point VQ codebook. Similarly, if the codebook is the K-dimensional Euclidean unit ball, then any isometry induces a mapping of the original signal to its top K principal components. This theory was extended this theory to state-of-the-art nonlinear decoders, such as general feedforward neural nets and convolutional neural nets.<br /><br />4. Various adversarial audio attacks have recently been developed to fool automatic speech recognition (ASR) systems. A defense against such attacks was proposed, based on the uncertainty introduced by dropout in neural networks. It was shown that the proposed defense is able to detect attacks created through optimized perturbations and frequency masking on a state-of-the-art end-to-end ASR system. Furthermore, the defense can be made robust against attacks that are immune to noise reduction. The defense was tested on Mozilla's CommonVoice dataset, the UrbanSound dataset, and an excerpt of the LibriSpeech dataset, showing that it achieves high detection accuracy in a wide range of scenarios.<br /><br />5. Robust machine learning formulations have emerged to address the prevalent vulnerability of deep neural networks to adversarial examples. Under this project, a connection was drawn between optimal robust learning and the privacy-utility tradeoff problem, which is a generalization of the rate-distortion problem. The saddle point of the game between a robust classifier and an adversarial perturbation can be found via the solution of a maximum conditional entropy problem. This information-theoretic perspective sheds light on the fundamental tradeoff between robustness and clean data performance, which ultimately arises from the geometric structure of the underlying data distribution and perturbation constraints.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2020<br>\n\t\t\t\t\tModified by: Maxim&nbsp;Raginsky</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nRapid advances in high-performance computing and widespread availability of massive datasets are bringing about a paradigm shift in the theory and practice of signal representations &mdash; we are steadily moving away from operational criteria revolving signal reconstruction fidelity after digital transmission and storage, and towards operational criteria linked to inference and learning. In particular, recent spectacular successes of deep neural networks in a number of challenging speech analysis and computer vision tasks can be attributed in large part to the fact that these architectures crucially rely on extracting intermediate \"compressed\" representations of the input signals. Such architectures are inspired by the human\u2019s ability to summarize sensory information into parsimonious, salient features that are rapidly and efficiently encoded, communicated to, and processed in the cerebral cortex.\n\nA signal representation is a compressed summary that only retains those features of the signal that are salient for a given inference task or class of tasks (e.g., reconstruction; classification; estimation; etc.). The major objective of this project was to develop a comprehensive theoretical and algorithmic framework for signal representations, which is sufficiently broad to cover both the traditional types of signal representations, such as vector quantization and sparse codes, and the more modern types, inspired by recent advances in machine learning and signal processing for Big Data. The statistical performance and the computational complexity of signal representations must be addressed in a unified manner by imposing structural constraints on the encoding map, the decoding map, and the model space of the representation, while simultaneously tailoring these objects to the class of tasks of interest.\n\nThe following was accomplished under this project: \n\n1. A key problem pertaining to signal representations for inference has to do with domain adaptation: in practice, one increasingly encounters the situation when the training data are sampled from one distribution, whereas the learned predictor is applied to data with a different distribution. Existing approaches to this problem are primarily ad hoc, and depend on fragile assumptions about the relationship between the training (or source) distribution and the test (or target) distribution. A quantitative framework was developed to capture the impact of signal representation on the performance of classifiers and initiated the study of domain adaptation through the lens of common signal representations for a class of tasks.\n\n2. An emerging problem in machine learning is to quantify the performance of a classifier under adversarial attacks, where an adversary slightly modifies an input, producing a forgery that is misclassified. Such attacks exploit weaknesses in the signal representations used by the classifier and have the potential to be devastating for driverless transportation systems, for instance. By exploiting the connection between the creation of such forgeries and the field of watermarking, a framework was developed for forgery detection that thwarts such attacks.\n\n3. A comprehensive theoretical approach was introduced for learning finite-dimensional representations of high-dimensional signals. A representation consists of a codebook and a decoder, i.e., a mapping from the codebook into the original signal space. The encoder maps each signal to that element of the codebook whose image under the decoder is the closest to the signal. Classical methods, such as VQ or PCA, can be represented in this way by fixing a class of linear decoders. For example, if the codebook consists of K orthonormal basis vectors, then any linear mapping induces a K-point VQ codebook. Similarly, if the codebook is the K-dimensional Euclidean unit ball, then any isometry induces a mapping of the original signal to its top K principal components. This theory was extended this theory to state-of-the-art nonlinear decoders, such as general feedforward neural nets and convolutional neural nets.\n\n4. Various adversarial audio attacks have recently been developed to fool automatic speech recognition (ASR) systems. A defense against such attacks was proposed, based on the uncertainty introduced by dropout in neural networks. It was shown that the proposed defense is able to detect attacks created through optimized perturbations and frequency masking on a state-of-the-art end-to-end ASR system. Furthermore, the defense can be made robust against attacks that are immune to noise reduction. The defense was tested on Mozilla's CommonVoice dataset, the UrbanSound dataset, and an excerpt of the LibriSpeech dataset, showing that it achieves high detection accuracy in a wide range of scenarios.\n\n5. Robust machine learning formulations have emerged to address the prevalent vulnerability of deep neural networks to adversarial examples. Under this project, a connection was drawn between optimal robust learning and the privacy-utility tradeoff problem, which is a generalization of the rate-distortion problem. The saddle point of the game between a robust classifier and an adversarial perturbation can be found via the solution of a maximum conditional entropy problem. This information-theoretic perspective sheds light on the fundamental tradeoff between robustness and clean data performance, which ultimately arises from the geometric structure of the underlying data distribution and perturbation constraints.\n\n\t\t\t\t\tLast Modified: 10/30/2020\n\n\t\t\t\t\tSubmitted by: Maxim Raginsky"
 }
}