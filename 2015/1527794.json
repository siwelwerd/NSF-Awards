{
 "awd_id": "1527794",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Empowerment of Disabled Individuals via an Adaptive Framework for Indirect Human-Robot Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 499932.0,
 "awd_amount": 523932.0,
 "awd_min_amd_letter_date": "2015-08-24",
 "awd_max_amd_letter_date": "2021-08-20",
 "awd_abstract_narration": "Thanks to advances in autonomy, an increasing variety of robotic devices have emerged over the last few years to assist disabled users with mobility and object manipulation.  However, users often report higher satisfaction from controlling and interacting with a robot, even when an autonomous robot shows better quantitative performance, because they tend to see the robot not merely as an agent for retrieving objects but as a quintessential tool for reasserting their domain of interaction with their environment and engaging their available faculties to the fullest.  Sadly, for these users effective interaction with a robotic device is frequently hindered by the fact that haptic feedback in the normal sense may not be possible, whether due to sensory and/or cognitive disabilities or to limitations imposed by motor disabilities that may prevent proper transfer of user intent in the absence of an appropriate user interface.  Motivated by these considerations, and in the hope of boosting the current low rates of assistive technology adoption by its intended users, the PI will in this project create a software framework that is independent of the robotic system and allows for adaptively compensating the level and type of human-robot interaction to increase user satisfaction based on real-time measurements of user performance.  Project outcomes will foster a new paradigm for the design of assistive technology, which will enable system developers to understand the impact of user preferences and incorporate them in their designs from the start, as opposed to the current inefficient practice of user testing a design after creating an expensive product or prototype.  \r\n\r\nThis research will push the envelope of human-robot interaction through the creation of models for understanding the underlying intent of users with disabilities, which may adversely affect their environmental perception and response.  By utilizing these empirical models to design an adaptive human-robot interface that can compensate for deficits and variability in user performance, the work will generate a novel framework for effective sharing of control between individuals with disabilities and their robotic assistants.  Furthermore, the control design for physical human-robot interaction that will be developed as part of this work will advance the field of autonomous robotics in general through the creation of new algorithms for physically interacting with users and their environments.  The research tasks include systematic modeling of user performance during human-robot interaction, estimation of user performance parameters within a generalized estimation framework, and design of adaptive Lyapunov-based control strategies to facilitate safe and efficient physical interaction of the robotic end-effector with the user and environmental objects.  The work will be informed by quantitative/qualitative data to be gathered from extensive user studies in the field, by the PI's previous experience in working with this class of users, and by his expertise in assistive robotics.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aman",
   "pi_last_name": "Behal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aman Behal",
   "pi_email_addr": "abehal@mail.ucf.edu",
   "nsf_id": "000493898",
   "pi_start_date": "2015-08-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Janan",
   "pi_last_name": "Smither",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Janan A Smither",
   "pi_email_addr": "janan.smither@ucf.edu",
   "nsf_id": "000331976",
   "pi_start_date": "2015-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "The University of Central Florida Board of Trustees",
  "inst_street_address": "4000 CENTRAL FLORIDA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "ORLANDO",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "4078230387",
  "inst_zip_code": "328168005",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "FL10",
  "org_lgl_bus_name": "THE UNIVERSITY OF CENTRAL FLORIDA BOARD OF TRUSTEES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RD7MXJV7DKT9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Central Florida",
  "perf_str_addr": "",
  "perf_city_name": "Orlando",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "328263252",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "FL10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 499932.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Fixed-base and now mobile collaborative robots (cobots) are designed to be safe around humans and are easily programmed to change their control algorithm depending on the task and context at hand. These robots are ubiquitous in industrial settings as well as in non-industrial settings where they serve as professional or personal service robots. Since each individual with a disability is uniquely abled depending on progression/extent/location of their injury from disease and/or trauma, the control interface between the robotic assistant and the user needs to allow for tunable interaction. Therefore, the main goal of this research project was to&nbsp;<span>allow for (a) adaptively compensating the level and type of human-robot interaction to increase user satisfaction, and (b) increased adoption of robotic assistants to reduce caregiver dependence.&nbsp;</span></p>\n<p><span>The experimental platform chosen for this research was MANUS - a Wheelchair Mounted Robotic Arm made by Exact Dynamics. We outfitted the robot with off-the-shelf sensors such as 3D vision sensors, force sensors, slip sensors etc. and modified gripper fingers so that the robot could effectively move through new scenarios and interact with novel objects. This, along with controllers that allowed the robot to perform desired tasks efficiently and safely, imbued the robot with the underlying autonomy that was then combined with a specialized graphical user interface through which the user could intervene to generate sliding-scale human robot interaction. The user interface was designed based on results from two large-scale user studies that allowed us to determine important human factors that affect the performance of users while teleoperating a robot.&nbsp;</span></p>\n<p><span>Along the way, several Ph.D. dissertations, journal and conference papers, and a book chapter were published in order to disseminate the results to communities of interest.&nbsp;High-school and undergraduate students were mentored by the investigators and their graduate students through this project so students were able to get experience with several robot platforms available in the market as well as related software tools, thereby, creating a research pipeline.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/24/2023<br>\n\t\t\t\t\tModified by: Aman&nbsp;Behal</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nFixed-base and now mobile collaborative robots (cobots) are designed to be safe around humans and are easily programmed to change their control algorithm depending on the task and context at hand. These robots are ubiquitous in industrial settings as well as in non-industrial settings where they serve as professional or personal service robots. Since each individual with a disability is uniquely abled depending on progression/extent/location of their injury from disease and/or trauma, the control interface between the robotic assistant and the user needs to allow for tunable interaction. Therefore, the main goal of this research project was to allow for (a) adaptively compensating the level and type of human-robot interaction to increase user satisfaction, and (b) increased adoption of robotic assistants to reduce caregiver dependence. \n\nThe experimental platform chosen for this research was MANUS - a Wheelchair Mounted Robotic Arm made by Exact Dynamics. We outfitted the robot with off-the-shelf sensors such as 3D vision sensors, force sensors, slip sensors etc. and modified gripper fingers so that the robot could effectively move through new scenarios and interact with novel objects. This, along with controllers that allowed the robot to perform desired tasks efficiently and safely, imbued the robot with the underlying autonomy that was then combined with a specialized graphical user interface through which the user could intervene to generate sliding-scale human robot interaction. The user interface was designed based on results from two large-scale user studies that allowed us to determine important human factors that affect the performance of users while teleoperating a robot. \n\nAlong the way, several Ph.D. dissertations, journal and conference papers, and a book chapter were published in order to disseminate the results to communities of interest. High-school and undergraduate students were mentored by the investigators and their graduate students through this project so students were able to get experience with several robot platforms available in the market as well as related software tools, thereby, creating a research pipeline.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/24/2023\n\n\t\t\t\t\tSubmitted by: Aman Behal"
 }
}