{
 "awd_id": "1502405",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Fellowship Award",
 "awd_titl_txt": "PostDoctoral Research Fellowship",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Victor Roytburd",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2015-03-27",
 "awd_max_amd_letter_date": "2015-03-27",
 "awd_abstract_narration": "This award is made as part of the FY 2015 Mathematical Sciences Postdoctoral Research Fellowships Program. Each of the fellowships supports a research and training project at a host institution in the mathematical sciences, including applications to other disciplines, under the mentorship of a sponsoring scientist. The title of the project for this fellowship to Damek Davis is \"Operator Splitting in Decentralized Optimization.\" The host institution for the fellowship is University of California, Los Angeles, and the sponsoring scientist is Dr. Wotao Yin.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Damek",
   "pi_last_name": "Davis",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Damek S Davis",
   "pi_email_addr": "",
   "nsf_id": "000675223",
   "pi_start_date": "2015-03-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Davis                   Damek          S",
  "inst_street_address": "",
  "inst_street_address_2": "",
  "inst_city_name": "Los Angeles",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "",
  "inst_zip_code": "900243096",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "",
  "org_prnt_uei_num": "",
  "org_uei_num": ""
 },
 "perf_inst": {
  "perf_inst_name": "Davis                   Damek          S",
  "perf_str_addr": null,
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951555",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "733500",
   "pgm_ele_name": "WORKFORCE IN THE MATHEMAT SCI"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9219",
   "pgm_ref_txt": "POSTDOCTORAL FELLOWSHIPS IN MATH SCIENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Operator-splitting is a set of techniques and algorithms that decompose complicated problems in optimization, monotone operator theory, PDE, and control into simple subproblems that are solved sequentially or in parallel. The high-level outcomes of this work consist of significant extensions of operator-splitting schemes to complex problem classes.<br />The main innovations of the proposed work include:<br />(1) A new operator-splitting scheme for solving monotone inclusions involving three operators, thereby solving an open problem which was first mentioned in an early 1979 paper by Lions and Mercier.&nbsp;<br />(2) A general framework for modeling and solving large-scale monotone inclusion problems, based on asynchronous stochastic variance-reduced estimators for nonlinear operators. Typical stochastic operator-splitting schemes are plagued by asymptotically slow convergence. In contrast, the proposed method converges linearly, while still only processing a single data-sample in each iteration. By fusing stochastic and chaotic multi-agent computation in networks of parallel processing units, this work significantly extends our ability to incorporate stochasticity in decentralized operator-splitting schemes.&nbsp;<br />(3) An analysis of the linear speedup of uncoordinated asynchronous stochastic optimization methods for nonsmooth, nonconvex optimization problems, wherein multiple agents work independently and chaotically together to minimize a cost function. We show that the speed of such methods scales up linearly with the number of agents involved in the computation (until a saturation point is reached).<br />(4) The first stochastic variance-reduced optimization method for general nonsmooth, nonconvex problems. This method achieves the best known complexity for this problem class. In particular, for machine learning problems involving n data samples, the complexity of the algorithm scales with n^(2/3) rather than n, which was previously the best dependence on n. Thus, the resulting algorithm is n^(1/3) times faster than all competing variance reduction methods.&nbsp;<br />(5) The first stochastic subgradient method for minimizing expectations of nonsmooth, nonconvex problems with provable convergence rate guarantees. The primary contribution of this work is a simple proof that the proposed algorithm converges at the same rate as the stochastic gradient method for smooth nonconvex problems. This result validates the use of stochastic subgradient methods in nonsmooth, nonconvex optimization as is common when optimizing neural networks.<br />Further broader impacts of the project include: The supervision of an undergraduate project in outlier detection in large-scale statistical learning problems; participation in a panel on mathematics research at a local public high-school; program committee membership at the Neural Information Processing Systems (NIPS) OPT2016 workshop; chairing of a session at the INFORMS international meeting in 2016; industry talks at high-profile tech companies; and over 20 talks at a variety of institutions and conferences.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/09/2017<br>\n\t\t\t\t\tModified by: Damek&nbsp;S&nbsp;Davis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOperator-splitting is a set of techniques and algorithms that decompose complicated problems in optimization, monotone operator theory, PDE, and control into simple subproblems that are solved sequentially or in parallel. The high-level outcomes of this work consist of significant extensions of operator-splitting schemes to complex problem classes.\nThe main innovations of the proposed work include:\n(1) A new operator-splitting scheme for solving monotone inclusions involving three operators, thereby solving an open problem which was first mentioned in an early 1979 paper by Lions and Mercier. \n(2) A general framework for modeling and solving large-scale monotone inclusion problems, based on asynchronous stochastic variance-reduced estimators for nonlinear operators. Typical stochastic operator-splitting schemes are plagued by asymptotically slow convergence. In contrast, the proposed method converges linearly, while still only processing a single data-sample in each iteration. By fusing stochastic and chaotic multi-agent computation in networks of parallel processing units, this work significantly extends our ability to incorporate stochasticity in decentralized operator-splitting schemes. \n(3) An analysis of the linear speedup of uncoordinated asynchronous stochastic optimization methods for nonsmooth, nonconvex optimization problems, wherein multiple agents work independently and chaotically together to minimize a cost function. We show that the speed of such methods scales up linearly with the number of agents involved in the computation (until a saturation point is reached).\n(4) The first stochastic variance-reduced optimization method for general nonsmooth, nonconvex problems. This method achieves the best known complexity for this problem class. In particular, for machine learning problems involving n data samples, the complexity of the algorithm scales with n^(2/3) rather than n, which was previously the best dependence on n. Thus, the resulting algorithm is n^(1/3) times faster than all competing variance reduction methods. \n(5) The first stochastic subgradient method for minimizing expectations of nonsmooth, nonconvex problems with provable convergence rate guarantees. The primary contribution of this work is a simple proof that the proposed algorithm converges at the same rate as the stochastic gradient method for smooth nonconvex problems. This result validates the use of stochastic subgradient methods in nonsmooth, nonconvex optimization as is common when optimizing neural networks.\nFurther broader impacts of the project include: The supervision of an undergraduate project in outlier detection in large-scale statistical learning problems; participation in a panel on mathematics research at a local public high-school; program committee membership at the Neural Information Processing Systems (NIPS) OPT2016 workshop; chairing of a session at the INFORMS international meeting in 2016; industry talks at high-profile tech companies; and over 20 talks at a variety of institutions and conferences.\n\n\t\t\t\t\tLast Modified: 09/09/2017\n\n\t\t\t\t\tSubmitted by: Damek S Davis"
 }
}