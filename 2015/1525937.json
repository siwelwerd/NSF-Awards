{
 "awd_id": "1525937",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Touchscreen Computer Interfaces for Working Dogs",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 499352.0,
 "awd_amount": 499352.0,
 "awd_min_amd_letter_date": "2015-08-26",
 "awd_max_amd_letter_date": "2015-08-26",
 "awd_abstract_narration": "For nearly a century, assistance dogs have improved the lives of thousands of people with disabilities.  These working dogs, which include hearing dogs, guide dogs, service dogs, and medical alert dogs, can provide independence and significantly enhance quality of life.  Recent research has shown that the dogs can interact with wearable technology to communicate important information to their handlers, such as a tornado siren sounding.  However, in a home environment, they rarely wear their service dog vests or harnesses even though the need for clear communication between dog and handler still exists.  The main goal of this research is to explore fundamental aspects of touchscreen interaction for dogs, to support communication between working dogs and their handlers or other humans.  Project outcomes will extend the state of the art in virtual touchscreen interfaces to animals, which is a new domain.  The work will contribute to the new field of Animal-Computer Interfaces, providing us with a better understanding of the physical and cognitive abilities of dogs, and to what extent they can interact with humans through technology.  In addition to the direct impact of further improving the quality of life for people with disabilities, the technologies developed as part of this research will ultimately benefit all users; pet dogs could be trained to use a touchscreen to request to go out, or to alert their owners of an intruder on the property, while medical alert dogs could directly summon help through a touchscreen.  Providing dogs with the ability to express specific needs to a handler could benefit the dogs as well, enhancing their safety and well-being.\r\n\r\nThe PI and her team will examine and adapt usability analysis and design techniques from the human field, such as Fitt's law and Power Law of Practice, to animal interactions, expanding the body of knowledge in interactive computing.   They will explore the physical and cognitive abilities of dogs to interact with touchscreens, as well as determining what dogs can sense and comprehend from virtual displays.  Three specific objectives will be addressed.\r\n\r\n1.\tTo examine factors in the design of dog-appropriate touchscreen affordances and to create corresponding design guidelines for developing touchscreen interfaces for working dogs.  The PI will experiment with size, color, shape, and placement of icons to determine what dogs can best comprehend.  She will also test the limits of canine cognitive load in remembering and differentiating patterns.\r\n\r\n2.\tTo create a working dog touchscreen interface prototype and perform a formal usability study with a variety of working dogs.  Building on the results of the first objective, the PI will create a touchscreen system and design a training protocol to familiarize working dogs with touchscreen tasks.  She will then perform a study employing usability metrics grounded in human interaction theory to develop fundamental canine-computer interaction theory for virtual displays.\r\n\r\n3.\tTo perform a technological probe field case study in the home of an assistance dog and handler to evaluate the effectiveness of a touchscreen system for a working dog.  Using the findings from the first two objectives, the PI will create a system and perform an ethnographic study in a home-use scenario.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Melody",
   "pi_last_name": "Jackson",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Melody M Jackson",
   "pi_email_addr": "melody@cc.gatech.edu",
   "nsf_id": "000071319",
   "pi_start_date": "2015-08-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "Zeagler",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Charles Zeagler",
   "pi_email_addr": "clintzeagler@gatech.edu",
   "nsf_id": "000584855",
   "pi_start_date": "2015-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "85 Fifth St NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303080760",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 499352.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The DogTouch project &nbsp;team sought to understand how working dogs can interact with virtual interfaces in the form of touchscreen interactions. &nbsp;The researchers implemented a system that allows virtual interfaces to be prototyped quickly and easily, and the team was able to test a variety of interface designs.</p>\n<p>They then performed three major studies. &nbsp;In the first study, they compared human movement laws (Fitts' law) to dogs selecting icons with their noses, and found that the human laws hold for dogs, creating the first canine interaction theory. &nbsp;They then experimented with color, shape, distance, and size of icons to generate guidelines for canine interaction designs. &nbsp;In addition to testing selection interfaces with dogs, the team also created and tested proportional interfaces (sliders) to determine if dogs could communicate quantified information, which was successful. &nbsp;The last study was a field study of a medical alert system that allowed a dog to call emergency services in a medical crisis.&nbsp;</p>\n<p><span>The DogTouch project opened a realm of new possibilities for dog-as-sensor systems. &nbsp;In addition to the medical alert scenario, in which a dog can summon help to avert a medical crisis, touchscreen systems could be used by cancer detection dogs to indicate what type of cancer has been found. &nbsp;A touchscreen could also be used for a bomb detection dog to indicate what explosive has been found, and what amount. &nbsp;Hearing dogs could use a touchscreen to indicate to their deaf human partners what sound they just heard. &nbsp;The possibilities for dogs to help humans, and even save lives, are signifcant. &nbsp;DogTouch represents the first formal study of virtual interactions for dogs.&nbsp;</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2018<br>\n\t\t\t\t\tModified by: Melody&nbsp;Jackson</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1525937/1525937_10392370_1543557432090_Touchscreen911Skyedited--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1525937/1525937_10392370_1543557432090_Touchscreen911Skyedited--rgov-800width.jpg\" title=\"Virtual interfaces for working dogs\"><img src=\"/por/images/Reports/POR/2018/1525937/1525937_10392370_1543557432090_Touchscreen911Skyedited--rgov-66x44.jpg\" alt=\"Virtual interfaces for working dogs\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A medical alert dog uses a touchscreen to call emergency services in a simulated medical crisis</div>\n<div class=\"imageCredit\">Melody Jackson</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Melody&nbsp;Jackson</div>\n<div class=\"imageTitle\">Virtual interfaces for working dogs</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe DogTouch project  team sought to understand how working dogs can interact with virtual interfaces in the form of touchscreen interactions.  The researchers implemented a system that allows virtual interfaces to be prototyped quickly and easily, and the team was able to test a variety of interface designs.\n\nThey then performed three major studies.  In the first study, they compared human movement laws (Fitts' law) to dogs selecting icons with their noses, and found that the human laws hold for dogs, creating the first canine interaction theory.  They then experimented with color, shape, distance, and size of icons to generate guidelines for canine interaction designs.  In addition to testing selection interfaces with dogs, the team also created and tested proportional interfaces (sliders) to determine if dogs could communicate quantified information, which was successful.  The last study was a field study of a medical alert system that allowed a dog to call emergency services in a medical crisis. \n\nThe DogTouch project opened a realm of new possibilities for dog-as-sensor systems.  In addition to the medical alert scenario, in which a dog can summon help to avert a medical crisis, touchscreen systems could be used by cancer detection dogs to indicate what type of cancer has been found.  A touchscreen could also be used for a bomb detection dog to indicate what explosive has been found, and what amount.  Hearing dogs could use a touchscreen to indicate to their deaf human partners what sound they just heard.  The possibilities for dogs to help humans, and even save lives, are signifcant.  DogTouch represents the first formal study of virtual interactions for dogs. \n\n\t\t\t\t\tLast Modified: 11/30/2018\n\n\t\t\t\t\tSubmitted by: Melody Jackson"
 }
}