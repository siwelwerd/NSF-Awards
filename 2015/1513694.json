{
 "awd_id": "1513694",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TWC: Medium: Distributed Differential Privacy",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "James Joshi",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 1200000.0,
 "awd_amount": 1200000.0,
 "awd_min_amd_letter_date": "2015-07-24",
 "awd_max_amd_letter_date": "2015-07-24",
 "awd_abstract_narration": "When data is controlled by diverse stake-holders, privacy concerns may limit sharing.  Loose federation can be an obstacle to dependability because lack of a global view can limit large-scale coordination and stability. Lack of a global view is one of the problems that contributed to the 2008 financial collapse. Ideally, an outside party could coordinate behavior, but this is often impossible: domains may be reluctant to share their local information.  Differential privacy can mitigate these concerns -- it provides a strong, attractive privacy guarantee that protects data owners from risks associated disclosure of their data, but this guarantee can be hard to achieve in distributed settings.  This project is developing practical tools for distributed private data analysis that can be used by non-experts in a variety of applications, allowing for important new analyses of distributed data. The project also includes substantial outreach activities, in the form of course development and workshop organization, and will train PhD students to be future leaders in the development of privacy technologies. \r\n\r\nThis project advances the practical theory of differential privacy in distributed settings. It has three main thrusts: 1) to extend the theory of differential privacy to work with relaxed guarantees, amenable to high-accuracy analyses when there are only a relatively small number of parties with limited ability to collude; 2) to develop programming languages and automatic verification tools capable of automatically certifying the  differential privacy properties of distributed systems, in which each party has only partial access to the data; and 3) to develop tool chains to implement differentially private algorithms in distributed settings, using (among other technologies) secure multi-party computation as a computational substrate.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "AARON",
   "pi_last_name": "ROTH",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "AARON ROTH",
   "pi_email_addr": "aaroth@cis.upenn.edu",
   "nsf_id": "000624673",
   "pi_start_date": "2015-07-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Pierce",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin C Pierce",
   "pi_email_addr": "bcpierce@cis.upenn.edu",
   "nsf_id": "000452070",
   "pi_start_date": "2015-07-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Andreas",
   "pi_last_name": "Haeberlen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andreas Haeberlen",
   "pi_email_addr": "ahae@cis.upenn.edu",
   "nsf_id": "000562850",
   "pi_start_date": "2015-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of the University of Pennsylvania",
  "perf_str_addr": "3451 Walnut Street P-221 FB",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046205",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 1200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Over the lifetime of this project, we developed a number of systems, languages, and theoretical frameworks to facilitate differentially private data analysis. Differential privacy is a notion of data privacy that allows an analyst to gain statistical insights from a large dataset without learning much about any particular person in the dataset. Differential privacy is a very strong guarantee, and in order to become confident that a data analysis method is indeed differentially private, we need to mathetically -prove- that the method satisfies differential privacy. But when this is done by hand, it can be error prone. One of the main thrusts of this project was to develop automated systems to aid in establishing differential privacy guarantees about software.&nbsp;</p>\n<p>&nbsp;</p>\n<p>These include:</p>\n<p>Adaptive Fuzz: A language that can automatically certify the correctness of differentially private programs that make adaptive choices about how to expend their privacy budget. This required developing</p>\n<p>Theory for adaptive composition: In conjunction with the above, we needed to develop the basic theory for \"privacy filters and odometers\", which are two methods for dealing with adaptive privacy parameter composition.&nbsp;</p>\n<p>Seabed: A framework for conducting private distributed data analysis leveraging cryptographic primitives.&nbsp;</p>\n<p>Honeycrisp: We developed a large scale framework for differentially private aggregation given the presence of a small trusted core of users.&nbsp;</p>\n<p>Orchard: Continuing our previous line of work on scaling differentially private analytics, we developed Orchard, a system that can answer queries about&nbsp;sensitive data that is held by millions of user devices, with strong&nbsp;differential privacy guarantees. Orchard combines high accuracy with good&nbsp;scalability, and it uses only a single untrusted party to facilitate&nbsp;the query. Moreover, whereas previous solutions that shared these properties&nbsp;were custom-built for specific queries, Orchard is general and can&nbsp;accept a wide range of queries.&nbsp;</p>\n<p>Fuzzi:&nbsp;Applying differential privacy at scale requires convenient ways to check that&nbsp;programs&nbsp;computing with sensitive data appropriately preserve privacy.&nbsp; In our paper, Testing Differential Privacy with Dual Interpreters (which was selected for a Distinguished Artifact Award at OOPSLA), we proposed a fully automated framework for TESTING differential privacy. Our framework, called FuzziDP, requires no programmer annotations, handles all previously verified or tested algorithms, and is the first fully automated framework to distinguish correct and buggy implementations of PrivTree, a probabilistically terminating algorithm that has not previously been mechanically checked.&nbsp;</p>\n<p>Persistently updating statistics: We developed two methods for persistently updating statistics under distributed models of differential privacy that pay cost equal to the number of times the statistic substantially changes, rather than the number of times we update the statistic. The first of these methods uses information and learning theoretic techniques and can be implemented in the local model; the other uses cryptographic techniques to simulate the centralized model of differential privacy.&nbsp;</p>\n<p>Processing Graph Data: An important limitation of our earlier work on&nbsp;Orchard and Honeycrisp is that these systems work only for relational&nbsp;data - each user individually has some confidential data, and the&nbsp;overall query works over the combined data; this approach does not work&nbsp;on graph-shaped data. For instance, suppose we wanted to run a query&nbsp;over the contact-tracing graph that Google's and Apple's GAEN system&nbsp;generates - e.g., to find out whether particular kinds of restaurants&nbsp;are more likely to generate superspreader events. In this case, each&nbsp;user holds a part of the graph (edges to other users they have been in&nbsp;close contact with), and we want to run a computation on the graph as a&nbsp;whole. We have developed a system called Mycelium that can do this,&nbsp;using a combination of fully-homomorphic encryption and a&nbsp;semi-centralized Mixnet.&nbsp;</p>\n<p>&nbsp;</p>\n<p>We also developed expository material as part of this project, both for graduate students and researchers, and for the general public.&nbsp;</p>\n<p>Theory for adaptive data analysis: We developed and substantially refined theory for \"adaptive data analysis\" over the course of this grant. This develops methods for using differential privacy to improve reproducibility in the empirical sciences. This incuded a substantial expository contribution, developing and making publicly available course content at http://www.adaptivedataanalysis.com</p>\n<p>The Ethical Algorithm: We (PI Roth, together with Michael Kearns) wrote and published a general audience book called \"The Ethical Algorithm\" that devotes a chapter to explaining differential privacy to a lay audience.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/29/2021<br>\n\t\t\t\t\tModified by: Aaron&nbsp;Roth</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOver the lifetime of this project, we developed a number of systems, languages, and theoretical frameworks to facilitate differentially private data analysis. Differential privacy is a notion of data privacy that allows an analyst to gain statistical insights from a large dataset without learning much about any particular person in the dataset. Differential privacy is a very strong guarantee, and in order to become confident that a data analysis method is indeed differentially private, we need to mathetically -prove- that the method satisfies differential privacy. But when this is done by hand, it can be error prone. One of the main thrusts of this project was to develop automated systems to aid in establishing differential privacy guarantees about software. \n\n \n\nThese include:\n\nAdaptive Fuzz: A language that can automatically certify the correctness of differentially private programs that make adaptive choices about how to expend their privacy budget. This required developing\n\nTheory for adaptive composition: In conjunction with the above, we needed to develop the basic theory for \"privacy filters and odometers\", which are two methods for dealing with adaptive privacy parameter composition. \n\nSeabed: A framework for conducting private distributed data analysis leveraging cryptographic primitives. \n\nHoneycrisp: We developed a large scale framework for differentially private aggregation given the presence of a small trusted core of users. \n\nOrchard: Continuing our previous line of work on scaling differentially private analytics, we developed Orchard, a system that can answer queries about sensitive data that is held by millions of user devices, with strong differential privacy guarantees. Orchard combines high accuracy with good scalability, and it uses only a single untrusted party to facilitate the query. Moreover, whereas previous solutions that shared these properties were custom-built for specific queries, Orchard is general and can accept a wide range of queries. \n\nFuzzi: Applying differential privacy at scale requires convenient ways to check that programs computing with sensitive data appropriately preserve privacy.  In our paper, Testing Differential Privacy with Dual Interpreters (which was selected for a Distinguished Artifact Award at OOPSLA), we proposed a fully automated framework for TESTING differential privacy. Our framework, called FuzziDP, requires no programmer annotations, handles all previously verified or tested algorithms, and is the first fully automated framework to distinguish correct and buggy implementations of PrivTree, a probabilistically terminating algorithm that has not previously been mechanically checked. \n\nPersistently updating statistics: We developed two methods for persistently updating statistics under distributed models of differential privacy that pay cost equal to the number of times the statistic substantially changes, rather than the number of times we update the statistic. The first of these methods uses information and learning theoretic techniques and can be implemented in the local model; the other uses cryptographic techniques to simulate the centralized model of differential privacy. \n\nProcessing Graph Data: An important limitation of our earlier work on Orchard and Honeycrisp is that these systems work only for relational data - each user individually has some confidential data, and the overall query works over the combined data; this approach does not work on graph-shaped data. For instance, suppose we wanted to run a query over the contact-tracing graph that Google's and Apple's GAEN system generates - e.g., to find out whether particular kinds of restaurants are more likely to generate superspreader events. In this case, each user holds a part of the graph (edges to other users they have been in close contact with), and we want to run a computation on the graph as a whole. We have developed a system called Mycelium that can do this, using a combination of fully-homomorphic encryption and a semi-centralized Mixnet. \n\n \n\nWe also developed expository material as part of this project, both for graduate students and researchers, and for the general public. \n\nTheory for adaptive data analysis: We developed and substantially refined theory for \"adaptive data analysis\" over the course of this grant. This develops methods for using differential privacy to improve reproducibility in the empirical sciences. This incuded a substantial expository contribution, developing and making publicly available course content at http://www.adaptivedataanalysis.com\n\nThe Ethical Algorithm: We (PI Roth, together with Michael Kearns) wrote and published a general audience book called \"The Ethical Algorithm\" that devotes a chapter to explaining differential privacy to a lay audience. \n\n \n\n\t\t\t\t\tLast Modified: 11/29/2021\n\n\t\t\t\t\tSubmitted by: Aaron Roth"
 }
}