{
 "awd_id": "1534433",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Record Linkage and Privacy-Preserving Methods for Big Data",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032927269",
 "po_email": "ceavey@nsf.gov",
 "po_sign_block_name": "Cheryl Eavey",
 "awd_eff_date": "2015-09-15",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 334243.0,
 "awd_amount": 334243.0,
 "awd_min_amd_letter_date": "2015-09-11",
 "awd_max_amd_letter_date": "2015-09-11",
 "awd_abstract_narration": "This research project will develop sound statistical and machine learning techniques for preserving privacy with linked data.  Social entities and their patterns of behavior is a crucial topic in the social sciences.  Research in this area has been invigorated by the growth of the modern information infrastructure, ease of data collection and storage, and the development of novel computational data analyses techniques.  However, in many application areas relevant and sensitive information is commonly located across multiple databases.  Data analysis is inherently impossible without merging databases, but at the cost of increasing the risk of a privacy violation.  This research will address the problem of how to perform valid statistical inference in the presence of multiple data sources, data sharing, and privacy in the age of \"big data.\"  The investigators' new modeling construct for inference and uncertainty quantification will contribute to both statistics and the many disciplines for which statistics is a principal tool.  The methods will have a wide range of applications in the social, economic, and behavioral sciences, including medicine, genetics, official statistics, and human rights violations.  The investigators will collaborate with post-doctoral researcher and with graduate and undergraduate students.  The statistical methods will be encapsulated in open-source software packages, allowing off-the-shelf use by practitioners while facilitating more detailed control and extensions.\r\n\r\nThis interdisciplinary research project will improve upon methods in record linkage and privacy using state-of-the-art techniques from statistics and machine learning.  Record linkage is the process of merging possible noisy databases with the goal of removing duplicate entries.  Privacy-preserving record linkage (PPRL) tries to identify records that refer to the same entities from multiple databases without compromising the privacy of the entities represented by these records.  The research will focus on three aims: (1) development of new Bayesian methods for PPRL, where the error can be propagated exactly across the entire linkage process and into statistical inference, including new privacy measures to capture a tradeoff between utility and risk of any individual risk in a linked database; (2) development of new robust methods for realizing synthetic data releases post-linkage with differential privacy guarantees and its relaxations to address additional layers of privacy and support broader data sharing; and (3) exploration of \"big data\" methods such as variational inference to address scalability and latent cluster exchangeability issues existing within linkage and privacy, such that the new methods can scale to multiple and large databases.  The new methods will be scalable and assess uncertainty throughout the entire linkage and privacy process and can be evaluated using Bayesian disclosure risk and Bayesian differential privacy.  The project is supported by the Methodology, Measurement, and Statistics Program and a consortium of federal statistical agencies as part of a joint activity to support research on survey and statistical methodology.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aleksandra",
   "pi_last_name": "Slavkovic",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Aleksandra B Slavkovic",
   "pi_email_addr": "sesa@psu.edu",
   "nsf_id": "000213257",
   "pi_start_date": "2015-09-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168027000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "133300",
   "pgm_ele_name": "Methodology, Measuremt & Stats"
  },
  {
   "pgm_ele_code": "772600",
   "pgm_ele_name": "Data Cyberinfrastructure"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  },
  {
   "pgm_ele_code": "880000",
   "pgm_ele_name": "SCIENCE RESOURCES STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 334243.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Over the past four years we have seen heightened public concerns regarding the privacy of personal data, regardless if they are part of government or commercial databases.&nbsp;&nbsp; At the same time, growth of modern information infrastructure, ease of data collection and storage, and development of novel computational data analyses techniques have invigorated our needs to study and understand social entities and their pattern of behaviors. However, relevant information in many application areas such as medicine, genetics, official statistics, and human rights violations is commonly located across multiple databases. Hence, many valuable data analyses are inherently impossible without merging databases, but at the cost of increasing risk of a privacy violation.&nbsp; In our project we have focused on how individual level data can be shared for research and related purposes without the identification of individuals involved. Our project has investigated a key problem of how to perform valid statistical inference in presence of multiple data sources, data sharing, and privacy in the age of &ldquo;big data.&rdquo;</p>\n<p>Privacy-protected data access, with single or multiple data sources, has been a growing topic in both the statistical and computer science literatures.&nbsp; Formal privacy methodology such as differential privacy is changing the established framework of how we publish and share sensitive data and how we perform statistical inference under privacy constraints. Differential privacy initially proposed by the cryptographic community nearly fifteen years ago has seen an explosion in theoretical and practical developments across many data domains (e.g., health, financial, genomic data, etc), and the U.S. Census is planning differentially private releases of 2020 census data. But numerous technical and practical subtleties exist that limit differential privacy usability in statistical applications.</p>\n<p>Among the theoretical and practical statistical problems we have addressed are:&nbsp; (a) development of the first class of models for record linkage and linear regression, which allows the error from the record linkage process to be propagated exactly into the linear regression, (b) the ability of researchers to remotely carry out accurate analyses on sensitive data stored behind firewalls, split across multiple centers without transfer or combination of actual data, with focus on generalized linear models and structural equation models commonly used in psychology, human development, and the behavioral sciences, demonstrated on data on kinship foster placement that came from multiple sources and could only be combined through a lengthy process with a trusted research network, (c) accounting formally for additional uncertainty introduced by a privacy method is essential in obtaining the correct statistical inference, and for the settings where each individual provides a sensitive binary value (e.g., citizen or not), we provide new differentially private methods to infer the population proportion via hypothesis tests and confidence intervals like it has been typically done without privacy constraints, (d) the release of differentially private summaries of high dimensional data such as functions and trajectories, often found in economics, finance, genetics, geoscience, anthropology, and kinesiology, illustrating our method on an application involving brain scans, and (e) the release of synthetic data essential for broadening access to sensitive data, where in a few different settings we have developed new differentially private tools, such as new measure of utility for synthetic data and subsequently a method for a method for creating differentially private synthetic datasets, a method to construct differentially-private synthetic contingency tables without using full table counts, relying on Bayesian methodology, as demonstrated on a subset of the 2016 American Community Survey (ACS) Public Use Microdata Sets (PUMS), the differentially private social network data analyzed with methods based on exponential random graph models, demonstrated with a number of large and small network examples, including ENRON email exchange dataset.</p>\n<p>Finally, we have worked to develop teaching materials on record linkage and data privacy for graduate students and researchers in statistics and other sciences, and have organized and participated in numerous cross-disciplinary workshops and conferences, including computer scientists, statisticians, and social scientists.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/20/2019<br>\n\t\t\t\t\tModified by: Aleksandra&nbsp;B&nbsp;Slavkovic</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOver the past four years we have seen heightened public concerns regarding the privacy of personal data, regardless if they are part of government or commercial databases.   At the same time, growth of modern information infrastructure, ease of data collection and storage, and development of novel computational data analyses techniques have invigorated our needs to study and understand social entities and their pattern of behaviors. However, relevant information in many application areas such as medicine, genetics, official statistics, and human rights violations is commonly located across multiple databases. Hence, many valuable data analyses are inherently impossible without merging databases, but at the cost of increasing risk of a privacy violation.  In our project we have focused on how individual level data can be shared for research and related purposes without the identification of individuals involved. Our project has investigated a key problem of how to perform valid statistical inference in presence of multiple data sources, data sharing, and privacy in the age of \"big data.\"\n\nPrivacy-protected data access, with single or multiple data sources, has been a growing topic in both the statistical and computer science literatures.  Formal privacy methodology such as differential privacy is changing the established framework of how we publish and share sensitive data and how we perform statistical inference under privacy constraints. Differential privacy initially proposed by the cryptographic community nearly fifteen years ago has seen an explosion in theoretical and practical developments across many data domains (e.g., health, financial, genomic data, etc), and the U.S. Census is planning differentially private releases of 2020 census data. But numerous technical and practical subtleties exist that limit differential privacy usability in statistical applications.\n\nAmong the theoretical and practical statistical problems we have addressed are:  (a) development of the first class of models for record linkage and linear regression, which allows the error from the record linkage process to be propagated exactly into the linear regression, (b) the ability of researchers to remotely carry out accurate analyses on sensitive data stored behind firewalls, split across multiple centers without transfer or combination of actual data, with focus on generalized linear models and structural equation models commonly used in psychology, human development, and the behavioral sciences, demonstrated on data on kinship foster placement that came from multiple sources and could only be combined through a lengthy process with a trusted research network, (c) accounting formally for additional uncertainty introduced by a privacy method is essential in obtaining the correct statistical inference, and for the settings where each individual provides a sensitive binary value (e.g., citizen or not), we provide new differentially private methods to infer the population proportion via hypothesis tests and confidence intervals like it has been typically done without privacy constraints, (d) the release of differentially private summaries of high dimensional data such as functions and trajectories, often found in economics, finance, genetics, geoscience, anthropology, and kinesiology, illustrating our method on an application involving brain scans, and (e) the release of synthetic data essential for broadening access to sensitive data, where in a few different settings we have developed new differentially private tools, such as new measure of utility for synthetic data and subsequently a method for a method for creating differentially private synthetic datasets, a method to construct differentially-private synthetic contingency tables without using full table counts, relying on Bayesian methodology, as demonstrated on a subset of the 2016 American Community Survey (ACS) Public Use Microdata Sets (PUMS), the differentially private social network data analyzed with methods based on exponential random graph models, demonstrated with a number of large and small network examples, including ENRON email exchange dataset.\n\nFinally, we have worked to develop teaching materials on record linkage and data privacy for graduate students and researchers in statistics and other sciences, and have organized and participated in numerous cross-disciplinary workshops and conferences, including computer scientists, statisticians, and social scientists.\n\n\t\t\t\t\tLast Modified: 12/20/2019\n\n\t\t\t\t\tSubmitted by: Aleksandra B Slavkovic"
 }
}