{
 "awd_id": "1526012",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Collaborative Research: Towards Interpretable Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 248790.0,
 "awd_amount": 248790.0,
 "awd_min_amd_letter_date": "2015-08-19",
 "awd_max_amd_letter_date": "2015-08-19",
 "awd_abstract_narration": "This research project investigates the design and development of machine learning algorithms that make decisions that are interpretable by humans.  As predictions of machine learning models are increasingly used in making decisions with critical consequences (e.g., in medicine or economics), it is important that decision makers understand the rationale behind these predictions.  The project defines interpretable algorithms through three key properties; Simplicity: intuitively comprehensible by users who are not experts in machine learning, Verifiability: a clear relationship between input features and model output, and Actionability: For a given input and desired output, the user should be able to identify changes to the input features that transform the model prediction to the desired output.  The project investigates how to design distance metrics supporting simplicity and verifiability, as well as algorithms to identify input changes to change outputs.  The project will be evaluated in a medical context, addressing the problem of early detection of hospital patients at risk of sudden deterioration.\r\n\r\nThis work builds on the well-understood k-Nearest-Neighbor classifier, which would inherently seem to provide simplicity and verifiability.  The challenge is in high dimensions, e.g., when used for document classification; differences are spread across more dimensions than are humanly comprehensible.  The project uses novel dimensionality reduction approaches to create dissimilarity metrics that are interpretable and accurate.  Visualization techniques to present this data will be explored, including techniques supporting more complex classification approaches such as ensembles.  The project investigates novel methods for delivering actionability in machine learning algorithms by identifying changes that can truly transform an entity's class membership - a problem that has recently been identified as surprisingly difficult.  A secondary outcome will be improvements in classifier robustness, as small changes that change class membership are a good indication of non-robustness.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yixin",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yixin Chen",
   "pi_email_addr": "chen@cse.wustl.edu",
   "nsf_id": "000302236",
   "pi_start_date": "2015-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington University",
  "inst_street_address": "1 BROOKINGS DR",
  "inst_street_address_2": "",
  "inst_city_name": "SAINT LOUIS",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "3147474134",
  "inst_zip_code": "631304862",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MO01",
  "org_lgl_bus_name": "WASHINGTON UNIVERSITY, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "L6NFUM28LQM5"
 },
 "perf_inst": {
  "perf_inst_name": "Washington University",
  "perf_str_addr": "One Brookings Drive",
  "perf_city_name": "Saint Louis",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "631304899",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 248790.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Machine learning has achieved great success in many applications. However, one of the key problems with machine learning algorithms is that their output is not always comprehensible to humans. Especially neural networks, which are now so popular, output probability estimates that supposedly communicate the certainty in a prediction. These probability estimates however are highly misleading and may well be the key reason why neural networks are considered so noninterpretable. If for example a neural network outputs that an image contains a specific object with 90% probability this does not mean that it is right 90% of the time.</p>\n<p>In this project, we have investigated how neural networks can be calibrated so that their outputs are \"truthful\" and really do match reality and human expectations. We developed and compared metrics to investigate how uncalibrated neural networks are and then investigated various approaches to recalibrate neural networks. Ultimately we found that this can be done with pretty high accuracy most of the time.</p>\n<p>Another key question that arises in the context of interpretable neural networks is what kind of feature spaces deep neural networks learn. There is a general misconception that neural networks are very accurate, however that their intermediate representations are a \"black box\". We decided to investigate this thoroughly and conjectured that the intermediate features representations of convolutional neural networks do indeed learn a linearized version of the input image manifold. Proving this conjecture is very hard and has so far been elusive. However, we managed to come up with an experimental set up that provides insights into whether this claim is true or not. If neural networks really learn a linearized version of the manifold, then traversals along the manifold must become linear in feature space. For example, if you have a picture of a man without facial hair, adding facial hair would move that image towards a completely different location in the image manifold. Similarly, aging people moves images from areas of the manifold populated by images of young people towards areas populated by images of older people. These transformations are highly nonlinear if performed in pixel space, however we managed to show that if you perform these transformations in deep feature space they reduce to a simple linear translation. By reconstructing the manipulated image we can show that the final outcome is indeed on the image manifold. <br /> <strong><br /> </strong>As a key area of application, we have applied our interpretable learning techniques to the medical domain. We have collaborated with the School of Medicine at Washington University in St. Louis and worked on predicting hospital readmissions for hospitalized patients. For this task, we need not only accurate predictions, but also explanations on potential risks. We have developed such a model that can be used by physicians.</p>\n<p>For broader impacts, we have disseminated our results in conferences, seminars, and workshops. Our research effort to understand the internal workings of neural networks has led us to designs of much more compact in&nbsp;your network architectures. These especially suitable for mobile applications. It is therefore highly likely that our research will&nbsp;have a significant impact through technology transfer. We have trained graduate and undergraduate students, and integrated research into the teaching of our graduate courses.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/25/2020<br>\n\t\t\t\t\tModified by: Yixin&nbsp;Chen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMachine learning has achieved great success in many applications. However, one of the key problems with machine learning algorithms is that their output is not always comprehensible to humans. Especially neural networks, which are now so popular, output probability estimates that supposedly communicate the certainty in a prediction. These probability estimates however are highly misleading and may well be the key reason why neural networks are considered so noninterpretable. If for example a neural network outputs that an image contains a specific object with 90% probability this does not mean that it is right 90% of the time.\n\nIn this project, we have investigated how neural networks can be calibrated so that their outputs are \"truthful\" and really do match reality and human expectations. We developed and compared metrics to investigate how uncalibrated neural networks are and then investigated various approaches to recalibrate neural networks. Ultimately we found that this can be done with pretty high accuracy most of the time.\n\nAnother key question that arises in the context of interpretable neural networks is what kind of feature spaces deep neural networks learn. There is a general misconception that neural networks are very accurate, however that their intermediate representations are a \"black box\". We decided to investigate this thoroughly and conjectured that the intermediate features representations of convolutional neural networks do indeed learn a linearized version of the input image manifold. Proving this conjecture is very hard and has so far been elusive. However, we managed to come up with an experimental set up that provides insights into whether this claim is true or not. If neural networks really learn a linearized version of the manifold, then traversals along the manifold must become linear in feature space. For example, if you have a picture of a man without facial hair, adding facial hair would move that image towards a completely different location in the image manifold. Similarly, aging people moves images from areas of the manifold populated by images of young people towards areas populated by images of older people. These transformations are highly nonlinear if performed in pixel space, however we managed to show that if you perform these transformations in deep feature space they reduce to a simple linear translation. By reconstructing the manipulated image we can show that the final outcome is indeed on the image manifold. \n \n As a key area of application, we have applied our interpretable learning techniques to the medical domain. We have collaborated with the School of Medicine at Washington University in St. Louis and worked on predicting hospital readmissions for hospitalized patients. For this task, we need not only accurate predictions, but also explanations on potential risks. We have developed such a model that can be used by physicians.\n\nFor broader impacts, we have disseminated our results in conferences, seminars, and workshops. Our research effort to understand the internal workings of neural networks has led us to designs of much more compact in your network architectures. These especially suitable for mobile applications. It is therefore highly likely that our research will have a significant impact through technology transfer. We have trained graduate and undergraduate students, and integrated research into the teaching of our graduate courses.\n\n\t\t\t\t\tLast Modified: 04/25/2020\n\n\t\t\t\t\tSubmitted by: Yixin Chen"
 }
}