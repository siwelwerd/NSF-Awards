{
 "awd_id": "1525398",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Bypassing the L1 Norm: Non-Convex Regularization, Convex Optimization, and Sparse Signal Processing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2015-08-01",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 315175.0,
 "awd_amount": 315175.0,
 "awd_min_amd_letter_date": "2015-07-08",
 "awd_max_amd_letter_date": "2015-07-08",
 "awd_abstract_narration": "Numerous problems in nonlinear signal processing and data science are successfully tackled through their formulation as ill-conditioned inverse problems. Such problems arise in medical imaging, speech and audio processing, biomedical time-series analysis, manufacturing, and remote sensing. Many ongoing advances in these fields are based on sparse regularization (i.e., the modeling of data as highly compressible when appropriately transformed). This research program aims to advance mathematical and computational tools to pose and solve ill-conditioned inverse problems by developing new techniques for sparse regularization. \r\n\r\nConvex formulations of problems arising in science and engineering are attractive because one may leverage a wealth of algorithms that are globally convergent and computationally efficient, even for large-scale non-smooth problems. While the L1 norm is a cornerstone of convex sparse regularization, it tends to under-estimate signal values. Non-convex sparse regularization is therefore a popular and valuable alternative; however, it is hampered by several complications: the optimal solution is generally a discontinuous function of the data and the objective function to be minimized generally possesses many sub-optimal local minima which can entrap optimization algorithms. \r\n\r\nThis research aims to exploit the effectiveness of non-convex sparse regularization without forgoing the principles of convex optimization, by applying the principle of convex relaxation to the objective function as a whole, rather than to the regularizer alone. In particular, this program undertakes the development of new non-separable non-convex penalty functions that ensure the convexity of the objective function (comprising data fidelity and sparse regularization terms) to be minimized.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ivan",
   "pi_last_name": "Selesnick",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Ivan W Selesnick",
   "pi_email_addr": "selesi@nyu.edu",
   "nsf_id": "000396571",
   "pi_start_date": "2015-07-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100121019",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 315175.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research project developed new methods for <strong>sparsity-based signal processing</strong>, including new problem formulations, algorithms, and software implementations. The general sparsity-based approach to signal processing provides a systematic principled framework for developing nonlinear methods for filtering, event-detection, resolution improvement, artifact reduction, the estimation of missing data, and other signal processing tasks.&nbsp;</p>\n<p>We have demonstrated the newly developed sparse signal processing techniques in biomedical time-series analysis, imaging, and machine fault monitoring. The novelty of the newly developed methods is the way in which they draw simultaneously on concepts of <strong>convex</strong> and <strong>non-convex</strong> functions. Algorithms, software, and theory for <strong>convex</strong> optimization have played an important role over the last decade and in modern-day signal processing. At the same time, <strong>non-convex</strong> penalty functions (also called regularizers), are widely used in the development of signal processing algorithms, as they are widely recognized as providing improved performance in some situations, compared to convex penalty functions. Generally, the convex and non-convex approaches to signal processing algorithm design have been seen as two distinct, mutually exclusive, approaches, each with their advantages and disadvantages.&nbsp;</p>\n<p>A goal of this research project has been to create new functions that allow for the development of algorithms that simultaneously leverage the merits of both convex optimization and non-convex regularization, in ways which were not previously possible. <em>The newly developed techniques show that there exists previously unrecognized avenues at the intersection of convex and non-convex approaches.</em></p>\n<p>At the center of this research is a newly developed alternative to the L1 norm. The L1 norm is a cornerstone of sparse signal processing, due largely to it being a <strong>convex</strong> function. When used as the penalty function (regularizer) of least squares problems, the L1 norm leads to convex optimization problems with no extraneous suboptimal local minima, and allows us to leverage globally convergent, computationally efficient, scalable convex optimization algorithms. However, the L1 norm systematically underestimates sparse signal values. On the other hand, <strong>non-convex</strong> regularizers can estimate high-amplitude sparse signal values more accurately than the L1 norm, so they are often preferred; but, then the objective function to be minimized, being non-convex and high dimensional, will generally possess many extraneous suboptimal local minima which entrap iterative optimization algorithms. The alternative to the L1 norm, developed in this research project, is a new <strong>non-convex</strong> multivariate penalty (regularizer) that preserves the <strong>convexity</strong> of the objective function to be minimized. We have also extended the technique so that it can be used for finding low-rank matrix approximations and other related inverse problems.&nbsp;</p>\n<p>This research has bridged the convex and the non-convex approaches (which are usually incompatible); thereby opening the benefits of both. A key discovery in this research is a novel way to use concepts from convex analysis to design non-separable non-convex multivariate functions with specialized properties enabling the construction of convex objective functions with non-convex regularizers. In particular, the construction involves a novel use of infimal convolution, an idea from convex analysis for combining convex functions to construct new non-trivial convex functions. The research is described in multiple journal papers and publicly available software packages.</p>\n<p>This project has also involved educational components. In addition to mentoring undergraduate and graduate students on signal processing research, the project PI has developed, revised, and taught an annual Python-based course on real-time digital signal processing at NYU Tandon. The enrollment of this course, open to both undergraduate and graduate students, has grown to 180 students in Fall 2019 (up from 63 in Fall 2015).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/29/2019<br>\n\t\t\t\t\tModified by: Ivan&nbsp;W&nbsp;Selesnick</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis research project developed new methods for sparsity-based signal processing, including new problem formulations, algorithms, and software implementations. The general sparsity-based approach to signal processing provides a systematic principled framework for developing nonlinear methods for filtering, event-detection, resolution improvement, artifact reduction, the estimation of missing data, and other signal processing tasks. \n\nWe have demonstrated the newly developed sparse signal processing techniques in biomedical time-series analysis, imaging, and machine fault monitoring. The novelty of the newly developed methods is the way in which they draw simultaneously on concepts of convex and non-convex functions. Algorithms, software, and theory for convex optimization have played an important role over the last decade and in modern-day signal processing. At the same time, non-convex penalty functions (also called regularizers), are widely used in the development of signal processing algorithms, as they are widely recognized as providing improved performance in some situations, compared to convex penalty functions. Generally, the convex and non-convex approaches to signal processing algorithm design have been seen as two distinct, mutually exclusive, approaches, each with their advantages and disadvantages. \n\nA goal of this research project has been to create new functions that allow for the development of algorithms that simultaneously leverage the merits of both convex optimization and non-convex regularization, in ways which were not previously possible. The newly developed techniques show that there exists previously unrecognized avenues at the intersection of convex and non-convex approaches.\n\nAt the center of this research is a newly developed alternative to the L1 norm. The L1 norm is a cornerstone of sparse signal processing, due largely to it being a convex function. When used as the penalty function (regularizer) of least squares problems, the L1 norm leads to convex optimization problems with no extraneous suboptimal local minima, and allows us to leverage globally convergent, computationally efficient, scalable convex optimization algorithms. However, the L1 norm systematically underestimates sparse signal values. On the other hand, non-convex regularizers can estimate high-amplitude sparse signal values more accurately than the L1 norm, so they are often preferred; but, then the objective function to be minimized, being non-convex and high dimensional, will generally possess many extraneous suboptimal local minima which entrap iterative optimization algorithms. The alternative to the L1 norm, developed in this research project, is a new non-convex multivariate penalty (regularizer) that preserves the convexity of the objective function to be minimized. We have also extended the technique so that it can be used for finding low-rank matrix approximations and other related inverse problems. \n\nThis research has bridged the convex and the non-convex approaches (which are usually incompatible); thereby opening the benefits of both. A key discovery in this research is a novel way to use concepts from convex analysis to design non-separable non-convex multivariate functions with specialized properties enabling the construction of convex objective functions with non-convex regularizers. In particular, the construction involves a novel use of infimal convolution, an idea from convex analysis for combining convex functions to construct new non-trivial convex functions. The research is described in multiple journal papers and publicly available software packages.\n\nThis project has also involved educational components. In addition to mentoring undergraduate and graduate students on signal processing research, the project PI has developed, revised, and taught an annual Python-based course on real-time digital signal processing at NYU Tandon. The enrollment of this course, open to both undergraduate and graduate students, has grown to 180 students in Fall 2019 (up from 63 in Fall 2015).\n\n\t\t\t\t\tLast Modified: 11/29/2019\n\n\t\t\t\t\tSubmitted by: Ivan W Selesnick"
 }
}