{
 "awd_id": "1453432",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Privacy-preserving learning for distributed data",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 540000.0,
 "awd_amount": 556200.0,
 "awd_min_amd_letter_date": "2015-01-23",
 "awd_max_amd_letter_date": "2018-07-17",
 "awd_abstract_narration": "Medical technologies such as imaging and sequencing make it possible to gather massive amounts of information at increasingly lower cost. Sharing data from studies can advance scientific understanding and improve healthcare outcomes. Concern about patient privacy, however, can preclude open data sharing, thus hampering progress in understanding stigmatized conditions such as mental health disorders.  This research seeks to understand how to analyze and learn from sensitive data held at different sites (such as medical centers) in a way that quantifiably and rigorously protects the privacy of the data.  \r\n\r\nThe framework used in this research is differential privacy, a recently-proposed model for measuring privacy risk in data sharing.  Differentially private algorithms provide approximate (noisy) answers to protect sensitive data, involving a tradeoff between privacy and utility.  This research studies how to combine private approximations from different sites to improve the overall quality or utility of the result. The main goals of this research are to understand the fundamental limits of private data sharing, to design algorithms for making private approximations and rules for combining them, and to understand the consequences of sites having more complex privacy and sharing restrictions.  The methods used to address these problems are a mix of mathematical techniques from statistics, computer science, and electrical engineering.\r\n\r\nThe educational component of this research will involve designing introductory university courses and material on data science, undergraduate research projects, curricular materials for graduate courses, and outreach to the growing data-hacker community via presentations, tutorial materials, and open-source software. \r\n\r\nThe primary aim of this research is bridge the gap between theory and practice by developing algorithmic principles for practical privacy-preserving algorithms. These algorithms will be validated on neuroimaging data used to understand and diagnose mental health disorders. Implementing the results of this research will create a blueprint for building practical privacy-preserving learning for research in healthcare and other fields.  The tradeoffs between privacy and utility in distributed systems lead naturally to more general questions of cost-benefit tradeoffs for learning problems, and the same algorithmic principles will shed light on information processing and machine learning in general distributed systems where messages may be noisy or corrupted.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anand",
   "pi_last_name": "Sarwate",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Anand D Sarwate",
   "pi_email_addr": "anand.sarwate@rutgers.edu",
   "nsf_id": "000608994",
   "pi_start_date": "2015-01-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "94 Brett Road",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7797",
   "pgm_ref_txt": "COMM & INFORMATION FOUNDATIONS"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 215865.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 8200.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 218356.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 113779.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p style=\"margin-top: 0px !important; margin-right: 0px; margin-bottom: 15px; margin-left: 0px; caret-color: #000000; color: #000000; font-family: Helvetica, arial, sans-serif; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; background-color: #ffffff; text-decoration: none;\">Medical technologies such as imaging and sequencing make it possible to gather massive amounts of information at increasingly lower cost. Sharing data from studies can advance scientific understanding and improve healthcare outcomes. Concerns about patient privacy, however, can preclude open data sharing, thus hampering progress in understanding stigmatized conditions such as mental health disorders. Suppose there are several research groups studying a brain disorder (e.g. early-onset Alzheimers). They want to collaborate to see if they can learn common patterns in the MRI images from their research subjects. However, due to privacy concerns, each site wants to protect against the risk that someone could identify one of their research subjects.</p>\n<p style=\"margin: 15px 0px; caret-color: #000000; color: #000000; font-family: Helvetica, arial, sans-serif; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; background-color: #ffffff; text-decoration: none;\">This research seeks to understand how to analyze and learn from sensitive data held at different sites in a way that quantifiably and rigorously protects the privacy of the data. The framework used for this work is differential privacy, which measures the privacy risk when publishing information derived from sensitive data. We can guarantee privacy by providing only approximate answers, which means we trade off accuracy (or some other measure of utility) with privacy risk. In this project we looked at several different aspects of applying differential privacy to scenarios similar to the collaborative research example above.<span class=\"Apple-converted-space\">&nbsp;</span></p>\n<p style=\"margin: 15px 0px; caret-color: #000000; color: #000000; font-family: Helvetica, arial, sans-serif; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; background-color: #ffffff; text-decoration: none;\">We first looked at the negative impact of differentally private approximations on utility. This impact is mitigated by more data; a larger sample size can enable better utility with lower privacy risk. Turning the question around, we ask: for a given desired level of privacy risk and utility, how much data do we need to meet both criteria? Comparing this to the amount of data needed when privacy is not a concern lets us quantify the cost of privacy in terms of the additional data needed. We looked at this cost in several illustrative but simple examples in a model of local differential privacy. This allowed us to understand if modeling assumptions can help improve the privacy-utility tradeoff.</p>\n<p style=\"margin: 15px 0px; caret-color: #000000; color: #000000; font-family: Helvetica, arial, sans-serif; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; background-color: #ffffff; text-decoration: none;\">A second branch of this work involved developing algorithms for decentralized learning and estimation using differential privacy. Here we focused on algorithms which try to estimate or factorize matrices that characterize the statistical structure of the underlying data. Some of the challenges involved in designing these methods are the potentially large number of messages that need to be exchanged, the total privacy risk and the quality of the result. This adds a third dimension (communication cost) to the privacy-utility tradeoff. When thinking about collaborative research projects, there are two points of comparison to evaluate these tradeoffs: one where a site simply uses its own data to perform the estimation (no collaboration, or a local solution) and one where the data from all sites is collected in one place (a global solution). This allows us to evaluate the cost of privacy, the cost of decentralization, and the benefits of collaboration.</p>\n<p style=\"margin: 15px 0px; caret-color: #000000; color: #000000; font-family: Helvetica, arial, sans-serif; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; background-color: #ffffff; text-decoration: none;\">The last part of this research involved putting the principles and ideas developed in the first two parts into practice. We worked with researchers in neuroimaging to implement and evaluate several of our methods, designing decentralized algorithms for key tasks in neuroimaging data processing pipelines. Through this process we also examined how some additional resources could enhance the privacy-utility tradeoff. In particular, we saw that if the sites could generate correlated random numbers, independent of the data, then we could achieve much better tradeoffs, sometimes as good as those for the global solution. We demonstrated this in a neuroimaging application to show that reasonable privacy-utility tradeoffs are possible when the consortium collectively has enough data.</p>\n<p style=\"margin: 15px 0px; caret-color: #000000; color: #000000; font-family: Helvetica, arial, sans-serif; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; background-color: #ffffff; text-decoration: none;\"><strong>Intellectual Merit:</strong><span class=\"Apple-converted-space\">&nbsp;</span>This proposal helped bridge the gap between the theory and practice of distributed privacy-preserving information processing. Overall, this project has led to a number of different insights around how privacy works in networked systems. In the intervening years, some of the models for decentralized estimaton have been rebranded as \"federated learning.\" While we take medical research as a canonical example, this project encompasses many other application domains in which data holders wish to collaborate but have an oblication to protect the privacy of their data. We hope that some of the results developed in this project can inform the design of future privacy-preserving federated learning algorithms that can be applied to scientific collaborations.</p>\n<p style=\"margin-top: 15px; margin-right: 0px; margin-bottom: 0px !important; margin-left: 0px; caret-color: #000000; color: #000000; font-family: Helvetica, arial, sans-serif; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; background-color: #ffffff; text-decoration: none;\"><strong>Broader Impact:</strong><span class=\"Apple-converted-space\">&nbsp;</span>This work has directly informed the design of a collaborative research system for neuroimaging research. Evaluating algorithms within this system will lead to new insights in what the possible privacy-utility tradeoffs are for real data. The project supported several undergraduate students through diversity-focused REU programs, many of whom have gone on to successful graduate careers. Finally, this project resulted in tutorials for signal processing and machine learning audiences as well as a short course for statisticans on differential privacy to help bring differential privacy into statistical practice.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/18/2021<br>\n\t\t\t\t\tModified by: Anand&nbsp;Sarwate</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Medical technologies such as imaging and sequencing make it possible to gather massive amounts of information at increasingly lower cost. Sharing data from studies can advance scientific understanding and improve healthcare outcomes. Concerns about patient privacy, however, can preclude open data sharing, thus hampering progress in understanding stigmatized conditions such as mental health disorders. Suppose there are several research groups studying a brain disorder (e.g. early-onset Alzheimers). They want to collaborate to see if they can learn common patterns in the MRI images from their research subjects. However, due to privacy concerns, each site wants to protect against the risk that someone could identify one of their research subjects.\nThis research seeks to understand how to analyze and learn from sensitive data held at different sites in a way that quantifiably and rigorously protects the privacy of the data. The framework used for this work is differential privacy, which measures the privacy risk when publishing information derived from sensitive data. We can guarantee privacy by providing only approximate answers, which means we trade off accuracy (or some other measure of utility) with privacy risk. In this project we looked at several different aspects of applying differential privacy to scenarios similar to the collaborative research example above. \nWe first looked at the negative impact of differentally private approximations on utility. This impact is mitigated by more data; a larger sample size can enable better utility with lower privacy risk. Turning the question around, we ask: for a given desired level of privacy risk and utility, how much data do we need to meet both criteria? Comparing this to the amount of data needed when privacy is not a concern lets us quantify the cost of privacy in terms of the additional data needed. We looked at this cost in several illustrative but simple examples in a model of local differential privacy. This allowed us to understand if modeling assumptions can help improve the privacy-utility tradeoff.\nA second branch of this work involved developing algorithms for decentralized learning and estimation using differential privacy. Here we focused on algorithms which try to estimate or factorize matrices that characterize the statistical structure of the underlying data. Some of the challenges involved in designing these methods are the potentially large number of messages that need to be exchanged, the total privacy risk and the quality of the result. This adds a third dimension (communication cost) to the privacy-utility tradeoff. When thinking about collaborative research projects, there are two points of comparison to evaluate these tradeoffs: one where a site simply uses its own data to perform the estimation (no collaboration, or a local solution) and one where the data from all sites is collected in one place (a global solution). This allows us to evaluate the cost of privacy, the cost of decentralization, and the benefits of collaboration.\nThe last part of this research involved putting the principles and ideas developed in the first two parts into practice. We worked with researchers in neuroimaging to implement and evaluate several of our methods, designing decentralized algorithms for key tasks in neuroimaging data processing pipelines. Through this process we also examined how some additional resources could enhance the privacy-utility tradeoff. In particular, we saw that if the sites could generate correlated random numbers, independent of the data, then we could achieve much better tradeoffs, sometimes as good as those for the global solution. We demonstrated this in a neuroimaging application to show that reasonable privacy-utility tradeoffs are possible when the consortium collectively has enough data.\nIntellectual Merit: This proposal helped bridge the gap between the theory and practice of distributed privacy-preserving information processing. Overall, this project has led to a number of different insights around how privacy works in networked systems. In the intervening years, some of the models for decentralized estimaton have been rebranded as \"federated learning.\" While we take medical research as a canonical example, this project encompasses many other application domains in which data holders wish to collaborate but have an oblication to protect the privacy of their data. We hope that some of the results developed in this project can inform the design of future privacy-preserving federated learning algorithms that can be applied to scientific collaborations.\nBroader Impact: This work has directly informed the design of a collaborative research system for neuroimaging research. Evaluating algorithms within this system will lead to new insights in what the possible privacy-utility tradeoffs are for real data. The project supported several undergraduate students through diversity-focused REU programs, many of whom have gone on to successful graduate careers. Finally, this project resulted in tutorials for signal processing and machine learning audiences as well as a short course for statisticans on differential privacy to help bring differential privacy into statistical practice.\n\n\t\t\t\t\tLast Modified: 11/18/2021\n\n\t\t\t\t\tSubmitted by: Anand Sarwate"
 }
}