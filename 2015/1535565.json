{
 "awd_id": "1535565",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AitF: FULL: Query Processing with Optimal Communication Cost",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2015-08-15",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 720000.0,
 "awd_amount": 720000.0,
 "awd_min_amd_letter_date": "2015-08-07",
 "awd_max_amd_letter_date": "2015-08-07",
 "awd_abstract_narration": "Big Data analytics is changing traditional query processing in two ways.  The first is a shift from single server or small-scale parallel relational databases to massively distributed architectures, where hundreds or thousands of servers are used during the computation of a single query.  The second is an increased complexity in the queries being issued, from single- or star-joins, to complex graph-like structured queries.  This project develops new algorithms for query processing over large distributed systems, which are optimized for the cost of communication, then implements and evaluates these algorithms in an open-source big data management system and service.\r\n\r\nThe project studies a new approach to query evaluation that computes the entire query at once, replacing the traditional approach based on a query plan. The theoretical part of this project builds on a new model, called the Massively Parallel Communication model (MPC), where the communication is the only cost. The system development is performed over the Myria big data management system and service.\r\n\r\nThe Intellectual Merit of the project consists in advancing the state of the art in both the theory and systems approaches to query evaluation in modern, massive-scale shared-nothing clusters.  It develops new, fundamental algorithms for processing queries over massively distributed architectures, with a provably optimal communication cost. The project implements and deploys these algorithms in a system, validating and informing the theoretical model. In particular, the project makes the following contributions: it develops provably optimal, one-round algorithms for skewed data; it studies how and when multiple rounds can be used to further reduce the communication cost; it experiments with these novel algorithms on clusters with up to 1000 worker processes; and it develops a new theoretical model for the communication cost on large shared-nothing architectures with heterogeneous hardware.\r\n\r\nThe Broader Impact of the project is to contribute to a new architecture for massively parallel query processing, where the traditional multi-step, single-join query evaluation approaches are replaced with novel, single-step, multi-join algorithms. This change has the potential to lead to more efficient big data analytics engines, allowing data analysts to explore large datasets more efficiently. As an immediate application, the project will impact the domain scientists who already use the Myria big data management system and service. All algorithmic discoveries in this project will be implemented in the Myria system, and will significantly improve query performance, allowing domain scientists to conduct more complex analytics and explorations over their data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dan",
   "pi_last_name": "Suciu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dan Suciu",
   "pi_email_addr": "suciu@cs.washington.edu",
   "nsf_id": "000218785",
   "pi_start_date": "2015-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Magdalena",
   "pi_last_name": "Balazinska",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Magdalena Balazinska",
   "pi_email_addr": "magda@cs.washington.edu",
   "nsf_id": "000094498",
   "pi_start_date": "2015-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "723900",
   "pgm_ele_name": "Algorithms in the Field"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "012Z",
   "pgm_ref_txt": "AitF FULL Projects"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 720000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project has made significant contributions to algorithms design for distributed procesing of big data.&nbsp; For best analysis, data scientists need to integrate multiple tables before computing their aggregate statistics, and this integration requires performing multiple joins over very large tables.&nbsp; Current distributed systems perform one join at a time, and each join requires a global reshuffling of the tables being joined.&nbsp; The algorithms developed in this project aim at performing a single reshuffling step, then computing all joins locally on the servers.&nbsp; Early in the project, we have identified that skew in the data represents a major bottleneck, because it leads to uneven distribution of the data.&nbsp; We have developed the first algorithm that is guaranteed to compute queries efficiently even if the data is skewed.&nbsp; The main insight was to use additional communication rounds to identify the skewed data items, and handle them separately.&nbsp; Our algorithm represented a breakthrough, although it was limited to binary relations (i.e. tables with only two attributes), and was rather complicated.&nbsp; Several other research groups have since them extended our algorithm to overcome these limitations.</p>\n<p><br />A second, unexpected outcome of this project was an entirely new approach to cardinality estimation, which was inspired by the distributed query processing algorithm.&nbsp; Traditional SQL query engines, both centralized and distributed, examine multiple query plans before deciding which plan to run, and they all suffer from the limitations of the cardinality estimation function, which is needed in order to estimate the costs of alternative plans.&nbsp; We observed that the same principle of distributing the data for query processing can be used to do cardinality estimation: the data is partitioned logically, then for each partition we use a simple formula to compute an upper bound of the number of tuples in returned by the query on that partition, and add up these bounds.&nbsp; We called this approach \"pessimistic cardinality estimation\", because it provides a guaranteed upper bound on the cardinality of the query.&nbsp; We built an experimental system by&nbsp; incorporating pessimistic cardinality estimation in Postgres, and proved empirically that it can lead to significantly better SQL query plans for expensive queries.</p>\n<p><br />A third major outcome of this project was an architecture for future database management systems that will need to manage samples as first class citizens.&nbsp; Data scientists have relied on samples to analyze populations of interest for decades.&nbsp; With the increase in the number of public data repositories, sample data becomes easier to access. It has not, however, become easier to analyze, because it is often biased with an unknown sampling probability, meaning data scientists must manually debias the sample with custom techniques to avoid inaccurate results.&nbsp; In addition to public data repositories, data scientists often have access to accurate, global aggregate information, for example from the Census Bureau.&nbsp; We have initiated the study of how database systems can be enhanced to answer global data analytics queries over biased samples and aggregate information.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/19/2020<br>\n\t\t\t\t\tModified by: Dan&nbsp;Suciu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project has made significant contributions to algorithms design for distributed procesing of big data.  For best analysis, data scientists need to integrate multiple tables before computing their aggregate statistics, and this integration requires performing multiple joins over very large tables.  Current distributed systems perform one join at a time, and each join requires a global reshuffling of the tables being joined.  The algorithms developed in this project aim at performing a single reshuffling step, then computing all joins locally on the servers.  Early in the project, we have identified that skew in the data represents a major bottleneck, because it leads to uneven distribution of the data.  We have developed the first algorithm that is guaranteed to compute queries efficiently even if the data is skewed.  The main insight was to use additional communication rounds to identify the skewed data items, and handle them separately.  Our algorithm represented a breakthrough, although it was limited to binary relations (i.e. tables with only two attributes), and was rather complicated.  Several other research groups have since them extended our algorithm to overcome these limitations.\n\n\nA second, unexpected outcome of this project was an entirely new approach to cardinality estimation, which was inspired by the distributed query processing algorithm.  Traditional SQL query engines, both centralized and distributed, examine multiple query plans before deciding which plan to run, and they all suffer from the limitations of the cardinality estimation function, which is needed in order to estimate the costs of alternative plans.  We observed that the same principle of distributing the data for query processing can be used to do cardinality estimation: the data is partitioned logically, then for each partition we use a simple formula to compute an upper bound of the number of tuples in returned by the query on that partition, and add up these bounds.  We called this approach \"pessimistic cardinality estimation\", because it provides a guaranteed upper bound on the cardinality of the query.  We built an experimental system by  incorporating pessimistic cardinality estimation in Postgres, and proved empirically that it can lead to significantly better SQL query plans for expensive queries.\n\n\nA third major outcome of this project was an architecture for future database management systems that will need to manage samples as first class citizens.  Data scientists have relied on samples to analyze populations of interest for decades.  With the increase in the number of public data repositories, sample data becomes easier to access. It has not, however, become easier to analyze, because it is often biased with an unknown sampling probability, meaning data scientists must manually debias the sample with custom techniques to avoid inaccurate results.  In addition to public data repositories, data scientists often have access to accurate, global aggregate information, for example from the Census Bureau.  We have initiated the study of how database systems can be enhanced to answer global data analytics queries over biased samples and aggregate information.\n\n \n\n\t\t\t\t\tLast Modified: 09/19/2020\n\n\t\t\t\t\tSubmitted by: Dan Suciu"
 }
}