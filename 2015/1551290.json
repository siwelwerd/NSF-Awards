{
 "awd_id": "1551290",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER:  Discriminative Spatiotemporal Models for Recognizing Humans, Objects, and their Interactions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2016-05-31",
 "tot_intn_awd_amt": 106011.0,
 "awd_amount": 106011.0,
 "awd_min_amd_letter_date": "2015-09-21",
 "awd_max_amd_letter_date": "2015-09-21",
 "awd_abstract_narration": "One of the goals of computer vision is to build a system that can see people and recognize their activities. Human actions are rarely performed in isolation -- the surrounding environment, nearby objects, and nearby humans affect the nature of the performed activity.\r\nExamples include actions such as \"eating\" and \"shaking hands.\" The research goal of this project is to approach human performance in understanding videos of activities defined by human-object and human-human interactions.\r\n\r\nThis project makes use of structured, contextual representations to make predictions given spatiotemporal data. It does so by extending recent successful work on object recognition to the space-time domain, introducing extensions for spatiotemporal grouping and contextual modeling. Video enables the extraction of additional dynamic cues absent in static images, but this poses additional computational burdens that are addressed through algorithmic innovations for approximate parsing and large-scale discriminative learning.\r\n\r\nTo place activity recognition on firm quantitative ground, the proposed models are evaluated using concrete metrics based on activities of daily living (ADL) and human proxemic models from the medical and anthropological communities. Examples include systems for automated monitoring of stroke patients interacting with everyday objects and automated analysis of crisis response team interactions during emergency drills. This project produces non-scripted, real-world, labeled action recognition datasets, of benefit to the research community as a whole.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Deva",
   "pi_last_name": "Ramanan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Deva Ramanan",
   "pi_email_addr": "deva@cs.cmu.edu",
   "nsf_id": "000083629",
   "pi_start_date": "2015-09-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 9220.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 96791.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explored spatiotemporal models for understanding humans, objects, and their interactions. The focus on human-object interaction is unique in that it operationalizes classic cognitive theories of object affordances, representing a major shift from prior approaches to visual recognition. A chair is identified not by direct visual properties but rather the functional property that it affords sitting, which in turn is identified through human interactions. Such predictive models of human-object interaction can be used to create next-generation assitive technology devises (that for example, help a motor impaired patient perform activities of daily living). Such a predictive system requires, at its core, the ability to understand human poses, 3D object geometry, and the relative spatial relationships between the two.</p>\n<p><br />This project has produced recognition systems that are capable of producing such reports. We have studied human-object interactions from both a \"third-person\" perspective, as well as from body-mounted wearable cameras. In both cases, we have demonstrated that depth cues (extracted wall-mounted kinect system or from a body-mounted depth camera) can provide crucial clues as to what objects and postures are visibile, and perhaps even more importantly, those objects and postures that cannot be directly seen due to an occlusion. To help spur further research in this growing area, we amassed an unprecedented dataset of activities of daily living (ADLs) from wearable cameras that is now in widespread use. Our dataset differs from previous collections in its sheer diversity of scenes and subjects (where previous activity datasets were often limited to a few actors or a few environments). We demonstrated that much of activity analysis is \"all about the objects\": much of what the human wearer is doing can be derived from object-centric feature representations that explicitly reasons about passive objects vs those being activity manipulated.</p>\n<p><br />The underlying innovations behind the developed systems involve the use of geometrically-constrained statistical models trained on massively-large training sets. From this perspective, our results show that big-data, combined with statistics and geometry, make for a powerful approach to understanding visual data streams. Importantly, big-data alone cannot be the final answer because of the \"long-tail\" phenomena: it is difficult to collect enough training examples of all possible human-object interactions. Instead, we developed geometric models that can predict never-before-seen object shapes and novel interactions. Because many object interactions involve precise grasps, we developed recognition systems that can report the precise 3D shapes of objects and detailed estimates of human pose. Finally, because we wish to analyze interactions in long temporal video streams, we introduced scalable models where computation grows manageably (linearly or sublinearly) with respect to time.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/04/2016<br>\n\t\t\t\t\tModified by: Deva&nbsp;Ramanan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project explored spatiotemporal models for understanding humans, objects, and their interactions. The focus on human-object interaction is unique in that it operationalizes classic cognitive theories of object affordances, representing a major shift from prior approaches to visual recognition. A chair is identified not by direct visual properties but rather the functional property that it affords sitting, which in turn is identified through human interactions. Such predictive models of human-object interaction can be used to create next-generation assitive technology devises (that for example, help a motor impaired patient perform activities of daily living). Such a predictive system requires, at its core, the ability to understand human poses, 3D object geometry, and the relative spatial relationships between the two.\n\n\nThis project has produced recognition systems that are capable of producing such reports. We have studied human-object interactions from both a \"third-person\" perspective, as well as from body-mounted wearable cameras. In both cases, we have demonstrated that depth cues (extracted wall-mounted kinect system or from a body-mounted depth camera) can provide crucial clues as to what objects and postures are visibile, and perhaps even more importantly, those objects and postures that cannot be directly seen due to an occlusion. To help spur further research in this growing area, we amassed an unprecedented dataset of activities of daily living (ADLs) from wearable cameras that is now in widespread use. Our dataset differs from previous collections in its sheer diversity of scenes and subjects (where previous activity datasets were often limited to a few actors or a few environments). We demonstrated that much of activity analysis is \"all about the objects\": much of what the human wearer is doing can be derived from object-centric feature representations that explicitly reasons about passive objects vs those being activity manipulated.\n\n\nThe underlying innovations behind the developed systems involve the use of geometrically-constrained statistical models trained on massively-large training sets. From this perspective, our results show that big-data, combined with statistics and geometry, make for a powerful approach to understanding visual data streams. Importantly, big-data alone cannot be the final answer because of the \"long-tail\" phenomena: it is difficult to collect enough training examples of all possible human-object interactions. Instead, we developed geometric models that can predict never-before-seen object shapes and novel interactions. Because many object interactions involve precise grasps, we developed recognition systems that can report the precise 3D shapes of objects and detailed estimates of human pose. Finally, because we wish to analyze interactions in long temporal video streams, we introduced scalable models where computation grows manageably (linearly or sublinearly) with respect to time.\n\n\t\t\t\t\tLast Modified: 08/04/2016\n\n\t\t\t\t\tSubmitted by: Deva Ramanan"
 }
}