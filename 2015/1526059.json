{
 "awd_id": "1526059",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Combining Reinforcement Learning and Deep Learning Methods to Address High-Dimensional Perception, Partial Observability and Delayed Reward",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 499886.0,
 "awd_amount": 499886.0,
 "awd_min_amd_letter_date": "2015-08-06",
 "awd_max_amd_letter_date": "2015-08-06",
 "awd_abstract_narration": "Consider the problem faced by a machine agent that has to interact with some dynamical environment to achieve some goals. Concretely, imagine an agent engaged in a virtual competition as a human would. It can see the screen composed of many moving objects. At any time, it can choose one of a dozen or so actions. Its action controls one of the objects on the screen, but it often is not clear which one. Every so often the an evaluation is given of the competition. At some point the competition ends. How should such an agent choose actions, or more importantly how can we build agents that can learn to compete, i.e., achieve high scores, through trial and error. In this project methods will be developed and evaluated to build such agents.   \r\n\r\nThe above problem is an instance of what is called a reinforcement learning (RL) problem. Such problems abound in sequential decision-making settings. Applications in industry include factory optimization, robotics, and chronic disease management (to list but three diverse domains of interest). Like many of these RL problems, Atari games (used as a testbed here to evaluate learning strategies) have three characteristics of interest to this project. First, they generate high-dimensional images and so the agent faces a difficult perception problem. Second, they often have deeply-delayed rewards; i.e., actions have long-term consequences. For example, losing a resource may not cost at the moment of loss, but could lead to very high losses much later when that resource is critically necessary. Third, they have deep partial observability, i.e., to compete effectively one has to often remember the deep past. For example, a location encountered far back in the past may become valuable much later because a critical resource becomes available at that time and the agent would have to find its way back to that location to use the resource. It is proposed to address these three challenges respectively with new neural network architectures for predicting the consequences of actions, new methods for intrinsically motivating agents even when reward is delayed, and new recurrent neural network architectures to remember the past effectively. Success of the proposed work is expected to significantly expand the scope of application of reinforcement learning. Finally, Atari games will be used instead of, say, factory optimization as an evaluation domain because they are readily available.  They will be used to draw high-school and under-represented undergraduate students interest into complex ideas underlying the proposed work; their fun visualizations will allow them to be integrated into teaching in the PIs' classes, and there are a variety of games that vary in the degree of difficulty of the three challenge dimensions allowing more effective control of the evaluations more effectively.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Satinder",
   "pi_last_name": "Baveja",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Satinder S Baveja",
   "pi_email_addr": "baveja@umich.edu",
   "nsf_id": "000100173",
   "pi_start_date": "2015-08-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Lewis",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Richard L Lewis",
   "pi_email_addr": "rickl@umich.edu",
   "nsf_id": "000484065",
   "pi_start_date": "2015-08-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Honglak",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Honglak Lee",
   "pi_email_addr": "honglak@eecs.umich.edu",
   "nsf_id": "000583664",
   "pi_start_date": "2015-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan, Ann Arbor",
  "perf_str_addr": "2260 Hayward Street",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 499886.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Combining Reinforcement learning (RL) with deep learning (also known as deep reinforcement learning or deep RL), holds promise as a key direction for developing autonomous AI. Towards this end, researchers must make progress on enabling AI that can learn on its own with little experience in real-world environments. However, there still lie challenges to be solved before this may be realized. In this project we focused on the following: deep RL architectures and learning methods that can mitigate the problem of sparse and delayed rewards; methods that can learn to model world dynamics; and deep RL architectures that can learn with from partially observable, high-dimensional observations, and transfer this learned knowledge across related tasks.</p>\n<p><br />At the most fundamental level, we tackled research on the discovery of learning methods for RL. While most research focuses on designing a learning objective that will produce a desirable learning behavior, we have shown that we can take a data-driven approach to enable RL methods that discover prediction targets [3] and internal reward guidance functions [4, 10] that support faster task-learning. By taking a data-driven approach, we automate the process of discovering solutions, which may support faster hypothesis generation and testing in the research community.&nbsp;<br /><br />Not all solutions can be practically discovered automatically via data-driven methods. For example, we still require methods that can tackle the challenge of sparse and delayed rewards in order to reduce the experience needed by learning systems. Here, researchers commonly rely on imitation learning with expert demonstrations. We found that an agent can imitate demonstrations of its own good trajectories, a form of self-imitation [6]. When combined with an external memory, we showed that this can help agents learn tasks in very large state-spaces with extremely sparse rewards [2]. This research has potential to impact researchers and practitioners (e.g., in robotics) by lessening the burden they commonly face in collecting expert demonstrations and hand-designing reward functions for their tasks.<br /><br />Another mechanism for reducing the experience needed by a learning system is to imbue it with the ability to generate its own data, for example, with a model for capturing the world dynamics. However learning such a model with high-dimensional observations (e.g. pixels) is very challenging. We pioneered deep RL approaches for learning predictive models from high-dimensional observation dynamics [9], which can be used for both video-prediction and model-based RL (e.g. for planning). We were also among the first in model-based RL to show that one can learn to predict long-horizon value instead of predicting high-dimensional observations [5]. This can be useful for RL researchers and practitioners when a model is only needed for planning and not for data-generation.<br /><br />High-dimensional observations such as first-person observations also introduce a challenge of partial observability, where early observations can be informative of observations many time-steps into the future. To facilitate progress on developing systems for this setting, we developed a benchmark in which the agent learns to act in 3D environments from first-person observations, and proposed an architecture that outperforms existing solutions [8]. We also developed an architecture which could learn to perform language instruction tasks with capability of generalizing to unseen instructions in the Minecraft environment [7] and long-horizon compositional tasks in a near photo-realistic 3D Kitchen environment [1]. These capacities will be critical to real-world applications such as robotics systems, where robots will need to navigate 3D spaces as they follow instructions provided by humans.<br /><br />Overall, this project has had a large impact on the field, amassing 9 publications and several invited talks at top venues, with over 1400 citations.&nbsp;<br /><br />References:</p>\n<p><span id=\"docs-internal-guid-d6d2556f-7fff-ee80-143a-27d8d49aba39\">&nbsp;</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\">[1] Wilka Carvalho, Anthony Liang, Kimin Lee, Sungryull Sohn, Honglak Lee, Richard Lewis, Satinder Singh. Reinforcement Learning for Sparse-Reward Object-Interaction Tasks in First-person Simulated 3D Environments. NeurIPS 2020 Deep RL Workshop.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">[2] Yijie Guo, Jongwook Choi, Marcin Moczulski, Shengyu Feng, Samy Bengio, Mohammad Norouzi, Honglak Lee. Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards. NeurIPS 2020.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">[3] Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Richard Lewis, Janarthana Rajendran, Junhyuk Oh, Hado van Hasselt, David Silver, Satinder Singh. Discovery of Useful Questions as Auxiliary Tasks.&nbsp; NeurIPS 2019.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">[4] Zeyu Zheng, Junhyuk Oh, Satinder Singh. On Learning Intrinsic Rewards for Policy Gradient Methods.&nbsp; NeurIPS 2018.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">[5] Junhyuk Oh, Satinder Singh, Honglak Lee. Value Prediction Network.&nbsp; NeurIPS 2017.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">[6] Junhyuk Oh, Yijie Guo, Satinder Singh, Honglak Lee. Self-Imitation Learning.&nbsp; ICML 2018.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">[7] Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli. Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning.&nbsp; ICML 2017.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">[8] Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, Honglak Lee. Control of Memory, Active Perception, and Action in Minecraft.&nbsp; ICML 2016.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">[9] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh. Action-Conditional Video Prediction using Deep Networks in Atari Games.&nbsp; NeurIPS 2015.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">[10] Xiaoxiao Guo, Satinder Singh, Richard Lewis, Honglak Lee. Deep Learning for Reward Design to Improve Monte Carlo Tree Search in ATARI Games. IJCAI 2016.</p>\n</li>\n</ul>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/01/2021<br>\n\t\t\t\t\tModified by: Honglak&nbsp;Lee</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nCombining Reinforcement learning (RL) with deep learning (also known as deep reinforcement learning or deep RL), holds promise as a key direction for developing autonomous AI. Towards this end, researchers must make progress on enabling AI that can learn on its own with little experience in real-world environments. However, there still lie challenges to be solved before this may be realized. In this project we focused on the following: deep RL architectures and learning methods that can mitigate the problem of sparse and delayed rewards; methods that can learn to model world dynamics; and deep RL architectures that can learn with from partially observable, high-dimensional observations, and transfer this learned knowledge across related tasks.\n\n\nAt the most fundamental level, we tackled research on the discovery of learning methods for RL. While most research focuses on designing a learning objective that will produce a desirable learning behavior, we have shown that we can take a data-driven approach to enable RL methods that discover prediction targets [3] and internal reward guidance functions [4, 10] that support faster task-learning. By taking a data-driven approach, we automate the process of discovering solutions, which may support faster hypothesis generation and testing in the research community. \n\nNot all solutions can be practically discovered automatically via data-driven methods. For example, we still require methods that can tackle the challenge of sparse and delayed rewards in order to reduce the experience needed by learning systems. Here, researchers commonly rely on imitation learning with expert demonstrations. We found that an agent can imitate demonstrations of its own good trajectories, a form of self-imitation [6]. When combined with an external memory, we showed that this can help agents learn tasks in very large state-spaces with extremely sparse rewards [2]. This research has potential to impact researchers and practitioners (e.g., in robotics) by lessening the burden they commonly face in collecting expert demonstrations and hand-designing reward functions for their tasks.\n\nAnother mechanism for reducing the experience needed by a learning system is to imbue it with the ability to generate its own data, for example, with a model for capturing the world dynamics. However learning such a model with high-dimensional observations (e.g. pixels) is very challenging. We pioneered deep RL approaches for learning predictive models from high-dimensional observation dynamics [9], which can be used for both video-prediction and model-based RL (e.g. for planning). We were also among the first in model-based RL to show that one can learn to predict long-horizon value instead of predicting high-dimensional observations [5]. This can be useful for RL researchers and practitioners when a model is only needed for planning and not for data-generation.\n\nHigh-dimensional observations such as first-person observations also introduce a challenge of partial observability, where early observations can be informative of observations many time-steps into the future. To facilitate progress on developing systems for this setting, we developed a benchmark in which the agent learns to act in 3D environments from first-person observations, and proposed an architecture that outperforms existing solutions [8]. We also developed an architecture which could learn to perform language instruction tasks with capability of generalizing to unseen instructions in the Minecraft environment [7] and long-horizon compositional tasks in a near photo-realistic 3D Kitchen environment [1]. These capacities will be critical to real-world applications such as robotics systems, where robots will need to navigate 3D spaces as they follow instructions provided by humans.\n\nOverall, this project has had a large impact on the field, amassing 9 publications and several invited talks at top venues, with over 1400 citations. \n\nReferences:\n\n \n\n\n[1] Wilka Carvalho, Anthony Liang, Kimin Lee, Sungryull Sohn, Honglak Lee, Richard Lewis, Satinder Singh. Reinforcement Learning for Sparse-Reward Object-Interaction Tasks in First-person Simulated 3D Environments. NeurIPS 2020 Deep RL Workshop.\n\n\n[2] Yijie Guo, Jongwook Choi, Marcin Moczulski, Shengyu Feng, Samy Bengio, Mohammad Norouzi, Honglak Lee. Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards. NeurIPS 2020.\n\n\n[3] Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Richard Lewis, Janarthana Rajendran, Junhyuk Oh, Hado van Hasselt, David Silver, Satinder Singh. Discovery of Useful Questions as Auxiliary Tasks.  NeurIPS 2019.\n\n\n[4] Zeyu Zheng, Junhyuk Oh, Satinder Singh. On Learning Intrinsic Rewards for Policy Gradient Methods.  NeurIPS 2018.\n\n\n[5] Junhyuk Oh, Satinder Singh, Honglak Lee. Value Prediction Network.  NeurIPS 2017.\n\n\n[6] Junhyuk Oh, Yijie Guo, Satinder Singh, Honglak Lee. Self-Imitation Learning.  ICML 2018.\n\n\n[7] Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli. Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning.  ICML 2017.\n\n\n[8] Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, Honglak Lee. Control of Memory, Active Perception, and Action in Minecraft.  ICML 2016.\n\n\n[9] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh. Action-Conditional Video Prediction using Deep Networks in Atari Games.  NeurIPS 2015.\n\n\n[10] Xiaoxiao Guo, Satinder Singh, Richard Lewis, Honglak Lee. Deep Learning for Reward Design to Improve Monte Carlo Tree Search in ATARI Games. IJCAI 2016.\n\n\n\n \n\n\t\t\t\t\tLast Modified: 03/01/2021\n\n\t\t\t\t\tSubmitted by: Honglak Lee"
 }
}