{
 "awd_id": "1539534",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Ubilytics: Harnessing Existing Device Ecosystems for Anywhere Sensemaking",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2014-08-31",
 "awd_exp_date": "2020-01-31",
 "tot_intn_awd_amt": 420663.0,
 "awd_amount": 420663.0,
 "awd_min_amd_letter_date": "2015-05-28",
 "awd_max_amd_letter_date": "2017-07-07",
 "awd_abstract_narration": "This research project addresses the fundamental question of how we can use the existing ecosystem of networked devices in our surroundings to make sense of and exploit massive, heterogeneous, and multi-scale data anywhere and at any time. Assembling these devices into unified sensemaking environments would enable deep analysis in the field. Examples include managing heterogeneous data in scientific lab notebooks, scaffolding undergraduate classroom learning with examples, manuals, and videos, and supporting police investigation by linking facts, findings, and evidence. On a higher level, this concept would stimulate our digital economy by supporting fields such as design and creativity, command and control, and scientific discovery. However, despite this ready access to a myriad of handheld devices as well as those integrated in our physical environments, each individual device is currently designed to be the focus of attention, cannot easily be combined with other devices to improve productivity, and has limited computational and storage resources. This project introduces a comprehensive new approach called ubiquitous analytics (ubilytics) for harnessing these ever-present digital devices into unified environments for anywhere analysis and sensemaking of data.\r\n\r\nUbilytics draws on human-computer interaction, visual analytics, and ubiquitous computing as well as a synthesis of distributed, extended, and embodied cognition, based on three principles.  First, universal interaction requires designing an interaction model that combines several devices into a holistic distributed interface, transparently bridges multiple devices, surfaces, and even physical objects, and unifies interaction with various data types.  Second, flexible visual structures must be created in order to generate representations that adapt to varying device dimensions, resolution, viewing angle, and distance, support space and layout management in ego-centric and world-centric configurations, and can utilize both novel and appropriated displays for output. Third, efficient distributed architecture must be achieved through methods for discovering, merging, and synchronizing heterogeneous devices with support for a generic component model to facilitate reuse, offloading costly computation into the cloud, and meshing ubilytics environments for collaboration. \r\n\r\nSensemaking is often attributed to professional analysts finding meaning from observed data, but this research will take a comprehensive view of sensemaking for both casual and expert users, in both dedicated and mobile settings, and with both large-scale and small-scale datasets. This work will therefore benefit society by focusing on three example domains: (1) scientific discovery, (2) classroom learning, and (3) police investigation. It will also advance discovery and understanding by integrating the research in an undergraduate programming course used as a testbed for learning in ubilytics environments. Another goal is to broaden participation of underrepresented groups by engaging in a women in engineering program as well as by mentoring minority undergraduate students during summer research internships. Results, software, and documentation will be disseminated under Open Source and Creative Commons licenses.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Niklas",
   "pi_last_name": "Elmqvist",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Niklas E Elmqvist",
   "pi_email_addr": "elm@umd.edu",
   "nsf_id": "000517418",
   "pi_start_date": "2015-05-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "Hornbake Building",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425141",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 5267.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 84436.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 73358.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 142615.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 114987.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-86062d6a-7fff-5850-434e-4106a73e419e\"> </span></p>\n<p dir=\"ltr\"><span>The Ubilytics project posed the fundamental question of how we can use the existing ecosystem of networked devices in our surroundings to understand and exploit massive, heterogeneous, and multi-scale data anywhere and at any time. Assembling these devices into unified sensemaking environments will enable deep analytical reasoning in the field, such as managing heterogeneous data in scientific lab notebooks, scaffolding undergraduate learning, and supporting investigative reporting by linking facts, findings, and evidence. On a higher level, this concept would stimulate our digital economy by supporting fields such as design and creativity, command and control, and scientific discovery.</span></p>\n<p dir=\"ltr\"><span>However, despite this ready access to myriad devices both small&mdash;smartphones, tablets, and smartwatches&mdash;and large&mdash;desktop computers, multitouch displays, and mixed reality headsets&mdash;each individual device is currently designed to be the focus of attention, and cannot easily be combined with other devices to improve productivity. Another limiting factor is that interfaces and visual representations are typically designed for a particular form factor, and adapt poorly to new settings when migrating between devices. Finally, the computational and storage resources of most mobile devices are insufficient for the complex analyses necessary as well as for persistently storing session state across fluctuating ensembles of devices.</span></p>\n<p dir=\"ltr\"><span>Ubiquitous analytics</span><span>--or </span><span>ubilytics</span><span>--is a comprehensive new approach for harnessing these ever-present digital devices into unified environments for anywhere analysis and sensemaking of data. It draws on human-computer interaction, visual analytics, and ubiquitous computing as well as a synthesis of distributed, extended, and embodied cognition. The latter challenge traditional cognitive science by insisting that cognition is not limited to the brain, but also involves the body, the physical world, and the sociocultural context. Instead of studying cognitive aids in isolation, ubilytics therefore takes a system-level view of cognition that engages different representational media&mdash;such as humans, physical artifacts, mobile devices, and large displays&mdash;as well as interactions that are used to bring these media in coordination with each other&mdash;such as verbal and gestural cues, touching and sketching, partitioning and arranging, and note-taking and annotation. Thus, ubilytic environments benefit sensemaking by distributing cognitive aids in space and time; by off-loading memory, deduction, and reasoning; by harnessing our innate perceptual, cognitive, motor, spatial, and social skills; and by multiplying interaction and display surfaces.</span></p>\n<p dir=\"ltr\"><span>The research in the ubilytics project revolved around three themes: (1) universal interaction, (2) flexible visual structures, and (3) efficient distributed architectures. Below I give examples of ubilytics research projects that span all three themes.</span></p>\n<p dir=\"ltr\"><span>----</span></p>\n<p dir=\"ltr\"><span><span><img src=\"https://lh5.googleusercontent.com/5_iGVAxIINFgoKEsYLhHtULSOMf9dsgxFhd83WDgotwpiNtzrkYxOSVXk_bN7sQDe4RzEtRsmOZOp8blE7H47XCWN99Dg-MddvYLi9bhCbobqADutohnPQKdbgqDtIEeI1KJ8T7t\" alt=\"\" width=\"624\" height=\"333\" /></span></span></p>\n<p dir=\"ltr\"><span>Central to ubiquitous analytics is </span><a href=\"http://vistrates.org/\"><span>Vistrates</span></a><span>, a component model and a literate computing platform for developing, assembling, and sharing visualization components. Built on top of the Webstrates and Codestrates open source projects, Vistrates features cross-cutting components for visual representation, interaction, collaboration, and device responsiveness maintained in a component repository. The environment is collaborative and allows novices and experts alike to compose component pipelines for specific analytical activities. This allows for easily creating cross-platform and cross-device visualization applications on any device capable of running modern web technology, as well as integrating existing web-based visualization resources such as D3, Vega-Lite, and Plot.ly into these applications.&nbsp;</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span>Paper:&nbsp;<a href=\"http://www.umiacs.umd.edu/~elm/projects/vistrates/vistrates.pdf\">PDF</a></p>\n<p dir=\"ltr\"><span>Video: </span><a href=\"https://youtu.be/nMmiWBJoJUc\"><span>https://youtu.be/nMmiWBJoJUc</span></a><span>&nbsp;</span></p>\n<p dir=\"ltr\"><span>----</span></p>\n<p dir=\"ltr\"><span><span><img src=\"https://lh3.googleusercontent.com/ii3JuQrfo4aQ4BXV1WRY8Uji5lX0fdSYssRcwHLXpLcxNBcKQhAuyBIp_FLdZ6e32wTjuXr9fVuCAScbaT9NO2vyVbMQho9fpwTrGNSDRGIMvfqE_AhjovVLXUmolb2EKzbfX48_\" alt=\"\" width=\"624\" height=\"352\" /></span></span></p>\n<p dir=\"ltr\"><span>With Vistrates as a foundation, we looked at the practicalities of actually distributing different views of a visualization application across multiple devices in the </span><span>Vistribute </span><span>project. By characterizing each view based on its preferred size, position, and relation to other views, Vistribute automatically calculates a layout across the available display surfaces in a ubiquitous sensemaking environment. This layout can change as devices enter and leave the environment, such as when powering up a laptop at a meeting, or pocketing a smartphone that is no longer needed.</span></p>\n<p dir=\"ltr\">Paper:&nbsp;<a href=\"http://users.umiacs.umd.edu/~elm/projects/vistribute/vistribute.pdf\">PDF</a></p>\n<p dir=\"ltr\"><span>Video: </span><a href=\"https://youtu.be/-ssJ_T-nbdI\"><span>https://youtu.be/-ssJ_T-nbdI</span></a><span>&nbsp;</span></p>\n<p dir=\"ltr\"><span>---</span></p>\n<p dir=\"ltr\"><span><span><img src=\"https://lh5.googleusercontent.com/TYmQXLPHwPZnzC53kskzEIOPk610PtaUElRYju7Ly6Mf8NJ1mDeGX_UQAAIXHXMW5B2_-A4iGMcShBSTInMI4tN6fn5-2OsXQVBRG10Y56g-HPb00m_Kd32igGokeikNl71mbCYW\" alt=\"\" width=\"624\" height=\"209\" /></span></span></p>\n<p dir=\"ltr\"><span>Looking to the future, I have increasingly been investigating how to use ubiquitous analytics for </span><span>situated sensemaking</span><span>, such as when in the field and on the go. Several projects have contributed to this development. In </span><span>There Is No Spoon</span><span>, collaborators and I deployed the ImAxes virtual reality system for multidimensional analysis at a U.S. federal agency for an entire year and observed how economic analysts and data scientists interacted with it for their own datasets. In </span><span>VisHive</span><span>, we studied how to build opportunistic and ad-hoc computational clouds using only web-based technology and mobile devices. And in ongoing work we are making a foray into mixed and augmented reality to support </span><span>situated visualization</span><span> for ubiquitous analytics in the recently funded </span><a href=\"https://sites.umiacs.umd.edu/elm/research/current-projects/dataworld/\"><span>DataWorld NSF project</span></a><span>.</span></p>\n<p dir=\"ltr\">There Is No Spoon: [<a href=\"http://users.umiacs.umd.edu/~elm/projects/nospoon/nospoon.pdf\">PDF</a>]</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/02/2020<br>\n\t\t\t\t\tModified by: Niklas&nbsp;E&nbsp;Elmqvist</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe Ubilytics project posed the fundamental question of how we can use the existing ecosystem of networked devices in our surroundings to understand and exploit massive, heterogeneous, and multi-scale data anywhere and at any time. Assembling these devices into unified sensemaking environments will enable deep analytical reasoning in the field, such as managing heterogeneous data in scientific lab notebooks, scaffolding undergraduate learning, and supporting investigative reporting by linking facts, findings, and evidence. On a higher level, this concept would stimulate our digital economy by supporting fields such as design and creativity, command and control, and scientific discovery.\nHowever, despite this ready access to myriad devices both small&mdash;smartphones, tablets, and smartwatches&mdash;and large&mdash;desktop computers, multitouch displays, and mixed reality headsets&mdash;each individual device is currently designed to be the focus of attention, and cannot easily be combined with other devices to improve productivity. Another limiting factor is that interfaces and visual representations are typically designed for a particular form factor, and adapt poorly to new settings when migrating between devices. Finally, the computational and storage resources of most mobile devices are insufficient for the complex analyses necessary as well as for persistently storing session state across fluctuating ensembles of devices.\nUbiquitous analytics--or ubilytics--is a comprehensive new approach for harnessing these ever-present digital devices into unified environments for anywhere analysis and sensemaking of data. It draws on human-computer interaction, visual analytics, and ubiquitous computing as well as a synthesis of distributed, extended, and embodied cognition. The latter challenge traditional cognitive science by insisting that cognition is not limited to the brain, but also involves the body, the physical world, and the sociocultural context. Instead of studying cognitive aids in isolation, ubilytics therefore takes a system-level view of cognition that engages different representational media&mdash;such as humans, physical artifacts, mobile devices, and large displays&mdash;as well as interactions that are used to bring these media in coordination with each other&mdash;such as verbal and gestural cues, touching and sketching, partitioning and arranging, and note-taking and annotation. Thus, ubilytic environments benefit sensemaking by distributing cognitive aids in space and time; by off-loading memory, deduction, and reasoning; by harnessing our innate perceptual, cognitive, motor, spatial, and social skills; and by multiplying interaction and display surfaces.\nThe research in the ubilytics project revolved around three themes: (1) universal interaction, (2) flexible visual structures, and (3) efficient distributed architectures. Below I give examples of ubilytics research projects that span all three themes.\n----\n\nCentral to ubiquitous analytics is Vistrates, a component model and a literate computing platform for developing, assembling, and sharing visualization components. Built on top of the Webstrates and Codestrates open source projects, Vistrates features cross-cutting components for visual representation, interaction, collaboration, and device responsiveness maintained in a component repository. The environment is collaborative and allows novices and experts alike to compose component pipelines for specific analytical activities. This allows for easily creating cross-platform and cross-device visualization applications on any device capable of running modern web technology, as well as integrating existing web-based visualization resources such as D3, Vega-Lite, and Plot.ly into these applications. \n Paper: PDF\nVideo: https://youtu.be/nMmiWBJoJUc \n----\n\nWith Vistrates as a foundation, we looked at the practicalities of actually distributing different views of a visualization application across multiple devices in the Vistribute project. By characterizing each view based on its preferred size, position, and relation to other views, Vistribute automatically calculates a layout across the available display surfaces in a ubiquitous sensemaking environment. This layout can change as devices enter and leave the environment, such as when powering up a laptop at a meeting, or pocketing a smartphone that is no longer needed.\nPaper: PDF\nVideo: https://youtu.be/-ssJ_T-nbdI \n---\n\nLooking to the future, I have increasingly been investigating how to use ubiquitous analytics for situated sensemaking, such as when in the field and on the go. Several projects have contributed to this development. In There Is No Spoon, collaborators and I deployed the ImAxes virtual reality system for multidimensional analysis at a U.S. federal agency for an entire year and observed how economic analysts and data scientists interacted with it for their own datasets. In VisHive, we studied how to build opportunistic and ad-hoc computational clouds using only web-based technology and mobile devices. And in ongoing work we are making a foray into mixed and augmented reality to support situated visualization for ubiquitous analytics in the recently funded DataWorld NSF project.\nThere Is No Spoon: [PDF]\n\n\t\t\t\t\tLast Modified: 06/02/2020\n\n\t\t\t\t\tSubmitted by: Niklas E Elmqvist"
 }
}