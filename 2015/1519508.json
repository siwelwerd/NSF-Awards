{
 "awd_id": "1519508",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Radically Improving Data Center Cost Effectiveness through Virtualized Convergence of Data Processing and Storage",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2015-12-31",
 "tot_intn_awd_amt": 149918.0,
 "awd_amount": 149918.0,
 "awd_min_amd_letter_date": "2015-06-24",
 "awd_max_amd_letter_date": "2015-06-24",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to enable the scaling of data centers in a very cost effective way. Global information technology business is reaching $3.7 Trillion, and data centers are an increasingly important cornerstone of future global information technology infrastructure. By providing timely solutions to improve the cost effectiveness of data centers, this project will contribute to accelerating the penetration of data center based cloud services into a wider spectrum of applications and hence generate considerable technical and societal impact. In addition, as the project disruptively relaxes the traditional distinct boundary among computation, storage, and networking, its success will influence the entire industry to fundamentally re-think and re-innovate data center design and implementation for future information technology industry advancement.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project will develop a working demonstration of a disruptive product that can significantly improve the cost effectiveness of future data center infrastructure. With inevitable slowing-down of Moore's Law and rapid growth of data volume, the conventional data center design practice demands formidable capital investment, which will seriously undermine the cost effectiveness of future data centers. Aiming to fundamentally address the data center cost effectiveness challenge, this project will develop a working demonstration of a software/hardware integrated product that can cohesively handle data storage and processing tasks in data centers at very low cost. The key intellectual merit is to fundamentally re-think the realization and coordination among computing, storage and networking. The product can outperform CPUs by at least an order of magnitude for critical data processing tasks in data centers and meanwhile significantly improve memory/storage hierarchy efficiency. The software/hardware integrated products installed across multiple server nodes can further form a virtualized platform, with which inter-node data transfer and computation coordination can be realized very efficiently.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hao",
   "pi_last_name": "Zhong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hao Zhong",
   "pi_email_addr": "hao.zhong@scaleflux.com",
   "nsf_id": "000686582",
   "pi_start_date": "2015-06-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "ScaleFlux Inc",
  "inst_street_address": "1038 Leigh Ave",
  "inst_street_address_2": "Suite 206",
  "inst_city_name": "San Jose",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "4086282291",
  "inst_zip_code": "951264129",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": null
 },
 "perf_inst": {
  "perf_inst_name": "ScaleFlux Inc",
  "perf_str_addr": "1038 Leigh Ave.",
  "perf_city_name": "San Jose",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "951264129",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "8033",
   "pgm_ref_txt": "Hardware Software Integration"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 149918.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to develop a product that can fundamentally improve data center server data processing performance. The progress of server performance improvement is being outgrown by data processing workloads as the society produces ever-exploding amount of data. It has been projected that the digital universe will increase by 10x from 4.4ZB at 2013 to 44ZB at 2020, and meanwhile the percentage of data that should be analyzed increases by 68%. This represents 17x increase of the total data which must be processed. Moreover, as more complicated data processing techniques are being deployed (e.g., deep learning and big data analytics), the overall data processing workloads will grown even noticeably higher than the increase of sheer data volume. However, current design practice cannot sustain the industry to correspondingly scale up the server performance to meet the data processing demands. Failure to adequately and timely address this server productivity crisis will cause unsustainable IT infrastructure cost structure and hinder the growth of data center business.</p>\n<p>Funded by NSF SBIR Phase I award, this project has developed the key hardware and software components underlying the product, and successfully developed a proof-of-concept prototype of the product. Extensive experiments have demonstrated that the proof-of-concept prototype can improve server data processing performance by 10x for handling big data workloads.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/08/2016<br>\n\t\t\t\t\tModified by: Hao&nbsp;Zhong</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project aims to develop a product that can fundamentally improve data center server data processing performance. The progress of server performance improvement is being outgrown by data processing workloads as the society produces ever-exploding amount of data. It has been projected that the digital universe will increase by 10x from 4.4ZB at 2013 to 44ZB at 2020, and meanwhile the percentage of data that should be analyzed increases by 68%. This represents 17x increase of the total data which must be processed. Moreover, as more complicated data processing techniques are being deployed (e.g., deep learning and big data analytics), the overall data processing workloads will grown even noticeably higher than the increase of sheer data volume. However, current design practice cannot sustain the industry to correspondingly scale up the server performance to meet the data processing demands. Failure to adequately and timely address this server productivity crisis will cause unsustainable IT infrastructure cost structure and hinder the growth of data center business.\n\nFunded by NSF SBIR Phase I award, this project has developed the key hardware and software components underlying the product, and successfully developed a proof-of-concept prototype of the product. Extensive experiments have demonstrated that the proof-of-concept prototype can improve server data processing performance by 10x for handling big data workloads.\n\n \n\n\t\t\t\t\tLast Modified: 01/08/2016\n\n\t\t\t\t\tSubmitted by: Hao Zhong"
 }
}