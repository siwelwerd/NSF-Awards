{
 "awd_id": "1523495",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC: Small: Modeling Human Communication Dynamics",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2015-01-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 115585.0,
 "awd_amount": 128000.0,
 "awd_min_amd_letter_date": "2015-01-20",
 "awd_max_amd_letter_date": "2015-04-06",
 "awd_abstract_narration": "Face-to-face communication is a highly dynamic process where participants mutually exchange and interpret linguistic and gestural signals. Even when only one person speaks at the time, other participants exchange information continuously amongst themselves and with the speaker through gesture, gaze, posture and facial expressions. To correctly interpret the high-level communicative signal, an observer needs to jointly integrate all spoken words, subtle prosodic changes and simultaneous gestures from all participants.\r\n\r\nThe proposed effort endeavors to create a new generation of computational models for modeling the interdependence between linguistic symbols and nonverbal signals during social interactions. This computational framework has wide applicability, including the recognition of human social behaviors, the synthesis of natural animations for robots and virtual humans, improved multimedia content analysis, and the diagnosis of social and behavioral disorders (e.g., autism spectrum disorder). This research effort is an important milestone, complementary to recent research efforts focusing on only two components (e.g., social signal processing, which focuses on nonverbal and social signals). The proposed unified approach to Social-Symbols-Signals will pave the way for new robust and efficient computational perception algorithms able to recognize high-level communicative behaviors (e.g., intent and sentiments) and will enable new computational tools for researchers in behavioral sciences.\r\n \r\nThe proposed research will advance this endeavor through the development of new probabilistic models for jointly capturing the interdependence between language, gestures and social signals, and novel computational representations, which integrates data-driven processing and logic rule-based approach (so that prior knowledge from social sciences can be easily included). Four fundamental research goals will be directly addressed: symbol-signal representation (joint representation of language and nonverbal), modeling social interdependence (joint modeling of communicative signals between multiple participants), variability in signal interpretations (variability with annotations of high-level communicative signals), and generalization and validation (generalization over different communicative signals and domains).\r\n\r\nThe proposed research will enable more natural interaction between users and embodied conversational dialogue systems, impacting the way in which computers are used, for example, in tutoring and in cultural and language training. The potential uses of such software and data go far beyond the scope of this project, making it possible, for example, to perform large scale corpus-based studies about social aspects of human face-to-face (multimodal) communication, or cognitive aspects of human multimodal processing. Following the investigators' past experience with sharing research software open-source, code and corpus annotations will be made available to the research community. These shared research results will be valuable for new researchers as well as important educational material for course development.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Louis-Philippe",
   "pi_last_name": "Morency",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Louis-Philippe Morency",
   "pi_email_addr": "morency@cs.cmu.edu",
   "nsf_id": "000519300",
   "pi_start_date": "2015-01-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 112000.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The over-arching goal of this grant was to create a new generation of computational models for modeling the interdependence between linguistic symbols and nonverbal signals during social interaction. Face-to-face communication is a highly dynamic process where participants mutually exchange and interpret linguistic and gestural signals. Even when only one person speaks at the time, other participants exchange information continuously amongst themselves and with the speaker through gesture, gaze, posture and facial expressions. To correctly interpret the high-level communicative signal, an observer needs to jointly integrate all spoken words, subtle prosodic changes and simultaneous gestures from all participants.</p>\n<p>This grant successfully address these research goals by creating in 2013 one of the largest dataset of transcribed and annotated social communication dynamics and following this achievement with the discovery of new computational algorithms for multimodal representations and social signal interpretations. One of the first landmark result of this grant was the 2014 paper titled &ldquo;Toward Crowdsourcing Micro-Level Behavior Annotations: The Challenges of Interface, Training, and Generalization&rdquo; which was published in the proceedings of the ACM International Conference on Intelligent User Interfaces (IUI 2014) and received a best paper award nomination. This paper presented a new technique to efficiently train and evaluate online crowdsourcing worker to perform detailed annotations of human communicative behaviors in videos. It enables the scalability required for large-corpus behavior analysis.</p>\n<p>A second important thrust of research in this project was the development of new multimodal algorithms to model the temporal dynamics in human communication. Communication modalities, such as speech and gesture, are inherently correlated. Most approaches to emotion detection fail to take advantage of the dependencies between modalities. &nbsp;Multimodal fusion is the process of integrating modalities for emotion prediction. To model the asynchrony between modalities, we proposed a Multi-View Hidden Conditional Random Fields (MV-HCRF) model specifically designed to learn when the audio signal (i.e., first view) should be synchronized with the visual signal (i.e., second view), showing state-of-the-art performances on three publicly available datasets. This work was later improved by combining two different perspectives for multimodal data classification: (1) intra-modality dynamics and (2) inter-modality dynamics. This work received the best paper award at the ACM International Conference on Multimodal Interaction (ICMI 2016).</p>\n<p>The generalizability of the proposed research was evaluated and validated on new research domains. One these domains is automatic opinion mining, also known as sentiment analysis. The internet contains a large number of opinions expressed by users about products, movies or events. While researchers and companies have successfully developed tools to mine opinions in text, opinions in videos was an unexplored territory. This research grant proposed some of the first algorithms to automatically recognize sentiments in multimodal data. A new dataset was created for this purpose, including detailed annotations of more than 2000 subjective multimodal opinions. The dataset enabled the development of a new computational representation of multimodal data, named multimodal dictionary, which allows to model the interaction between human gestures and spoken words. All these resources, including the 2013 dataset, were made freely available for researchers and academia.</p>\n<p>The impact of this research grant goes beyond computer science. The computational models developed as part of this research grant are already having impact in the medical field. These models helped identify behaviors related to psychological distress such as depression and post-trauma stress disorder. For example, it was discovered that subjects with psychological distress such as depression do smile as often as non-distress people but show significantly less intense smiles and shorter average durations. Furthermore, the research found evidence of reduced emotional variability in the facial expressions of subjects with depression and PTSD and revealed that there are strong gender-dependent effects within different behavioral indicators. The development of computational models of human communication dynamics enable a new generation of decision support tools for healthcare providers.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2016<br>\n\t\t\t\t\tModified by: Louis-Philippe&nbsp;Morency</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe over-arching goal of this grant was to create a new generation of computational models for modeling the interdependence between linguistic symbols and nonverbal signals during social interaction. Face-to-face communication is a highly dynamic process where participants mutually exchange and interpret linguistic and gestural signals. Even when only one person speaks at the time, other participants exchange information continuously amongst themselves and with the speaker through gesture, gaze, posture and facial expressions. To correctly interpret the high-level communicative signal, an observer needs to jointly integrate all spoken words, subtle prosodic changes and simultaneous gestures from all participants.\n\nThis grant successfully address these research goals by creating in 2013 one of the largest dataset of transcribed and annotated social communication dynamics and following this achievement with the discovery of new computational algorithms for multimodal representations and social signal interpretations. One of the first landmark result of this grant was the 2014 paper titled \"Toward Crowdsourcing Micro-Level Behavior Annotations: The Challenges of Interface, Training, and Generalization\" which was published in the proceedings of the ACM International Conference on Intelligent User Interfaces (IUI 2014) and received a best paper award nomination. This paper presented a new technique to efficiently train and evaluate online crowdsourcing worker to perform detailed annotations of human communicative behaviors in videos. It enables the scalability required for large-corpus behavior analysis.\n\nA second important thrust of research in this project was the development of new multimodal algorithms to model the temporal dynamics in human communication. Communication modalities, such as speech and gesture, are inherently correlated. Most approaches to emotion detection fail to take advantage of the dependencies between modalities.  Multimodal fusion is the process of integrating modalities for emotion prediction. To model the asynchrony between modalities, we proposed a Multi-View Hidden Conditional Random Fields (MV-HCRF) model specifically designed to learn when the audio signal (i.e., first view) should be synchronized with the visual signal (i.e., second view), showing state-of-the-art performances on three publicly available datasets. This work was later improved by combining two different perspectives for multimodal data classification: (1) intra-modality dynamics and (2) inter-modality dynamics. This work received the best paper award at the ACM International Conference on Multimodal Interaction (ICMI 2016).\n\nThe generalizability of the proposed research was evaluated and validated on new research domains. One these domains is automatic opinion mining, also known as sentiment analysis. The internet contains a large number of opinions expressed by users about products, movies or events. While researchers and companies have successfully developed tools to mine opinions in text, opinions in videos was an unexplored territory. This research grant proposed some of the first algorithms to automatically recognize sentiments in multimodal data. A new dataset was created for this purpose, including detailed annotations of more than 2000 subjective multimodal opinions. The dataset enabled the development of a new computational representation of multimodal data, named multimodal dictionary, which allows to model the interaction between human gestures and spoken words. All these resources, including the 2013 dataset, were made freely available for researchers and academia.\n\nThe impact of this research grant goes beyond computer science. The computational models developed as part of this research grant are already having impact in the medical field. These models helped identify behaviors related to psychological distress such as depression and post-trauma stress disorder. For example, it was discovered that subjects with psychological distress such as depression do smile as often as non-distress people but show significantly less intense smiles and shorter average durations. Furthermore, the research found evidence of reduced emotional variability in the facial expressions of subjects with depression and PTSD and revealed that there are strong gender-dependent effects within different behavioral indicators. The development of computational models of human communication dynamics enable a new generation of decision support tools for healthcare providers.\n\n\t\t\t\t\tLast Modified: 10/30/2016\n\n\t\t\t\t\tSubmitted by: Louis-Philippe Morency"
 }
}