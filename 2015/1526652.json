{
 "awd_id": "1526652",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small:Testing in the Presence of Continuous Change",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2015-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 425000.0,
 "awd_amount": 425000.0,
 "awd_min_amd_letter_date": "2015-07-20",
 "awd_max_amd_letter_date": "2015-07-20",
 "awd_abstract_narration": "Many organizations that develop software are utilizing continuous integration\r\nprocesses, in which engineers merge code frequently with the mainline codebase,\r\nand continuously regression test it.  The pace at which evolution occurs in\r\nsuch environments, and the numbers of tests that must be run, can be staggering.\r\nTo ensure the quality of software in these situations, testing processes need\r\nto adapt quickly to changes in available resources, testing request frequency,\r\nand product release cycles.  Tests must be chosen and scheduled appropriately,\r\ntests must be analyzed for robustness, and testing processes must be\r\nappropriately monitored.  The proposed work addresses these needs, and by\r\nfocusing on software dependability, the work promises to benefit both\r\nproducers (i.e., software developers) and consumers (i.e., the public at large)\r\nof software.\r\n\r\nThis work will create new techniques for scheduling test cases during\r\ncontinuous testing based on their historical effectiveness, the relationships\r\nbetween tests and changes, and their potential for execution; these techniques\r\nwill also be able to adapt to release practices and environmental changes, and\r\nwill be supported by analysis techniques that are efficient enough for use in\r\nCI testing situations.  The work will involve studying the effect of problems\r\nrelated to test design, and developing lightweight static and dynamic\r\ntechniques for analyzing test cases and test suites to determine whether they\r\nmay be problematic. The work will lead to the creation of new approaches for\r\ntest monitoring, and the definition of new metrics that capture test and test\r\nsuite attributes and states for the assessment of testing results in continuous\r\ntesting processes.  Finally, the work will result in the creation of\r\ninfrastructure to support rigorous empirical assessments of the techniques\r\ndeveloped.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sebastian",
   "pi_last_name": "Elbaum",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Sebastian G Elbaum",
   "pi_email_addr": "selbaum@virginia.edu",
   "nsf_id": "000412723",
   "pi_start_date": "2015-07-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gregg",
   "pi_last_name": "Rothermel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gregg Rothermel",
   "pi_email_addr": "gerother@ncsu.edu",
   "nsf_id": "000092147",
   "pi_start_date": "2015-07-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Nebraska-Lincoln",
  "inst_street_address": "2200 VINE ST # 830861",
  "inst_street_address_2": "",
  "inst_city_name": "LINCOLN",
  "inst_state_code": "NE",
  "inst_state_name": "Nebraska",
  "inst_phone_num": "4024723171",
  "inst_zip_code": "685032427",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NE01",
  "org_lgl_bus_name": "BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA",
  "org_prnt_uei_num": "",
  "org_uei_num": "HTQ6K6NJFHA6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Nebraska-Lincoln",
  "perf_str_addr": "256 Avery Hall",
  "perf_city_name": "Lincoln",
  "perf_st_code": "NE",
  "perf_st_name": "Nebraska",
  "perf_zip_code": "685880150",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NE01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 425000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The work we have performed has provided a greater understanding of the challenges associated with testing in continuous integration environments, and produced highly novel approaches and practices for improving the cost-effectiveness of testing techniques operating in such environments, which are far more ubiquitous in practice than in research.</p>\n<p>This project resulted in several significant findings and outcomes; we summarize these next.</p>\n<p>In continuous integration environments, test cases, test suites and commits are commonly queued for execution. Test results obtained during queuing time can be continuously used to significantly improve the decisions made regarding whether and when to execute a queued test.</p>\n<p>Prioritization at the level of test suites is no longer sustainable or useful when operating under large and frequently updated code bases. &nbsp;Instead, approaches that operate at the commit level result in substantially greater gains by enabling larger changes in the testing schedule.</p>\n<p>Continuous integration testing requires new metrics to account for the richer test state space that occurs in continuous integration environments. That test state space is no longer binary and includes states such as \"queued\", \"scheduled\", \"under execution\", \"failed to build\", and \"passed/failed\". Test executions can also be viewed at several different levels of granularity such as the test, suite, build, and configuration levels.</p>\n<p>Keeping code changes small and integrating code frequently is often a wise approach for developers; however, the continuous integration server may be more efficient operating on different scales (from single commits to clusters of related commits) at different times and under different contexts to avoid redundant operations.</p>\n<p>Lightweight static analyses can be cost-effective for pinpointing patterns associated with faults in tests such as those associated with execution order dependencies, time coordination, and hard-coded paths. Such faults in tests can cause incorrect or flaky test behavior, or lead to unnecessary inefficiencies.</p>\n<p>Test suite type does affect the effectiveness of selection techniques, but it can do so differently across different types of test dependence tracking techniques, so different selection techniques may be required across different types of test suites.</p>\n<p>The collection and curation of continuous integration testing datasets, such as those ones produced through this project, will be crucial to assist in the evolution of test design and execution techniques.</p>\n<p>Overall, the body of knowledge generated by this work should allow software development organizations - especially those that operate at large scale and speed - to utilize and maintain tests more cost-effectively and reduce the incidence of faults released into production software. This also positively affects (even if indirectly) all segments of society that depend on software.</p>\n<p>In addition, the work under this proposal resulted in techniques to reduce users&rsquo; effort when performing program synthesis, and to automatically assess the quality of recommender systems beyond the traditional metrics which offer only coarse, one-dimensional views of system performance.&nbsp; Last, this work led to the identification of important misalignments between the techniques and the tools built by the research community for use in automated test case generation and program analysis, and the remedies that address these misalignments.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/08/2018<br>\n\t\t\t\t\tModified by: Sebastian&nbsp;G&nbsp;Elbaum</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe work we have performed has provided a greater understanding of the challenges associated with testing in continuous integration environments, and produced highly novel approaches and practices for improving the cost-effectiveness of testing techniques operating in such environments, which are far more ubiquitous in practice than in research.\n\nThis project resulted in several significant findings and outcomes; we summarize these next.\n\nIn continuous integration environments, test cases, test suites and commits are commonly queued for execution. Test results obtained during queuing time can be continuously used to significantly improve the decisions made regarding whether and when to execute a queued test.\n\nPrioritization at the level of test suites is no longer sustainable or useful when operating under large and frequently updated code bases.  Instead, approaches that operate at the commit level result in substantially greater gains by enabling larger changes in the testing schedule.\n\nContinuous integration testing requires new metrics to account for the richer test state space that occurs in continuous integration environments. That test state space is no longer binary and includes states such as \"queued\", \"scheduled\", \"under execution\", \"failed to build\", and \"passed/failed\". Test executions can also be viewed at several different levels of granularity such as the test, suite, build, and configuration levels.\n\nKeeping code changes small and integrating code frequently is often a wise approach for developers; however, the continuous integration server may be more efficient operating on different scales (from single commits to clusters of related commits) at different times and under different contexts to avoid redundant operations.\n\nLightweight static analyses can be cost-effective for pinpointing patterns associated with faults in tests such as those associated with execution order dependencies, time coordination, and hard-coded paths. Such faults in tests can cause incorrect or flaky test behavior, or lead to unnecessary inefficiencies.\n\nTest suite type does affect the effectiveness of selection techniques, but it can do so differently across different types of test dependence tracking techniques, so different selection techniques may be required across different types of test suites.\n\nThe collection and curation of continuous integration testing datasets, such as those ones produced through this project, will be crucial to assist in the evolution of test design and execution techniques.\n\nOverall, the body of knowledge generated by this work should allow software development organizations - especially those that operate at large scale and speed - to utilize and maintain tests more cost-effectively and reduce the incidence of faults released into production software. This also positively affects (even if indirectly) all segments of society that depend on software.\n\nIn addition, the work under this proposal resulted in techniques to reduce users? effort when performing program synthesis, and to automatically assess the quality of recommender systems beyond the traditional metrics which offer only coarse, one-dimensional views of system performance.  Last, this work led to the identification of important misalignments between the techniques and the tools built by the research community for use in automated test case generation and program analysis, and the remedies that address these misalignments. \n\n \n\n\t\t\t\t\tLast Modified: 08/08/2018\n\n\t\t\t\t\tSubmitted by: Sebastian G Elbaum"
 }
}