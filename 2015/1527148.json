{
 "awd_id": "1527148",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Real Time Observation, Inference and Intervention of Co-Robot Systems Towards Individually Customized Performance Feedback Based on Students' Affective States",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Haury",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 342574.0,
 "awd_amount": 342574.0,
 "awd_min_amd_letter_date": "2015-08-11",
 "awd_max_amd_letter_date": "2015-08-11",
 "awd_abstract_narration": "This NSF National Robotics Initiative project will investigate the potential of a cycle of observation, inference and intervention by co-robot systems to enhance students' affective states and improve their performance on engineering laboratory tasks. Co-robots are robots that work side-by-side with humans, assisting them and adapting to their needs. The two-way exchange of knowledge between students and co-robots creates a reciprocal relationship, in which each party learns from the other in service of a common goal. Affective states, such as frustration and engagement, play a major role in students' performance on everyday learning tasks. A student who is overly stressed or distracted may commit errors that would be otherwise easy to avoid. A co-robot system that is cognizant of students' affective states can intervene to prevent these errors. The results of this project may provide a template for skill-based instruction on topics well beyond engineering. Currently, such learning requires extensive interactions between a student and an instructor, with the instructor providing intensive feedback at all times. In many cases, personality mismatches or other issues between instructor and student can lead to frustration, learning difficulties, and eventual dropout. Furthermore, one-on-one learning is limited by scalability challenges, as an increase in the number of students, without a proportional increase in trained instructors, can result in decreases in quality and quantity of instructor time allocated to each student. Co-robot learning systems will be able to mitigate these challenges by providing both real time and scalable feedback systems that adapt to the individual needs of students and help to minimize the amount of human instructor time required by each student. \r\n\r\nThis research will acquire facial, auditory, and body gesture data from students using the integrated visual, audio and depth sensory system of the co-robot.  The system will make statistical inferences of students' affective states, based on machine learning classification of facial and body language data.  Visual feedback will be used to present students with interventions (visual instructions and commentary) intended to enhance their affective state and improve their performance on laboratory tasks.  The project will assess the impact of co-robots' ability to improve students' affective states and enhance students' performance on laboratory tasks over repeated iterations of learning and testing.  This project will lead to a better understanding of how students interact and function during potentially stressful laboratory activities. The co-robot systems proposed in this work will help discover the correlations that exists between students' affect and task performance. Co-robots will actively adapt to the manner in which students learn complex engineering tasks and the affective states that accompany that learning. Co-robot systems that predict the effectiveness of specific intervention strategies for each student and situation will lead to individually-tailored student feedback that serves both students and instructors towards enhancing student performance over time. This proposal advances the impact of co-robots into educational research and practice and extends knowledge of how to succinctly represent the complexities of human behavior in digital form.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Conrad",
   "pi_last_name": "Tucker",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Conrad S Tucker",
   "pi_email_addr": "conradt@andrew.cmu.edu",
   "nsf_id": "000612280",
   "pi_start_date": "2015-08-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Timothy",
   "pi_last_name": "Brick",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Timothy R Brick",
   "pi_email_addr": "trb21@psu.edu",
   "nsf_id": "000684469",
   "pi_start_date": "2015-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "The Pennsylvania State University",
  "perf_str_addr": "Hammond Building",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021400",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 342574.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Understanding the challenges that each student experiences while performing in-class STEM activities (e.g., engineering lab) is difficult to capture and quantify due to the number of students per instructor, limited historical data captured for each student, and limited feedback provided to each student regarding their progress throughout the semester. Co-robots, designed and assembled using&nbsp; commercial, off-the-shelf technologies, provide i) students with an introduction to co-robots in STEM, while minimizing the complexities often associated with robotics (i.e., students are already familiar with gaming devices and tablets, which are the main components of the STEM co-robots developed in this project), and ii) a practical means of capturing, storing and mining students&rsquo; real time experiences during STEM activities in ways that provide customized, real time feedback to students. This project advances education research beyond qualitative means of capturing students&rsquo; performance during STEM activities, towards &nbsp;quantitative models of student performance that can be used by co-robots (learning about STEM environments and student-object associations), students (learning how to overcome challenges during STEM activities, based on co-robot feedback) and instructors (learning how to enhance the STEM pedagogical experience, which has been reported to be a strong influencing factor impacting student retention in STEM).</p>\n<p>The integration of co-robots in STEM education has the potential to provide deeper insights into the challenges experienced by students from underrepresented groups that may not be well understood or go unreported.</p>\n<p>The outcomes of this project extend far beyond education and include healthcare and cybersecurity. The facial keypoint algorithms that co-robots use to quantify students&rsquo; affective states, can be adapted to capture healthcare data such as a patients&rsquo; pain level, their heart rate variations, etc. From a cybersecurity perspective, computer vision algorithms have the potential to identify misinformation resulting in videos and images that have been manipulated.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/18/2018<br>\n\t\t\t\t\tModified by: Conrad&nbsp;S&nbsp;Tucker</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1527148/1527148_10386455_1537244506668_NSFreport--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1527148/1527148_10386455_1537244506668_NSFreport--rgov-800width.jpg\" title=\"Award 1527148 Highlights\"><img src=\"/por/images/Reports/POR/2018/1527148/1527148_10386455_1537244506668_NSFreport--rgov-66x44.jpg\" alt=\"Award 1527148 Highlights\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Highlights from Award 1527148</div>\n<div class=\"imageCredit\">Penn State</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Conrad&nbsp;S&nbsp;Tucker</div>\n<div class=\"imageTitle\">Award 1527148 Highlights</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nUnderstanding the challenges that each student experiences while performing in-class STEM activities (e.g., engineering lab) is difficult to capture and quantify due to the number of students per instructor, limited historical data captured for each student, and limited feedback provided to each student regarding their progress throughout the semester. Co-robots, designed and assembled using  commercial, off-the-shelf technologies, provide i) students with an introduction to co-robots in STEM, while minimizing the complexities often associated with robotics (i.e., students are already familiar with gaming devices and tablets, which are the main components of the STEM co-robots developed in this project), and ii) a practical means of capturing, storing and mining students? real time experiences during STEM activities in ways that provide customized, real time feedback to students. This project advances education research beyond qualitative means of capturing students? performance during STEM activities, towards  quantitative models of student performance that can be used by co-robots (learning about STEM environments and student-object associations), students (learning how to overcome challenges during STEM activities, based on co-robot feedback) and instructors (learning how to enhance the STEM pedagogical experience, which has been reported to be a strong influencing factor impacting student retention in STEM).\n\nThe integration of co-robots in STEM education has the potential to provide deeper insights into the challenges experienced by students from underrepresented groups that may not be well understood or go unreported.\n\nThe outcomes of this project extend far beyond education and include healthcare and cybersecurity. The facial keypoint algorithms that co-robots use to quantify students? affective states, can be adapted to capture healthcare data such as a patients? pain level, their heart rate variations, etc. From a cybersecurity perspective, computer vision algorithms have the potential to identify misinformation resulting in videos and images that have been manipulated.\n\n\t\t\t\t\tLast Modified: 09/18/2018\n\n\t\t\t\t\tSubmitted by: Conrad S Tucker"
 }
}