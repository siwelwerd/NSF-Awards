{
 "awd_id": "1546500",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Collaborative Research: F: Stochastic Approximation for Subspace and Multiview Representation Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 394518.0,
 "awd_amount": 402518.0,
 "awd_min_amd_letter_date": "2015-09-03",
 "awd_max_amd_letter_date": "2019-07-15",
 "awd_abstract_narration": "Unsupervised learning of useful features, or representations, is one of the most basic challenges of machine learning. Unsupervised representation learning techniques capitalize on unlabeled data which is often cheap and abundant and sometimes virtually unlimited. The goal of these ubiquitous techniques is to learn a representation that reveals intrinsic low-dimensional structure in data, disentangles underlying factors of variation by incorporating universal AI priors such as smoothness and sparsity, and is useful across multiple tasks and domains. \r\n\r\nThis project aims to develop new theory and methods for representation learning that can easily scale to large datasets. In particular, this project is concerned with methods for large-scale unsupervised feature learning, including Principal Component Analysis (PCA) and Partial Least Squares (PLS). To capitalize on massive amounts of unlabeled data, this project will develop appropriate computational approaches and study them in the ?data laden? regime. Therefore, instead of viewing representation learning as dimensionality reduction techniques and focusing on an empirical objective on finite data, these methods are studied with the goal of optimizing a population objective based on sample. This view suggests using Stochastic Approximation approaches, such as Stochastic Gradient Descent (SGD) and Stochastic Mirror Descent, that are incremental in nature and process each new sample with a computationally cheap update. Furthermore, this view enables a rigorous analysis of benefits of stochastic approximation algorithms over traditional finite-data methods. The project aims to develop stochastic approximation approaches to PCA and PLS and related problems and extensions, including deep, and sparse variants, and analyze these problems in the data-laden regime.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nathan",
   "pi_last_name": "Srebro",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nathan Srebro",
   "pi_email_addr": "nati@ttic.edu",
   "nsf_id": "000489181",
   "pi_start_date": "2015-09-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Toyota Technological Institute at Chicago",
  "inst_street_address": "6045 S KENWOOD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7738340409",
  "inst_zip_code": "606372803",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO",
  "org_prnt_uei_num": "ERBJF4DMW6G4",
  "org_uei_num": "ERBJF4DMW6G4"
 },
 "perf_inst": {
  "perf_inst_name": "Toyota Technological Institute at Chicago",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606372902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 394518.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-109d8f95-7fff-261c-1fc8-5b32d9cab4e3\"><br /> </span></p>\n<p dir=\"ltr\"><span>The primary objective of the project was developing modern large scale methods for finding relevant dimensions, directions to subspaces in data.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The first part of the project concerned development of stochastic and distributed methods for handling large scale linear dimensionality reduction methods, both based on single-view data (as in PCA) and based on multi-view data (as in CCA and PLS).&nbsp; Finding the most important linear directions in the data, or the directions that are most conserved in multiple views, has been a basic building block in statistics, data analysis and machine learning, for decades.&nbsp; Multi-view methods in particular are important in leveraging the relationship between different modalities, such as audio and visual queues, or the relationship between multiple related tasks.&nbsp; We develop methods that allow doing so on a much larger scale where traditional linear-algebraic methods cannot go. These methods employ recent developments on stochastic optimization, that is working on only samples from the data at each step instead of the entire dataset, which might be too large to handle in its entirety.&nbsp; The methods also allow using multiple computers in a distributed fashion, which is often necessary for handling massive data sets.&nbsp; Due to the importance of multi-view dimensionality reduction techniques and the increase in data set sizes in the past decade, these advances can have broad impact across scientific and engineering applications.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>In the second part of the project we went beyond linear models and investigated generalizations and extensions to dimensionality reduction, and more generally to the notion of &ldquo;dimension&rdquo;, and to learning low dimensional representations. &nbsp; We investigated theoretical notions of generalized dimensions, and deep learning approaches to learning low dimensional representations, as well as other notions of finding relevant directions.&nbsp; In particular, we consider the important problem of generalizing from a few environments to other very different environments (e.g. generalizing a pedestrian detection system trained on data from a few cities with certain cameras, to other cities with different visual characteristics and for images captured by cameras with different optics).&nbsp; A possible approach to such generalization is by learning relevant dimensions in the data that are invariant across environments.&nbsp; Our research elucidates the form of invariants that can and cannot be captured using such methods.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Related to the main theme of the project, we also investigated several other directions that emerged during the course of the research: (1) learning predictors that are robust to malicious perturbations in the input; and (2) methods for obtaining valid statistical answers to arbitrarily adaptive statistical queries.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Beyond its direct impact through research, multiple graduate students and post-doctoral researchers received training and mentorship through the project, including a female post-doctoral researcher.&nbsp; The two post-doctoral researchers involved in the project are now working in top technology companies (Google and Microsoft), helping bring research ideas to practice.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/31/2022<br>\n\t\t\t\t\tModified by: Nathan&nbsp;Srebro</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n \nThe primary objective of the project was developing modern large scale methods for finding relevant dimensions, directions to subspaces in data. \n\n \nThe first part of the project concerned development of stochastic and distributed methods for handling large scale linear dimensionality reduction methods, both based on single-view data (as in PCA) and based on multi-view data (as in CCA and PLS).  Finding the most important linear directions in the data, or the directions that are most conserved in multiple views, has been a basic building block in statistics, data analysis and machine learning, for decades.  Multi-view methods in particular are important in leveraging the relationship between different modalities, such as audio and visual queues, or the relationship between multiple related tasks.  We develop methods that allow doing so on a much larger scale where traditional linear-algebraic methods cannot go. These methods employ recent developments on stochastic optimization, that is working on only samples from the data at each step instead of the entire dataset, which might be too large to handle in its entirety.  The methods also allow using multiple computers in a distributed fashion, which is often necessary for handling massive data sets.  Due to the importance of multi-view dimensionality reduction techniques and the increase in data set sizes in the past decade, these advances can have broad impact across scientific and engineering applications.\n\n \nIn the second part of the project we went beyond linear models and investigated generalizations and extensions to dimensionality reduction, and more generally to the notion of \"dimension\", and to learning low dimensional representations.   We investigated theoretical notions of generalized dimensions, and deep learning approaches to learning low dimensional representations, as well as other notions of finding relevant directions.  In particular, we consider the important problem of generalizing from a few environments to other very different environments (e.g. generalizing a pedestrian detection system trained on data from a few cities with certain cameras, to other cities with different visual characteristics and for images captured by cameras with different optics).  A possible approach to such generalization is by learning relevant dimensions in the data that are invariant across environments.  Our research elucidates the form of invariants that can and cannot be captured using such methods.\n\n \nRelated to the main theme of the project, we also investigated several other directions that emerged during the course of the research: (1) learning predictors that are robust to malicious perturbations in the input; and (2) methods for obtaining valid statistical answers to arbitrarily adaptive statistical queries.\n\n \nBeyond its direct impact through research, multiple graduate students and post-doctoral researchers received training and mentorship through the project, including a female post-doctoral researcher.  The two post-doctoral researchers involved in the project are now working in top technology companies (Google and Microsoft), helping bring research ideas to practice.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/31/2022\n\n\t\t\t\t\tSubmitted by: Nathan Srebro"
 }
}