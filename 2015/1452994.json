{
 "awd_id": "1452994",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: A Hardware and Software Architecture for Data-Centric Parallel Computing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2015-02-15",
 "awd_exp_date": "2020-01-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2015-01-21",
 "awd_max_amd_letter_date": "2019-02-27",
 "awd_abstract_narration": "Energy efficiency is the key challenge facing computer systems. To improve\r\nperformance under a limited energy budget, systems are becoming increasingly\r\nparallel, featuring many smaller and simpler cores, and heterogeneous,\r\nfeaturing cores specialized for certain tasks. Even with these improvements,\r\ntwo critical challenges remain. First, without reducing data movement, memory\r\naccesses and communication will dominate energy consumption. Thus, limiting\r\ndata movement must become a primary design objective. Second, these systems\r\nwill be highly complex, and will need powerful abstractions to shield\r\nprogrammers from this complexity. Current systems are designed in a\r\ncomputation-centric way that is a poor match for these challenges. Memory\r\nhierarchies are hardware-managed and opaque to software, which needlessly\r\nincreases data movement; and runtimes lack the proper hardware mechanisms and\r\nsoftware policies to manage heterogeneous resources efficiently.\r\n\r\nThis research project takes a holistic approach to addressing these challenges, by\r\nco-designing an architecture and runtime system that efficiently run dynamic\r\nparallel applications on systems with heterogeneous cores and memories.\r\nRedesigning hardware to be directly exploited by a dynamic runtime enables (a)\r\nmany more opportunities to reduce data movement, (b) better usage of\r\nheterogeneous resources, and (c) much faster adaptation to changing application\r\nneeds and available resources. Three key components underlie this design.\r\nFirst, a scalable memory system incorporates combinations of heterogeneous\r\nmemory technologies to improve efficiency, and exposes them to software, which\r\ncan divide these physical memories into many virtual cache and memory\r\nhierarchies to finely control data placement. Second, specialized programmable\r\nengines orchestrate communication among cores, accelerate intensive runtime\r\nfunctions such as load balancing, and monitor how tasks use hardware resources\r\nto guide runtime decisions. Third, a hardware-accelerated runtime leverages\r\nthis novel architectural support to place data and computation to minimize data\r\nmovement, use the most suitable core for each task, and quickly respond to\r\nchanging application needs. This runtime targets a high-level programming model\r\nthat lets programmers express fine-grained and irregular task, data, and\r\npipeline parallelism. These techniques build on an analytical design approach\r\nthat makes hardware easy to understand and predict, and enables runtimes to\r\nnavigate multi-dimensional tradeoffs efficiently.\r\n\r\nIf successful, this project will make heterogeneous systems more efficient,\r\nmore broadly applicable, and easier to program. It will especially benefit\r\napplications with dynamic and fine-grained parallelism, advancing key emerging\r\ndomains where these workloads are pervasive, such as graph analytics and online\r\ndata-intensive services. In addition, the infrastructure developed as part of\r\nthis project will be publicly released, enabling others to build on the results\r\nof this work.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Sanchez Martin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Sanchez Martin",
   "pi_email_addr": "sanchez@csail.mit.edu",
   "nsf_id": "000636815",
   "pi_start_date": "2015-01-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 200000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 200000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Computer systems are becoming increasingly parallel and specialized, and improving performance requires exploiting high degrees of parallelism. For example, multicore chips feature tens to hundreds cores, and specialized accelerators such as GPUs execute thousands of concurrent computation streams. However, parallelism and specialization alone are insufficient to improve performance in a sustainable way, due to three key challenges. First, memory accesses and communication limit performance in current systems, requiring new techniques to minimize data movement. Second, parallelism is limited in many applications, and current systems squander most of this available parallelism. Third, current parallel and heterogeneous systems are extremely complex, and need powerful abstractions to shield programmers from this complexity.&nbsp;</p>\n<p><span> <br /><strong>Intellectual Merit: </strong>This project has taken a holistic approach to address the challenges of data movement, limited parallelism, and complexity. We have co-designed hardware support, programming models, and runtime and compiler techniques that efficiently run dynamic parallel applications on systems with heterogeneous cores and memories. Our research has produced the following main outcomes: <br /> <br />First, we have investigated and designed <em>software-defined memory hierarchies</em>, a reconfigurable memory system that transparently adapts to application needs. Hardware and a software runtime collaborate to profile applications and learn high-performance configurations, building near-optimal memory hierarchies for each application. Moreover, we have designed hardware and runtime techniques that place both data and computation across the system to further reduce data movement. These techniques achieve order-of-magnitude reductions in data movement, and leverage both heterogeneous memory technologies and heterogeneous cores without exposing their complexity to programmers. <br /> <br />Second, we have investigated techniques to reduce data movement and communication on <em>irregular and dynamic applications</em>. Irregular applications, such as graph analytics, are dominated by memory accesses and synchronization. They use existing hardware poorly, which has been optimized for regular applications like linear algebra. To address this issue, we have designed simple hardware techniques that exploit high-level program semantics, such as commutativity and task structure, to avoid synchronization and dramatically reduce memory accesses. These techniques improve performance by 2-11x and achieve order-of-magnitude data movement reductions on applications that operate on irregular data structures, like graphs and hash tables. <br /> <br />Third, we have designed <em>object-based memory hierarchies</em>, a new memory hierarchy organization that exploits the object-based semantics of modern programming languages like Java, Rust, or Go. This memory system is built from the ground up to move and manage variable-sized objects instead of fixed-size chunks (e.g., cache lines) as in existing systems. Managing objects matches application-level semantics, reducing data movement. A new hardware-software interface hides the memory layout, and new hardware techniques exploit this interface to avoid the overheads of conventional memory hierarchies, like associative lookups and address translation, and to enable new optimizations, like object-based compression. As a result, object-based hierarchies improve performance by 30%-3x and reduce memory hierarchy energy by 2.6x. <br /> <br />Fourth, we have designed <em>Swarm</em>, a new parallel architecture for efficient fine-grain parallelism and synchronization. Swarm programs consist of tiny tasks with programmer-specified timestamps. Swarm runs tasks speculatively and out of order, and efficiently speculates thousands of tasks ahead of the earliest active task to uncover ordered parallelism. We have designed novel hardware support and programming models that make ordered speculation scalable and very efficient. Swarm makes parallel programming easier and uncovers abundant parallelism, scaling applications that are conventionally deemed sequential. At 256 cores, Swarm outperforms conventional parallel algorithms by one to two orders of magnitude, and achieves speedups of up to 600x. We have shown that Swarm's techniques can be used to reduce data movement, applied them to build specialized accelerators, and leveraged them to design a compiler that automatically parallelizes sequential applications. <br /> <br />To prototype and evaluate these techniques, we have developed a substantial amount of infrastructure, including state-of-the-art simulators, runtimes, benchmarks, an FPGA accelerator, and a compiler. We have released this infrastructure under open-source licenses, allowing others to build on the results of our work, both in research and in the classroom. <br /> <br />The results of this project have been disseminated through 24 publications in top-tier venues, which have been recognized through several awards. <br /><br /><strong>Broader Impacts: </strong>The techniques developed in this project make parallel and heterogeneous systems faster, more efficient, much more broadly applicable, and easier to program. These techniques especially benefit dynamic, irregular applications, advancing key emerging domains where these workloads are pervasive, such as big-data analytics and online data-intensive services. By innovating at both the hardware and software levels, these techniques have achieved performance and efficiency gains that neither hardware-only nor software-only approaches can provide. <br /> <br />Finally, this project has supported the training and professional development of 12 Ph.D. students, 4 M.Eng. students, and 13 undergraduate students. Five students have earned their doctoral degrees working under this project, and have graduated to leading research positions in academia and industry.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/29/2020<br>\n\t\t\t\t\tModified by: Daniel&nbsp;Sanchez Martin</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nComputer systems are becoming increasingly parallel and specialized, and improving performance requires exploiting high degrees of parallelism. For example, multicore chips feature tens to hundreds cores, and specialized accelerators such as GPUs execute thousands of concurrent computation streams. However, parallelism and specialization alone are insufficient to improve performance in a sustainable way, due to three key challenges. First, memory accesses and communication limit performance in current systems, requiring new techniques to minimize data movement. Second, parallelism is limited in many applications, and current systems squander most of this available parallelism. Third, current parallel and heterogeneous systems are extremely complex, and need powerful abstractions to shield programmers from this complexity. \n\n \nIntellectual Merit: This project has taken a holistic approach to address the challenges of data movement, limited parallelism, and complexity. We have co-designed hardware support, programming models, and runtime and compiler techniques that efficiently run dynamic parallel applications on systems with heterogeneous cores and memories. Our research has produced the following main outcomes: \n \nFirst, we have investigated and designed software-defined memory hierarchies, a reconfigurable memory system that transparently adapts to application needs. Hardware and a software runtime collaborate to profile applications and learn high-performance configurations, building near-optimal memory hierarchies for each application. Moreover, we have designed hardware and runtime techniques that place both data and computation across the system to further reduce data movement. These techniques achieve order-of-magnitude reductions in data movement, and leverage both heterogeneous memory technologies and heterogeneous cores without exposing their complexity to programmers. \n \nSecond, we have investigated techniques to reduce data movement and communication on irregular and dynamic applications. Irregular applications, such as graph analytics, are dominated by memory accesses and synchronization. They use existing hardware poorly, which has been optimized for regular applications like linear algebra. To address this issue, we have designed simple hardware techniques that exploit high-level program semantics, such as commutativity and task structure, to avoid synchronization and dramatically reduce memory accesses. These techniques improve performance by 2-11x and achieve order-of-magnitude data movement reductions on applications that operate on irregular data structures, like graphs and hash tables. \n \nThird, we have designed object-based memory hierarchies, a new memory hierarchy organization that exploits the object-based semantics of modern programming languages like Java, Rust, or Go. This memory system is built from the ground up to move and manage variable-sized objects instead of fixed-size chunks (e.g., cache lines) as in existing systems. Managing objects matches application-level semantics, reducing data movement. A new hardware-software interface hides the memory layout, and new hardware techniques exploit this interface to avoid the overheads of conventional memory hierarchies, like associative lookups and address translation, and to enable new optimizations, like object-based compression. As a result, object-based hierarchies improve performance by 30%-3x and reduce memory hierarchy energy by 2.6x. \n \nFourth, we have designed Swarm, a new parallel architecture for efficient fine-grain parallelism and synchronization. Swarm programs consist of tiny tasks with programmer-specified timestamps. Swarm runs tasks speculatively and out of order, and efficiently speculates thousands of tasks ahead of the earliest active task to uncover ordered parallelism. We have designed novel hardware support and programming models that make ordered speculation scalable and very efficient. Swarm makes parallel programming easier and uncovers abundant parallelism, scaling applications that are conventionally deemed sequential. At 256 cores, Swarm outperforms conventional parallel algorithms by one to two orders of magnitude, and achieves speedups of up to 600x. We have shown that Swarm's techniques can be used to reduce data movement, applied them to build specialized accelerators, and leveraged them to design a compiler that automatically parallelizes sequential applications. \n \nTo prototype and evaluate these techniques, we have developed a substantial amount of infrastructure, including state-of-the-art simulators, runtimes, benchmarks, an FPGA accelerator, and a compiler. We have released this infrastructure under open-source licenses, allowing others to build on the results of our work, both in research and in the classroom. \n \nThe results of this project have been disseminated through 24 publications in top-tier venues, which have been recognized through several awards. \n\nBroader Impacts: The techniques developed in this project make parallel and heterogeneous systems faster, more efficient, much more broadly applicable, and easier to program. These techniques especially benefit dynamic, irregular applications, advancing key emerging domains where these workloads are pervasive, such as big-data analytics and online data-intensive services. By innovating at both the hardware and software levels, these techniques have achieved performance and efficiency gains that neither hardware-only nor software-only approaches can provide. \n \nFinally, this project has supported the training and professional development of 12 Ph.D. students, 4 M.Eng. students, and 13 undergraduate students. Five students have earned their doctoral degrees working under this project, and have graduated to leading research positions in academia and industry.\n\n\t\t\t\t\tLast Modified: 05/29/2020\n\n\t\t\t\t\tSubmitted by: Daniel Sanchez Martin"
 }
}