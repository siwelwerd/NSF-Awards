{
 "awd_id": "1525919",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Collaborative Research: Towards Interpretable Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Wei Ding",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2015-08-19",
 "awd_max_amd_letter_date": "2020-06-15",
 "awd_abstract_narration": "This research project investigates the design and development of machine learning algorithms that make decisions that are interpretable by humans.  As predictions of machine learning models are increasingly used in making decisions with critical consequences (e.g., in medicine or economics), it is important that decision makers understand the rationale behind these predictions.  The project defines interpretable algorithms through three key properties; Simplicity: intuitively comprehensible by users who are not experts in machine learning, Verifiability: a clear relationship between input features and model output, and Actionability: For a given input and desired output, the user should be able to identify changes to the input features that transform the model prediction to the desired output.  The project investigates how to design distance metrics supporting simplicity and verifiability, as well as algorithms to identify input changes to change outputs.  The project will be evaluated in a medical context, addressing the problem of early detection of hospital patients at risk of sudden deterioration.\r\n\r\nThis work builds on the well-understood k-Nearest-Neighbor classifier, which would inherently seem to provide simplicity and verifiability.  The challenge is in high dimensions, e.g., when used for document classification; differences are spread across more dimensions than are humanly comprehensible.  The project uses novel dimensionality reduction approaches to create dissimilarity metrics that are interpretable and accurate.  Visualization techniques to present this data will be explored, including techniques supporting more complex classification approaches such as ensembles.  The project investigates novel methods for delivering actionability in machine learning algorithms by identifying changes that can truly transform an entity's class membership - a problem that has recently been identified as surprisingly difficult.  A secondary outcome will be improvements in classifier robustness, as small changes that change class membership are a good indication of non-robustness.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kilian",
   "pi_last_name": "Weinberger",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kilian Weinberger",
   "pi_email_addr": "kilianweinberger@cornell.edu",
   "nsf_id": "000576980",
   "pi_start_date": "2015-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "402 Gates Hall",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148537501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As part of this research effort we investigated how to make machine learning algorithms more interpretable and easier to understand.&nbsp;Our first research thrust was to look into instance based algorithms that classify data points by similarity to instances in a data base (e.g. k-nearest neighbors). In general these algorithms are more interpretable, because a certificate (the similar example in a data base) can be produced. We developed the Word Mover's Distance, which defines a new distance function between documents, and later the BertScore (ICLR 2020). The publications have accumulated thousands of citations and have introduced a new way to think about document distances. Essentially, we compute the distance between two documents as the amount of distance each word would have to \"travel\" if it had to move from its own embedding location to an embedding of a word in the other document.&nbsp;<br />We further investigated the inner workings of neural networks, and developed \"Deep learning with stochastic depth\", a method to regulate redundancy inside deep networks. Previous research had indicated that redundancy plays an important role in generalization of deep neural networks. With our stochastic depth approach we introduced a regulator to increase (!) the redundancy of deep architectures. We could show that this increase does indeed result in improved generalization.&nbsp;These findings lead to the development of DenseNets, a novel architecture that inherits the benefits of stochastic depth, without increasing redundancy (and therefore wasting computation). DenseNets connect all layers within a network with all others (of matching size), instead of the more predominant layer-by-layer pipeline connectivity. DenseNets have since become standard approaches within the machine learning and vision community.&nbsp;<br />Another approach to obtain interpretable machine learning algorithms is to make their outputs well defined probabilities. We therefore analyzed the calibration of neural network predictions and showed that they are typically not well calibrated. This mis-calibration originates from an overfitting process with respect to the &nbsp;loss function. We showed that a simple temperature rescaling process can counter this effect and make predictions better calibrated. Temperature scaling has since become a standard tool to improve the output interpretability of neural network classifiers. In follow-up research we examined the implications of calibrated machine learning algorithms and reasonable definitions of fairness.&nbsp;<br />Recent years have seen an explosion of research on Graph Convolutional Neural Networks. In 2019 we investigated the inner workings of these approaches and showed that often times they are overly complex. A common approach in the research community was to translate deep convolutional neural networks from the vision literature to graph applications. This approach has resulted in overly deep and complicated networks. We showed that these are often not necessary. Instead, one can use a simple graph propagation algorithm, followed by logistic regression, to obtain similar results (in a fraction of the training time).&nbsp;<br />In summary, the research for this grant was a great success. We developed new metric learning algorithms, contributed significantly to the understanding of deep neural networks, introduced new architectures that have been broadly adopted, and simplified existing algorithms to a degree where they are easily understood.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/23/2021<br>\n\t\t\t\t\tModified by: Kilian&nbsp;Weinberger</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAs part of this research effort we investigated how to make machine learning algorithms more interpretable and easier to understand. Our first research thrust was to look into instance based algorithms that classify data points by similarity to instances in a data base (e.g. k-nearest neighbors). In general these algorithms are more interpretable, because a certificate (the similar example in a data base) can be produced. We developed the Word Mover's Distance, which defines a new distance function between documents, and later the BertScore (ICLR 2020). The publications have accumulated thousands of citations and have introduced a new way to think about document distances. Essentially, we compute the distance between two documents as the amount of distance each word would have to \"travel\" if it had to move from its own embedding location to an embedding of a word in the other document. \nWe further investigated the inner workings of neural networks, and developed \"Deep learning with stochastic depth\", a method to regulate redundancy inside deep networks. Previous research had indicated that redundancy plays an important role in generalization of deep neural networks. With our stochastic depth approach we introduced a regulator to increase (!) the redundancy of deep architectures. We could show that this increase does indeed result in improved generalization. These findings lead to the development of DenseNets, a novel architecture that inherits the benefits of stochastic depth, without increasing redundancy (and therefore wasting computation). DenseNets connect all layers within a network with all others (of matching size), instead of the more predominant layer-by-layer pipeline connectivity. DenseNets have since become standard approaches within the machine learning and vision community. \nAnother approach to obtain interpretable machine learning algorithms is to make their outputs well defined probabilities. We therefore analyzed the calibration of neural network predictions and showed that they are typically not well calibrated. This mis-calibration originates from an overfitting process with respect to the  loss function. We showed that a simple temperature rescaling process can counter this effect and make predictions better calibrated. Temperature scaling has since become a standard tool to improve the output interpretability of neural network classifiers. In follow-up research we examined the implications of calibrated machine learning algorithms and reasonable definitions of fairness. \nRecent years have seen an explosion of research on Graph Convolutional Neural Networks. In 2019 we investigated the inner workings of these approaches and showed that often times they are overly complex. A common approach in the research community was to translate deep convolutional neural networks from the vision literature to graph applications. This approach has resulted in overly deep and complicated networks. We showed that these are often not necessary. Instead, one can use a simple graph propagation algorithm, followed by logistic regression, to obtain similar results (in a fraction of the training time). \nIn summary, the research for this grant was a great success. We developed new metric learning algorithms, contributed significantly to the understanding of deep neural networks, introduced new architectures that have been broadly adopted, and simplified existing algorithms to a degree where they are easily understood. \n\n\t\t\t\t\tLast Modified: 11/23/2021\n\n\t\t\t\t\tSubmitted by: Kilian Weinberger"
 }
}