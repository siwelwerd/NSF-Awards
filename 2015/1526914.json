{
 "awd_id": "1526914",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Collaborative Research: Approximate Learning and Inference in Graphical Models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Maria Zemankova",
 "awd_eff_date": "2015-10-01",
 "awd_exp_date": "2019-09-30",
 "tot_intn_awd_amt": 164088.0,
 "awd_amount": 164088.0,
 "awd_min_amd_letter_date": "2015-09-15",
 "awd_max_amd_letter_date": "2015-09-15",
 "awd_abstract_narration": "This project is designing efficient methods for approximate learning and prediction in graphical models.  In a typical setting, the parameters of an unknown graphical model are to be estimated from data observations.  Once learned, the parameters are often used to make predictions about unseen data.  The learning problem can be solved by estimating the parameters of the model that generate the observed data with the highest probability (a process known as maximum likelihood estimation), and the prediction task is typically performed by a statistical inference method.  As exact learning and prediction are computationally intractable, in practice, we seek to replace the maximum likelihood estimation and prediction tasks with more tractable surrogates.  This project is developing such surrogates that (a) can be leveraged for learning in large, real-world graphical models with hidden variables, (b) are orders of magnitude faster than the current state-of-the-art methods, and (c) provide a rigorous alternative to the more heuristic methods that are often employed at scale.\r\n\r\nUnder certain conditions, surrogates can outperform exact maximum likelihood estimation combined with an approximate inference algorithm for prediction.  However, many of the typical approaches are much too slow or too limited in power to be used to learn the kinds of large-scale graphical models with many hidden variables that arise in practice.  This project studies the design of fast, distributed approximate learning and inference procedures based on the Bethe approximation, a surrogate that is known to perform well in practice.  The core observation is that approximate maximum likelihood estimation using the Bethe surrogate can be reduced to solving a series of approximate inference problems via the Frank-Wolfe algorithm.  The benefit of this approach is that many fast, combinatorial algorithms already exist for approximate inference in graphical models.  In addition to provable bounds and convergence rates, the methods are practically evaluated using several publicly available datasets, primarily social network and image data.  Baselines will be application-specific methods known to work well on those datasets, with the developed code made publicly available.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tony",
   "pi_last_name": "Jebara",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tony Jebara",
   "pi_email_addr": "jebara@cs.columbia.edu",
   "nsf_id": "000093679",
   "pi_start_date": "2015-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "2960 Broadway",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 164088.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project designed efficient methods for scalable, approximate learning and prediction in graphical models.&nbsp; The work was motivated by the limitations of existing statistical models:&nbsp; they were difficult to apply to the kinds of high dimensional data prediction problems with continuous features that are typical in scientific and machine learning applications.&nbsp; As a result, only simple statistical models had been considered, even if these simple models made unreasonable data modeling assumptions.&nbsp; Here, we developed general methods that can efficiently handle complicated heterogeneous data sets by transforming prediction tasks over complicated models into simpler prediction tasks using a variational method known as the Bethe free energy (BFE) approximation.&nbsp; The approach makes few assumptions about the underlying model and can be applied to yield approximations of the most popular prediction tasks in statistical models: marginal inference (predicting the probability of an outcome under the model), maximum a posteriori (MAP) inference (computing the most likely outcome), and marginal MAP inference (computing the most likely outcome under a marginal distribution).<br /><br />We provided rigorous theoretical characterizations of the Bethe approximation.&nbsp; In particular, we showed that the notion of attractive graphical models, which are often used to model cooperative games in economics, can be extended to the setting of continuous random variables and enjoys many of the same properties that it does in the discrete case. Additionally, we proved that, under mild assumptions, our approach is guaranteed to outperform existing message-passing based approaches, which can fail to produce meaningful results even if the model is a multivariate Gaussian distribution. <br /><br />Furthermore, our work on the Bethe approximation and other foundational aspects of graphical modeling enabled an important link between that literature and the neural network literature. While graphical models are the canonical way of representing conditional independence structure in structured generative models, they lack the nonlinear modeling power that has recently been popularized through neural networks. We developed technical connections between graphical modeling and variational autoencoders (a recent powerful type of neural network). These connections allow practitioners to now leverage nonlinear neural modeling and embedding techniques to manipulate distributions over structured spaces such as graphs and social networks. Essentially, the aforementioned Bethe approximations extend the output of variational autoencoders to be graph-structured as opposed to iid (independent identically distributed) which was a previous limitation in the literature. Furthermore, we developed other extensions to the output space of variational autoencoders to apply beyond Gaussian assumptions to so-called Bernoulli and Multinomial spaces. We also extended the latent spaces of variational autoencoders to allow Dirichlet distributions. These extensions permits neural networks to find more parsimonious encodings of a broad swath of graphical models that admit general graph-dependency structures between Gaussian, Bernoulli and Multinomial random variables.<br /><br />The developed methods were applied to large scale computer vision problems, e.g., depth estimation, where they were able to yield fast, accurate results over high-resolution images.&nbsp; In addition, we showed that variational methods related to the Bethe free energy can be extended to improve the performance of existing approaches to statistical relational models with continuous variables, Markov logic networks, and variational autoencoders.&nbsp; In the former case, only Gaussian models had been seriously considered in the existing literature - our approach can be applied more generally. In the latter case, we demonstrated empirically that we can incorporate known information about the relationship between data observations in order to improve the quality of the variational encoding. Other applications that this effort explored included recommendation on music and movie data-sets as well as predictions on social network problems. State of the art performance was achieved on several benchmarks.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/19/2020<br>\n\t\t\t\t\tModified by: Tony&nbsp;Jebara</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project designed efficient methods for scalable, approximate learning and prediction in graphical models.  The work was motivated by the limitations of existing statistical models:  they were difficult to apply to the kinds of high dimensional data prediction problems with continuous features that are typical in scientific and machine learning applications.  As a result, only simple statistical models had been considered, even if these simple models made unreasonable data modeling assumptions.  Here, we developed general methods that can efficiently handle complicated heterogeneous data sets by transforming prediction tasks over complicated models into simpler prediction tasks using a variational method known as the Bethe free energy (BFE) approximation.  The approach makes few assumptions about the underlying model and can be applied to yield approximations of the most popular prediction tasks in statistical models: marginal inference (predicting the probability of an outcome under the model), maximum a posteriori (MAP) inference (computing the most likely outcome), and marginal MAP inference (computing the most likely outcome under a marginal distribution).\n\nWe provided rigorous theoretical characterizations of the Bethe approximation.  In particular, we showed that the notion of attractive graphical models, which are often used to model cooperative games in economics, can be extended to the setting of continuous random variables and enjoys many of the same properties that it does in the discrete case. Additionally, we proved that, under mild assumptions, our approach is guaranteed to outperform existing message-passing based approaches, which can fail to produce meaningful results even if the model is a multivariate Gaussian distribution. \n\nFurthermore, our work on the Bethe approximation and other foundational aspects of graphical modeling enabled an important link between that literature and the neural network literature. While graphical models are the canonical way of representing conditional independence structure in structured generative models, they lack the nonlinear modeling power that has recently been popularized through neural networks. We developed technical connections between graphical modeling and variational autoencoders (a recent powerful type of neural network). These connections allow practitioners to now leverage nonlinear neural modeling and embedding techniques to manipulate distributions over structured spaces such as graphs and social networks. Essentially, the aforementioned Bethe approximations extend the output of variational autoencoders to be graph-structured as opposed to iid (independent identically distributed) which was a previous limitation in the literature. Furthermore, we developed other extensions to the output space of variational autoencoders to apply beyond Gaussian assumptions to so-called Bernoulli and Multinomial spaces. We also extended the latent spaces of variational autoencoders to allow Dirichlet distributions. These extensions permits neural networks to find more parsimonious encodings of a broad swath of graphical models that admit general graph-dependency structures between Gaussian, Bernoulli and Multinomial random variables.\n\nThe developed methods were applied to large scale computer vision problems, e.g., depth estimation, where they were able to yield fast, accurate results over high-resolution images.  In addition, we showed that variational methods related to the Bethe free energy can be extended to improve the performance of existing approaches to statistical relational models with continuous variables, Markov logic networks, and variational autoencoders.  In the former case, only Gaussian models had been seriously considered in the existing literature - our approach can be applied more generally. In the latter case, we demonstrated empirically that we can incorporate known information about the relationship between data observations in order to improve the quality of the variational encoding. Other applications that this effort explored included recommendation on music and movie data-sets as well as predictions on social network problems. State of the art performance was achieved on several benchmarks.\n\n\t\t\t\t\tLast Modified: 02/19/2020\n\n\t\t\t\t\tSubmitted by: Tony Jebara"
 }
}