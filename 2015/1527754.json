{
 "awd_id": "1527754",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TWC: Small: Fundamental Limits in Differential Privacy",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927991",
 "po_email": "namla@nsf.gov",
 "po_sign_block_name": "Nina Amla",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 495190.0,
 "awd_amount": 495190.0,
 "awd_min_amd_letter_date": "2015-08-14",
 "awd_max_amd_letter_date": "2015-08-14",
 "awd_abstract_narration": "Differential Privacy has emerged as a well-grounded approach to balancing personal privacy and societal as well as commercial use of data.  The basic idea is to add random noise to analysis results sufficient to obscure the impact of any single individual's data on the analysis, thus protecting individual privacy.  While general approaches to providing differential privacy exist, in many cases the bounds are not tight; more noise is added than needed.  This project uses information theoretic techniques to explore the fundamental privacy/accuracy tradeoffs in differential privacy.  The success of the proposed research will make progress towards a safer and more secure nation where the respect for individuals' privacy is not compromised. The proposed research is strongly integrated with an education plan that aims to develop a new graduate level course on algorithmic foundations of privacy.\r\n\r\nThis project will investigate several topics: (1) characterizing the fundamental tradeoffs between the privacy guarantee and the utility of the released data, by applying information theoretic tools and methods to identify tight bounds on achieving differential privacy; (2) designing data privatization mechanisms for individuals that achieve both computational efficiency and the optimal tradeoffs between utility and privacy; and (3) providing a privacy calculus for macroscopic analyses of complex data processing systems, consisting of various components each with its own privacy guarantees. The privacy calculus aims to provide new representations and computational tools for characterizing how privacy components interact in a large system, analogous to how network calculus allows researchers to characterize complex non-linear communication systems using familiar tools from linear systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sewoong",
   "pi_last_name": "Oh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sewoong Oh",
   "pi_email_addr": "sewoong@cs.washington.edu",
   "nsf_id": "000642594",
   "pi_start_date": "2015-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618207473",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 495190.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica; color: #000000} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica; color: #000000; min-height: 14.0px} -->\n<p class=\"p1\">The objective of this proposal is to make fundamental progresses on privacy-preserving data analyses, and explore the optimal tradeoffs involved. Towards this goal, the PI makes the following two contributions: macroscopic and microscopic.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\"><span>&nbsp;</span>In macroscopic analysis of privacy, the PI provides the mathematical foundations for the macroscopic analyses of complex privacy-preserving systems. In particular,<span>&nbsp; </span>the &lsquo;privacy calculus&rsquo; provides a new representation and corresponding computational tools for characterizing the fundamental limit on how those privacy guarantees operate. This research direction is motivated by those in communication networks where network calculus allows researchers to analyze the performance guarantee in a complex non-linear communication systems using familiar tools from linear systems. The PI borrows techniques from decision theory and Blackwell's famous comparisons theorem to design a new representation of privacy preserving mechanisms as 2-dimensional regions. This region perspective allows one to measure precisely how privacy preserving mechanisms interact. Given a series of interactive accesses to a database, with differing privacy guarantees and accessing differing parts of the database, this gives a novel tool that can calculate how much privacy is lost by the composition of those mechanisms. This is critical as the modern privacy preserving systems require a large number of accesses to the database, as in training deep neural networks in a differentially private manner. This will continue to have impact on how privacy is analyzed, as the need for privacy is increasing and legal requirements are getting stricter. Further, the analysis technique developed in this project has a large implication in other domains where binary hypothesis testing is central, namely adversarial training. Generative adversarial training is emerging as a central tool in unsupervised learning. As binary hypothesis testing is at the center of the architecture of the training, the PI brings the tools from privacy analysis to analyze adversarial training. In particular, it is shown that one of the fundamental challenges in adversarial training known as mode collapse can be elegantly mitigated with a novel discriminator that takes as input multiple samples at the same time. This design is entirely derived from the region definition of privacy, which in this setting turns out to be a measure of how severe mode collapse is. Mode collapse refers to the phenomenon where samples from the trained generator has much less diversity compared to the training data.<span>&nbsp;</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">In the microscopic analysis, the PI provides analytical characterizations (and numerical computational methods) for the fundamental tradeoff between privacy and utility under various canonical scenarios. This research direction is motivated by those in information theory where exact results on fundamental limits are highly appreciated. The PI designs efficient mechanisms that achieve the optimal tradeoff between utility and privacy. This research direction is motivated by those in coding theory where efficient methods that achieve the optimal performance are highly appreciated. Even though the problem of finding the utility maximizing privacy mechanism is oftentimes computationally intractable, the PI introduces a necessary structure that all optimal mechanisms must satisfy. By utilizing this structure to guide the search for an optimal mechanism, this gives an efficient and explicit construction of optimal mechanisms for a broad range of applications of interest, including hypothesis testing, multi-party computation, and noise adding mechanisms. This construction in turn will provide the characterization of the fundamental utility-privacy tradeoffs, and guide the practical design of efficient and effective mechanisms for privacy-aware applications such as telemetry. In particular, we design staircase mechanisms, which is a novel family of mechanisms that include the set of optimal mechanisms in many applications. Exploiting specific staircase-like structure, one can find which staircase mechanism is optimal for certain settings, and identify the tradeoffs involved. By replacing sub-optimal algorithms that are prevalent in practice, staircase mechanisms can significantly improve the utility of privacy preserving mechanisms.<span>&nbsp;</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">On top of making fundamental progresses in privacy-aware data analyses, the<span>&nbsp; </span>results are playing crucial roles in privacy analysis in large scale machine learning algorithms to private data release, such as US Census. This allows easy access to privacy mechanisms for developers and also the public, by allowing data analysts to seamlessly embed privacy guarantees into the design. The research is strongly integrated with a graduate course on statistical analysis of data and mentor undergraduate research activities.</p>\n<p class=\"p2\">&nbsp;</p>\n<p>&nbsp;</p>\n</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/22/2020<br>\n\t\t\t\t\tModified by: Sewoong&nbsp;Oh</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\nThe objective of this proposal is to make fundamental progresses on privacy-preserving data analyses, and explore the optimal tradeoffs involved. Towards this goal, the PI makes the following two contributions: macroscopic and microscopic.\n \n In macroscopic analysis of privacy, the PI provides the mathematical foundations for the macroscopic analyses of complex privacy-preserving systems. In particular,  the \u2018privacy calculus\u2019 provides a new representation and corresponding computational tools for characterizing the fundamental limit on how those privacy guarantees operate. This research direction is motivated by those in communication networks where network calculus allows researchers to analyze the performance guarantee in a complex non-linear communication systems using familiar tools from linear systems. The PI borrows techniques from decision theory and Blackwell's famous comparisons theorem to design a new representation of privacy preserving mechanisms as 2-dimensional regions. This region perspective allows one to measure precisely how privacy preserving mechanisms interact. Given a series of interactive accesses to a database, with differing privacy guarantees and accessing differing parts of the database, this gives a novel tool that can calculate how much privacy is lost by the composition of those mechanisms. This is critical as the modern privacy preserving systems require a large number of accesses to the database, as in training deep neural networks in a differentially private manner. This will continue to have impact on how privacy is analyzed, as the need for privacy is increasing and legal requirements are getting stricter. Further, the analysis technique developed in this project has a large implication in other domains where binary hypothesis testing is central, namely adversarial training. Generative adversarial training is emerging as a central tool in unsupervised learning. As binary hypothesis testing is at the center of the architecture of the training, the PI brings the tools from privacy analysis to analyze adversarial training. In particular, it is shown that one of the fundamental challenges in adversarial training known as mode collapse can be elegantly mitigated with a novel discriminator that takes as input multiple samples at the same time. This design is entirely derived from the region definition of privacy, which in this setting turns out to be a measure of how severe mode collapse is. Mode collapse refers to the phenomenon where samples from the trained generator has much less diversity compared to the training data. \n \nIn the microscopic analysis, the PI provides analytical characterizations (and numerical computational methods) for the fundamental tradeoff between privacy and utility under various canonical scenarios. This research direction is motivated by those in information theory where exact results on fundamental limits are highly appreciated. The PI designs efficient mechanisms that achieve the optimal tradeoff between utility and privacy. This research direction is motivated by those in coding theory where efficient methods that achieve the optimal performance are highly appreciated. Even though the problem of finding the utility maximizing privacy mechanism is oftentimes computationally intractable, the PI introduces a necessary structure that all optimal mechanisms must satisfy. By utilizing this structure to guide the search for an optimal mechanism, this gives an efficient and explicit construction of optimal mechanisms for a broad range of applications of interest, including hypothesis testing, multi-party computation, and noise adding mechanisms. This construction in turn will provide the characterization of the fundamental utility-privacy tradeoffs, and guide the practical design of efficient and effective mechanisms for privacy-aware applications such as telemetry. In particular, we design staircase mechanisms, which is a novel family of mechanisms that include the set of optimal mechanisms in many applications. Exploiting specific staircase-like structure, one can find which staircase mechanism is optimal for certain settings, and identify the tradeoffs involved. By replacing sub-optimal algorithms that are prevalent in practice, staircase mechanisms can significantly improve the utility of privacy preserving mechanisms. \n \nOn top of making fundamental progresses in privacy-aware data analyses, the  results are playing crucial roles in privacy analysis in large scale machine learning algorithms to private data release, such as US Census. This allows easy access to privacy mechanisms for developers and also the public, by allowing data analysts to seamlessly embed privacy guarantees into the design. The research is strongly integrated with a graduate course on statistical analysis of data and mentor undergraduate research activities.\n \n\n \n\n\n\n\n\t\t\t\t\tLast Modified: 01/22/2020\n\n\t\t\t\t\tSubmitted by: Sewoong Oh"
 }
}