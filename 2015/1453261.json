{
 "awd_id": "1453261",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Algorithmic Aspects of Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2015-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2015-01-30",
 "awd_max_amd_letter_date": "2019-08-13",
 "awd_abstract_narration": "Algorithms and complexity are the theoretical foundation and backbone of machine learning. Yet over the past few decades an uncomfortable truth has set in that worst-case analysis is not the right framework to study it: every model that is interesting enough to use in practice leads to computationally hard problems. The goal of the PI's research agenda is to move beyond worst-case analysis. This involves formalizing when and why heuristics -- such as alternating minimization and Gibbs sampling -- work as well as designing fundamentally new algorithms for some of the basic tasks in machine learning. This project has already had a number of successes such as provable algorithms for nonnegative matrix factorization, topic modeling and learning mixture models.\r\n\r\nThe PI will investigate several new directions in topics such as sparse coding, inference in graphical models, inverse problems for tensors, and semi-random models. These projects will leverage a wide range of modern tools to give new provable algorithms in each of these settings, and will involve making new connections between alternating minimization and approximate gradient descent, analyzing Gibbs sampling through correlation decay and coupling, connecting tensor completion and quantum complexity and rethinking the standard distributional models used in machine learning. These projects cut across several areas of computer science and applied mathematics and will build new bridges between them, as well as expanding the reach of theory into a number of domains where there is a serious gap in our current understanding. Bridging theory and practice has significant broader impact.  The PI will continue to mentor undergraduate and graduate students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ankur",
   "pi_last_name": "Moitra",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ankur Moitra",
   "pi_email_addr": "moitra@MIT.EDU",
   "nsf_id": "000649382",
   "pi_start_date": "2015-01-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 196141.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 98920.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 101259.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 103680.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">The goal of this project was to design new algorithms for some fundamental problems in machine learning (described below). Additionally it contributed new perspectives in the form of (1) A new framework for analyzing non-convex optimization (2) Using semi-definite programming hierarchies to explore the statistical limits of computationally efficient algorithms for representation learning (3) A toolbox for making algorithms based on the methods of moments robust to adversarial corruptions and (4) new non-markov chain based approaches for approximate counting and inference.<span>&nbsp;</span></p>\n<p class=\"p1\">Dictionary Learning: Many types of data turn out to be sparse in an appropriately chosen bases (e.g. representing signals in a wavelet basis). Modern approaches such as dictionary learning attempt to discover such a basis automatically from data by solving a challenging non-convex optimization problem. Despite their widespread use, there are limited provable guarantees. The PI and collaborators gave simple new algorithms for dictionary learning that work in the practically important overcomplete setting. Their analysis was based on a new general framework for analyzing alternating minimization by viewing it as noisy gradient descent on an unknown convex function instead.<span>&nbsp;</span></p>\n<p class=\"p1\">Tensor Completion: A popular model for building recommendation systems is to view our data as coming from a low-rank matrix or tensor. While the problem of completing an unknown low-rank matrix from few observations has been intensively studied, and tight bounds are known, much less is known about completing low-rank tensors. The PI and collaborators gave the first algorithms that asymptotically improve upon the naive algorithm of completing the low-rank tensor as a collection of unrelated low-rank matrices. Furthermore the PI and collaborators showed computational vs. statistical tradeoffs based on existing conjectures about the difficulty of refuting random constraint satisfaction problems.<span>&nbsp;</span></p>\n<p class=\"p1\">Robustness: In 1960, Tukey asked: Are there e&#8192;cient estimators for learning the parameters of a one-dimensional Gaussian in the presence of noise? His paper launched the field of robust statistics and there are many simple, provably robust estimators in one-dimension. However they all lead to hard optimization problems in high-dimensions. In practice, this means that robust statistical methods are limited to dimension at most six. The PI and collaborators gave the first efficient algorithm for robustly learning a high-dimensional Gaussian. Later, the PI and collaborators extended this to mixtures of Gaussians. The area of algorithmic robust statistics has subsequently seen a flurry of activity, with many exciting results.&nbsp;</p>\n<p class=\"p1\">Sampling: A graphical model is a natural framework for modeling statistical dependencies for high-dimensional distributions. Most existing algorithms for sampling and inference rely on the notion of correlation decay, which stipulates that there are no long-range dependencies. In many cases, correlation decay demarcates where the problem goes from computationally easy to hard. The PI considered the problem of uniformly sampling from the set of satisfying assignments to a bounded degree instance of k-SAT. The PI gave the first algorithms that work when the degree is exponentially large in k. It gives a powerful new methodology for designing algorithms that work even when the solution space is disconnected (the freezing regime) where no local markov chain can work. Furthermore the PI gave applications to inference in a natural class of graphical models.<span>&nbsp;</span></p>\n<p class=\"p1\">The project also supported the training of several graduate students and postdocs. Furthermore the PI supervised over fifty summer research projects in mathematics at the highschool and undergraduate level, including many for underrepresented minorities. The PI also wrote a book titled Algorithmic Aspects of Machine Learning which has been used as a primary text in courses run at other universities, and was recently published by Cambridge University Press.<span>&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/27/2021<br>\n\t\t\t\t\tModified by: Ankur&nbsp;Moitra</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The goal of this project was to design new algorithms for some fundamental problems in machine learning (described below). Additionally it contributed new perspectives in the form of (1) A new framework for analyzing non-convex optimization (2) Using semi-definite programming hierarchies to explore the statistical limits of computationally efficient algorithms for representation learning (3) A toolbox for making algorithms based on the methods of moments robust to adversarial corruptions and (4) new non-markov chain based approaches for approximate counting and inference. \nDictionary Learning: Many types of data turn out to be sparse in an appropriately chosen bases (e.g. representing signals in a wavelet basis). Modern approaches such as dictionary learning attempt to discover such a basis automatically from data by solving a challenging non-convex optimization problem. Despite their widespread use, there are limited provable guarantees. The PI and collaborators gave simple new algorithms for dictionary learning that work in the practically important overcomplete setting. Their analysis was based on a new general framework for analyzing alternating minimization by viewing it as noisy gradient descent on an unknown convex function instead. \nTensor Completion: A popular model for building recommendation systems is to view our data as coming from a low-rank matrix or tensor. While the problem of completing an unknown low-rank matrix from few observations has been intensively studied, and tight bounds are known, much less is known about completing low-rank tensors. The PI and collaborators gave the first algorithms that asymptotically improve upon the naive algorithm of completing the low-rank tensor as a collection of unrelated low-rank matrices. Furthermore the PI and collaborators showed computational vs. statistical tradeoffs based on existing conjectures about the difficulty of refuting random constraint satisfaction problems. \nRobustness: In 1960, Tukey asked: Are there e&#8192;cient estimators for learning the parameters of a one-dimensional Gaussian in the presence of noise? His paper launched the field of robust statistics and there are many simple, provably robust estimators in one-dimension. However they all lead to hard optimization problems in high-dimensions. In practice, this means that robust statistical methods are limited to dimension at most six. The PI and collaborators gave the first efficient algorithm for robustly learning a high-dimensional Gaussian. Later, the PI and collaborators extended this to mixtures of Gaussians. The area of algorithmic robust statistics has subsequently seen a flurry of activity, with many exciting results. \nSampling: A graphical model is a natural framework for modeling statistical dependencies for high-dimensional distributions. Most existing algorithms for sampling and inference rely on the notion of correlation decay, which stipulates that there are no long-range dependencies. In many cases, correlation decay demarcates where the problem goes from computationally easy to hard. The PI considered the problem of uniformly sampling from the set of satisfying assignments to a bounded degree instance of k-SAT. The PI gave the first algorithms that work when the degree is exponentially large in k. It gives a powerful new methodology for designing algorithms that work even when the solution space is disconnected (the freezing regime) where no local markov chain can work. Furthermore the PI gave applications to inference in a natural class of graphical models. \nThe project also supported the training of several graduate students and postdocs. Furthermore the PI supervised over fifty summer research projects in mathematics at the highschool and undergraduate level, including many for underrepresented minorities. The PI also wrote a book titled Algorithmic Aspects of Machine Learning which has been used as a primary text in courses run at other universities, and was recently published by Cambridge University Press. \n\n \n\n\t\t\t\t\tLast Modified: 11/27/2021\n\n\t\t\t\t\tSubmitted by: Ankur Moitra"
 }
}