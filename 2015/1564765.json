{
 "awd_id": "1564765",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2015-09-01",
 "awd_exp_date": "2016-12-31",
 "tot_intn_awd_amt": 114649.0,
 "awd_amount": 114649.0,
 "awd_min_amd_letter_date": "2015-09-24",
 "awd_max_amd_letter_date": "2015-09-24",
 "awd_abstract_narration": "III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families\r\nSwaminathan Vishwanathan, Purdue University; Manfred Warmuth, University of California, Santa Cruz\r\n\r\nMachine learning is currently indispensible for building predictive models from massive data sets. A large majority of widely used machine learning algorithms  are based on minimizing a convex loss function. A fundamental problem with all such models is that they are not robust to outliers. To address this limitation, this project develops probabilistic models based on a parametric family of distributions, namely, the t-exponential family, that lead to quasi-convex loss functions and yield models that  are robust to outliers. \r\n\r\nThe key challenge when working with the t-exponential family of distributions, as in the case of the exponential family, is to compute the log-partition function and perform inference efficiently. The project addresses this challenge in two specific cases.  For problems with small number of classes exact iterative schemes are being developed. For problems where the number of classes is exponentially large, approximate inference techniques are being developed by extending variational methods. \r\n\r\nIn partnership with Google, some of the data mining algorithms resulting from this project are being applied to a challenging real-world problem of recognizing text in photos (the PhotoOCR problem). The project offers opportunities for research-based advanced training of graduate students as well as research opportuinities for undergraduates in machine learning and data mining. Algorithms for constructing predictive models from data that are robust in the presence of outliers are likely to find use in a broad range of applications. Open source implementions of algorithms, publications, and data sets resulting from the project are being made available through the project web page at: http://learning.stat.purdue.edu/wiki/tentropy/start",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vishwanathan",
   "pi_last_name": "Swaminathan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vishwanathan Swaminathan",
   "pi_email_addr": "vishy@ucsc.edu",
   "nsf_id": "000517025",
   "pi_start_date": "2015-09-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Cruz",
  "inst_street_address": "1156 HIGH ST",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA CRUZ",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8314595278",
  "inst_zip_code": "950641077",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "CA19",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA SANTA CRUZ",
  "org_prnt_uei_num": "",
  "org_uei_num": "VXUFPE4MCZH5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Cruz",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "950641077",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "CA19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 114649.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Minimizing convex loss functions is the holy grail of Machine Learning. A plethora of models based on this paradigm have been proposed over the past several decades. A fundamental problem with all such models is that they are not robust to outliers. In contrast, we explored building probabilistic models with a parametric family of distributions that have been proposed in statistical physics. This leads to loss functions that are quasi-convx and flatten out for misclassified points which are far away from the decision boundary. Consequenently, the models are robust to outliers. In our research we found ways to make the models based on these new distribution families practical. We applied them to some challenging real world problems including binary classification in the presence of noisy labels, as well as ranking and recommendation.&nbsp;We showed that&nbsp; our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386,133 &times; 49,824, 519 pairwise interactions between items to be ranked, our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation. We trained graduate students, submitted research articles to conferences, and also released open source code for all our algorithms.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/05/2018<br>\n\t\t\t\t\tModified by: Vishwanathan&nbsp;Swaminathan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMinimizing convex loss functions is the holy grail of Machine Learning. A plethora of models based on this paradigm have been proposed over the past several decades. A fundamental problem with all such models is that they are not robust to outliers. In contrast, we explored building probabilistic models with a parametric family of distributions that have been proposed in statistical physics. This leads to loss functions that are quasi-convx and flatten out for misclassified points which are far away from the decision boundary. Consequenently, the models are robust to outliers. In our research we found ways to make the models based on these new distribution families practical. We applied them to some challenging real world problems including binary classification in the presence of noisy labels, as well as ranking and recommendation. We showed that  our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386,133 &times; 49,824, 519 pairwise interactions between items to be ranked, our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation. We trained graduate students, submitted research articles to conferences, and also released open source code for all our algorithms.\n\n\t\t\t\t\tLast Modified: 11/05/2018\n\n\t\t\t\t\tSubmitted by: Vishwanathan Swaminathan"
 }
}