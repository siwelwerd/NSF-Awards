{
 "awd_id": "1514395",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Novel microLIDAR Design and Sensing Algorithms for Flapping-Wing Micro-Aerial Vehicles",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Miller",
 "awd_eff_date": "2015-08-01",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 326655.0,
 "awd_amount": 372655.0,
 "awd_min_amd_letter_date": "2015-06-08",
 "awd_max_amd_letter_date": "2018-05-01",
 "awd_abstract_narration": "This project makes it possible for a tiny robotic bee to sense its distance to any nearby object. Such depth sensing for a small robot insect pushes the limits of sensor and algorithm design in terms size, weight, computing, and power. The key idea is joint design; every part of the robotic insect is optimized together, from wing design and optics to intelligent algorithms and efficient computation. This is possible by inter-disciplinary work across scientists and engineers from diverse backgrounds. The lessons learned through this project can be applied to transform other applications that involve small devices including medical sensors and endoscope imaging, smart homes and the internet of things, agricultural and industrial monitoring systems, and mobile vision for search and rescue.\r\n\r\nLidar sensing has enabled large robotic cars to navigate complex environments. This proposal introduces designs for \"micro-lidar\" that can be used on insect-scale aerial robots. Making micro-lidar work on small platforms involves four intertwined research thrusts. The first thrust uses MEMS mirrors and wide-angle optics to sense and modulate the laser pulses. The second thrust is adapting signal processing algorithms to estimate range data at this scale. The third thrust is developing novel perception and navigation algorithms to map the indoor environments using a micro-aerial vehicle. The fourth thrust is to improve robotic insect flight to allow novel manipulations that require knowledge of the surrounding range map. The utility of these sensors will be demonstrated on the robobee for novel maneuvers and building topo-feature maps of indoor environments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Karthik",
   "pi_last_name": "Dantu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Karthik Dantu",
   "pi_email_addr": "kdantu@buffalo.edu",
   "nsf_id": "000663253",
   "pi_start_date": "2015-06-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Buffalo",
  "inst_street_address": "520 LEE ENTRANCE STE 211",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "7166452634",
  "inst_zip_code": "142282577",
  "inst_country_name": "United States",
  "cong_dist_code": "26",
  "st_cong_dist_code": "NY26",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "GMZUKXFDJMA9",
  "org_uei_num": "LMCJKRFW5R81"
 },
 "perf_inst": {
  "perf_inst_name": "University at Buffalo",
  "perf_str_addr": "402 Crofts Hall",
  "perf_city_name": "Buffalo",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "142602500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "26",
  "perf_st_cong_dist": "NY26",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9232",
   "pgm_ref_txt": "RES OPPOR AWARDS(ROA) (SUPPLEM"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 116976.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 111219.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 136460.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project advanced the state-of-the-art in micro-air flapping vehicles in several ways. We developed a novel depth sensor that is capable of changing the region of interest at run-time.&nbsp; We also developed signal processing algorithms to use such an adaptive depth sensor to reason about dynamic scenes.&nbsp;This sensor would be of great interest to applications such as driverless cars that employ fixed-view depth sensors today, and could grearly benefit from the adaptivity provided by a sensor that is capable of changing its region of interest programmatically.</p>\n<p>We have also advanced the state-of-the-art in visual mapping in several ways. This project addressed perceptual aliasing in indoor mapping by aiding visual sensing with Wi-Fi sensing. We also improved visual mapping algorithms when robots repeatedly visit the same scene (such as service robots like robot vacuum cleaners) by incorporating permanence reasoning and using such reasoning to improve mapping. We also developed ways to build integrate small visual maps by using Wi-Fi sensing when mapping large urban areas such as airports and malls. Lastly, we demonstrated a way to run computationally heavy mapping algorithms on resource-constrained devices such as smartphones by offloading some of the computation to edge devices.&nbsp;</p>\n<p>Sensing of the robotic bee was advanced in this project by incorporating a combination of an optic flow sensor with an IMU to provide accurate attitude estimation with 5mW of power. The flapping wing vehicle design has been redesigned to incorporate the payload required by changing the electrical and mechanical design of the robot. Another advancement was an untethered flight demonstration using solar cells. Finally, in order to make such flapping-wing micro-air vehicles more accessible, we developed a methodology to simulate the dynamics of such an aerial vehicle using quadrotors which are much more easily available for researchers.&nbsp;</p>\n<p>The proposal helped research studies for four Ph.D. students including two women. It also supported several undergraduate students to conduct research including three minority students. Through supplementary support, this project supported research training for a minority faculty member from a primarily-undergraduate institution. The faculty conducted several local workshops to middle school students using robot building and programming to teach students STEM topics.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/07/2020<br>\n\t\t\t\t\tModified by: Karthik&nbsp;Dantu</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1514395/1514395_10367999_1586291830673_ScreenShot2020-04-07at4.33.57PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1514395/1514395_10367999_1586291830673_ScreenShot2020-04-07at4.33.57PM--rgov-800width.jpg\" title=\"Untethered Robotic Bee Flight\"><img src=\"/por/images/Reports/POR/2020/1514395/1514395_10367999_1586291830673_ScreenShot2020-04-07at4.33.57PM--rgov-66x44.jpg\" alt=\"Untethered Robotic Bee Flight\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure: (a) \ufffdX-Wing\ufffd version of the RoboBee, with four wings. (b) Detail of piezoelectric actuators with upgraded reinforcements. (c) Complete vehicle, with power and drive electronics and solar cells. (d) Frames from a flight sequence demonstrating untethered flight.</div>\n<div class=\"imageCredit\">Prof. Rob Wood, Harvard</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Karthik&nbsp;Dantu</div>\n<div class=\"imageTitle\">Untethered Robotic Bee Flight</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1514395/1514395_10367999_1586291928053_ScreenShot2020-04-07at4.03.29PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1514395/1514395_10367999_1586291928053_ScreenShot2020-04-07at4.03.29PM--rgov-800width.jpg\" title=\"Distributed Visual Mapping\"><img src=\"/por/images/Reports/POR/2020/1514395/1514395_10367999_1586291928053_ScreenShot2020-04-07at4.03.29PM--rgov-66x44.jpg\" alt=\"Distributed Visual Mapping\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure: Our map merging methodology (left), and an example of merged maps from four bots (right)</div>\n<div class=\"imageCredit\">Prof. Karthik Dantu, University at BuffaloXZ</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Karthik&nbsp;Dantu</div>\n<div class=\"imageTitle\">Distributed Visual Mapping</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1514395/1514395_10367999_1586292015424_ScreenShot2020-04-07at4.35.00PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1514395/1514395_10367999_1586292015424_ScreenShot2020-04-07at4.35.00PM--rgov-800width.jpg\" title=\"Adaptive LiDAR Design\"><img src=\"/por/images/Reports/POR/2020/1514395/1514395_10367999_1586292015424_ScreenShot2020-04-07at4.35.00PM--rgov-66x44.jpg\" alt=\"Adaptive LiDAR Design\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure: Ray diagram: In (I) we show the co-located ray-diagram of the pulsed LIDAR that is modulated by a MEMS device. In (II) we show our actual setup, with a Lightware LIDAR and a Mirrorcle MEMS mirror.</div>\n<div class=\"imageCredit\">Prof Sanjeev Koppal, University of Florida</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Karthik&nbsp;Dantu</div>\n<div class=\"imageTitle\">Adaptive LiDAR Design</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project advanced the state-of-the-art in micro-air flapping vehicles in several ways. We developed a novel depth sensor that is capable of changing the region of interest at run-time.  We also developed signal processing algorithms to use such an adaptive depth sensor to reason about dynamic scenes. This sensor would be of great interest to applications such as driverless cars that employ fixed-view depth sensors today, and could grearly benefit from the adaptivity provided by a sensor that is capable of changing its region of interest programmatically.\n\nWe have also advanced the state-of-the-art in visual mapping in several ways. This project addressed perceptual aliasing in indoor mapping by aiding visual sensing with Wi-Fi sensing. We also improved visual mapping algorithms when robots repeatedly visit the same scene (such as service robots like robot vacuum cleaners) by incorporating permanence reasoning and using such reasoning to improve mapping. We also developed ways to build integrate small visual maps by using Wi-Fi sensing when mapping large urban areas such as airports and malls. Lastly, we demonstrated a way to run computationally heavy mapping algorithms on resource-constrained devices such as smartphones by offloading some of the computation to edge devices. \n\nSensing of the robotic bee was advanced in this project by incorporating a combination of an optic flow sensor with an IMU to provide accurate attitude estimation with 5mW of power. The flapping wing vehicle design has been redesigned to incorporate the payload required by changing the electrical and mechanical design of the robot. Another advancement was an untethered flight demonstration using solar cells. Finally, in order to make such flapping-wing micro-air vehicles more accessible, we developed a methodology to simulate the dynamics of such an aerial vehicle using quadrotors which are much more easily available for researchers. \n\nThe proposal helped research studies for four Ph.D. students including two women. It also supported several undergraduate students to conduct research including three minority students. Through supplementary support, this project supported research training for a minority faculty member from a primarily-undergraduate institution. The faculty conducted several local workshops to middle school students using robot building and programming to teach students STEM topics. \n\n\t\t\t\t\tLast Modified: 04/07/2020\n\n\t\t\t\t\tSubmitted by: Karthik Dantu"
 }
}