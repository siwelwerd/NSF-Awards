{
 "awd_id": "1546452",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Collaborative Research: F: Nomadic Algorithms for Machine Learning in the Cloud",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-01-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 610432.0,
 "awd_amount": 610432.0,
 "awd_min_amd_letter_date": "2015-09-14",
 "awd_max_amd_letter_date": "2015-09-14",
 "awd_abstract_narration": "With an ever increasing ability to collect and archive data, massive data sets are becoming increasingly common. These data sets are often too big to fit into the main memory of a single computer, and so there is a great need for developing scalable and sophisticated machine learning methods for their analysis. In particular, one has to devise strategies to distribute the computation across multiple machines. However, stochastic optimization  and inference algorithms that are so effective for large-scale machine learning appear to be inherently sequential.\r\n\r\nThe main research goal of this project is to develop a novel \"nomadic\" framework that overcomes this barrier.  This will be done by showing that many modern machine learning problems have a certain \"double separability\" property. The aim is to exploit this property to develop convergent,  asynchronous, distributed, and fault tolerant algorithms that are well-suited for achieving high performance on commodity hardware that is prevalent on today's cloud computing platforms. In particular, over a four year period, the following will be developed: (i) parallel stochastic optimization algorithms for the multi-machine cloud computing setting, (ii) theoretical guarantees of convergence, (iii) open source code under a permissive license, (iv) application of these techniques to a variety of problem domains such as topic models and mixture models. In addition, a cohort of students who can transfer their skills to both industry and academia will be trained, and a graduate level course on scalable machine learning will be developed.\r\n \r\nThe proposed research will enable practitioners in different application areas to quickly solve their big data problems.  The results of the project will be disseminated widely through papers and open source software. Course material will be developed for the education of students in the area of Scalable Machine Learning, and the course will be co-taught at UCSC and UT Austin. The project will recruit women and minority students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Inderjit",
   "pi_last_name": "Dhillon",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Inderjit S Dhillon",
   "pi_email_addr": "inderjit@cs.utexas.edu",
   "nsf_id": "000200521",
   "pi_start_date": "2015-09-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "201 E. 24th Street, C0200",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121229",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 610432.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>As data grows in size and complexity, it is a contemporary challenge to develop scalable, robust and distributed algorithms for big data analytics. In paticular,<span>&nbsp;data sets are often too big to fit into the main memory of a single computer, and so there is a great need for developing scalable and sophisticated machine learning methods for their analysis.</span></span></p>\n<p>For this project, we have explored novel scalable algorithms and framework for large-scale machine learning problems, such as matrix completion, topic modeling, kernel machines, extreme classification, federated learning,&nbsp; and sequence-to-sequence prediction. We have published papers and released software for these problems in addition to training students with expertise in these areas.</p>\n<p>In particular, we have developed (i)&nbsp; nomadic distributed, decentralized algorithms for&nbsp;very large-scale matrix completion, topic modeling and mixture modeling, (ii) communication&shy; efficient distributed block minimization algorithms for large-scale nonlinear kernel machines, (iii) parallel primal-dual sparse methods for large-scale extreme classification poblems, (iv) robust and efficient federated learning methods, and (v) stabilized, scalable methods for training sequence-to-sequence deep learning models. These algorithms allow us to train larger machine learning models for practical problems in natural language processing, recommender systems, object recognition, and information retrieval.</p>\n<p><span>The results from this funded project have been disseminated through publications in various venues, such as KDD, NeurIPS, ICML, AISTATS, and IEEE Transactions, which are leading conferences and journals in machine learning and data mining. In terms of education and training, multiple Ph.D. students, including one female student, obtained their degree supported by funding from this project.</span></p>\n<p>&nbsp;</p>\n<div class=\"page\" title=\"Page 2\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p>&nbsp;</p>\n</div>\n</div>\n</div>\n</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n</div>\n</div>\n</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/13/2022<br>\n\t\t\t\t\tModified by: Inderjit&nbsp;S&nbsp;Dhillon</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\n\nAs data grows in size and complexity, it is a contemporary challenge to develop scalable, robust and distributed algorithms for big data analytics. In paticular, data sets are often too big to fit into the main memory of a single computer, and so there is a great need for developing scalable and sophisticated machine learning methods for their analysis.\n\nFor this project, we have explored novel scalable algorithms and framework for large-scale machine learning problems, such as matrix completion, topic modeling, kernel machines, extreme classification, federated learning,  and sequence-to-sequence prediction. We have published papers and released software for these problems in addition to training students with expertise in these areas.\n\nIn particular, we have developed (i)  nomadic distributed, decentralized algorithms for very large-scale matrix completion, topic modeling and mixture modeling, (ii) communication&shy; efficient distributed block minimization algorithms for large-scale nonlinear kernel machines, (iii) parallel primal-dual sparse methods for large-scale extreme classification poblems, (iv) robust and efficient federated learning methods, and (v) stabilized, scalable methods for training sequence-to-sequence deep learning models. These algorithms allow us to train larger machine learning models for practical problems in natural language processing, recommender systems, object recognition, and information retrieval.\n\nThe results from this funded project have been disseminated through publications in various venues, such as KDD, NeurIPS, ICML, AISTATS, and IEEE Transactions, which are leading conferences and journals in machine learning and data mining. In terms of education and training, multiple Ph.D. students, including one female student, obtained their degree supported by funding from this project.\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n \n\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 02/13/2022\n\n\t\t\t\t\tSubmitted by: Inderjit S Dhillon"
 }
}