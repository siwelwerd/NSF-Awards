{
 "awd_id": "1741191",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Statistical Foundation of Predictivity: A Novel Architecture for Big Data Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2018-01-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 900000.0,
 "awd_amount": 900000.0,
 "awd_min_amd_letter_date": "2017-08-29",
 "awd_max_amd_letter_date": "2023-02-13",
 "awd_abstract_narration": "Identifying variables that are good for prediction, especially in the context of BIG DATA, is an important challenge. The scientific literature currently lacks research that directly considers a variable set's potential ability to predict, referred to as \"predictivity\", as a parameter to be estimated. This project sets out to lay down statistical foundations for measures of predictivity, and proposes a novel framework for maximizing predictivity in big data learning. The research includes an application to big data in urban planning, addressing prediction problems in New York City's Vision Zero project. In collaboration with the NYC Department of Transportation, the PI and his team will identify risk factors and their combinations that are associated with traffic accidents and their outcomes, and improve accident prevention and victim outcome prediction. \r\n\r\nA novel sample-based measure of predictivity, the I-score, that is effective in differentiating between noisy and predictive variables in big data is proposed.   This measure can be related to a lower bound for the correct prediction rate. Guided by this I-score, variable sets of high potential predictivity can be identified. This high predictivity often resides within complex interactions among the variables. To fully leverage the predictivity in an identified variable set, powerful classifiers based on deep architectures will be constructed. Novel strategies are proposed for scalable computational implementation of the proposed framework. Systematic evaluation of the proposed methods, comparing with current strategies, will be carried out using simulations and benchmark real data sets.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shaw-Hwa",
   "pi_last_name": "Lo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shaw-Hwa Lo",
   "pi_email_addr": "slo@stat.columbia.edu",
   "nsf_id": "000471911",
   "pi_start_date": "2017-08-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tian",
   "pi_last_name": "Zheng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tian Zheng",
   "pi_email_addr": "tzheng@stat.columbia.edu",
   "nsf_id": "000149965",
   "pi_start_date": "2017-08-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Xuan",
   "pi_last_name": "Di",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xuan Di",
   "pi_email_addr": "sharon.di@columbia.edu",
   "nsf_id": "000730829",
   "pi_start_date": "2021-12-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277922",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 900000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NSF Outcome Report</p>\n<p>The success of artificial intelligence (AI) in the past decade has produced a deluge of AI applications&nbsp; that affects nearly every part of our daily lives. More complicated AI-systems often are based on deep learning algorithms, which use multiple layers to learn hidden structure of large-scale data sets. Deep learning has three common families of computational models: Artificial Neural Network (ANN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN). As computers become faster and more powerful, there is an exploding amount of literature dedicated to developing deeper and more complicated learning algorithms with no intention to explain what makes variables or features good for prediction. Common approaches to identify variables good for prediction include adopting significant-based criteria, or employing cross validation and/or an independent test set to assess prediction performance of variables. The former approach suffers from the fact that significant variables are NOT automatically good predictors, while the latter may fail when the model used to obtain prediction is incorrect. Even though various explainable machine learning approaches have been suggested to create a second (simpler) model to explain the first (complex) model, these approaches inevitably invite the hard-to-answer question - how large is the discrepancy between the simpler model and the complex model for a particular instance at hand.</p>\n<p>The proposed research on explainable artificial intelligence (XAI) is motivated by direct measurement of variables' predictivity. Not only can it explain why significant variables are not necessarily good for prediction but also why certain variables should be included in a predictive model while others should not.</p>\n<p>In summary, we provided a flexible approach to contribute to the major issues regarding explainability, interpretability, transparency, and trust worthiness in black-box algorithms. We introduced and implemented a nonparametric and interaction-based feature selection methodology and used this as a replacement for predefined filters that are widely used in ultra-deep CNNs. Under this paradigm, we presented an ICNN that extracts important features. The proposed architecture uses these extracted features to construct influential and predictive variable modules that are directly associated with the predictivity of the response variable. The proposed design and its many characteristics provide an extremely flexible pipeline that can learn, extract useful information, and identify the hidden potential from any large-scale or high-dimensional data set. Because of these characteristics, we can presume the outcome of this work will be another stride in the direction of the ultimate goal - full transparency and optimized predictivity. With these goals, confidence and trust in deep learning algorithms will inevitably follow. On a broader scale, these results will be able to make large impacts in vastly different applications, from self-driving technology to healthcare.</p>\n<p><strong>Contributed References&nbsp;</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p>[1] S.-H. Lo and T. Zheng, Backward haplotype transmission association (bhta) algorithm a fast multiple-marker screening method, Human heredity, vol. 53, no. 4, pp. 197?215, 2002.</p>\n<p>[2] H. Chernoff, S.-H. Lo, and T. Zheng, Discovering influential variables: a method of partitions,? The Annals of applied statistics, vol. 3, no. 4, pp. 1335-1369, 2009.</p>\n<p>[3] Wang, H., Lo, S., Zheng, T., and Hu, I. (2012) &ldquo;Interaction-based feature selection and classification</p>\n<p>for high-dimensional biological data&rdquo; <em>Bioinformatics</em>, <strong>28</strong>(21), 2834-2842</p>\n<p>doi: 10.1093/bioinformatics/bts531 ,</p>\n<p>[4] A. Lo, H. Chernoff, T. Zheng, and S.-H. Lo, Why significant variables aren't automatically good predictors,? Proceedings of the National Academy of Sciences, vol. 112, no. 45, pp. 13892-13897, 2015.</p>\n<p>[5] A. Lo, H. Chernoff, T. Zheng, and S.-H. Lo. Framework for making better predictions by directly estimating variables predictivity, Proceedings of the National Academy of Sciences, vol. 113, no. 50, pp. 14 277?14 282, 2016.</p>\n<p>[6] S.-H. Lo and Y. Yin, A novel interaction-based methodology towards explainable ai with better understanding of pneumonia chest x-ray images, arXiv preprint arXiv:2104.12672, 2021.</p>\n<p>[7]&nbsp;Shaw-Hwa Lo and Yiqiao Yin, An interaction-based convolutional neural network (icnn) towards better understanding of covid-19 x-ray images. <a href=\"https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.mdpi.com%2Fjournal%2Falgorithms&amp;data=05%7C01%7Cihu%40gmu.edu%7C3a33ba544dc5405362fc08dbd8ad2bf6%7C9e857255df574c47a0c00546460380cb%7C0%7C0%7C638342010533520804%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=K3euMKgJ2CmBmBNg%2FfgbI79Z2EVep5YVUw6cvs476gI%3D&amp;reserved=0\">https://www.mdpi.com/journal/algorithms</a>. <em>Algorithms</em>&nbsp;<strong>2021</strong>,&nbsp;<em>14</em>(11), 337;&nbsp;<a href=\"https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdoi.org%2F10.3390%2Fa14110337&amp;data=05%7C01%7Cihu%40gmu.edu%7C3a33ba544dc5405362fc08dbd8ad2bf6%7C9e857255df574c47a0c00546460380cb%7C0%7C0%7C638342010533520804%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=7cFfMrLyt87NAQ5wB6FsguUNkh77WlIqZrQ1BJpH7IQ%3D&amp;reserved=0\">https://doi.org/10.3390/a14110337</a>&nbsp;19 November 2021</p>\n<p>[8]&nbsp;Shaw-Hwa Lo and Yiqiao Yin. A novel interaction-based methodology towards explainable AI with better understanding of Pneumonia Chest X-ray Images. Discover Artificial Intell&nbsp;116(2021)</p>\n<p>[9]&nbsp;S. H. Lo and Yiqiao Yin. Language Semantics Interpretation with an Interaction-Based Recurrent Neural Network. Machine Learning and&nbsp;Knowledge Extraction. 2021; 3(4):922-945</p>\n<p><a href=\"https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdoi.org%2F10.3390%2Fmake3040046&amp;data=05%7C01%7Cihu%40gmu.edu%7C3a33ba544dc5405362fc08dbd8ad2bf6%7C9e857255df574c47a0c00546460380cb%7C0%7C0%7C638342010533520804%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=nmJ%2ByLY2uN7loQWtacXVRNdlyuD2xfzr8Fmq83RWl9c%3D&amp;reserved=0\">https://doi.org/10.3390/make3040046</a></p>\n<p>[10]&nbsp;Shaw-Hwa, Lo and Yiqiao Yin. Explainable Methods in Chest X-ray Classification. To appear in Advances in Machine Learning and Artificial Intelligence. 2022.</p>\n<p>[11]&nbsp;Shaw-Hwa Lo and Yiqiao Yin. XAI Towards Healthcare, accepted, AAAI 22' Workshop, (2022).</p>\n<p>[12]&nbsp;Shaw-Hwa Lo and Yiqiao Yin. A Novel Approach to Adopt Explainable Artificial Intelligence in X-ray Image Classification. Citation: Shaw-Hwa Lo and Yiqiao Yin (2022) A Novel Approach to Adopt Explainable Artificial Intelligence in X-ray Image Classification. Adv. Mach. Lear. Art. Inte. 3(1).</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/05/2023<br>\nModified by: Shaw-Hwa&nbsp;Lo</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n NSF Outcome Report\n\n\nThe success of artificial intelligence (AI) in the past decade has produced a deluge of AI applications that affects nearly every part of our daily lives. More complicated AI-systems often are based on deep learning algorithms, which use multiple layers to learn hidden structure of large-scale data sets. Deep learning has three common families of computational models: Artificial Neural Network (ANN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN). As computers become faster and more powerful, there is an exploding amount of literature dedicated to developing deeper and more complicated learning algorithms with no intention to explain what makes variables or features good for prediction. Common approaches to identify variables good for prediction include adopting significant-based criteria, or employing cross validation and/or an independent test set to assess prediction performance of variables. The former approach suffers from the fact that significant variables are NOT automatically good predictors, while the latter may fail when the model used to obtain prediction is incorrect. Even though various explainable machine learning approaches have been suggested to create a second (simpler) model to explain the first (complex) model, these approaches inevitably invite the hard-to-answer question - how large is the discrepancy between the simpler model and the complex model for a particular instance at hand.\n\n\nThe proposed research on explainable artificial intelligence (XAI) is motivated by direct measurement of variables' predictivity. Not only can it explain why significant variables are not necessarily good for prediction but also why certain variables should be included in a predictive model while others should not.\n\n\nIn summary, we provided a flexible approach to contribute to the major issues regarding explainability, interpretability, transparency, and trust worthiness in black-box algorithms. We introduced and implemented a nonparametric and interaction-based feature selection methodology and used this as a replacement for predefined filters that are widely used in ultra-deep CNNs. Under this paradigm, we presented an ICNN that extracts important features. The proposed architecture uses these extracted features to construct influential and predictive variable modules that are directly associated with the predictivity of the response variable. The proposed design and its many characteristics provide an extremely flexible pipeline that can learn, extract useful information, and identify the hidden potential from any large-scale or high-dimensional data set. Because of these characteristics, we can presume the outcome of this work will be another stride in the direction of the ultimate goal - full transparency and optimized predictivity. With these goals, confidence and trust in deep learning algorithms will inevitably follow. On a broader scale, these results will be able to make large impacts in vastly different applications, from self-driving technology to healthcare.\n\n\nContributed References\n\n\n[1] S.-H. Lo and T. Zheng, Backward haplotype transmission association (bhta) algorithm a fast multiple-marker screening method, Human heredity, vol. 53, no. 4, pp. 197?215, 2002.\n\n\n[2] H. Chernoff, S.-H. Lo, and T. Zheng, Discovering influential variables: a method of partitions,? The Annals of applied statistics, vol. 3, no. 4, pp. 1335-1369, 2009.\n\n\n[3] Wang, H., Lo, S., Zheng, T., and Hu, I. (2012) Interaction-based feature selection and classification\n\n\nfor high-dimensional biological data Bioinformatics, 28(21), 2834-2842\n\n\ndoi: 10.1093/bioinformatics/bts531 ,\n\n\n[4] A. Lo, H. Chernoff, T. Zheng, and S.-H. Lo, Why significant variables aren't automatically good predictors,? Proceedings of the National Academy of Sciences, vol. 112, no. 45, pp. 13892-13897, 2015.\n\n\n[5] A. Lo, H. Chernoff, T. Zheng, and S.-H. Lo. Framework for making better predictions by directly estimating variables predictivity, Proceedings of the National Academy of Sciences, vol. 113, no. 50, pp. 14 277?14 282, 2016.\n\n\n[6] S.-H. Lo and Y. Yin, A novel interaction-based methodology towards explainable ai with better understanding of pneumonia chest x-ray images, arXiv preprint arXiv:2104.12672, 2021.\n\n\n[7]Shaw-Hwa Lo and Yiqiao Yin, An interaction-based convolutional neural network (icnn) towards better understanding of covid-19 x-ray images. https://www.mdpi.com/journal/algorithms. Algorithms2021,14(11), 337;https://doi.org/10.3390/a1411033719 November 2021\n\n\n[8]Shaw-Hwa Lo and Yiqiao Yin. A novel interaction-based methodology towards explainable AI with better understanding of Pneumonia Chest X-ray Images. Discover Artificial Intell116(2021)\n\n\n[9]S. H. Lo and Yiqiao Yin. Language Semantics Interpretation with an Interaction-Based Recurrent Neural Network. Machine Learning andKnowledge Extraction. 2021; 3(4):922-945\n\n\nhttps://doi.org/10.3390/make3040046\n\n\n[10]Shaw-Hwa, Lo and Yiqiao Yin. Explainable Methods in Chest X-ray Classification. To appear in Advances in Machine Learning and Artificial Intelligence. 2022.\n\n\n[11]Shaw-Hwa Lo and Yiqiao Yin. XAI Towards Healthcare, accepted, AAAI 22' Workshop, (2022).\n\n\n[12]Shaw-Hwa Lo and Yiqiao Yin. A Novel Approach to Adopt Explainable Artificial Intelligence in X-ray Image Classification. Citation: Shaw-Hwa Lo and Yiqiao Yin (2022) A Novel Approach to Adopt Explainable Artificial Intelligence in X-ray Image Classification. Adv. Mach. Lear. Art. Inte. 3(1).\n\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 11/05/2023\n\n\t\t\t\t\tSubmitted by: Shaw-HwaLo\n"
 }
}