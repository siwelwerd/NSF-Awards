{
 "awd_id": "1733961",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "COLLABORATIVE RESEARCH:  Jurors' Use of Scientific Information",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "reginald sheehan",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 131178.0,
 "awd_amount": 145950.0,
 "awd_min_amd_letter_date": "2017-07-31",
 "awd_max_amd_letter_date": "2018-07-13",
 "awd_abstract_narration": "Juries often rely on and interpret evidence based on scientific research when making decisions in civil and criminal cases. The validity and reliability of scientific information varies, yet jurors generally are non-experts who do not possess the necessary tools to differentiate between weak and strong scientific information when making decisions. To assess the issue of interpretation of scientific evidence in court, this project uses an experimental approach to examine jurors' ability to interpret and act on scientific evidence. To do so, this project consists of jury simulations that will examine jurors' and juries' sensitivity to strong versus weak scientific information presented in court. \r\n\r\nThe project includes two jury simulation experiments designed to test whether fuzzy trace theory, a well-developed theory in cognitive science, applies in the context of jury research, particularly when compared to other safeguards for jury decision making with respect to scientific evidence. This multidisciplinary, multi-method research will examine when and how jurors' inferences are appropriately calibrated to the strength of scientific information, whether a safeguard derived from decision-making theory can improve that calibration, and how various measures relevant to the processing of scientific information are related to one another. The project addresses fundamental questions about how humans reason with and make inferences and decisions based on the quality of relevant scientific data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tess",
   "pi_last_name": "Neal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tess Neal",
   "pi_email_addr": "tessneal@iastate.edu",
   "nsf_id": "000650414",
   "pi_start_date": "2017-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Nicholas",
   "pi_last_name": "Schweitzer",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Nicholas J Schweitzer",
   "pi_email_addr": "njs@asu.edu",
   "nsf_id": "000338384",
   "pi_start_date": "2017-09-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University, West Campus",
  "perf_str_addr": "4701 W. Thunderbird Rd.",
  "perf_city_name": "Glendale",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "853064908",
  "perf_ctry_code": "US",
  "perf_cong_dist": "08",
  "perf_st_cong_dist": "AZ08",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "137200",
   "pgm_ele_name": "LSS-Law And Social Sciences"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 131178.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 14772.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-fb639738-7fff-e8a1-3ded-ce05d2242518\"> </span></p>\n<p dir=\"ltr\"><span>The project involved two jury simulation experiments using diverse participants to answer three questions. First, we wanted to know whether jurors could tell the difference between high-quality evidence and low-quality evidence when the evidence is scientifically complicated - and how differences in the quality of the evidence would weigh into their decision processes. Second, we wanted to know whether differences between people, such as how good they are with numbers and how much they like to think, would predict their ability to understand and make informed decisions about complicated scientific evidence when serving on a jury. Finally, we wanted to discover whether an innovative visual decision aid used to break down complicated scientific evidence into more understandable information could help jurors make the best use of that information in their decision processes (as compared to other often-used safeguards like cross-examination and jury instructions from the judge).&nbsp;</span></p>\n<p>To discover the answers to these questions, we engaged in a three-year collaborative effort by scholars at two institutions (Arizona State University and the University of Nebraska-Lincoln). The project included two separate but complementary experiments. Five features were common to both. First, in the interest of replicable and robust science, each experiment was preregistered on the Open Science Framework. Second, each manipulated the quality of the scientific evidence in the case. Third, each manipulated the presence of a Fuzzy Trace Theory-inspired decision aid. Fourth, individual differences related to processing of and attitudes toward scientific information were measured to examine their relationship to participants? calibration. Fifth, each experiment employed community participants and presented the mock trial on videotape. Toward conceptual replication, the experiments included both civil (Experiment 1) and criminal trials (Experiment 2) and employed different kinds of complex scientific evidence (fMRI signal-to-noise ratios; mtDNA match statistics).</p>\n<p>Overall, results revealed juror-participants were somewhat sensitive to the varying quality of evidence, but differences between people were important: jurors with stronger scientific reasoning and numeracy skills were more aware of the differences in evidence quality, understood the evidence better, and discussed the scientific evidence more during deliberation. Unfortunately, the decision aid did not appear to help calibrate people to the strength of the evidence. Together, these results provide evidence that people who serve as jurors may be generally able to differentiate between higher- and lower-quality scientific evidence, but that understanding may not translate to down-stream decisions like we might hope (e.g., the perceived credibility of the expert witnesses who testify about such evidence, verdict decisions, damage decisions).&nbsp;</p>\n<p>This project contributed to the education, training, and professional development of a diverse group of students and scholars. It supported 3 formal Research Experiences for Undergraduate (REU) students, as well as training and professional development opportunities for the PIs and Co-PIs, two PhD students in psychology-law (including one recipient of an NSF GRFP fellowship [Emily Denne]), 4 master?s students in psychology (two of whom were also involved in the project as undergraduates before moving into graduate school), and 10 undergraduate students. We traveled together to the American Psychology-Law Society conference in New Orleans, LA in Spring 2020 and delivered the results at a symposium (along with the UNL team). We are collaboratively writing up at least two papers for publication now. (And note that we are still finishing coding for the deliberation data - with which we will be able to discern whether more-calibrated jurors are able to help less-calibrated jurors become more calibrated through discussions during deliberation).</p>\n<p>The OSF Preregistration for experiment 1: <a href=\"https://osf.io/2q3vj\">https://osf.io/2q3vj</a></p>\n<p>The OSF Preregistration for experiment 2: <a href=\"https://osf.io/ec7ts\">https://osf.io/ec7ts</a></p>\n<p>The OSF Project page (where the datafiles and materials will be uploaded as soon as we finish the analysis of the deliberation data): <a href=\"https://osf.io/jvbfx/\">https://osf.io/jvbfx/</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/28/2020<br>\n\t\t\t\t\tModified by: Tess&nbsp;Neal</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe project involved two jury simulation experiments using diverse participants to answer three questions. First, we wanted to know whether jurors could tell the difference between high-quality evidence and low-quality evidence when the evidence is scientifically complicated - and how differences in the quality of the evidence would weigh into their decision processes. Second, we wanted to know whether differences between people, such as how good they are with numbers and how much they like to think, would predict their ability to understand and make informed decisions about complicated scientific evidence when serving on a jury. Finally, we wanted to discover whether an innovative visual decision aid used to break down complicated scientific evidence into more understandable information could help jurors make the best use of that information in their decision processes (as compared to other often-used safeguards like cross-examination and jury instructions from the judge). \n\nTo discover the answers to these questions, we engaged in a three-year collaborative effort by scholars at two institutions (Arizona State University and the University of Nebraska-Lincoln). The project included two separate but complementary experiments. Five features were common to both. First, in the interest of replicable and robust science, each experiment was preregistered on the Open Science Framework. Second, each manipulated the quality of the scientific evidence in the case. Third, each manipulated the presence of a Fuzzy Trace Theory-inspired decision aid. Fourth, individual differences related to processing of and attitudes toward scientific information were measured to examine their relationship to participants? calibration. Fifth, each experiment employed community participants and presented the mock trial on videotape. Toward conceptual replication, the experiments included both civil (Experiment 1) and criminal trials (Experiment 2) and employed different kinds of complex scientific evidence (fMRI signal-to-noise ratios; mtDNA match statistics).\n\nOverall, results revealed juror-participants were somewhat sensitive to the varying quality of evidence, but differences between people were important: jurors with stronger scientific reasoning and numeracy skills were more aware of the differences in evidence quality, understood the evidence better, and discussed the scientific evidence more during deliberation. Unfortunately, the decision aid did not appear to help calibrate people to the strength of the evidence. Together, these results provide evidence that people who serve as jurors may be generally able to differentiate between higher- and lower-quality scientific evidence, but that understanding may not translate to down-stream decisions like we might hope (e.g., the perceived credibility of the expert witnesses who testify about such evidence, verdict decisions, damage decisions). \n\nThis project contributed to the education, training, and professional development of a diverse group of students and scholars. It supported 3 formal Research Experiences for Undergraduate (REU) students, as well as training and professional development opportunities for the PIs and Co-PIs, two PhD students in psychology-law (including one recipient of an NSF GRFP fellowship [Emily Denne]), 4 master?s students in psychology (two of whom were also involved in the project as undergraduates before moving into graduate school), and 10 undergraduate students. We traveled together to the American Psychology-Law Society conference in New Orleans, LA in Spring 2020 and delivered the results at a symposium (along with the UNL team). We are collaboratively writing up at least two papers for publication now. (And note that we are still finishing coding for the deliberation data - with which we will be able to discern whether more-calibrated jurors are able to help less-calibrated jurors become more calibrated through discussions during deliberation).\n\nThe OSF Preregistration for experiment 1: https://osf.io/2q3vj\n\nThe OSF Preregistration for experiment 2: https://osf.io/ec7ts\n\nThe OSF Project page (where the datafiles and materials will be uploaded as soon as we finish the analysis of the deliberation data): https://osf.io/jvbfx/\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/28/2020\n\n\t\t\t\t\tSubmitted by: Tess Neal"
 }
}