{
 "awd_id": "1740102",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: SI2:SSE: Extending the Physics Reach of LHCb in Run 3 Using Machine Learning in the Real-Time Data Ingestion and Reduction System",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Seung-Jong Park",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 224621.0,
 "awd_amount": 224621.0,
 "awd_min_amd_letter_date": "2017-08-24",
 "awd_max_amd_letter_date": "2017-08-24",
 "awd_abstract_narration": "In the past 200 years, physicists have discovered the basic constituents of ordinary matter and the developed a very successful theory to describe the interactions (forces) between them.  All atoms, and the molecules from which they are built, can be described in terms of these constituents.  The nuclei of atoms are bound together by strong nuclear interactions.  Their decays result from strong and weak nuclear interactions. Electromagnetic forces bind atoms together, and bind atoms into molecules. The electromagnetic, weak nuclear, and strong nuclear forces are described in terms of quantum field theories.  The predictions of these theories can be very, very precise, and they have been validated with equally precise experimental measurements.  Most recently, a new fundamental particle required to unify the weak and electromagnetic interactions, the Higgs boson, was discovered at the Large Hadron Collider (LHC), located at the CERN laboratory in Switzerland. Despite the vast amount of knowledge acquired over the past century about the fundamental particles and forces of nature, many important questions still remain unanswered. For example, most of the matter in the universe that interacts gravitationally does not have ordinary electromagnetic or nuclear interactions.  As it has only been observed via its gravitation interactions, it is called dark matter.  What is it?  Equally interesting, why is there so little anti-matter in the universe when the fundamental interactions we know describe matter and anti-matter as almost perfect mirror images of each other? The LHC was built to discover and study the Higgs boson and to search for answers to these questions. The first data-taking run (Run 1, 2010-2012) of the LHC was a huge success, producing over 1000 journal articles, highlighted by the discovery of the Higgs boson. The current LHC run (Run 2, 2015-present) has already produced many world-leading results; however, the most interesting questions remained unanswered. The LHCb experiment, located on the LHC at CERN, has unique potential to answer some of these questions. LHCb is searching for signals of dark matter produced in high-energy particle collisions at the LHC, and performing high-precision studies of rare processes that could reveal the existence of the as-yet-unknown forces that caused the matter/anti-matter imbalance observed in our universe. The primary goal of this project - supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences - is developing and deploying software utilizing Machine Learning (ML) that will enable the LHCb experiment to significantly improve its discovery potential in Run 3 (2021-2023). Specifically, the ML developed will greatly increase the sensitivity to many proposed types of dark matter and new forces by making it possible to much more efficiently identify and study potential signals -- using the finite computing resources available.  \r\n\r\nThe data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, on which both PIs work, produce about 100 terabytes of data per second, close to a zettabyte of data per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year, comparable to the largest-scale industrial data sets. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. Trigger-system designs are dictated by the rate at which the sensors can be read out, the computational power of the data-ingestion system, and the available storage space for the data. The LHCb detector is being upgraded for Run 3 (2021-2023), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger are analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To process all the data on CPU farms, ML will be used to develop and deploy new trigger algorithms. The specific objectives of this proposal are to more fully characterize LHCb data using ML and build algorithms using these characterizations: to replace the most computationally expensive parts of the event pattern recognition; to increase the performance of the event-classification algorithms; and to reduce the number of bytes persisted per event without degrading physics performance. Many potential explanations for dark matter and the matter/anti-matter asymmetry of our universe are currently inaccessible due to trigger-system limitations. As HEP computing budgets are projected to be approximately flat moving forward, the LHCb trigger system must be redesigned for the experiment to realize its full potential. This redesign must go beyond scalable technical upgrades; radical new strategies are needed.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Sokoloff",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Michael D Sokoloff",
   "pi_email_addr": "mike.sokoloff@uc.edu",
   "nsf_id": "000233322",
   "pi_start_date": "2017-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Cincinnati Main Campus",
  "inst_street_address": "2600 CLIFTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CINCINNATI",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "5135564358",
  "inst_zip_code": "452202872",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "OH01",
  "org_lgl_bus_name": "CINCINNATI UNIV OF",
  "org_prnt_uei_num": "DZ4YCZ3QSPR5",
  "org_uei_num": "DZ4YCZ3QSPR5"
 },
 "perf_inst": {
  "perf_inst_name": "CERN",
  "perf_str_addr": "Route de Meyrin, 385",
  "perf_city_name": "Meyrin",
  "perf_st_code": "",
  "perf_st_name": "RI REQUIRED",
  "perf_zip_code": "",
  "perf_ctry_code": "SZ",
  "perf_cong_dist": "",
  "perf_st_cong_dist": "",
  "perf_ctry_name": "Switzerland",
  "perf_ctry_flag": "0"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "724400",
   "pgm_ele_name": "COMPUTATIONAL PHYSICS"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  },
  {
   "pgm_ref_code": "8005",
   "pgm_ref_txt": "Scientific Software Elements"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 224621.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In the past 200 years, physicists have discovered the basic constituents of ordinary matter (called quarks and leptons) and developed a very successful theory to describe the interactions (forces) between them. All atoms, and the molecules which are built from them, can be described in terms of these constituents.&nbsp; The nuclei of atoms are bound together by strong nuclear interactions. Their decays result from strong and weak nuclear interactions. Electromagnetic forces bind atoms together, and bind atoms into molecules. The electromagnetic, weak nuclear, and strong nuclear forces are described in terms of quantum field theories, together called the Standard Model of particle physics (SM).&nbsp; Despite this great progress, many profound questions remain to be addressed. For example, the universe consists primarily of matter with only a very small amount of anti-matter. This asymmetry cannot be explained by the known interactions which describe matter and anti-matter as almost perfect mirror images of each other.&nbsp; Similarly, most of the matter in the universe that interacts gravitationally does not have ordinary electromagnetic or nuclear interactions. As it has only been observed via its gravitation interactions, it is called dark matter. The field of particle physics, also called high energy physics (HEP), endeavors to better understand the details of the Standard Model and also to look for evidence of physics beyond the Standard Model.</p>\n<p><br />The LHCb experiment, located at CERN's Large Hadron Collider (LHC), has unique potential to answer some of these questions. During Run 3 (scheduled to start accumulating data in 2022) its sensor arrays will produce about 100 terabytes of data per second, close to a zettabyte of data per year.&nbsp; Even after drastic data-reduction performed by custom-built read-out electronics, the data volume will be about 10 exabytes per year, comparable to the largest-scale industrial data sets. Such large data sets cannot be stored indefinitely; therefore, all HEP experiments employ a data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. As HEP computing budgets are projected to be approximately flat moving forward, the LHCb trigger system must be redesigned for the experiment to realize its full potential.The primary goal of this award was to develop machine learning algorithms that can out-perform traditional, heuristic algorithms, for use in the trigger software.</p>\n<p><br />The LHC produces symmetric, counter-rotating beams of high energy protons that collide inside the VErtex LOcator (VELO) detector of the LHCb experiment.&nbsp; The trajectories of electrically charged particles emerging from proton-proton collisions that traverse the VELO are reconstructed in software.&nbsp; The beams cross each other about 30 million times per second and each beam crossing produces about 5 proton-proton collisions with enough tracks to be called visible. Once tracks are reconstructed, the trigger software needs to (i) identify the proton-proton collision points, called primary vertices (PVs), (ii) determine which tracks are associated with each PV, and (iii) determine which are secondary tracks not emerging directly from any PV.&nbsp;</p>\n<p><br />Our primary focus was developing a hybrid algorithm, called pv-finder, to identify PVs with very high efficiency.&nbsp; A kernel density estimator (KDE) was built heuristically from the ensemble of reconstructed track parameters&nbsp; and used as a set of input features for&nbsp; deep neural networks (DNNs) trained using simulated data to produce histograms from which simple (and very fast) heuristic algorithms produce lists of PV candidate positions. The best DNN achieves almost 98% efficiency for simulated data, significantly better than the default heuristic algorithm used by the experiment.</p>\n<p><br />We also investigated alternative DNN architectures and found that a modified version of the U-Net architecture, developed for use in bio-medical imaging, appears to produce very similar results to our original pv-finder. How these very different architectures lead to such similar results is a puzzle.&nbsp; We also studied variations of the original model, changing the numbers of \"neurons\" in the DNNs and the types of connections between them. In general, we observed that larger networks (defined by the numbers of parameters, including the numbers of neurons and the numbers of connections) tended to perform better. While this seems reasonable, we do not understand exactly how this happens.</p>\n<p><br />Finally, we investigated building \"track-to-KDE\" networks that could be connected to the original \"KDE-to-histogram\" networks to produce \"end-to-end\" DNNs to find PVs directly from the ensembles of track parameters.&nbsp; We demonstrated proof-of-principle versions, although they achieved lower efficiency than the hybrid algorithms, and higher false positive rates. All of these results have been presented at conferences and published in conference proceedings.</p>\n<p><br />Over the course of this award, three post-docs and four undergraduates (the students funded separately) developed expertise in machine learning and had opportunities to present their results.<br /><br /></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/29/2021<br>\n\t\t\t\t\tModified by: Michael&nbsp;D&nbsp;Sokoloff</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1740102/1740102_10518462_1640814616153_EffVsFP_29Jun21--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1740102/1740102_10518462_1640814616153_EffVsFP_29Jun21--rgov-800width.jpg\" title=\"vCHEP paper plot\"><img src=\"/por/images/Reports/POR/2021/1740102/1740102_10518462_1640814616153_EffVsFP_29Jun21--rgov-66x44.jpg\" alt=\"vCHEP paper plot\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This plot compares the performance of pv-finder models reported in previous years (labeled ACAT-2019 and CtD-2020) with new models described in detail in the text of our vCHEP-2021 paper (https://par.nsf.gov/biblio/10311259)</div>\n<div class=\"imageCredit\">Michael D Sokoloff</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Michael&nbsp;D&nbsp;Sokoloff</div>\n<div class=\"imageTitle\">vCHEP paper plot</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIn the past 200 years, physicists have discovered the basic constituents of ordinary matter (called quarks and leptons) and developed a very successful theory to describe the interactions (forces) between them. All atoms, and the molecules which are built from them, can be described in terms of these constituents.  The nuclei of atoms are bound together by strong nuclear interactions. Their decays result from strong and weak nuclear interactions. Electromagnetic forces bind atoms together, and bind atoms into molecules. The electromagnetic, weak nuclear, and strong nuclear forces are described in terms of quantum field theories, together called the Standard Model of particle physics (SM).  Despite this great progress, many profound questions remain to be addressed. For example, the universe consists primarily of matter with only a very small amount of anti-matter. This asymmetry cannot be explained by the known interactions which describe matter and anti-matter as almost perfect mirror images of each other.  Similarly, most of the matter in the universe that interacts gravitationally does not have ordinary electromagnetic or nuclear interactions. As it has only been observed via its gravitation interactions, it is called dark matter. The field of particle physics, also called high energy physics (HEP), endeavors to better understand the details of the Standard Model and also to look for evidence of physics beyond the Standard Model.\n\n\nThe LHCb experiment, located at CERN's Large Hadron Collider (LHC), has unique potential to answer some of these questions. During Run 3 (scheduled to start accumulating data in 2022) its sensor arrays will produce about 100 terabytes of data per second, close to a zettabyte of data per year.  Even after drastic data-reduction performed by custom-built read-out electronics, the data volume will be about 10 exabytes per year, comparable to the largest-scale industrial data sets. Such large data sets cannot be stored indefinitely; therefore, all HEP experiments employ a data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. As HEP computing budgets are projected to be approximately flat moving forward, the LHCb trigger system must be redesigned for the experiment to realize its full potential.The primary goal of this award was to develop machine learning algorithms that can out-perform traditional, heuristic algorithms, for use in the trigger software.\n\n\nThe LHC produces symmetric, counter-rotating beams of high energy protons that collide inside the VErtex LOcator (VELO) detector of the LHCb experiment.  The trajectories of electrically charged particles emerging from proton-proton collisions that traverse the VELO are reconstructed in software.  The beams cross each other about 30 million times per second and each beam crossing produces about 5 proton-proton collisions with enough tracks to be called visible. Once tracks are reconstructed, the trigger software needs to (i) identify the proton-proton collision points, called primary vertices (PVs), (ii) determine which tracks are associated with each PV, and (iii) determine which are secondary tracks not emerging directly from any PV. \n\n\nOur primary focus was developing a hybrid algorithm, called pv-finder, to identify PVs with very high efficiency.  A kernel density estimator (KDE) was built heuristically from the ensemble of reconstructed track parameters  and used as a set of input features for  deep neural networks (DNNs) trained using simulated data to produce histograms from which simple (and very fast) heuristic algorithms produce lists of PV candidate positions. The best DNN achieves almost 98% efficiency for simulated data, significantly better than the default heuristic algorithm used by the experiment.\n\n\nWe also investigated alternative DNN architectures and found that a modified version of the U-Net architecture, developed for use in bio-medical imaging, appears to produce very similar results to our original pv-finder. How these very different architectures lead to such similar results is a puzzle.  We also studied variations of the original model, changing the numbers of \"neurons\" in the DNNs and the types of connections between them. In general, we observed that larger networks (defined by the numbers of parameters, including the numbers of neurons and the numbers of connections) tended to perform better. While this seems reasonable, we do not understand exactly how this happens.\n\n\nFinally, we investigated building \"track-to-KDE\" networks that could be connected to the original \"KDE-to-histogram\" networks to produce \"end-to-end\" DNNs to find PVs directly from the ensembles of track parameters.  We demonstrated proof-of-principle versions, although they achieved lower efficiency than the hybrid algorithms, and higher false positive rates. All of these results have been presented at conferences and published in conference proceedings.\n\n\nOver the course of this award, three post-docs and four undergraduates (the students funded separately) developed expertise in machine learning and had opportunities to present their results.\n\n\n\n \n\n\t\t\t\t\tLast Modified: 12/29/2021\n\n\t\t\t\t\tSubmitted by: Michael D Sokoloff"
 }
}