{
 "awd_id": "1738888",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "STTR Phase II:  Dynamic Robust Hand Model for Gesture Intent Recognition",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Muralidharan Nair",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 1091828.0,
 "awd_min_amd_letter_date": "2017-08-31",
 "awd_max_amd_letter_date": "2020-06-18",
 "awd_abstract_narration": "The broader impact/commercial potential of this project stems from addressing the\r\nimportant hand gesture based input challenges of VR/AR (expected to grow to $150B by\r\n2020), Robotics and IoT ($135B by 2019 and $1.7T by 2020 respectively). This technology,\r\nif successful in mitigating the high technical risks, represents a huge leap in the state of the\r\nart in 3D hand models for gesture recognition and has the potential to be the industry\r\nstandard for AR, VR, Robotics and IoT applications with broad societal impact in education,\r\nmedical and healthcare. Its broader impact is further amplified by the potential in serving the\r\nneeds of the disabled community in improving their quality of life by being better able to\r\ncommunicate, learn and adapt to their interaction needs.\r\n\r\nThis Small Business Technology Transfer (STTR) Phase 2 project aims to significantly\r\nadvance current 3D hand gesture recognition technology by developing a dynamic hand\r\ntracking model for gesture intent recognition. It is robust against occlusion and tolerant to\r\nvariations in camera orientation and position. This research will result in a transformative\r\nleap above the current state of academic and commercial hand models and overcome key\r\ntechnical hurdles that have so far proven difficult to overcome. It solves the following key\r\nchallenges and involves very high technical risks: 1) robust hand tracking while holding\r\nobjects and 2) robust tangible interactions using objects without using any fiducial markers\r\n2) low profile hand wearable for touch interaction detection. This Phase 2 project will\r\nachieve these objectives by 1) data acquisition and hand-object pose estimation, 2)\r\nunderstanding user intents with enhanced tangible interactions, and 3) system validation\r\nand user testing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Raja",
   "pi_last_name": "Jasti",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Raja Jasti",
   "pi_email_addr": "raja@zeroui.com",
   "nsf_id": "000632415",
   "pi_start_date": "2017-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Karthik",
   "pi_last_name": "Ramani",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Karthik Ramani",
   "pi_email_addr": "ramani@purdue.edu",
   "nsf_id": "000284152",
   "pi_start_date": "2017-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "ZeroUI Inc",
  "inst_street_address": "10570 WHITNEY WAY",
  "inst_street_address_2": "",
  "inst_city_name": "CUPERTINO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "4088630555",
  "inst_zip_code": "950144442",
  "inst_country_name": "United States",
  "cong_dist_code": "17",
  "st_cong_dist_code": "CA17",
  "org_lgl_bus_name": "ZEROUI, INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "TV7ZN27KF9W8"
 },
 "perf_inst": {
  "perf_inst_name": "ZeroUI Inc",
  "perf_str_addr": "100 W. San Fernando St. Ste 114",
  "perf_city_name": "San Jose",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "951132260",
  "perf_ctry_code": "US",
  "perf_cong_dist": "18",
  "perf_st_cong_dist": "CA18",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "159100",
   "pgm_ele_name": "STTR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1591",
   "pgm_ref_txt": "STTR PHASE II"
  },
  {
   "pgm_ref_code": "165E",
   "pgm_ref_txt": "SBIR Phase IIB"
  },
  {
   "pgm_ref_code": "169E",
   "pgm_ref_txt": "SBIR Tech Enhan Partner (TECP)"
  },
  {
   "pgm_ref_code": "4080",
   "pgm_ref_txt": "ADVANCED COMP RESEARCH PROGRAM"
  },
  {
   "pgm_ref_code": "5373",
   "pgm_ref_txt": "SMALL BUSINESS PHASE II"
  },
  {
   "pgm_ref_code": "6840",
   "pgm_ref_txt": "ROBOTICS"
  },
  {
   "pgm_ref_code": "8035",
   "pgm_ref_txt": "Hardware Devices"
  },
  {
   "pgm_ref_code": "8240",
   "pgm_ref_txt": "SBIR/STTR CAP"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 750000.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 149999.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 10000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 181829.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has developed a breakthrough innovation in 3D hand gesture intent recognition for AR and VR&nbsp;that can 1) robustly work across different 3D cameras,orientations, positions and occlusions 2) allow for using any object as a handheld tool 3) enable touch interactions on hands. This research is a transformative leap above the current state of academic and commercialhand models and overcomes key technical hurdles that have so far proven difficult to overcome. It addresses a key challenge in gesture recognition while enabling natural spatial interactions for Virtual and Augmented Reality (VR/AR) and many other applications enabled by 3D depth cameras. It also resulted in a novel hand wearable for gesure intent recognition for use in robotics, AR and VR. It resolves some very challenging issues that have prevented pure vision based approaches from succeeding in AR/VR such as start-stop of gestures, unintended interactions, modality switching, simple operations such as attending to other devices, and so on. It reduces fatigue and enables never before possible interactions and workflows with hands in roboitics, AR and VR.&nbsp;</p>\n<p>This project's broad commercial impact stems from addressing the important hand gesture based input challenges of VR/AR (expected to grow to $150B by 2020 - Digi-Capital), Robotics and IoT ($135B by2019 and $1.7T by 2020 respectively - IDC). Piper Jaffray highlights new market opportunities for peripheral devices that bring hands and feet into VR. This technology, being successful in mitigating the high technical risks, represents a huge leap in the state of the art in 3D hand models for gesture recognition and has the potential to be the industry standard for AR, VR, Robotics and IoT applications with broadsocietal impact in education, medical and healthcare.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/19/2020<br>\n\t\t\t\t\tModified by: Raja&nbsp;Jasti</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1738888/1738888_10519881_1603159589283_Photo2copy--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1738888/1738888_10519881_1603159589283_Photo2copy--rgov-800width.jpg\" title=\"Ziro\"><img src=\"/por/images/Reports/POR/2020/1738888/1738888_10519881_1603159589283_Photo2copy--rgov-66x44.jpg\" alt=\"Ziro\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Hand controlled Educational Robotics Kit</div>\n<div class=\"imageCredit\">ZeroUI</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Raja&nbsp;Jasti</div>\n<div class=\"imageTitle\">Ziro</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project has developed a breakthrough innovation in 3D hand gesture intent recognition for AR and VR that can 1) robustly work across different 3D cameras,orientations, positions and occlusions 2) allow for using any object as a handheld tool 3) enable touch interactions on hands. This research is a transformative leap above the current state of academic and commercialhand models and overcomes key technical hurdles that have so far proven difficult to overcome. It addresses a key challenge in gesture recognition while enabling natural spatial interactions for Virtual and Augmented Reality (VR/AR) and many other applications enabled by 3D depth cameras. It also resulted in a novel hand wearable for gesure intent recognition for use in robotics, AR and VR. It resolves some very challenging issues that have prevented pure vision based approaches from succeeding in AR/VR such as start-stop of gestures, unintended interactions, modality switching, simple operations such as attending to other devices, and so on. It reduces fatigue and enables never before possible interactions and workflows with hands in roboitics, AR and VR. \n\nThis project's broad commercial impact stems from addressing the important hand gesture based input challenges of VR/AR (expected to grow to $150B by 2020 - Digi-Capital), Robotics and IoT ($135B by2019 and $1.7T by 2020 respectively - IDC). Piper Jaffray highlights new market opportunities for peripheral devices that bring hands and feet into VR. This technology, being successful in mitigating the high technical risks, represents a huge leap in the state of the art in 3D hand models for gesture recognition and has the potential to be the industry standard for AR, VR, Robotics and IoT applications with broadsocietal impact in education, medical and healthcare.\n\n\t\t\t\t\tLast Modified: 10/19/2020\n\n\t\t\t\t\tSubmitted by: Raja Jasti"
 }
}