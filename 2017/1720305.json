{
 "awd_id": "1720305",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "OP: Collaborative Research: Novel Feature-Based, Randomized Methods for Large-Scale Inversion",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Leland Jameson",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 148999.0,
 "awd_amount": 148999.0,
 "awd_min_amd_letter_date": "2017-05-15",
 "awd_max_amd_letter_date": "2017-05-15",
 "awd_abstract_narration": "The desire to form an image of a region of space from externally collected data arises in applications ranging from detecting and characterizing cancers in the body, to quantifying the distribution of water, oil, or subsurface pollutants, and to the timely accurate identification of explosives in crowded venues. The physics associated with signal propagation and sensing in these problems creates substantial computational challenges for transforming raw data into useful information. The research team in this project aims to develop computational methods that greatly reduce the cost of real time imaging by providing improvements in statistical inverse theory, numerical inversion methods, simulation models, and hybrid imaging models.  The main thrusts of the project will be tested on imaging applications in medical tomography, environmental remediation, and airport security imaging. The techniques form the basis for addressing analogous problems associated with inversion of optical signals across a wide range of spatial and temporal scales. As part of the project, a modular course will be developed to teach these new methods at the graduate level. The course materials will be made available over the internet.\r\n\r\nThe large-scale imaging, or inverse, problems addressed by this collaborative team require the minimization of a parameter-dependent function that expresses the misfit of predicted measurements for a candidate image and actual measurement data. The potentially large number of parameters must be minimized over an ever-increasing huge number of measurements, while concurrently some unknown set of the data may be redundant.  Detailed images, however, are not always needed for addressing relevant, practical questions and decision making. A combination of computational techniques will be developed to make large-scale parameter-dependent minimization computationally feasible.  Furthermore, novel efficient approaches for inferring critical image features will be developed, obviating need for complete reconstruction of an image. The research builds on recent methods that exploit randomization to compute accurate estimates of solutions at greatly reduced computational cost, and on the efficient construction of smaller, approximate, reduced order numerical models that are accurate for relevant sets of parameters, and thus reduce the cost of full simulation of the sensing physics. Probabilistic approaches for inference of critical image features that guide image interpretation and decision making will be developed. The mathematics associated with this approach requires these methods to capitalize on other new tools also under development in this project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "de Sturler",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eric de Sturler",
   "pi_email_addr": "sturler@vt.edu",
   "nsf_id": "000315274",
   "pi_start_date": "2017-05-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "225 Stanger St",
  "perf_city_name": "Blacksburg",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "240610001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "VA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8990",
   "pgm_ref_txt": "Optics and Photonics"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 148999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The costs for solving large 3D inverse problems with many measurements in&nbsp; imaging technology are formidable. The main costs arise from the repeated computer simulations that model the transmission and scattering of a signal (x-rays, infrared light, sound waves, and so on) through the media to be imaged and the resulting measurements. Some applications require real-time solutions to support crucial decisions, with access to, at best, a fast workstation. The PIs propose algorithms to reduce runtimes by orders of magnitude. These algorithms also support faster Bayesian inversion for Uncertainty Quantification (UQ), which in turn supports better informed decisions.</p>\n<p>The aim of UQ is to provide more than a good solution. It provides, in addition, information about how likely a certain solution is given the measured data relative to variations of that solution, or more generally it can provide a probability distribution of solutions given the measured data. In addition to providing such information for the solutions of shape-based inverse problems, where one solves for a parameter vector that defines the location and shape of an anomaly, the PIs also propose methods that compute the probability that a certain region is inside an anomaly (say, that some region of the body is inside a tumor, or that some region of the subsurface contains a pollutant).&nbsp;</p>\n<div>\n<div>\n<div>The main vehicle for computing this information is Bayesian inversion, which is very expensive. First, there are the high costs, already mentioned, for simulating the physics to evaluate the likelihood of a solution, the probability that the measured data corresponds to a given parameter vector. Second, computing the most likely solution given the data and prior (known or assumed) information on the parameter solution requires sampling from the, so-called, posterior probability distribution, defined by the likelihood and a prior model, using a Markov chain Monte Carlo (MCMC) process, where each sample evaluation requires the expensive computation of the likelihood. Computing  important statistics from the posterior distribution and/or an accurate approximation of this distribution may require a few million samples, even for a modest number of parameters, with each sample requiring the solution of hundreds to a few thousand very large linear or nonlinear&nbsp;systems, derived from discretized partial differential equations.</div>\n<div>The PIs have developed  several approaches to drastically reduce the cost of evaluating the likelihood using randomization, for example by multiplying the misfit (the difference between the measured data and the computed data given a parameter vector) by a random matrix and taking the norm. This drastically reduces the number of linear systems to solve and hence reduces the computational cost, while providing unbiased estimates for the likelihood. In addition, the PIs have developed proposal distributions for importance sampling that increase the effective number of samples by a substantial factor, and hence require fewer total samples in the MCMC process, further reducing the computational cost. Other approaches that have been developed in this project to reduce the cost of these expensive computations are the use of model reduction (a reduced order model provides a cheap approximation to the full model) for approximating the likelihood in maximum likelihood estimation, combined with a stochastic method to estimate the difference between the reduced model and the (expensive) full model, and an efficient way of updating the reduced model if this difference is too large. The PIs have also adapted optimization methods to exploit model reduction efficiently and robustly. Furthermore, they have developed randomized methods to efficiently compute reduced order models. In addition, they have derived and tested a better method for parameterizing shapes in shape-based inversion methods and a new approach to reduce the cost of large linear solves directly by computing more efficiently preconditioners for a large sequence of linear systems.</div>\n<div>The methods developed by the PIs have much broader application than the problems that were the primary area of investigation. As a result, this research has also led to improved computational methods to solve other expensive optimization problems, such as topology optimization for the design of optimal structures and other methods to optimize the shape of a structure. The project has also led to the development of improved hybrid regularization methods for the solution of very large linear inverse problems, including problems where streaming is essential.</div>\n<div>Three PhD students were supported as part of this project, which also involved collaboration with a postdoctoral scholar, introducing several young scientists to these important problems and methods.&nbsp;</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/28/2022<br>\n\t\t\t\t\tModified by: Eric&nbsp;De Sturler</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe costs for solving large 3D inverse problems with many measurements in  imaging technology are formidable. The main costs arise from the repeated computer simulations that model the transmission and scattering of a signal (x-rays, infrared light, sound waves, and so on) through the media to be imaged and the resulting measurements. Some applications require real-time solutions to support crucial decisions, with access to, at best, a fast workstation. The PIs propose algorithms to reduce runtimes by orders of magnitude. These algorithms also support faster Bayesian inversion for Uncertainty Quantification (UQ), which in turn supports better informed decisions.\n\nThe aim of UQ is to provide more than a good solution. It provides, in addition, information about how likely a certain solution is given the measured data relative to variations of that solution, or more generally it can provide a probability distribution of solutions given the measured data. In addition to providing such information for the solutions of shape-based inverse problems, where one solves for a parameter vector that defines the location and shape of an anomaly, the PIs also propose methods that compute the probability that a certain region is inside an anomaly (say, that some region of the body is inside a tumor, or that some region of the subsurface contains a pollutant). \n\n\nThe main vehicle for computing this information is Bayesian inversion, which is very expensive. First, there are the high costs, already mentioned, for simulating the physics to evaluate the likelihood of a solution, the probability that the measured data corresponds to a given parameter vector. Second, computing the most likely solution given the data and prior (known or assumed) information on the parameter solution requires sampling from the, so-called, posterior probability distribution, defined by the likelihood and a prior model, using a Markov chain Monte Carlo (MCMC) process, where each sample evaluation requires the expensive computation of the likelihood. Computing  important statistics from the posterior distribution and/or an accurate approximation of this distribution may require a few million samples, even for a modest number of parameters, with each sample requiring the solution of hundreds to a few thousand very large linear or nonlinear systems, derived from discretized partial differential equations.\nThe PIs have developed  several approaches to drastically reduce the cost of evaluating the likelihood using randomization, for example by multiplying the misfit (the difference between the measured data and the computed data given a parameter vector) by a random matrix and taking the norm. This drastically reduces the number of linear systems to solve and hence reduces the computational cost, while providing unbiased estimates for the likelihood. In addition, the PIs have developed proposal distributions for importance sampling that increase the effective number of samples by a substantial factor, and hence require fewer total samples in the MCMC process, further reducing the computational cost. Other approaches that have been developed in this project to reduce the cost of these expensive computations are the use of model reduction (a reduced order model provides a cheap approximation to the full model) for approximating the likelihood in maximum likelihood estimation, combined with a stochastic method to estimate the difference between the reduced model and the (expensive) full model, and an efficient way of updating the reduced model if this difference is too large. The PIs have also adapted optimization methods to exploit model reduction efficiently and robustly. Furthermore, they have developed randomized methods to efficiently compute reduced order models. In addition, they have derived and tested a better method for parameterizing shapes in shape-based inversion methods and a new approach to reduce the cost of large linear solves directly by computing more efficiently preconditioners for a large sequence of linear systems.\nThe methods developed by the PIs have much broader application than the problems that were the primary area of investigation. As a result, this research has also led to improved computational methods to solve other expensive optimization problems, such as topology optimization for the design of optimal structures and other methods to optimize the shape of a structure. The project has also led to the development of improved hybrid regularization methods for the solution of very large linear inverse problems, including problems where streaming is essential.\nThree PhD students were supported as part of this project, which also involved collaboration with a postdoctoral scholar, introducing several young scientists to these important problems and methods. \n\n\n\n\t\t\t\t\tLast Modified: 02/28/2022\n\n\t\t\t\t\tSubmitted by: Eric De Sturler"
 }
}