{
 "awd_id": "1741317",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Collaborative Research: Taming Big Networks via Embedding",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2018-01-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 400001.0,
 "awd_amount": 400001.0,
 "awd_min_amd_letter_date": "2017-08-30",
 "awd_max_amd_letter_date": "2017-08-30",
 "awd_abstract_narration": "In the Internet Age, information entities and objects are interconnected, thereby forming gigantic information networks. Recently, network embedding methods, that create low-dimensional feature representations that preserve the structure of data points in their original space, have been shown to be greatly beneficial for many data mining and machine learning problems over networks. Despite significant research progress, we are still lacking powerful network embedding techniques with theoretical guarantees to effectively deal with massive, heterogeneous, complex and dynamic networks. The PIs aim to develop a new generation of network embedding methods for analyzing massive networks. The research project has the potential to significantly transform graph mining and network analysis. The PIs also plan to develop open course materials and open source software tools that integrate information network analysis and machine learning. \r\n\r\nThis project consists of four synergistic research thrusts. First, it develops model-based network embedding to leverage the first-order and second-order proximity of networks. Second, it devises a family of inductive network embedding methods that are able to leverage both linkage information and side information. Third, it develops both local clustering and deep learning based network embedding methods to attack the complex structure of networks such as locality and non-linearity. Fourth, it develops online and stochastic optimization algorithms for different network embedding methods to tackle the fast growth and evolution of modern massive networks. The new methods developed in this project enjoy faster rates of convergence in optimization, lower computational complexities, and statistical learning guarantees. The targeted applications include but are not limited to semantic search and information retrieval in social/information network analysis, expert finding in bibliographical database, and recommendation systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jiawei",
   "pi_last_name": "Han",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jiawei Han",
   "pi_email_addr": "hanj@illinois.edu",
   "nsf_id": "000339011",
   "pi_start_date": "2017-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618207473",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 400001.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project, &ldquo;NSF BIGDATA: F: Collaborative Research: Taming Big Networks via Embedding&rdquo;, NSF IIS 17-41317 (01/01/2018-8/31/2023), has achieved fruitful and high-impact results, with over 100 research papers in major research venues and numerous citations.&nbsp; The project has generated 14 PhDs, and many Masters/B.S.&rsquo;s.&nbsp; Six PhDs have become assistant professors in universities (including Georgia Tech, UCSD, Emory, Virginia Tech, Washington U. at St. Louis, and Univ. of Virginia) and two of them have received ACM SIGKDD Dissertation Award Runner-Up.&nbsp; The research has also attracted a good number of female and minority students to join our team (for example, 6 of our current 12 PhD students are female) and the students have received a few prominent awards from industry including Google, Microsoft, and Amazon PhD Fellowships for PhDs and two Siebel Scholar Fellowships for MS&rsquo;s.&nbsp; The project has proceeded in the direction as planned: Taming Big networks via Embedding and Large Foundational Models and transform massive, unstructured but interconnected data into actionable knowledge, with a new, promising paradigm: data-to-network-to-knowledge, by integrating semi-structured and unstructured data, constructing organized heterogeneous information networks, and then developing powerful mining mechanisms on such organized networks.&nbsp; We have been working on principles, methodologies and algorithms for mining massive corpora and transforming text knowledge into relatively structured heterogeneous information networks.&nbsp;</p>\n<p>Five selected papers in the attached images demonstrate its intellectual merit and broad impact. All the associated software is open source in GitHub.&nbsp;</p>\n<ol>\n<li>PENNER: Pattern-enhanced Nested Named Entity Recognition in Biomedical Literature (2018). PENNER is a novel and effective pattern-enhanced nested named entity recognition method that relies on massive corpora plus only very weak supervision.&nbsp; Compared with a state-of-the-art BioNER system, PubTator, PENNER shows great improvement at recognizing genes, chemicals, diseases and species. It can also accurately extract new types of entities, such as biological process and treatment, that are not annotated by PubTator. Publication: Xuan Wang, Yu Zhang, Qi Li, Cathy Wu, and Jiawei Han, \"PENNER: Pattern-enhanced Nested Named Entity Recognition in Biomedical Literature\", in Proc. 2018 Int. Conf. on Bioinformatics and Biomedicine (BIBM'18). <strong></strong></li>\n<li>NetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network (2020).&nbsp; We propose a novel automatic topic taxonomy construction framework, NetTaxo, which learns term embeddings from both text and network as contexts.&nbsp; We conduct an instance-level selection for network motifs, which further refines term embedding according to the granularity and semantics of each taxonomy node. Clustering is then applied to obtain sub-topics under a taxonomy node, which achieves high performance on real-world datasets. Publication: Jingbo Shang, Xinyang Zhang, Liyuan Liu, Sha Li and Jiawei Han, &ldquo;NetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network&rdquo;, in Proc. 2020 Int. World Wide Web Conf. (WWW&rsquo;20). (Jingbo Shang's thesis: 2020 ACM SIGKDD Dissertation Award RunnerUp).</li>\n<li>ETypeClus: Corpus-based Open-Domain Event Type Induction (2021).&nbsp;&nbsp; We develop a corpus-based open-domain event type induction method, ETypeClus, that automatically discovers a set of event types from a given corpus. &nbsp;We represent each event type as a cluster of &lt;predicate sense, object head&gt; pairs. Our method selects salient predicates and object heads, disambiguates predicate senses using only a verb sense dictionary, and obtains event types by jointly embedding and clustering in a latent spherical space. Our experiments show ETypeClus can discover salient and high-quality event types.&nbsp; Publication: Jiaming Shen, Yunyi Zhang, Heng Ji and Jiawei Han, \"Corpus-based Open-Domain Event Type Induction\", in Proc. 2021 Conf. on Empirical Methods in Natural Language Processing (EMNLP'21).</li>\n<li>TopClus: Topic Discovery via Latent Space Clustering of Language Model Embeddings (2022). TopClus is a joint latent space learning and clustering framework built upon PLM embeddings. In the latent space, topic-word and document-topic distributions are jointly modeled so that the discovered topics can be interpreted by coherent and distinctive terms and meanwhile serve as meaningful summaries of the documents.&nbsp; TopClus generates significantly more coherent and diverse topics than strong topic models, and offers better topic-wise document representations, based on both automatic and human evaluations.&nbsp; Publication: &nbsp;Yu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang and Jiawei Han, &ldquo;Topic Discovery via Latent Space Clustering of Language Model Embeddings&rdquo;, in Proc. The ACM Web Conf. 2022 (WWW&rsquo;22).</li>\n<li>Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks (2023). We propose Edgeformers, a framework built upon graph-enhanced Transformers, to perform edge and node representation learning by modeling texts on edges in a contextualized way. In edge representation learning, we inject network information into each Transformer layer when encoding edge texts; in node representation learning, we aggregate edge representations through an attention mechanism within each node&rsquo;s ego-graph. Our experiments show that Edgeformers consistently outperform state-of-the-art baselines in edge classification and link prediction. Publication: Bowen Jin, Yu Zhang, Yu Meng, Jiawei Han, &ldquo;Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks&rdquo;, in Proc. 2023 Int. Conf. on Learning Representations (ICLR&rsquo;23).</li>\n</ol><br>\n<p>\n Last Modified: 12/10/2023<br>\nModified by: Jiawei&nbsp;Han</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702265109733_3_ETypeClus--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702265109733_3_ETypeClus--rgov-800width.jpg\" title=\"ETypeClus: Corpus-based Open-Domain Event Type Induction (2021)\"><img src=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702265109733_3_ETypeClus--rgov-66x44.jpg\" alt=\"ETypeClus: Corpus-based Open-Domain Event Type Induction (2021)\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Jiaming Shen, Yunyi Zhang, Heng Ji and Jiawei Han, \"Corpus-based Open-Domain Event Type Induction\", in Proc. 2021 Conf. on Empirical Methods in Natural Language Processing (EMNLP'21)</div>\n<div class=\"imageCredit\">NSF IIS 17-41317</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jiawei&nbsp;Han\n<div class=\"imageTitle\">ETypeClus: Corpus-based Open-Domain Event Type Induction (2021)</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702264766991_1_PENNER--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702264766991_1_PENNER--rgov-800width.jpg\" title=\"PENNER: Pattern-enhanced Nested Named Entity Recognition in Biomedical Literature (2018)\"><img src=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702264766991_1_PENNER--rgov-66x44.jpg\" alt=\"PENNER: Pattern-enhanced Nested Named Entity Recognition in Biomedical Literature (2018)\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Xuan Wang, Yu Zhang, Qi Li, Cathy Wu, and Jiawei Han, \"PENNER: Pattern-enhanced Nested Named Entity Recognition in Biomedical Literature\", in Proc. 2018 Int. Conf. on Bioinformatics and Biomedicine (BIBM'18)</div>\n<div class=\"imageCredit\">NSF IIS 17-41317</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jiawei&nbsp;Han\n<div class=\"imageTitle\">PENNER: Pattern-enhanced Nested Named Entity Recognition in Biomedical Literature (2018)</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702265269080_5_Edgeformer--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702265269080_5_Edgeformer--rgov-800width.jpg\" title=\"Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks (2023)\"><img src=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702265269080_5_Edgeformer--rgov-66x44.jpg\" alt=\"Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks (2023)\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Bowen Jin, Yu Zhang, Yu Meng, Jiawei Han, \ufffdEdgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks\ufffd, in Proc. 2023 Int. Conf. on Learning Representations (ICLR\ufffd23)</div>\n<div class=\"imageCredit\">NSF IIS 17-41317</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jiawei&nbsp;Han\n<div class=\"imageTitle\">Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks (2023)</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702264951762_2_NetTaxo--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702264951762_2_NetTaxo--rgov-800width.jpg\" title=\"NetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network (2020)\"><img src=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702264951762_2_NetTaxo--rgov-66x44.jpg\" alt=\"NetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network (2020)\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Jingbo Shang, Xinyang Zhang, Liyuan Liu, Sha Li and Jiawei Han, \ufffdNetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network\ufffd, in Proc. 2020 Int. World Wide Web Conf. (WWW\ufffd20)</div>\n<div class=\"imageCredit\">NSF IIS 17-41317</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jiawei&nbsp;Han\n<div class=\"imageTitle\">NetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network (2020)</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702265196122_4_TopClus--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702265196122_4_TopClus--rgov-800width.jpg\" title=\"TopClus: Topic Discovery via Latent Space Clustering of Language Model Embeddings (2022)\"><img src=\"/por/images/Reports/POR/2023/1741317/1741317_10519506_1702265196122_4_TopClus--rgov-66x44.jpg\" alt=\"TopClus: Topic Discovery via Latent Space Clustering of Language Model Embeddings (2022)\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Yu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang and Jiawei Han, \ufffdTopic Discovery via Latent Space Clustering of Language Model Embeddings\ufffd, in Proc. The ACM Web Conf. 2022 (WWW\ufffd22)</div>\n<div class=\"imageCredit\">NSF IIS 17-41317</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jiawei&nbsp;Han\n<div class=\"imageTitle\">TopClus: Topic Discovery via Latent Space Clustering of Language Model Embeddings (2022)</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project, NSF BIGDATA: F: Collaborative Research: Taming Big Networks via Embedding, NSF IIS 17-41317 (01/01/2018-8/31/2023), has achieved fruitful and high-impact results, with over 100 research papers in major research venues and numerous citations. The project has generated 14 PhDs, and many Masters/B.S.s. Six PhDs have become assistant professors in universities (including Georgia Tech, UCSD, Emory, Virginia Tech, Washington U. at St. Louis, and Univ. of Virginia) and two of them have received ACM SIGKDD Dissertation Award Runner-Up. The research has also attracted a good number of female and minority students to join our team (for example, 6 of our current 12 PhD students are female) and the students have received a few prominent awards from industry including Google, Microsoft, and Amazon PhD Fellowships for PhDs and two Siebel Scholar Fellowships for MSs. The project has proceeded in the direction as planned: Taming Big networks via Embedding and Large Foundational Models and transform massive, unstructured but interconnected data into actionable knowledge, with a new, promising paradigm: data-to-network-to-knowledge, by integrating semi-structured and unstructured data, constructing organized heterogeneous information networks, and then developing powerful mining mechanisms on such organized networks. We have been working on principles, methodologies and algorithms for mining massive corpora and transforming text knowledge into relatively structured heterogeneous information networks.\n\n\nFive selected papers in the attached images demonstrate its intellectual merit and broad impact. All the associated software is open source in GitHub.\n\nPENNER: Pattern-enhanced Nested Named Entity Recognition in Biomedical Literature (2018). PENNER is a novel and effective pattern-enhanced nested named entity recognition method that relies on massive corpora plus only very weak supervision. Compared with a state-of-the-art BioNER system, PubTator, PENNER shows great improvement at recognizing genes, chemicals, diseases and species. It can also accurately extract new types of entities, such as biological process and treatment, that are not annotated by PubTator. Publication: Xuan Wang, Yu Zhang, Qi Li, Cathy Wu, and Jiawei Han, \"PENNER: Pattern-enhanced Nested Named Entity Recognition in Biomedical Literature\", in Proc. 2018 Int. Conf. on Bioinformatics and Biomedicine (BIBM'18). \nNetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network (2020). We propose a novel automatic topic taxonomy construction framework, NetTaxo, which learns term embeddings from both text and network as contexts. We conduct an instance-level selection for network motifs, which further refines term embedding according to the granularity and semantics of each taxonomy node. Clustering is then applied to obtain sub-topics under a taxonomy node, which achieves high performance on real-world datasets. Publication: Jingbo Shang, Xinyang Zhang, Liyuan Liu, Sha Li and Jiawei Han, NetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network, in Proc. 2020 Int. World Wide Web Conf. (WWW20). (Jingbo Shang's thesis: 2020 ACM SIGKDD Dissertation Award RunnerUp).\nETypeClus: Corpus-based Open-Domain Event Type Induction (2021). We develop a corpus-based open-domain event type induction method, ETypeClus, that automatically discovers a set of event types from a given corpus. We represent each event type as a cluster of \n\n\nTopClus: Topic Discovery via Latent Space Clustering of Language Model Embeddings (2022). TopClus is a joint latent space learning and clustering framework built upon PLM embeddings. In the latent space, topic-word and document-topic distributions are jointly modeled so that the discovered topics can be interpreted by coherent and distinctive terms and meanwhile serve as meaningful summaries of the documents. TopClus generates significantly more coherent and diverse topics than strong topic models, and offers better topic-wise document representations, based on both automatic and human evaluations. Publication: Yu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang and Jiawei Han, Topic Discovery via Latent Space Clustering of Language Model Embeddings, in Proc. The ACM Web Conf. 2022 (WWW22).\nEdgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks (2023). We propose Edgeformers, a framework built upon graph-enhanced Transformers, to perform edge and node representation learning by modeling texts on edges in a contextualized way. In edge representation learning, we inject network information into each Transformer layer when encoding edge texts; in node representation learning, we aggregate edge representations through an attention mechanism within each nodes ego-graph. Our experiments show that Edgeformers consistently outperform state-of-the-art baselines in edge classification and link prediction. Publication: Bowen Jin, Yu Zhang, Yu Meng, Jiawei Han, Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks, in Proc. 2023 Int. Conf. on Learning Representations (ICLR23).\n\t\t\t\t\tLast Modified: 12/10/2023\n\n\t\t\t\t\tSubmitted by: JiaweiHan\n"
 }
}