{
 "awd_id": "1703051",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Medium: A Visual Cloud for Virtual Reality Applications",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 916000.0,
 "awd_amount": 916000.0,
 "awd_min_amd_letter_date": "2017-07-18",
 "awd_max_amd_letter_date": "2017-07-18",
 "awd_abstract_narration": "The ability to collect videos can revolutionize interactions with the world by enabling powerful virtual reality video applications in education, tourism, tele-presence and others. These new applications involve processing and serving 360-degree stereoscopic videos, which require a dramatic improvement in technology to manage and process the massive-scale visual data necessary for truly immersive experiences. Systems that support VR video also represent an excellent educational tool as students can experience scenes in a truly immersive way and hence better convey content. \r\nThis project builds a visual cloud that provides seamless access to a new database management system with hardware acceleration and edge computation that enables the efficient and real-time management of massive-scale image data and virtual reality (VR) applications built on top of it. \r\nThe project develops a new hardware and software stack for VR data processing with execution in public clouds. The stack includes a new storage manager that significantly increases data ingest and retrieval throughputs for multidimensional array data compared with existing systems, as motivated by the extreme needs of VR applications. The storage manager utilizes novel hardware technologies (non-volatile memory) and provides novel approximate and multi-resolution data storage capabilities. The project also develops a new runtime system for high-throughput and large-scale array processing by developing a new API for expressing VR pipelines as a graph of user-defined functions, a library of specialized implementations of known VR algorithms for different types of hardware (CPU, GPU, FPGA, and 3D XPoint), and associated optimizers and schedulers. Finally, the project develops new techniques to enable real-time VR applications. They include an FPGA-based acceleration platform for real-time VR video processing and algorithms and software components for prefetching and caching VR data close to the viewers and processing that data in the viewing device.  http://visualcloud.cs.washington.edu",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Magdalena",
   "pi_last_name": "Balazinska",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Magdalena Balazinska",
   "pi_email_addr": "magda@cs.washington.edu",
   "nsf_id": "000094498",
   "pi_start_date": "2017-07-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Luis",
   "pi_last_name": "Ceze",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Luis Ceze",
   "pi_email_addr": "luisceze@cs.washington.edu",
   "nsf_id": "000083036",
   "pi_start_date": "2017-07-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn Ave NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 916000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Advances in technology have made it possible to collect large volumes of video data in diverse environments. For example, cameras are set up in nature to observe animal behaviors or they are installed on city streets to monitor traffic. Specialized camera configurations can also capture immersive, 360-degree virtual experiences. Advances in AI make it possible to automatically extract content from those videos, opening the possibility to ask complex questions (e.g., ?Find all instances of cars going around a bus and driving near pedestrians in a crosswalk?). The challenge is that working with large volumes of video data can be slow and cumbersome. This project developed multiple tools and techniques to facilitate the task of building interesting applications on top of video data by providing support for video data management.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Project website: https://db.cs.washington.edu/projects/visualworld/</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project resulted in a first system, called LightDB, that efficiently manages virtual, augmented, and mixed reality video content (VAMR). VAMR video differs from its two-dimensional counterpart in that it is spherical with angular dimensions (a viewer can look straight but also up, down, and behind) and applications that consume such videos often have demanding latency and throughput requirements. To address these challenges, LightDB exposes a spherical data model to applications. That is, an application developer can directly reason about a stream of spherical images, without worrying how those spheres are encoded and stored in video files. LightDB supports a rich set of operations over this data model, and automatically transforms declarative queries into executable physical plans. The LightDB system executes those plans efficiently, including by leveraging hardware accelerators that were developed in the context of this project. The paper describing LightDB was published in the Proceedings of the VLDB Endowment, Vol 11, No 10 (2018). The source code is available on Github and accessible through the project website.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project also produced a suite of new technologies for video data storage. The common way to store video data is to store each video as a single, encoded file. This type of storage method, however, is inefficient when applications need to extract only subsets of a video to answer a query.&nbsp; This project developed TASM, a new tile-based storage manager for video data. TASM uses a feature in modern video codecs called ?tiles? that enables spatial random access into encoded videos. TASM physically tunes stored videos by optimizing their tile layouts given the video content and a query workload. Additionally, TASM dynamically tunes that layout in response to changes in the query workload or if the query workload and video contents are incrementally discovered. The paper describing TASM was published in the Proceedings of ICDE in 2021. The source code is available on Github and accessible through the project website.</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project also developed a second video storage system, called VSS. VSS was designed to decouple high-level video operations from the low-level details required to store and efficiently retrieve video data. VSS was designed to be the storage subsystem of a video data management system (VDBMS) and is responsible for: (1) transparently and automatically arranging the data on disk in an efficient, granular format; (2) caching frequently-retrieved regions in the most useful formats; and (3) eliminating redundancies found in videos captured from multiple cameras with overlapping fields of view. The paper describing VSS was published in the Proceedings of SIGMOD 2021.&nbsp; The source code is available on Github and accessible through the project website.</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project also developed a third video storage technology, called Vignette. Vignette provides perception-based video compression in the cloud. Vignette complements off-the-shelf compression software and hardware codec implementations. Vignette's compression technique uses a neural network to predict saliency information (i.e., where the users typically look), uses this information during transcoding, and its storage manager integrates perceptual information into the video storage system. Vignette was published in Proceedings of SOCC 2019.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To accelerate innovation in the broad area of video data management, the project also developed Visual Road, a benchmark that evaluates the performance of these systems. Visual Road comes with a data generator and a suite of queries over cameras positioned within a simulated metropolitan environment. The Visual Road benchmark was published in the Proceedings of SIGMOD 2019. The source code is available on Github and accessible through the project website.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Given the importance and volume of video data today, the above technologies (which are available as open source software and which have been presented at multiple conferences and in other settings) can significantly improve our ability to manage video data, including data for virtual reality applications. In addition to developing novel technologies, this project helped to educate multiple graduate and undergraduate students, and engaged in K-12 outreach including by creating a short activity based on the video data management research developed in this project for the Allen School CS Education Week.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/28/2022<br>\n\t\t\t\t\tModified by: Magdalena&nbsp;Balazinska</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Advances in technology have made it possible to collect large volumes of video data in diverse environments. For example, cameras are set up in nature to observe animal behaviors or they are installed on city streets to monitor traffic. Specialized camera configurations can also capture immersive, 360-degree virtual experiences. Advances in AI make it possible to automatically extract content from those videos, opening the possibility to ask complex questions (e.g., ?Find all instances of cars going around a bus and driving near pedestrians in a crosswalk?). The challenge is that working with large volumes of video data can be slow and cumbersome. This project developed multiple tools and techniques to facilitate the task of building interesting applications on top of video data by providing support for video data management.\n\n \nProject website: https://db.cs.washington.edu/projects/visualworld/\n\n \nThis project resulted in a first system, called LightDB, that efficiently manages virtual, augmented, and mixed reality video content (VAMR). VAMR video differs from its two-dimensional counterpart in that it is spherical with angular dimensions (a viewer can look straight but also up, down, and behind) and applications that consume such videos often have demanding latency and throughput requirements. To address these challenges, LightDB exposes a spherical data model to applications. That is, an application developer can directly reason about a stream of spherical images, without worrying how those spheres are encoded and stored in video files. LightDB supports a rich set of operations over this data model, and automatically transforms declarative queries into executable physical plans. The LightDB system executes those plans efficiently, including by leveraging hardware accelerators that were developed in the context of this project. The paper describing LightDB was published in the Proceedings of the VLDB Endowment, Vol 11, No 10 (2018). The source code is available on Github and accessible through the project website.\n\n \nThis project also produced a suite of new technologies for video data storage. The common way to store video data is to store each video as a single, encoded file. This type of storage method, however, is inefficient when applications need to extract only subsets of a video to answer a query.  This project developed TASM, a new tile-based storage manager for video data. TASM uses a feature in modern video codecs called ?tiles? that enables spatial random access into encoded videos. TASM physically tunes stored videos by optimizing their tile layouts given the video content and a query workload. Additionally, TASM dynamically tunes that layout in response to changes in the query workload or if the query workload and video contents are incrementally discovered. The paper describing TASM was published in the Proceedings of ICDE in 2021. The source code is available on Github and accessible through the project website.\n \nThis project also developed a second video storage system, called VSS. VSS was designed to decouple high-level video operations from the low-level details required to store and efficiently retrieve video data. VSS was designed to be the storage subsystem of a video data management system (VDBMS) and is responsible for: (1) transparently and automatically arranging the data on disk in an efficient, granular format; (2) caching frequently-retrieved regions in the most useful formats; and (3) eliminating redundancies found in videos captured from multiple cameras with overlapping fields of view. The paper describing VSS was published in the Proceedings of SIGMOD 2021.  The source code is available on Github and accessible through the project website.\n \nThis project also developed a third video storage technology, called Vignette. Vignette provides perception-based video compression in the cloud. Vignette complements off-the-shelf compression software and hardware codec implementations. Vignette's compression technique uses a neural network to predict saliency information (i.e., where the users typically look), uses this information during transcoding, and its storage manager integrates perceptual information into the video storage system. Vignette was published in Proceedings of SOCC 2019. \n\n \nTo accelerate innovation in the broad area of video data management, the project also developed Visual Road, a benchmark that evaluates the performance of these systems. Visual Road comes with a data generator and a suite of queries over cameras positioned within a simulated metropolitan environment. The Visual Road benchmark was published in the Proceedings of SIGMOD 2019. The source code is available on Github and accessible through the project website.\n\n \nGiven the importance and volume of video data today, the above technologies (which are available as open source software and which have been presented at multiple conferences and in other settings) can significantly improve our ability to manage video data, including data for virtual reality applications. In addition to developing novel technologies, this project helped to educate multiple graduate and undergraduate students, and engaged in K-12 outreach including by creating a short activity based on the video data management research developed in this project for the Allen School CS Education Week.\n\n\t\t\t\t\tLast Modified: 11/28/2022\n\n\t\t\t\t\tSubmitted by: Magdalena Balazinska"
 }
}