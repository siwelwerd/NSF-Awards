{
 "awd_id": "1745365",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Top-down processes to extract meaning from images",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Kurt Thoroughman",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2019-09-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2017-07-12",
 "awd_max_amd_letter_date": "2017-07-12",
 "awd_abstract_narration": "Humans rapidly process images and scenes so they can understand what is occurring in the world around them. Humans do this so well that they outperform existing computational models' ability to understand what elements exist in the scene, their location, or the actions they are involved in. One limitation of computational models is that they do not provide a detailed interpretation of a scene's individual components the way that humans do. For example, the computational model may successfully label an image as containing a horse, but humans will also naturally identify smaller components of the horse, such as the eyes, ears, mouth, mane, legs, tail, and so on. Identifying these individual components and their relationships is an essential part of human visual processing. Such differences in visual understanding create a challenge for constructing artificial computational systems that see and interpret the world similarly to humans. These fundamental limitations are related to the fact that existing computational systems rely primarily on what is called 'bottom-up processing', the sequential processing of visual features from simple to complex ones, which does not account for how human cognition influences meaningful recognition of the image. Our main goal is to investigate the computational principles and neurobiological systems that allow for integration of cognitive experience within visual processing. We combine psychological studies in humans, neurophysiological recordings of brain tissue, and computational work to build an integrative model capable of extracting complex meaning from images, in a way that more closely resembles human capabilities. The research will have broad implications in understanding how the brain processes images and the neural circuits that are involved. Additionally, the insights obtained from this project could have applications in a broad range of domains including robot vision, automatic navigation, surveillance, and automatic clinical image understanding. As part of the project we will establish a summer course based on the research products in which we will train the next generation of scholars at the interface of brains, minds, and machines. \r\n\r\nIn the human brain, information flows both from low to higher visual areas, as well as in the opposite direction throughout ventral visual cortex. This bi-directional processing has a fundamental role in cortical computations. Yet, the functions implemented by the top-down components form an open problem in visual cognition. Understanding the limitations of feed-forward processing (from lower to higher visual regions) will shed light on the mechanisms by which prior knowledge is integrated with bottom-up inputs, and guide development of new algorithms for extracting useful meaning from sensory input. Applications of deep network models now play a significant role in machine learning across multiple domains, but these fail to capture fundamental aspects of visual processing. Our research program examines the possibility that existing feed-forward recognition models constitute a first stage leading to the initial activation of category candidates, which is incomplete and often inaccurate. The first stage then triggers the application of class-specific processes, which recover a richer and more accurate interpretation of the visible scene, and reject initial false candidates. The proposal involves three main components: (i) Psychophysics experiments to evaluate the accuracy and speed of how humans extract meaning from images; (ii) Invasive neurophysiological recordings along the human ventral visual cortex to understand the neural circuits involved in extracting meaning from a novel data set of minimal images; (iii) A computational model that integrates bottom-up computations with top-down signals to extract meaning from images. The research efforts will be combined with educational and outreach activities aimed at disseminating the scientific insights and incorporating cutting-edge research into training opportunities for undergraduate and graduate students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gabriel",
   "pi_last_name": "Kreiman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gabriel Kreiman",
   "pi_email_addr": "gabriel.kreiman@tch.harvard.edu",
   "nsf_id": "000488121",
   "pi_start_date": "2017-07-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Children's Hospital Corporation",
  "inst_street_address": "300 LONGWOOD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6179192729",
  "inst_zip_code": "021155724",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "CHILDREN'S HOSPITAL CORPORATION, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "Z1L9F1MM1RY3"
 },
 "perf_inst": {
  "perf_inst_name": "Children's Hospital Corporation",
  "perf_str_addr": "",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155713",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1699",
   "pgm_ref_txt": "COGNEURO"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>There has been notable progress in developing machines that can perform visual pattern recognition tasks (e.g. face recognition for smart phones, automatic analysis of clinical images, object labeling in a scene to guide self-driving cars, among many others). Despite these advances, scene understanding remains a daunting task and humans outperform even the best computer vision approaches in most visual tasks. To build the next generation of Artificial Intelligence vision systems, we need to better understand the neural and cognitive processes that involve image interpretation and jointly develop computational models that are inspired and constrained by neuroscience. In this project, we examined a critical aspect of image interpretation, namely, what are the minimal elements that can lead to recognition? We studied the minimal components in space (how much can an image be reduced while still being intelligible) and also the minimal components in time (what are the minimal temporal cues that can be provided to recognize actions). We combined behavioral experiments, invasive neurophysiological recordings, and computational models. We show that visual recognition of objects and actions can be achieved by efficiently combining spatial and motion cues in configurations where each source on its own is insufficient for recognition. We identifyed minimal videos: these are short and tiny video clips in which objects, parts, and actions can be reliably recognized, but any reduction in either space or time makes them unrecognizable. Human recognition in minimal videos is invariably accompanied by full interpretation of the internal components of the video. We further recorded neural data in response to minimal images and our initial findings found specific neural circuits that mimic the behavioral measurements and provide insights about the top-down mechanisms involved in recognition. State-of-the-art computer vision systems failed to account for the behavioral and physiological observations, further emphasizing the current gap between machine and human vision. These observations prompted us to begin to develop novel computational approaches that can dynamically integrate top-down signals for scene interpretation. This line of work is highly interdisciplinary and bridges neuroscience, cognitive science, and cognitive science, leading us to train new students that can readily understand and move between these fields. On a longer timescale, we expect that the synergisms between neural science and computer science will help build a new generation of intelligent machines.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/21/2019<br>\n\t\t\t\t\tModified by: Gabriel&nbsp;Kreiman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThere has been notable progress in developing machines that can perform visual pattern recognition tasks (e.g. face recognition for smart phones, automatic analysis of clinical images, object labeling in a scene to guide self-driving cars, among many others). Despite these advances, scene understanding remains a daunting task and humans outperform even the best computer vision approaches in most visual tasks. To build the next generation of Artificial Intelligence vision systems, we need to better understand the neural and cognitive processes that involve image interpretation and jointly develop computational models that are inspired and constrained by neuroscience. In this project, we examined a critical aspect of image interpretation, namely, what are the minimal elements that can lead to recognition? We studied the minimal components in space (how much can an image be reduced while still being intelligible) and also the minimal components in time (what are the minimal temporal cues that can be provided to recognize actions). We combined behavioral experiments, invasive neurophysiological recordings, and computational models. We show that visual recognition of objects and actions can be achieved by efficiently combining spatial and motion cues in configurations where each source on its own is insufficient for recognition. We identifyed minimal videos: these are short and tiny video clips in which objects, parts, and actions can be reliably recognized, but any reduction in either space or time makes them unrecognizable. Human recognition in minimal videos is invariably accompanied by full interpretation of the internal components of the video. We further recorded neural data in response to minimal images and our initial findings found specific neural circuits that mimic the behavioral measurements and provide insights about the top-down mechanisms involved in recognition. State-of-the-art computer vision systems failed to account for the behavioral and physiological observations, further emphasizing the current gap between machine and human vision. These observations prompted us to begin to develop novel computational approaches that can dynamically integrate top-down signals for scene interpretation. This line of work is highly interdisciplinary and bridges neuroscience, cognitive science, and cognitive science, leading us to train new students that can readily understand and move between these fields. On a longer timescale, we expect that the synergisms between neural science and computer science will help build a new generation of intelligent machines.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/21/2019\n\n\t\t\t\t\tSubmitted by: Gabriel Kreiman"
 }
}