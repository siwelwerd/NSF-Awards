{
 "awd_id": "1738492",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase II:  Large-Scale Behavioral Analysis Utilizing Convolutional Neural Networks and Its Application to In-store Retail Marketing",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2017-09-15",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 1101330.0,
 "awd_min_amd_letter_date": "2017-09-14",
 "awd_max_amd_letter_date": "2021-10-06",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be to enhance infrastructure for research and education by exposing a previously unavailable dataset: fine-grained human interaction with a physical environment. Humans are always building on and shaping the world but there is little hard data to further examine the effects this has. Beyond retail, this technology could affect how teachers layout classrooms, how disaster workers provide relief, or how factories keep their workers safe. The subtle physical details that affect humans everyday will be understood and investigated in ways not possible without the proposed system. This technology will benefit society by increasing economic efficiency because retailers can meet customer needs more easily. This translates into large potential commercial value because of the size of the retail market and because retailers are under pressure to deliver differentiated customer experiences that customers cannot get online or at big-box stores. Such experiences are enabled by understanding the customer at a much deeper level which in turn is enabled by the technology proposed here.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase II project uses sensor placement models, statistical methods, and face recognition to fully realize the commercial potential resulting from the Phase I development of a video analytics system for understanding human behavior. Research in video-based behavior recognition has seen renewed excitement because of deep learning but current work only addresses pieces of the problem. Critical missing components are robust face identification and registration, a process to install as few cameras as possible while maximizing the viewable area of a store, and video analysis results that are meaningful when combined with other data such as retail transactions. The research objectives of extracting biometric data (such as facial features) from video, automatically computing the optimal positions of cameras, and correlating behavior metrics with business operations are essential to improve the retail experience for shoppers and make the developments undertaken in Phase I more commercially relevant to potential end users of the technology.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Everett",
   "pi_last_name": "Berry",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Everett Berry",
   "pi_email_addr": "everett@perceiveinc.com",
   "nsf_id": "000713539",
   "pi_start_date": "2017-09-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Perceive, Inc.",
  "inst_street_address": "9059 TECHNOLOGY LN",
  "inst_street_address_2": "",
  "inst_city_name": "FISHERS",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654308561",
  "inst_zip_code": "460382828",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "IN05",
  "org_lgl_bus_name": "PERCEIVE, INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "KTPGSKMJ7MX6"
 },
 "perf_inst": {
  "perf_inst_name": "Perceive, Inc.",
  "perf_str_addr": "320 North St.",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479066503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "165E",
   "pgm_ref_txt": "SBIR Phase IIB"
  },
  {
   "pgm_ref_code": "169E",
   "pgm_ref_txt": "SBIR Tech Enhan Partner (TECP)"
  },
  {
   "pgm_ref_code": "5373",
   "pgm_ref_txt": "SMALL BUSINESS PHASE II"
  },
  {
   "pgm_ref_code": "7453",
   "pgm_ref_txt": "GRAPHICS & VISUALIZATION"
  },
  {
   "pgm_ref_code": "8032",
   "pgm_ref_txt": "Software Services and Applications"
  },
  {
   "pgm_ref_code": "8240",
   "pgm_ref_txt": "SBIR/STTR CAP"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 750000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 149999.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 10000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 191331.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-76f2d509-7fff-b3de-0c96-ad17c00d9ad7\"> </span></p>\n<p dir=\"ltr\"><span>Perceive has completed all of its Phase II SBIR objectives, all but one of its TECP objectives, and nearly all of its Phase IIB objectives. and has built a robust computer vision system capable of tracking and identifying human behavior. The system uses open source easy-install camera sensors to build a 3D map, a data processing infrastructure to convert the raw video into analytics, and the most accurate tracking algorithm available today according to benchmarks.. The team conducted research into convolutional neural networks, 3D scene reconstruction and mesh networks and developed engineering solutions for a scalable video processing pipeline and simple hardware installation.</span></p>\n<p dir=\"ltr\"><span>In recent years, convolutional neural networks have been extraordinarily successful on single image tasks such as classification and object detection. However, results on video sequences have been mixed and researchers have yet to settle on an architecture which works well for classifying events that take place over time. For example, it is easy with modern networks to detect the number of people in a single image but difficult to detect actions such as ?people shopping? or ?people walking?. A major goal of this grant was to research the possibilities of convolutional networks for action recognition with an idea that they would be commercially useful in retail environments where shoppers, retail associates, and merchandise all interact with to conduct commerce. Many months of investigation revealed that although convolutional networks for action recognition work in some limited cases, they are far from being general purpose or performant enough to be useful for retail video. Instead other technologies were developed which address the challenge of action recognition through a multi-stage pipeline.</span></p>\n<p dir=\"ltr\"><span>Other technologies are important for the commercial development of this system. First, it is not enough to know what is happening: it is also important to know where it is happening. To this end Perceive investigated 3D multi-view scene reconstruction algorithms which could combine the inputs of multiple camera views to create one 3D model of an environment. A key challenge is to require as little configuration of the cameras as possible so that the system can be installed effortlessly. In the course of this grant Perceive has leveraged its knowledge of 3D geometry for computer vision and the specific constraints of the retail environment to develop calibration, point matching, and pixel error correction algorithms for this challenge. A second important technology is camera hardware which is cost-effective to install. Perceive designed and deployed a proprietary power solution which does not require any technicians to install. Further, the software can now run on regular security cameras for larger impact.</span></p>\n<p dir=\"ltr\"><span>Perceive believes that systems which extend into the physical world from the digital world are just beginning to be created. These systems will augment human abilities and enable service personel, factory workers, and others to be more efficient and more effective. The technologies developed, including sensors, algorithms, and software infrastructure, represent key pieces of this future and by commercializing them Perceive endeavors to drive their adoption in society and advance human potential. The code and data for this work is available at </span><a href=\"http://prcvlabs.org\"><span>http://prcvlabs.org</span></a><span>.&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/22/2021<br>\n\t\t\t\t\tModified by: Everett&nbsp;Berry</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nPerceive has completed all of its Phase II SBIR objectives, all but one of its TECP objectives, and nearly all of its Phase IIB objectives. and has built a robust computer vision system capable of tracking and identifying human behavior. The system uses open source easy-install camera sensors to build a 3D map, a data processing infrastructure to convert the raw video into analytics, and the most accurate tracking algorithm available today according to benchmarks.. The team conducted research into convolutional neural networks, 3D scene reconstruction and mesh networks and developed engineering solutions for a scalable video processing pipeline and simple hardware installation.\nIn recent years, convolutional neural networks have been extraordinarily successful on single image tasks such as classification and object detection. However, results on video sequences have been mixed and researchers have yet to settle on an architecture which works well for classifying events that take place over time. For example, it is easy with modern networks to detect the number of people in a single image but difficult to detect actions such as ?people shopping? or ?people walking?. A major goal of this grant was to research the possibilities of convolutional networks for action recognition with an idea that they would be commercially useful in retail environments where shoppers, retail associates, and merchandise all interact with to conduct commerce. Many months of investigation revealed that although convolutional networks for action recognition work in some limited cases, they are far from being general purpose or performant enough to be useful for retail video. Instead other technologies were developed which address the challenge of action recognition through a multi-stage pipeline.\nOther technologies are important for the commercial development of this system. First, it is not enough to know what is happening: it is also important to know where it is happening. To this end Perceive investigated 3D multi-view scene reconstruction algorithms which could combine the inputs of multiple camera views to create one 3D model of an environment. A key challenge is to require as little configuration of the cameras as possible so that the system can be installed effortlessly. In the course of this grant Perceive has leveraged its knowledge of 3D geometry for computer vision and the specific constraints of the retail environment to develop calibration, point matching, and pixel error correction algorithms for this challenge. A second important technology is camera hardware which is cost-effective to install. Perceive designed and deployed a proprietary power solution which does not require any technicians to install. Further, the software can now run on regular security cameras for larger impact.\nPerceive believes that systems which extend into the physical world from the digital world are just beginning to be created. These systems will augment human abilities and enable service personel, factory workers, and others to be more efficient and more effective. The technologies developed, including sensors, algorithms, and software infrastructure, represent key pieces of this future and by commercializing them Perceive endeavors to drive their adoption in society and advance human potential. The code and data for this work is available at http://prcvlabs.org. \n\n \n\n\t\t\t\t\tLast Modified: 09/22/2021\n\n\t\t\t\t\tSubmitted by: Everett Berry"
 }
}