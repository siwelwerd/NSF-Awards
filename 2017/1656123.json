{
 "awd_id": "1656123",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Shrinkage for Vector Autoregressions and Impulse Response Estimation",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032927280",
 "po_email": "nlutz@nsf.gov",
 "po_sign_block_name": "Nancy Lutz",
 "awd_eff_date": "2017-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 236385.0,
 "awd_amount": 236385.0,
 "awd_min_amd_letter_date": "2017-06-20",
 "awd_max_amd_letter_date": "2022-06-21",
 "awd_abstract_narration": "The core of Macroeconomic policy analysis is understanding the impact of unexpected events, news, and variation on core economic variables including GDP, inflation, wages, investment, and employment. Applied macroeconomic research focuses on estimation of these impacts known as \"impulse response functions\". Current estimation methods are less precise than desirable, and are difficult to implement with a large number of variables. This project develops new methods which produce sharper and more precise estimates of these effects, allowing for more precise understanding of the macro economy and economic policy. The methods are based on combination (or ensembles) of simpler methods. The new method can be much more precise than existing simpler methods.\r\n\r\nThis project explores impulse response function (IRF) estimation in vector auto-regressions (VARs) by model combination. Estimates from lower-dimensional models (VARs and ARs of lower order) will be combined by standard model averaging methods. The IRF is a non-linear function of the VAR coefficients. The investigator develops a large-sample (asymptotic) approximation to the distribution of the combination IRF. Using this asymptotic approximation, this research calculates the approximate mean-squared error (MSE) of the combination IRF, and shows how to estimate the MSE using an appropriate information criterion which is similar to a Mallows criterion or Focused information criterion. The combination weights can then be selected to minimize this criterion function, resulting in a practical combination estimator. The goal of the project is to study the statistical properties of this combination method and extend its application to high dimensional contexts.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bruce",
   "pi_last_name": "Hansen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bruce Hansen",
   "pi_email_addr": "behansen@wisc.edu",
   "nsf_id": "000448818",
   "pi_start_date": "2017-06-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "1180 Observatory Drive",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061393",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "132000",
   "pgm_ele_name": "Economics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1320",
   "pgm_ref_txt": "ECONOMICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 236385.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Economic data is routinely characterized by complex dependence between observations. Data can be correlated over time (time-series data), or within sampled units, such as schools or states. Econometric estimates require the reporting of measures of precision known as standard errors, which gauge the accuracy of the esitimates. These standard errors need to account for the complex dependence of the sampled data, as the dependence has a great effect on the precision of estimation. This research project has concerned developing improved standard errors-- improved methods of estimation of precision.</p>\n<p>One of the results of this project has been a revision of a classical econometric theoretical result known as the Gauss-Markov Theorem, or the BLUE Theorem. BLUE stands for \"Best Linear Unbiased Estimator\". This is routinely taught to students at all levels. The new discovery is that the classical estimator is actually BUE, for \"Best Unbiased Estimator\", that the restriction to the class of linear estimators is unnecessary. This alters our understanding of core estimation methods and their efficiency.</p>\n<p>Another core result has focused on the precision of least squares estimation under clustered (correlated) dependence. We show that conventional measures of precisions can be dramatically wrong, by providing estimates of precision that are much too small. This can lead to ill-informed decisions. In this project, we show that this problem can be rectified if the measure of precision is modified by using a method called the \"jackknife\", which measures precision by a resampling technique based on sequential omission of correlated clusters of observations. We are able to show that this jackknife technique provides a conservative estimator for the sampling variance, under broader conditions than previously realized, and that confidence intervals constructed using these methods have improved coverage properties. These methods are expected to improve estimation and inference methods in applied econometric practice.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/17/2023<br>\n\t\t\t\t\tModified by: Bruce&nbsp;Hansen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nEconomic data is routinely characterized by complex dependence between observations. Data can be correlated over time (time-series data), or within sampled units, such as schools or states. Econometric estimates require the reporting of measures of precision known as standard errors, which gauge the accuracy of the esitimates. These standard errors need to account for the complex dependence of the sampled data, as the dependence has a great effect on the precision of estimation. This research project has concerned developing improved standard errors-- improved methods of estimation of precision.\n\nOne of the results of this project has been a revision of a classical econometric theoretical result known as the Gauss-Markov Theorem, or the BLUE Theorem. BLUE stands for \"Best Linear Unbiased Estimator\". This is routinely taught to students at all levels. The new discovery is that the classical estimator is actually BUE, for \"Best Unbiased Estimator\", that the restriction to the class of linear estimators is unnecessary. This alters our understanding of core estimation methods and their efficiency.\n\nAnother core result has focused on the precision of least squares estimation under clustered (correlated) dependence. We show that conventional measures of precisions can be dramatically wrong, by providing estimates of precision that are much too small. This can lead to ill-informed decisions. In this project, we show that this problem can be rectified if the measure of precision is modified by using a method called the \"jackknife\", which measures precision by a resampling technique based on sequential omission of correlated clusters of observations. We are able to show that this jackknife technique provides a conservative estimator for the sampling variance, under broader conditions than previously realized, and that confidence intervals constructed using these methods have improved coverage properties. These methods are expected to improve estimation and inference methods in applied econometric practice.\n\n\t\t\t\t\tLast Modified: 09/17/2023\n\n\t\t\t\t\tSubmitted by: Bruce Hansen"
 }
}