{
 "awd_id": "1712839",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:   Higher-Order Asymptotics and Accurate Inference for Post-Selection",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 119995.0,
 "awd_amount": 119995.0,
 "awd_min_amd_letter_date": "2017-08-02",
 "awd_max_amd_letter_date": "2017-08-02",
 "awd_abstract_narration": "Many statistical analyses utilize a model selection procedure. Perhaps the most common model selection problem is that of variable selection in linear regression. Principled motivations for selection include the desire for interpretability, prevention of over-fitting, and concerns about statistical power. A practical motivation arises when the data are high-dimensional, with more explanatory variables than observations. Relevant applications span the entire domain of scientific inquiry, from neuroscience, medicine and physics, to economics, sociology, and psychology. A large catalogue of variable selection procedures is now available, and the statistics community has turned its focus to the question of inference after selection. Standard methods of statistical inference are no longer valid when the same data are used to both select a model and make inferences about that model. It is fundamentally important to have accurate post-selection inference procedures that are also powerful enough to detect observed departures from scientific hypotheses and that avoid the strong distributional assumptions needed for exact inference in finite samples. This research aims to develop post-selection inference methodology that is both accurate and powerful, with particular emphasis on reducing statistical errors that depend on the sample size. \r\n\r\nThe goal of this project is to further understanding of the asymptotic theory of post-selection inference, particularly selective inference based on the CovTest and truncated Gaussian (TG) statistic, as well as simultaneous inference using the post-selection intervals (PoSI) procedure. The first two procedures can be motivated by selective error control, i.e., error control for the selected model parameters. The PoSI method seeks to control family-wise error rates for all possible sub-model parameters. While these procedures yield valid post-selection inference, without strong assumptions they are particularly vulnerable to the effects of violation of key assumptions in the realistic setting of small to moderate sample sizes, such as overly-conservative or inaccurate confidence intervals, and low power. In this project, asymptotic expansions, saddle-point approximations, the bootstrap, and related techniques from higher-order asymptotics will be employed to improve accuracy and power for these post-selection inference procedures.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Kolassa",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "John E Kolassa",
   "pi_email_addr": "kolassa@stat.rutgers.edu",
   "nsf_id": "000313097",
   "pi_start_date": "2017-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "110 Frelinghuysen Rd",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548019",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 119995.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<div>This project produced publications, software, and presentations facilitating statistical inference in the presence of model selection.&nbsp; All publications listed below were supported by this grant.</div>\n<div>\n<div>Intellectual merit:&nbsp; A common paradigm for determining whether a potential causal factor is associated with an effect as reflected in a set of experimental data is to calculate to what extent the data observed would be likely under the hypothesis of no link between the potential causal factor and a response, or, alternatively, to report a range of assessments of the relationship between these factors consistent with the data set.&nbsp; The measure of how likely the data are under the hypothesis of no link is called a p-value, and the range of plausible association measures is called a confidence interval.</div>\n<div>Both of these measures (the p-value and the confidence interval) have probabilistic interpretations, and if multiple causal factors, or multiple responses, are considered, and the most striking results are reported, the probabilistic interpretations of these results are undermined.&nbsp; The process of making inference from data after selecting the most striking results is called post-selection inference.</div>\n<div>This project investigates inferential techniques that can be applied to post-selection inference.&nbsp; The approaches typically involve probability calculations much more delicate than those involved in inferential techniques that do not involve model selection.&nbsp; Such tools are reviewed by Kuchibhotla, Kolassa, and Kuffner (2021), a manuscript accepted in Annual Reviews.&nbsp; Cohen, Kolassa, and Sackrowithz (2018), published in Biometrical Journal, provide a penalized likelihood approach to this problem.</div>\n<div>Broader Impact:&nbsp; The question of multiplicity of inference has lead to the unreliability of scientific research using conventional tools without adjusting for post-selection inference, as recounted by Kuchibhotla, Kolassa, and Kuffner (2021).&nbsp; The p-value has come under particular scrutiny in this respect.&nbsp; Zawada, Kolassa, and Seifu (2019) and Altan, Amaratunga, Cabrera, Garren, Geyes, Kolassa, LeBlond, Li, Kiao, Liu, Lubomirski, Miro-Quesada, Novick, Peterson, Otava, Reckermann, Schofield, Tan, Tatikola, Tekle, Thomas, and Vokovinsky (2021) address the reliability of the p-value as a tool for statistical inference. Kolassa and Kuffner (2020), in the Annals of Statistics, explore approximation tools needed for the application of Bayesian methods, which provide an alternative approach to solving the post-selection inference tools.&nbsp; Kolassa, Chen, Seifu, and Zhong (2021), submitted to a proceedings volume, address the related problem of accurate inference experimental designs in which the multiplicity is generated, not by multiple causal factors or responses, but by multiple looks at the data in a multi-stage testing design.&nbsp; &nbsp; Zhang and Kolassa (2021), submitted, address a computational problem that arises when large numbers of factors are controlled for in certain models applicable to measuring effects of factors on lifetimes, and commonly used in cancer research.</div>\n</div>\n<div>In order to make an impact in the practice of science, tools for applying methods described here must be readily available to the public.&nbsp;</div>\n<div>Various R packages were constructed to implement these approaches.&nbsp; MultNonParam, hosted on the standard R repository CRAN, provides code to perform some of the analyses described in&nbsp;Kuchibhotla, Kolassa, and Kuffner (2021).&nbsp; PHInfiniteEstimates, also hosted on CRAN, provides calculations as described in&nbsp;Zhang and Kolassa (2021).&nbsp; Packages bivcornish and twostage, hosted on github, jointly provide the analysis of&nbsp;Kolassa, Chen, Seifu, and Zhong (2021).&nbsp; &nbsp;Package ChangePoint, also hosted on github, provides the analysis of&nbsp;Cohen, Kolassa, and Sackrowithz (2018).</div>\n<div>These theoretical, methodological, and computational advances in post-selection inference support replicable scientific research in that they properly account for experimental error, leading to more reliable and trusted scientific results.</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/05/2021<br>\n\t\t\t\t\tModified by: John&nbsp;E&nbsp;Kolassa</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis project produced publications, software, and presentations facilitating statistical inference in the presence of model selection.  All publications listed below were supported by this grant.\n\nIntellectual merit:  A common paradigm for determining whether a potential causal factor is associated with an effect as reflected in a set of experimental data is to calculate to what extent the data observed would be likely under the hypothesis of no link between the potential causal factor and a response, or, alternatively, to report a range of assessments of the relationship between these factors consistent with the data set.  The measure of how likely the data are under the hypothesis of no link is called a p-value, and the range of plausible association measures is called a confidence interval.\nBoth of these measures (the p-value and the confidence interval) have probabilistic interpretations, and if multiple causal factors, or multiple responses, are considered, and the most striking results are reported, the probabilistic interpretations of these results are undermined.  The process of making inference from data after selecting the most striking results is called post-selection inference.\nThis project investigates inferential techniques that can be applied to post-selection inference.  The approaches typically involve probability calculations much more delicate than those involved in inferential techniques that do not involve model selection.  Such tools are reviewed by Kuchibhotla, Kolassa, and Kuffner (2021), a manuscript accepted in Annual Reviews.  Cohen, Kolassa, and Sackrowithz (2018), published in Biometrical Journal, provide a penalized likelihood approach to this problem.\nBroader Impact:  The question of multiplicity of inference has lead to the unreliability of scientific research using conventional tools without adjusting for post-selection inference, as recounted by Kuchibhotla, Kolassa, and Kuffner (2021).  The p-value has come under particular scrutiny in this respect.  Zawada, Kolassa, and Seifu (2019) and Altan, Amaratunga, Cabrera, Garren, Geyes, Kolassa, LeBlond, Li, Kiao, Liu, Lubomirski, Miro-Quesada, Novick, Peterson, Otava, Reckermann, Schofield, Tan, Tatikola, Tekle, Thomas, and Vokovinsky (2021) address the reliability of the p-value as a tool for statistical inference. Kolassa and Kuffner (2020), in the Annals of Statistics, explore approximation tools needed for the application of Bayesian methods, which provide an alternative approach to solving the post-selection inference tools.  Kolassa, Chen, Seifu, and Zhong (2021), submitted to a proceedings volume, address the related problem of accurate inference experimental designs in which the multiplicity is generated, not by multiple causal factors or responses, but by multiple looks at the data in a multi-stage testing design.    Zhang and Kolassa (2021), submitted, address a computational problem that arises when large numbers of factors are controlled for in certain models applicable to measuring effects of factors on lifetimes, and commonly used in cancer research.\n\nIn order to make an impact in the practice of science, tools for applying methods described here must be readily available to the public. \nVarious R packages were constructed to implement these approaches.  MultNonParam, hosted on the standard R repository CRAN, provides code to perform some of the analyses described in Kuchibhotla, Kolassa, and Kuffner (2021).  PHInfiniteEstimates, also hosted on CRAN, provides calculations as described in Zhang and Kolassa (2021).  Packages bivcornish and twostage, hosted on github, jointly provide the analysis of Kolassa, Chen, Seifu, and Zhong (2021).   Package ChangePoint, also hosted on github, provides the analysis of Cohen, Kolassa, and Sackrowithz (2018).\nThese theoretical, methodological, and computational advances in post-selection inference support replicable scientific research in that they properly account for experimental error, leading to more reliable and trusted scientific results.\n\n \n\n\t\t\t\t\tLast Modified: 10/05/2021\n\n\t\t\t\t\tSubmitted by: John E Kolassa"
 }
}