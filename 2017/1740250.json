{
 "awd_id": "1740250",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SI2:SSE: MAtrix, TEnsor, and Deep-Learning Optimized Routines (MATEDOR)",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Seung-Jong Park",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2017-08-29",
 "awd_max_amd_letter_date": "2018-09-25",
 "awd_abstract_narration": "A number of scientific software applications from important fields, including applications in deep learning, data mining, astrophysics, image and signal processing, hydrodynamics, and more, do many computations on small matrices (also known as \"tensors\") and using widely available standard linear-algebra software libraries. Scientists are trying to make these applications run faster by running them on advanced high performance computing (HPC) systems, that are heterogeneous systems that use processors of many different types, such as \"accelerators\" - that use of specialized computer hardware to perform some functions more efficiently than standard, general-purpose processors - and \"co-processors\" - that can run certain specialized functions in parallel with the central processor. However, standard linear algebra software libraries cannot make use of these specialized hardware components, and so the scientific applications mentioned above do not become much faster. Many existing linear algebra libraries, including libraries supplied by commercial vendors of computing technology have been tried to no avail. This issue is now critical because advancements in science from important fields are being held back due to the lack of progress in speeding up software. This project will address this through research and development that will create efficient software that can repetitively execute tensor operations grouped together in \"batches\" and which can be written to run very efficiently and quickly on the types of hardware components that exist in HPC systems. In addition to the research and development, several students will be engaged in the project, thus helping develop a critically needed component of the U.S. workforce.\r\n\r\nThe trend in high performance computing (HPC) toward large-scale, heterogeneous systems with GPU accelerators and coprocessors has made the near total absence of linear algebra software for small matrix or tensor operations especially noticeable. Given the fundamental importance of numerical libraries to science and engineering applications of all types, the need for libraries that can perform batched operations on small matrices or tensors has become acute. This MAtrix, TEnsor, and Deep-learning Optimized Routines (MATEDOR) project seeks to provide a solution to this problem by developing a sustainable and portable library for such small computations. Future releases of MATEDOR are expected to have a significant impact on application areas that use small matrices and tensors and need to exploit the power of advanced computing architectures. Such application areas include deep-learning, data mining, metabolic networks, computational fluid dynamics, direct and multi-frontal solvers, image and signal processing, and many more. This team has a proven record of providing software infrastructure that is widely adopted and used, that supports ongoing community contributions, and that becomes incorporated in vendor libraries (e.g., Intel's MKL and NVIDIA's CUBLAS) and other software tools and frameworks (e.g., MATLAB and R). Students will be regularly integrated into the project activities, and this group of PIs has an exceptionally strong record of community outreach, having given numerous performance optimization and software tutorials at conferences and Users Group meetings.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stanimire",
   "pi_last_name": "Tomov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stanimire Tomov",
   "pi_email_addr": "tomov@icl.utk.edu",
   "nsf_id": "000492784",
   "pi_start_date": "2018-09-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Azzam",
   "pi_last_name": "Haidar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Azzam Haidar",
   "pi_email_addr": "haidar@icl.utk.edu",
   "nsf_id": "000715989",
   "pi_start_date": "2017-08-29",
   "pi_end_date": "2018-09-25"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Stanimire",
   "pi_last_name": "Tomov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stanimire Tomov",
   "pi_email_addr": "tomov@icl.utk.edu",
   "nsf_id": "000492784",
   "pi_start_date": "2017-08-29",
   "pi_end_date": "2018-09-25"
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Knoxville",
  "inst_street_address": "201 ANDY HOLT TOWER",
  "inst_street_address_2": "",
  "inst_city_name": "KNOXVILLE",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "8659743466",
  "inst_zip_code": "379960001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "TN02",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "LXG4F9K8YZK5",
  "org_uei_num": "FN2YCS2YAUW3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Knoxville",
  "perf_str_addr": "1 Circle Park",
  "perf_city_name": "Knoxville",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "379960003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "TN02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  },
  {
   "pgm_ref_code": "8005",
   "pgm_ref_txt": "Scientific Software Elements"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span style=\"font-size: 10.5pt; font-family: Arial; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The MAtrix, TEnsor, and Deep-learning Optimized Routines (MATEDOR) project provided software technologies and standard Applications Programming Interfaces (APIs), along with a sustainable and portable library for large-scale computations whose individual components rely on very small matrix or tensor computations. The main target was the acceleration of applications from important fields that fit this profile, including deep learning, data mining, astrophysics, image and signal processing, hydrodynamics, and more. Attempts to make these applications run faster by using widely available standard linear-algebra software libraries on advanced high-performance computing (HPC) systems had not been successful before, using the classical approaches. The performance gains using the MATEDOR approach often yielded more than a 10x speedup over the current classical approaches. The developments were released through the Matrix for GPUs and Multicore Architectures (</span><a style=\"text-decoration: none;\" href=\"https://icl.utk.edu/magma/\"><span style=\"font-size: 10.5pt; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;\">MAGMA</span></a><span style=\"font-size: 10.5pt; font-family: Arial; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">) library which had a significant impact on application areas that use small matrices and tensors and that needed to exploit the power of advanced computing architectures with GPUs and coprocessors.</span></p>\n<p><span style=\"font-size: 10.5pt; font-family: Arial; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The linear algebra (LA) operations on small matrices or tensors were established in MATEDOR as \"batched\" operations, and now are widely known as batched LA operations in the community (Figure 1). Working closely with interested application developers, MATEDOR defined modular, language agnostic interfaces that could be implemented so as to work seamlessly with the compiler and be optimizable using techniques such as code replacement and inlining. This provided the application developers, compilers, and runtime systems with the option of launching batched workloads using a single call according to the standard interface. This allowed the entire LA community to collectively address a wide range of small matrix or tensor problems, often accelerating codes using batched interfaces more than tenfold. Success in such an effort was possible through innovations in the interface design, computational and numerical optimizations, as well as packaging and deployment at the user end to trigger final stages of tuning at the moment of execution. </span></p>\n<p><span style=\"font-size: 10.5pt; font-family: Arial; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To standardize the API for batched routines, </span><a style=\"text-decoration: none;\" href=\"https://dl.acm.org/doi/pdf/10.1145/3431921\"><span style=\"font-size: 10.5pt; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;\">MATEDOR proposed an API</span></a><span style=\"font-size: 10.5pt; font-family: Arial; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> that is very similar to the standard BLAS and LAPACK API. The definition of the new API for Batched BLAS and LAPACK routines was a result of a community effort and discussions organized through two workshops on Batched, Reproducible, and Reduced Precision BLAS and two SIAM minisymposia (Figure 2).</span></p>\n<p><span style=\"font-size: 10.5pt; font-family: Arial; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The power of the MATEDOR interface was demonstrated by delivering a high-performance numerical library for batched LA for the modern processor architectures. The MATEDOR library includes BLAS and LAPACK for many small dense problems, tensor, and application-specific operations (e.g, for deep-learning). The batched LAPACK routines are constructed as much as possible out of calls to batched BLAS and their look-alikes required in sparse computation context. The MATEDOR batched LA standard and software infrastructure are widely adopted and used, support ongoing community contributions, and get incorporated into vendor libraries (e.g., Intel's MKL, NVIDIA's CUBLAS, and AMD's ROCm Math libraries) and into other software tools and frameworks (e.g., MATLAB and R). Vendor math libraries from Nvidia, AMD, and Intel continue to add more Batched BLAS and LAPACK functionalities in their math libraries. Currently, the MAGMA library provides the most complete set of highly optimized Batched BLAS and LAPACK. MAGMA releases of batched LA routines had a significant and immediate impact on accelerating applications areas in deep-learning, data mining, metabolic networks, CFD, direct and multifrontal solvers, image and signal processing, and many more (Figure 3). MAGMA has more than 40K downloads per year from the ICL servers.</span></p>\n<p><span style=\"font-size: 10.5pt; font-family: Arial; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">MATEDOR is application-motivated and designed to impact application areas from deep-learning, to data mining, CFD, solvers, image and signal processing, and others that need small matrix/tensor computations. Along with regular integration of students into the research activities, MATEDOR achieved an exceptionally strong record of community outreach through numerous performance optimization and software tutorials at conferences and Users Group meetings. MATEDOR was used in training undergraduate students through NSF Summer Research Experiences for Undergraduates (REU) programs at UTK and in extending the educational curriculum/instructional material in undergraduate and graduate courses for advancing fundamental research. MATEDOR was used in the Linear Algebra Preparation for Emergent Neural Network Architectures (LAPENNA) NSF CyberTraining Program at UTK to deliver algorithmic and computational techniques, numerical and programming procedures, and AI software. This resulted in the development of </span><a style=\"text-decoration: none;\" href=\"https://bitbucket.org/icl/magmadnn/\"><span style=\"font-size: 10.5pt; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;\">MagmaDNN</span></a><span style=\"font-size: 10.5pt; font-family: Arial; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, an open-source HPC deep learning framework that used the Batched LA capabilities of MAGMA as a computational backend to accelerate AI applications.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/24/2023<br>\n\t\t\t\t\tModified by: Stanimire&nbsp;Tomov</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1740250/1740250_10519184_1677275014520_Fig1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1740250/1740250_10519184_1677275014520_Fig1--rgov-800width.jpg\" title=\"Batched Linear Algebra\"><img src=\"/por/images/Reports/POR/2023/1740250/1740250_10519184_1677275014520_Fig1--rgov-66x44.jpg\" alt=\"Batched Linear Algebra\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Defining batched workloads using a single call according to a standard interface (Left) and software technologies for developing high-performance numerical library for batched liner algebra computations (Right).</div>\n<div class=\"imageCredit\">MATEDOR Poster NSF CSSI PI Meeting 2020</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Stanimire&nbsp;Tomov</div>\n<div class=\"imageTitle\">Batched Linear Algebra</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1740250/1740250_10519184_1677275313564_Fig2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1740250/1740250_10519184_1677275313564_Fig2--rgov-800width.jpg\" title=\"Community Effort Towards Batched BLAS Standardization\"><img src=\"/por/images/Reports/POR/2023/1740250/1740250_10519184_1677275313564_Fig2--rgov-66x44.jpg\" alt=\"Community Effort Towards Batched BLAS Standardization\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Community Effort and Activities Towards Standardization of Batched Linear Algebra</div>\n<div class=\"imageCredit\">MATEDOR Poster NSF CSSI PI Meeting 2020</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Stanimire&nbsp;Tomov</div>\n<div class=\"imageTitle\">Community Effort Towards Batched BLAS Standardization</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1740250/1740250_10519184_1677275638295_Fig3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1740250/1740250_10519184_1677275638295_Fig3--rgov-800width.jpg\" title=\"Batched Linear Algebrain Applications\"><img src=\"/por/images/Reports/POR/2023/1740250/1740250_10519184_1677275638295_Fig3--rgov-66x44.jpg\" alt=\"Batched Linear Algebrain Applications\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Breadth of MATEDOR's Batched linear algebra impact on application domain.</div>\n<div class=\"imageCredit\">MATEDOR Poster NSF CSSI PI Meeting 2020</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Stanimire&nbsp;Tomov</div>\n<div class=\"imageTitle\">Batched Linear Algebrain Applications</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe MAtrix, TEnsor, and Deep-learning Optimized Routines (MATEDOR) project provided software technologies and standard Applications Programming Interfaces (APIs), along with a sustainable and portable library for large-scale computations whose individual components rely on very small matrix or tensor computations. The main target was the acceleration of applications from important fields that fit this profile, including deep learning, data mining, astrophysics, image and signal processing, hydrodynamics, and more. Attempts to make these applications run faster by using widely available standard linear-algebra software libraries on advanced high-performance computing (HPC) systems had not been successful before, using the classical approaches. The performance gains using the MATEDOR approach often yielded more than a 10x speedup over the current classical approaches. The developments were released through the Matrix for GPUs and Multicore Architectures (MAGMA) library which had a significant impact on application areas that use small matrices and tensors and that needed to exploit the power of advanced computing architectures with GPUs and coprocessors.\n\nThe linear algebra (LA) operations on small matrices or tensors were established in MATEDOR as \"batched\" operations, and now are widely known as batched LA operations in the community (Figure 1). Working closely with interested application developers, MATEDOR defined modular, language agnostic interfaces that could be implemented so as to work seamlessly with the compiler and be optimizable using techniques such as code replacement and inlining. This provided the application developers, compilers, and runtime systems with the option of launching batched workloads using a single call according to the standard interface. This allowed the entire LA community to collectively address a wide range of small matrix or tensor problems, often accelerating codes using batched interfaces more than tenfold. Success in such an effort was possible through innovations in the interface design, computational and numerical optimizations, as well as packaging and deployment at the user end to trigger final stages of tuning at the moment of execution. \n\nTo standardize the API for batched routines, MATEDOR proposed an API that is very similar to the standard BLAS and LAPACK API. The definition of the new API for Batched BLAS and LAPACK routines was a result of a community effort and discussions organized through two workshops on Batched, Reproducible, and Reduced Precision BLAS and two SIAM minisymposia (Figure 2).\n\nThe power of the MATEDOR interface was demonstrated by delivering a high-performance numerical library for batched LA for the modern processor architectures. The MATEDOR library includes BLAS and LAPACK for many small dense problems, tensor, and application-specific operations (e.g, for deep-learning). The batched LAPACK routines are constructed as much as possible out of calls to batched BLAS and their look-alikes required in sparse computation context. The MATEDOR batched LA standard and software infrastructure are widely adopted and used, support ongoing community contributions, and get incorporated into vendor libraries (e.g., Intel's MKL, NVIDIA's CUBLAS, and AMD's ROCm Math libraries) and into other software tools and frameworks (e.g., MATLAB and R). Vendor math libraries from Nvidia, AMD, and Intel continue to add more Batched BLAS and LAPACK functionalities in their math libraries. Currently, the MAGMA library provides the most complete set of highly optimized Batched BLAS and LAPACK. MAGMA releases of batched LA routines had a significant and immediate impact on accelerating applications areas in deep-learning, data mining, metabolic networks, CFD, direct and multifrontal solvers, image and signal processing, and many more (Figure 3). MAGMA has more than 40K downloads per year from the ICL servers.\n\nMATEDOR is application-motivated and designed to impact application areas from deep-learning, to data mining, CFD, solvers, image and signal processing, and others that need small matrix/tensor computations. Along with regular integration of students into the research activities, MATEDOR achieved an exceptionally strong record of community outreach through numerous performance optimization and software tutorials at conferences and Users Group meetings. MATEDOR was used in training undergraduate students through NSF Summer Research Experiences for Undergraduates (REU) programs at UTK and in extending the educational curriculum/instructional material in undergraduate and graduate courses for advancing fundamental research. MATEDOR was used in the Linear Algebra Preparation for Emergent Neural Network Architectures (LAPENNA) NSF CyberTraining Program at UTK to deliver algorithmic and computational techniques, numerical and programming procedures, and AI software. This resulted in the development of MagmaDNN, an open-source HPC deep learning framework that used the Batched LA capabilities of MAGMA as a computational backend to accelerate AI applications.\n\n\t\t\t\t\tLast Modified: 02/24/2023\n\n\t\t\t\t\tSubmitted by: Stanimire Tomov"
 }
}