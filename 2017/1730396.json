{
 "awd_id": "1730396",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRI: II-New: An Infrastructure of Display Devices to Study Visual Analytics Beyond the Desktop",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922568",
 "po_email": "wnilsen@nsf.gov",
 "po_sign_block_name": "Wendy Nilsen",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 273552.0,
 "awd_amount": 273552.0,
 "awd_min_amd_letter_date": "2017-05-10",
 "awd_max_amd_letter_date": "2017-05-10",
 "awd_abstract_narration": "Visual Analytics studies how to help people make sense of complex data by providing interactive visual interfaces for data analysis and presentation. By making data visible and interactive, visual analytics enables scientists and researchers to detect patterns, formulate novel hypotheses and derive new insights on relevant scientific, social, and economic issues. Existing methods and technologies however focus exclusively on standard desktop interfaces and, as such, do not take advantage of the remarkable opportunities non-traditional display devices such as large screen displays, touch tables, and stereoscopic displays, can bring. In this project the research team explores these opportunities by developing methods for collaborative visual data analysis; real-time data monitoring in mission-critical settings; and immersive virtual navigation of \"data spaces\".\r\n\r\nThe research the PI carries out includes: (1) user studies to understand how people solve data analysis problems in collaborative, data monitoring and immersive settings; (2) development of novel interaction techniques and applications for touch and 3D interfaces; (3) advanced evaluation methods that take advantage of rich user experience sensors such as EEG and eye-tracking devices. Research in these three directions will expand the horizon of visual analytics and increase our knowledge of how it can help people extract useful and actionable knowledge out of complex and large data sets.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Enrico",
   "pi_last_name": "Bertini",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Enrico Bertini",
   "pi_email_addr": "e.bertini@northeastern.edu",
   "nsf_id": "000636771",
   "pi_start_date": "2017-05-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Claudio",
   "pi_last_name": "Silva",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Claudio T Silva",
   "pi_email_addr": "csilva@nyu.edu",
   "nsf_id": "000124941",
   "pi_start_date": "2017-05-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "NYU Tandon School of Engineering",
  "perf_str_addr": "2 Metrotech Center",
  "perf_city_name": "Brooklyn",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "112013846",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "NY07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 273552.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>\n<div class=\"sh-color-black sh-color\">The major goals of the project were to build computational and hardware infrastructure and use it to develop research on visualization beyond the desktop. The research includes the study of collaborative visual analytics on touch surfaces, development of visual data monitoring applications with ambient displays, and the study of immersive visual analytics methods with virtual reality devices. We described our main objective as to \"explore the untapped opportunities raised from exploring visual analytics with touch, ambient, and immersive displays\", and to develop several specific projects that benefit from a unified approach. We firmly believe that having this infrastructure would contribute well beyond the projects originally listed on the proposal, and that they would affect the work of a great number of students at the Visualization Imaging and Data Analysis (VIDA) Center at New York University.<br /><br />As part of the project, we have built a large multiple monitor array which operates as a unit. The installed display wall in our laboratory is composed of twenty 4K displays, totaling almost 166 million pixels. The displays are logically divided into columns of 4, each controlled by a fast CPU machine with a dual GPU configuration. A separate main computer controls the set of columns via a 10 Gigabit Ethernet connection. We have acquired a large touch screen, and various pieces of VR/AR equipment and eye tracking and EEG hardware.<br /><br />We have also built sensor building capabilities. We have invested in equipment able to perform dynamic 3D data acquisition. We have built a lab with an Ultimaker S5 3D Printer, Bantam Tools Desktop PCB Milling Machine, Siglent SDS1204X-E Oscilloscope, an Epilog Mini 12&rdquo;x24&rdquo; 40W Laser Engraver and WEN 33075 Benchtop Milling Machine with Miter Saw on the 11th floor of 370 Jay Street. We do also have a stock of most used components for fast prototyping as well as general purpose tools and network equipment.<br /><br />The infrastructure listed above has been used to support several research projects. We list a few representative ones below, and we end with broader impacts of our work.</div>\n<div class=\"sh-color-black sh-color\"><br />OpenSpace is open-source interactive data visualization software designed to visualize the entire known universe and portray our ongoing efforts to investigate the cosmos. The infrastructure acquired in this project has been used in the development of the open-source OpenSpace system. OpenSpace takes on the mission of providing an integrated view of all these sources of data and supports interactive exploration of the known universe from the millimeter scale showing instruments on spacecraft to billions of light years when visualizing the early universe. It brings the latest techniques from data visualization research to the general public. OpenSpace supports interactive presentation of dynamic data from observations, simulations, and space mission planning and operations. OpenSpace has also been used in a number of events, and the project has an active outreach program. We estimate that OpenSpace has been experienced by over 1M people as part of various events and presentations. OpenSpace (source and binaries) is available at:&nbsp;<a class=\"sh-color-blue sh-color\" rel=\"noopener noreferrer\" href=\"https://www.openspaceproject.com/\" target=\"_blank\">https://www.openspaceproject.com/</a><br /><br />ARIES is a digital initiative between NYU and the Frick Collection's Digital Art History Lab (DAHL). It stems from our frustration regarding the inability of currently available software to manipulate images in a way that was truly intuitive and useful for art historians. Traditionally, art historians have used light boxes or tables on which they placed slides or other reproductive images. In the physical world, they were able to move these around at will, organizing and reorganizing images as desired. In this way, images from multiple sources were brought together and compared to identify similarities, differences, stylistic links, and relationships for further research. The transition from analog photographs and transparencies to digital image files has rendered this workflow obsolete, yet art historians still lack well-designed, unified computational tools that are able to replace what can be done in the analog world. ARIES is a digital initiative between NYU and the Frick Collection's Digital Art History Lab (DAHL). ARIES has over 900 registered users.<br /><br />The infrastructure has supported countless student-led projects. Including, for instance, the IEEE VAST 2019 paper on &ldquo;FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System&rdquo; has been selected to receive the Best Paper Award. (An open source implementation is available:&nbsp;<a class=\"sh-color-blue sh-color\" rel=\"noopener noreferrer\" href=\"https://visflow.org/flowsense/\" target=\"_blank\">https://visflow.org/flowsense/</a>).<br /><br />The project supported the research of many PhD students, including two recipients of NYU Tandon&rsquo;s Pearl Brownstein Doctoral Research Award. Those students have accepted positions at top US institutions, including the University of Illinois at Chicago, Adobe Research, and Bosch Research.</div>\n</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/19/2022<br>\n\t\t\t\t\tModified by: Enrico&nbsp;Bertini</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThe major goals of the project were to build computational and hardware infrastructure and use it to develop research on visualization beyond the desktop. The research includes the study of collaborative visual analytics on touch surfaces, development of visual data monitoring applications with ambient displays, and the study of immersive visual analytics methods with virtual reality devices. We described our main objective as to \"explore the untapped opportunities raised from exploring visual analytics with touch, ambient, and immersive displays\", and to develop several specific projects that benefit from a unified approach. We firmly believe that having this infrastructure would contribute well beyond the projects originally listed on the proposal, and that they would affect the work of a great number of students at the Visualization Imaging and Data Analysis (VIDA) Center at New York University.\n\nAs part of the project, we have built a large multiple monitor array which operates as a unit. The installed display wall in our laboratory is composed of twenty 4K displays, totaling almost 166 million pixels. The displays are logically divided into columns of 4, each controlled by a fast CPU machine with a dual GPU configuration. A separate main computer controls the set of columns via a 10 Gigabit Ethernet connection. We have acquired a large touch screen, and various pieces of VR/AR equipment and eye tracking and EEG hardware.\n\nWe have also built sensor building capabilities. We have invested in equipment able to perform dynamic 3D data acquisition. We have built a lab with an Ultimaker S5 3D Printer, Bantam Tools Desktop PCB Milling Machine, Siglent SDS1204X-E Oscilloscope, an Epilog Mini 12\"x24\" 40W Laser Engraver and WEN 33075 Benchtop Milling Machine with Miter Saw on the 11th floor of 370 Jay Street. We do also have a stock of most used components for fast prototyping as well as general purpose tools and network equipment.\n\nThe infrastructure listed above has been used to support several research projects. We list a few representative ones below, and we end with broader impacts of our work.\n\nOpenSpace is open-source interactive data visualization software designed to visualize the entire known universe and portray our ongoing efforts to investigate the cosmos. The infrastructure acquired in this project has been used in the development of the open-source OpenSpace system. OpenSpace takes on the mission of providing an integrated view of all these sources of data and supports interactive exploration of the known universe from the millimeter scale showing instruments on spacecraft to billions of light years when visualizing the early universe. It brings the latest techniques from data visualization research to the general public. OpenSpace supports interactive presentation of dynamic data from observations, simulations, and space mission planning and operations. OpenSpace has also been used in a number of events, and the project has an active outreach program. We estimate that OpenSpace has been experienced by over 1M people as part of various events and presentations. OpenSpace (source and binaries) is available at: https://www.openspaceproject.com/\n\nARIES is a digital initiative between NYU and the Frick Collection's Digital Art History Lab (DAHL). It stems from our frustration regarding the inability of currently available software to manipulate images in a way that was truly intuitive and useful for art historians. Traditionally, art historians have used light boxes or tables on which they placed slides or other reproductive images. In the physical world, they were able to move these around at will, organizing and reorganizing images as desired. In this way, images from multiple sources were brought together and compared to identify similarities, differences, stylistic links, and relationships for further research. The transition from analog photographs and transparencies to digital image files has rendered this workflow obsolete, yet art historians still lack well-designed, unified computational tools that are able to replace what can be done in the analog world. ARIES is a digital initiative between NYU and the Frick Collection's Digital Art History Lab (DAHL). ARIES has over 900 registered users.\n\nThe infrastructure has supported countless student-led projects. Including, for instance, the IEEE VAST 2019 paper on \"FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System\" has been selected to receive the Best Paper Award. (An open source implementation is available: https://visflow.org/flowsense/).\n\nThe project supported the research of many PhD students, including two recipients of NYU Tandon\u2019s Pearl Brownstein Doctoral Research Award. Those students have accepted positions at top US institutions, including the University of Illinois at Chicago, Adobe Research, and Bosch Research.\n\n\n\t\t\t\t\tLast Modified: 05/19/2022\n\n\t\t\t\t\tSubmitted by: Enrico Bertini"
 }
}