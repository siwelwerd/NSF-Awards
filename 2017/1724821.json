{
 "awd_id": "1724821",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CIF21 DIBBs: EI: SLATE and the Mobility of Capability",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927092",
 "po_email": "alsuarez@nsf.gov",
 "po_sign_block_name": "Alejandro Suarez",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 3998494.0,
 "awd_amount": 3998494.0,
 "awd_min_amd_letter_date": "2017-06-12",
 "awd_max_amd_letter_date": "2020-07-31",
 "awd_abstract_narration": "Much of science today is propelled by multi-institutional research collaborations that require computing environments that connect instrumentation, data, and computational resources. \u00a0These resources are distributed among university research computing centers, national-scale high performance computing facilities, and commercial cloud service providers. The scale of the data and complexity of the science drive this diversity, and the need to aggregate resources from many sources into scalable computing systems. \u00a0The heterogeneity of resources causes scientists to spend more time on the technical aspects of computation and data management than on discoveries and knowledge creation, while computing support staff are required to invest more effort integrating domain specific software stacks with limited applicability beyond the community served. \u00a0Services Layer At The Edge (SLATE) provides technology that simplifies connecting university and laboratory data center capabilities to the national cyberinfrastructure ecosystem and thus expands the reach of domain-specific science gateways and multi-site research platforms.\r\n\r\nSLATE implements 'cyberinfrastructure as code' by augmenting the canonical Science DMZ pattern with a generic, programmable, secure and trusted underlayment platform. This platform hosts advanced container-centric services needed for higher-level capabilities such as data transfer nodes, software and data caches, workflow services and science gateway components. \u00a0SLATE uses best-of-breed data center virtualization components, and where available, software defined networking, to enable distributed automation of deployment and service lifecycle management tasks by domain experts. As such it simplifies creation of scalable platforms that connect research teams, institutions and resources to accelerate science while reducing operational costs and development cycle times. Since SLATE needs only commodity components for its functional layers, it is used in building distributed systems across all data center types and scales thus enabling creation of ubiquitous, science-driven cyberinfrastructure.\r\n\t\r\nThis award by the Office of Advanced Cyberinfrastructure is jointly supported by the Computational Physics within the NSF Directorate for Mathematical and Physical Sciences.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Gardner",
   "pi_mid_init": "W",
   "pi_sufx_name": "Jr",
   "pi_full_name": "Robert W Gardner",
   "pi_email_addr": "rwg@hep.uchicago.edu",
   "nsf_id": "000458001",
   "pi_start_date": "2017-06-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shawn",
   "pi_last_name": "McKee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shawn McKee",
   "pi_email_addr": "smckee@umich.edu",
   "nsf_id": "000320041",
   "pi_start_date": "2017-06-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Joseph",
   "pi_last_name": "Breen",
   "pi_mid_init": "R",
   "pi_sufx_name": "III",
   "pi_full_name": "Joseph R Breen",
   "pi_email_addr": "Joe.Breen@utah.edu",
   "nsf_id": "000543876",
   "pi_start_date": "2017-06-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Chicago",
  "perf_str_addr": "5735 S Ellis Ave., Ste 200",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606375418",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "724400",
   "pgm_ele_name": "COMPUTATIONAL PHYSICS"
  },
  {
   "pgm_ele_code": "772600",
   "pgm_ele_name": "Data Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7569",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE/SCIENCE"
  },
  {
   "pgm_ref_code": "8048",
   "pgm_ref_txt": "Data Infrstr Bldg Blocks-DIBBs"
  },
  {
   "pgm_ref_code": "8084",
   "pgm_ref_txt": "CDS&E"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 3248494.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 250000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 250000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-c6fd555a-7fff-f779-8302-cc296fe6f4a3\">&nbsp;</span></p>\n<p dir=\"ltr\"><span>SLATE (Services Layer at the Edge)</span></p>\n<p dir=\"ltr\"><span>The SLATE project represented a pioneering effort to simplify the process of connecting university and laboratory data center resources to the national cyberinfrastructure ecosystem.&nbsp; It provided a service that empowered the development of multi-institution and multi-site computation and data delivery platforms, enabling science organizations to efficiently utilize shared resources. SLATE leveraged Kubernetes, a leading container orchestration platform, along with components from the cloud-native ecosystem, while also exploring software-defined networking for optimization. A crucial aspect of SLATE's mission was to lead community efforts in developing new federated edge security and trust delegation models.</span></p>\n<p dir=\"ltr\"><span>The SLATE project achieved several key outcomes and accomplishments, each contributing to the project's success and impact: Continuous Operation: Once deployed, the central SLATE services operated continuously with negligible downtime throughout the entire project period, ensuring uninterrupted access to resources.&nbsp; Cloud-Based Infrastructure: The migration of central SLATE services to cloud-based infrastructure increased robustness and high availability, enhancing the platform's stability.&nbsp; Support for Data Caches: SLATE supported data caches of various types deployed on edge servers in production, improving data access and distribution.&nbsp; Production Clusters: The project successfully supported 15 SLATE edge clusters in production operation across as many institutions, expanding the platform's reach.&nbsp; Federated operation: SLATE supported a network of caching servers in federated operation mode, optimizing data delivery with minimal operations effort. <span id=\"docs-internal-guid-0018302e-7fff-6699-cbb9-802bf1b581c2\"><span>A novel trust framework and security model was developed to provide assurances to resource providers while limiting operator priviledges to a well-defined scope.&nbsp;</span></span>SLATE demonstrated the use of deployed applications on a FABRIC network testbed node at CERN.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/27/2023<br>\n\t\t\t\t\tModified by: Robert&nbsp;Gardner</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nSLATE (Services Layer at the Edge)\nThe SLATE project represented a pioneering effort to simplify the process of connecting university and laboratory data center resources to the national cyberinfrastructure ecosystem.  It provided a service that empowered the development of multi-institution and multi-site computation and data delivery platforms, enabling science organizations to efficiently utilize shared resources. SLATE leveraged Kubernetes, a leading container orchestration platform, along with components from the cloud-native ecosystem, while also exploring software-defined networking for optimization. A crucial aspect of SLATE's mission was to lead community efforts in developing new federated edge security and trust delegation models.\nThe SLATE project achieved several key outcomes and accomplishments, each contributing to the project's success and impact: Continuous Operation: Once deployed, the central SLATE services operated continuously with negligible downtime throughout the entire project period, ensuring uninterrupted access to resources.  Cloud-Based Infrastructure: The migration of central SLATE services to cloud-based infrastructure increased robustness and high availability, enhancing the platform's stability.  Support for Data Caches: SLATE supported data caches of various types deployed on edge servers in production, improving data access and distribution.  Production Clusters: The project successfully supported 15 SLATE edge clusters in production operation across as many institutions, expanding the platform's reach.  Federated operation: SLATE supported a network of caching servers in federated operation mode, optimizing data delivery with minimal operations effort. A novel trust framework and security model was developed to provide assurances to resource providers while limiting operator priviledges to a well-defined scope. SLATE demonstrated the use of deployed applications on a FABRIC network testbed node at CERN.\n\n\t\t\t\t\tLast Modified: 10/27/2023\n\n\t\t\t\t\tSubmitted by: Robert Gardner"
 }
}