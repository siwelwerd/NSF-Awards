{
 "awd_id": "1661755",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: A New Neat Framework for Statistical Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 229399.0,
 "awd_amount": 229400.0,
 "awd_min_amd_letter_date": "2017-01-24",
 "awd_max_amd_letter_date": "2017-01-24",
 "awd_abstract_narration": "The pendulum in Artificial Intelligence (AI) research has periodically swung from so called \"neat\" or mathematically rigorous approaches, and \"scruffy\" or more adhoc approaches. In recent years, real-world data across varied fields of science and engineering are increasingly complex, and involve a large number of variables, which has resulted in a surge of scruffier methods. This proposal develops a general \"neat\" framework for such modern settings by leveraging state of the art developments in two of the most popular subfields of machine learning methods: graphical models and high-dimensional statistical methods. These developments have in common that a complex model parameter is expressed as a superposition of simple components, which is then leveraged for tractable inference and learning.\r\n\r\nOur unified framework results not only in a unified picture of these developments but also provides newer methods to work with such high-dimensional data. The research thus impacts problems across science and engineering wherever statistical machine learning approaches are being used (such as genomics, natural language processing and image analysis, to name a few). The work on a unified framework for statistical machine learning problems is highly coupled with a push for imparting training to students on what we call \"comptastical\" thinking. This combines both computational and statistical thinking required for addressing the problems of limited computation and limited data inherent in modern statistical AI application domains. The proposal also develops an infrastructure for component-based courses with relationally organized lecture module components.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Pradeep",
   "pi_last_name": "Ravikumar",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Pradeep K Ravikumar",
   "pi_email_addr": "pradeepr@cs.cmu.edu",
   "nsf_id": "000553653",
   "pi_start_date": "2017-01-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 45126.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 184274.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project had the goal of unified frameworks for statistical estimation, encompassing graphical models and structurally constrained models, and unified computational schemes for solving the resulting statistical estimation problems. Towards these goals, we developed novel classes of component based models, where the parameter is a superposition of parameter components each of which has separate structure. We also developed novel computational schemes to solve these estimation problems, ranging from novel greedy algorithms, to novel scalable higher order algorithms, where we showed, contrary to then conventional wisdom, that higher order methods are indeed scalable even to big data settings. We also developed a novel class of what we called elementary estimators, where we showed that even under the constraint that the algorithm use only a small number of steps, we could construct estimators that have near optimal statistical guarantees. Another key contribution was novel computational schemes to solve latent variable models, where some variables are systematically unobserved, and which are increasingly popular in recent years, due in part to the greater flexibility of this class of models. In contrast to state of the art relaxation based approaches, or direct non-convex optimization approaches, we leveraged our work on component based models to provide a novel regularization scheme that could tractably encourage the model to have the desired latent structure.<br /><br />The main upshots of the project were thus to show that the principles of parsimony and simplicity has great positive consequences, both computationally and statistically, for statistical machine learning, provided one uses carefully constructed algorithms and statistical models.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/30/2019<br>\n\t\t\t\t\tModified by: Pradeep&nbsp;K&nbsp;Ravikumar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project had the goal of unified frameworks for statistical estimation, encompassing graphical models and structurally constrained models, and unified computational schemes for solving the resulting statistical estimation problems. Towards these goals, we developed novel classes of component based models, where the parameter is a superposition of parameter components each of which has separate structure. We also developed novel computational schemes to solve these estimation problems, ranging from novel greedy algorithms, to novel scalable higher order algorithms, where we showed, contrary to then conventional wisdom, that higher order methods are indeed scalable even to big data settings. We also developed a novel class of what we called elementary estimators, where we showed that even under the constraint that the algorithm use only a small number of steps, we could construct estimators that have near optimal statistical guarantees. Another key contribution was novel computational schemes to solve latent variable models, where some variables are systematically unobserved, and which are increasingly popular in recent years, due in part to the greater flexibility of this class of models. In contrast to state of the art relaxation based approaches, or direct non-convex optimization approaches, we leveraged our work on component based models to provide a novel regularization scheme that could tractably encourage the model to have the desired latent structure.\n\nThe main upshots of the project were thus to show that the principles of parsimony and simplicity has great positive consequences, both computationally and statistically, for statistical machine learning, provided one uses carefully constructed algorithms and statistical models.\n\n\t\t\t\t\tLast Modified: 09/30/2019\n\n\t\t\t\t\tSubmitted by: Pradeep K Ravikumar"
 }
}