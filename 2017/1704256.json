{
 "awd_id": "1704256",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Closed Loop Perceptual Planning for Dynamic Locomotion",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 779450.0,
 "awd_amount": 787450.0,
 "awd_min_amd_letter_date": "2017-08-13",
 "awd_max_amd_letter_date": "2022-05-18",
 "awd_abstract_narration": "Modern robots can be seen moving about a variety of terrains and environments, using wheels, legs, and other means, engaging in life-like hopping, jumping, walking, crawling, and running. They execute motions called gaits. An example of a gait is a horse trotting or galloping. Likewise, humans execute walking, running and skipping gaits. Essentially, for either a biological or mechanical systems, a gait is a locomotion pattern that involves large-amplitude body oscillations. Naturally, these motions cause impacts with terrain that jostle on-board perceptual systems and directly influence what the robots actually \"see\" as they move.  For instance, the body motion of a bounding horse-like robot may result in significant occlusions and oscillations in on-board camera systems that confound motion estimation and perceptual feedback.\r\n\r\nFocusing on complex mobility robots, this project seeks to better understand the coupling between locomotion and visual perception to improve perceptual feedback for closed-loop motion estimation. The work is organized around two key questions: 1) How should a robot look to move well? 2) How should a robot move to see well? To address the first challenge, the periodic structure of gait-based motions will be leveraged to improve perceptual filtering as the robot carries out fixed (pre-determined) motions.  The second half of the project will derive perceptual objectives and a new perceptual gait design framework to guide how high degree-of-freedom, complex mobility robots should move (locomote).  The goal is to optimize feedback for closed-loop motion implementation, on-line adaptation, and learning, which are currently difficult or impossible for many complex mobility robots.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Howard",
   "pi_last_name": "Choset",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Howard M Choset",
   "pi_email_addr": "choset@cs.cmu.edu",
   "nsf_id": "000167332",
   "pi_start_date": "2017-08-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Aaron",
   "pi_last_name": "Johnson",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Aaron M Johnson",
   "pi_email_addr": "amj1@andrew.cmu.edu",
   "nsf_id": "000729040",
   "pi_start_date": "2017-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 779450.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This work addresses the problem of how to estimate various state variables for systems that are moving while moving through their environemtns. Take for example, a galloping legged system. The head goes up and down while the system moves, and yet biological examples of such systems seemlessly infer features and stably track them. THe same is true of a snake that undulates through its environment. Note, that this work considers systems who sensory device undergoes an undulatory path, characterized by a phase variable throughout the its gait. In particular, we are interested in using vision to infer features.&nbsp;</p>\n<p>One system consduered in this work is a leggerd robot, moving at a non-trivial speed throughout its environemnt. Methods for state estimation that rely on visual information are challenging on legged robots because of rapid changes in the viewing angle of onboard cameras. In this work,we show that by leveraging structure in the way that therobot locomotes, the accuracy of visual-inertial SLAM in these challenging scenarios can be increased. We present a method that takes advantage of the underlying periodic predictabilityoften present in the motion of legged robots to improve the performance of the feature tracking module within a visualinertial SLAM system. Our method performs multi-session SLAM on a single robot, where each session is responsible for mapping during a distinct portion of the robot&rsquo;s gait cycle. Our method produces lower absolute trajectory error than several state-of-the-art methods for visual-inertial SLAM in both asimulated environment and on data collected on a quadrupedal robot executing dynamic gaits. On real-world bounding gaits, our median trajectory error was less than 35% of the error ofthe next best estimate provided by state-of-the-art methods.</p><br>\n<p>\n Last Modified: 03/25/2024<br>\nModified by: Howard&nbsp;M&nbsp;Choset</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis work addresses the problem of how to estimate various state variables for systems that are moving while moving through their environemtns. Take for example, a galloping legged system. The head goes up and down while the system moves, and yet biological examples of such systems seemlessly infer features and stably track them. THe same is true of a snake that undulates through its environment. Note, that this work considers systems who sensory device undergoes an undulatory path, characterized by a phase variable throughout the its gait. In particular, we are interested in using vision to infer features.\n\n\nOne system consduered in this work is a leggerd robot, moving at a non-trivial speed throughout its environemnt. Methods for state estimation that rely on visual information are challenging on legged robots because of rapid changes in the viewing angle of onboard cameras. In this work,we show that by leveraging structure in the way that therobot locomotes, the accuracy of visual-inertial SLAM in these challenging scenarios can be increased. We present a method that takes advantage of the underlying periodic predictabilityoften present in the motion of legged robots to improve the performance of the feature tracking module within a visualinertial SLAM system. Our method performs multi-session SLAM on a single robot, where each session is responsible for mapping during a distinct portion of the robots gait cycle. Our method produces lower absolute trajectory error than several state-of-the-art methods for visual-inertial SLAM in both asimulated environment and on data collected on a quadrupedal robot executing dynamic gaits. On real-world bounding gaits, our median trajectory error was less than 35% of the error ofthe next best estimate provided by state-of-the-art methods.\t\t\t\t\tLast Modified: 03/25/2024\n\n\t\t\t\t\tSubmitted by: HowardMChoset\n"
 }
}