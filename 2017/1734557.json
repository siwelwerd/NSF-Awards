{
 "awd_id": "1734557",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Scalable Multimodal Tactile Sensing for Robotic Manipulators in Manufacturing",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Bruce Kramer",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 750000.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2017-07-27",
 "awd_abstract_narration": "The research focuses on sensors and software to enable robots to have an artificial sense of touch. Current robot hands and grippers rarely have the ability to sense their environments; they operate by precisely repeating pre-programmed motions. This prevents the use of robots in applications where the state of the world is unknowable in advance, since such use requires a robot to sense and react to unexpected events. The project will design novel and sophisticated touch sensors and equip robot hands with them.  The sensors will include multiple types of touch sensors and integrate them to detect different types and characteristics of contacts: fast versus slow, transitory versus maintained, etc.  These data will be interpreted by software algorithms that learn how to make use of the newly acquired touch data. This approach takes its inspiration from the human hand, which is also equipped with multiple types of tactile sensing elements relaying information to the nervous system. The proximate goal of the work is to build tools for material handling in cluttered environments, a task that can enable e-commerce and supply chains to run more efficiently, help manufacturers become more efficient and competitive, and reduce injuries to workers by assisting with repetitive tasks that are known to cause injuries.\r\n\r\nNovel sensor design, computation and planning will be integrated to endow robots with a sense of touch, thereby enabling manipulation in cluttered environments. At the hardware level, the sensors will combine piezoelectric and resistive strain sensing to capture both transient, high-frequency responses and absolute strain measurements. Sensor sheets will be stacked in multiple sensing layers to provide a rich signal set that departs from the \"one location, one taxel\" approach that is normally taken and maximum use will be made of the rich data by combining model-based and data-driven approaches to define motor control primitives.  These will be combined to implement algorithms for motor control and planning that use the tactile data in a target task: bin-picking and kitting in manufacturing. The overall goal is a compact, multi-modal, scalable tactile sensing system for robotic manipulators, along with the low-level motor skills and high level planning algorithms that use tactile data for manipulating in clutter.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matei",
   "pi_last_name": "Ciocarlie",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Matei T Ciocarlie",
   "pi_email_addr": "mtc2103@columbia.edu",
   "nsf_id": "000674926",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Allen",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Peter K Allen",
   "pi_email_addr": "allen@cs.columbia.edu",
   "nsf_id": "000183444",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ioannis",
   "pi_last_name": "Kymissis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ioannis Kymissis",
   "pi_email_addr": "johnkym@ee.columbia.edu",
   "nsf_id": "000177392",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "2960 Broadway",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "030E",
   "pgm_ref_txt": "CONTROL SYSTEMS"
  },
  {
   "pgm_ref_code": "082E",
   "pgm_ref_txt": "MFG MACHINES & METROLOGY"
  },
  {
   "pgm_ref_code": "087E",
   "pgm_ref_txt": "SENSING & CONTROL"
  },
  {
   "pgm_ref_code": "8013",
   "pgm_ref_txt": "High Risk/Reward Innovative Research"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 750000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-6f7b8154-7fff-336b-1158-b3e518e17669\"> </span></p>\n<p dir=\"ltr\"><span>In this project, we have advanced the state of the art in robotic manipulation using tactile sensing from multiple perspectives: development of novel tactile robotic sensors and fingers based on multiple transduction modalities, development of novel tactile-based manipulation algorithms, and the integration between the two.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Intellectual merit: highlights of the results from this project include work at the conceptual, algorithmic, and hardware levels. As a key example from an algorithmic perspective, we have developed MAT, an adaptive, closed-loop tactile algorithm to reinforcement-learn dexterous robotic grasping for multi-fingered hands with noisy tactile readings and no visual information. The method utilizes tactile and proprioceptive information to act through both fine finger motions and larger regrasp movements to execute stable grasps. A novel curriculum of action motion magnitude makes learning more tractable and helps turn common failure cases into successes. Careful selection of features that exhibit small sim-to-real gaps enables this tactile grasping policy, trained purely in simulation, to transfer well to real world environments without the need for additional learning. Tested with real world objects, MAT achieves mid-to-high 90% success rates and significantly improves upon open-loop grasping under calibration error.&nbsp; The robustness of MAT even under considerable calibration error reduces the need for a perfectly calibrated grasping setup. MAT can be easily added to any existing open-loop grasp planner to close the final gap between failed grasps and success.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Hardware development highlights include the development of a novel multicurved robotic finger with accurate touch localization and normal force detection over complex, three-dimensional surfaces. Existing methods for building touch sensors have proven difficult to integrate into robot fingers due to multiple challenges, including difficulty in covering multicurved surfaces, high wire count, or packaging constraints preventing their use in dexterous hands. In contrast, the key to our approach is the novel use of overlapping signals from light emitters and receivers embedded in a transparent waveguide layer that covers the functional areas of the finger. By measuring light transport between every emitter and receiver, we show that we can obtain a very rich signal set that changes in response to deformation of the finger due to touch. We then show that purely data-driven deep learning methods are able to extract useful information from such data, such as contact location and applied normal force, without the need for analytical models. The final result is a fully integrated, sensorized robot finger, with a low wire count and using easily accessible manufacturing methods, designed for easy integration into dexterous manipulators.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Highlighting the importance of combining hardware and software advances in tactile manipulation, we recently explored the ability to execute complex dexterous tasks using only the sensor information that is available from the tactile fingers developed in this project. In particular, finger-gaiting manipulation is an important skill to achieve large-angle in-hand re-orientation of objects. However, achieving these gaits with arbitrary orientations of the hand is challenging due to the unstable nature of the task. In recent work, we used model-free reinforcement learning (RL) in a simulated environment to demonstrate finger-gaiting for rotation about an axis purely using on-board proprioceptive and tactile feedback as generated by our real tactile fingers. We showed that tactile sensing makes it possible to learn finger-gaiting with significantly improved sample complexity than the state-of-the-art, and are currently working to transfer these results to our real hand prototypes equipped with tactile fingers.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Broader impact: we hope that our work on multimodal, low-cost sensors that are easy to build and integrate, along with algorithms based on touch data, will lower the barriers to entry for general research on sensorimotor skills based on tactile information. In particular, we note that we have open-sourced our novel finger designs for use in the academic and research community. Our work is a stepping stone towards a versatile manipulator operating based on tactile feedback that will be able to handle variations in the task with little hardware modifications. This ability is needed in areas as varied as logistics, warehouse management, healthcare, services, etc., culminating with home robots.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/07/2022<br>\n\t\t\t\t\tModified by: Matei&nbsp;T&nbsp;Ciocarlie</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1734557/1734557_10507703_1641587305380_columbia_tactile_lightshow_2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1734557/1734557_10507703_1641587305380_columbia_tactile_lightshow_2--rgov-800width.jpg\" title=\"Optics-based tactile finger\"><img src=\"/por/images/Reports/POR/2022/1734557/1734557_10507703_1641587305380_columbia_tactile_lightshow_2--rgov-66x44.jpg\" alt=\"Optics-based tactile finger\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The optics-based multicurved tactile finger developed in this project (shown here without outer skin).</div>\n<div class=\"imageCredit\">Columbia University</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Matei&nbsp;T&nbsp;Ciocarlie</div>\n<div class=\"imageTitle\">Optics-based tactile finger</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1734557/1734557_10507703_1638464996430_future_tactile--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1734557/1734557_10507703_1638464996430_future_tactile--rgov-800width.jpg\" title=\"Multi-Fingered Adaptive Tactile Grasping via Deep Reinforcement Learning\"><img src=\"/por/images/Reports/POR/2021/1734557/1734557_10507703_1638464996430_future_tactile--rgov-66x44.jpg\" alt=\"Multi-Fingered Adaptive Tactile Grasping via Deep Reinforcement Learning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">MAT achieves mid-to-high 90% success rates and significantly improves upon open-loop grasping under calibration error.  The robustness of MAT even under considerable calibration error reduces the need for a perfectly calibrated grasping setup.</div>\n<div class=\"imageCredit\">Peter Allen</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Peter&nbsp;K&nbsp;Allen</div>\n<div class=\"imageTitle\">Multi-Fingered Adaptive Tactile Grasping via Deep Reinforcement Learning</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR_DELETED_ITEMS/1734557_10507703_1641588623051_columbia_tactile_robot_hand--rgov-214x142.jpg\" original=\"/por/images/Reports/POR_DELETED_ITEMS/1734557_10507703_1641588623051_columbia_tactile_robot_hand--rgov-800width.jpg\" title=\"Sensorized robot hand\"><img src=\"/por/images/Reports/POR_DELETED_ITEMS/1734557_10507703_1641588623051_columbia_tactile_robot_hand--rgov-66x44.jpg\" alt=\"Sensorized robot hand\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Dexterous robot hand equipped with tactile sensors.</div>\n<div class=\"imageCredit\">Columbia University</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Matei&nbsp;T&nbsp;Ciocarlie</div>\n<div class=\"imageTitle\">Sensorized robot hand</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nIn this project, we have advanced the state of the art in robotic manipulation using tactile sensing from multiple perspectives: development of novel tactile robotic sensors and fingers based on multiple transduction modalities, development of novel tactile-based manipulation algorithms, and the integration between the two. \n\n \nIntellectual merit: highlights of the results from this project include work at the conceptual, algorithmic, and hardware levels. As a key example from an algorithmic perspective, we have developed MAT, an adaptive, closed-loop tactile algorithm to reinforcement-learn dexterous robotic grasping for multi-fingered hands with noisy tactile readings and no visual information. The method utilizes tactile and proprioceptive information to act through both fine finger motions and larger regrasp movements to execute stable grasps. A novel curriculum of action motion magnitude makes learning more tractable and helps turn common failure cases into successes. Careful selection of features that exhibit small sim-to-real gaps enables this tactile grasping policy, trained purely in simulation, to transfer well to real world environments without the need for additional learning. Tested with real world objects, MAT achieves mid-to-high 90% success rates and significantly improves upon open-loop grasping under calibration error.  The robustness of MAT even under considerable calibration error reduces the need for a perfectly calibrated grasping setup. MAT can be easily added to any existing open-loop grasp planner to close the final gap between failed grasps and success.\n\n \nHardware development highlights include the development of a novel multicurved robotic finger with accurate touch localization and normal force detection over complex, three-dimensional surfaces. Existing methods for building touch sensors have proven difficult to integrate into robot fingers due to multiple challenges, including difficulty in covering multicurved surfaces, high wire count, or packaging constraints preventing their use in dexterous hands. In contrast, the key to our approach is the novel use of overlapping signals from light emitters and receivers embedded in a transparent waveguide layer that covers the functional areas of the finger. By measuring light transport between every emitter and receiver, we show that we can obtain a very rich signal set that changes in response to deformation of the finger due to touch. We then show that purely data-driven deep learning methods are able to extract useful information from such data, such as contact location and applied normal force, without the need for analytical models. The final result is a fully integrated, sensorized robot finger, with a low wire count and using easily accessible manufacturing methods, designed for easy integration into dexterous manipulators.\n\n \nHighlighting the importance of combining hardware and software advances in tactile manipulation, we recently explored the ability to execute complex dexterous tasks using only the sensor information that is available from the tactile fingers developed in this project. In particular, finger-gaiting manipulation is an important skill to achieve large-angle in-hand re-orientation of objects. However, achieving these gaits with arbitrary orientations of the hand is challenging due to the unstable nature of the task. In recent work, we used model-free reinforcement learning (RL) in a simulated environment to demonstrate finger-gaiting for rotation about an axis purely using on-board proprioceptive and tactile feedback as generated by our real tactile fingers. We showed that tactile sensing makes it possible to learn finger-gaiting with significantly improved sample complexity than the state-of-the-art, and are currently working to transfer these results to our real hand prototypes equipped with tactile fingers.\n\n \nBroader impact: we hope that our work on multimodal, low-cost sensors that are easy to build and integrate, along with algorithms based on touch data, will lower the barriers to entry for general research on sensorimotor skills based on tactile information. In particular, we note that we have open-sourced our novel finger designs for use in the academic and research community. Our work is a stepping stone towards a versatile manipulator operating based on tactile feedback that will be able to handle variations in the task with little hardware modifications. This ability is needed in areas as varied as logistics, warehouse management, healthcare, services, etc., culminating with home robots.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/07/2022\n\n\t\t\t\t\tSubmitted by: Matei T Ciocarlie"
 }
}