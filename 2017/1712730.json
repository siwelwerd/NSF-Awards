{
 "awd_id": "1712730",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:  Statistical Estimation with Algebraic Structure",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 279998.0,
 "awd_amount": 279998.0,
 "awd_min_amd_letter_date": "2017-05-02",
 "awd_max_amd_letter_date": "2019-09-13",
 "awd_abstract_narration": "Scientific and engineering disciplines ranging from structural biology to computer vision rely on data collection and analysis to guide scientific discovery. Critically, in such applications, the systems under study constrain and govern the structure of information in collected data. The goal of this research project is to develop a family of statistical models that enables a systematic extraction of relevant statistical information from these datasets by bringing together interdisciplinary concepts from statistics and optimization. The approach under development aims to provide a new set of statistical tools that is adapted to this class of problems and that could have a transformative impact on several scientific disciplines.\r\n\r\nThis project is articulated around a core set of techniques to analyze datasets in the context of a latent algebraic structure, often arising from the physical laws underlying the data collection processes. Unlike more traditional statistical problems where a linear underlying structure is often built into the model, data-driven science generates problems with algebraic but often non-linear structure. The project focuses on problems of central importance in a variety of scientific and engineering disciplines, including signal processing, structural biology, and computer vision, that share a similar feature: the need to leverage algebraic structure in order to extract information from data. The project aims at developing a systematic approach to analyze this family of problems, together with a general procedure to construct computationally efficient algorithms using low-rank tensor decomposition. Importantly, these methods can be proved to be statistically optimal and therefore make the most efficient use of collected data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Afonso",
   "pi_last_name": "Bandeira",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Afonso Bandeira",
   "pi_email_addr": "bandeira@cims.nyu.edu",
   "nsf_id": "000716801",
   "pi_start_date": "2017-05-02",
   "pi_end_date": "2019-09-13"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Wein",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander S Wein",
   "pi_email_addr": "aswein@ucdavis.edu",
   "nsf_id": "000756709",
   "pi_start_date": "2019-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100121110",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 47659.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 182267.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 50072.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to develop new methods for understanding statistical inference problems by leveraging their underlying algebraic structure. One focus was on the class of so-called &ldquo;orbit recovery&rdquo; problems which are motivated by real-world applications such as multi-reference alignment (MRA) and cryo-electron microscopy (cryo-EM). Cryo-EM is a biological imaging technique that produces many noisy images of a molecule that are all taken from different random unknown viewing directions; the associated image processing task is to use these images to reconstruct a high-resolution model of the 3-dimensional molecular structure. The orbit recovery framework is a theoretical abstraction that generalizes this setup, replacing the unknown rotation of the molecule with any linear action of a compact group. In cryo-EM, the compact group is SO(3), whereas in the simpler MRA problem, the group is the cyclic group Z_n acting by circularly shifting the coordinates of an n-dimensional signal vector. This project successfully advanced the theoretical understanding of orbit recovery problems in a variety of ways. First, the theoretical sample complexity was precisely pinned down: for both MRA and cryo-EM, it is necessary and sufficient for the number of samples to scale as the third power of the noise variance. This is intimately related to the underlying algebraic structure: the degree-3 invariant polynomials of the signal are enough to determine the signal, but the degree-2 invariants are not. An analogous algebraic criterion can be used to determine the sample complexity of any given orbit recovery problem. An additional challenge is to show that the signal can be reconstructed from the samples using a computationally-efficient algorithm, whereas a priori one needs to solve a system of polynomial equations. Progress has also been made on this front: for certain orbit recovery problems over finite groups (such as MRA), tensor decomposition methods can be used to obtain a polynomial-time algorithm with provable guarantees. Infinite groups such as SO(3) pose an additional challenge; for the simpler case of SO(2), a polynomial-time algorithm has been obtained using a novel tensor network framework. While it remains an open problem to give a polynomial-time algorithm for the more complicated cryo-EM problem, the above results constitute substantial progress toward this goal. Since orbit recovery problems are prevalent in real-world applications such as signal processing and biological imaging, this line of work has the potential to influence the technologies used in practice and to foster collaboration between theoreticians and practitioners.</p>\n<p>A second focus of the project has been on the so-called &ldquo;low-degree polynomial method&rdquo; which connects the computational complexity of statistical inference problems to their underlying algebraic structure, namely low-degree polynomials of the data. This method has been successful in understanding statistical-to-computational gaps in a variety of fundamental statistical tasks such as planted clique, sparse PCA, community detection, and tensor PCA. The key idea is to investigate whether low-degree algorithms (which are logarithmic-degree multivariate polynomials of the input) can solve a particular statistical task; if all low-degree algorithms can be proven to fail in some parameter regime, this implies failure of various leading algorithmic approaches such as spectral methods and approximate message passing, and thus provides evidence that no polynomial-time algorithm exists. This project successfully advanced the low-degree method in a number of ways. While prior work on the low-degree method applied only to detection problems (i.e. identifying whether or not a planted signal is present), new progress was made on adapting this framework to other types of problems: estimation (i.e. recovering the planted signal), random optimization problems (with no planted signal), and certification/refutation problems. This required a number of new proof techniques which seem to be broadly applicable to many statistical problems. The results of this project have lead to an improved understanding of many problems, including certifying bounds on the Sherrington-Kirkpatrick Hamiltonian, subexponential-time sparse PCA, planted dense subgraph recovery, finding a large independent set in a random graph, refuting colorability in random graphs, positive PCA, planted sparse vector in a subspace, group testing, and sparse regression. Many of these results include new algorithms that succeed in the best known parameter regimes, as well as matching lower bounds based on the low-degree framework. This line of work has helped to establish the low-degree method as a leading framework for understanding statistical-to-computational gaps in a large (and growing) collection of fundamental statistical problems. As a result, this work has the potential to transform the field of computational statistics; the PI is hopeful that in the future, the low-degree method will become a cornerstone topic taught in graduate programs in statistics and computer science. Furthermore, this line of work gives a theoretical understanding of the optimal algorithms for many fundamental statistical questions; this has the potential to influence the methods used in practice throughout fields such as machine learning and data science.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/06/2021<br>\n\t\t\t\t\tModified by: Alexander&nbsp;S&nbsp;Wein</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to develop new methods for understanding statistical inference problems by leveraging their underlying algebraic structure. One focus was on the class of so-called \"orbit recovery\" problems which are motivated by real-world applications such as multi-reference alignment (MRA) and cryo-electron microscopy (cryo-EM). Cryo-EM is a biological imaging technique that produces many noisy images of a molecule that are all taken from different random unknown viewing directions; the associated image processing task is to use these images to reconstruct a high-resolution model of the 3-dimensional molecular structure. The orbit recovery framework is a theoretical abstraction that generalizes this setup, replacing the unknown rotation of the molecule with any linear action of a compact group. In cryo-EM, the compact group is SO(3), whereas in the simpler MRA problem, the group is the cyclic group Z_n acting by circularly shifting the coordinates of an n-dimensional signal vector. This project successfully advanced the theoretical understanding of orbit recovery problems in a variety of ways. First, the theoretical sample complexity was precisely pinned down: for both MRA and cryo-EM, it is necessary and sufficient for the number of samples to scale as the third power of the noise variance. This is intimately related to the underlying algebraic structure: the degree-3 invariant polynomials of the signal are enough to determine the signal, but the degree-2 invariants are not. An analogous algebraic criterion can be used to determine the sample complexity of any given orbit recovery problem. An additional challenge is to show that the signal can be reconstructed from the samples using a computationally-efficient algorithm, whereas a priori one needs to solve a system of polynomial equations. Progress has also been made on this front: for certain orbit recovery problems over finite groups (such as MRA), tensor decomposition methods can be used to obtain a polynomial-time algorithm with provable guarantees. Infinite groups such as SO(3) pose an additional challenge; for the simpler case of SO(2), a polynomial-time algorithm has been obtained using a novel tensor network framework. While it remains an open problem to give a polynomial-time algorithm for the more complicated cryo-EM problem, the above results constitute substantial progress toward this goal. Since orbit recovery problems are prevalent in real-world applications such as signal processing and biological imaging, this line of work has the potential to influence the technologies used in practice and to foster collaboration between theoreticians and practitioners.\n\nA second focus of the project has been on the so-called \"low-degree polynomial method\" which connects the computational complexity of statistical inference problems to their underlying algebraic structure, namely low-degree polynomials of the data. This method has been successful in understanding statistical-to-computational gaps in a variety of fundamental statistical tasks such as planted clique, sparse PCA, community detection, and tensor PCA. The key idea is to investigate whether low-degree algorithms (which are logarithmic-degree multivariate polynomials of the input) can solve a particular statistical task; if all low-degree algorithms can be proven to fail in some parameter regime, this implies failure of various leading algorithmic approaches such as spectral methods and approximate message passing, and thus provides evidence that no polynomial-time algorithm exists. This project successfully advanced the low-degree method in a number of ways. While prior work on the low-degree method applied only to detection problems (i.e. identifying whether or not a planted signal is present), new progress was made on adapting this framework to other types of problems: estimation (i.e. recovering the planted signal), random optimization problems (with no planted signal), and certification/refutation problems. This required a number of new proof techniques which seem to be broadly applicable to many statistical problems. The results of this project have lead to an improved understanding of many problems, including certifying bounds on the Sherrington-Kirkpatrick Hamiltonian, subexponential-time sparse PCA, planted dense subgraph recovery, finding a large independent set in a random graph, refuting colorability in random graphs, positive PCA, planted sparse vector in a subspace, group testing, and sparse regression. Many of these results include new algorithms that succeed in the best known parameter regimes, as well as matching lower bounds based on the low-degree framework. This line of work has helped to establish the low-degree method as a leading framework for understanding statistical-to-computational gaps in a large (and growing) collection of fundamental statistical problems. As a result, this work has the potential to transform the field of computational statistics; the PI is hopeful that in the future, the low-degree method will become a cornerstone topic taught in graduate programs in statistics and computer science. Furthermore, this line of work gives a theoretical understanding of the optimal algorithms for many fundamental statistical questions; this has the potential to influence the methods used in practice throughout fields such as machine learning and data science.\n\n\t\t\t\t\tLast Modified: 07/06/2021\n\n\t\t\t\t\tSubmitted by: Alexander S Wein"
 }
}