{
 "awd_id": "1712041",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:  Statistical Inference Using Random Forests and Related Methods",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 119802.0,
 "awd_amount": 119802.0,
 "awd_min_amd_letter_date": "2017-07-19",
 "awd_max_amd_letter_date": "2017-07-19",
 "awd_abstract_narration": "This project seeks to develop methods to quantify uncertainty in machine learning algorithms and to incorporate machine learning and statistical inference. Machine learning has been enormously successful at using data to make predictions; it is used in an extensive range of applications from handwriting recognition to high frequency trading to driverless cars and personalized medicine. However, while machine learning algorithms make good predictions, they tell humans very little about how those predictions were arrived at: What were the important factors? How did they affect the prediction? They also don't distinguish predictions for which there is a lot of information about the probability of different outcomes (even if that covers a wide range) from those where very little information is available. For example, a machine learning algorithm may very accurately predict whether a person is likely to develop diabetes, but provides little if any information regarding how that person might lower his or her risk. This project will build on initial mathematical theory to develop methods to explain how Random Forests arrive at their predictions and how statistically confident those predictions are, and produce ways to link machine learning methods to other statistical models.\r\n\r\n\r\nThis project seeks to develop methods to quantify uncertainty in machine learning algorithms and to incorporate machine learning and statistical inference. The project will extend on a theoretical framework representing Random Forests as U-statistics to produce a practical implementation of statistical uncertainty quantification in machine learning. In particular, it will improve on methods to estimate sample variability in Random Forest predictions, develop computationally efficient screening tools for covariate and interaction selection, and incorporate ensemble methods as non-parametric terms in partially-linear models while retaining statistical inference via a modified boosting algorithm. These methods will be demonstrated on a citizen science data base in ornithology and in various biomedical applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lucas",
   "pi_last_name": "Mentch",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Lucas K Mentch",
   "pi_email_addr": "lkm31@pitt.edu",
   "nsf_id": "000710108",
   "pi_start_date": "2017-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pittsburgh",
  "inst_street_address": "4200 FIFTH AVENUE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4126247400",
  "inst_zip_code": "152600001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "MKAGLD59JRL1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pittsburgh",
  "perf_str_addr": "University Club",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152132303",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  },
  {
   "pgm_ele_code": "745400",
   "pgm_ele_name": "MSPA-INTERDISCIPLINARY"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8007",
   "pgm_ref_txt": "BioMaPS"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 119802.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This award successfully developed several novel theoretical and methodological results related to the quantification of uncertainty in modern machine learning algorithms.&nbsp; Machine learning has been enormously successful at using data to make predictions; it is used in an extensive range of applications from handwriting recognition to high frequency trading to driverless cars and personalized medicine. However, while machine learning algorithms make good predictions, they tell humans very little about how those predictions were generated. What were the important factors? How did they affect the prediction? They also don't distinguish predictions for which there is a lot of information about the probability of different outcoemes (even if that covers a wide range) from those where very little information is available. For example, a machine learning algorithm may very accurately predict whether a person is likely to develop diabetes, but provide little if any information regarding how that person might lower his or her risk. This project built on PI&rsquo;s initial mathematical results to develop methods to explain how random forests &ndash; one of the most popular &ldquo;off-the-shelf&rdquo; machine learning tools &ndash;arrive at their predictions and to quantify how statistically confident one can be in those predictions.</p>\n<p>&nbsp;</p>\n<p>Specifically, the work extended a theoretical framework representing random forests predictions as U-statistics to produce a practical implementation of statistical uncertainty quantification in machine learning. In particular, methods to estimate the sampling variability in random forest predictions were newly developed and made orders of magnitude more computationally efficient.&nbsp; First-of-their-kind theoretical results were also established detailing how quickly random forest predictions converge to their asymptotic distributions.&nbsp; Finally, several new inferential procedures were also developed to aid scientists in discovering complex signal hidden within large, high-dimensional modern datasets.&nbsp; The PIs on this project established several successful interdisciplinary collaborations with physicians, psychiatrists, ecologists, ornithologists, and political scientists in order to ensure a successful translation of the theoretical results into practically useful scientific tools.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/09/2021<br>\n\t\t\t\t\tModified by: Lucas&nbsp;K&nbsp;Mentch</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis award successfully developed several novel theoretical and methodological results related to the quantification of uncertainty in modern machine learning algorithms.  Machine learning has been enormously successful at using data to make predictions; it is used in an extensive range of applications from handwriting recognition to high frequency trading to driverless cars and personalized medicine. However, while machine learning algorithms make good predictions, they tell humans very little about how those predictions were generated. What were the important factors? How did they affect the prediction? They also don't distinguish predictions for which there is a lot of information about the probability of different outcoemes (even if that covers a wide range) from those where very little information is available. For example, a machine learning algorithm may very accurately predict whether a person is likely to develop diabetes, but provide little if any information regarding how that person might lower his or her risk. This project built on PI\u2019s initial mathematical results to develop methods to explain how random forests &ndash; one of the most popular \"off-the-shelf\" machine learning tools &ndash;arrive at their predictions and to quantify how statistically confident one can be in those predictions.\n\n \n\nSpecifically, the work extended a theoretical framework representing random forests predictions as U-statistics to produce a practical implementation of statistical uncertainty quantification in machine learning. In particular, methods to estimate the sampling variability in random forest predictions were newly developed and made orders of magnitude more computationally efficient.  First-of-their-kind theoretical results were also established detailing how quickly random forest predictions converge to their asymptotic distributions.  Finally, several new inferential procedures were also developed to aid scientists in discovering complex signal hidden within large, high-dimensional modern datasets.  The PIs on this project established several successful interdisciplinary collaborations with physicians, psychiatrists, ecologists, ornithologists, and political scientists in order to ensure a successful translation of the theoretical results into practically useful scientific tools.\n\n \n\n\t\t\t\t\tLast Modified: 09/09/2021\n\n\t\t\t\t\tSubmitted by: Lucas K Mentch"
 }
}