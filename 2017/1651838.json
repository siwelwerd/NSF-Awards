{
 "awd_id": "1651838",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER:Matrix Products: Algorithms and Applications",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2017-03-15",
 "awd_exp_date": "2022-02-28",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 408000.0,
 "awd_min_amd_letter_date": "2017-04-11",
 "awd_max_amd_letter_date": "2021-04-20",
 "awd_abstract_narration": "Methods for multiplying matrices are routinely used to approach computational problems from a huge variety of applications: finding good routes in networks, pattern detection in networks, simulating motion in computer graphics and animation, protein and RNA structure prediction in biochemistry, questions in quantum mechanics, machine learning, electronics, scientific computing, and anywhere linear systems of equations need to be solved. The ability to multiply large matrices faster would have tangible impact on the world. For the past fifty years, computer scientists have been developing a rich mathematical theory of matrix multiplication algorithms;  still, it is not clear exactly how fast matrices can be multiplied, nor what the best algorithms would even look like.  The main goal of the PI is to deepen and extend the theory of matrix multiplication, and to search for faster algorithms for the problem.\r\n \r\nThe most studied version of matrix multiplication is when the matrix entries come from an underlying ring such as the integers (Z), and the \"plus\" and \"times\" operations are addition and multiplication over Z. The algorithmic progress on ring matrix multiplication is a prime example of algorithmic ingenuity. For decades the trivial approach was deemed optimal until deep theory led to significant and surprising improvements. The theoretical study of ring matrix multiplication algorithms aims to pinpoint the exponent \"omega\" of matrix multiplication, considered to be the main measure of progress on the problem. The number omega is the smallest real number for which there is an algorithm that multiplies two square matrices of dimension n using n^(omega+o(1)) operations (additions and multiplications of numbers). Since the output has size n^2, omega is at least 2; the most recent bound omega < 2.373 was obtained by the PI. The PI aims to investigate new approaches to improving the bound on omega and related parameters, with a long-term goal of designing a fast and practical algorithm.\r\n \r\nThe impressive improvements above only apply to ring matrix multiplication. However, in many applications, different, potentially more complex matrix products are needed. For instance, in computing shortest paths in a network, one relies on the so called distance product of real matrices for which the \"plus\" operation is minimum and the \"times\" operation is addition. The matrix product is no longer over a ring, but rather over a semiring. Non-ring matrix products are not as well understood as ring matrix multiplication; some, such as the distance product, don't even seem to admit much faster algorithms than the brute-force algorithm that follows from their definition. The second major goal of the PI is to study a large variety of non-ring matrix products, develop algorithms for them, and broaden and strengthen their applications.\r\n \r\nThis project has several educational goals. These include mentoring undergraduate and graduate students, the development of new courses directly related to the described topics, and incorporating these topics into existing core algorithms courses. The lectures and project materials will be available on the course website for the general public. The PI is wholeheartedly committed to diversity. The PI has experience in recruiting and mentoring both undergraduate and graduate minority students, and will continue to take an active role in seeking and recruiting students from diverse cultures and backgrounds.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Virginia",
   "pi_last_name": "Williams",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Virginia V Williams",
   "pi_email_addr": "virgito@gmail.com",
   "nsf_id": "000640555",
   "pi_start_date": "2017-04-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Ave",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 179656.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 71041.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 81514.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 75789.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Matrix Multiplication is one of the most basic algebraic operations outside basic arithmetic. Matrices are 2-dimensional arrays of numbers. The product C of two n by n matrices A and B is also an n by n matrix. The (i,j) entry of C, C[i,j] is defined mathematically using the entries of A in its i-th row, and the entries of B in its j-th column, as C[i,j] = A[i,1]*B[1,j]+ A[i,2]*B[2,j]+?+A[i,n]*B[n,j].</p>\n<p>Thus, each entry of C can be computed by performing n products and n-1 additions of numbers, giving 2n-1 arithmetic operations. Altogether, all n^2 entries of C can be computed using 2n^3-n^2 operations, i.e. in O(n^3) time.</p>\n<p>This is the ?brute-force? algorithm for matrix multiplication. In 1969, Strassen surprised everyone by showing that there is an asymptotically faster algorithm, using only O(n^{2.81}) operations. We use the constant ?omega? to be the smallest constant such that one can multiply n by n matrices using O(n^omega) operations. We know from Strassen that omega&lt;2.81. Since C has n^2 entries, we need at least n^2 operations, and so omega&gt;=2.</p>\n<p>One of the most important unanswered questions in computer algorithms is, what is omega? If omega=2, then there would be an optimal algorithm.</p>\n<p>Why do we want to study matrix multiplication (MM)? MM is at the heart of a wide variety of algorithmic problems. Any time a change in coordinates is needed (e.g. in computer animation or finite element simulations) one needs to perform MM. Finding patterns in networks, be it from biology, the internet, or the social sciences, necessitate MM. MM is everywhere.</p>\n<p>After Strassen?s discovery that the brute force algorithm is not optimal, a long line of research led to the development of more and more complex, and asymptotically faster algorithms for MM. In 1986 Coppersmith and Winograd showed that omega&lt;2.376. More than two decades later, Stothers (2010) and the PI (2012) gave slight improvements, showing that omega&lt;2.37288. Le Gall (2014) made another modest improvement to omega&lt;2.37287. As part of this Career grant, the PI and Alman (2021) developed new techniques and showed that omega&lt;2.37286. This is the current state of the art.</p>\n<p>Since 2010 the improvements to omega have been tiny. Why is this? As part of this grant, the PI and Alman (2018, 2019) gave a compelling reason. The toolbox that all algorithms so far can be formalized and generalized as the so called ``Universal Method?? for MM algorithms. The PI and Alman showed that this method cannot produce an optimal algorithm! The best one can potentially get is omega&lt;2.16. The Universal Method is a vast generalization of the methods used over the years. The less general Laser Method that still covers the current techniques, is even less powerful, as shown by Ambainis, Filmus and Le Gall (2015), and can at best achieve omega&lt;2.3725, very close to our current bounds. There is still hope to get an O(n^{2.17}) time algorithm by using the full power of the Universal Method. This would be a breakthrough in MM algorithms research and could have enormous practical implications.</p>\n<p>Beyond unraveling the mysteries of MM and omega, the project also focused on finding new interesting applications of MM. The PI and her coauthors uncovered many new applications of MM leading to the best algorithms to date for a large variety of problems. Several applications focused on shortest paths problems in graphs. For instance, imagine that you have a graph and two vertices s and t and you want to find the shortest path from s to t. Unfortunately, the graph is unstable, and some connections can fail. E.g. in a routing network, a traffic accident could make a route segment unavailable. The goal is now to find a shortest path from s to t, but preparing for any failed edge in the graph. We want to have an optimal way to reroute, no matter what route segment fails. The PI and coauthors (2021, 2022) obtained the state-of-the-art algorithms for this problem and extensions.</p>\n<p>Many other MM-based algorithms for graph problems were uncovered: for a large variety of small weight variants of the all-pairs shortest paths problem (2021), finding cycles in directed graphs (2018), listing lowest common ancestors in directed acyclic graphs (2022), and dynamic all-pairs shortest paths (2021). MM-based algorithms for several non-graph problems were also developed. One such problem is a mathematical formalization of RNA-folding: given a string of letters describing an RNA sequence, what is the best chemical folding of the sequence (under a natural notion of ``best??). The PI and coauthors (2019) obtained the first algorithm whose running time is sub-cubic in the length of the RNA sequence. Another example is from social choice (2021): a new algorithm for computing all winners of a well-known voting method, the Schulze Voting Rule.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/18/2022<br>\n\t\t\t\t\tModified by: Virginia&nbsp;V&nbsp;Williams</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMatrix Multiplication is one of the most basic algebraic operations outside basic arithmetic. Matrices are 2-dimensional arrays of numbers. The product C of two n by n matrices A and B is also an n by n matrix. The (i,j) entry of C, C[i,j] is defined mathematically using the entries of A in its i-th row, and the entries of B in its j-th column, as C[i,j] = A[i,1]*B[1,j]+ A[i,2]*B[2,j]+?+A[i,n]*B[n,j].\n\nThus, each entry of C can be computed by performing n products and n-1 additions of numbers, giving 2n-1 arithmetic operations. Altogether, all n^2 entries of C can be computed using 2n^3-n^2 operations, i.e. in O(n^3) time.\n\nThis is the ?brute-force? algorithm for matrix multiplication. In 1969, Strassen surprised everyone by showing that there is an asymptotically faster algorithm, using only O(n^{2.81}) operations. We use the constant ?omega? to be the smallest constant such that one can multiply n by n matrices using O(n^omega) operations. We know from Strassen that omega&lt;2.81. Since C has n^2 entries, we need at least n^2 operations, and so omega&gt;=2.\n\nOne of the most important unanswered questions in computer algorithms is, what is omega? If omega=2, then there would be an optimal algorithm.\n\nWhy do we want to study matrix multiplication (MM)? MM is at the heart of a wide variety of algorithmic problems. Any time a change in coordinates is needed (e.g. in computer animation or finite element simulations) one needs to perform MM. Finding patterns in networks, be it from biology, the internet, or the social sciences, necessitate MM. MM is everywhere.\n\nAfter Strassen?s discovery that the brute force algorithm is not optimal, a long line of research led to the development of more and more complex, and asymptotically faster algorithms for MM. In 1986 Coppersmith and Winograd showed that omega&lt;2.376. More than two decades later, Stothers (2010) and the PI (2012) gave slight improvements, showing that omega&lt;2.37288. Le Gall (2014) made another modest improvement to omega&lt;2.37287. As part of this Career grant, the PI and Alman (2021) developed new techniques and showed that omega&lt;2.37286. This is the current state of the art.\n\nSince 2010 the improvements to omega have been tiny. Why is this? As part of this grant, the PI and Alman (2018, 2019) gave a compelling reason. The toolbox that all algorithms so far can be formalized and generalized as the so called ``Universal Method?? for MM algorithms. The PI and Alman showed that this method cannot produce an optimal algorithm! The best one can potentially get is omega&lt;2.16. The Universal Method is a vast generalization of the methods used over the years. The less general Laser Method that still covers the current techniques, is even less powerful, as shown by Ambainis, Filmus and Le Gall (2015), and can at best achieve omega&lt;2.3725, very close to our current bounds. There is still hope to get an O(n^{2.17}) time algorithm by using the full power of the Universal Method. This would be a breakthrough in MM algorithms research and could have enormous practical implications.\n\nBeyond unraveling the mysteries of MM and omega, the project also focused on finding new interesting applications of MM. The PI and her coauthors uncovered many new applications of MM leading to the best algorithms to date for a large variety of problems. Several applications focused on shortest paths problems in graphs. For instance, imagine that you have a graph and two vertices s and t and you want to find the shortest path from s to t. Unfortunately, the graph is unstable, and some connections can fail. E.g. in a routing network, a traffic accident could make a route segment unavailable. The goal is now to find a shortest path from s to t, but preparing for any failed edge in the graph. We want to have an optimal way to reroute, no matter what route segment fails. The PI and coauthors (2021, 2022) obtained the state-of-the-art algorithms for this problem and extensions.\n\nMany other MM-based algorithms for graph problems were uncovered: for a large variety of small weight variants of the all-pairs shortest paths problem (2021), finding cycles in directed graphs (2018), listing lowest common ancestors in directed acyclic graphs (2022), and dynamic all-pairs shortest paths (2021). MM-based algorithms for several non-graph problems were also developed. One such problem is a mathematical formalization of RNA-folding: given a string of letters describing an RNA sequence, what is the best chemical folding of the sequence (under a natural notion of ``best??). The PI and coauthors (2019) obtained the first algorithm whose running time is sub-cubic in the length of the RNA sequence. Another example is from social choice (2021): a new algorithm for computing all winners of a well-known voting method, the Schulze Voting Rule.\n\n\t\t\t\t\tLast Modified: 08/18/2022\n\n\t\t\t\t\tSubmitted by: Virginia V Williams"
 }
}