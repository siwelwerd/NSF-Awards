{
 "awd_id": "1725543",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: Collaborative Research: Harnessing the Power of High-Bandwidth Memory via Provably Efficient Parallel Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2017-09-15",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2017-09-13",
 "awd_max_amd_letter_date": "2017-09-13",
 "awd_abstract_narration": "An important bottleneck for many parallel scientific applications is memory performance.  Recently, vendors have introduced a new memory called high-bandwidth memory (HBM) as an approach to alleviate this bottleneck.  This project will develop an algorithmic foundation for using HBM.  The project has the potential for a broad economic, technological, and scientific impact, since industry has an investment in this technology and many of the nation's strategic codes are being run on HBM-capable machines.  The PIs will integrate research with education at the graduate and undergraduate levels by training PhD, MS, and honors program BS students in cross-cutting issues encompassing algorithm design, high-performance software, and processor architecture. \r\n \r\nThe new approach offered by vendors is to bond memory (HBM) directly to the processor chip, which allows for more connections, enabling higher bandwidth.  Although the size of the new memory is larger than modern on-chip caches, physical constraints limit the capacity of the memory to be significantly smaller than DRAM.  HBM does not cleanly fit in the standard memory hierarchy.  This project will develop a foundational understanding of how to algorithmically design codes for HBM enhanced architectures. Overcoming these intellectual challenges to achieve multi-core scalability using HBM requires new algorithms, models, and abstractions, spearheaded by this collaboration between researchers who study hardware issues, high performance computing challenges, and theoretical modeling and analysis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Ferdman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Ferdman",
   "pi_email_addr": "mferdman@cs.stonybrook.edu",
   "nsf_id": "000634656",
   "pi_start_date": "2017-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Bender",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Michael A Bender",
   "pi_email_addr": "bender@cs.stonybrook.edu",
   "nsf_id": "000092778",
   "pi_start_date": "2017-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117794000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project developed an algorithmic foundation for managing high-bandwidth memory (HBM). &nbsp;HBM is a new type of memory technology that has higher bandwidth than traditional DRAM, but similar latency. &nbsp;HBM's bandwidth is so high because it is placed directly onto the processor package (unlike DRAM). &nbsp;HBM thus augments the existing memory hierarchy by providing a memory that can be accessed with up to 5x higher bandwidth than DDR4 (today's DRAM technology) when feeding a CPU, and up to 20x higher bandwidth when feeding a GPU.</p>\n<p dir=\"ltr\"><br />The challenge with modeling HBM is that it does not cleanly fit in the standard memory hierarchy. &nbsp;This project developed a foundational understanding of how to algorithmically design codes for HBM enhanced architectures.&nbsp; Specifically, the project developed new algorithms, models, and abstractions, as a result of a tightly coupled collaboration between researchers who study hardware issues, HPC challenges, and theoretical modeling and analysis.&nbsp; Among our key findings is the fact that automatic HBM management is indeed possible in multicore systems. Among the successes from this project were a series of papers that show the theoretical underpinning of how to manage high-bandwidth memory automatically, establish tight bounds for parallel paging, and desmonstrate algorithms for online parallel paging with optimal makespan.&nbsp; Our works have shown how to manage HBM automatically, as efficiently as a standard cache, and with provably good performance guarantees.&nbsp;&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/18/2022<br>\n\t\t\t\t\tModified by: Michael&nbsp;Ferdman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project developed an algorithmic foundation for managing high-bandwidth memory (HBM).  HBM is a new type of memory technology that has higher bandwidth than traditional DRAM, but similar latency.  HBM's bandwidth is so high because it is placed directly onto the processor package (unlike DRAM).  HBM thus augments the existing memory hierarchy by providing a memory that can be accessed with up to 5x higher bandwidth than DDR4 (today's DRAM technology) when feeding a CPU, and up to 20x higher bandwidth when feeding a GPU.\n\nThe challenge with modeling HBM is that it does not cleanly fit in the standard memory hierarchy.  This project developed a foundational understanding of how to algorithmically design codes for HBM enhanced architectures.  Specifically, the project developed new algorithms, models, and abstractions, as a result of a tightly coupled collaboration between researchers who study hardware issues, HPC challenges, and theoretical modeling and analysis.  Among our key findings is the fact that automatic HBM management is indeed possible in multicore systems. Among the successes from this project were a series of papers that show the theoretical underpinning of how to manage high-bandwidth memory automatically, establish tight bounds for parallel paging, and desmonstrate algorithms for online parallel paging with optimal makespan.  Our works have shown how to manage HBM automatically, as efficiently as a standard cache, and with provably good performance guarantees.  \n\n\t\t\t\t\tLast Modified: 12/18/2022\n\n\t\t\t\t\tSubmitted by: Michael Ferdman"
 }
}