{
 "awd_id": "1718846",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Linguistic Structure in Neural Sequence Models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 395002.0,
 "awd_amount": 395002.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2017-07-27",
 "awd_abstract_narration": "Over the past 25 years, the field of artificial intelligence has made great strides in the ability to automatically analyze and generate sequential data.  Much of this progress has come by building probabilistic models.  For example, mathematical descriptions of how words are typically used in context are based on a scientific understanding of the relationships among letters, sounds, words, and phrases, thanks to the field of linguistics.  Probabilistic models based on this understanding have allowed us to develop computational, data-driven methods for reasoning about the likely structure and meaning of sentences.  In the same way, probabilistic models of sequences of events have led to computational methods for predicting the unfolding of future events and reconstructing the ordering of past ones.  This project starts with sophisticated probabilistic models of linguistic structure and event sequences, and aims to make them more powerful, by using \"deep learning\" (neural networks) to increase their sensitivity to contextual effects.  Deep learning has already recently had a revolutionary impact on artificial intelligence.  This research will focus on using deep learning to enhance probabilistic models in settings where the model must discover structure that is not provided in its training data, such as the compositional units of language or the causal relations among events.\r\n\r\nThe planned model design will not focus on hand-engineered features, but rather on broad representational choices.  The overall architectures are motivated by certain basic notions that linguists and other modelers have found indispensable in their analyses of empirical data as follows: (1) stick-breaking processes that respect duality of patterning, the linguistic notion that a word's internal form is not necessarily related to its external usage but is governed by separate rules or by chance; (2) finite-state transducers that can capture local editing that transforms an input sequence into an output sequence; (3) context-free grammars that can model hierarchical structure to help explain word sequences; and (4) temporal point processes that can capture process intensity, where different events are competing to occur next, and combinations of earlier events combine to elevate or suppress the rates of later events.  The project will infuse these probabilistic techniques with recurrent neural networks, in particular, long short-term memory (LSTM) networks.  In some cases, exact inference in the resulting models will not be tractable, necessitating the design of Monte Carlo or variational approximations.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Eisner",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Jason M Eisner",
   "pi_email_addr": "jason@cs.jhu.edu",
   "nsf_id": "000185365",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N CHARLES ST",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 395002.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>AI systems achieve intelligent behavior by manipulating data structures that represent inputs, outputs, intermediate steps, and background knowledge.&nbsp; AI has traditionally relied on carefully designed discrete representations.&nbsp; However, many recent AI systems have abandoned representation <em>design</em>.&nbsp; Instead they rely on neural networks, which learn uninterpretable continuous representations that somehow manage to get the job done.</p>\n<p>In this project, we investigated some of the ways in which AI systems might <em>combine</em> neural networks with knowledgeable design.&nbsp; Our overarching goal was to allow a human designer to equip their AI system with an appropriate understanding of the data domain -- enabling the system to generalize correctly from a small training dataset.</p>\n<p>We built several specific neural-network probability models of natural language data.&nbsp; Our models were able to make better predictions because they were aware that sentences are built out of meaningful units (phrases, words, stems, suffixes, etc.), which in turn are built out of even smaller units (letters, phonemes, punctuation).&nbsp; Because the models know that the units of a language tend to work together as a coherent system, they can use observations of some of the units to successfully predict properties of other units of the same language, and to analyze specific sentences in the language.</p>\n<p>We also developed more general frameworks to allow data scientists to build models of their sequence data:</p>\n<ul>\n<li>We developed a discipline whereby anyone can design a specialized model of event sequences.&nbsp; The designer writes rules that specify how each event modifies a database of facts.&nbsp; A neural network then predicts future events based on the facts currently in the database and the rules that put them there.</li>\n<li>We developed a discipline whereby anyone can design a specialized model that maps one sequence of symbols to another.&nbsp; The designer writes regular expressions that specify what sequences are allowed and how they can be aligned.&nbsp; A neural network then scores whether a given aligned output sequence is likely, given the input sequence.</li>\n</ul>\n<p>Our modeling approaches can be used to help automate linguistic analysis as well as to improve the technology of natural language processing, behavioral modeling, and educational software.</p>\n<p>This project also contributed to the training of a strong cohort of researchers who are sophisticated at modeling natural language and other sequential data.&nbsp; The project directly trained 10 PhD students and 4 pre-PhD students, all of whom have continued in the field.&nbsp; 5 of the PhD students (so far) have completed their dissertations using work done under this project.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/15/2022<br>\n\t\t\t\t\tModified by: Jason&nbsp;M&nbsp;Eisner</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAI systems achieve intelligent behavior by manipulating data structures that represent inputs, outputs, intermediate steps, and background knowledge.  AI has traditionally relied on carefully designed discrete representations.  However, many recent AI systems have abandoned representation design.  Instead they rely on neural networks, which learn uninterpretable continuous representations that somehow manage to get the job done.\n\nIn this project, we investigated some of the ways in which AI systems might combine neural networks with knowledgeable design.  Our overarching goal was to allow a human designer to equip their AI system with an appropriate understanding of the data domain -- enabling the system to generalize correctly from a small training dataset.\n\nWe built several specific neural-network probability models of natural language data.  Our models were able to make better predictions because they were aware that sentences are built out of meaningful units (phrases, words, stems, suffixes, etc.), which in turn are built out of even smaller units (letters, phonemes, punctuation).  Because the models know that the units of a language tend to work together as a coherent system, they can use observations of some of the units to successfully predict properties of other units of the same language, and to analyze specific sentences in the language.\n\nWe also developed more general frameworks to allow data scientists to build models of their sequence data:\n\nWe developed a discipline whereby anyone can design a specialized model of event sequences.  The designer writes rules that specify how each event modifies a database of facts.  A neural network then predicts future events based on the facts currently in the database and the rules that put them there.\nWe developed a discipline whereby anyone can design a specialized model that maps one sequence of symbols to another.  The designer writes regular expressions that specify what sequences are allowed and how they can be aligned.  A neural network then scores whether a given aligned output sequence is likely, given the input sequence.\n\n\nOur modeling approaches can be used to help automate linguistic analysis as well as to improve the technology of natural language processing, behavioral modeling, and educational software.\n\nThis project also contributed to the training of a strong cohort of researchers who are sophisticated at modeling natural language and other sequential data.  The project directly trained 10 PhD students and 4 pre-PhD students, all of whom have continued in the field.  5 of the PhD students (so far) have completed their dissertations using work done under this project.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 08/15/2022\n\n\t\t\t\t\tSubmitted by: Jason M Eisner"
 }
}