{
 "awd_id": "1703319",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Closed Loop Perceptual Planning for Dynamic Locomotion",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 320000.0,
 "awd_amount": 320000.0,
 "awd_min_amd_letter_date": "2017-08-13",
 "awd_max_amd_letter_date": "2017-09-07",
 "awd_abstract_narration": "Modern robots can be seen moving about a variety of terrains and environments, using wheels, legs, and other means, engaging in life-like hopping, jumping, walking, crawling, and running. They execute motions called gaits. An example of a gait is a horse trotting or galloping. Likewise, humans execute walking, running and skipping gaits. Essentially, for either a biological or mechanical systems, a gait is a locomotion pattern that involves large-amplitude body oscillations. Naturally, these motions cause impacts with terrain that jostle on-board perceptual systems and directly influence what the robots actually \"see\" as they move.  For instance, the body motion of a bounding horse-like robot may result in significant occlusions and oscillations in on-board camera systems that confound motion estimation and perceptual feedback.\r\n\r\nFocusing on complex mobility robots, this project seeks to better understand the coupling between locomotion and visual perception to improve perceptual feedback for closed-loop motion estimation. The work is organized around two key questions: 1) How should a robot look to move well? 2) How should a robot move to see well? To address the first challenge, the periodic structure of gait-based motions will be leveraged to improve perceptual filtering as the robot carries out fixed (pre-determined) motions.  The second half of the project will derive perceptual objectives and a new perceptual gait design framework to guide how high degree-of-freedom, complex mobility robots should move (locomote).  The goal is to optimize feedback for closed-loop motion implementation, on-line adaptation, and learning, which are currently difficult or impossible for many complex mobility robots.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kostas",
   "pi_last_name": "Daniilidis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kostas Daniilidis",
   "pi_email_addr": "kostas@cis.upenn.edu",
   "nsf_id": "000207772",
   "pi_start_date": "2017-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": "3330 Walnut Street",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191043409",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 320000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-5892137d-7fff-a1f9-3881-a20bbefdedcf\"> </span></p>\n<p dir=\"ltr\"><span>Robot perception and locomotion can highly benefit from studying how effortlessly biological species see and move through a complex world. A key insight from biology is the efficiency of sensing and control in terms of bandwidth and energy. Visual stimuli leave the eye as spike trains and information propagates from neuron to neuron with spikes at irregular time intervals. Cameras producing such spikes (events) instead of frames are now available, and this project was about the study of perception for locomotion from such event inputs.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The foundation for the perception of self-motion is the optical flow, the apparent motion of the projection of a real scene on an eye or camera. The first outcome of the project was a neural network that would take a temporal window of events as input and output optical flow via an encoder-decoder architecture. The network is trained by optimizing a loss function that represents motion blur of events after the events are warped with the predicted flow values. Optical flow prediction was extensively tested in indoor and outdoor sequences from event cameras mounted on ground robots, cars, motorcycles, and drones, and under several weather conditions and seasons or times during the day. The network extended to predict the time-to-collision to objects and predict the heading direction of the moving camera. For locomotion scenarios, the output of the network was modified to predict the parallax from the terrain, namely, the extrusions from a virtual plane, modeling, thus, not only obstacles but any abnormalities of a rough terrain.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Finally, first steps were taken to move from graphics processor convolutional architectures to spiking networks. A leaky integrate-and-fire neuron model was used in a shallow architecture to produce the contribution of flow in eight different directions mimicking motion direction selective neurons. The firing rate of these neurons was then used to compose a self-supervised loss function representing the motion blur.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Results of this project will have an impact on computational neuroscience because neuron dynamics applied were tested on real event data on very long sequences. When spiking processors with low power consumption will be available, such optical flow systems could be deployed on extremely light aerial vehicles or swarm robots on air or on the ground.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/10/2022<br>\n\t\t\t\t\tModified by: Kostas&nbsp;Daniilidis</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1703319/1703319_10513970_1646946232926_IMG_0464--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1703319/1703319_10513970_1646946232926_IMG_0464--rgov-800width.jpg\" title=\"RHex with Dynamic Vision Sensor\"><img src=\"/por/images/Reports/POR/2022/1703319/1703319_10513970_1646946232926_IMG_0464--rgov-66x44.jpg\" alt=\"RHex with Dynamic Vision Sensor\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Bioinspired hexapedal robot designed for locomotion on hough terrain equipped with an event-based Dynamic Vision Sensor.</div>\n<div class=\"imageCredit\">Kostas Daniilidis</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Kostas&nbsp;Daniilidis</div>\n<div class=\"imageTitle\">RHex with Dynamic Vision Sensor</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nRobot perception and locomotion can highly benefit from studying how effortlessly biological species see and move through a complex world. A key insight from biology is the efficiency of sensing and control in terms of bandwidth and energy. Visual stimuli leave the eye as spike trains and information propagates from neuron to neuron with spikes at irregular time intervals. Cameras producing such spikes (events) instead of frames are now available, and this project was about the study of perception for locomotion from such event inputs. \nThe foundation for the perception of self-motion is the optical flow, the apparent motion of the projection of a real scene on an eye or camera. The first outcome of the project was a neural network that would take a temporal window of events as input and output optical flow via an encoder-decoder architecture. The network is trained by optimizing a loss function that represents motion blur of events after the events are warped with the predicted flow values. Optical flow prediction was extensively tested in indoor and outdoor sequences from event cameras mounted on ground robots, cars, motorcycles, and drones, and under several weather conditions and seasons or times during the day. The network extended to predict the time-to-collision to objects and predict the heading direction of the moving camera. For locomotion scenarios, the output of the network was modified to predict the parallax from the terrain, namely, the extrusions from a virtual plane, modeling, thus, not only obstacles but any abnormalities of a rough terrain. \nFinally, first steps were taken to move from graphics processor convolutional architectures to spiking networks. A leaky integrate-and-fire neuron model was used in a shallow architecture to produce the contribution of flow in eight different directions mimicking motion direction selective neurons. The firing rate of these neurons was then used to compose a self-supervised loss function representing the motion blur. \nResults of this project will have an impact on computational neuroscience because neuron dynamics applied were tested on real event data on very long sequences. When spiking processors with low power consumption will be available, such optical flow systems could be deployed on extremely light aerial vehicles or swarm robots on air or on the ground.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 03/10/2022\n\n\t\t\t\t\tSubmitted by: Kostas Daniilidis"
 }
}