{
 "awd_id": "1725734",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: Secure, Highly-Parallel Training of Deep Neural Networks in the Cloud Using General-Purpose  Shared-Memory Platforms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2017-08-30",
 "awd_max_amd_letter_date": "2017-08-30",
 "awd_abstract_narration": "Society is beginning to witness an explosion in the use of Deep Neural Networks (DNNs), with\u00a0major impacts on many facets of human life, including health, finances, family life, and\u00a0entertainment. To train DNNs, practitioners have preferred to use GPUs and, recently,\u00a0specialized hardware accelerators.\u00a0 Despite constituting the bulk of a data center?s compute\u00a0resources, general-purpose shared-memory multiprocessors have been regarded as unattractive\u00a0platforms. In this project, the Principal Investigators (PIs) think that these platforms\u00a0have high potential. Consequently, this project will develop new techniques to dramatically\u00a0improve shared-memory multiprocessor performance in training DNNs.\u00a0 Already, shared-memory\u00a0servers are compelling for several reasons: they can support a high-degree of parallelism,\u00a0are general-purpose and easy to program, and provide flexible, fine-grain inter-core\u00a0communication.\u00a0 However, efficiently using shared-memory servers to train DNNs imposes\u00a0\r\nsignificant challenges. First, fine-grain synchronization is still expensive, and latencies\u00a0are non-trivial. In addition, when DNN training moves to an environment with multiple users\u00a0sharing the same physical shared-memory platform in the cloud, privacy and integrity become\u00a0major concerns.\r\n\r\nTo overcome these challenges, this project will synergistically address architecture and\u00a0security issues.\u00a0 On the architecture side, it will augment a highly-parallel shared-memory\u00a0server with support for synchronization, data movement, data sharing, and DNN sparsity\u00a0structuring.\u00a0 On the security side, it will investigate how shared-memory servers create\u00a0novel privacy and integrity threats (for example, leaking the DNN?s sparse structure and\u00a0forcing incorrect model generation), and how to defend against those threats.\u00a0 The project?s\u00a0broader impact is to help enable ?neural network training for everyone,? by making a\u00a0ubiquitous and easy-to-program platform a viable and safe target for running these\u00a0important, emerging workloads.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Josep",
   "pi_last_name": "Torrellas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Josep Torrellas",
   "pi_email_addr": "torrellas@cs.uiuc.edu",
   "nsf_id": "000488177",
   "pi_start_date": "2017-08-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Fletcher",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher Fletcher",
   "pi_email_addr": "cwfletch@illinois.edu",
   "nsf_id": "000743178",
   "pi_start_date": "2017-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618207473",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this proposal, the Principal Investigators (PIs) have focused on designinghigh-performance and secure general-purpose computers for performing deep&nbsp;neural network (DNN) training. General-purpose computers such as CPUs haveseveral advantages: they are easy to program, exhibit a high degree of&nbsp;parallelism, and support flexible, fine-grain communication between cores.&nbsp;Hence, applications can be written with many irregular tasks synchronizing&nbsp;at fine granularity.</p>\n<p>The technical outcome of this work has been the design of two mechanisms to&nbsp;speed-up the training of DNNs on processors, and the discovery of three attacks&nbsp;on DNN training (and the outline of possible defenses).&nbsp;</p>\n<p>The PIs speed-up the execution of DNN training by skipping operations when&nbsp;there are zeros involved in the computations. The presence of zeros, called&nbsp;Sparsity, is very common, and such zeros cause operations to be ineffectual(that is, to have no effect). The PIs have designed changes to the DNN training&nbsp;algorithms that skip such ineffectual operations. They have also designed new&nbsp;processor hardware that, automatically and without requiring any program change,&nbsp;skip ineffectual operations.</p>\n<p>The attacks discovered by the PIs operate as follows. In one attack, calledCache Telepathy, the attacker observes the cache hierarchy use during the&nbsp;training operation, and is able to obtain the architecture (shape and other&nbsp;parameters) of the DNN. In the second attack, called Game of Threads, theattacker changes the scheduling of the threads that participate in thetraining of a DNN. With this technique, the attacker is able to change the&nbsp;training outcome, resulting in either a degradation of the training accuracy&nbsp;or an incorrect training outcome. Finally, in the third attack, calledMicroScope, the attacker is able to repeatedly re-execute the same instructionsof the victim program without the victim noticing it. As a result, the attackercan fully characterize the instructions executed and, therefore, leak&nbsp;information about the program.</p>\n<p>This work has been done in close collaboration with researchers at Intel&nbsp;Corporation. It is hoped that the designs and attacks/defenses proposed willhave an impact on Intel products. In addition, the work has funded 8 graduatestudents, of which two already completed their PhD.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/24/2021<br>\n\t\t\t\t\tModified by: Josep&nbsp;Torrellas</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this proposal, the Principal Investigators (PIs) have focused on designinghigh-performance and secure general-purpose computers for performing deep neural network (DNN) training. General-purpose computers such as CPUs haveseveral advantages: they are easy to program, exhibit a high degree of parallelism, and support flexible, fine-grain communication between cores. Hence, applications can be written with many irregular tasks synchronizing at fine granularity.\n\nThe technical outcome of this work has been the design of two mechanisms to speed-up the training of DNNs on processors, and the discovery of three attacks on DNN training (and the outline of possible defenses). \n\nThe PIs speed-up the execution of DNN training by skipping operations when there are zeros involved in the computations. The presence of zeros, called Sparsity, is very common, and such zeros cause operations to be ineffectual(that is, to have no effect). The PIs have designed changes to the DNN training algorithms that skip such ineffectual operations. They have also designed new processor hardware that, automatically and without requiring any program change, skip ineffectual operations.\n\nThe attacks discovered by the PIs operate as follows. In one attack, calledCache Telepathy, the attacker observes the cache hierarchy use during the training operation, and is able to obtain the architecture (shape and other parameters) of the DNN. In the second attack, called Game of Threads, theattacker changes the scheduling of the threads that participate in thetraining of a DNN. With this technique, the attacker is able to change the training outcome, resulting in either a degradation of the training accuracy or an incorrect training outcome. Finally, in the third attack, calledMicroScope, the attacker is able to repeatedly re-execute the same instructionsof the victim program without the victim noticing it. As a result, the attackercan fully characterize the instructions executed and, therefore, leak information about the program.\n\nThis work has been done in close collaboration with researchers at Intel Corporation. It is hoped that the designs and attacks/defenses proposed willhave an impact on Intel products. In addition, the work has funded 8 graduatestudents, of which two already completed their PhD.\n\n\t\t\t\t\tLast Modified: 01/24/2021\n\n\t\t\t\t\tSubmitted by: Josep Torrellas"
 }
}