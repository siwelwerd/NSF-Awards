{
 "awd_id": "1733701",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AitF: Collaborative Research: A Framework of Simultaneous Acceleration and Storage Reduction on Deep Neural Networks Using Structured Matrices",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2017-09-15",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 348037.0,
 "awd_amount": 348037.0,
 "awd_min_amd_letter_date": "2017-09-13",
 "awd_max_amd_letter_date": "2017-09-13",
 "awd_abstract_narration": "\u00a0 \u00a0 \u00a0\u00a0Deep neural networks (DNNs) have emerged as a class of powerful techniques for learning solutions in a number of challenging problem domains, including computer vision, natural language processing and bioinformatics.  These solutions have been enabled mainly because we now have computational accelerators able to sift though the myriad of data required to train a neural network.   As the size of DNN models continues to grow, computational and memory resource requirements for training will also grow, limiting deployment of deep learning in many practical applications. \r\n\r\n      Leveraging the theory of structured matrices, this project will develop a general framework for efficient DNN training and inference, providing a significant reduction in algorithmic complexity measures in terms of both computation and storage.  \r\nThe project, if successful, should fundamentally impact a broad class of deep learning applications.  It will explore accelerating this new structure for deep learning algorithms targeting emerging accelerator architectures, and will evaluate the benefits of these advances across a number of application domains, including big data analytics, cognitive systems, unmanned vehicles and aerial systems, and wearable devices.  The interdisciplinary nature of this project bridges the areas of matrix theory, machine learning, and computer architecture, and will affect education at both Northeastern and CCNY, including the involvement of underrepresented and undergraduate students in the rich array of research tasks.\r\n\r\n\u00a0 \u00a0 \u00a0The project will: (1) for the first time, develop a general theoretical framework for structured matrix-based DNN models and perform detailed analysis and investigation of error bounds, convergence, fast training algorithms, etc.; (2) develop low-space-cost and high-speed inference and training schemes for the fully connected layers of DNNs; (3) impose a weight tensor with structure and enable low computational and space cost convolutional layers; (4) develop high-performance and energy-efficient implementations of deep learning systems on high-performance parallel platforms, low-power embedded platforms, as well as emerging computing paradigms and devices; (5) perform a comprehensive evaluation of the proposed approaches on different performance metrics in a variety of platforms.\u00a0\u00a0The project will deliver tuned implementations targeting a range of computational platforms, including ASICs, FPGAs, GPUs and cloud servers. The hardware optimizations will focus on producing high-speed and low-cost implementations of deep learning systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xue",
   "pi_last_name": "Lin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xue Lin",
   "pi_email_addr": "xuelin@coe.neu.edu",
   "nsf_id": "000724544",
   "pi_start_date": "2017-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Kaeli",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "David R Kaeli",
   "pi_email_addr": "kaeli@ece.neu.edu",
   "nsf_id": "000179508",
   "pi_start_date": "2017-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "723900",
   "pgm_ele_name": "Algorithms in the Field"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 348037.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project mitigates the gap between ever-growing deep neural network (DNN) model size and computation amount, and available computing and memory resources in the execution devices. This project develops solutions to the challenging problem based on structured matrices. Deep neural network layers can be represented as weight matrices or tensors. If a neural network can be trained such that the weight matrices or tensors are in the format of structured matrices, both the memory storage and computation amount can be reduced, resulting in significant acceleration of both DNN training and inference. The proposed framework has been tested on both high-performance parallel platforms such as GPUs and FPGAs as well as low-power embedded and mobile platforms, demonstrating superior performance in both model accuracy and execution speed.&nbsp;</p>\n<div class=\"page\" title=\"Page 6\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>The proposed work will significantly advance the state-of-the-art on deep learning by proposing the first general and theoretically sound framework for reducing computational complexity of both inference and training, as well as significantly reducing weight storage space. It will provide a clear path to: (i) promoting wider adoption of low-power and low-complexity DNN systems in embedded systems and unmanned vehicles/aerial systems, and (ii) enabling further scale-up of DNN sizes by reducing the computational complexity.&nbsp;</span>The proposed research will enhance economic opportunities via solutions that support the wider adoption of deep learning systems for big data analytics, cognitive systems, unmanned vehicles and aerial systems, and embedded and wearable devices. The research project has published 19 conference papers, 2 journal papers, 1 patent, and design contest 1st place in ISLPED'20.</p>\n</div>\n</div>\n</div>\n</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/24/2021<br>\n\t\t\t\t\tModified by: Xue&nbsp;Lin</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project mitigates the gap between ever-growing deep neural network (DNN) model size and computation amount, and available computing and memory resources in the execution devices. This project develops solutions to the challenging problem based on structured matrices. Deep neural network layers can be represented as weight matrices or tensors. If a neural network can be trained such that the weight matrices or tensors are in the format of structured matrices, both the memory storage and computation amount can be reduced, resulting in significant acceleration of both DNN training and inference. The proposed framework has been tested on both high-performance parallel platforms such as GPUs and FPGAs as well as low-power embedded and mobile platforms, demonstrating superior performance in both model accuracy and execution speed. \n\n\n\n\n\nThe proposed work will significantly advance the state-of-the-art on deep learning by proposing the first general and theoretically sound framework for reducing computational complexity of both inference and training, as well as significantly reducing weight storage space. It will provide a clear path to: (i) promoting wider adoption of low-power and low-complexity DNN systems in embedded systems and unmanned vehicles/aerial systems, and (ii) enabling further scale-up of DNN sizes by reducing the computational complexity. The proposed research will enhance economic opportunities via solutions that support the wider adoption of deep learning systems for big data analytics, cognitive systems, unmanned vehicles and aerial systems, and embedded and wearable devices. The research project has published 19 conference papers, 2 journal papers, 1 patent, and design contest 1st place in ISLPED'20.\n\n\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/24/2021\n\n\t\t\t\t\tSubmitted by: Xue Lin"
 }
}