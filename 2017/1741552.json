{
 "awd_id": "1741552",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Small: Collaborative Research: Learning from Demonstration for Cloud Robotics",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Reid Simmons",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2018-09-30",
 "tot_intn_awd_amt": 218528.0,
 "awd_amount": 226480.0,
 "awd_min_amd_letter_date": "2017-05-08",
 "awd_max_amd_letter_date": "2017-05-08",
 "awd_abstract_narration": "The proposed work seeks to leverage cloud computing to enable robots to efficiently learn from remote human domain experts - \"Cloud Learning from Demonstration.\" Building on RobotsFor.Me, a remote robotics research lab, this research will unite Learning from Demonstration (LfD) and Cloud Robotics to enable anyone with Internet access to teach a robot household tasks. The value of this work stems from three aspects. First is the remote system that can learn task models from a series of remote demonstrations from a single user, focusing on learning high-level tasks as opposed to low-level motor skills. The second is the extension of learning from demonstration to multiple teachers. This represents an important relaxation of a limiting assumption to focus on evaluating teacher strengths and effectively handling distinct task solutions. Finally, transparency mechanisms to allow a remote user to develop a correct mental model about the robot?s learning process.\r\n\r\nThe long term goal of this research is to one day make personal robots accessible to everyday people. The interactive learning framework based on RobotsFor.Me provides unique opportunities for education and outreach. Thomaz and Chernova will outreach to K-12 teachers and students by creating an education portal surrounding RobotsFor.Me containing hands-on workshop curricula. This material will be integrated with the WPI Frontiers program for middle school students, and the GT ePDN professional education network for teachers. A key impact on students at GT and WPI will be direct involvement in this research agenda, and integration with AI, robotics and HRI courses. Chernova is the Diversity Coordinator in the Robotics Engineering Program, and faculty advisor for Women In Robotics Engineering and Women in Technology student groups which will enable braod exposure. Thomaz mentors the RoboWomen graduate women?s group. Software components will also be made available as open source and the PIs have a collaboration plan in place with researchers at Willow Garage, and through student internships will transfer technology to their labs.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sonia",
   "pi_last_name": "Chernova",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sonia Chernova",
   "pi_email_addr": "chernova@cc.gatech.edu",
   "nsf_id": "000083882",
   "pi_start_date": "2017-05-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "Office of Sponsored Programs",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320420",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 218528.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 7952.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p.p1 {margin: 0.0px 0.0px 10.0px 0.0px; line-height: 14.0px; font: 12.0px Arial; color: #000000} span.s1 {font-kerning: none} -->\n<p class=\"p1\"><span class=\"s1\">Service robots hold the promise of helping solve issues facing our society, ranging from eldercare to education. A critical issue is that we cannot preprogram these robots with every skill needed to play a useful role in society, they will need to acquire new relevant skills after they are deployed. The field of robot learning from demonstration aims to enable everyday people to expand robot capabilities through demonstrations of desired behavior instead of explicit programming.&nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">Prior LfD techniques have focused on teachers that are co-located with the robot. However, co-located users may not always be able to provide the data required for robust learning and operation. Users may have limited abilities to perform demonstrations due to a physical impairment. They may not have sufficient expertise to demonstrate the task, or the time to generate the diversity and number of examples that state of the art Machine Learning algorithms require to build general models of tasks. Or there may not be a co-located user at all, and the robot must be taught a new task remotely, e.g., while located in a fully automated factory or a hazardous site.</span></p>\n<p class=\"p1\"><span class=\"s1\">This project has advanced the state of the art through:</span></p>\n<p class=\"p1\"><span class=\"s1\">(i) The development of innovative remote teleoperation interfaces for complex robotic systems.&nbsp; The user's ability to quickly and effectively control a robot manipulator is key to learning from a remote user.&nbsp; Our work has introduced remote manipulation interfaces that are highly robust to high latency and limited bandwidth requirements, as commonly encountered in remote applications.&nbsp; Our interface is now being validated for use by NASA to control the free-flying AstroBee robot on the International Space Station.</span></p>\n<p class=\"p1\"><span class=\"s1\">(ii) The development of student-driven learning techniques for autonomous systems. Within the paradigm of learning from demonstration, the burden is typically placed on the user to determine what training data the robot needs.&nbsp; Our work has contributed techniques that enable robots to identify what to pay attention to when learning, as well as means for actively asking a wide range of questions that can help guide the learning process and make learning more efficient.&nbsp; Such techniques may be particularly useful in remote scenarios, where the user's situational awareness is more limited.&nbsp;&nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">(iii) The development of a novel learning representation that leverages object information and hierarchical task structures to improve the efficiency and scalability of exploration-based learning techniques.&nbsp;&nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">All of the above learning methods, although aimed at remote learning scenarios, can also be applied in co-located interactions.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/21/2019<br>\n\t\t\t\t\tModified by: Sonia&nbsp;Chernova</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nService robots hold the promise of helping solve issues facing our society, ranging from eldercare to education. A critical issue is that we cannot preprogram these robots with every skill needed to play a useful role in society, they will need to acquire new relevant skills after they are deployed. The field of robot learning from demonstration aims to enable everyday people to expand robot capabilities through demonstrations of desired behavior instead of explicit programming. \nPrior LfD techniques have focused on teachers that are co-located with the robot. However, co-located users may not always be able to provide the data required for robust learning and operation. Users may have limited abilities to perform demonstrations due to a physical impairment. They may not have sufficient expertise to demonstrate the task, or the time to generate the diversity and number of examples that state of the art Machine Learning algorithms require to build general models of tasks. Or there may not be a co-located user at all, and the robot must be taught a new task remotely, e.g., while located in a fully automated factory or a hazardous site.\nThis project has advanced the state of the art through:\n(i) The development of innovative remote teleoperation interfaces for complex robotic systems.  The user's ability to quickly and effectively control a robot manipulator is key to learning from a remote user.  Our work has introduced remote manipulation interfaces that are highly robust to high latency and limited bandwidth requirements, as commonly encountered in remote applications.  Our interface is now being validated for use by NASA to control the free-flying AstroBee robot on the International Space Station.\n(ii) The development of student-driven learning techniques for autonomous systems. Within the paradigm of learning from demonstration, the burden is typically placed on the user to determine what training data the robot needs.  Our work has contributed techniques that enable robots to identify what to pay attention to when learning, as well as means for actively asking a wide range of questions that can help guide the learning process and make learning more efficient.  Such techniques may be particularly useful in remote scenarios, where the user's situational awareness is more limited.  \n(iii) The development of a novel learning representation that leverages object information and hierarchical task structures to improve the efficiency and scalability of exploration-based learning techniques.  \nAll of the above learning methods, although aimed at remote learning scenarios, can also be applied in co-located interactions.\n\n \n\n\t\t\t\t\tLast Modified: 02/21/2019\n\n\t\t\t\t\tSubmitted by: Sonia Chernova"
 }
}