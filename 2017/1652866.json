{
 "awd_id": "1652866",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Designing Ultra-Energy-Efficient Intelligent Hardware with On-Chip Learning, Attention, and Inference",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2017-03-15",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 472195.0,
 "awd_amount": 472195.0,
 "awd_min_amd_letter_date": "2017-03-02",
 "awd_max_amd_letter_date": "2023-02-13",
 "awd_abstract_narration": "Building intelligent computers that can perform cognitive tasks (e.g., learning, recognition) as well as humans do has been a long-standing goal of computing research. State-of-the-art deep learning and neuromorphic algorithms have recently advanced the software performance for cognitive applications. However, such algorithms are computation-memory-communication intensive, which makes the hardware design challenging to perform low-power real-time training and classification on portable platforms. Furthermore, to optimize system-level power, efficient power delivery and supply voltage regulation of such large-scale hardware systems also becomes a critical concern. This project will address these challenges across multiple disciplines of hardware and software design, towards the overarching goal of building brain-inspired intelligent computing systems that are ultra-energy-efficient for various cognitive tasks in computer vision, speech, robotics and biomedical applications. The success of this research is likely to impact many user-centric computing systems in  society and industry, including wearable, mobile, and edge computing. This project also entails integrative education and outreach plans through a new interdisciplinary coursework development, undergraduate/graduate student training, and a summer outreach program for high school students.\r\n\r\nIn this project, energy-efficient circuits, architectures and algorithms will be designed to incorporate learning, attention and inference computations in area-/power-constrained mobile/wearable hardware platforms. The particular technologies that will be developed to achieve large improvement in energy-efficiency include: (1) computation redundancy minimization of state-of-the-art deep learning algorithms with bio-inspired attention models, (2) novel memory compression schemes that apply to both software and hardware implementation, (3) real-time on-chip learning methods that consume low power on mobile/wearable devices, (4) efficient on-chip voltage regulators that can adapt to abrupt changes in cognitive workloads, and (5) cross-layer optimization of circuit, architecture and algorithm. The outcomes of this research will feature new very-large-scale integration (VLSI) systems that can learn and perform cognitive tasks in real-time with superior power efficiency, opening up possibilities for ubiquitous intelligence in small-form-factor devices.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jae-sun",
   "pi_last_name": "Seo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jae-sun Seo",
   "pi_email_addr": "js3528@cornell.edu",
   "nsf_id": "000929203",
   "pi_start_date": "2017-03-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "PO Box 876011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852876011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 81238.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 100003.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 88060.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 107214.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 3601.0
  }
 ],
 "por": null
}