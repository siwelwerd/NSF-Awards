{
 "awd_id": "1717060",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: NeTS: Small: Theoretical Foundations for Cache Networks: Performance Models,  Algorithms, and Applications",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Darleen Fisher",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 300135.0,
 "awd_amount": 300135.0,
 "awd_min_amd_letter_date": "2017-07-19",
 "awd_max_amd_letter_date": "2018-09-18",
 "awd_abstract_narration": "Caching systems are a core component of Internet data infrastructures. They enable low-cost access to a fast, but limited cache space that stores a selective subset of popular data items drawn from a large collection of data records that are stored in slow, persistent media. Caching systems greatly improve the performance of various services in, for example, information retrieval, data analytics, social networks and e-commence. Caching systems are already widely deployed but need to scale efficiently to support emerging big data applications. To better serve multiple flows of data requests on a caching system, a fundamental question is whether the cache space should be pooled together to serve these flows jointly or be divided to serve them separately. While system-based approaches have yielded good intuition and first-order solutions, it is important that new theories be developed to provide a quantitative characterization of cache networks that can lead to optimal or near-optimal solutions. This project will develop a much-needed theoretical foundation for cache systems in emerging data processing systems, with concrete plans to transition the theoretical results into practical implementations. \r\n\r\nThe research will be carried across the following interrelated thrusts. (1) Characterizing miss ratios of competing flows on least-recently-used (LRU) caching: A unified theoretical framework will be developed to investigate critical factors that impact the cache miss ratios, including request rates, data popularities, item sizes, and overlapped data items across different request flows. The new insights will be used to directly improve the performance of real caching systems. (2) Optimizing data caching for server clusters: this thrust investigates whether servers should be pooled together or not, and how to optimize the sizes of different server clusters as well as where to route data requests for multiple caching clusters.  A joint optimization of data caching and job scheduling will also be addressed.  (3) Transition from theories into practice: This thrust will leverage open source projects, Memcached, Redis, Hadoop, Spark and Tachyon, to validate the theoretical results by real experiments, and to transition theories into working systems by adding new modules and modifying existing code.\r\n\r\nThis project may benefit society, industry and academia. The insights from the mathematical analysis and the new algorithms developed through this project are expected to play a key role in improving the performance of cache networks, for example, for in-memory key-value stores. They can contribute to both theoretical research and practical technologies. A specific focus will bridge the traditional separation between stochastic operations research and computer engineering education. The research results from this proposal will be integrated into a new graduate-level course. Other broader impacts include industry collaborations for practical use cases and technology transfer, undergraduate summer programs, strategies for engaging women and other under-represented groups, and the development of a strong research lab so that it is also a teaching lab.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ness",
   "pi_last_name": "Shroff",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ness Shroff",
   "pi_email_addr": "shroff@ece.osu.edu",
   "nsf_id": "000445637",
   "pi_start_date": "2018-09-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Jian",
   "pi_last_name": "Tan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jian Tan",
   "pi_email_addr": "tan.252@osu.edu",
   "nsf_id": "000726619",
   "pi_start_date": "2017-07-19",
   "pi_end_date": "2018-09-18"
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "2015 Neil Ave STE 205",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101272",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "736300",
   "pgm_ele_name": "Networking Technology and Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 300135.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Caching systems are already widely deployed but need to scale efficiently to support emerging big data applications.&nbsp; A fundamental question is how to efficiently organize the cache space and optimize the caching algorithms.</span></p>\n<p>As part of this project we solved the following open problems.&nbsp;</p>\n<p>&nbsp;</p>\n<p><span>Strategically prefetching data has been utilized in practice to improve caching performance. Apart from caching data items upon requests, they can be prefetched into the cache before requests actually occur. The caching and prefetching operations compete for the limited cache space, whose size is typically much smaller than the number of data items. A key question is how to design an optimal prefetching and caching policy, assuming that the future requests can be predicted to certain extend. This question is non-trivial even under an idealized assumption that the future requests are precisely known. In this project we investigated this problem and developed&nbsp; a low complexity provably efficient solution that only exploits predictions in the near future. Moreover, this&nbsp; &nbsp;policy can be applied in real time.&nbsp;</span></p>\n<p><span>Caching at the edge is becoming increasingly prevalent with the dramatic increases in data demands for end users coupled with ever reducing memory costs. Caching at the edge is substantially different from that at the core and needs to take into account personalized data demands. For example, an individual user may not be interested in requesting the same data item again, if he/she has already recently requested it. Such individualized dynamics are not apparent in the aggregated data requests at the cores and have not been considered in the popularity-driven caching designs for the core. Hence, these traditional caching policies designed for the core could induce significant inefficiency when applied at the edges. To address this issue w new edge caching policies optimized for the individualized demands by leveraging overhearing opportunities at the wireless edges have been developed as part of this project. Further, new online learning mechanisms have been developed to efficiently cache content in edge caching systems.&nbsp;</span></p>\n<p><span><br /></span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/30/2022<br>\n\t\t\t\t\tModified by: Ness&nbsp;Shroff</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nCaching systems are already widely deployed but need to scale efficiently to support emerging big data applications.  A fundamental question is how to efficiently organize the cache space and optimize the caching algorithms.\n\nAs part of this project we solved the following open problems. \n\n \n\nStrategically prefetching data has been utilized in practice to improve caching performance. Apart from caching data items upon requests, they can be prefetched into the cache before requests actually occur. The caching and prefetching operations compete for the limited cache space, whose size is typically much smaller than the number of data items. A key question is how to design an optimal prefetching and caching policy, assuming that the future requests can be predicted to certain extend. This question is non-trivial even under an idealized assumption that the future requests are precisely known. In this project we investigated this problem and developed  a low complexity provably efficient solution that only exploits predictions in the near future. Moreover, this   policy can be applied in real time. \n\nCaching at the edge is becoming increasingly prevalent with the dramatic increases in data demands for end users coupled with ever reducing memory costs. Caching at the edge is substantially different from that at the core and needs to take into account personalized data demands. For example, an individual user may not be interested in requesting the same data item again, if he/she has already recently requested it. Such individualized dynamics are not apparent in the aggregated data requests at the cores and have not been considered in the popularity-driven caching designs for the core. Hence, these traditional caching policies designed for the core could induce significant inefficiency when applied at the edges. To address this issue w new edge caching policies optimized for the individualized demands by leveraging overhearing opportunities at the wireless edges have been developed as part of this project. Further, new online learning mechanisms have been developed to efficiently cache content in edge caching systems. \n\n\n\n\n\t\t\t\t\tLast Modified: 07/30/2022\n\n\t\t\t\t\tSubmitted by: Ness Shroff"
 }
}