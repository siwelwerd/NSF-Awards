{
 "awd_id": "1718160",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: SMALL: Virtualized Accelerators for Scalable, Composable Architectures",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922594",
 "po_email": "kkaravan@nsf.gov",
 "po_sign_block_name": "Karen Karavanic",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2017-07-14",
 "awd_max_amd_letter_date": "2021-02-18",
 "awd_abstract_narration": "This project seeks to develop fundamental technologies to enable the next-generation of computing devices that will power future ubiquitous computing devices such as smartphones, self-driving cars, and autonomous robots. The project develops novel tools and techniques at both the hardware and software layers of computer systems. This project will also train new graduate engineers in architecting complex computing systems, modern software and hardware design methodologies, and cutting edge machine learning techniques. All of these skillsets are in broad demand in US industry but have been underrepresented in STEM education.\r\n\r\nHeterogeneous architectures comprising general purpose processors, graphics processors, and hardware accelerators designed for specific computing tasks have been widely adopted in today's computing systems for both edge and cloud devices. Specialized computing blocks provide tremendous benefits in energy efficiency. However, a major challenge in the design of such systems is the loss of generality and flexibility that has limited their adoption to a small set of application domains that do not often change. Increased flexibility could be unlocked if accelerators were built from smaller dynamically composable blocks, but existing approaches are difficult to program and scale poorly. This project proposes a design flow to generate a templated System-on-Chip (Soc) with a composable accelerator system that can be physically instantiated for a range of computing platforms. Through a virtualization layer, collections of physical hardware blocks are exposed to software as virtual accelerators. To efficiently search the large design space of the SoC, new design space exploration techniques are under investigation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Brooks",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "David M Brooks",
   "pi_email_addr": "dbrooks@eecs.harvard.edu",
   "nsf_id": "000091383",
   "pi_start_date": "2017-07-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gu-Yeon",
   "pi_last_name": "Wei",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gu-Yeon Wei",
   "pi_email_addr": "guyeon@eecs.harvard.edu",
   "nsf_id": "000086808",
   "pi_start_date": "2017-07-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Ryan",
   "pi_last_name": "Adams",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Ryan P Adams",
   "pi_email_addr": "rpa@princeton.edu",
   "nsf_id": "000623159",
   "pi_start_date": "2017-07-14",
   "pi_end_date": "2021-02-18"
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "33 Oxford Street",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382901",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-97877a4d-7fff-aa1a-21cd-e6877b414045\"> </span></p>\n<p dir=\"ltr\">Exponential advances in integrated circuits (IC) technology over the past 50+ years has led to amazing computing capabilities culminating in devices like our smart phones and emerging artificial intelligence (AI) services like chatGPT. In the early days of computing up to the late 1990's, engineering effort focused on making general computing devices capable of performing more complex tasks at higher speeds. This led to central processing unit (CPU) chips, like the x86 processors from Intel, to push the limits of managing power consumption and cost-effective cooling. This led to the era of multi-core CPU designs that sought to further improve performance and energy efficiency via parallelism. In other words, enable parallel processing cores to operate more efficiently and simultaneously handle multiple tasks at the same time. As portable devices, such as smartphones, gained their foothold in our daily lives, energy-efficient computing under strict size, weight, and power (SWAP) constraints became the primary design objective of the late 2000's and early 2010's. This led chip designers to specialize circuitry for specific computing tasks such as audio and video decoding, as just two of many examples. The era of hardware specialization and acceleration was visible in modern system-on-chip (SoC) devices from companies such as Apple, Qualcomm, and Samsung, who were building application processor (AP) SoCs for mobile devices. This shift in the architecture of computing chips enabled higher performance and capabilities, buttressed by continued advances in silicon chip processing technologies. This was the state of the art in 2016 when researchers at Harvard proposed to the NSF to explore \"Virtualized accelerators for scalable, composable architectures.\"</p>\n<p dir=\"ltr\">State-of-the-art SoCs at the time comprised general purpose CPUs, GPUs (graphics processing units), and a multitude of hardware accelerators. However, the observation was that the push towards specialized hardware, while highly efficient at specific tasks for which they were designed, were inflexible, leading to a growing gap between highly flexible but less efficient general-purpose CPUs and GPUs versus the highly specialized accelerators. The Harvard researchers sought to balance the efficiency of specialization with flexibility via a new design strategy focused on accelerator composability and composable architectures. In a nutshell, can accelerators be designed out of smaller dynamically composable blocks?&nbsp;</p>\n<p dir=\"ltr\">While the goal of creating scalable and composable accelerators was not new, the researchers proposed to leverage emerging design tools, optimization algorithms, and virtualization technologies, which had not been thoroughly investigated at that time. The proposed intellectual merit was that modern tools could navigate the vast design space of possible designs and configurations to find globally optimal solutions. If successful, the broader impact was future hardware accelerator designs that largely retained their efficiency while gaining flexibility to better accommodate future unforeseen applications. The research also sought to train the next generation of hardware designers equipped to take a broader, holistic approach to designing future heterogeneous computing systems.&nbsp;</p>\n<p>The early years of the project focused on laying the foundation for the research to be carried out. The researchers developed open-source simulation tools (gem5-Aladdin) and corresponding workload drivers (SMAUG for DNN modeling). gem5-Aladdin is an architectural simulator that facilitates modeling complex heterogeneous SoCs comprising both general purpose compute units with hardware accelerators. SMAUG (Simulating Machine Learning Accelerators Using gem5-Aladdin) is a framework that supports exploration of computing systems for machine learning applications. SMAUG was open-sourced and available to the public via GitHub. SMAUG was augmented with a Bayesian optimization framework to more efficiently explore large design spaces. The resulting tool, called&nbsp;BayesOpt-gem5-Aladdin, enables cost-aware coherence interfaces for many-accelerator SoCs. It was used to study the design of an SoC comprising seven hardware accelerators and compare different coherence strategies, e.g., non-coherent, LLC-coherent or fully-coherent. This work also supported the design of SoC chips, which were fabricated and tested, targeting machine learning tasks such as speech recognition and neural machine translation (FlexASR) and natural language processing/understanding (NLP/NLU) tasks (EdgeBERT). The tools also enabled simulation-based studies of flexible/programmable accelerator designs that tightly couple general-purpose CPUs with inline accelerators and design of composable accelerators for 3D mapping. Research results were published in leading computer architecture and integration circuits conferences. The final phase of the project has focused on additional design exploration tools (AccelMerger) and ML-based performance estimation tools (PerfSage).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/07/2023<br>\n\t\t\t\t\tModified by: Gu-Yeon&nbsp;Wei</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nExponential advances in integrated circuits (IC) technology over the past 50+ years has led to amazing computing capabilities culminating in devices like our smart phones and emerging artificial intelligence (AI) services like chatGPT. In the early days of computing up to the late 1990's, engineering effort focused on making general computing devices capable of performing more complex tasks at higher speeds. This led to central processing unit (CPU) chips, like the x86 processors from Intel, to push the limits of managing power consumption and cost-effective cooling. This led to the era of multi-core CPU designs that sought to further improve performance and energy efficiency via parallelism. In other words, enable parallel processing cores to operate more efficiently and simultaneously handle multiple tasks at the same time. As portable devices, such as smartphones, gained their foothold in our daily lives, energy-efficient computing under strict size, weight, and power (SWAP) constraints became the primary design objective of the late 2000's and early 2010's. This led chip designers to specialize circuitry for specific computing tasks such as audio and video decoding, as just two of many examples. The era of hardware specialization and acceleration was visible in modern system-on-chip (SoC) devices from companies such as Apple, Qualcomm, and Samsung, who were building application processor (AP) SoCs for mobile devices. This shift in the architecture of computing chips enabled higher performance and capabilities, buttressed by continued advances in silicon chip processing technologies. This was the state of the art in 2016 when researchers at Harvard proposed to the NSF to explore \"Virtualized accelerators for scalable, composable architectures.\"\nState-of-the-art SoCs at the time comprised general purpose CPUs, GPUs (graphics processing units), and a multitude of hardware accelerators. However, the observation was that the push towards specialized hardware, while highly efficient at specific tasks for which they were designed, were inflexible, leading to a growing gap between highly flexible but less efficient general-purpose CPUs and GPUs versus the highly specialized accelerators. The Harvard researchers sought to balance the efficiency of specialization with flexibility via a new design strategy focused on accelerator composability and composable architectures. In a nutshell, can accelerators be designed out of smaller dynamically composable blocks? \nWhile the goal of creating scalable and composable accelerators was not new, the researchers proposed to leverage emerging design tools, optimization algorithms, and virtualization technologies, which had not been thoroughly investigated at that time. The proposed intellectual merit was that modern tools could navigate the vast design space of possible designs and configurations to find globally optimal solutions. If successful, the broader impact was future hardware accelerator designs that largely retained their efficiency while gaining flexibility to better accommodate future unforeseen applications. The research also sought to train the next generation of hardware designers equipped to take a broader, holistic approach to designing future heterogeneous computing systems. \n\nThe early years of the project focused on laying the foundation for the research to be carried out. The researchers developed open-source simulation tools (gem5-Aladdin) and corresponding workload drivers (SMAUG for DNN modeling). gem5-Aladdin is an architectural simulator that facilitates modeling complex heterogeneous SoCs comprising both general purpose compute units with hardware accelerators. SMAUG (Simulating Machine Learning Accelerators Using gem5-Aladdin) is a framework that supports exploration of computing systems for machine learning applications. SMAUG was open-sourced and available to the public via GitHub. SMAUG was augmented with a Bayesian optimization framework to more efficiently explore large design spaces. The resulting tool, called BayesOpt-gem5-Aladdin, enables cost-aware coherence interfaces for many-accelerator SoCs. It was used to study the design of an SoC comprising seven hardware accelerators and compare different coherence strategies, e.g., non-coherent, LLC-coherent or fully-coherent. This work also supported the design of SoC chips, which were fabricated and tested, targeting machine learning tasks such as speech recognition and neural machine translation (FlexASR) and natural language processing/understanding (NLP/NLU) tasks (EdgeBERT). The tools also enabled simulation-based studies of flexible/programmable accelerator designs that tightly couple general-purpose CPUs with inline accelerators and design of composable accelerators for 3D mapping. Research results were published in leading computer architecture and integration circuits conferences. The final phase of the project has focused on additional design exploration tools (AccelMerger) and ML-based performance estimation tools (PerfSage).\n\n\t\t\t\t\tLast Modified: 02/07/2023\n\n\t\t\t\t\tSubmitted by: Gu-Yeon Wei"
 }
}