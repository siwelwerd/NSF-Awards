{
 "awd_id": "1651570",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: A Runtime for Fast Data Analysis on Modern Hardware",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2017-04-15",
 "awd_exp_date": "2022-03-31",
 "tot_intn_awd_amt": 592920.0,
 "awd_amount": 592920.0,
 "awd_min_amd_letter_date": "2017-04-06",
 "awd_max_amd_letter_date": "2021-02-17",
 "awd_abstract_narration": "The computer revolution that continuously transformed our society throughout the past 60 years happened because every year computer processors reliably became faster. Unfortunately, this trend has stopped.  New processors can no longer easily be made faster. Instead, new computer hardware uses parallelism or specialized components to achieve performance, which has made it much harder to build high-performance applications since most existing data processing systems run 10-100x slower than they could even on current processors and will have even more trouble on emerging hardware. To drive advances in information processing, computer systems that automatically map applications to emerging hardware are needed. This is a challenging intellectual problem.\r\n\r\nThis project proposes \"Weld\", a run-time for data-intensive parallel computation on modern hardware.  The project includes 2 main research thrusts:\r\n   \r\n     *An intermediate language (IL) for data-intensive computation that can capture common data-intensive\r\napplications but is easy to optimize for parallel hardware.  This language enables mapping workloads to diverse hardware like CPUs and GPUs. \r\n     *A runtime API that lets Weld dynamically optimize across different libraries used in the same program.   This API will allow Weld to perform complex optimizations like loop blocking across parallel libraries, unlocking speedups not yet possible.\r\n\r\nSuccess of this project will result in the creation of software that automatically maps existing key data intensive applications (e.g., data analytics, machine learning and search) to emerging hardware devices and achieves a 10-100x speedup over current applications. Beyond producing new technology, this project will train the next generation of engineers in high performance processing, online teaching resources, and research mentoring for undergraduate and graduate students. Together, education and new technology may make industrial, scientific, and government users of big data 10-100x more productive and enable the next generation of knowledge-driven systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matei",
   "pi_last_name": "Zaharia",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matei Zaharia",
   "pi_email_addr": "matei@cs.stanford.edu",
   "nsf_id": "000684140",
   "pi_start_date": "2017-04-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943054100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 226683.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 118473.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 122040.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 125724.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explored techniques to continue scaling up computing on modern computer hardware, in order to make it easier for a large range of organizations to run large-scale computations such as data analytics and machine learning. Over the past 60 years, computer processors have usually gotten faster each year, bringing applications along with them, but recently, processor speeds have stopped to increase, and applications need to leverage parallelism or specialized hardware to continue scaling up. These are much more challenging for software developers and users.</p>\n<p>In this project, we developed several techniques and systems to make high performance computing on modern hardware more broadly accessible. First, we developed Weld, a runtime that makes it easy for software developers to write fast versions of key routines that can then be combined and optimized together into a highly efficient program. We showed that Weld is easy to integrate into today's widely used software libraries, and can thus be used to accelerate existing applications without rewriting them. Second, we developed Split Annotations, a method to provide information about existing routines in a piece of software that will then allow it to be executed more efficiently at runtime by minimizing memory movement. Like Weld, Split Annotations can be used to accelerate and scale up existing software without modifying it. Third, we developed a number of techniques to speed up neural network computations in particular -- the computations that power modern AI advances in natural language processing, computer vision, audio, and other areas. These techniques take advantage of mathematical properties of the operators in a neural network to run the same computation more efficiently or parallelize it better across devices. They include TASO for optimizing operator graphs, Pipeline Parallelism for increasing parallelism, and FlexFlow for parallelizing a network across multiple dimensions. Across these areas, the work in this project can scale up existing applications by 10-100x on modern hardware with little effort from software developers.</p>\n<p>We have released the major pieces of software we developed under open source licenses, including Weld, Split Annotations, Sparser, TASO, FlexFlow, and Pipeline Parallelism (including an integration into NVIDIA's Megatron-LM deep learning framework). Several of these projects have been deployed in industry at various companies, or extended by researchers.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/31/2022<br>\n\t\t\t\t\tModified by: Matei&nbsp;Zaharia</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project explored techniques to continue scaling up computing on modern computer hardware, in order to make it easier for a large range of organizations to run large-scale computations such as data analytics and machine learning. Over the past 60 years, computer processors have usually gotten faster each year, bringing applications along with them, but recently, processor speeds have stopped to increase, and applications need to leverage parallelism or specialized hardware to continue scaling up. These are much more challenging for software developers and users.\n\nIn this project, we developed several techniques and systems to make high performance computing on modern hardware more broadly accessible. First, we developed Weld, a runtime that makes it easy for software developers to write fast versions of key routines that can then be combined and optimized together into a highly efficient program. We showed that Weld is easy to integrate into today's widely used software libraries, and can thus be used to accelerate existing applications without rewriting them. Second, we developed Split Annotations, a method to provide information about existing routines in a piece of software that will then allow it to be executed more efficiently at runtime by minimizing memory movement. Like Weld, Split Annotations can be used to accelerate and scale up existing software without modifying it. Third, we developed a number of techniques to speed up neural network computations in particular -- the computations that power modern AI advances in natural language processing, computer vision, audio, and other areas. These techniques take advantage of mathematical properties of the operators in a neural network to run the same computation more efficiently or parallelize it better across devices. They include TASO for optimizing operator graphs, Pipeline Parallelism for increasing parallelism, and FlexFlow for parallelizing a network across multiple dimensions. Across these areas, the work in this project can scale up existing applications by 10-100x on modern hardware with little effort from software developers.\n\nWe have released the major pieces of software we developed under open source licenses, including Weld, Split Annotations, Sparser, TASO, FlexFlow, and Pipeline Parallelism (including an integration into NVIDIA's Megatron-LM deep learning framework). Several of these projects have been deployed in industry at various companies, or extended by researchers.\n\n\t\t\t\t\tLast Modified: 12/31/2022\n\n\t\t\t\t\tSubmitted by: Matei Zaharia"
 }
}