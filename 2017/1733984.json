{
 "awd_id": "1733984",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative research: role of the motor system in phonological and phonetic processing",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 284772.0,
 "awd_amount": 284772.0,
 "awd_min_amd_letter_date": "2017-06-13",
 "awd_max_amd_letter_date": "2017-06-13",
 "awd_abstract_narration": "Across the world's languages, certain sound combinations occur more frequently than others.  However, the basis of these regularities is unknown. Some researchers have suggested that combinations such as \"lbog\" are rare because they require more complex articulatory gestures than combinations like  \"blog.\"  Another possibility is that combinations such as \"lbog\" tend to be avoided because they violate much more abstract linguistic constraints on allowable syllable structures. The investigators will evaluate these possibilities by examining the role of the articulatory motor system in speech perception using both noninvasive brain stimulation and behavioral methods.  Clarifying the relative contributions of speech motor processes and linguistic knowledge is critical to the diagnosis and treatment of speech language disorders, to first- and second-language acquisition, and to reading. \r\n\r\nThis research explores the role of embodiment and abstraction in speech perception. It proposes that speech is represented at multiple levels (embodied phonetics and abstract phonological rules) with different susceptibility to motor simulation, depending on (a) the level of analysis and (b) a speaker's linguistic experience. The proposed experiments test this hypothesis by combining brain stimulation and behavioral experiments. Using MRI-guided Transcranial Magnetic Stimulation (TMS), the research team will disrupt activity in brain regions linked to motor action (the cortical representation of the left orbicularis oris muscle in BA4) and brain regions linked to combinatorial phonological operations (left pars triangularis, PTr, BA 45), and assess the impact on two tasks that rely differentially on phonetic processing or the putatively abstract phonological computation of syllable structure. While it is unlikely that either task or brain region is selective to a single level of analysis, these experiments gauge whether they differ in their degree of participation. If phonetic categorization requires motor simulation, then disruption of BA4 should affect phonetic categorization more than it affects the computation of syllable structure. Alternatively, if the computation of syllable structure relies on disembodied processes effected by the PTr, then the disruption of BA45 should produce a stronger effect on syllable structure than on phonetic categorization. Projects 1 and 2 examine these predictions with speakers of English and Russian (languages that contrast in their syllabic inventory). The group comparison evaluates experience-dependent plasticity or uniformity in the engagement of the motor system across tasks. Projects 3 and 4 suppress articulation mechanically. If the greater contribution of BA4 to phonetic categorization reflects its role in motor simulation, then results from disruption of the orbicularis oris muscle by TMS and mechanical suppression  should converge.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Iris",
   "pi_last_name": "Berent",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Iris Berent",
   "pi_email_addr": "i.berent@neu.edu",
   "nsf_id": "000648194",
   "pi_start_date": "2017-06-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 284772.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>What is the basis of the human capacity for language? For example, why is it that English speakers <em>blog</em>, but not <em>lbog</em>? Laypeople often believe that the answer lies in talking: <em>blog </em>is far easier to articulate.</p>\n<p>Research in linguistics, however, suggests a different explanation. In this view, the brain of every English speaker encodes a set of abstract rules that are tacit&mdash;they operate entirely &ldquo;under the hood&rdquo;, even in young children and infants. Those rules ban structures like <em>lbog</em>. So in that second view, <em>lbog </em>&nbsp;sounds &ldquo;bad&rdquo; because it violates abstract rules of language.</p>\n<p>&nbsp;So why is <em>lbog </em>&ldquo;bad&rdquo; &mdash;is it because of how you <em>talk </em>or because of what you <em>know</em>?</p>\n<p>&nbsp;Scientist have been debating this question. This debate has been hard to settle because each of the two views has some evidence in its support.</p>\n<p>Many previous studies have suggested that speech perception <em>does</em> in fact rely on tacit &ldquo;talking&rdquo;.</p>\n<p>For example, there is evidence that merely hearing <em>ba </em>activates the motor area of the brain that controls the lips (which are used to articulate the <em>b </em>sound), whereas hearing <em>da </em>activates the brain area that controls the tongue (which, in turn, controls the <em>d</em> sound).&nbsp; So &ldquo;tacit talking&rdquo; clearly plays a role in hearing speech.</p>\n<p>&nbsp;Still, these results don&rsquo;t quite solve the puzzle. Past findings have only shown that &ldquo;tacit talking&rdquo; plays a role when we perceive <strong><em>individual</em></strong> speech sounds (such as <em>b</em>, <em>d</em>), whereas the contrast between <em>blog/lbog</em> concerns the <strong><em>combination</em></strong> of different sounds to form syllables. And what&rsquo;s true of individual sounds may not necessarily &ldquo;scale up&rdquo; to sound-combinations.&nbsp; &nbsp;</p>\n<p>So, what is the role of &ldquo;talking&rdquo; (that is, action) in perceiving speech: do we rely on &ldquo;tacit talking&rdquo; across the board&mdash;in identifying individual sounds <em>and </em>in grasping their combinations? Or does our ability to grasp sound-combinations invoke knowledge of abstract rules?</p>\n<p>&nbsp;To find out, this NSF-funded research examined the role of &ldquo;tacit talking&rdquo; in two distinct levels of speech perception: (a) the identification of individual sounds (e.g., <em>b/p</em>); and (b) grasping the combination of speech sounds (e.g., <em>blog/lbog</em>).</p>\n<p>&nbsp;To examine the role of &ldquo;talking&rdquo;, we disrupted the articulatory motor system in two distinct ways. One set of experiments had people bite on either their lips or tongues, and in so doing, disrupt the motor activity of these articulators. Another set of experiments stimulated the brain motor system that controls the lips using a technique known as <a href=\"https://en.wikipedia.org/wiki/Transcranial_magnetic_stimulation\">Transcranial Magnetic Stimulation</a> (TMS). The results from the two sets of experiments converged.</p>\n<p>&nbsp;When we examined <strong><em>individual</em></strong> speech sounds, like <em>b</em> or <em>d</em> &ldquo;talking&rdquo; played a role. Consequently, when motor activity was disrupted (either mechanically, by biting, or via TMS), speech perception was correspondingly impaired. For example, stimulating the lips specifically changed the perception of <em>b</em> (a sound produced by the lips) more than <em>d</em> (a sound produced by the tongue). All this is in line with previous research.</p>\n<p>&nbsp;Interestingly, when participants attempted to grasp sound <strong><em>combinations</em></strong>&mdash;that is, to identify syllables like <em>blog </em>and <em>lbog</em>, here, perception was relatively unaffected by the stimulation of the articulatory motor system, and this was the case regardless of whether the stimulation was applied mechanically (by biting), or to the brain (via TMS). &nbsp;The TMS studies further showed that, rather than relying on the brain motor system, the perception of sound-combination was more strongly affected when we stimulated Broca&rsquo;s area&mdash;one of the main language hubs in the brain.</p>\n<p>Altogether, then, the results of this project suggest two conclusions.&nbsp; First, hearing individual speech sounds relies on talking. To determine whether you hear the sound <em>b</em>, your brain tacitly simulates what it would be to <em>say </em>that sound (using your lips). It is no wonder, then, that when our mouths are masked, minute differences are hard to perceive (e.g., did you say <em>bee </em>or <em>pea</em>?). But when you grasp how sounds combine to form syllables (like <em>blog</em>), here, you seem to invoke abstract knowledge, possibly, knowledge of abstract rules.</p>\n<p>Speech perception, then, relies on two different tricks (not just one): talking and knowledge of abstract rules. Talking helps us perceive individual sounds; abstract knowledge helps us grasp their combination.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/13/2023<br>\n\t\t\t\t\tModified by: Iris&nbsp;Berent</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1733984/1733984_10492587_1671656398242_ScreenShot2022-12-21at3.58.36PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1733984/1733984_10492587_1671656398242_ScreenShot2022-12-21at3.58.36PM--rgov-800width.jpg\" title=\"Speech perception relies on two distinct tricks (not one).\"><img src=\"/por/images/Reports/POR/2022/1733984/1733984_10492587_1671656398242_ScreenShot2022-12-21at3.58.36PM--rgov-66x44.jpg\" alt=\"Speech perception relies on two distinct tricks (not one).\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Panel A shows the two ingredients of speech perception: identifying individual sounds and combining them. Panel B illustrates the TMS studies. For each ingredient (identification/combination), we stimulated either the lip motor area (the orbicularis oris, OO) or Broca\ufffds area (the pars triangularis)</div>\n<div class=\"imageCredit\">None</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Iris&nbsp;Berent</div>\n<div class=\"imageTitle\">Speech perception relies on two distinct tricks (not one).</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nWhat is the basis of the human capacity for language? For example, why is it that English speakers blog, but not lbog? Laypeople often believe that the answer lies in talking: blog is far easier to articulate.\n\nResearch in linguistics, however, suggests a different explanation. In this view, the brain of every English speaker encodes a set of abstract rules that are tacit&mdash;they operate entirely \"under the hood\", even in young children and infants. Those rules ban structures like lbog. So in that second view, lbog  sounds \"bad\" because it violates abstract rules of language.\n\n So why is lbog \"bad\" &mdash;is it because of how you talk or because of what you know?\n\n Scientist have been debating this question. This debate has been hard to settle because each of the two views has some evidence in its support.\n\nMany previous studies have suggested that speech perception does in fact rely on tacit \"talking\".\n\nFor example, there is evidence that merely hearing ba activates the motor area of the brain that controls the lips (which are used to articulate the b sound), whereas hearing da activates the brain area that controls the tongue (which, in turn, controls the d sound).  So \"tacit talking\" clearly plays a role in hearing speech.\n\n Still, these results don\u2019t quite solve the puzzle. Past findings have only shown that \"tacit talking\" plays a role when we perceive individual speech sounds (such as b, d), whereas the contrast between blog/lbog concerns the combination of different sounds to form syllables. And what\u2019s true of individual sounds may not necessarily \"scale up\" to sound-combinations.   \n\nSo, what is the role of \"talking\" (that is, action) in perceiving speech: do we rely on \"tacit talking\" across the board&mdash;in identifying individual sounds and in grasping their combinations? Or does our ability to grasp sound-combinations invoke knowledge of abstract rules?\n\n To find out, this NSF-funded research examined the role of \"tacit talking\" in two distinct levels of speech perception: (a) the identification of individual sounds (e.g., b/p); and (b) grasping the combination of speech sounds (e.g., blog/lbog).\n\n To examine the role of \"talking\", we disrupted the articulatory motor system in two distinct ways. One set of experiments had people bite on either their lips or tongues, and in so doing, disrupt the motor activity of these articulators. Another set of experiments stimulated the brain motor system that controls the lips using a technique known as Transcranial Magnetic Stimulation (TMS). The results from the two sets of experiments converged.\n\n When we examined individual speech sounds, like b or d \"talking\" played a role. Consequently, when motor activity was disrupted (either mechanically, by biting, or via TMS), speech perception was correspondingly impaired. For example, stimulating the lips specifically changed the perception of b (a sound produced by the lips) more than d (a sound produced by the tongue). All this is in line with previous research.\n\n Interestingly, when participants attempted to grasp sound combinations&mdash;that is, to identify syllables like blog and lbog, here, perception was relatively unaffected by the stimulation of the articulatory motor system, and this was the case regardless of whether the stimulation was applied mechanically (by biting), or to the brain (via TMS).  The TMS studies further showed that, rather than relying on the brain motor system, the perception of sound-combination was more strongly affected when we stimulated Broca\u2019s area&mdash;one of the main language hubs in the brain.\n\nAltogether, then, the results of this project suggest two conclusions.  First, hearing individual speech sounds relies on talking. To determine whether you hear the sound b, your brain tacitly simulates what it would be to say that sound (using your lips). It is no wonder, then, that when our mouths are masked, minute differences are hard to perceive (e.g., did you say bee or pea?). But when you grasp how sounds combine to form syllables (like blog), here, you seem to invoke abstract knowledge, possibly, knowledge of abstract rules.\n\nSpeech perception, then, relies on two different tricks (not just one): talking and knowledge of abstract rules. Talking helps us perceive individual sounds; abstract knowledge helps us grasp their combination.\n\n \n\n\t\t\t\t\tLast Modified: 01/13/2023\n\n\t\t\t\t\tSubmitted by: Iris Berent"
 }
}