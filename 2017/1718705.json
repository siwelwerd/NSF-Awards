{
 "awd_id": "1718705",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Auditory and Haptic Based Brain-Computer Interfaces Using In-Ear Electrodes",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 497745.0,
 "awd_amount": 497745.0,
 "awd_min_amd_letter_date": "2017-08-16",
 "awd_max_amd_letter_date": "2017-08-16",
 "awd_abstract_narration": "Brain Computer Interfaces (BCIs) are systems that allow people to control computers and other devices with brain signals alone.   BCIs have the potential to improve the lives of many individuals with severe physical disabilities, by allowing them to communicate and control their environment without needing muscle movement or voice.  After more than two decades of research, BCIs have become robust and reliable enough to consider them for mainstream applications, such as hands-free and voice-free control of devices.  However, the most common BCIs require visual attention, which restricts their utility for people with visual impairments, or for mobile environments (such as driving) where diverting visual attention is dangerous or not possible.  Intriguing alternatives to visual displays are auditory (sounds) or haptic (touch or sensations such as vibrations) based BCI interfaces, or multimodal BCI interfaces which are a combination of these.  In addition to providing an alternate interface for people with visual impairments or for special purposes, nonvisual BCI control of devices could also be useful in everyday life, such as when responding to a text message in a movie theater without looking at a device screen or speaking.  This project will extend the state of the art in BCIs by evolving the body of knowledge in auditory, haptic, and multimodal stimuli, which are relatively unexplored areas of the field.  Additionally, the research will create and explore small, wearable in-ear electrodes to detect brain signals, and thus will contribute to the emerging field of mobile BCIs which will open possibilities for large numbers of mainstream users. Current BCI systems cover a wide and varying range of brain signals and recording technologies; this research focuses on \"evoked-response\" electroencephalograph (EEG) approaches, that is to say brain signals that are triggered due to a stimulus such as a sound, flashing light, or touch.\r\n\r\nTo these ends, the project will study auditory and haptic cues to determine the best ways to map them to input choices.  One of the biggest challenges with auditory and haptic interfaces is how to label a stimulus so it is meaningful to the BCI user.  To address this problem, the project will explore novel methods of encoding the labeling and mapping of auditory and haptic stimuli into the stimuli themselves (in a manner analogous to how it is possible to label visual stimuli, such as when the target is a flashing letter so the user can determine the meaning of a cue by looking at it).  The plan is to leverage research in sonification (which largely focuses on presenting data in auditory \"displays\") to devise techniques for encoding speech or patterned tones (such as Morse code) into audio cues in such a way that a user can simply listen to the cue to determine its meaning.  The team will also experiment with a variety of multimodal approaches (combining visual, auditory, and haptic cues in a single system) in order to achieve higher accuracy than is possible with a single stimulus mode alone.  Finally, the project will evaluate the effectiveness of in-ear EEG electrodes in detecting brain responses to auditory and haptic stimuli, experimenting with electrode design, placement within the ear, and various filters and classifiers to improve the signal-to-noise ratio.   The results of the experiments in alternative stimuli will be combined with the optimized wearable electrode system, to create the first hands-free, voice-free, vision-free interfaces for mainstream users.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Melody",
   "pi_last_name": "Jackson",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Melody M Jackson",
   "pi_email_addr": "melody@cc.gatech.edu",
   "nsf_id": "000071319",
   "pi_start_date": "2017-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute Of Technology",
  "perf_str_addr": "225 North Avenue",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 497745.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our main goal was to study ways of providing hands-free, eyes-free, voice-free input modalities for everyday, wearable brain-computer interfaces. Our objective was to enable new interaction paradigms for populations with visual disability, physical disability, or for mainstream users in mobile and wearable environments where a visual display is not feasible.&nbsp;&nbsp;</p>\n<p>We studied auditory and haptic (sensory) cues for EEG-based evoked-potential BCIs to determine the best ways to map them to input choices. The user controls the interface with selective attention, paying attention to the stimulus that represents the desired choice. Our team examined critical features of audio and haptic stimuli and explored machine learning techniques to identify the corresponding changes in brain signals. We compared stimuli modalities, to lay a foundation for multimodal approaches, such as combining visual, auditory, and haptic cues in order to achieve higher accuracy than with a single stimulus mode alone. &nbsp; We also experimented with ways of making these interfaces mobile, with customized, in-ear electrode designs instead of scalp electrodes. We also developed and tested improved active scalp-worn electrodes that do not require any conductive gel (dry electrodes) but perform noise reduction to allow a stronger brain signal to be collected.&nbsp;&nbsp;</p>\n<p>The intellectual contributions of this research include:</p>\n<ul>\n<li>making significant progress in creating and studying hands-free, eyes-free, voice-free input modalities for brain-computer interfaces.&nbsp;&nbsp;</li>\n<li>creating, testing, and analyzing a haptic interface for BCIs based on vibrotactile stimuli placed on the fingers, which improved the accuracy of haptic interfaces from the literature .</li>\n<li>extensively studying auditory stimuli including pure tones, instruments (timbres), modulated voice, and spatially separated sounds, with up to 6 distinct sounds in a selection space.&nbsp; We discovered that spatially separating sounds provides the greatest accuracy and evaluated the other auditory modes for effectiveness.</li>\n<li>creating and evaluating ten different in-ear electrode designs for signal strength, comfort, wearability, and noise reduction, contributing a significant body of knowledge to the wearable BCI field.</li>\n<li>developing a new dry active scalp electrode based on driving right leg circuits, designed towards wearable systems, and integrated it with the Open BCI system.&nbsp;</li>\n</ul>\n<p>The broader impacts of this research include:</p>\n<ul>\n<li> New input modalities making it possible that brain-computer interfaces could be made wearable and mobile for both disabled and mainstream users.</li>\n<li>Contributions to the field of wearable computing, adding direct brain control for wearable devices</li>\n<li>New possibilities for more comfortable, reliable, and accurate assistive technology for people with visual disabilities and physical disabilities</li>\n</ul>\n<ol> </ol><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2021<br>\n\t\t\t\t\tModified by: Melody&nbsp;Jackson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur main goal was to study ways of providing hands-free, eyes-free, voice-free input modalities for everyday, wearable brain-computer interfaces. Our objective was to enable new interaction paradigms for populations with visual disability, physical disability, or for mainstream users in mobile and wearable environments where a visual display is not feasible.  \n\nWe studied auditory and haptic (sensory) cues for EEG-based evoked-potential BCIs to determine the best ways to map them to input choices. The user controls the interface with selective attention, paying attention to the stimulus that represents the desired choice. Our team examined critical features of audio and haptic stimuli and explored machine learning techniques to identify the corresponding changes in brain signals. We compared stimuli modalities, to lay a foundation for multimodal approaches, such as combining visual, auditory, and haptic cues in order to achieve higher accuracy than with a single stimulus mode alone.   We also experimented with ways of making these interfaces mobile, with customized, in-ear electrode designs instead of scalp electrodes. We also developed and tested improved active scalp-worn electrodes that do not require any conductive gel (dry electrodes) but perform noise reduction to allow a stronger brain signal to be collected.  \n\nThe intellectual contributions of this research include:\n\nmaking significant progress in creating and studying hands-free, eyes-free, voice-free input modalities for brain-computer interfaces.  \ncreating, testing, and analyzing a haptic interface for BCIs based on vibrotactile stimuli placed on the fingers, which improved the accuracy of haptic interfaces from the literature .\nextensively studying auditory stimuli including pure tones, instruments (timbres), modulated voice, and spatially separated sounds, with up to 6 distinct sounds in a selection space.  We discovered that spatially separating sounds provides the greatest accuracy and evaluated the other auditory modes for effectiveness.\ncreating and evaluating ten different in-ear electrode designs for signal strength, comfort, wearability, and noise reduction, contributing a significant body of knowledge to the wearable BCI field.\ndeveloping a new dry active scalp electrode based on driving right leg circuits, designed towards wearable systems, and integrated it with the Open BCI system. \n\n\nThe broader impacts of this research include:\n\n New input modalities making it possible that brain-computer interfaces could be made wearable and mobile for both disabled and mainstream users.\nContributions to the field of wearable computing, adding direct brain control for wearable devices\nNew possibilities for more comfortable, reliable, and accurate assistive technology for people with visual disabilities and physical disabilities\n\n \n\n\t\t\t\t\tLast Modified: 12/30/2021\n\n\t\t\t\t\tSubmitted by: Melody Jackson"
 }
}