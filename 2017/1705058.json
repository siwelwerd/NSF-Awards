{
 "awd_id": "1705058",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: An Interaction Manager for Language and Force Exhanges in Human-Robot Physical Collaboration",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Juan Wachs",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 1048618.0,
 "awd_amount": 1048618.0,
 "awd_min_amd_letter_date": "2017-07-25",
 "awd_max_amd_letter_date": "2022-08-05",
 "awd_abstract_narration": "This project will develop a computational and data-driven framework for a robot assistant to collaborate with humans in everyday tasks that involve physical interaction, such as handing over or moving an object together. Using models learned from observing humans perform such tasks, the robot will engage in back-and-forth communication, where turns can be both spoken utterances and force exchanges. Robots that can collaborate to perform physical tasks could provide assistance in a variety of settings, such as performing household chores, supporting the elderly to remain independent, and assisting human workers on the factory floor.\r\n\r\nThe transformative idea of the proposal is to generalize the methodology of dialog processing to include physical interaction. The fundamental challenge is how to bridge the gap between the symbolic processing of language and the low-level control of force exchanges. The concept of interaction primitives (IPs) is introduced to model physical interactions. Further, a planning and execution framework in the form of an interaction manager is proposed. The proposed interaction manager broadens the traditional dialogue modeling paradigm so that information can flow from more abstract to lower levels, and vice versa: language affects physical interaction, and physical interaction affects what is said. To build the interaction manager, a targeted data collection where humans perform tasks of interest will be performed. Since physical interaction data is invariably sparse, statistical learning on the data will be complemented by model-based generalizations, allowing robots to collaborate with humans in highly variable and unstructured environments. The research will inform new course development, and involve several undergraduate and graduate students.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Milos",
   "pi_last_name": "Zefran",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Milos Zefran",
   "pi_email_addr": "mzefran@uic.edu",
   "nsf_id": "000207388",
   "pi_start_date": "2017-07-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Barbara",
   "pi_last_name": "DiEugenio",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Barbara DiEugenio",
   "pi_email_addr": "bdieugen@uic.edu",
   "nsf_id": "000192544",
   "pi_start_date": "2017-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Chicago",
  "inst_street_address": "809 S MARSHFIELD AVE M/C 551",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3129962862",
  "inst_zip_code": "606124305",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "IL07",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "W8XEAJDKMXH3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Chicago",
  "perf_str_addr": "851 S. Morgan St.",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606077052",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "IL07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 1048618.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-19a867d2-7fff-1fe5-16c8-129fd8365476\"> </span></p>\n<p dir=\"ltr\"><span>There is a growing need for domestic robots that can support the independent living of individuals who may require assistance in performing various activities of daily living (ADLs). The main challenge in these tasks is that the interaction is inherently multimodal: it employs a back-and-forth emblematic of human conversation but includes additional modalities such as force exchanges or pointing gestures besides speech. We developed a Multimodal Interaction Manager (MIM) that can process input from multiple sensors and generate appropriate responses of the robot helping its human partner complete a specific task successfully. The foundation of the MIM is a newly developed formalism, Hierarchical Bipartite Action-Transition Networks (HBATNs), that represent the task as an interconnected set of networks that model both participants simultaneously, hence providing a framework for determining the human intent and planning subsequent actions of the robot. We also developed a methodology to automatically generate the interaction planner, the core of the MIM, through reinforcement learning (RL).</span></p>\n<p dir=\"ltr\"><span>An important multimodal task that involves close physical coupling between the two participants and often occurs when providing assistance during ADLs is collaborative manipulation. When two humans collaborate in a manipulation task, they are able to reach a consensus on various aspects of the task. Of particular interest are mechanisms that humans use to decide where to move the object, and who leads the motion. We developed a hierarchical robot control framework that emulates human behavior in communicating a motion destination to a human collaborator and in responding to their actions. At the top level, the controller consists of a set of finite-state machines corresponding to different levels of commitment of the robot to its desired goal configuration. The control architecture is loosely based on the human strategy observed in the human-human experiments, and the key component is a real-time intent recognizer that helps the robot respond to human actions.</span></p>\n<p dir=\"ltr\"><span>Robots that can collaborate to perform physical tasks could provide help in a variety of settings. In everyday life, they would provide convenience, for example in performing household chores. In contrast, in elderly care, they can potentially alleviate societal concerns, for example by supporting the elderly to age in place, i.e., in their homes, while enabling limited care worker resources to be better focused on more difficult tasks and on higher-needs patients. There is also a need for robot assistants in commercial settings, such as in manufacturing and warehouse logistics. While safety is an important concern when robots and humans work in close proximity, productivity may be significantly boosted if humans can interact with robots in the same way they interact with other humans.</span></p>\n<p dir=\"ltr\"><span>A diverse set of graduate and undergraduate students was involved in the research. Two PhD students have successfully defended their PhD dissertations and several are within a year of graduation. All of them have acquired highly valuable skills in human-robot interaction and assistive robotics. The results of the research have been published in more than 10 conference papers.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/30/2024<br>\nModified by: Milos&nbsp;Zefran</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThere is a growing need for domestic robots that can support the independent living of individuals who may require assistance in performing various activities of daily living (ADLs). The main challenge in these tasks is that the interaction is inherently multimodal: it employs a back-and-forth emblematic of human conversation but includes additional modalities such as force exchanges or pointing gestures besides speech. We developed a Multimodal Interaction Manager (MIM) that can process input from multiple sensors and generate appropriate responses of the robot helping its human partner complete a specific task successfully. The foundation of the MIM is a newly developed formalism, Hierarchical Bipartite Action-Transition Networks (HBATNs), that represent the task as an interconnected set of networks that model both participants simultaneously, hence providing a framework for determining the human intent and planning subsequent actions of the robot. We also developed a methodology to automatically generate the interaction planner, the core of the MIM, through reinforcement learning (RL).\n\n\nAn important multimodal task that involves close physical coupling between the two participants and often occurs when providing assistance during ADLs is collaborative manipulation. When two humans collaborate in a manipulation task, they are able to reach a consensus on various aspects of the task. Of particular interest are mechanisms that humans use to decide where to move the object, and who leads the motion. We developed a hierarchical robot control framework that emulates human behavior in communicating a motion destination to a human collaborator and in responding to their actions. At the top level, the controller consists of a set of finite-state machines corresponding to different levels of commitment of the robot to its desired goal configuration. The control architecture is loosely based on the human strategy observed in the human-human experiments, and the key component is a real-time intent recognizer that helps the robot respond to human actions.\n\n\nRobots that can collaborate to perform physical tasks could provide help in a variety of settings. In everyday life, they would provide convenience, for example in performing household chores. In contrast, in elderly care, they can potentially alleviate societal concerns, for example by supporting the elderly to age in place, i.e., in their homes, while enabling limited care worker resources to be better focused on more difficult tasks and on higher-needs patients. There is also a need for robot assistants in commercial settings, such as in manufacturing and warehouse logistics. While safety is an important concern when robots and humans work in close proximity, productivity may be significantly boosted if humans can interact with robots in the same way they interact with other humans.\n\n\nA diverse set of graduate and undergraduate students was involved in the research. Two PhD students have successfully defended their PhD dissertations and several are within a year of graduation. All of them have acquired highly valuable skills in human-robot interaction and assistive robotics. The results of the research have been published in more than 10 conference papers.\n\n\n\t\t\t\t\tLast Modified: 01/30/2024\n\n\t\t\t\t\tSubmitted by: MilosZefran\n"
 }
}