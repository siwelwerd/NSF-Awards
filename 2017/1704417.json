{
 "awd_id": "1704417",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF:Medium:Collaborative Research:Estimation, Learning, and Memory: The Quest for Statistically Optimal Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 550000.0,
 "awd_amount": 550000.0,
 "awd_min_amd_letter_date": "2017-05-04",
 "awd_max_amd_letter_date": "2020-07-21",
 "awd_abstract_narration": "The goal of this project is to develop new, efficient algorithms that extract as much information as is possible from a given quantity of data.  In particular, this research aims to develop an understanding of how to leverage structure that is present in natural language settings, medical and genomic settings, and network- or graph-based settings. Many fundamental types of structure are encountered repeatedly in widely varying scientific and technological settings; our goal is to build on a recent body of work that focused on the simplest unstructured settings, and develop broadly applicable tools and insights to these diverse settings.   A central component of this project is a close interaction and transfer of ideas, problems, and techniques, between the theory community, the machine learning community, and the broader set of data-centric researchers and practitioners.\r\n\r\nFrom a technical perspective, this research focuses on three fundamental types of structure: geometric structure, algebraic or low-rank structure, and the structure that is present in sequential\r\ndata (such as natural language).   For the first two types of structure, the research focus is on understanding the possibilities and limitations in the sparse data regime where the amount of data is comparable to, or sublinear in, the dimensionality of the data.  In the third setting, the focus is on understanding the role of memory for learning and prediction tasks.\r\n\r\nBeyond the direct research goals of the project, the PIs are extensively involved in teaching and outreach, including designing UW?s new data sciences curriculum, and developing new courses on algorithms and foundational aspects of data sciences at Stanford.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Valiant",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory J Valiant",
   "pi_email_addr": "gvaliant@cs.stanford.edu",
   "nsf_id": "000603941",
   "pi_start_date": "2017-05-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "353 Serra Mall",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943059025",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 97470.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 98231.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 249641.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 104658.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project resulted in significant new insights and algorithms for a variety of data-centric learning and optimization tasks.&nbsp; Additionally, the project supported a number of PhD students and postdoctoral researchers, of whom seven have graduated and now work as academics at top universities or research scientists in industry.&nbsp; Below is an overview of some of the main research results.<br /><br />One research direction supported by this project was an effort to rethink the assumptions underlying data, and design algorithms that still work well (with strong theoretical guarantees) without relying on strong independence (or i.i.d.) assumptions on the data.&nbsp; The punchline that has emerged from a series of results supported by this project, is that good algorithms and estimators can often be designed that enjoy surprising success guarantees, even in settings without strong assumptions on the underlying data .&nbsp; One example is work (appearing in COLT?19 and COLT'21) on \"selective prediction\" that considers the problem of predicting statistics of future datapoints, given access to a sequence of observations. Provided the predictor has the power to select 1) when the prediction is made, and 2) the window-length to which the prediction pertains, accurate prediction is possible even for worst-case (bounded) data without any assumptions about how future data is related to past data. This surprising result on accurately predicting the future without assumptions on how the future is related to the past, motivated the consideration of more general formulations where accurate inferences are possible for worst-case data. For example, given worst case data but an understanding of the process by which the data is partitioned into a test and training set, when is accurate prediction or learning possible? Work (supported by this project) appearing at Neurips'20 described a framework for thinking about such generalizations, and gave a practical and theoretically near-optimal algorithm for predictions in such settings.&nbsp; This initial work on worst-case data serves as an enlightening counterpoint to the predominant theoretical frameworks for learning and statistics that either posits a distribution or generative process underlying the data, makes strong structural assumptions about the data, or measures performance with respect to a limited class of benchmarks.<br /><br />A different thrust of research supported under this project is the role of memory in optimization and learning.&nbsp; The memory footprint of modern large-scale learning algorithms is, increasingly, one of the limiting bottlenecks in practice (in conjunction with the more traditional resources such as the amount of data and runtime).&nbsp; Despite this, prior to work on this project, little was understood about the role of memory size in optimization or learning.&nbsp; Two significant results of this project established that there are fundamental problems that require significant memory to solve efficiently: for both linear regression, as well as a more general optimization framework (optimizing convex functions given the ability to query the function value and gradient at desired points), any algorithm with memory that is not significantly super-linear in the dimension of the problem, exhibits significantly slower convergence in comparison to optimal (large-memory) algorithms.&nbsp; These results--appearing in top venues (STOC'19, COLT'22)--were the first of their kind, and provide a new perspective on the challenge of designing efficient and practical optimization algorithms.&nbsp; This project also supported work on the algorithmic front, which described an efficient low-memory algorithm that applies to settings with a certain type of structure, eschews the above impossibility results (COLT'22).<br /><br />Beyond the above lines of research, this project supported a number of other successful research efforts, including introducing new algorithms for learning HMMS (NeurIPS'17) and learning sequences using only short-term memory (STOC'18), a new extremely efficient algorithm for estimating the spectrum of a graph (KDD'19), work introducing a framework and algorithms to allow trained ML models to \"forget\" data (NeurIPS'19), new algorithms for training deep neural networks that enforce structural invariants (ICML'19), work to understand the implicit regularization of deep neural networks (COLT'19), efforts to formalize the problem of generating more data (ICML'20), and providing a rigorous framework in which to probe the ability of Transformer networks to perform \"in context\" learning (Neurips'22).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/06/2023<br>\n\t\t\t\t\tModified by: Gregory&nbsp;J&nbsp;Valiant</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project resulted in significant new insights and algorithms for a variety of data-centric learning and optimization tasks.  Additionally, the project supported a number of PhD students and postdoctoral researchers, of whom seven have graduated and now work as academics at top universities or research scientists in industry.  Below is an overview of some of the main research results.\n\nOne research direction supported by this project was an effort to rethink the assumptions underlying data, and design algorithms that still work well (with strong theoretical guarantees) without relying on strong independence (or i.i.d.) assumptions on the data.  The punchline that has emerged from a series of results supported by this project, is that good algorithms and estimators can often be designed that enjoy surprising success guarantees, even in settings without strong assumptions on the underlying data .  One example is work (appearing in COLT?19 and COLT'21) on \"selective prediction\" that considers the problem of predicting statistics of future datapoints, given access to a sequence of observations. Provided the predictor has the power to select 1) when the prediction is made, and 2) the window-length to which the prediction pertains, accurate prediction is possible even for worst-case (bounded) data without any assumptions about how future data is related to past data. This surprising result on accurately predicting the future without assumptions on how the future is related to the past, motivated the consideration of more general formulations where accurate inferences are possible for worst-case data. For example, given worst case data but an understanding of the process by which the data is partitioned into a test and training set, when is accurate prediction or learning possible? Work (supported by this project) appearing at Neurips'20 described a framework for thinking about such generalizations, and gave a practical and theoretically near-optimal algorithm for predictions in such settings.  This initial work on worst-case data serves as an enlightening counterpoint to the predominant theoretical frameworks for learning and statistics that either posits a distribution or generative process underlying the data, makes strong structural assumptions about the data, or measures performance with respect to a limited class of benchmarks.\n\nA different thrust of research supported under this project is the role of memory in optimization and learning.  The memory footprint of modern large-scale learning algorithms is, increasingly, one of the limiting bottlenecks in practice (in conjunction with the more traditional resources such as the amount of data and runtime).  Despite this, prior to work on this project, little was understood about the role of memory size in optimization or learning.  Two significant results of this project established that there are fundamental problems that require significant memory to solve efficiently: for both linear regression, as well as a more general optimization framework (optimizing convex functions given the ability to query the function value and gradient at desired points), any algorithm with memory that is not significantly super-linear in the dimension of the problem, exhibits significantly slower convergence in comparison to optimal (large-memory) algorithms.  These results--appearing in top venues (STOC'19, COLT'22)--were the first of their kind, and provide a new perspective on the challenge of designing efficient and practical optimization algorithms.  This project also supported work on the algorithmic front, which described an efficient low-memory algorithm that applies to settings with a certain type of structure, eschews the above impossibility results (COLT'22).\n\nBeyond the above lines of research, this project supported a number of other successful research efforts, including introducing new algorithms for learning HMMS (NeurIPS'17) and learning sequences using only short-term memory (STOC'18), a new extremely efficient algorithm for estimating the spectrum of a graph (KDD'19), work introducing a framework and algorithms to allow trained ML models to \"forget\" data (NeurIPS'19), new algorithms for training deep neural networks that enforce structural invariants (ICML'19), work to understand the implicit regularization of deep neural networks (COLT'19), efforts to formalize the problem of generating more data (ICML'20), and providing a rigorous framework in which to probe the ability of Transformer networks to perform \"in context\" learning (Neurips'22).\n\n\t\t\t\t\tLast Modified: 07/06/2023\n\n\t\t\t\t\tSubmitted by: Gregory J Valiant"
 }
}