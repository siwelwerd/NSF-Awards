{
 "awd_id": "1652561",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Robots that Help People",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2017-03-15",
 "awd_exp_date": "2022-02-28",
 "tot_intn_awd_amt": 549437.0,
 "awd_amount": 549437.0,
 "awd_min_amd_letter_date": "2017-03-02",
 "awd_max_amd_letter_date": "2020-06-19",
 "awd_abstract_narration": "As robots become more prevalent, it is crucial to develop ways for people to collaborate with them.  This proposal aims to create collaborative robots through a combination of communication, perception, and action.  Existing approaches are typically tailored to specific applications; yet people want to talk to robots about everything they can see and do.  To address such limitations, this project will create a unified framework to enable robots to communicate with people to learn their needs, plan how to achieve them, and then perceive and act in the world in order to meet those needs.  The research will be demonstrated with robots that can assist with household tasks, such as cooking and cleaning, as well as in manufacturing settings, such as collaborative assembly.  The project will expose many people to collaborative robotics through an internship program with local high schools, a regional robotics conference, and the Million Object Challenge.\r\n\r\nThis project will create a model, the Human-Robot Collaborative Partially Observable Markov Decision Process, that enables robots to 1) automatically acquire object-oriented models of objects in the physical world; 2) communicate with people to understand their needs and how to meet them; and 3) act to change the world in ways that meet people's needs.  Creating a unified framework requires bridging gaps between different aspects of the robotic system.  This project focuses on creating a single probabilistic graphical model to represent the robot's states and actions in a hierarchical framework, allowing the robot to make plans that take into account its own uncertainty and to communicate with a person about everything it can see and everything it can do.  Focusing on collaboration leads to reformulations of traditional problems in computer vision, planning, and natural language understanding enabling the robot to collaborate in new and more natural ways.  \r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stefanie",
   "pi_last_name": "Tellex",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefanie Tellex",
   "pi_email_addr": "stefie10@cs.brown.edu",
   "nsf_id": "000651585",
   "pi_start_date": "2017-03-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "Office of Sponsored Projects",
  "perf_city_name": "Providence",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129093",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 102696.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 106018.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 109444.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 231279.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As robots become more powerful and more autonomous, it is crucial to develop ways for people to collaborate with them.&nbsp; The aim of this research program was to create robots that collaborate with people to meet their needs by creating a multimodal decision-theoretic model, the Human-Robot Collaborative POMDP (Partially Observable Markov Decision Process), that unifies perception, action, and communication.&nbsp; Existing approaches to human-robot collaboration rely on models that are tailored to specific domains, and that do not span from perception to action to communication; yet people want to talk toa robot about everything it can see and everything it can do.&nbsp; To address these limitations, this project enables a robot to acquire models for detecting, localizing, and manipulating objects in the world, plan in very large spaces to find appropriate actions, and communicate with people to learn about their needs.&nbsp; Integrating communication, perception, and planning in a unified framework has enabled a robot to interpret a person's requests at different levels of abstraction, plan a sequence of actions to fulfill that request, and recover from failure when unexpected events occur.&nbsp;&nbsp;</p>\n<p>This proposal as resulted in advances in perception for object search, language understanding for natural language commands at different levels of abstraction that also incorporate the robot's history or past behavior as well as its goal, and mapping English to rich multimodal semantic spaces. The work has been published at top conferences including ICRA and RSS, and resulted in the training of multiple undergraduate and Ph.D. students.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/08/2022<br>\n\t\t\t\t\tModified by: Stefanie&nbsp;Tellex</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAs robots become more powerful and more autonomous, it is crucial to develop ways for people to collaborate with them.  The aim of this research program was to create robots that collaborate with people to meet their needs by creating a multimodal decision-theoretic model, the Human-Robot Collaborative POMDP (Partially Observable Markov Decision Process), that unifies perception, action, and communication.  Existing approaches to human-robot collaboration rely on models that are tailored to specific domains, and that do not span from perception to action to communication; yet people want to talk toa robot about everything it can see and everything it can do.  To address these limitations, this project enables a robot to acquire models for detecting, localizing, and manipulating objects in the world, plan in very large spaces to find appropriate actions, and communicate with people to learn about their needs.  Integrating communication, perception, and planning in a unified framework has enabled a robot to interpret a person's requests at different levels of abstraction, plan a sequence of actions to fulfill that request, and recover from failure when unexpected events occur.  \n\nThis proposal as resulted in advances in perception for object search, language understanding for natural language commands at different levels of abstraction that also incorporate the robot's history or past behavior as well as its goal, and mapping English to rich multimodal semantic spaces. The work has been published at top conferences including ICRA and RSS, and resulted in the training of multiple undergraduate and Ph.D. students. \n\n \n\n\t\t\t\t\tLast Modified: 07/08/2022\n\n\t\t\t\t\tSubmitted by: Stefanie Tellex"
 }
}