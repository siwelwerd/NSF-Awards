{
 "awd_id": "1718380",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Integrated Knowledge Discovery and Analysis Using Sum-of-Squares Proofs",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 439997.0,
 "awd_amount": 445997.0,
 "awd_min_amd_letter_date": "2017-08-30",
 "awd_max_amd_letter_date": "2020-06-02",
 "awd_abstract_narration": "Developing approaches to subjects as diverse as advertising, bioinformatics, counterterrorism, fraud detection, politics, sociology, and so on are based on data analysis. This project will develop new algorithms for data analysis with strong guarantees of their correctness and efficiency. Tools based on these algorithms will be usable as-is, without expert knowledge, in innovative data-driven applications far removed from academia. The project will also involve the training of students in advanced techniques for data analysis.\r\n\r\nThe algorithms considered in this project involve automated reasoning with an algebraic logic known as \"sum-of-squares.\" Automated reasoning requires a delicate trade-off between expressiveness and simplicity, to facilitate reasoning that is both fast and effective. Sum-of-squares is capable of expressing much statistical reasoning, and yet is sufficiently simple to allow the design of tractable algorithms for reasoning. This project will consider how these algorithms can be used to reason about an overall distribution or population from a sample of data drawn from it. The main aim of the project is to develop efficient algorithms that guarantee that all of the relevant statistical facts are discovered during data analysis. The project will further develop these algorithms to solve problems in domains such as Computer Vision and Natural Language Processing.\r\n\r\nIn addition to the development of domain-specific algorithms, the project will consider sum-of-squares reasoning with high-degree polynomials. Although such reasoning in the standard sense is intractable, the project aims to simulate reasoning with such high-degree expressions with the aid of the sample of data from the distribution to be reasoned about. The project will also investigate the expressive power of sum-of-squares with such high-degree expressions. Specifically, the project will investigate whether or not such reasoning can simulate other logics such as resolution (or vice-versa), and will further investigate the extent of its ability to basic capture statistical notions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brendan",
   "pi_last_name": "Juba",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Brendan A Juba",
   "pi_email_addr": "bjuba@wustl.edu",
   "nsf_id": "000672054",
   "pi_start_date": "2017-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington University",
  "inst_street_address": "1 BROOKINGS DR",
  "inst_street_address_2": "",
  "inst_city_name": "SAINT LOUIS",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "3147474134",
  "inst_zip_code": "631304862",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MO01",
  "org_lgl_bus_name": "WASHINGTON UNIVERSITY, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "L6NFUM28LQM5"
 },
 "perf_inst": {
  "perf_inst_name": "Washington University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "631304899",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 439997.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 6000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Machine learning studies algorithms that use data to identify rules for making predictions. Theoretically, we suppose that each example in the data was generated randomly according to some common probability distribution. The rules used for prediction are statements that are likely to be true for data drawn from that probability distribution. There may be many different such rules that are true of a given probability distribution. These are the \"laws\" that the data obeys, and any such rule is a piece of knowledge that we could discover about the data; such a rule may be verified by examining the data, if we knew to look for it. These rules may help us draw a variety of conclusions, not just predictions, but also determining what is possible in a domain. In general, we cannot hope to identify all of this knowledge.<br /><br />The primary focus of this project was to develop efficient algorithms that can use this knowledge to draw inferences by using the data directly. In particular, we considered several different families of inferences, for different kinds of knowledge, and presented algorithms that could implicitly use the knowledge present in the data to draw conclusions.<br /><br />One of the representations we focused on used polynomials. As long as the exponents in the polynomial expressions are not too large, the algorithms for reasoning with these representations are tractable to execute. We showed that reasoning with these representations can directly capture simple kinds of \"if-then\" reasoning with Boolean expressions, in addition to the obvious kind of arithmetic reasoning. We also demonstrated that such expressions can empirically capture some simple kinds of shapes in images. We developed algorithms that could robustly determine whether or not a shape expressed in this way is present in an image, and established guarantees that the algorithm would find a shape if it is present, under sufficiently favorable conditions.</p>\n<p><br />Furthermore, we studied methods for learning dynamic environments, where the rules are now laws governing how the environment evolves. Here, we are interested in being able to determine what actions to take to achieve some particular kind of outcome. We found tractable algorithms for finding good actions to take when \"good\" outcomes are determined by a linear function, and for finding explicit representations of the environment when the environment may be described by certain simple kinds of rules.</p>\n<p>We also examined the problem of determining whether or not accurate predictions are possible on an identifiable subset of the distribution. That is, we search for a \"condition,\" together with a prediction rule such that when the condition is true, the prediction rule is accurate. We showed that under conditions that are not much more demanding than those necessary to analyze the correctness of a linear prediction rule, a tractable algorithm can solve this problem, identifying a linear prediction rule together with a simple condition. In particular, we also considered a variant of the problem in which we seek \"personalized\" predictions, where we seek the condition containing a given point -- representing the attributes of an individual -- giving the most accurate predictions. We gave tractable algorithms for this variant as well, and note that furthermore, they may be used to \"explain\" the predictions produced by black-box predictors such as neural networks.<br /><br />On the other hand, we also studied how much data is necessary to find such conditions, and found that as one might expect, it scales inversely with how unlikely the condition is. Unfortunately, in many domains, one seeks to make accurate predictions about rare conditions, and our result then implies that a prohibitively large amount of data is then necessary.</p>\n<p>&nbsp;</p>\n<p>We also considered the use of an unusual representation of probability distributions, and showed that it subsumes the standard representation that determines the probability of a given point, and also subsumes another family of distributions, \"determinantal point processes,\" that capture distributions on diverse sets -- e.g., when we pick one item of a given type while shopping. We showed that we can nevertheless efficiently support the usual kind of operations on this new representation. Preliminary experiments suggest that this new representation will give more accurate approximations to data distributions.<br /><br />We also considered the problem of finding the best-fitting determinantal point process for a given set of data. We confirmed the long-standing conjecture that this problem is intractable.<br /><br />Code for the experiments in these projects, where applicable, has been made publicly available.<br /><br />This project helped with the training of three Ph.D. students (one female), five Master's students (two female), and thirteen undergraduates (one under-represented minority, six female), in research in computer science. Two of the Master's students and four of the undergraduates have so far entered Ph.D. programs in Computer Science, Data Science, or Statistics. The PI has already taught courses that incorporate some of the findings of the project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/24/2021<br>\n\t\t\t\t\tModified by: Brendan&nbsp;A&nbsp;Juba</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMachine learning studies algorithms that use data to identify rules for making predictions. Theoretically, we suppose that each example in the data was generated randomly according to some common probability distribution. The rules used for prediction are statements that are likely to be true for data drawn from that probability distribution. There may be many different such rules that are true of a given probability distribution. These are the \"laws\" that the data obeys, and any such rule is a piece of knowledge that we could discover about the data; such a rule may be verified by examining the data, if we knew to look for it. These rules may help us draw a variety of conclusions, not just predictions, but also determining what is possible in a domain. In general, we cannot hope to identify all of this knowledge.\n\nThe primary focus of this project was to develop efficient algorithms that can use this knowledge to draw inferences by using the data directly. In particular, we considered several different families of inferences, for different kinds of knowledge, and presented algorithms that could implicitly use the knowledge present in the data to draw conclusions.\n\nOne of the representations we focused on used polynomials. As long as the exponents in the polynomial expressions are not too large, the algorithms for reasoning with these representations are tractable to execute. We showed that reasoning with these representations can directly capture simple kinds of \"if-then\" reasoning with Boolean expressions, in addition to the obvious kind of arithmetic reasoning. We also demonstrated that such expressions can empirically capture some simple kinds of shapes in images. We developed algorithms that could robustly determine whether or not a shape expressed in this way is present in an image, and established guarantees that the algorithm would find a shape if it is present, under sufficiently favorable conditions.\n\n\nFurthermore, we studied methods for learning dynamic environments, where the rules are now laws governing how the environment evolves. Here, we are interested in being able to determine what actions to take to achieve some particular kind of outcome. We found tractable algorithms for finding good actions to take when \"good\" outcomes are determined by a linear function, and for finding explicit representations of the environment when the environment may be described by certain simple kinds of rules.\n\nWe also examined the problem of determining whether or not accurate predictions are possible on an identifiable subset of the distribution. That is, we search for a \"condition,\" together with a prediction rule such that when the condition is true, the prediction rule is accurate. We showed that under conditions that are not much more demanding than those necessary to analyze the correctness of a linear prediction rule, a tractable algorithm can solve this problem, identifying a linear prediction rule together with a simple condition. In particular, we also considered a variant of the problem in which we seek \"personalized\" predictions, where we seek the condition containing a given point -- representing the attributes of an individual -- giving the most accurate predictions. We gave tractable algorithms for this variant as well, and note that furthermore, they may be used to \"explain\" the predictions produced by black-box predictors such as neural networks.\n\nOn the other hand, we also studied how much data is necessary to find such conditions, and found that as one might expect, it scales inversely with how unlikely the condition is. Unfortunately, in many domains, one seeks to make accurate predictions about rare conditions, and our result then implies that a prohibitively large amount of data is then necessary.\n\n \n\nWe also considered the use of an unusual representation of probability distributions, and showed that it subsumes the standard representation that determines the probability of a given point, and also subsumes another family of distributions, \"determinantal point processes,\" that capture distributions on diverse sets -- e.g., when we pick one item of a given type while shopping. We showed that we can nevertheless efficiently support the usual kind of operations on this new representation. Preliminary experiments suggest that this new representation will give more accurate approximations to data distributions.\n\nWe also considered the problem of finding the best-fitting determinantal point process for a given set of data. We confirmed the long-standing conjecture that this problem is intractable.\n\nCode for the experiments in these projects, where applicable, has been made publicly available.\n\nThis project helped with the training of three Ph.D. students (one female), five Master's students (two female), and thirteen undergraduates (one under-represented minority, six female), in research in computer science. Two of the Master's students and four of the undergraduates have so far entered Ph.D. programs in Computer Science, Data Science, or Statistics. The PI has already taught courses that incorporate some of the findings of the project.\n\n\t\t\t\t\tLast Modified: 12/24/2021\n\n\t\t\t\t\tSubmitted by: Brendan A Juba"
 }
}