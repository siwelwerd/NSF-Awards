{
 "awd_id": "1711335",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CCSS: Collaborative Research: Ubiquitous Sensing for VR/AR Immersive Communication: A Machine Learning Perspective",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": "7032924568",
 "po_email": "hdai@nsf.gov",
 "po_sign_block_name": "Huaiyu Dai",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2017-06-28",
 "awd_max_amd_letter_date": "2017-06-28",
 "awd_abstract_narration": "Virtual and augmented reality systems comprise multi-view camera sensors that capture a scene from multiple perspectives. The captured data is then used to construct an immersive representation of the scene on the user's head mounted display. Such systems are poised to enable and enhance numerous important applications, e.g., inspection of large-scale infrastructure, archival of historical sites, search and rescue, disaster response, military reconnaissance, natural resource management, and immersive telepresence. However, due to its emerging nature, virtual/augmented reality immersive communication is presently limited to gaming or entertainment demonstrations featuring off-line captured/computer-generated content, studio-type settings, and high-end workstations to sustain its high data/computing workload. Moreover, there is little understanding of the fundamental trade-offs between the required signal acquisition density and sensor locations across space and time, the dynamics of the captured scene (motion, geometry, and textures), the available network and system resources, and the delivered immersion quality. This renders existing solutions impractical for deployment on bandwidth and energy constrained remote sensors. The project addresses these challenges via rigorous analysis and concerted algorithmic and application advances at the intersection of multi-view space-time sensing and signal representation, delay-sensitive communication, and machine learning. Education and outreach activities will immerse students in the exciting areas of visual sensing, wireless communications, and machine learning, and will engage underrepresented students spanning K-12 through undergraduate levels.\r\n\r\nThe objective of this project is to efficiently capture a remote environment using multiple camera sensors with the highest possible reconstruction quality under limited sampling and communication resources. This is achieved through four interrelated research tasks: (i) analysis of optimal space-time sampling policies that determine the sensors' locations and sampling rates to minimize the remote scene's reconstruction error; (ii) design of optimal signal representation methods that embed the sampled data jointly across space and time according to the allocated sampling rates; (iii) design of online learning sampling policies based on spectral graph theory that take sampling actions while exploring new sensor locations in the absence of a priori scene viewpoint signal knowledge; and (vi) design of computationally efficient self-organizing reinforcement learning methods that allow the wireless sensors to compute optimal transmission scheduling policies that meet the low-latency requirements of the overlaying virtual/augmented reality application while conserving their available energy. Integration, experimentation, and prototyping activities will be conducted to asses and validate the research advances in real-world settings. These technical advances will enable diverse applications of transformative impact.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nicholas",
   "pi_last_name": "Mastronarde",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Nicholas H Mastronarde",
   "pi_email_addr": "nmastron@buffalo.edu",
   "nsf_id": "000614623",
   "pi_start_date": "2017-06-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Buffalo",
  "inst_street_address": "520 LEE ENTRANCE STE 211",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "7166452634",
  "inst_zip_code": "142282577",
  "inst_country_name": "United States",
  "cong_dist_code": "26",
  "st_cong_dist_code": "NY26",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "GMZUKXFDJMA9",
  "org_uei_num": "LMCJKRFW5R81"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Buffalo",
  "perf_str_addr": "230 Davis Hall",
  "perf_city_name": "Buffalo",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "142602500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "26",
  "perf_st_cong_dist": "NY26",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "153E",
   "pgm_ref_txt": "Wireless comm & sig processing"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Virtual and augmented reality (VR/AR) immersive communication systems comprise multi-view camera sensors that capture a scene from multiple perspectives. The captured data is then communicated to -- and used to construct an immersive representation of the scene on -- the user's head mounted display. Such systems will enable and enhance numerous important applications such as inspection of large-scale infrastructure, archival of historical sites, search and rescue, disaster response, military reconnaissance, natural resource management, and immersive telepresence. However, such setups have been limited to gaming or entertainment demonstrations featuring off-line captured/generated content in controlled settings and have traditionally relied on high-end workstations to sustain its high data/computing workload. This renders existing solutions impractical for field deployment on bandwidth and energy constrained remote camera sensors. The objective of this five-year collaborative project was to overcome these limitations through rigorous analysis, concerted algorithmic development, and application advances to enable the efficient capture of a remote environment using a limited number of camera sensors and to communicate the captured information to an aggregation point with the highest possible reconstruction quality under limited communication resources.</p>\n<p>The University at Buffalo's primary contributions to this project focused on the design of computationally efficient reinforcement learning algorithms that allow the wireless sensors to learn optimal transmission scheduling policies that meet the low-latency requirements of the overlaying VR/AR application while conserving their available energy. The key outcomes towards these goals were as follows: 1) Novel reinforcement learning algorithms that systematically integrate domain knowledge (in the form of basic system models) into the learning process to achieve orders of magnitude improvement in key performance metrics, such as convergence speed and compute/memory complexity, compared to well-established benchmarks; 2) Novel approximation-based reinforcement learning algorithms that enable precise control of the trade-off between the algorithm's compute/memory complexity and the associated learning approximation error; 3) New theory showing that, due to certain properties of the wireless sensor model and the transmission scheduling problem, the approximation-based reinforcement learning algorithm achieves a bounded approximation error with respect to the optimal solution; 4) A novel efficient arithmetic hardware accelerator that achieves 2.6x lower delay, 7x lower power, and 7.2x smaller area than an unoptimized baseline hardware architecture for implementing our new domain-knowledge driven reinforcement algorithms; and 5) A new stochastic computing-based hardware accelerator that further reduces the critical path delay of the arithmetic hardware by 86.7% and only requires 0.74% of the energy consumption. We also showed that it is possible to use extremely short stochastic representations without sacrificing learning performance due to the resilience of the proposed reinforcement learning algorithms to random variations in their inputs.</p>\n<p>Additional important contributions to this project focused on leveraging unmanned aerial vehicles (UAVs) for the acquisition and communication of sampled VR/AR data to users, and on mobile-edge cooperative multi-user VR/AR computing and streaming services. The key outcomes towards these goals were as follows: 1) Design and analysis of an efficient radio resource management optimization framework for a multi-tier multi-band millimeter wave cellular network integrating UAV-based aerial small cells for enhanced coverage and throughput; 2) A novel open software/hardware framework called the University at Buffalo's Airborne Networking and Communications Testbed (UB-ANC), which enables the design, implementation, and testing of multi-UAV applications in software and facilitates seamless transition to deployment on hardware; and 3) Analysis of a novel communication system that integrates scalable multi-layer 360 degree video tiling, viewport-adaptive rate-distortion optimal resource allocation, and VR-centric edge computing and caching, to enable high-quality untethered VR streaming.<strong>&nbsp;</strong></p>\n<p>Through this project, the PI trained and mentored three graduate students, including two females, and twelve undergraduate students, including two female students and three students from underrepresented minority groups in STEM. The undergraduate students have leveraged their experiences to secure internships at federal labs, launch successful careers in industry, and pursue graduate study.&nbsp;&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/11/2022<br>\n\t\t\t\t\tModified by: Nicholas&nbsp;H&nbsp;Mastronarde</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668189187492_ImmersiveCommIllustr--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668189187492_ImmersiveCommIllustr--rgov-800width.jpg\" title=\"VR/AR Scenario\"><img src=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668189187492_ImmersiveCommIllustr--rgov-66x44.jpg\" alt=\"VR/AR Scenario\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Multiple viewpoints of a scene are captured simultaneously. A remote VR/AR user can navigate between different viewpoints over time.</div>\n<div class=\"imageCredit\">Jacob Chakareski</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Nicholas&nbsp;H&nbsp;Mastronarde</div>\n<div class=\"imageTitle\">VR/AR Scenario</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668189394204_qualitative--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668189394204_qualitative--rgov-800width.jpg\" title=\"Qualitative Comparison of Reinforcement Learning Algorithms\"><img src=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668189394204_qualitative--rgov-66x44.jpg\" alt=\"Qualitative Comparison of Reinforcement Learning Algorithms\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">State-action pairs updated in each step (white boxes) for simple communication system. (left) Q-learning; (middle) Post-decision state learning; (right) Virtual experience learning.</div>\n<div class=\"imageCredit\">Nicholas Mastronarde</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nicholas&nbsp;H&nbsp;Mastronarde</div>\n<div class=\"imageTitle\">Qualitative Comparison of Reinforcement Learning Algorithms</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668189609675_valueapproxfig--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668189609675_valueapproxfig--rgov-800width.jpg\" title=\"Value Function Approximation\"><img src=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668189609675_valueapproxfig--rgov-66x44.jpg\" alt=\"Value Function Approximation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Comparison of the optimal value function and an approximate value function for an energy-harvesting sensor transmitting delay-sensitive data.</div>\n<div class=\"imageCredit\">Nikhilesh Sharma</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nicholas&nbsp;H&nbsp;Mastronarde</div>\n<div class=\"imageTitle\">Value Function Approximation</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668192694055_hardwaredesign--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668192694055_hardwaredesign--rgov-800width.jpg\" title=\"Hardware Design for Fast Reinforcement Learning\"><img src=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668192694055_hardwaredesign--rgov-66x44.jpg\" alt=\"Hardware Design for Fast Reinforcement Learning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Action evaluation hardware accelerator designs for a simple wireless transmission system. (a) Arithmetic hardware accelerator; (b) Alternative state-value estimator (SVE) with a stochastic computing based transition probability distribution estimation (TPDE) module.</div>\n<div class=\"imageCredit\">Jianchi Sun</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nicholas&nbsp;H&nbsp;Mastronarde</div>\n<div class=\"imageTitle\">Hardware Design for Fast Reinforcement Learning</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668192914121_UbancArchitecture--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668192914121_UbancArchitecture--rgov-800width.jpg\" title=\"UB-ANC Emulator Software Architecture\"><img src=\"/por/images/Reports/POR/2022/1711335/1711335_10497043_1668192914121_UbancArchitecture--rgov-66x44.jpg\" alt=\"UB-ANC Emulator Software Architecture\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The UB-ANC Emulator's Software Architecture. The University at Buffalo\ufffds Airborne Networking and Communications Testbed (UB-ANC) is an open SW/HW framework that enables the design, implementation, and testing of multi-UAV applications in software and facilitates seamless transition to hardware.</div>\n<div class=\"imageCredit\">Jalil Modares</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nicholas&nbsp;H&nbsp;Mastronarde</div>\n<div class=\"imageTitle\">UB-ANC Emulator Software Architecture</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nVirtual and augmented reality (VR/AR) immersive communication systems comprise multi-view camera sensors that capture a scene from multiple perspectives. The captured data is then communicated to -- and used to construct an immersive representation of the scene on -- the user's head mounted display. Such systems will enable and enhance numerous important applications such as inspection of large-scale infrastructure, archival of historical sites, search and rescue, disaster response, military reconnaissance, natural resource management, and immersive telepresence. However, such setups have been limited to gaming or entertainment demonstrations featuring off-line captured/generated content in controlled settings and have traditionally relied on high-end workstations to sustain its high data/computing workload. This renders existing solutions impractical for field deployment on bandwidth and energy constrained remote camera sensors. The objective of this five-year collaborative project was to overcome these limitations through rigorous analysis, concerted algorithmic development, and application advances to enable the efficient capture of a remote environment using a limited number of camera sensors and to communicate the captured information to an aggregation point with the highest possible reconstruction quality under limited communication resources.\n\nThe University at Buffalo's primary contributions to this project focused on the design of computationally efficient reinforcement learning algorithms that allow the wireless sensors to learn optimal transmission scheduling policies that meet the low-latency requirements of the overlaying VR/AR application while conserving their available energy. The key outcomes towards these goals were as follows: 1) Novel reinforcement learning algorithms that systematically integrate domain knowledge (in the form of basic system models) into the learning process to achieve orders of magnitude improvement in key performance metrics, such as convergence speed and compute/memory complexity, compared to well-established benchmarks; 2) Novel approximation-based reinforcement learning algorithms that enable precise control of the trade-off between the algorithm's compute/memory complexity and the associated learning approximation error; 3) New theory showing that, due to certain properties of the wireless sensor model and the transmission scheduling problem, the approximation-based reinforcement learning algorithm achieves a bounded approximation error with respect to the optimal solution; 4) A novel efficient arithmetic hardware accelerator that achieves 2.6x lower delay, 7x lower power, and 7.2x smaller area than an unoptimized baseline hardware architecture for implementing our new domain-knowledge driven reinforcement algorithms; and 5) A new stochastic computing-based hardware accelerator that further reduces the critical path delay of the arithmetic hardware by 86.7% and only requires 0.74% of the energy consumption. We also showed that it is possible to use extremely short stochastic representations without sacrificing learning performance due to the resilience of the proposed reinforcement learning algorithms to random variations in their inputs.\n\nAdditional important contributions to this project focused on leveraging unmanned aerial vehicles (UAVs) for the acquisition and communication of sampled VR/AR data to users, and on mobile-edge cooperative multi-user VR/AR computing and streaming services. The key outcomes towards these goals were as follows: 1) Design and analysis of an efficient radio resource management optimization framework for a multi-tier multi-band millimeter wave cellular network integrating UAV-based aerial small cells for enhanced coverage and throughput; 2) A novel open software/hardware framework called the University at Buffalo's Airborne Networking and Communications Testbed (UB-ANC), which enables the design, implementation, and testing of multi-UAV applications in software and facilitates seamless transition to deployment on hardware; and 3) Analysis of a novel communication system that integrates scalable multi-layer 360 degree video tiling, viewport-adaptive rate-distortion optimal resource allocation, and VR-centric edge computing and caching, to enable high-quality untethered VR streaming. \n\nThrough this project, the PI trained and mentored three graduate students, including two females, and twelve undergraduate students, including two female students and three students from underrepresented minority groups in STEM. The undergraduate students have leveraged their experiences to secure internships at federal labs, launch successful careers in industry, and pursue graduate study.  \n\n\t\t\t\t\tLast Modified: 11/11/2022\n\n\t\t\t\t\tSubmitted by: Nicholas H Mastronarde"
 }
}