{
 "awd_id": "1661374",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Visual Question Answering (VQA)",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 517022.0,
 "awd_amount": 517022.0,
 "awd_min_amd_letter_date": "2016-12-02",
 "awd_max_amd_letter_date": "2020-05-07",
 "awd_abstract_narration": "This project addresses the problem of Visual Question Answering (VQA). Given an image and a free-form natural language question about the image (e.g., \"What kind of store is this?\", \"How many people are waiting in the queue?\", \"Is it safe to cross the street?\"), the machine's task is to automatically produce a concise, accurate, free-form, natural language answer (\"bakery\", \"5\", \"Yes\"). VQA is directly applicable to a variety of applications of high societal impact that involve humans eliciting situationally-relevant information from visual data; where humans and machines must collaborate to extract information from pictures. Examples include aiding visually-impaired users in understanding their surroundings, analysts in making decisions based on large quantities of surveillance, and interacting with a robot. This project has the potential to fundamentally improve the way visually-impaired users live their daily lives, and revolutionize how society at large interacts with visual data. \r\n\r\nThis research enables that VQA represents not a single narrowly-defined problem (e.g., image classification) but rather a rich spectrum of semantic scene understanding problems and associated research directions. Each question in VQA may lie at a different point on this spectrum: from questions that directly map to existing well-studied computer-vision problems (\"What is this room called?\" = indoor scene recognition) all the way to questions that require an integrated approach of vision (scene), language (semantics), and reasoning (understanding) over a knowledge base (\"Does the pizza in the back row next to the bottle of Coke seem vegetarian?\"). Consequently, this work maps to a sequence of waypoints along this spectrum. Motivated by addressing VQA from a variety of perspectives, this research program is generating new datasets, knowledge, and techniques in (i) pure computer vision (ii) integrating vision + language (iii) integrating vision + language + common sense (iv) building interpretable models and (v) combining a portfolio of methods. In addition, novel contributions are being made to (a) training the machine to be curious and actively ask questions to learn (b) using VQA as a modality to learn more about the visual world than what existing annotation modalities allow and (c) training the machine to know what it knows and what it does not.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Devi",
   "pi_last_name": "Parikh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Devi Parikh",
   "pi_email_addr": "parikh@gatech.edu",
   "nsf_id": "000571087",
   "pi_start_date": "2016-12-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320420",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 104201.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 97647.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 101240.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 105006.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 108928.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project's overarching goal was to develop a state-of-the-art Visual Question Answering (VQA) system capable of accurately answering free-form natural language questions about images. VQA is a challenging task that requires integrating computer vision and natural language processing techniques to create a multimodal AI system capable of understanding the content of an image and generating accurate answers to questions about it.</p>\n<p>To achieve this goal, the project made significant research contributions that are documented in publications at top-tier computer vision, natural language processing, machine learning, and artificial intelligence conferences and journals. In addition to VQA, the research covered several tasks related to multimodal AI, including visual commonsense reasoning, caption-based image retrieval, grounding referring expressions, multi-modal verification, visual dialog, and embodied agents navigating to answer questions about their environment.</p>\n<p>The results of the research achieved state-of-the-art performance on these tasks, pushing the boundaries of the field.</p>\n<p>The project's impact is also evident in the publicly available datasets and code, which have been widely used by researchers and developers worldwide, further advancing the development of multimodal AI systems.</p>\n<p>The project's impact on the field is expected to continue as researchers and developers build upon the insights and techniques developed in the project to create more advanced multimodal AI systems for various applications.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/23/2023<br>\n\t\t\t\t\tModified by: Devi&nbsp;Parikh</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project's overarching goal was to develop a state-of-the-art Visual Question Answering (VQA) system capable of accurately answering free-form natural language questions about images. VQA is a challenging task that requires integrating computer vision and natural language processing techniques to create a multimodal AI system capable of understanding the content of an image and generating accurate answers to questions about it.\n\nTo achieve this goal, the project made significant research contributions that are documented in publications at top-tier computer vision, natural language processing, machine learning, and artificial intelligence conferences and journals. In addition to VQA, the research covered several tasks related to multimodal AI, including visual commonsense reasoning, caption-based image retrieval, grounding referring expressions, multi-modal verification, visual dialog, and embodied agents navigating to answer questions about their environment.\n\nThe results of the research achieved state-of-the-art performance on these tasks, pushing the boundaries of the field.\n\nThe project's impact is also evident in the publicly available datasets and code, which have been widely used by researchers and developers worldwide, further advancing the development of multimodal AI systems.\n\nThe project's impact on the field is expected to continue as researchers and developers build upon the insights and techniques developed in the project to create more advanced multimodal AI systems for various applications.\n\n\t\t\t\t\tLast Modified: 03/23/2023\n\n\t\t\t\t\tSubmitted by: Devi Parikh"
 }
}