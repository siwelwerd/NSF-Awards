{
 "awd_id": "1706964",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Infant-Robot Interaction as an Early Intervention Strategy",
 "cfda_num": "47.041",
 "org_code": "07020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Grace Hwang",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 307000.0,
 "awd_min_amd_letter_date": "2017-08-15",
 "awd_max_amd_letter_date": "2019-05-30",
 "awd_abstract_narration": "Typically developing (TD) infants use movement to explore their environment, to interact, and to control their bodies. By moving, they learn which movements lead to desired outcomes: obtaining a smile from a caregiver, reaching a toy, etc.  In contrast to TD infants, infants at risk (AR) for developmental delays often have difficulty moving and decreased motivation for movement. Consequently, less movement experience results in less exploration and may contribute to delays in development. The goal of this project is to develop a robot that can interact with an infant and encourage the infant to explore different types of movements. The robot will guide and engage the infant to produce movements in ranges that the infant is not experiencing on his/her own. The proposed research has the potential to advance knowledge about which aspects of movement infants can adjust and how to most effectively guide their movement to help them learn to control their bodies. End users (therapists and parents) are participating in the development of the infant-robot interaction system with the goal that it be easily adaptable to other conditions, such as autism. The project includes education and training components: 1) a K-12 STEM outreach component ties the research to educating youth about STEM and motor disabilities, and 2) graduate students in engineering, computer science, and biokinesiology receive training and mentorship to become effective interdisciplinary researchers. \r\n\r\nThis research project is developing an interactive infant-robot system by: 1) analyzing existing movement data from TD and AR infants to produce expected behavior distributions for movement characteristics; 2) using those results to inform the contingent feedback for the robot to determine which movement characteristics infants can act to adjust; and 3) determining whether personalized, robot-guided feedback (motivating the infant in the desired movement) is more effective than a standard robot reward (fixed robot movement pattern). Wearable sensor and 3D vision technology is being used to quantify infant movement characteristics: limb movement quantity, duration, peak acceleration, and amplitude. A non-cumbersome light-weight wearable eye-tracker is being used to measure visual attention. The hypotheses to be tested are that: 1) infants are able to perceive and act on information to adjust the amplitude, peak acceleration, and duration of their limb movements; and 2) personalized, robot-guided feedback is more effective than a fixed robot reward at eliciting target movement behaviors.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CBET",
 "org_div_long_name": "Division of Chemical, Bioengineering, Environmental, and Transport Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Beth",
   "pi_last_name": "Smith",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Beth Smith",
   "pi_email_addr": "beth.smith@usc.edu",
   "nsf_id": "000729666",
   "pi_start_date": "2017-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Maja",
   "pi_last_name": "Matari\u0107",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Maja J Matari\u0107",
   "pi_email_addr": "mataric@usc.edu",
   "nsf_id": "000410606",
   "pi_start_date": "2017-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900899006",
  "perf_ctry_code": "US",
  "perf_cong_dist": "34",
  "perf_st_cong_dist": "CA34",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169800",
   "pgm_ele_name": "DS -Developmental Sciences"
  },
  {
   "pgm_ele_code": "534200",
   "pgm_ele_name": "Disability & Rehab Engineering"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 300000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 7000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Typically developing infants engage in motor babbling, exploratory movements that allow them to learn what movements lead to desired outcomes: obtaining a smile from a caregiver, reaching and grasping a toy, etc. Through this necessary developmental perception-action process, they learn to control their bodies and interact with the environment. In contrast to infants with typical development (TD), infants at risk (AR) for developmental disability often have neuromotor impairments involving strength, proprioception, or coordination that lead to greater difficulty with and decreased motivation for movement. Less motor babbling results in less exploration and suboptimal developmental interactions. The goal of this project was to develop a socially assistive, non-contact, infant-robot interaction system to provide contingent positive feedback to increase exploration and expand early movement practice. The intellectual merit of the results of our project is advancing knowledge about which aspects of movement infants can adjust and how to most effectively guide their movement to enhance their neuromotor control.</p>\n<p>The outcome of our award are as follows. In Year 1, we showed that infants learned that their leg movement activated the robot, the robot motivated them to move, some mimicked the action of the robot (kicking a ball), and they responded best to robot responses that changed over time. We found that infants were more visually focused on the robot when the robot moved than when the robot just spoke. In Year 2, we found that infants were more likely to look at areas of the robot that were surprising. In addition, we developed personalized thresholds for leg movement activation of the robot to adapt and personalize the challenge level presented by the robot based on the infant's performance. In Year 3, we used a new analysis method, sample entropy, to quantify variability and exploration in infant leg movement patterns as the infants explored the contingent learning task. This was important for determining that infants did not merely move more to make the robot activate, they changed how they moved to make the robot activate. In addition, we found that infants with TD and AR tended to explore and move in different ways. In Year 3, we also used an additional tool, head-mounted eye tracking, to determine that infants demonstrated predictive gaze when they learn the robot reward was contingent on their behavior. This was important because the existing definition of an infant learning the task is only that the infant moved more, leading to possible misclassification since moving more can be due to other factors beyond learning that their leg movements activated the robot (e.g., general excitement, fussiness). We showed that infants who learned the task showed predictive gaze: they showed visual anticipation of the robot reward as opposed to a reactive visual response to the robot reward. This was true in both infants with TD and infants AR. In Year 4, we developed personalized models of infant behavior and infant-robot interaction to support robot action selection. In Year 5, we evaluated the impact of body pose features on infant affect recognition from video data collected during the infant-robot interaction studies. Our work found that models leveraging both pose and facial features showed superior performance compared to models using only one modality when evaluated across entire infant-robot interactions. Next, we demonstrated a meaningful relationship between the length of input time windows and affect prediction performance. Using our approach for evaluating affect prediction models, we showed that model performance changes over the course of an individual infant-robot interaction as infant behavior evolves.</p>\n<p>The broader impact of the results of our project are impacting both the field of socially assistive robotics (SAR) and the field of pediatric physical therapy. In SAR, the project has contributed effective models for classifying affective behaviors in infants, a novel and challenging user population. The development of models for real-time prediction of infant affect will support personalized SAR interactions to improve pediatric physical therapy intervention and the field of pediatric physical therapy. SAR has the potential to be used as an intervention to respond in a personalized way to each infant and reinforce desired infant behaviors. The results have been disseminated within the principal disciplines of the project.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/01/2022<br>\n\t\t\t\t\tModified by: Beth&nbsp;Smith</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1706964/1706964_10514963_1665595674642_FigureAndLegend--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1706964/1706964_10514963_1665595674642_FigureAndLegend--rgov-800width.jpg\" title=\"Infant-robot interaction\"><img src=\"/por/images/Reports/POR/2022/1706964/1706964_10514963_1665595674642_FigureAndLegend--rgov-66x44.jpg\" alt=\"Infant-robot interaction\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This setup was used to develop socially assistive robot-supported infant movement development, with a focus on infants at risk for developmental disabilities. It shows an infant, wearing a head-mounted eye-tracker and motion sensors on all four limbs, seated across from a socially assistive robot.</div>\n<div class=\"imageCredit\">Beth Smith</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Beth&nbsp;Smith</div>\n<div class=\"imageTitle\">Infant-robot interaction</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nTypically developing infants engage in motor babbling, exploratory movements that allow them to learn what movements lead to desired outcomes: obtaining a smile from a caregiver, reaching and grasping a toy, etc. Through this necessary developmental perception-action process, they learn to control their bodies and interact with the environment. In contrast to infants with typical development (TD), infants at risk (AR) for developmental disability often have neuromotor impairments involving strength, proprioception, or coordination that lead to greater difficulty with and decreased motivation for movement. Less motor babbling results in less exploration and suboptimal developmental interactions. The goal of this project was to develop a socially assistive, non-contact, infant-robot interaction system to provide contingent positive feedback to increase exploration and expand early movement practice. The intellectual merit of the results of our project is advancing knowledge about which aspects of movement infants can adjust and how to most effectively guide their movement to enhance their neuromotor control.\n\nThe outcome of our award are as follows. In Year 1, we showed that infants learned that their leg movement activated the robot, the robot motivated them to move, some mimicked the action of the robot (kicking a ball), and they responded best to robot responses that changed over time. We found that infants were more visually focused on the robot when the robot moved than when the robot just spoke. In Year 2, we found that infants were more likely to look at areas of the robot that were surprising. In addition, we developed personalized thresholds for leg movement activation of the robot to adapt and personalize the challenge level presented by the robot based on the infant's performance. In Year 3, we used a new analysis method, sample entropy, to quantify variability and exploration in infant leg movement patterns as the infants explored the contingent learning task. This was important for determining that infants did not merely move more to make the robot activate, they changed how they moved to make the robot activate. In addition, we found that infants with TD and AR tended to explore and move in different ways. In Year 3, we also used an additional tool, head-mounted eye tracking, to determine that infants demonstrated predictive gaze when they learn the robot reward was contingent on their behavior. This was important because the existing definition of an infant learning the task is only that the infant moved more, leading to possible misclassification since moving more can be due to other factors beyond learning that their leg movements activated the robot (e.g., general excitement, fussiness). We showed that infants who learned the task showed predictive gaze: they showed visual anticipation of the robot reward as opposed to a reactive visual response to the robot reward. This was true in both infants with TD and infants AR. In Year 4, we developed personalized models of infant behavior and infant-robot interaction to support robot action selection. In Year 5, we evaluated the impact of body pose features on infant affect recognition from video data collected during the infant-robot interaction studies. Our work found that models leveraging both pose and facial features showed superior performance compared to models using only one modality when evaluated across entire infant-robot interactions. Next, we demonstrated a meaningful relationship between the length of input time windows and affect prediction performance. Using our approach for evaluating affect prediction models, we showed that model performance changes over the course of an individual infant-robot interaction as infant behavior evolves.\n\nThe broader impact of the results of our project are impacting both the field of socially assistive robotics (SAR) and the field of pediatric physical therapy. In SAR, the project has contributed effective models for classifying affective behaviors in infants, a novel and challenging user population. The development of models for real-time prediction of infant affect will support personalized SAR interactions to improve pediatric physical therapy intervention and the field of pediatric physical therapy. SAR has the potential to be used as an intervention to respond in a personalized way to each infant and reinforce desired infant behaviors. The results have been disseminated within the principal disciplines of the project.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/01/2022\n\n\t\t\t\t\tSubmitted by: Beth Smith"
 }
}