{
 "awd_id": "1718540",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: High-Level Programming Models for GPUs",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927885",
 "po_email": "abanerje@nsf.gov",
 "po_sign_block_name": "Anindya Banerjee",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 390388.0,
 "awd_amount": 390388.0,
 "awd_min_amd_letter_date": "2017-05-31",
 "awd_max_amd_letter_date": "2017-05-31",
 "awd_abstract_narration": "Modern Graphics-Processor Units (GPUs) are capable of performance that, just a few years ago, would have been classified as supercomputer-level. With the trend of integrating GPU cores into heterogeneous multicore processors, GPUs are becoming an important source of future performance growth in mainstream processors.  Unfortunately, GPUs are notoriously hard to program, especially for irregular parallel computations.  This project aims to address the challenges of programming GPUs by supporting higher-level programming models with advanced compilation techniques.  The intellectual merits of the proposed work are that it advances the state of the art in compilation techniques and programming models for GPUs and other accelerator architectures.  The broader impact of the project is to widen the applicability of GPUs to a wider range of computational problems and, in turn, to help make GPUs useful to a broader community of users by supporting higher-level programming models for GPUs that are easier to program.\r\n\r\nThe project focuses on the use of Nested Data Parallelism (NDP) and the supporting global flattening transformation, which supports irregular parallelism by compiling it down to flat data parallelism.  While NDP provides a high-level elegant programming model for many kinds of irregular parallel computations, a straightforward implementation is not competitive with hand-written GPU code. The goal of this project it to develop and evaluate a collection of techniques for compiling NDP code to GPU with the objective of making NDP competitive with hand-written CUDA and OpenCL code.  The work will be carried out in the context of a compiler for Blelloch's NESL language, which is a small first-order functional language that embodies the core concepts of NDP. NESL provides a small, but expressive, context for the proposed research.  The work is evaluated by benchmarking against handwritten CUDA and OpenCL solutions for various irregular parallel algorithms.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Reppy",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "John H Reppy",
   "pi_email_addr": "jhr@cs.uchicago.edu",
   "nsf_id": "000115461",
   "pi_start_date": "2017-05-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Chicago",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606375418",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7943",
   "pgm_ref_txt": "PROGRAMMING LANGUAGES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 390388.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>GPUs and other accelerator architectures provide high-performance computing at a commodity cost, but their rapid hardware evolution and low-level programming models make them difficult to program. &nbsp;This is particularly true for programming irregular parallel computations on General-Purpose Graphics Processing Units (GP-GPUs). &nbsp;One promising approach to this problem is Nested Data Parallelism (NDP). &nbsp;Data-parallel computations are characterized by a structure of applying the same operation to a large number of different data values in parallel (e.g., adding two large vectors of numbers to produce a vector of sums). &nbsp;NDP generalizes this model by allowing the operation being applied to itself be a parallel operation. &nbsp;While NDP provides a high-level elegant programming model for many kinds of irregular parallel computations, the traditional straightforward implementation technique is not competitive with hand-written GPU code.</p>\n<p><br />The traditional approach to compiling NDP programs is to apply a global program transformation, called flattening, that converts the irregular NDP into a regular flat-data-parallel (FDP) program. &nbsp;Such an FDP program is a much better fit for the parallelism provided by a GPU. &nbsp;Unfortunately, the process of flattening can introduce many inefficiencies. &nbsp;The Nessie project has focused on developing better compilation techniques for compiling NDP code to GPUs. &nbsp;At the core of the project was a new intermediate representation (IR), called lambda-cu, which was designed to express the three distinct levels of computation involved: the serial CPU code, CPU tasks, and the computations performed by GPU threads. &nbsp;The project involved developing a scheduler for that uses integer-linear programming (ILP) to determine a schedule that supports optimal fusing of GPU kernels. &nbsp;The ILP model is based on empirical evaluation of the GPUs under various test loads. &nbsp;We developed a backend that generates CUDA code from the lambda-cu IR. &nbsp;The resulting generated code was competitive with hand-written GPU code on a number of benchmarks. &nbsp;To enable fusion during scheduling, it is necessary to analyze the \"shape\" of computations to identify potential opportunities for fusion. &nbsp;Previously, this analysis was performed after the global flattening transformation, but we developed a novel approach of analyzing the shape of the NDP code and then preserving the shape information during the flattening transformation. &nbsp;This analysis required a novel mechanism for determining and specifying the irregular structure of NDP data structures. &nbsp;The advantage of this approach is that it requires a less-complicated analysis and results in more precise information that enables additional optimizations --- for example, we can now detect when nested arrays have rectangular structure, which results in much more efficient computation. &nbsp;The results of the project have been published in open-access form. &nbsp;The software has been available to other researchers and educators, and has been used to teach a course in compiling parallel languages at the University of Copenhagen.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/13/2021<br>\n\t\t\t\t\tModified by: John&nbsp;H&nbsp;Reppy</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nGPUs and other accelerator architectures provide high-performance computing at a commodity cost, but their rapid hardware evolution and low-level programming models make them difficult to program.  This is particularly true for programming irregular parallel computations on General-Purpose Graphics Processing Units (GP-GPUs).  One promising approach to this problem is Nested Data Parallelism (NDP).  Data-parallel computations are characterized by a structure of applying the same operation to a large number of different data values in parallel (e.g., adding two large vectors of numbers to produce a vector of sums).  NDP generalizes this model by allowing the operation being applied to itself be a parallel operation.  While NDP provides a high-level elegant programming model for many kinds of irregular parallel computations, the traditional straightforward implementation technique is not competitive with hand-written GPU code.\n\n\nThe traditional approach to compiling NDP programs is to apply a global program transformation, called flattening, that converts the irregular NDP into a regular flat-data-parallel (FDP) program.  Such an FDP program is a much better fit for the parallelism provided by a GPU.  Unfortunately, the process of flattening can introduce many inefficiencies.  The Nessie project has focused on developing better compilation techniques for compiling NDP code to GPUs.  At the core of the project was a new intermediate representation (IR), called lambda-cu, which was designed to express the three distinct levels of computation involved: the serial CPU code, CPU tasks, and the computations performed by GPU threads.  The project involved developing a scheduler for that uses integer-linear programming (ILP) to determine a schedule that supports optimal fusing of GPU kernels.  The ILP model is based on empirical evaluation of the GPUs under various test loads.  We developed a backend that generates CUDA code from the lambda-cu IR.  The resulting generated code was competitive with hand-written GPU code on a number of benchmarks.  To enable fusion during scheduling, it is necessary to analyze the \"shape\" of computations to identify potential opportunities for fusion.  Previously, this analysis was performed after the global flattening transformation, but we developed a novel approach of analyzing the shape of the NDP code and then preserving the shape information during the flattening transformation.  This analysis required a novel mechanism for determining and specifying the irregular structure of NDP data structures.  The advantage of this approach is that it requires a less-complicated analysis and results in more precise information that enables additional optimizations --- for example, we can now detect when nested arrays have rectangular structure, which results in much more efficient computation.  The results of the project have been published in open-access form.  The software has been available to other researchers and educators, and has been used to teach a course in compiling parallel languages at the University of Copenhagen.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/13/2021\n\n\t\t\t\t\tSubmitted by: John H Reppy"
 }
}