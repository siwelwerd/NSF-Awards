{
 "awd_id": "1734304",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CompCog: Computational, distributed accounts of human memory: improving cognitive models",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 499969.0,
 "awd_amount": 499969.0,
 "awd_min_amd_letter_date": "2017-07-19",
 "awd_max_amd_letter_date": "2020-12-02",
 "awd_abstract_narration": "Memory is among the most impressive aspects of human cognition, allowing us to learn new words or new ideas from just a few examples.  However, the scientific understanding of how this learning occurs is limited.  This research project focuses on how learning occurs in the context of memory for language. Within the human mind, there is something like a dictionary that tells people what words mean (semantics) and how words are combined to make grammatical sentences (syntax). How does the mind learn this dictionary from experience with a language? Computer simulations can help science better understand this learning process. This scientific understanding can, in turn, help teach languages in the classroom and aid in the early detection of language deficits, whether it be developmental deficits in children, or age-related deficits in adults. Furthermore, improving the ability of computers to simulate language learning processes can also lead to the development of better technology such as machine translation, web search, and virtual assistants.  This project considers how a better understanding of language learning can help us avoid common pitfalls of memory connected to the use of language.  For example, humans easily over-generalize and judge a \"book by its cover\", associating certain occupations or personality traits with a gender.  If we know how people come up with associations between words and concepts, we can also detect and prevent prejudices in language to help ensure that artificial intelligence applications, such as web search, do not produce prejudiced results.  The project supports an interdisciplinary and diverse team of researchers and students at Penn State, attracting college students to engage with research in cognitive science and artificial intelligence.\r\n\r\nIn this project, the researchers are designing a new model of human memory, the Hierarchical Holographic Model.   This computational model helps explain certain aspects of how words and languages are learned.  The model draws on the successes of artificial intelligence and deep neural networks, and applies these insights to psychology.   With this model, the researchers investigate the question of whether human memory has the ability to detect arbitrarily indirect associations between concepts.  The model uses a recursive learning process, building on previously learned knowledge to acquire new knowledge, which allows the model to learn arbitrarily indirect and abstract relationships between words. The researchers consider evidence that sensitivity to abstract relations between words improves the ability of the computer model to learn syntax, such as parts-of-speech, and to use words appropriately to construct grammatical sentences. This work will be assessed against human language data and competing computational models. The success of the computational model should provide evidence that (1) language acquisition depends on indirect associations, and (2) human memory must be able to form indirect associations to facilitate it.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Prasenjit",
   "pi_last_name": "Mitra",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Prasenjit Mitra",
   "pi_email_addr": "pmitra@ist.psu.edu",
   "nsf_id": "000028784",
   "pi_start_date": "2020-12-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Reitter",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "David T Reitter",
   "pi_email_addr": "david.reitter@gmail.com",
   "nsf_id": "000543948",
   "pi_start_date": "2017-07-19",
   "pi_end_date": "2019-11-01"
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Kelly",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew A Kelly",
   "pi_email_addr": "m.alex.kelly@bucknell.edu",
   "nsf_id": "000801920",
   "pi_start_date": "2019-11-01",
   "pi_end_date": "2020-12-02"
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "316D IST Building",
  "perf_city_name": "State College",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 499969.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The key tasks outlined in the NSF grant proposal have been completed and have been published as we detail in what follows. The numbers refer to the section numbers in the original proposal.</p>\n<p>5.1 Task 1: Pure mediated semantic priming: Our proposed model, the Hierarchical Holographic Model (HHM), is unable toreplicate the pure mediated priming effect documented by Jones (2010). We did not pursue this task further. Instead, we pivoted to syntactic priming, which HHM can capture,though it is outperformed by transformer-based neural language models. We document ourfindings in Kelly et al. (2020).</p>\n<p>5.2 Task 2: Ordering words to form grammatical sentences:HHMs successfully performed the word ordering task. We document our findings inKelly et al. (2017) and in Kelly et al. (2019).</p>\n<p>5.3 Task 3: Judgements of acceptability: HHM is able to do the judgement of acceptability task, but not nearly as well asconventional neural language models. It also appears that much of the variability injudgements of acceptability is accounted for by spelling errors, rather than the syntacticrelationships HHM is designed to capture. We have a paper on our findings set in Wang et al. (2020).</p>\n<p>5.4 Task 4: Part of speech tagging and &ldquo;super-tagging&rdquo;:HHM is able to perform tagging or \"super-tagging\" of part of speech, as documentedin in Kelly et al. (2017) and in our upcoming paper Kelly et al. (2019).</p>\n<p>6.2 The semantic and syntactic relationship continuum:Additionally, we test HHM on Chomsky (1956)&rsquo;s classic nonsense sentence &ldquo;Colorless green ideas sleep furiously&rdquo; and find that HHM is able to detect that &ldquo;Colorless green ideas sleep furiously&rdquo; is more grammatical than the inverted alternative &ldquo;Furiously sleep ideas green colorless.&rdquo; This work is documented in our paper Kelly (2019),currently under review.</p>\n<p>To extend Kelly et al. (2020) will require additional syntactic priming data from human participants. Ph.D. candidate Zixin Tang explored an extension of the proposed work. HHM is a computational model of high-level, abstract, syntactic representations. Second-language speakers tend to apply first-language syntactic structures to their second-language,suggesting that in the mind of the speaker, there are syntactic representations that areshared between the first and second language. Zixin&rsquo;s work investigates what syntactic representations are shared between the first and second language in the mind of a second language speaker and how these sharedrepresentations can be modelled. HHM is one modelling approach that could be used tocapture the shared representations. Other approaches include formalisms from theoretical linguistics, such as minimalist grammar, harmonic grammar, or combinatory categorical grammar (Steedman &amp; Baldridge, 2011). Another approach would be to apply a deep neural language model, such as BERT (Devlin et al., 2019). Zixin&rsquo;s research is in an early,exploratory stage, and the precise formalism, theory, or modelling technique to be used has not yet been settled.</p>\n<p>References</p>\n<p>Chomsky, N. (1956). Three models for the description of language. IRE Transactions onInformation Theory, 2, 113&ndash;124. https://doi.org/10.1109/TIT.1956.1056813</p>\n<p>Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of deepbidirectional transformers for language understanding, In Proceedings of the 2019Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Minneapolis, Minnesota, Association forComputational Linguistics. https://doi.org/10.18653/v1/N19-1423</p>\n<p>Jones, L. L. (2010). Pure mediated priming: A retrospective semantic matching model.Journal of Experimental Psychology: Learning, Memory, and Cognition, 36 (1), 135.</p>\n<p>Kelly, M. A., Reitter, D., &amp; West, R. L. (2017). Degrees of separation in semantic andsyntactic relationships (M. K. van Vugt, A. P. Banks, &amp; W. G. Kennedy, Eds.). In M. K. van Vugt, A. P. Banks, &amp; W. G. Kennedy (Eds.), Proceedings of the 15thInternational Conference on Cognitive Modeling. Warwick, U.K., University ofWarwick.https://iccm-conference.neocities.org/2017/ICCMprogram_files/paper_42.pdf</p>\n<p>Kelly, M. A., Reitter, D., West, R., &amp; Ghafurian, M. (2019). Indirect associations inlearning semantic and syntactic lexical relationships. PsyArXiv.https://doi.org/10.31234/osf.io/ytnjp</p>\n<p>Kelly, M. A., Xu, Y., Calvillo, J., &amp; Reitter, D. (2020). Predicting syntactic priming fromsentence embedding vectors (S. Denison, M. Mack, Y. Xu, &amp; B. C. Armstrong,Eds.). In S. Denison, M. Mack, Y. Xu, &amp; B. C. Armstrong (Eds.), Proceedings of the42nd annual conference of the cognitive science society, Austin, TX, CognitiveScience Society. https://clcs.sdsu.edu/pubs/cogsci2020_whichsentence_0529.pdf</p>\n<p>Steedman, M., &amp; Baldridge, J. (2011). Combinatory categorial grammar (R. Borsley &amp;K. Borjars, Eds.). In R. Borsley &amp; K. Borjars (Eds.), Non-transformational syntax:Formal and explicit models of grammar. Wiley-Blackwell.</p>\n<p>Wang, J., Kelly, M. A., &amp; Reitter, D. (2020). Do we need neural models to explain humanjudgments of acceptability? (S. Denison, M. Mack, Y. Xu, &amp; B. C. Armstrong,Eds.). In S. Denison, M. Mack, Y. Xu, &amp; B. C. Armstrong (Eds.), Proceedings of the42nd annual conference of the cognitive science society, Austin, TX, CognitiveScience Society. https://arxiv.org/abs/1909.08663</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/02/2023<br>\n\t\t\t\t\tModified by: Prasenjit&nbsp;Mitra</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe key tasks outlined in the NSF grant proposal have been completed and have been published as we detail in what follows. The numbers refer to the section numbers in the original proposal.\n\n5.1 Task 1: Pure mediated semantic priming: Our proposed model, the Hierarchical Holographic Model (HHM), is unable toreplicate the pure mediated priming effect documented by Jones (2010). We did not pursue this task further. Instead, we pivoted to syntactic priming, which HHM can capture,though it is outperformed by transformer-based neural language models. We document ourfindings in Kelly et al. (2020).\n\n5.2 Task 2: Ordering words to form grammatical sentences:HHMs successfully performed the word ordering task. We document our findings inKelly et al. (2017) and in Kelly et al. (2019).\n\n5.3 Task 3: Judgements of acceptability: HHM is able to do the judgement of acceptability task, but not nearly as well asconventional neural language models. It also appears that much of the variability injudgements of acceptability is accounted for by spelling errors, rather than the syntacticrelationships HHM is designed to capture. We have a paper on our findings set in Wang et al. (2020).\n\n5.4 Task 4: Part of speech tagging and \"super-tagging\":HHM is able to perform tagging or \"super-tagging\" of part of speech, as documentedin in Kelly et al. (2017) and in our upcoming paper Kelly et al. (2019).\n\n6.2 The semantic and syntactic relationship continuum:Additionally, we test HHM on Chomsky (1956)\u2019s classic nonsense sentence \"Colorless green ideas sleep furiously\" and find that HHM is able to detect that \"Colorless green ideas sleep furiously\" is more grammatical than the inverted alternative \"Furiously sleep ideas green colorless.\" This work is documented in our paper Kelly (2019),currently under review.\n\nTo extend Kelly et al. (2020) will require additional syntactic priming data from human participants. Ph.D. candidate Zixin Tang explored an extension of the proposed work. HHM is a computational model of high-level, abstract, syntactic representations. Second-language speakers tend to apply first-language syntactic structures to their second-language,suggesting that in the mind of the speaker, there are syntactic representations that areshared between the first and second language. Zixin\u2019s work investigates what syntactic representations are shared between the first and second language in the mind of a second language speaker and how these sharedrepresentations can be modelled. HHM is one modelling approach that could be used tocapture the shared representations. Other approaches include formalisms from theoretical linguistics, such as minimalist grammar, harmonic grammar, or combinatory categorical grammar (Steedman &amp; Baldridge, 2011). Another approach would be to apply a deep neural language model, such as BERT (Devlin et al., 2019). Zixin\u2019s research is in an early,exploratory stage, and the precise formalism, theory, or modelling technique to be used has not yet been settled.\n\nReferences\n\nChomsky, N. (1956). Three models for the description of language. IRE Transactions onInformation Theory, 2, 113&ndash;124. https://doi.org/10.1109/TIT.1956.1056813\n\nDevlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of deepbidirectional transformers for language understanding, In Proceedings of the 2019Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Minneapolis, Minnesota, Association forComputational Linguistics. https://doi.org/10.18653/v1/N19-1423\n\nJones, L. L. (2010). Pure mediated priming: A retrospective semantic matching model.Journal of Experimental Psychology: Learning, Memory, and Cognition, 36 (1), 135.\n\nKelly, M. A., Reitter, D., &amp; West, R. L. (2017). Degrees of separation in semantic andsyntactic relationships (M. K. van Vugt, A. P. Banks, &amp; W. G. Kennedy, Eds.). In M. K. van Vugt, A. P. Banks, &amp; W. G. Kennedy (Eds.), Proceedings of the 15thInternational Conference on Cognitive Modeling. Warwick, U.K., University ofWarwick.https://iccm-conference.neocities.org/2017/ICCMprogram_files/paper_42.pdf\n\nKelly, M. A., Reitter, D., West, R., &amp; Ghafurian, M. (2019). Indirect associations inlearning semantic and syntactic lexical relationships. PsyArXiv.https://doi.org/10.31234/osf.io/ytnjp\n\nKelly, M. A., Xu, Y., Calvillo, J., &amp; Reitter, D. (2020). Predicting syntactic priming fromsentence embedding vectors (S. Denison, M. Mack, Y. Xu, &amp; B. C. Armstrong,Eds.). In S. Denison, M. Mack, Y. Xu, &amp; B. C. Armstrong (Eds.), Proceedings of the42nd annual conference of the cognitive science society, Austin, TX, CognitiveScience Society. https://clcs.sdsu.edu/pubs/cogsci2020_whichsentence_0529.pdf\n\nSteedman, M., &amp; Baldridge, J. (2011). Combinatory categorial grammar (R. Borsley &amp;K. Borjars, Eds.). In R. Borsley &amp; K. Borjars (Eds.), Non-transformational syntax:Formal and explicit models of grammar. Wiley-Blackwell.\n\nWang, J., Kelly, M. A., &amp; Reitter, D. (2020). Do we need neural models to explain humanjudgments of acceptability? (S. Denison, M. Mack, Y. Xu, &amp; B. C. Armstrong,Eds.). In S. Denison, M. Mack, Y. Xu, &amp; B. C. Armstrong (Eds.), Proceedings of the42nd annual conference of the cognitive science society, Austin, TX, CognitiveScience Society. https://arxiv.org/abs/1909.08663\n\n\t\t\t\t\tLast Modified: 05/02/2023\n\n\t\t\t\t\tSubmitted by: Prasenjit Mitra"
 }
}