{
 "awd_id": "1734868",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NCS-FO:Collaborative Research:Decoding and Reconstructing the Neural Basis of Real World Social Perception",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jonathan Fritz",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 490074.0,
 "awd_amount": 490074.0,
 "awd_min_amd_letter_date": "2017-08-07",
 "awd_max_amd_letter_date": "2017-08-07",
 "awd_abstract_narration": "Social and affective perception is the critical input that governs how we interact with others during everyday life. Consequently, having a model of the neurobiological basis of social and affective perception is critical for understanding the neural basis of human behavior. The overwhelming majority of our understanding of the neural basis of social and affective perception comes from studies done in artificial lab settings, which cannot capture the richness, complexity, and salience of real-world social interactions. This project aims to fill this gap in knowledge. To accomplish this goal, the researchers will record electrical brain activity from patients undergoing neurosurgical treatment for epilepsy. To determine the region of the brain responsible for their seizures, these patients are implanted with electrodes in various parts of their brain and then they spend 1-2 weeks in the hospital during which they interact with doctors, nurses, friend and family visitors, etc. This award will support research into using the recordings from their brains to understand how these patients perceive and understand the actions, emotions, and communication during these interactions on a moment-to-moment basis. The results of these studies have the potential to transform our understanding of social and affective perception by illuminating the neural basis of these processes during real life, meaningful interactions. The lack of models of the neural basis of natural, real world social and affective perception is a critical impediment to understanding these processes and ultimately a developing treatments for debilitating neurological and psychiatric disorders of social and affective perception, such as autism, post traumatic stress disorder, etc. In addition, through education, mentoring, and teaching, this award will provide an avenue for new researchers to take advantage of the rare and valuable opportunity for basic neuroscientific research provided by direct recordings from the human brain. This research is supported by the EHR Core Research Program, providing funding for fundamental research in STEM learning and learning environments, broadening participation in STEM, and STEM workforce development. \r\n\r\nModels of social visual perception developed using unnatural stimuli often assume that neurons have unchanging response sensitivity and are organized into bottom-up hierarchies. While some recent models acknowledge the role of feedback, they remain simplistic with a relatively limited number of core systems and often neglect of the role of social context and dynamic prior knowledge. These models are unlikely to fully generalize to natural social vision where the system can rapidly and actively adapt its response to optimize processing of rich and complex natural visual input. The PI and colleagues will combine intracranial EEG (iEEG) recordings captured during long stretches of natural visual behavior with cutting-edge computer vision, machine learning, and statistical analyses to understand the neural basis of natural, real-world visual perception. The goal of their program of research is to develop the first fully ecologically validated models of social perception. The researchers will use recent advances in iEEG in combination with cutting-edge gaze tracking technology, video analysis tools, and big data statistical and machine learning tools to understand the rapid, complex neural information processing that occurs during real-world social vision. The project will involve decoding the spatiotemporal patterns of neural activity and reconstruct the expressive features of people they see at these different levels on a moment-to-moment basis. The multidisciplinary nature of this project provides an excellent environment for students and postdocs to be trained in computational methods, statistics, and neuroscience. Given the rapid advance of high-level computational and statistical methods in neuroscience, this multidisciplinary training is critical for modern neuroscientists. Enhanced understanding of the mechanisms involved in social cognition has implications for teaching and learning. For example, knowing more about how people form impressions of one another can inform teachers' abilities to recognize and respond to students and other stakeholders in educational settings.\r\n\r\nThis project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NSF-NCS), a multidisciplinary program jointly supported by the Directorates for Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Max",
   "pi_last_name": "G'Sell",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Max G'Sell",
   "pi_email_addr": "mgsell@andrew.cmu.edu",
   "nsf_id": "000682802",
   "pi_start_date": "2017-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Louis-Philippe",
   "pi_last_name": "Morency",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Louis-Philippe Morency",
   "pi_email_addr": "morency@cs.cmu.edu",
   "nsf_id": "000519300",
   "pi_start_date": "2017-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "798000",
   "pgm_ele_name": "ECR-EDU Core Research"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "8551",
   "pgm_ref_txt": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0417",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001718DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 490074.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-9430eead-7fff-3657-c733-a89d1802c69b\">\n<p dir=\"ltr\"><span>This grant supported the development of new approaches to understanding the neural basis of natural visual facial expression and affect perception.&nbsp; Toward this goal, we created a new experimental framework for synchronously recording intracranial EEG (iEEG), eye gaze, and videos of the subject's field of view.&nbsp; Novel machine learning techniques were then used to process and annotate the videos, with the resulting annotations then linked to the subject's eye gaze information.&nbsp; This provided fully-annotated, multi-hour recordings of the object of fixation for every visual saccade of the subject; a level of detail that has never previously been available for natural viewing settings.&nbsp; This information on visual stimuli and saccade locations was then merged with the synchronous iEEG recordings to provide dense label information of the visual stimuli present during each moment of the continuous neural recordings.</span></p>\n<br />\n<p dir=\"ltr\"><span>The resulting data set was analyzed using novel techniques from statistical learning, demonstrating a range of new phenomena and discoveries about the neural basis of social perception.&nbsp; The team was able to characterize neural responses to the natural viewing of faces, and to make a range of comparisons for those responses.&nbsp; We were able to show a difference in the neural response to faces in the natural setting, compared to more traditional fixed viewing experiments, demonstrating that a broader range of neural circuits were involved in the response to natural viewing.&nbsp; The team also demonstrated that the neural response in natural viewing captured social context, by showing that the emotional state of viewed faces, along with facial features like lip and cheek position, jaw position, and head tilt, could be inferred from the neural responses.&nbsp; Finally, we were able to show that contextual information, encoded in the neural data before a saccade, influenced the neural response to the viewed face; and that further, the target of a saccade could be predicted ahead of time from the neural signal in regions associated with perceptual processing.</span></p>\n<br />\n<p dir=\"ltr\"><span>Toward these analyses, we developed new tools for analyzing visual information of social settings, and for modeling the connection between neural recordings and visual stimuli.&nbsp; To account for the uneven sampling of neural activity provided by iEEG probes, we developed sparse, localized factor models of multivariate neural activity.&nbsp; We designed new modeling techniques to incorporate pre-saccade contextual information into the tuning of classification thresholds for saccade-level neural data.&nbsp; We also developed novel models of temporal and spatial patterns in classification error to give insight into the localization of information about visual stimuli in neural recordings. We developed methods to analyze fine-grained facial expressions related to a person's emotional states. We also explored innovative ways to develop AI sensing technologies that are </span><span>privacy-aware</span><span>, reducing the possibility that identity-related markers are encoded in visual features, such as facial expressions. Finally, we developed a hybrid annotation tool which integrates state-of-the-art object recognition algorithms with judgements and annotations from human coders.</span></p>\n<br />\n<p dir=\"ltr\"><span>Results from this project were shared at top conferences and published in top journals in both neuroscience and machine learning, including Nature Communications, The Journal of Neuroscience, the Vision Sciences Society, the Society for Neuroscience, NeurIPS, Association for Computational Linguistics (ACL) and ICML.&nbsp; Concrete examples from the project were incorporated as examples in machine learning courses taught by the PIs.&nbsp;&nbsp;</span>The projects also provided training and experience for 2 postdoctoral researchers, 4 PhD students and 2 Masters students.</p>\n</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/25/2022<br>\n\t\t\t\t\tModified by: Louis-Philippe&nbsp;Morency</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThis grant supported the development of new approaches to understanding the neural basis of natural visual facial expression and affect perception.  Toward this goal, we created a new experimental framework for synchronously recording intracranial EEG (iEEG), eye gaze, and videos of the subject's field of view.  Novel machine learning techniques were then used to process and annotate the videos, with the resulting annotations then linked to the subject's eye gaze information.  This provided fully-annotated, multi-hour recordings of the object of fixation for every visual saccade of the subject; a level of detail that has never previously been available for natural viewing settings.  This information on visual stimuli and saccade locations was then merged with the synchronous iEEG recordings to provide dense label information of the visual stimuli present during each moment of the continuous neural recordings.\n\n\nThe resulting data set was analyzed using novel techniques from statistical learning, demonstrating a range of new phenomena and discoveries about the neural basis of social perception.  The team was able to characterize neural responses to the natural viewing of faces, and to make a range of comparisons for those responses.  We were able to show a difference in the neural response to faces in the natural setting, compared to more traditional fixed viewing experiments, demonstrating that a broader range of neural circuits were involved in the response to natural viewing.  The team also demonstrated that the neural response in natural viewing captured social context, by showing that the emotional state of viewed faces, along with facial features like lip and cheek position, jaw position, and head tilt, could be inferred from the neural responses.  Finally, we were able to show that contextual information, encoded in the neural data before a saccade, influenced the neural response to the viewed face; and that further, the target of a saccade could be predicted ahead of time from the neural signal in regions associated with perceptual processing.\n\n\nToward these analyses, we developed new tools for analyzing visual information of social settings, and for modeling the connection between neural recordings and visual stimuli.  To account for the uneven sampling of neural activity provided by iEEG probes, we developed sparse, localized factor models of multivariate neural activity.  We designed new modeling techniques to incorporate pre-saccade contextual information into the tuning of classification thresholds for saccade-level neural data.  We also developed novel models of temporal and spatial patterns in classification error to give insight into the localization of information about visual stimuli in neural recordings. We developed methods to analyze fine-grained facial expressions related to a person's emotional states. We also explored innovative ways to develop AI sensing technologies that are privacy-aware, reducing the possibility that identity-related markers are encoded in visual features, such as facial expressions. Finally, we developed a hybrid annotation tool which integrates state-of-the-art object recognition algorithms with judgements and annotations from human coders.\n\n\nResults from this project were shared at top conferences and published in top journals in both neuroscience and machine learning, including Nature Communications, The Journal of Neuroscience, the Vision Sciences Society, the Society for Neuroscience, NeurIPS, Association for Computational Linguistics (ACL) and ICML.  Concrete examples from the project were incorporated as examples in machine learning courses taught by the PIs.  The projects also provided training and experience for 2 postdoctoral researchers, 4 PhD students and 2 Masters students.\n\n\n\t\t\t\t\tLast Modified: 01/25/2022\n\n\t\t\t\t\tSubmitted by: Louis-Philippe Morency"
 }
}