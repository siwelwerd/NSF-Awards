{
 "awd_id": "1710657",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Building a Framework for Developing and Evaluating Contextualized Items in Science Assessment (DECISA)",
 "cfda_num": "47.076",
 "org_code": "11060000",
 "po_phone": "7032927303",
 "po_email": "jjesse@nsf.gov",
 "po_sign_block_name": "Jolene Jesse",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-09-30",
 "tot_intn_awd_amt": 1230321.0,
 "awd_amount": 1230321.0,
 "awd_min_amd_letter_date": "2017-01-17",
 "awd_max_amd_letter_date": "2020-04-23",
 "awd_abstract_narration": "Abstract\r\n\r\nThis collaborative project involving the University of Colorado at Denver and the University of Washington at Seattle in conjunction with Facet Innovations, will build a framework for addressing the use of contextualized items in the assessment of STEM learning. The primary goal is to systematically investigate the effects of characteristics of contextualized items on student performance to strengthen practices in science assessments, ensure fairness in science testing, and increase support for both assessment and instructional purposes. Test items with contexts are called contextualized items which include supplemental information that precedes or follows a test item question. Such information may include a description of a lab setup, a natural phenomenon, or a practical problem often depicted as a scenario, background, vignette, or cover story. The project findings will help to understand how students make sense of contextualized items focusing on complex scientific concepts that they usually encounter in science assessments. Currently, contextualized items are constructed from either conventional wisdom or non-contextualized item writing rules. Such items could mislead students to attend to irrelevant information or interfere with the targeted construct, and, therefore lead to inaccurate inferences about student learning. This project will develop a framework for developing items to help address this problem. Approximately, 70 classroom teachers and 4800 students, in secondary grades, will participate in the study. \r\n\r\nThe project will offer a theoretical articulation of the characteristics of contextualized items and empirically test the effects on student performance. It seeks to address four gaps in the literature on contextualized items: (1) insufficient knowledge about how to conceptualize construct-relevant contextualized items; (2) lack of research on contextualized items in science; (3) lack of research that systematically studies the characteristics of contexts and evaluates their effects on student performance; and (4) the need for studies that examine differential effects related to subgroups of students to gain a greater clarity of what types of contexts affect whom. The research design and data analyses will be guided by three research questions: (1) what are critical context characteristics that may affect student performance and should therefore be considered when developing science test items? (2) what are context characteristics associated with construct-relevant variance? (3) what context characteristics are associated with differential student performance patterns due to gender, ELL status, and socioeconomic-status variables?\r\n\r\nThe project will take place over three years through a two phase process. During Phase 1 the project will refine a proposed theoretical framework that will identify the item context characteristics and articulate the item development guidelines. During Phase 2, the goal is to apply the framework by selecting, revising, and developing science items with varying profiles of contexts; conduct field tests of the items; and perform a range of psychometric and statistical procedures with test scores, and qualitative analyses of students' cognitive interview responses and teacher interviews. Items resulting from this process will aim to evoke students' stored knowledge relevant to the content and/or process skills targeted. The project will involve a team of researchers specialized in assessment development and validation, science education, content knowledge, linguists, and expert classroom teachers. The item development approach and items generated from this project will have immediate implications for researchers and practitioners in science education nationally and internationally.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "EES",
 "org_div_long_name": "Div. of Equity for Excellence in STEM",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Maria Araceli",
   "pi_last_name": "Ruiz-Primo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Maria Araceli Ruiz-Primo",
   "pi_email_addr": "aruiz@stanford.edu",
   "nsf_id": "000079454",
   "pi_start_date": "2017-01-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "3160 Porter Drive",
  "perf_city_name": "Palo Alto",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943041212",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "798000",
   "pgm_ele_name": "ECR-EDU Core Research"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0414",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001415DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 1230321.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This collaborative project involving Stanford University and the University of Washington, Seattle in conjunction with Facet Innovations, studied the use of contextualized items in science. The primary goal was to systematically investigate effects of characteristics of contextualized items on student performance in order to address validity issues and strengthen practices in science assessments. Test items with contexts are called contextualized items and include supplemental information that precedes or follows a test item question. Such information includes a description of a lab setup, a natural phenomenon, or a practical problem often depicted as a scenario, background, vignette, or cover story. Currently, contextualized items are constructed from either conventional wisdom or writing rules on non-contextualized item, which could mislead students to focus on irrelevant information or interfere with the targeted construct, therefore resulting in inaccurate score inferences.</p>\n<p>In this project we developed:</p>\n<p>1. A coding system to gather information about characteristics of item context. This approach leads to the application of using Multiple Correspondence Analysis (MCA) to search for dimensions of codes that were associated with item difficulty.</p>\n<p>2. An approach to analyze and develop items based on the most critical physics content knowledge being assessed for two physics topics, Forces &amp; Motion and Energy.</p>\n<p>3. A testlet development approach that allows to gather confirming and disconfirming evidence about students&rsquo; understanding.</p>\n<p>4. A meta-survey that uses open-ended questions to approach Response Process Evaluation (RPE) of items.</p>\n<p><strong>Context Coding</strong>. This context coding system includes 65 questions that can be applied to contexts of different types: (1) <em>General Context of a Testlet</em>, a context (setting) for a testlet or cluster from which more than one question stems; (2) <em>Sub-Testlet Context</em>, contexts for a subset or sugroup of the questions within a testlet; (3) <em>Item Context in a Testlet</em>; and (4) <em>Independent Item Context</em>. An individual item is a stand-alone one that does not belong to a testlet. The intent of cataloging the context characteristics with this coding system was to learn about what item developers decide to include, ignore, or enhance in various science contextualized items.</p>\n<p>The coding attends to two context components: the <em>narrative</em> and the <em>illustrations</em>, if any. It also includes codes to gather information about technical qualities of items. The narrative coding focuses on two dimensions: characteristics of the storyline and characteristics of the subject-matter content, such as in a forces and motion item what information about the object(s), force(s), and motion(s) is provided. We are not aware of any context coding that focuses on content. The illustration coding focuses on the subject-matter content reflected in illustrations that should be considered by the test taker (e.g., does the illustration depict some type of force? Is the direction of motion depicted?)</p>\n<p><strong>Fundamental Ideas</strong>. We defined a <em>fundamental idea</em> as an essential piece of knowledge, understanding, or application of a specific skill that is required to correctly interpret, represent, and respond or solve a test item. Fundamental ideas are considered necessary for test takers to reason about an item. Our proposed list of fundamental ideas for the forces and motion topic was revised in different iterations by physics educators, physicists, physics doctoral students, and science education doctoral students. This list was used to guide our item development of prompts and options and were empirically verified with student responses and interviews.&nbsp;</p>\n<p><strong>Testlet Development Approach</strong>. Two perspectives guided our item development approach: (a) a <strong><em>conceptually oriented analysis</em></strong> in item development using <strong>fundamental ideas</strong> in forces and motion in terms of what can be inferred about students&rsquo; understanding from the correct and incorrect students responses; and (b) a <strong><em>prototype design</em></strong> that allows to gather information about students&rsquo; understanding from different angles and synthesize this information as evidence that can confirm and/or disconfirm the inferences about students&rsquo; understanding (How students&rsquo; responses to different items support, dispute, or modify the inferences made about what they understand?). To adequately determine the students&rsquo; level of understanding, we stress the need for providing both <em>confirming</em> and <em>disconfirming</em> evidence of inferences of student understandings, in terms of fundamental ideas and alternative ideas (facets), across different clusters of items. &nbsp;(facets)</p>\n<p><strong>Response Process <strong>Items&nbsp;</strong>Evaluation</strong>. We developed a 26-question survey, aiming to gather information about examinees&rsquo; reactions and perceptions to item contexts, prompts, and options that can help to revise items, and immediately test these revisions across new samples of examinees, if necessary.</p>\n<p>Approximately, 87 secondary science teachers and 2,700 students as well as more than 4,000 MTurk respondents, participated in the project. We collected and analyzed 189 students&rsquo; talk aloud protocols and interviews and about 428 respondents on MTurk provided information from 30 of the items developed in the project. Findings from this project offer novel perspectives to develop contextualized items and help to understand what may affect student performance and how to gather evidence about students&rsquo; understanding.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/31/2020<br>\n\t\t\t\t\tModified by: Maria Araceli&nbsp;Ruiz-Primo</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis collaborative project involving Stanford University and the University of Washington, Seattle in conjunction with Facet Innovations, studied the use of contextualized items in science. The primary goal was to systematically investigate effects of characteristics of contextualized items on student performance in order to address validity issues and strengthen practices in science assessments. Test items with contexts are called contextualized items and include supplemental information that precedes or follows a test item question. Such information includes a description of a lab setup, a natural phenomenon, or a practical problem often depicted as a scenario, background, vignette, or cover story. Currently, contextualized items are constructed from either conventional wisdom or writing rules on non-contextualized item, which could mislead students to focus on irrelevant information or interfere with the targeted construct, therefore resulting in inaccurate score inferences.\n\nIn this project we developed:\n\n1. A coding system to gather information about characteristics of item context. This approach leads to the application of using Multiple Correspondence Analysis (MCA) to search for dimensions of codes that were associated with item difficulty.\n\n2. An approach to analyze and develop items based on the most critical physics content knowledge being assessed for two physics topics, Forces &amp; Motion and Energy.\n\n3. A testlet development approach that allows to gather confirming and disconfirming evidence about students\u2019 understanding.\n\n4. A meta-survey that uses open-ended questions to approach Response Process Evaluation (RPE) of items.\n\nContext Coding. This context coding system includes 65 questions that can be applied to contexts of different types: (1) General Context of a Testlet, a context (setting) for a testlet or cluster from which more than one question stems; (2) Sub-Testlet Context, contexts for a subset or sugroup of the questions within a testlet; (3) Item Context in a Testlet; and (4) Independent Item Context. An individual item is a stand-alone one that does not belong to a testlet. The intent of cataloging the context characteristics with this coding system was to learn about what item developers decide to include, ignore, or enhance in various science contextualized items.\n\nThe coding attends to two context components: the narrative and the illustrations, if any. It also includes codes to gather information about technical qualities of items. The narrative coding focuses on two dimensions: characteristics of the storyline and characteristics of the subject-matter content, such as in a forces and motion item what information about the object(s), force(s), and motion(s) is provided. We are not aware of any context coding that focuses on content. The illustration coding focuses on the subject-matter content reflected in illustrations that should be considered by the test taker (e.g., does the illustration depict some type of force? Is the direction of motion depicted?)\n\nFundamental Ideas. We defined a fundamental idea as an essential piece of knowledge, understanding, or application of a specific skill that is required to correctly interpret, represent, and respond or solve a test item. Fundamental ideas are considered necessary for test takers to reason about an item. Our proposed list of fundamental ideas for the forces and motion topic was revised in different iterations by physics educators, physicists, physics doctoral students, and science education doctoral students. This list was used to guide our item development of prompts and options and were empirically verified with student responses and interviews. \n\nTestlet Development Approach. Two perspectives guided our item development approach: (a) a conceptually oriented analysis in item development using fundamental ideas in forces and motion in terms of what can be inferred about students\u2019 understanding from the correct and incorrect students responses; and (b) a prototype design that allows to gather information about students\u2019 understanding from different angles and synthesize this information as evidence that can confirm and/or disconfirm the inferences about students\u2019 understanding (How students\u2019 responses to different items support, dispute, or modify the inferences made about what they understand?). To adequately determine the students\u2019 level of understanding, we stress the need for providing both confirming and disconfirming evidence of inferences of student understandings, in terms of fundamental ideas and alternative ideas (facets), across different clusters of items.  (facets)\n\nResponse Process Items Evaluation. We developed a 26-question survey, aiming to gather information about examinees\u2019 reactions and perceptions to item contexts, prompts, and options that can help to revise items, and immediately test these revisions across new samples of examinees, if necessary.\n\nApproximately, 87 secondary science teachers and 2,700 students as well as more than 4,000 MTurk respondents, participated in the project. We collected and analyzed 189 students\u2019 talk aloud protocols and interviews and about 428 respondents on MTurk provided information from 30 of the items developed in the project. Findings from this project offer novel perspectives to develop contextualized items and help to understand what may affect student performance and how to gather evidence about students\u2019 understanding.\n\n \n\n\t\t\t\t\tLast Modified: 12/31/2020\n\n\t\t\t\t\tSubmitted by: Maria Araceli Ruiz-Primo"
 }
}