{
 "awd_id": "1726188",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI: High Performance Digital Pathology Using Big Data and Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2018-01-01",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2017-09-15",
 "awd_max_amd_letter_date": "2017-09-15",
 "awd_abstract_narration": "This project, developing a digital imaging system, aims to automatically characterize an enormous archive of digital images from a pathology lab. Based on open source programming language packages and using deep learning technologies, the effort entails developing software that will automatically annotate and classify these images. Thus the system consists of an annotated archive tool for high performance digital pathology involving digital images from pathology slides produced in clinical operations. This tool, presently unavailable, enables observation, annotation, and classification of images from tissues in pathology slides in order to create a very large data base that may be analyzed with algorithms that are designed to process and interpret the image data. By applying state of the art machine learning, the effort is expected to generate a sustainable facility to rapidly collect large amounts of data automatically. This facility enables deep learning systems to systematically address many operational challenges, such as ingestion of large, complex images.\r\n\r\nBroader Impacts:\r\nThe instrumentation provides a useful technology capability.  The work builds on the researchers' history of providing unencumbered resources for fields including human language technology and neuroscience. Several large, comprehensive databases of pathology slides will be released in an unencumbered manner; no comparable databases currently exist in terms of the quantity of data proposed. The urban setting of the project, as well as the diverse nature of the institution's client population, make it ideal for collecting this type of clinical data.  A new generation of healthcare professionals will be trained using these resources to validate their knowledge in the longer term.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Joseph",
   "pi_last_name": "Picone",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Joseph Picone",
   "pi_email_addr": "joseph.picone@gmail.com",
   "nsf_id": "000301445",
   "pi_start_date": "2017-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Iyad",
   "pi_last_name": "Obeid",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Iyad Obeid",
   "pi_email_addr": "iobeid@temple.edu",
   "nsf_id": "000231883",
   "pi_start_date": "2017-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yuri",
   "pi_last_name": "Persidsky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuri Persidsky",
   "pi_email_addr": "yuri.persidsky@temple.edu",
   "nsf_id": "000742304",
   "pi_start_date": "2017-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tunde",
   "pi_last_name": "Farkas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tunde Farkas",
   "pi_email_addr": "tunde.farkas@temple.edu",
   "nsf_id": "000742305",
   "pi_start_date": "2017-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Temple University",
  "inst_street_address": "1805 N BROAD ST",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2157077547",
  "inst_zip_code": "191226104",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "PA02",
  "org_lgl_bus_name": "TEMPLE UNIVERSITY-OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "QD4MGHFDJKU1",
  "org_uei_num": "QD4MGHFDJKU1"
 },
 "perf_inst": {
  "perf_inst_name": "Temple University",
  "perf_str_addr": "1947, North 12th Street, ENGR 70",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191226018",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "PA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "118900",
   "pgm_ele_name": "Major Research Instrumentation"
  },
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1189",
   "pgm_ref_txt": "MAJOR RESEARCH INSTRUMENTATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"WordSection1\">\n<p>There are three significant outcomes from this project:</p>\n<p>&nbsp;First, an Aperio AT2 scanner has been deployed at Temple Hospital for four years and is being used on a daily basis to scan slides for our corpora. It is also used by pathologists to support their tumor board reviews and personal research projects. We scan approximately 400 slides per day and also support the daily operational needs of the pathologists. We have integrated the scanner into our computer cluster - neuronix.nedcdata.org. Neuronix is unique in that we have successfully connected three highly secure networks (Temple?s main campus public network and two HIPAA-secured private networks) so that we can easily move data from hospital operations to research, once the data is properly deidentified. The networks are interconnected via VPNs and satisfy all necessary security protocols. All students and staff allowed access to these networks have undergone thorough HIPAA and CITI training and are certified to handle this type of data. The deidentified data is released via an anonymous rsync server and directly browsable from our organization?s web site. Our ability to access Temple Hospital data directly, deidentify it under an approved IRB protocol, and release it as open source data makes this facility fairly unique.</p>\n<p>Second, we have scanned over 80,000 slides including a 13,000+ slide subset provided by Fox Chase Cancer Center (FCCC, affiliated with Temple). We have released an annotated breast tissue corpus containing 3,505 slides. A rough breakdown of pathologies included in the corpus include: Breast Tumor (6%), Urinary Prostate (36%), Gastrointestinal (18%), Lymph Nodes (8%) and Other (32%). The FCCC subset contains 18 types of tissue (38.5% prostate, 16.5% gynecological, 45% other). We will release the remaining unannotated data as well in Spring 2022 to support machine learning experiments in unsupervised and self-supervised learning. Annotations are released in two formats: csv and xml. The former is the preference of researchers; the latter is what is generated from our annotation tools.</p>\n<p>Corresponding patient reports and supporting immunohistochemical stains are provided as part of the metadata available with the corpus. The microscopic diagnoses given by the primary pathologist in these reports detail the pathological findings within each tissue site, but not within each specific slide. The microscopic diagnoses informed our annotation process. Further differentiation of cancerous and precancerous labels, as well as the location of their focus on a slide, was accomplished with supplemental immunohistochemically (IHC) stained slides. When distinguishing whether a focus is a nonneoplastic feature versus a cancerous growth, pathologists employ antigen targeting stains to the tissue in question to confirm the diagnosis. For example, a nonneoplastic feature of usual ductal hyperplasia will display diffuse staining for cytokeratin&nbsp;5 (CK5) and no diffuse staining for estrogen receptor (ER), while a cancerous growth of ductal carcinoma in situ will have negative or focally positive staining for CK5 and diffuse staining for ER. Many tissue samples contain cancerous and non-cancerous features with morphological overlaps that cause variability between annotators. The informative fields IHC slides provide could play an integral role in machine model pathology diagnostics.</p>\n<p>Third, we developed an automated image classification system based on deep learning principles that classifies regions with an accuracy of at least 75% for all but one label (nneo). This system is available as open source software. An overview of the system is provided in the attached image labeled Figure 4. The core deep learning technology used is based on Convolutional Neural Networks (CNNs) using a configuration known as ResNet18. Transfer learning is used to augment the training process. Traditional image processing methods such as binarization, dilation, erosion, hole filling and region removal based on ranks are used to segment the images, rank areas and form regions of interest, which are known as patches. This not only improves performance but also greatly reduces the overall processing time. A postprocessing step is applied in which patch detections are compared with their neighbors and adjusted using a majority voting scheme of its 8 neighbors to make the final decision. This approach removes a minority of mis-detected patches and greatly increases accuracy.</p>\n<p>&nbsp;A summary of performance is shown in the attached image labeled Figure 5. This provides performance in the form of a confusion matrix. Whole slide classification performance (e.g., cancer vs. non-cancerous) is very high ? 94% accuracy. &nbsp;The code is written in Python and makes use of the Pytorch&nbsp; machine learning package.</p>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/01/2022<br>\n\t\t\t\t\tModified by: Joseph&nbsp;Picone</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638569442_figure_04--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638569442_figure_04--rgov-800width.jpg\" title=\"Automated Classification of Digital Pathology Images\"><img src=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638569442_figure_04--rgov-66x44.jpg\" alt=\"Automated Classification of Digital Pathology Images\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An overview of our automated digital pathology classification system based on deep learning principles.</div>\n<div class=\"imageCredit\">Joseph Picone</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Joseph&nbsp;Picone</div>\n<div class=\"imageTitle\">Automated Classification of Digital Pathology Images</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638353408_figure_02--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638353408_figure_02--rgov-800width.jpg\" title=\"A Typical Digital Image\"><img src=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638353408_figure_02--rgov-66x44.jpg\" alt=\"A Typical Digital Image\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This is a typical image produced by scanning a slide containing breast tissue. Images are extremely high resolution - often 50K x 50K pixels - and average 300 Mbytes in size.</div>\n<div class=\"imageCredit\">Joseph Picone</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Joseph&nbsp;Picone</div>\n<div class=\"imageTitle\">A Typical Digital Image</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638479410_figure_03--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638479410_figure_03--rgov-800width.jpg\" title=\"An Annotated Image\"><img src=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638479410_figure_03--rgov-66x44.jpg\" alt=\"An Annotated Image\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">For each slide, we typically annotate 5 to 10 regions related to the particular pathology of interest and five regions of \"background\". This provides a rich amount of training data for our machine learning systems.</div>\n<div class=\"imageCredit\">Joseph Picone</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Joseph&nbsp;Picone</div>\n<div class=\"imageTitle\">An Annotated Image</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638258276_figure_01--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638258276_figure_01--rgov-800width.jpg\" title=\"The Aperio AT2 Scanner\"><img src=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643638258276_figure_01--rgov-66x44.jpg\" alt=\"The Aperio AT2 Scanner\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An Aperio AT2 scanner has been in operation a Temple Hospital for four years and has been integrated into the pathologists' workflow for board reviews and special research interests.</div>\n<div class=\"imageCredit\">Joseph Picone</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Joseph&nbsp;Picone</div>\n<div class=\"imageTitle\">The Aperio AT2 Scanner</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643639170242_figure_05--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643639170242_figure_05--rgov-800width.jpg\" title=\"A Confusion Matrix for Our Image Classification System\"><img src=\"/por/images/Reports/POR/2022/1726188/1726188_10522322_1643639170242_figure_05--rgov-66x44.jpg\" alt=\"A Confusion Matrix for Our Image Classification System\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A table containing error rates for our image classification system in the form of a confusion matrix. All but one label (nneo) are classified with an accuracy greater than 75%.</div>\n<div class=\"imageCredit\">Joseph Picone</div>\n<div class=\"imageSubmitted\">Joseph&nbsp;Picone</div>\n<div class=\"imageTitle\">A Confusion Matrix for Our Image Classification System</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n\nThere are three significant outcomes from this project:\n\n First, an Aperio AT2 scanner has been deployed at Temple Hospital for four years and is being used on a daily basis to scan slides for our corpora. It is also used by pathologists to support their tumor board reviews and personal research projects. We scan approximately 400 slides per day and also support the daily operational needs of the pathologists. We have integrated the scanner into our computer cluster - neuronix.nedcdata.org. Neuronix is unique in that we have successfully connected three highly secure networks (Temple?s main campus public network and two HIPAA-secured private networks) so that we can easily move data from hospital operations to research, once the data is properly deidentified. The networks are interconnected via VPNs and satisfy all necessary security protocols. All students and staff allowed access to these networks have undergone thorough HIPAA and CITI training and are certified to handle this type of data. The deidentified data is released via an anonymous rsync server and directly browsable from our organization?s web site. Our ability to access Temple Hospital data directly, deidentify it under an approved IRB protocol, and release it as open source data makes this facility fairly unique.\n\nSecond, we have scanned over 80,000 slides including a 13,000+ slide subset provided by Fox Chase Cancer Center (FCCC, affiliated with Temple). We have released an annotated breast tissue corpus containing 3,505 slides. A rough breakdown of pathologies included in the corpus include: Breast Tumor (6%), Urinary Prostate (36%), Gastrointestinal (18%), Lymph Nodes (8%) and Other (32%). The FCCC subset contains 18 types of tissue (38.5% prostate, 16.5% gynecological, 45% other). We will release the remaining unannotated data as well in Spring 2022 to support machine learning experiments in unsupervised and self-supervised learning. Annotations are released in two formats: csv and xml. The former is the preference of researchers; the latter is what is generated from our annotation tools.\n\nCorresponding patient reports and supporting immunohistochemical stains are provided as part of the metadata available with the corpus. The microscopic diagnoses given by the primary pathologist in these reports detail the pathological findings within each tissue site, but not within each specific slide. The microscopic diagnoses informed our annotation process. Further differentiation of cancerous and precancerous labels, as well as the location of their focus on a slide, was accomplished with supplemental immunohistochemically (IHC) stained slides. When distinguishing whether a focus is a nonneoplastic feature versus a cancerous growth, pathologists employ antigen targeting stains to the tissue in question to confirm the diagnosis. For example, a nonneoplastic feature of usual ductal hyperplasia will display diffuse staining for cytokeratin 5 (CK5) and no diffuse staining for estrogen receptor (ER), while a cancerous growth of ductal carcinoma in situ will have negative or focally positive staining for CK5 and diffuse staining for ER. Many tissue samples contain cancerous and non-cancerous features with morphological overlaps that cause variability between annotators. The informative fields IHC slides provide could play an integral role in machine model pathology diagnostics.\n\nThird, we developed an automated image classification system based on deep learning principles that classifies regions with an accuracy of at least 75% for all but one label (nneo). This system is available as open source software. An overview of the system is provided in the attached image labeled Figure 4. The core deep learning technology used is based on Convolutional Neural Networks (CNNs) using a configuration known as ResNet18. Transfer learning is used to augment the training process. Traditional image processing methods such as binarization, dilation, erosion, hole filling and region removal based on ranks are used to segment the images, rank areas and form regions of interest, which are known as patches. This not only improves performance but also greatly reduces the overall processing time. A postprocessing step is applied in which patch detections are compared with their neighbors and adjusted using a majority voting scheme of its 8 neighbors to make the final decision. This approach removes a minority of mis-detected patches and greatly increases accuracy.\n\n A summary of performance is shown in the attached image labeled Figure 5. This provides performance in the form of a confusion matrix. Whole slide classification performance (e.g., cancer vs. non-cancerous) is very high ? 94% accuracy.  The code is written in Python and makes use of the Pytorch  machine learning package.\n\n\n\t\t\t\t\tLast Modified: 02/01/2022\n\n\t\t\t\t\tSubmitted by: Joseph Picone"
 }
}