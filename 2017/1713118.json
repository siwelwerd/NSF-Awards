{
 "awd_id": "1713118",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:  Statistical Inference for High-Frequency Data",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 140562.0,
 "awd_amount": 140562.0,
 "awd_min_amd_letter_date": "2017-05-24",
 "awd_max_amd_letter_date": "2017-05-24",
 "awd_abstract_narration": "To pursue the promise of the big data revolution, the current project is concerned with a particular form of such data, high frequency data (HFD), where series of observations can see data updates in fractions of milliseconds. With technological advances in data collection, HFD occurs in medicine (from neuroscience to patient care), finance and economics, geosciences (such as earthquake data), marine science (fishing and shipping), and other areas. The research focuses on how to extract information from complex big data and how to turn data into knowledge. In particular, the project aims to develop cutting-edge mathematics and statistical methodology to uncover the dependence structure governing a HFD system. The new dependence structure will permit the \"borrowing\" of information from adjacent time periods, and also from other series from a panel of data. It is expected that the results will lead to more efficient estimators and better prediction and that this approach will form a new paradigm for HFD. In addition to developing a general theory, the project is concerned with applications to financial data, including risk management, forecasting, and portfolio management. More precise estimators, with improved margins of error, will be useful in all these areas of finance. The results are expected to be of interest to investors, regulators, and policymakers, and the results are entirely in the public domain. \r\n\r\nThe goal of this project is to create a unified framework for inference in high frequency data, based on dividing the observations and the parameter process into blocks. The work pursues two paths, both involving the fundamental structure of the data architecture. A \"within block\" approach uses contiguity to make the structure of the observations more accessible in local neighborhoods. The \"between block\" approach sets up a tool for using stochastic calculus to study the relationship between parameters in blocks that are adjacent (in time and space). It also permits the integration of high and low frequency models. This is achieved without altering current models. A final part of the project is devoted to further study of the observed asymptotic variance, in particular work on tuning parameters and inferential interpretation. Both the \"within block\" and \"between block\" approaches are formulated to cover general time varying \"parameters\" that are usually estimated from high frequency data series, not only volatility, but also skewness (leverage effect), regression coefficients, and parameter dynamics (such as volatility of volatility). In both cases, the observed data and also parameter processes may have large dimension (large panel size) in addition to high frequency observation. The within block approach permits contiguity to be stated jointly for the latent underlying processes and the microstructure/observation noise. For the between block approach, the investigators will further develop a new way to look at the dependence relationships between the parameters.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lan",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lan Zhang",
   "pi_email_addr": "lanzhang@uic.edu",
   "nsf_id": "000313767",
   "pi_start_date": "2017-05-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Chicago",
  "inst_street_address": "809 S MARSHFIELD AVE M/C 551",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3129962862",
  "inst_zip_code": "606124305",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "IL07",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "W8XEAJDKMXH3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Chicago",
  "perf_str_addr": "601 S Morgan St, UH2430",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606077124",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "IL07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 140562.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">Background: This project focuses on creating a statistical approach to handling big data in real-time application, thus turning data into knowledge. The vast amount of data comes from its high frequency and its high dimensionality. High frequency reflects the faster arriving nature of the data: for example, financial quotes and trades occur in irregular intervals of milli-seconds or even nano-seconds. High dimension refers to a large portfolio with hundreds of asset at any given moment. This type of big data poses challenges far beyond the capability of standard statistical devices.<span>&nbsp;</span></p>\n<p class=\"p1\">Intellectual Merit: This project creates a unified framework for inference in high frequency data, based on dividing the observations, and the parameter process, into time blocks. We pursue two paths, both involving the fundamental structure of the data architecture. A within block approach uses probabilistic approximations to make the structure of the observations more accessible in local neighborhoods. The between block approach sets up a mathematical tool to relate the latent parameters that are adjacent in time and/or space.<span>&nbsp;</span></p>\n<p class=\"p1\">The availability of high frequency financial data has generated a substantial number of estimators based on intra-day data, improving the quality of large areas of financial econometrics. However, estimating the standard error of these estimators is often forbidding due to the complexity of the microstructure, as well as time and edge irregularity in a short time frame.<span>&nbsp; </span>A main finding of this project is the construction of an off-the-shelf uncertainty measure in \"Assessment of Uncertainty in High Frequency Data: The Observed Asymptotic Variance\" (Econometrica, 2017), and the further robust implementation in \"The Observed Asymptotic Variance: Hard Edges, and a Regression Approach\" (Journal of Econometrics, 2021).&nbsp;</p>\n<p class=\"p1\">Another contribution of the project is to develop an inferential method that is robust to the edge effects (the phasing in and out of the estimator in each block). Edge effects are mildly problematic when estimating latent quantities for one day or longer, but can overwhelm<span>&nbsp; </span>estimation within minutes or hours. For the estimation of volatility in the presence of microstructure noise, this problem is mostly solved in \"The Algebra of Two Scales Estimation, and the S-TSRV: High Frequency Estimation that is Robust to Sampling Times\" (Journal of Econometrics, 2019). The Smoothed TSRV (S-TSRV) combines pre-averaging and two-scales estimation in a way that leads to the cancellation of most of the edge effect. The cancellation, which is based on algebraic properties, also removes the effect of irregular sampling times, again to high order. A key result is that estimators based on noisy data have a strong representation in terms of estimators based on data that are free of noise. Apart from being useful in its own right, this technology also enters crucially in the work on high dimensional data discussed below.</p>\n<p class=\"p1\">A third contribution is to develop a feasible Principal Component Analysis (PCA) in the setting of big data. This is a data-driven unsupervised learning approach. Armed with this approach in \"The Five Trolls under the Bridge: Principal Component Analysis with Asynchronous and Noisy High Frequency Data\" (JASA, 2020), the PIs develop a robust covariance estimator ( Smoothed-TSRV) for data with microstructure noise. We then pass from estimated time-varying covariance matrix to PCA. In an application, our first principal component (PC) closely matches but potentially outperforms the S&amp;P 100 market index. The accuracy of our PCA may provide a firmer footing on which to export the index concept to markets (such as commodities) where there is less theoretical basis for how to weigh index components. Indices currently do exist in these cases, of course, but with less foundation than in the case for equities. Indices have substantial social value.</p>\n<p class=\"p1\">Broader Impacts: The data are mostly financial, and interest focuses on quantities like realized daily volatility, correlations, leverage effect, volatility risk, fraction of jumps, and so on. We also work on applications to risk management, forecasting, and portfolio management. More precise estimators, with more precise standard errors, will be useful in all these areas of finance. In particular, as we have seen in the recent financial crisis, markets can change abruptly. Better measurements of market behavior will make early detection possible when market conditions change, greatly aiding the management of risk. The results will be of interest to investors, regulators and policymakers. The dependence structure also has use in other areas of research that use high frequency data, including neural science, earthquakes, turbulence, and other areas where streaming data are available.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/25/2022<br>\n\t\t\t\t\tModified by: Lan&nbsp;Zhang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Background: This project focuses on creating a statistical approach to handling big data in real-time application, thus turning data into knowledge. The vast amount of data comes from its high frequency and its high dimensionality. High frequency reflects the faster arriving nature of the data: for example, financial quotes and trades occur in irregular intervals of milli-seconds or even nano-seconds. High dimension refers to a large portfolio with hundreds of asset at any given moment. This type of big data poses challenges far beyond the capability of standard statistical devices. \nIntellectual Merit: This project creates a unified framework for inference in high frequency data, based on dividing the observations, and the parameter process, into time blocks. We pursue two paths, both involving the fundamental structure of the data architecture. A within block approach uses probabilistic approximations to make the structure of the observations more accessible in local neighborhoods. The between block approach sets up a mathematical tool to relate the latent parameters that are adjacent in time and/or space. \nThe availability of high frequency financial data has generated a substantial number of estimators based on intra-day data, improving the quality of large areas of financial econometrics. However, estimating the standard error of these estimators is often forbidding due to the complexity of the microstructure, as well as time and edge irregularity in a short time frame.  A main finding of this project is the construction of an off-the-shelf uncertainty measure in \"Assessment of Uncertainty in High Frequency Data: The Observed Asymptotic Variance\" (Econometrica, 2017), and the further robust implementation in \"The Observed Asymptotic Variance: Hard Edges, and a Regression Approach\" (Journal of Econometrics, 2021). \nAnother contribution of the project is to develop an inferential method that is robust to the edge effects (the phasing in and out of the estimator in each block). Edge effects are mildly problematic when estimating latent quantities for one day or longer, but can overwhelm  estimation within minutes or hours. For the estimation of volatility in the presence of microstructure noise, this problem is mostly solved in \"The Algebra of Two Scales Estimation, and the S-TSRV: High Frequency Estimation that is Robust to Sampling Times\" (Journal of Econometrics, 2019). The Smoothed TSRV (S-TSRV) combines pre-averaging and two-scales estimation in a way that leads to the cancellation of most of the edge effect. The cancellation, which is based on algebraic properties, also removes the effect of irregular sampling times, again to high order. A key result is that estimators based on noisy data have a strong representation in terms of estimators based on data that are free of noise. Apart from being useful in its own right, this technology also enters crucially in the work on high dimensional data discussed below.\nA third contribution is to develop a feasible Principal Component Analysis (PCA) in the setting of big data. This is a data-driven unsupervised learning approach. Armed with this approach in \"The Five Trolls under the Bridge: Principal Component Analysis with Asynchronous and Noisy High Frequency Data\" (JASA, 2020), the PIs develop a robust covariance estimator ( Smoothed-TSRV) for data with microstructure noise. We then pass from estimated time-varying covariance matrix to PCA. In an application, our first principal component (PC) closely matches but potentially outperforms the S&amp;P 100 market index. The accuracy of our PCA may provide a firmer footing on which to export the index concept to markets (such as commodities) where there is less theoretical basis for how to weigh index components. Indices currently do exist in these cases, of course, but with less foundation than in the case for equities. Indices have substantial social value.\nBroader Impacts: The data are mostly financial, and interest focuses on quantities like realized daily volatility, correlations, leverage effect, volatility risk, fraction of jumps, and so on. We also work on applications to risk management, forecasting, and portfolio management. More precise estimators, with more precise standard errors, will be useful in all these areas of finance. In particular, as we have seen in the recent financial crisis, markets can change abruptly. Better measurements of market behavior will make early detection possible when market conditions change, greatly aiding the management of risk. The results will be of interest to investors, regulators and policymakers. The dependence structure also has use in other areas of research that use high frequency data, including neural science, earthquakes, turbulence, and other areas where streaming data are available.\n\n \n\n\t\t\t\t\tLast Modified: 01/25/2022\n\n\t\t\t\t\tSubmitted by: Lan Zhang"
 }
}