{
 "awd_id": "1704932",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Causal Inference: Identification, Learning, and Decision-Making",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 265000.0,
 "awd_amount": 265000.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2017-07-27",
 "awd_abstract_narration": "Understanding the causal mechanisms underlying an observed phenomenon is one of the primary goals of science. The realization that statistical associations in themselves are insufficient for elucidating those mechanisms has led researchers to enrich traditional statistical analysis with techniques based on \"causal inference\". Most of the recent advances in the field, however, operate under overly optimistic assumptions, which are often not met in practical, large-scale situations. This project seeks to develop a sound and general causal inference theory to cover those situations. The goal is to design a framework for decision-making of intelligent systems, including (1) learning a causal representation of the data-generating environment (learning), (2) performing efficient inference leveraging the learned model (planning/inference), and (3) using the new inferred representation, based on (1) and (2), to decide how to act next (decision-making). The new finding will benefit investigators in every area of the empirical sciences, including artificial intelligence, machine learning, statistics, economics, and the health and social sciences. The research is expected to fundamentally change the practice of data science in areas where the standard causal assumptions are violated (i.e., missing data, selection bias, and confounding bias). The work on decision-making is expected to pave the way toward the design of an \"automated scientist\", i.e., a program that combines both observational and experimental data, conducts its own experiments, and decides on the best choices of actions and policies. The project also helps to disseminate the principles of causal inference throughout the sciences by (1) engaging in the establishment of new \"data science\" curriculum where causal inference plays a central role, and (2) developing new educational materials for students and the general public explaining the practice of causal inference (e.g., book). Furthermore, the project supports the causal inference community by fostering a number of educational initiatives such as forums, workshops, and the creation of new incentives for the development of educational material (e.g., a \"Causality Education Award\").\r\n\r\nMaking claims about the existence of causal connections (structural learning), the magnitude of causal effects (identification), and designing optimal interventions (decision-making) are some of the most important tasks found throughout data-driven fields. This project studies identification, learning, and decision-making settings where (1) data are missing not at random, (2) non-parametric estimation is not feasible, and (3) aggregated behavior does not translate into guidance for individual-level decision-making. Specifically, the project considers the problem when measurements are systematically distorted (missing data), which has received an enormous amount of attention in the statistical literature, but has not essentially been investigated in the context of causal inference when data are missing not at random. The project further aims to leverage the special properties of linear models, the most common first approximation to non-parametric causal inference, to elucidate causal relationships in data, and to facilitate sensitivity analysis in such models.  Finally, the project considers the fundamental problem on how causal and counterfactual knowledge can speed-up experimentation and support principled decision-making. The goal is to develop a complete algorithmic theory to determine when a particular causal effect can be learned from data and how to incorporate causal knowledge learned (possibly by experimentation) so that it can be amortized over new environmental conditions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Judea",
   "pi_last_name": "Pearl",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Judea Pearl",
   "pi_email_addr": "judea@cs.ucla.edu",
   "nsf_id": "000179905",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "UCLA Computer Science Department",
  "perf_str_addr": "4532 Boelter Hall",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951596",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 265000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Inferring causal effects from large data collections is a fundamental challenge found throughout the data sciences. There is a growing literature developing a solid understanding of the conditions under which causal conclusions can be drawn from a combination of observational and experimental datasets. This project focuses on three tightly-connected topics of causality research, namely, 1) identification of causal distributions, 2) learning a qualitative description of the underlying causal mechanisms (i.e., constructing a causal explanation), and making decisions based on uncertain and only partially specified causally knowledge about the underlying environment. The results developed in this project have appeared in dozens of papers in the leading artificial intelligence and machine learning venues and were disseminated to talks, lectures, workshops, and tutorials. Below are some of these highlights (see the PI's report for a more detailed description).</span><br /><br /><span>A) Identification: Causal effect identifiability is concerned with establishing the effect of intervening on a set of variables on another set of variables from observational or interventional distributions under causal assumptions that are usually encoded in the form of a causal graph. We investigated the generalization of this problem in many different and pervasive dimensions, including partial-observability, stochastic interventions, and Markov-equivalence classes.</span><span>&nbsp;&nbsp;</span><br /><br /><span>&nbsp;</span><span>B. Learning: The challenge of learning the causal structure underlying a particular phenomenon is undertaken by connecting the set of conditional independences (CIs) readable from the observational data, on the one side, with the set of corresponding constraints implied over the graphical structure, on the other, which are tied through a graphical criterion known as d-separation. We introduced a graphical representation that can be used to determine if two causal graphs are interventionally equivalent. Given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions. We provided a formal graphical characterization of this equivalence. We further extended the FCI algorithm, originally designed to operate based on CIs, to combine observational and interventional datasets, including new orientation rules particular to this setting.</span><br /><span>&nbsp;</span><br /><span>C. Decision-Making: A dynamic treatment regime (DTR) consists of a sequence of decision rules, one per stage of intervention, that dictates how to determine patients' treatment assignment based on evolving treatments and covariates' history. These regimes are particularly useful for managing chronic disorders and are arguably one of the critical aspects of developing more personalized decision-making systems.</span><br /><br /><span>In addition to these technical results, the work of co-PI Pearl received several recognitions, including the 2020 AIJ Classic Paper Award as well as an honorary fellowship of the Royal Statical Society. Also, during the course of this grant, the co-PI published the <em>Book of Why</em> (joint with D. Mackenzie), which became a best seller and has been recognized in different science-related venues; for further details, see also</span><span>&nbsp;</span><span id=\"OBJ_PREFIX_DWT6501_com_zimbra_url\" class=\"Object\"><span id=\"OBJ_PREFIX_DWT6553_com_zimbra_url\" class=\"Object\"><a href=\"http://bayes.cs.ucla.edu/WHY/\" target=\"_blank\">http://bayes.cs.ucla.edu/WHY/</a></span></span><span>.</span><span>&nbsp;</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/09/2020<br>\n\t\t\t\t\tModified by: Judea&nbsp;Pearl</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nInferring causal effects from large data collections is a fundamental challenge found throughout the data sciences. There is a growing literature developing a solid understanding of the conditions under which causal conclusions can be drawn from a combination of observational and experimental datasets. This project focuses on three tightly-connected topics of causality research, namely, 1) identification of causal distributions, 2) learning a qualitative description of the underlying causal mechanisms (i.e., constructing a causal explanation), and making decisions based on uncertain and only partially specified causally knowledge about the underlying environment. The results developed in this project have appeared in dozens of papers in the leading artificial intelligence and machine learning venues and were disseminated to talks, lectures, workshops, and tutorials. Below are some of these highlights (see the PI's report for a more detailed description).\n\nA) Identification: Causal effect identifiability is concerned with establishing the effect of intervening on a set of variables on another set of variables from observational or interventional distributions under causal assumptions that are usually encoded in the form of a causal graph. We investigated the generalization of this problem in many different and pervasive dimensions, including partial-observability, stochastic interventions, and Markov-equivalence classes.  \n\n B. Learning: The challenge of learning the causal structure underlying a particular phenomenon is undertaken by connecting the set of conditional independences (CIs) readable from the observational data, on the one side, with the set of corresponding constraints implied over the graphical structure, on the other, which are tied through a graphical criterion known as d-separation. We introduced a graphical representation that can be used to determine if two causal graphs are interventionally equivalent. Given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions. We provided a formal graphical characterization of this equivalence. We further extended the FCI algorithm, originally designed to operate based on CIs, to combine observational and interventional datasets, including new orientation rules particular to this setting.\n \nC. Decision-Making: A dynamic treatment regime (DTR) consists of a sequence of decision rules, one per stage of intervention, that dictates how to determine patients' treatment assignment based on evolving treatments and covariates' history. These regimes are particularly useful for managing chronic disorders and are arguably one of the critical aspects of developing more personalized decision-making systems.\n\nIn addition to these technical results, the work of co-PI Pearl received several recognitions, including the 2020 AIJ Classic Paper Award as well as an honorary fellowship of the Royal Statical Society. Also, during the course of this grant, the co-PI published the Book of Why (joint with D. Mackenzie), which became a best seller and has been recognized in different science-related venues; for further details, see also http://bayes.cs.ucla.edu/WHY/. \n\n\t\t\t\t\tLast Modified: 10/09/2020\n\n\t\t\t\t\tSubmitted by: Judea Pearl"
 }
}