{
 "awd_id": "1652569",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Plenoptic Signal Processing --- A Framework for Sampling, Detection, and Estimation using Plenoptic Functions",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2017-02-15",
 "awd_exp_date": "2024-01-31",
 "tot_intn_awd_amt": 532000.0,
 "awd_amount": 564000.0,
 "awd_min_amd_letter_date": "2017-02-09",
 "awd_max_amd_letter_date": "2021-02-19",
 "awd_abstract_narration": "The interactions of light with objects in a scene are often complex. An image --- which only captures 2D spatial variations --- is poorly equipped to unravel these interactions and infer properties of a scene including its shape, reflectance, and its composition. This is especially true for scenes that have sharp reflections, refractions, and volumetric scattering. This research models interactions of light with scenes using light rays and their transformations. The central hypothesis underlying the research is the idea that problems of shape, reflectance and material composition estimation are often simpler and well-posed when they are studied using light rays and their transformations. A wide-range of real-world objects and scenes stand to benefit from progress made in this research; this includes scenes with complex configurations that lead to inter-reflections,  objects with shine, specularities, and spatially-varying reflectances, as well as objects that are transparent, or translucent. A diverse set of applications including machine vision, microscopy, and consumer photography stand to benefit from this research. The education and outreach components of this project disseminates image processing research in the broader Pittsburgh area via camera building workshops and lab demos for middle/high-school students, and professional development courses for physics teachers.\r\n\r\nThe focus of the research is to develop novel acquisition and processing methods for scene understanding by studying characterizations of light that go beyond images. In particular, the research analyzes the properties of two signals: the plenoptic function, which captures spatial, temporal, angular, and spectral variations of light, and the plenoptic light transport, which captures how light propagates through a scene. The central hypothesis of the research is that the plenoptic function and light transport provide a rich encoding of how light interacts with a scene; hence, unlike image-based inference, plenoptic inference can be fundamentally well-conditioned even for scenes that interact with light in a complex manner.  To this end, the research develops novel low-dimensional models for plenoptic functions that are based on physical laws governing interaction of light with a scene. The research also builds novel computational cameras that acquire light propagates in a scene by decomposing into light paths of varying complexity, and subsequently estimating the 3D shape, reflectance, and material composition.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aswin",
   "pi_last_name": "Sankaranarayanan",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Aswin C Sankaranarayanan",
   "pi_email_addr": "saswin@andrew.cmu.edu",
   "nsf_id": "000623495",
   "pi_start_date": "2017-02-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  },
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7797",
   "pgm_ref_txt": "COMM & INFORMATION FOUNDATIONS"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 249534.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 113483.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 95136.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 105847.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This CAREER project looked at shaping properties of light and its propagation in a scene with the goal of simplifying computer vision problems relating to shape, reflectance and material composition estimation. Such problems are often hard to solve due to inherent ambiguities in the solution as well as the computationally intensive processing that is involved. The project addresses these from the mindset that the imaging system performs computations in optics and that a judicious design of the imaging instrument&mdash;one that leverages all the dimensions of light and its transport&mdash;can provide important design-space degrees of freedom for improved robustness in solutions.</p>\n<p><strong>Intellectual Merit.&nbsp;</strong>The results in this project have led to advancements in many of the classical problems in computer vision. It has advanced 3D scanning, especially in many of its hardest instances. This includes scanning of objects with very complex geometries, where the object exhibits rich self-occlusion; a key result in this space looked at the use of kaleidoscopes for getting a dense set of imagery from which we extract a full 3D model of the object. Objects with spatially varying reflectance is another instance where 3D scanning is hard; here, the presence of specular reflections, shine, and iridescence makes visual scanning hard. The project also led to a rich body of results on 3D scanning of objects that are beyond the line of sight, as well as those that are in scattering media (for example, fog and biological tissue). The research has also led to high-speed 3D scanners that use single-photon sensors to perform structured light-based scanning.</p>\n<p>&nbsp;The spectral properties of light play a very important role in the study the material composition of a scene, finding applications in diverse scientific fields. This often requires measuring a hyperspectral image of a scene, which captures how the scene varies both in space and spectrum---a process that is extremely time-consuming. To alleviate this, we have relied on two key observations. First, despite the high dimensional nature of hyperspectral images, the number of distinct materials in any given scene is very small; this leads to a concise low-dimensional representation of the hyperspectral image. Second, owing to this low diversity, capturing a small set of spectrally filtered images of the scene suffices for most sensing and inference tasks. Exploiting these two observations, we have built many computational imaging systems that effectively and even adaptive probe the scene to identify materials. We have developed new tools to modulate the spectral properties of light, as well as characterize fundamental resolution limits in how well space and spectrum can be resolved with certain classes of spectral cameras. All of this lays the groundwork for making hyperspectral cameras more practical by introducing computing into the sensing pipeline and moving the computational burden into the optical domain.</p>\n<p>Our work has also led to innovations in photography, including the development of novel under-display cameras, where the camera is placed behind the display. We have looked at post-capture manipulation of lighting in the photograph both using physical laws of image formation as well as data-driven manipulation techniques.</p>\n<p>Given that light and its propagation is central to imaging, we have found applications of our work in applications as disparate as energy systems and virtual reality display design. We have developed systems that use images of the cloud to forecast solar power availability; since occlusion by clouds are the primary reason for loss of solar power, we explored data-driven method to model cloud evolution in sky images to predict occlusion in future time instants. Displays, on the other hand, are the optical dual of cameras. Our work looked at light manipulations that would provide natural focusing and occlusion cues in display systems. It has led to the development of new 3D displays that reduce the gap between reality and the virtual work presented to the eye.</p>\n<p><strong>Broader Impacts.&nbsp;</strong>This project has spurred a range of activities for K-12 outreach. We have offered multiple workshops targeting middle and high school students. These workshops spanned topics in photography, 3D scanning, displays, microscopy and image processing. Sankaranarayanan mentored many undergraduate researchers on research projects; nine of them went on to pursue doctoral research in computer vision and imaging related topics. Sankaranarayanan also organized the IEEE International Conference on Computational Photography, at Pittsburgh, in 2018. The research outcomes of this work were also integrated in the courses taught by the PI; this included photography-related work in his image processing class, the use of Fourier optics for in-class demos in his signal processing class, and showcasing of relevant results in his deep learning class. A survey article on computational photography as well as numerous popular science articles were written for broader dissemination.</p><br>\n<p>\n Last Modified: 06/04/2024<br>\nModified by: Aswin&nbsp;C&nbsp;Sankaranarayanan</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552578613_LightCurtain--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552578613_LightCurtain--rgov-800width.png\" title=\"Light curtains\"><img src=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552578613_LightCurtain--rgov-66x44.png\" alt=\"Light curtains\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Scattering media like fog scramble light propagation in a scene leading to a loss in contrast (as seen in the center image). We have designed a system called light curtain that images only along light paths that are not scattered by the media, which provides a high contrast image (shown in right).</div>\n<div class=\"imageCredit\">Aswin Sankaranarayanan</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Aswin&nbsp;C&nbsp;Sankaranarayanan\n<div class=\"imageTitle\">Light curtains</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552239118_HSI2--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552239118_HSI2--rgov-800width.png\" title=\"Programmable spectrometry\"><img src=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552239118_HSI2--rgov-66x44.png\" alt=\"Programmable spectrometry\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We propose programmable spectrometry, a camera that can be  changed to classify materials optically. The parameters of the cameras are learnt jointly with a classifier like a support vector machine or a neural network. (left) Shown are classification results between real and fake materials.</div>\n<div class=\"imageCredit\">Aswin Sankaranarayanan</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Aswin&nbsp;C&nbsp;Sankaranarayanan\n<div class=\"imageTitle\">Programmable spectrometry</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552941549_VR--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552941549_VR--rgov-800width.png\" title=\"Dense focal stacks\"><img src=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552941549_VR--rgov-66x44.png\" alt=\"Dense focal stacks\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We have designed a 3D display that creates a virtual scene as a dense set of planes, each at a different distance from the eye. Shown above are two images, as seen by the eye when focused at different distances. Such a display naturally satisfies the eyes' ability to focus at different distances.</div>\n<div class=\"imageCredit\">Aswin Sankaranarayanan</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Aswin&nbsp;C&nbsp;Sankaranarayanan\n<div class=\"imageTitle\">Dense focal stacks</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552196331_Kaleidoscope--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552196331_Kaleidoscope--rgov-800width.png\" title=\"Shape from Kaleidoscope image\"><img src=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552196331_Kaleidoscope--rgov-66x44.png\" alt=\"Shape from Kaleidoscope image\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Recovering a full 3D model of an object is a hard problem, requiring images and depth from any view point. Here we obtain such models from a single image of the object using a kaleidoscope. The kaleidoscope provides hundreds of views, from which we tease out the full shape.</div>\n<div class=\"imageCredit\">Aswin Sankaranarayanan</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Aswin&nbsp;C&nbsp;Sankaranarayanan\n<div class=\"imageTitle\">Shape from Kaleidoscope image</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552298577_HSI--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552298577_HSI--rgov-800width.png\" title=\"KRISM - An Hyperspectral Camera\"><img src=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552298577_HSI--rgov-66x44.png\" alt=\"KRISM - An Hyperspectral Camera\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">KRISM is a novel camera design that adaptive senses a low-rank approximation to the scene's hyperspectral image. Shown above is the camera design and a representative result showing high spatial and spectral resolution.</div>\n<div class=\"imageCredit\">Aswin Sankaranarayanan</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Aswin&nbsp;C&nbsp;Sankaranarayanan\n<div class=\"imageTitle\">KRISM - An Hyperspectral Camera</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552358427_IllumSep--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552358427_IllumSep--rgov-800width.png\" title=\"Separating a photograph based on illumination color.\"><img src=\"/por/images/Reports/POR/2024/1652569/1652569_10472582_1717552358427_IllumSep--rgov-66x44.png\" alt=\"Separating a photograph based on illumination color.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Scenes are often light by multiple light sources. We have designed a technique to decompose a photo based on illumination color. Shown above is an outdoor scene, lit by two sources---a warm indoor light and a cool skylight. We are able to separate it to the images shown on the right.</div>\n<div class=\"imageCredit\">Aswin Sankaranarayanan</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Aswin&nbsp;C&nbsp;Sankaranarayanan\n<div class=\"imageTitle\">Separating a photograph based on illumination color.</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis CAREER project looked at shaping properties of light and its propagation in a scene with the goal of simplifying computer vision problems relating to shape, reflectance and material composition estimation. Such problems are often hard to solve due to inherent ambiguities in the solution as well as the computationally intensive processing that is involved. The project addresses these from the mindset that the imaging system performs computations in optics and that a judicious design of the imaging instrumentone that leverages all the dimensions of light and its transportcan provide important design-space degrees of freedom for improved robustness in solutions.\n\n\nIntellectual Merit.The results in this project have led to advancements in many of the classical problems in computer vision. It has advanced 3D scanning, especially in many of its hardest instances. This includes scanning of objects with very complex geometries, where the object exhibits rich self-occlusion; a key result in this space looked at the use of kaleidoscopes for getting a dense set of imagery from which we extract a full 3D model of the object. Objects with spatially varying reflectance is another instance where 3D scanning is hard; here, the presence of specular reflections, shine, and iridescence makes visual scanning hard. The project also led to a rich body of results on 3D scanning of objects that are beyond the line of sight, as well as those that are in scattering media (for example, fog and biological tissue). The research has also led to high-speed 3D scanners that use single-photon sensors to perform structured light-based scanning.\n\n\nThe spectral properties of light play a very important role in the study the material composition of a scene, finding applications in diverse scientific fields. This often requires measuring a hyperspectral image of a scene, which captures how the scene varies both in space and spectrum---a process that is extremely time-consuming. To alleviate this, we have relied on two key observations. First, despite the high dimensional nature of hyperspectral images, the number of distinct materials in any given scene is very small; this leads to a concise low-dimensional representation of the hyperspectral image. Second, owing to this low diversity, capturing a small set of spectrally filtered images of the scene suffices for most sensing and inference tasks. Exploiting these two observations, we have built many computational imaging systems that effectively and even adaptive probe the scene to identify materials. We have developed new tools to modulate the spectral properties of light, as well as characterize fundamental resolution limits in how well space and spectrum can be resolved with certain classes of spectral cameras. All of this lays the groundwork for making hyperspectral cameras more practical by introducing computing into the sensing pipeline and moving the computational burden into the optical domain.\n\n\nOur work has also led to innovations in photography, including the development of novel under-display cameras, where the camera is placed behind the display. We have looked at post-capture manipulation of lighting in the photograph both using physical laws of image formation as well as data-driven manipulation techniques.\n\n\nGiven that light and its propagation is central to imaging, we have found applications of our work in applications as disparate as energy systems and virtual reality display design. We have developed systems that use images of the cloud to forecast solar power availability; since occlusion by clouds are the primary reason for loss of solar power, we explored data-driven method to model cloud evolution in sky images to predict occlusion in future time instants. Displays, on the other hand, are the optical dual of cameras. Our work looked at light manipulations that would provide natural focusing and occlusion cues in display systems. It has led to the development of new 3D displays that reduce the gap between reality and the virtual work presented to the eye.\n\n\nBroader Impacts.This project has spurred a range of activities for K-12 outreach. We have offered multiple workshops targeting middle and high school students. These workshops spanned topics in photography, 3D scanning, displays, microscopy and image processing. Sankaranarayanan mentored many undergraduate researchers on research projects; nine of them went on to pursue doctoral research in computer vision and imaging related topics. Sankaranarayanan also organized the IEEE International Conference on Computational Photography, at Pittsburgh, in 2018. The research outcomes of this work were also integrated in the courses taught by the PI; this included photography-related work in his image processing class, the use of Fourier optics for in-class demos in his signal processing class, and showcasing of relevant results in his deep learning class. A survey article on computational photography as well as numerous popular science articles were written for broader dissemination.\t\t\t\t\tLast Modified: 06/04/2024\n\n\t\t\t\t\tSubmitted by: AswinCSankaranarayanan\n"
 }
}