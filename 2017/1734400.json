{
 "awd_id": "1734400",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: INT: Co-Multi-Robotic Exploration of the Benthic Seafloor - New Methods for Distributed Scene Understanding and Exploration in the Presence of Communication Constraints",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928950",
 "po_email": "rwachter@nsf.gov",
 "po_sign_block_name": "Ralph Wachter",
 "awd_eff_date": "2018-01-01",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 1337101.0,
 "awd_amount": 1337101.0,
 "awd_min_amd_letter_date": "2017-08-24",
 "awd_max_amd_letter_date": "2017-08-24",
 "awd_abstract_narration": "This project addresses the control and communications among underwater robotic vehicles to explore and map in ocean environments, where the communications are inherently low bandwidth, may be degraded and even disrupted due to natural ocean phenomena.  The research focuses on coordinating robots and cooperating teams of such robots under such conditions with limited human intervention.  It is expected the principles learned from this project will be generalizable to deployment of teams of cooperating robots operating in harsh environments with similar communication challenges such as what might be expected in the aftermath of natural disasters.\r\n \r\nThe project addresses technical challenges with robots learning to describe their environment using high-level descriptors, using state of the art unsupervised machine learning techniques, and exchanging compact messages with each other to keep the description model consistent across all the cooperating robots. This high-level scene description is used by the robots to efficiently communicate the state of the exploration to each other, and to the human operator as possible. Furthermore, the human operators can efficiently control the robot team by specifying their interests in terms of these learned scene descriptors. The automatically generated exploration trajectories aim to maximize the information content, human interest, and spatial coverage, while taking into account the difficult constraints imposed by communication range in such harsh environments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yogesh",
   "pi_last_name": "Girdhar",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Yogesh A Girdhar",
   "pi_email_addr": "ygirdhar@whoi.edu",
   "nsf_id": "000693174",
   "pi_start_date": "2017-08-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Kinsey",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "James C Kinsey",
   "pi_email_addr": "jkinsey@whoi.edu",
   "nsf_id": "000526097",
   "pi_start_date": "2017-08-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Brian",
   "pi_last_name": "Claus",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brian Claus",
   "pi_email_addr": "bclaus@whoi.edu",
   "nsf_id": "000718729",
   "pi_start_date": "2017-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Woods Hole Oceanographic Institution",
  "inst_street_address": "266 WOODS HOLE RD",
  "inst_street_address_2": "",
  "inst_city_name": "WOODS HOLE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "5082893542",
  "inst_zip_code": "025431535",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "MA09",
  "org_lgl_bus_name": "WOODS HOLE OCEANOGRAPHIC INSTITUTION",
  "org_prnt_uei_num": "",
  "org_uei_num": "GFKFBWG2TV98"
 },
 "perf_inst": {
  "perf_inst_name": "Woods Hole Oceanographic Institution",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "025431041",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "MA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 1337101.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Untetherd underwater robots operate under severe bandwidth and sensing constraint, which has limited their deplyment on missions that require close interactions with a human operator. The problem becomes especially challenging when these robots use vision as their primary sensing modaity which eliminates direct human control of the robots; and when the environment is unknown, which is the case with the vast majority of our oceans. This project has developed several key technologies and algorithms to mitigate these issues. First we developed and proposed the use of an unsupervised online learning algorithm for processing seafloor imagery, to produce continusoly updating semantic seafloor maps. We then propose the use of these maps to rapidly learn a reward fuction online by intelligently asking the human operators quations about what to pay attention to. This reward function when used in an informative path planning framework, enables interactive control of a vision guided robot that can operate in low bandwith conditions. Scaling the approach to multiple robots, we developed an approach for a team of robots to efficienly communicate with each other to learn a shared unsupervised semantic representation model that is consistent across all the robots, and can then be used to learn a shared reward function, enabling interactive control a team of vision guided robots. In support of the main research thread, we developed the CUREE robot system, with 6 cameras and sufficient computing power and manunverability to enable vision guided operations in low altitude on complex terrains.</p>\n<p>The techniques developed during this project have potential to significantly impact how we understand our oceans, and the ecosystems they contain, which till now have been hard to study. This project has focused on monitoring coral reefs, which are currently degrading worldwide, as the primary application of the research. Field testing of the robots has been a large component of this research. This interdisciplinary nature of this project has enabled unique training of several graduate and undergraduate students in robotics, AI/ML, fieldwork in remote environments, and marine ecology.&nbsp;</p>\n<p>The project will continue with the support of external funding aimed at increasing the technoilogical readiness level of the work and potential commercialization, and&nbsp;collaborations with several marine ecologists to scale up coral reef monitoring restoration efforts.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/29/2023<br>\n\t\t\t\t\tModified by: Yogesh&nbsp;A&nbsp;Girdhar</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1734400/1734400_10518680_1685352610935_curee-hydrophone-glamour--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1734400/1734400_10518680_1685352610935_curee-hydrophone-glamour--rgov-800width.jpg\" title=\"CUREE: Curious Underwater Robot for Ecosystem Exploration\"><img src=\"/por/images/Reports/POR/2023/1734400/1734400_10518680_1685352610935_curee-hydrophone-glamour--rgov-66x44.jpg\" alt=\"CUREE: Curious Underwater Robot for Ecosystem Exploration\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">CUREE is an autonomous vision guided robot designed for interactive monitoring of complex environments such as coral reefs.</div>\n<div class=\"imageCredit\">Austin Greene</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Yogesh&nbsp;A&nbsp;Girdhar</div>\n<div class=\"imageTitle\">CUREE: Curious Underwater Robot for Ecosystem Exploration</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nUntetherd underwater robots operate under severe bandwidth and sensing constraint, which has limited their deplyment on missions that require close interactions with a human operator. The problem becomes especially challenging when these robots use vision as their primary sensing modaity which eliminates direct human control of the robots; and when the environment is unknown, which is the case with the vast majority of our oceans. This project has developed several key technologies and algorithms to mitigate these issues. First we developed and proposed the use of an unsupervised online learning algorithm for processing seafloor imagery, to produce continusoly updating semantic seafloor maps. We then propose the use of these maps to rapidly learn a reward fuction online by intelligently asking the human operators quations about what to pay attention to. This reward function when used in an informative path planning framework, enables interactive control of a vision guided robot that can operate in low bandwith conditions. Scaling the approach to multiple robots, we developed an approach for a team of robots to efficienly communicate with each other to learn a shared unsupervised semantic representation model that is consistent across all the robots, and can then be used to learn a shared reward function, enabling interactive control a team of vision guided robots. In support of the main research thread, we developed the CUREE robot system, with 6 cameras and sufficient computing power and manunverability to enable vision guided operations in low altitude on complex terrains.\n\nThe techniques developed during this project have potential to significantly impact how we understand our oceans, and the ecosystems they contain, which till now have been hard to study. This project has focused on monitoring coral reefs, which are currently degrading worldwide, as the primary application of the research. Field testing of the robots has been a large component of this research. This interdisciplinary nature of this project has enabled unique training of several graduate and undergraduate students in robotics, AI/ML, fieldwork in remote environments, and marine ecology. \n\nThe project will continue with the support of external funding aimed at increasing the technoilogical readiness level of the work and potential commercialization, and collaborations with several marine ecologists to scale up coral reef monitoring restoration efforts. \n\n\t\t\t\t\tLast Modified: 05/29/2023\n\n\t\t\t\t\tSubmitted by: Yogesh A Girdhar"
 }
}