{
 "awd_id": "1718262",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Modeling Vividness and Symbolism for Decoding Visual Rhetoric",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 449978.0,
 "awd_amount": 449978.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2019-04-24",
 "awd_abstract_narration": "This project develops systems for analyzing and inferring the non-literal messages conveyed in the media through persuasive images and text. Computational representations of two persuasive strategies are devised to model the mapping between observable information and underlying messages. First, this project models \"vividness\": through analyses of how human subjects perceive images and text, the system aims to identify relevant regions in which creative techniques were used to draw the viewer's attention. Second, this project models \"symbolism\": through analyses of semantic relationships between concrete objects and abstract concepts, the system aims to decode symbolic associations that humans make. The ability to automatically understand vividness and symbolism is key to building computational intelligence that can make inferences about what the media implies. This interdisciplinary project also has an educational component of potentially increasing the media literacy of school students, and involving college students from diverse backgrounds into computational research. The work can be used to discover patterns in how the visual rhetoric in the media evolved over time or how it differs in different cultures.\r\n\r\nThis research pursues three directions. First, a framework for judging vividness (i.e., to what degree an image as a whole is vivid; what part of an image is vivid; and whether a text snippet is vivid) is developed. Data about the vividness of a variety of images and text is collected from human annotators. Cues and techniques such as saliency, attention, sentiment, memorability and abnormality are used to build prediction models for vividness. Second, two pipelines for detecting symbolic references are developed. One pipeline hypothesizes potential signifiers from an image, then uses textual resources to map these to signifieds. The other pipeline directly hypothesizes what the signifieds might be, and obtains training data for these from web resources. The outputs from these pipelines are combined to generate the signifier-signified pairs. Third, a method for generating explanations of the strategies is developed, using the vividness and symbolism outputs. Numerous resources to be shared with the research community are developed over the course of the project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Adriana",
   "pi_last_name": "Kovashka",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Adriana Kovashka",
   "pi_email_addr": "kovashka@cs.pitt.edu",
   "nsf_id": "000690095",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Rebecca",
   "pi_last_name": "Hwa",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rebecca Hwa",
   "pi_email_addr": "rebecca.hwa@gwu.edu",
   "nsf_id": "000462757",
   "pi_start_date": "2017-07-27",
   "pi_end_date": "2019-04-24"
  }
 ],
 "inst": {
  "inst_name": "University of Pittsburgh",
  "inst_street_address": "4200 FIFTH AVENUE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4126247400",
  "inst_zip_code": "152600001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "MKAGLD59JRL1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pittsburgh",
  "perf_str_addr": "",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152132303",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 449978.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project transformed the computer vision field in two ways.  First, it put an emphasis on the importance of analyzing persuasive  media, e.g. advertisements and images in political articles. We  developed a variety of techniques for visual reasoning that tackled the specific challenges of  persuasive imagery. Second, the project allowed us to develop methods  that examine and leverage the complementarity of visual and textual  information, in persuasive media and general multimodal data. As a  related problem, we also examined how visual reasoning methods overfit  to specific aspects of the multimodal inputs (e.g. text) and how robust  reasoning methods are to shallow&nbsp; changes in the input.</p>\n<p>We focused on five separate research thrusts. The first line of research,  resulting in a recent acceptance to ICCV 2021, was directly related to  symbolism and focused on detecting persuasive atypicality in  advertisement images. In an earlier work we annotated images in our advertisement dataset (CVPR 2017, TPAMI 2019) as showing objects in a typical or  atypical (abnormal) manner. In this project, we developed a new technique to detect atypicality, based on the intuition that  relative position of objects with respect to one another is a strong  indicator of atypicality, and can be learned by modeling context. We are currently working on evaluating how well  language models can parse symbolism, i.e. infer what is being  symbolized by a symbol (e.g. \"dragon symbolizes ___\" where ___ is  \"danger\").</p>\n<p>In a second line of research (published in BMVC 2018), we examined the relationship between the image visuals, and the slogan  appearing in the image. We studied the distinct ways in which these  reinforce each other and jointly make a single argument, without  necessarily making the exact same point and being redundant. The latter,  i.e. literal alignment between image and text, has been commonly  studied in vision-language tasks such as image captioning. In contrast,  we find that the image and text pair for a single ad complement each  other in more creative ways. For example, one of image/text can be  purposefully ambiguous, in order to capture the viewer's attention in  decoding the ambiguity; the image and text can even individually appear  to contradict each other, but when viewed together, make a unified  argument. In a follow-up work, appearing in a CVPR 2020 workshop, we tackled decoding the allusions that narratives make. Advertisements are a  type of narrative, so as a form of  preliminary exploration, we first focused on textual-only narratives, specifically choosing the correct ending for a story. We examined the connection between context and endings, by  looking at any relationships between context/ending words, according to a  knowledge base resource.</p>\n<p>The third line of research focused on undestanding the  relation between images and text in multimodal political articles. We  extended our NeurIPS 2019 conference paper into a IJCV 2021 journal  version. That work's goal was to infer political bias from images, by  using text as an auxiliary modality. A follow-up  work (presented at ECCV 2020) developed a cross-modal  retrieval method which relied on within-modality constraints to help  deal with the complementarity of image-text pairs and the diverse  appearance of imagery that corresponds to the same topic.</p>\n<p>A fourth line of work, inspired initially by our work on  advertisements, was to  train a scene graph generation model from weak  supervision contained in  captions. This was a follow-up to our prior  work, which trained object  detection algortihms from captions. It was  presented at CVPR 2021.</p>\n<p>Our fifth line of work was to examine robustness of visual  question-answering (VQA) models. In one work, appearing in WACV 2021, investigated how to make use of  external knowledge base information, for performing a visual reasoning  task on our ads dataset. We discovered that because of the way the  evaluation task is set up, it is easy for the model to find shallow  \"shortcuts\" and ignore knowledge pieces, which are needed for reasoning  on less-common brands. We tackled the problem through three stochastic  masking techniques. In the follow-up work, appearing in AAAI 2021, we  showed that the shortcut problem exists in other reasoning datasets and that reasoning methods (including the recent transformer models) suffer greatly from  simple input changes that should not change the meaning of the question  and answer. We proposed masking on a curriculum to ameliorate the issue.  In our final work, appearing in CVPR 2021, we examined how robust VQA  models are to training and testing on different datasets.</p>\n<p>Our work funded four graduate students for multiple semesters, two of whom graduated, one is female, and one will pursue a career in academia. It also resulted in two publicly released datasets.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2021<br>\n\t\t\t\t\tModified by: Adriana&nbsp;Kovashka</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project transformed the computer vision field in two ways.  First, it put an emphasis on the importance of analyzing persuasive  media, e.g. advertisements and images in political articles. We  developed a variety of techniques for visual reasoning that tackled the specific challenges of  persuasive imagery. Second, the project allowed us to develop methods  that examine and leverage the complementarity of visual and textual  information, in persuasive media and general multimodal data. As a  related problem, we also examined how visual reasoning methods overfit  to specific aspects of the multimodal inputs (e.g. text) and how robust  reasoning methods are to shallow  changes in the input.\n\nWe focused on five separate research thrusts. The first line of research,  resulting in a recent acceptance to ICCV 2021, was directly related to  symbolism and focused on detecting persuasive atypicality in  advertisement images. In an earlier work we annotated images in our advertisement dataset (CVPR 2017, TPAMI 2019) as showing objects in a typical or  atypical (abnormal) manner. In this project, we developed a new technique to detect atypicality, based on the intuition that  relative position of objects with respect to one another is a strong  indicator of atypicality, and can be learned by modeling context. We are currently working on evaluating how well  language models can parse symbolism, i.e. infer what is being  symbolized by a symbol (e.g. \"dragon symbolizes ___\" where ___ is  \"danger\").\n\nIn a second line of research (published in BMVC 2018), we examined the relationship between the image visuals, and the slogan  appearing in the image. We studied the distinct ways in which these  reinforce each other and jointly make a single argument, without  necessarily making the exact same point and being redundant. The latter,  i.e. literal alignment between image and text, has been commonly  studied in vision-language tasks such as image captioning. In contrast,  we find that the image and text pair for a single ad complement each  other in more creative ways. For example, one of image/text can be  purposefully ambiguous, in order to capture the viewer's attention in  decoding the ambiguity; the image and text can even individually appear  to contradict each other, but when viewed together, make a unified  argument. In a follow-up work, appearing in a CVPR 2020 workshop, we tackled decoding the allusions that narratives make. Advertisements are a  type of narrative, so as a form of  preliminary exploration, we first focused on textual-only narratives, specifically choosing the correct ending for a story. We examined the connection between context and endings, by  looking at any relationships between context/ending words, according to a  knowledge base resource.\n\nThe third line of research focused on undestanding the  relation between images and text in multimodal political articles. We  extended our NeurIPS 2019 conference paper into a IJCV 2021 journal  version. That work's goal was to infer political bias from images, by  using text as an auxiliary modality. A follow-up  work (presented at ECCV 2020) developed a cross-modal  retrieval method which relied on within-modality constraints to help  deal with the complementarity of image-text pairs and the diverse  appearance of imagery that corresponds to the same topic.\n\nA fourth line of work, inspired initially by our work on  advertisements, was to  train a scene graph generation model from weak  supervision contained in  captions. This was a follow-up to our prior  work, which trained object  detection algortihms from captions. It was  presented at CVPR 2021.\n\nOur fifth line of work was to examine robustness of visual  question-answering (VQA) models. In one work, appearing in WACV 2021, investigated how to make use of  external knowledge base information, for performing a visual reasoning  task on our ads dataset. We discovered that because of the way the  evaluation task is set up, it is easy for the model to find shallow  \"shortcuts\" and ignore knowledge pieces, which are needed for reasoning  on less-common brands. We tackled the problem through three stochastic  masking techniques. In the follow-up work, appearing in AAAI 2021, we  showed that the shortcut problem exists in other reasoning datasets and that reasoning methods (including the recent transformer models) suffer greatly from  simple input changes that should not change the meaning of the question  and answer. We proposed masking on a curriculum to ameliorate the issue.  In our final work, appearing in CVPR 2021, we examined how robust VQA  models are to training and testing on different datasets.\n\nOur work funded four graduate students for multiple semesters, two of whom graduated, one is female, and one will pursue a career in academia. It also resulted in two publicly released datasets.\n\n\t\t\t\t\tLast Modified: 10/29/2021\n\n\t\t\t\t\tSubmitted by: Adriana Kovashka"
 }
}