{
 "awd_id": "1700032",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TWC: Small: Collaborative: Secure and Usable Mobile Authentication for People with Visual Impairment",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Shannon Beck",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 58888.0,
 "awd_amount": 58888.0,
 "awd_min_amd_letter_date": "2016-11-07",
 "awd_max_amd_letter_date": "2016-11-07",
 "awd_abstract_narration": "Mobile authentication is necessary for preventing unauthorized access to mobile devices with increasingly more private information. Despite significant progress in mobile authentication for sighted people, secure and usable mobile authentication for people with visual impairment remains largely under-explored. This project is to develop, prototype and evaluate novel secure and usable mobile authentication techniques for people with visual impairment. \r\n\r\nThere are three research thrusts. The first thrust is to develop CurveAuth, which authenticates a user based on his/her finger-drawing curves on the device screen. The second thrust is to develop TapAuth, which authenticates a user based on his/her sequence of rhythmic finger taps or slides on the device screen. The third thrust is to develop ShakeAuth, which authenticates a user based on his/her sequence of rhythmic shakes of the device. This research also includes a plan to prototype and evaluates the proposed techniques through comprehensive user experiments. \r\n\r\nSuccessful development of the proposed techniques will have profound impact on allowing the visually impaired to fully embrace the power of modern mobile devices for improving their quality of life. Project outcomes will be made publicly available online in the forms of talks, publications, datasets, and Android/iOS apps.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rui",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rui Zhang",
   "pi_email_addr": "ruizhang@udel.edu",
   "nsf_id": "000651467",
   "pi_start_date": "2016-11-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Delaware",
  "inst_street_address": "550 S COLLEGE AVE",
  "inst_street_address_2": "",
  "inst_city_name": "NEWARK",
  "inst_state_code": "DE",
  "inst_state_name": "Delaware",
  "inst_phone_num": "3028312136",
  "inst_zip_code": "197131324",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DE00",
  "org_lgl_bus_name": "UNIVERSITY OF DELAWARE",
  "org_prnt_uei_num": "",
  "org_uei_num": "T72NHKM259N3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Delaware",
  "perf_str_addr": "18 Amstel Ave",
  "perf_city_name": "Newark",
  "perf_st_code": "DE",
  "perf_st_name": "Delaware",
  "perf_zip_code": "197162599",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DE00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 58887.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Mobile authentication is necessary for preventing unauthorized access to mobile devices with increasingly more private information. Despite significant progress in mobile authentication for sighted people, secure and usable mobile authentication for people with visual impairment remains largely under-explored. To fill this gap, this project seeks to develop, implement, and evaluate a suite of secure and usable mobile authentication techniques for the visually impaired and has resulted in the following major accomplishments.</p>\n<p><br />(1) Rhythm-based authentication: We have designed a mobile authentication technique in which a user performs a sequence of rhythmic taps/slides on a device screen and is authenticated and admitted only when the features extracted from her rhythmic taps/slides match those stored on the device. This technique is highly resilient to brute force guessing attacks, robust to shoulder surfing attacks, and immune to smudge attacks. Moreover, it does not require the user to look at the screen while performing rhythmic taps/slides, making it particularly suitable for users with visual impairment.&nbsp;</p>\n<p><br />(2) Mobile face authentication: We have designed a practical face authentication technique for mobile devices. This technique simultaneously takes a face video with the front camera and a fingertip video with the rear camera on commercial off-the-shelf mobile devices. It then detects whether the videos are from a live person by comparing the two photoplethysmograms independently extracted from the face and fingertip videos, which should be highly consistent if the two videos are for the same live person and taken at the same time. This technique is strongly resilient to photo-based and video-based forgery attacks.</p>\n<p><br />(3) Mobile two-factor authentication: We have developed a secure and usable mobile two-factor authentication (2FA) technique that does not involve user interactions. This technique automatically transmits a user?s 2FA response via inaudible acoustic signals to the login browser and extract individual speaker and microphone fingerprints of a mobile device to defend against the man-in-the-middle attack. Moreover, it explores two-way acoustic ranging to thwart the colocated attack.</p>\n<p><br />Major research results have been disseminated to academia and industry through paper presentations and publications in leading conferences such as IEEE INFOCOM and ACM MobiCom. A new course module based on the research results of this project has been developed and taught at the University of Delaware. The project has also provided training in mobile authentication, Android programming, data collection, machine learning, and algorithm design for two graduate students (one at the University of Hawaii and the other at the University of Delaware).&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/21/2019<br>\n\t\t\t\t\tModified by: Rui&nbsp;Zhang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMobile authentication is necessary for preventing unauthorized access to mobile devices with increasingly more private information. Despite significant progress in mobile authentication for sighted people, secure and usable mobile authentication for people with visual impairment remains largely under-explored. To fill this gap, this project seeks to develop, implement, and evaluate a suite of secure and usable mobile authentication techniques for the visually impaired and has resulted in the following major accomplishments.\n\n\n(1) Rhythm-based authentication: We have designed a mobile authentication technique in which a user performs a sequence of rhythmic taps/slides on a device screen and is authenticated and admitted only when the features extracted from her rhythmic taps/slides match those stored on the device. This technique is highly resilient to brute force guessing attacks, robust to shoulder surfing attacks, and immune to smudge attacks. Moreover, it does not require the user to look at the screen while performing rhythmic taps/slides, making it particularly suitable for users with visual impairment. \n\n\n(2) Mobile face authentication: We have designed a practical face authentication technique for mobile devices. This technique simultaneously takes a face video with the front camera and a fingertip video with the rear camera on commercial off-the-shelf mobile devices. It then detects whether the videos are from a live person by comparing the two photoplethysmograms independently extracted from the face and fingertip videos, which should be highly consistent if the two videos are for the same live person and taken at the same time. This technique is strongly resilient to photo-based and video-based forgery attacks.\n\n\n(3) Mobile two-factor authentication: We have developed a secure and usable mobile two-factor authentication (2FA) technique that does not involve user interactions. This technique automatically transmits a user?s 2FA response via inaudible acoustic signals to the login browser and extract individual speaker and microphone fingerprints of a mobile device to defend against the man-in-the-middle attack. Moreover, it explores two-way acoustic ranging to thwart the colocated attack.\n\n\nMajor research results have been disseminated to academia and industry through paper presentations and publications in leading conferences such as IEEE INFOCOM and ACM MobiCom. A new course module based on the research results of this project has been developed and taught at the University of Delaware. The project has also provided training in mobile authentication, Android programming, data collection, machine learning, and algorithm design for two graduate students (one at the University of Hawaii and the other at the University of Delaware). \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 02/21/2019\n\n\t\t\t\t\tSubmitted by: Rui Zhang"
 }
}