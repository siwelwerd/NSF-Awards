{
 "awd_id": "1717569",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI:   Small:  Collaborative Research:  Hidden Parameter Markov Decision Processes: Exploiting Structure in Families of Tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 208000.0,
 "awd_amount": 208000.0,
 "awd_min_amd_letter_date": "2017-08-04",
 "awd_max_amd_letter_date": "2017-08-04",
 "awd_abstract_narration": "Part 1\r\nMachine learning has the potential to automate many complex, real-life tasks. However, learning algorithms typically require a substantial amount of data from each specific task they are asked to solve, requiring repeated interactions with the world, each of which take time and effort. Many real-life learning scenarios involve repeated interactions with tasks that are similar, but not identical. For example, an immunologist may encounter HIV patients with different comorbid conditions and latent viral reservoirs - each has a similar disease but a different progression, requiring individualized treatment; a robot may have to manipulate objects of different size and weight - each requiring similar but not identical grasping strategies. In such cases treating all of the tasks as the same results in poor performance, but learning to solve each as if they were completely different takes far too long. This project will develop intelligent agents that can use knowledge gained when solving prior tasks to much more rapidly learn new tasks that are similar but not quite the same.\r\n\r\nThe principal technical component of this project will lie in rigorously defining what it means for tasks to be related and in producing algorithms for leveraging that definition to enable rapid learning. To do so, the project will introduce the Hidden-Parameter Markov Decision Process, which models a family of tasks through a parameter which describes variation through the family but is hidden from the learner. The project will investigate methods that exploit this structure by learning a model of task variation and then seeking to identify the parameter value for each specific task. The planned work will focus on healthcare applications, where families of related but distinct tasks are common (i.e. each patient will have unique characteristics).  However, the project aims to produce foundational learning algorithms applicable to many application areas, ranging from robotics to systems design. This research will also be integrated into the courses taught by the PIs at Harvard and Brown and made available online; the PIs will include a diverse population, including REUs, both in these classes and in their research groups.\r\n\r\nPart 2\r\nMany real-life learning scenarios involve repeated interactions with tasks that have similar, but not identical, dynamics.  For example, an immunologist may encounter HIV patients with different comorbid conditions and latent viral reservoirs; a robot may have to manipulate objects of different size and weight.  These cases describe a family of related tasks, each of which is similar but not quite the same. An intelligent agent should be able to transfer knowledge learned during previous experiences to rapidly solve new tasks in the same family. However, while many algorithms have been developed to transfer knowledge, the lack of a model of task relatedness inhibits our ability to formally understand the benefits of such algorithms or the structure they exploit.\r\n\r\nThe planned work will model such scenarios by embedding the tasks on a low dimensional manifold that captures relevant variation between instances.  Each location on this manifold (unobserved by the agent) describes a task instance, forming a sufficient statistic for solving the task in the context of the task family.  Preliminary work by the PIs has shown that it is possible to learn such a manifold after solving just a few individual task instances and enable the rapid optimization of policies for new task instances.  Building on these promising initial results, the PIs plan to: 1) Develop methods for task family characterization, by determining whether a collection of tasks can be modeled via a single manifold or consists of several clusters; whether a new task belongs to an existing cluster or manifold; and if so, and whether or not transfer is worthwhile. 2) Scale inference by adapting recent results from machine learning to deal with large state and action spaces. 3) Generate policies using Bayesian reinforcement learning algorithms, and by exploiting formal links between state and policy representations.\r\n\r\nIn addition to synthetic domains, progress on these directions will be applied to problems of treatment optimization for patients with HIV, sepsis, and depression via clinical collaborations that the PIs have with world-experts in these diseases.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "George",
   "pi_last_name": "Konidaris",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "George D Konidaris",
   "pi_email_addr": "George_konidaris@brown.edu",
   "nsf_id": "000732307",
   "pi_start_date": "2017-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "115 Waterman Street",
  "perf_city_name": "Providence",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 208000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Reinforcement learning is an important machine learning paradigm whereby an agent learns to solve a task by trial-and-error interaction with an environment. However, a major obstacle to making reinforcement learning agents useful in practice is the very high number of interactions required to learn to solve a new task from scratch. This project was focused on the question of how an agent can reuse information from prior tasks to accelerate learning in future tasks, thereby avoiding repeatedly having to learn new tasks from scratch. In particular, it focused on learning scenarios where the agent repeatedly interacts with tasks that have similar, but not identical, dynamics, as is common in human learning. For example, a teenager learning to drive does so with different cars and under different road conditions, or an endocrinologist may encounter diabetics with different comorbid conditions. These cases describe a family of related tasks, each of which is similar but not quite the same. An intelligent agent should be both robust to these variations - able to drive different models of car - and be able to exploit previous experiences to rapidly adjust to new tasks, for example adjusting to a particular trip's conditions after just a few moments of driving, rather than having to learn from scratch each time.</p>\n<p><br />The project resulted in the development of new algorithms that enabled reinforcement learning agents to transfer knowledge - in several different forms - across families of related tasks, in environments with rich state and action spaces. The resulting methods scaled effective transfer for reinforcement learning up to complex simulated tasks, including domains with pixel inputs and simulated robot domains. The resulting advances substantially reduced the cost of learning tasks from scratch, and led to several published papers at high-impact venues in machine learning, artificial intelligence, and robotics. The project also led to new methods that improved the reliability of existing single-task reinforcement learning algorithms, by improving the robustness and reducing the complexity of deep reinforcement learning, and generating a new method for exploration in continuous control tasks.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/28/2021<br>\n\t\t\t\t\tModified by: George&nbsp;D&nbsp;Konidaris</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nReinforcement learning is an important machine learning paradigm whereby an agent learns to solve a task by trial-and-error interaction with an environment. However, a major obstacle to making reinforcement learning agents useful in practice is the very high number of interactions required to learn to solve a new task from scratch. This project was focused on the question of how an agent can reuse information from prior tasks to accelerate learning in future tasks, thereby avoiding repeatedly having to learn new tasks from scratch. In particular, it focused on learning scenarios where the agent repeatedly interacts with tasks that have similar, but not identical, dynamics, as is common in human learning. For example, a teenager learning to drive does so with different cars and under different road conditions, or an endocrinologist may encounter diabetics with different comorbid conditions. These cases describe a family of related tasks, each of which is similar but not quite the same. An intelligent agent should be both robust to these variations - able to drive different models of car - and be able to exploit previous experiences to rapidly adjust to new tasks, for example adjusting to a particular trip's conditions after just a few moments of driving, rather than having to learn from scratch each time.\n\n\nThe project resulted in the development of new algorithms that enabled reinforcement learning agents to transfer knowledge - in several different forms - across families of related tasks, in environments with rich state and action spaces. The resulting methods scaled effective transfer for reinforcement learning up to complex simulated tasks, including domains with pixel inputs and simulated robot domains. The resulting advances substantially reduced the cost of learning tasks from scratch, and led to several published papers at high-impact venues in machine learning, artificial intelligence, and robotics. The project also led to new methods that improved the reliability of existing single-task reinforcement learning algorithms, by improving the robustness and reducing the complexity of deep reinforcement learning, and generating a new method for exploration in continuous control tasks. \n\n \n\n\t\t\t\t\tLast Modified: 12/28/2021\n\n\t\t\t\t\tSubmitted by: George D Konidaris"
 }
}