{
 "awd_id": "1739772",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: SI2: SSE: Extending the Physics Reach of LHCb in Run 3 Using Machine Learning in the Real-Time Data Ingestion and Reduction System",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Seung-Jong Park",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 275000.0,
 "awd_amount": 275000.0,
 "awd_min_amd_letter_date": "2017-08-24",
 "awd_max_amd_letter_date": "2017-08-24",
 "awd_abstract_narration": "In the past 200 years, physicists have discovered the basic constituents of ordinary matter and the developed a very successful theory to describe the interactions (forces) between them.  All atoms, and the molecules from which they are built, can be described in terms of these constituents.  The nuclei of atoms are bound together by strong nuclear interactions.  Their decays result from strong and weak nuclear interactions. Electromagnetic forces bind atoms together, and bind atoms into molecules. The electromagnetic, weak nuclear, and strong nuclear forces are described in terms of quantum field theories.  The predictions of these theories can be very, very precise, and they have been validated with equally precise experimental measurements.  Most recently, a new fundamental particle required to unify the weak and electromagnetic interactions, the Higgs boson, was discovered at the Large Hadron Collider (LHC), located at the CERN laboratory in Switzerland. Despite the vast amount of knowledge acquired over the past century about the fundamental particles and forces of nature, many important questions still remain unanswered. For example, most of the matter in the universe that interacts gravitationally does not have ordinary electromagnetic or nuclear interactions.  As it has only been observed via its gravitation interactions, it is called dark matter.  What is it?  Equally interesting, why is there so little anti-matter in the universe when the fundamental interactions we know describe matter and anti-matter as almost perfect mirror images of each other? The LHC was built to discover and study the Higgs boson and to search for answers to these questions. The first data-taking run (Run 1, 2010-2012) of the LHC was a huge success, producing over 1000 journal articles, highlighted by the discovery of the Higgs boson. The current LHC run (Run 2, 2015-present) has already produced many world-leading results; however, the most interesting questions remained unanswered. The LHCb experiment, located on the LHC at CERN, has unique potential to answer some of these questions. LHCb is searching for signals of dark matter produced in high-energy particle collisions at the LHC, and performing high-precision studies of rare processes that could reveal the existence of the as-yet-unknown forces that caused the matter/anti-matter imbalance observed in our universe. The primary goal of this project - supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences - is developing and deploying software utilizing Machine Learning (ML) that will enable the LHCb experiment to significantly improve its discovery potential in Run 3 (2021-2023). Specifically, the ML developed will greatly increase the sensitivity to many proposed types of dark matter and new forces by making it possible to much more efficiently identify and study potential signals -- using the finite computing resources available.  \r\n\r\nThe data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, on which both PIs work, produce about 100 terabytes of data per second, close to a zettabyte of data per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year, comparable to the largest-scale industrial data sets. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. Trigger-system designs are dictated by the rate at which the sensors can be read out, the computational power of the data-ingestion system, and the available storage space for the data. The LHCb detector is being upgraded for Run 3 (2021-2023), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger are analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To process all the data on CPU farms, ML will be used to develop and deploy new trigger algorithms. The specific objectives of this proposal are to more fully characterize LHCb data using ML and build algorithms using these characterizations: to replace the most computationally expensive parts of the event pattern recognition; to increase the performance of the event-classification algorithms; and to reduce the number of bytes persisted per event without degrading physics performance. Many potential explanations for dark matter and the matter/anti-matter asymmetry of our universe are currently inaccessible due to trigger-system limitations. As HEP computing budgets are projected to be approximately flat moving forward, the LHCb trigger system must be redesigned for the experiment to realize its full potential. This redesign must go beyond scalable technical upgrades; radical new strategies are needed.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mike",
   "pi_last_name": "Williams",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mike Williams",
   "pi_email_addr": "mwill@mit.edu",
   "nsf_id": "000630872",
   "pi_start_date": "2017-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "CERN",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "",
  "perf_st_name": "RI REQUIRED",
  "perf_zip_code": "",
  "perf_ctry_code": "SZ",
  "perf_cong_dist": "",
  "perf_st_cong_dist": "",
  "perf_ctry_name": "Switzerland",
  "perf_ctry_flag": "0"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "724400",
   "pgm_ele_name": "COMPUTATIONAL PHYSICS"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  },
  {
   "pgm_ref_code": "8005",
   "pgm_ref_txt": "Scientific Software Elements"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 275000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This grant provided support for a group at MIT to work on the LHCb experiment at the Large Hadron Collider (LHC) at CERN, Geneva, Switzerland. During the time covered by this grant, we made major contributions to the operation of LHCb, especially in the real-time data-analysis and data-acquisition system (referred to as a trigger in our field). The LHCb trigger makes heavy use of machine learning (ML) algorithms to determine which events to keep for future analysis. The primary objective of this proposal was to expand the use of ML in the LHCb trigger, greatly improving its performance while satisfying its stringent robustness and sustainability requirements.<br /><br />Our group developed new algorithms and strategies used to speed up how particle properties are inferred from the sparse detector data.&nbsp; We developed new algorithms that run on GPUs for the next LHCb data-taking run, including prototypes for the primary event-selection algorithms. We also studied novel data-compression and generative-modeling strategies for LHCb. We developed a novel ML loss function that allows an analyst to control potential biases of the classifier on a protected feature. This work was presented at NeurIPS 2020 Physical Sciences and published in JHEP.<br /><br />The grad student supported by this grant become an LHCb trigger expert, and served many periods as the official on-call expert in charge of dealing with any problems with the system while running the experiment. As part of his physics PhD requirements, he co-led a high-profile physics analysis of LHCb data (dark photons). This work led to a publication in Physical Review Letters. He became the first student to obtain the new PhD in Physics, Statistics, and Data Science at MIT. He subsequently did a short internship with NASA before starting a job as a Data Science Consultant.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/29/2021<br>\n\t\t\t\t\tModified by: Mike&nbsp;Williams</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis grant provided support for a group at MIT to work on the LHCb experiment at the Large Hadron Collider (LHC) at CERN, Geneva, Switzerland. During the time covered by this grant, we made major contributions to the operation of LHCb, especially in the real-time data-analysis and data-acquisition system (referred to as a trigger in our field). The LHCb trigger makes heavy use of machine learning (ML) algorithms to determine which events to keep for future analysis. The primary objective of this proposal was to expand the use of ML in the LHCb trigger, greatly improving its performance while satisfying its stringent robustness and sustainability requirements.\n\nOur group developed new algorithms and strategies used to speed up how particle properties are inferred from the sparse detector data.  We developed new algorithms that run on GPUs for the next LHCb data-taking run, including prototypes for the primary event-selection algorithms. We also studied novel data-compression and generative-modeling strategies for LHCb. We developed a novel ML loss function that allows an analyst to control potential biases of the classifier on a protected feature. This work was presented at NeurIPS 2020 Physical Sciences and published in JHEP.\n\nThe grad student supported by this grant become an LHCb trigger expert, and served many periods as the official on-call expert in charge of dealing with any problems with the system while running the experiment. As part of his physics PhD requirements, he co-led a high-profile physics analysis of LHCb data (dark photons). This work led to a publication in Physical Review Letters. He became the first student to obtain the new PhD in Physics, Statistics, and Data Science at MIT. He subsequently did a short internship with NASA before starting a job as a Data Science Consultant.\n\n\t\t\t\t\tLast Modified: 11/29/2021\n\n\t\t\t\t\tSubmitted by: Mike Williams"
 }
}