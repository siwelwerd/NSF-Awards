{
 "awd_id": "1651740",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Automatic Speech-Based Longitudinal Emotion and Mood Recognition for Mental Health Monitoring and Treatment",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2017-02-01",
 "awd_exp_date": "2023-01-31",
 "tot_intn_awd_amt": 548781.0,
 "awd_amount": 564781.0,
 "awd_min_amd_letter_date": "2017-01-31",
 "awd_max_amd_letter_date": "2021-02-15",
 "awd_abstract_narration": "Effective treatment and monitoring for individuals with mental health disorders is an enduring societal challenge. Regular monitoring increases access to preventative treatment, but is often cost prohibitive or infeasible given high demands placed on health care providers. Yet, it is critical for individuals with Bipolar Disorder (BPD), a chronic psychiatric illness characterized by mood transitions between healthy and pathological states. Transitions into pathological states are associated with profound disruptions in personal, social, vocational functioning, and emotion regulation. This Faculty Early Career Development Program (CAREER) project investigates new approaches in speech-based mood monitoring by taking advantage of the link between speech, emotion, and mood. The approach includes processing data with short-term variation (speech), estimating mid-term variation (emotion), and then using patterns in emotion to recognize long-term variation (mood). The educational outreach includes a design challenge, created with Iridescent, a science education nonprofit, that teaches emotion recognition to underserved children and their parents in informal learning settings. \r\n\r\nThe research investigates methods to model naturalistic, longitudinal speech data and associate emotion patterns with mood, addressing current challenges in speech emotion recognition and assistive technology that include: generalizability, robustness, and performance.  The approaches generalize to conditions whose symptoms include atypical emotion, such as post-traumatic stress disorder, anxiety, depression, and stress. The research forwards emotion as an intermediate step to simplify the mapping between speech and mood; emotion dysregulation is a common BPD symptom. Emotion is quantified over time in terms of valence and activation to improve generalizability.  Nuisance modulations are controlled to improve robustness.  Together, they result in a set of low-dimensional secondary features whose variations are due to emotion.  These secondary features are segmented to create a coarser temporal description of emotion.  This provides a means to map between speech (a quickly varying signal) and user state (a slowly varying signal), advancing the state-of-the-art.  The results provide quantitative insight into the relationship between emotion variation and user state variation, providing new directions and links between the fields of emotion recognition and assistive technology.  The focus on modeling emotional data using time series techniques results in breakthroughs in the design of emotion recognition and assistive technology algorithms.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Emily",
   "pi_last_name": "Provost",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Emily M Provost",
   "pi_email_addr": "emilykmp@umich.edu",
   "nsf_id": "000607930",
   "pi_start_date": "2017-01-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "2260 Hayward, Beyster Bldg.",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 133489.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 120622.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 108891.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 99390.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 102389.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Accomplishments: &nbsp;<br /></strong>The proposal addressed current challenges in speech emotion recognition and assistive technology that included: generalizability, robustness, and performance. The research objective was to create new approaches that perform accurately across people and in new environments. We focused on the measurement of emotional valence (positive vs. negative) and activation (calm vs. excited), motivated by the fact that assessments of valence and activation are based on observable behavior rather than cognitive state. We introduced techniques to control for unrelated variation, which improved performance. We found that measurements of valence and activation could be used as secondary measurements to understand user state, beyond emotion. The results provided quantitative insight into the relationship between emotion variation and user state variation, providing new directions and links between the fields of emotion recognition and assistive technology. The focus on modeling emotional data using time series techniques resulted in transformative breakthroughs in the design of emotion recognition and assistive technology algorithms.&nbsp;&nbsp;The project has resulted in 14 publications, including four journal papers.&nbsp;&nbsp;One journal paper was selected in the top five papers from IEEE Transactions of Affective Computing 2022 and was the runner-up for best paper of the year.&nbsp;One conference paper was selected as Best Paper, Honorable Mention at ICMI 2016.&nbsp;&nbsp;The findings are being extended to understand how emotion, measured in the context of daily life, is affected by mood.</p>\n<p><br /><strong>Intellectual Merit:</strong>&nbsp;<br />Our goals included exploring methods to accurately measure emotion from natural conversational data. This domain is challenging due to the contextualized nature of emotion. We advocated for a focus on valence and activation, rather than categorical labels (e.g., happiness, anger) to mitigate the effects of context.&nbsp;&nbsp;We then created new methods that could handle the variability present in real-world, and varied, environments. The results have led to a new understanding of how to robustly and generalizably measure emotion from speech. The findings are leading to new designs for emotion classification systems.<br /><br /><strong>Broader Impact:</strong>&nbsp;<br />The project has supported three PhD students (two female, one GEM fellow), two MS students (one female), and three undergraduate researchers (two female).&nbsp; PI Mower Provost developed the course &ldquo;Applied Machine Learning for Affective Computing&rdquo; and extended the material for a project-oriented class for an undergraduate audience, &ldquo;Applied Machine Learning for Computing Human Behavior&rdquo;.&nbsp; She also presented the work both in academic seminars and as a part of ongoing outreach efforts.&nbsp; Data from this project are evaluated for emotion and have been released to the community.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/27/2023<br>\n\t\t\t\t\tModified by: Emily&nbsp;Provost</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAccomplishments:  \nThe proposal addressed current challenges in speech emotion recognition and assistive technology that included: generalizability, robustness, and performance. The research objective was to create new approaches that perform accurately across people and in new environments. We focused on the measurement of emotional valence (positive vs. negative) and activation (calm vs. excited), motivated by the fact that assessments of valence and activation are based on observable behavior rather than cognitive state. We introduced techniques to control for unrelated variation, which improved performance. We found that measurements of valence and activation could be used as secondary measurements to understand user state, beyond emotion. The results provided quantitative insight into the relationship between emotion variation and user state variation, providing new directions and links between the fields of emotion recognition and assistive technology. The focus on modeling emotional data using time series techniques resulted in transformative breakthroughs in the design of emotion recognition and assistive technology algorithms.  The project has resulted in 14 publications, including four journal papers.  One journal paper was selected in the top five papers from IEEE Transactions of Affective Computing 2022 and was the runner-up for best paper of the year. One conference paper was selected as Best Paper, Honorable Mention at ICMI 2016.  The findings are being extended to understand how emotion, measured in the context of daily life, is affected by mood.\n\n\nIntellectual Merit: \nOur goals included exploring methods to accurately measure emotion from natural conversational data. This domain is challenging due to the contextualized nature of emotion. We advocated for a focus on valence and activation, rather than categorical labels (e.g., happiness, anger) to mitigate the effects of context.  We then created new methods that could handle the variability present in real-world, and varied, environments. The results have led to a new understanding of how to robustly and generalizably measure emotion from speech. The findings are leading to new designs for emotion classification systems.\n\nBroader Impact: \nThe project has supported three PhD students (two female, one GEM fellow), two MS students (one female), and three undergraduate researchers (two female).  PI Mower Provost developed the course \"Applied Machine Learning for Affective Computing\" and extended the material for a project-oriented class for an undergraduate audience, \"Applied Machine Learning for Computing Human Behavior\".  She also presented the work both in academic seminars and as a part of ongoing outreach efforts.  Data from this project are evaluated for emotion and have been released to the community.\n\n \n\n\t\t\t\t\tLast Modified: 04/27/2023\n\n\t\t\t\t\tSubmitted by: Emily Provost"
 }
}