{
 "awd_id": "1651851",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Structure, Complexity, and Conditioning in Nonsmooth Optimization",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Pedro Embid",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 419122.0,
 "awd_amount": 419122.0,
 "awd_min_amd_letter_date": "2017-04-17",
 "awd_max_amd_letter_date": "2021-06-03",
 "awd_abstract_narration": "Recent years have seen an unprecedented growth of large data sets in various high-impact fields, such as seismology, data science, information technology, and environmental science.  The task of extracting useful information from such data sets typically leads to solving a large-scale optimization problem.  The sheer size of such problems poses great challenges for optimization specialists.  The investigator aims to advance the reach of large-scale optimization in such settings, with vital applications throughout science and engineering.  The resulting methodology and algorithms create a systematic approach to discover trends and phenomena underlying the observed data, and lead to a well-grounded mechanism for making predictions about unobserved data.  The project lies firmly at the interface between theory and computation.  Therefore, an effective mix of numerical experimentation, teaching, and discovery is central to the proposal.  Graduate students participate in the work of the project.\r\n\r\n     The investigator's strategy rests on three interrelated pillars: structure, computational complexity, and conditioning in nonsmooth optimization.  Efficiency of numerical methods is best judged through rigorous rates of convergence.  Convergence guarantees of an algorithm become much more potent, however, if they match best possible guarantees that any algorithm can have within the problem class.  The search for such \"optimal methods\" underpins computational complexity.  Measures of the problem's conditioning -- an indication of its difficulty -- play a central role in the subject and are intimately tied to stability of the underlying problem.  The theory and algorithms, moreover, benefit greatly from exploiting rich underlying structure prevalent in applications, such as separation of smooth and simple nonsmooth functional components, smooth conjugate representations, saddle-point reformulations, etc.  Convex optimization techniques and variational analytic insight guide the investigator's approach.  Pervasive large-scale problems in data science and engineering can directly benefit from this work.  The project integrates research and teaching in all aspects.  Graduate students participate in the work of the project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dmitriy",
   "pi_last_name": "Drusvyatskiy",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dmitriy Drusvyatskiy",
   "pi_email_addr": "ddrusv@u.washington.edu",
   "nsf_id": "000676222",
   "pi_start_date": "2017-04-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn Ave NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981950001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126600",
   "pgm_ele_name": "APPLIED MATHEMATICS"
  },
  {
   "pgm_ele_code": "804800",
   "pgm_ele_name": "Division Co-Funding: CAREER"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 59436.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 88329.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 87775.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 90421.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 93161.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Recent advances in optimization, statistics, and machine learning have dramatically improved our capacity to efficiently learn from massive datasets, leading to rapid progress for example in image processing (facial recognition, CT/MRI scans), control and planning (robotics, self-driving cars), natural language (text analysis), and generative AI (chatbots). Although the complex learning systems used in these areas have enjoyed widespread success, we have yet to find a coherent mathematical foundation that can explain not only why they work and what tasks they provably solve, but also how practitioners can improve their performance. A key challenge is that most learning tasks are solved by applying simple gradient-based optimization algorithms to highly nonlinear problems, and except for a few special cases, there is no guarantee that they will find global optima nor work reliably. With this in mind, my work during the award period focused on developing provably efficient optimization algorithms for large-scale data scientific tasks. </span><span>Some highlights include:</span></p>\n<div>\n<p><span>1)<span>&nbsp;&nbsp;&nbsp;&nbsp; </span></span><strong><span>Stochastic Algorithms for Nonsmooth Optimization</span></strong><span>: This work established a foundational result showing that the workshorse algorithm for large scale optimization---the stochastic subgradient method---converges to a meaningful limit for virtually all problems on interest in large scale optimization.</span></p>\n<p><span>2)<span>&nbsp;&nbsp;&nbsp;&nbsp; </span></span><strong><span>Sample Complexity in Nonsmooth Optimization</span></strong><span>: This work provided the first convergence rate for a wide range of nonsmooth problems, commonly arising in practice. This research was recognized by the SIAM Optimization Best Paper Prize 2023.</span></p>\n<p><span>3)<span>&nbsp;&nbsp;&nbsp;&nbsp; </span></span><strong><span>Structured Recovery in Statistical Problems</span></strong><span>: This work has shown that for a number of important problems in statistics and signal processing, local search algorithms converge exponentially fast to global minimizers.</span></p>\n<p><span>4)<span>&nbsp;&nbsp;&nbsp;&nbsp; </span></span><strong><span>Avoiding Spurious Critical Points</span></strong><span>: This work proved that common optimization algorithms &nbsp;converge only to local minimizers on typical functions.</span></p>\n<p><span>5)<span>&nbsp;&nbsp;&nbsp;&nbsp; </span></span><strong><span>Robustness to Tuning Parameters and Heavy Tailed Noise</span></strong><span>: Focusing on practical algorithmic use, we analyzed the stability of algorithms against stepsize misspecification and developed stochastic algorithms that are robust to outliers.</span></p>\n<p><strong><span>6)<span>&nbsp;&nbsp;&nbsp; </span></span></strong><strong><span>Flat Minima in Deep Learning</span></strong><span>: Addressing a core issue in deep learning, we provided justifications for why those models around which the objective function looks flat generalize better to unseen data.<strong><span>&nbsp;</span></strong></span></p>\n<p><span>7)<span>&nbsp;&nbsp;&nbsp;&nbsp; </span></span><strong><span>Learning with Distributional Shifts</span></strong><span>: Addressing the challenges of dynamic data distributions in machine learning, this work developed a framework and algorithmic techniques for handling learning problems with strategic agents.</span></p>\n<p><strong><span>8)<span>&nbsp;&nbsp;&nbsp;&nbsp; </span></span></strong><strong>First-order algorithms for structured signal recovery. </strong><span>The PI has explored algorithms for recovering structured signals from noisy measurements, and developed specialized algorithms utilizing so-called gauge duality and level-set reparametrizations.</span><strong>&nbsp;</strong></p>\n</div>\n<div>\n<p>&nbsp;</p>\n<p>During the award period, the PI advised four graduate students, who have gone on to successfully obtain industry and academic positions. The PI disseminated the sponsiored work widely through numerous lectures at conferences, workshops, and colloquia, as well as through survey articles that are accessible to a wide and diverse audience.<strong></strong></p>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2023<br>\n\t\t\t\t\tModified by: Dmitriy&nbsp;Drusvyatskiy</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nRecent advances in optimization, statistics, and machine learning have dramatically improved our capacity to efficiently learn from massive datasets, leading to rapid progress for example in image processing (facial recognition, CT/MRI scans), control and planning (robotics, self-driving cars), natural language (text analysis), and generative AI (chatbots). Although the complex learning systems used in these areas have enjoyed widespread success, we have yet to find a coherent mathematical foundation that can explain not only why they work and what tasks they provably solve, but also how practitioners can improve their performance. A key challenge is that most learning tasks are solved by applying simple gradient-based optimization algorithms to highly nonlinear problems, and except for a few special cases, there is no guarantee that they will find global optima nor work reliably. With this in mind, my work during the award period focused on developing provably efficient optimization algorithms for large-scale data scientific tasks. Some highlights include:\n\n\n1)     Stochastic Algorithms for Nonsmooth Optimization: This work established a foundational result showing that the workshorse algorithm for large scale optimization---the stochastic subgradient method---converges to a meaningful limit for virtually all problems on interest in large scale optimization.\n\n2)     Sample Complexity in Nonsmooth Optimization: This work provided the first convergence rate for a wide range of nonsmooth problems, commonly arising in practice. This research was recognized by the SIAM Optimization Best Paper Prize 2023.\n\n3)     Structured Recovery in Statistical Problems: This work has shown that for a number of important problems in statistics and signal processing, local search algorithms converge exponentially fast to global minimizers.\n\n4)     Avoiding Spurious Critical Points: This work proved that common optimization algorithms  converge only to local minimizers on typical functions.\n\n5)     Robustness to Tuning Parameters and Heavy Tailed Noise: Focusing on practical algorithmic use, we analyzed the stability of algorithms against stepsize misspecification and developed stochastic algorithms that are robust to outliers.\n\n6)    Flat Minima in Deep Learning: Addressing a core issue in deep learning, we provided justifications for why those models around which the objective function looks flat generalize better to unseen data. \n\n7)     Learning with Distributional Shifts: Addressing the challenges of dynamic data distributions in machine learning, this work developed a framework and algorithmic techniques for handling learning problems with strategic agents.\n\n8)     First-order algorithms for structured signal recovery. The PI has explored algorithms for recovering structured signals from noisy measurements, and developed specialized algorithms utilizing so-called gauge duality and level-set reparametrizations. \n\n\n\n \n\nDuring the award period, the PI advised four graduate students, who have gone on to successfully obtain industry and academic positions. The PI disseminated the sponsiored work widely through numerous lectures at conferences, workshops, and colloquia, as well as through survey articles that are accessible to a wide and diverse audience.\n\n\n \n\n\t\t\t\t\tLast Modified: 10/30/2023\n\n\t\t\t\t\tSubmitted by: Dmitriy Drusvyatskiy"
 }
}