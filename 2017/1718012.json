{
 "awd_id": "1718012",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Depth from Differential Defocus",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 449999.0,
 "awd_amount": 449999.0,
 "awd_min_amd_letter_date": "2017-07-25",
 "awd_max_amd_letter_date": "2017-07-25",
 "awd_abstract_narration": "This project will explore a new class of depth sensors. The new sensors operate by observing small changes in optical defocus through a single lens, and they require very small amounts of digital computation. The distinguishing feature of these sensors is that they can be much smaller and lower power than existing depth sensor technologies. By enabling depth sensing capabilities on smaller platforms, they help accelerate the creation of smart micro-scale systems and an effective Internet of things. Depth sensors produce two-dimensional images where each pixel's value is the distance to a scene point along a corresponding ray. A variety of these sensors exist, and they are already fueling advances in autonomous navigation, gesture-driven interfaces, robotics, and more.\r\n\r\nThis research will develop sensors based on a new visual cue called differential defocus. Like the well-known passive depth cues of stereo and depth-from-defocus, this new cue avoids spending power on broadcasting light. But unlike the existing passive cues, it calculates depth using simple analytic expressions that are easy to compute. To establish differential defocus as a new way to sense depth, this project aims to discover a complete stack of knowledge, from mathematical foundations to algorithms and hardware prototypes. The mathematical foundations include a catalog of depth constraints that correspond to many forms of differential defocus, such as differential camera motion, sensor motion, change of focal length, and change of aperture. At the hardware level, the project will pursue both single-shot and multi-shot designs that incorporate deformable lenses and customized photosensors. Algorithmically, the project will explore methods for using back-propagation to fine-tune the parameters of depth computations. Going further, it will explore back-propagation into the optical dimension, in order to enable the optimization of optical and computational parameters together, in a synergistic manner.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Todd",
   "pi_last_name": "Zickler",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Todd Zickler",
   "pi_email_addr": "zickler@eecs.harvard.edu",
   "nsf_id": "000118883",
   "pi_start_date": "2017-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "33 Oxford Street",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382933",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 449999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The principal disciplinary field of the project is computer vision, and in particular low-level vision, which concerns the processing of ambient light into useful information about the surrounding environment's three dimensional shape, motion, and material properties. Within this area, the project focuses on the recovery of the scene depth, meaning for each recorded pixel on a retina or image sensor, the distance to the nearest three-dimensional scene point along the back-projected ray from that pixel.</p>\n<p><br />The project's outcomes can be described in three main categories:</p>\n<ol>\n<li>Mathematical foundations. It creates a catalog of mathematical equations that relate scene depth to the image measurements that are collected by a camera that undergoes small (differential) changes to its optics, including differential camera motion, sensor motion, change of focal length, and change of aperture.</li>\n<li>Hardware prototypes. It designs, fabricates, and tests a variety of prototype cameras. These prototypes incorporate adaptive components such as deformable lenses and customized optics based on metasurfaces, which are a class of nanophotonic optical technologies that fortuitously matured during the award period.</li>\n<li>End-to-end training. It develops methods for using back-propagation and stochastic gradient descent to fine-tune the parameters of the feed-forward depth computations, and to produce efficient confidence maps and other representations of residual uncertainty. It also develops methods for simultaneously optimizing the optics, by extending back-propagation to adjust millions of nanophotonic features on a metasurface optical element.</li>\n</ol>\n<p>Together, these new forms of visual processing provide a foundation for small depth sensing systems that come closer to mimicking the visual depth-sensing capabilities of jumping spiders and other multi-retinae arthropods. Such systems might be useful on insect-sized robots, for example, or for depth sensing on wearable devices, including compact augmented reality systems.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/01/2023<br>\n\t\t\t\t\tModified by: Todd&nbsp;Zickler</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe principal disciplinary field of the project is computer vision, and in particular low-level vision, which concerns the processing of ambient light into useful information about the surrounding environment's three dimensional shape, motion, and material properties. Within this area, the project focuses on the recovery of the scene depth, meaning for each recorded pixel on a retina or image sensor, the distance to the nearest three-dimensional scene point along the back-projected ray from that pixel.\n\n\nThe project's outcomes can be described in three main categories:\n\nMathematical foundations. It creates a catalog of mathematical equations that relate scene depth to the image measurements that are collected by a camera that undergoes small (differential) changes to its optics, including differential camera motion, sensor motion, change of focal length, and change of aperture.\nHardware prototypes. It designs, fabricates, and tests a variety of prototype cameras. These prototypes incorporate adaptive components such as deformable lenses and customized optics based on metasurfaces, which are a class of nanophotonic optical technologies that fortuitously matured during the award period.\nEnd-to-end training. It develops methods for using back-propagation and stochastic gradient descent to fine-tune the parameters of the feed-forward depth computations, and to produce efficient confidence maps and other representations of residual uncertainty. It also develops methods for simultaneously optimizing the optics, by extending back-propagation to adjust millions of nanophotonic features on a metasurface optical element.\n\n\nTogether, these new forms of visual processing provide a foundation for small depth sensing systems that come closer to mimicking the visual depth-sensing capabilities of jumping spiders and other multi-retinae arthropods. Such systems might be useful on insect-sized robots, for example, or for depth sensing on wearable devices, including compact augmented reality systems.\n\n\t\t\t\t\tLast Modified: 06/01/2023\n\n\t\t\t\t\tSubmitted by: Todd Zickler"
 }
}