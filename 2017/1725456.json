{
 "awd_id": "1725456",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: Collaborative Research: Ula! - An Integrated Deep Neural Network (DNN) Acceleration Framework with Enhanced Unsupervised Learning Capability",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Danella Zhao",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 520000.0,
 "awd_amount": 520000.0,
 "awd_min_amd_letter_date": "2017-07-22",
 "awd_max_amd_letter_date": "2017-07-22",
 "awd_abstract_narration": "In light of very recent revolutions of unsupervised learning algorithms (e.g., generative adversarial networks and dual-learning) and the emergence of their applications, three PIs/co-PI from Duke and UCSB form a team to design Ula! - an integrated DNN acceleration framework with enhanced unsupervised learning capability. The project revolutionizes the DNN research by introducing an integrated unsupervised learning computation framework with three vertically-integrated components from the aspects of software (algorithm), hardware (computing), and application (realization). The project echoes the call from the BRAIN Initiative (2013) and the Nanotechnology-Inspired Grand Challenge for Future Computing (2015) from the White House. The research outcomes will benefit both Computational Intelligence (CI) and Computer Architecture (CA) industries at large by introducing a synergy between computing paradigm and artificial intelligence (AI). The corresponding education components\u00a0 enhance existing curricula and pedagogy by introducing interdisciplinary modules on the software/hardware co-design for AI with creative teaching practices, and give special attentions to women and underrepresented minority groups.\r\n\r\nThe project performs three tasks: (1) At the software level, a generalized hierarchical decision-making (GHDM) system is designed to efficiently execute the state-of-the-art unsupervised learning and reinforcement learning processes with substantially reduced computation cost; (2) At the hardware level, a novel DNN computing paradigm is designed with enhanced unsupervised learning supports, based on the novelties in near data computing, GPU architecture, and FGPA + heterogeneous platforms; (3) At the application level, the usage of Ula! is exploited in scenarios that can greatly benefit from unsupervised learning and reinforcement learning. The developed techniques are also demonstrated and evaluated on three representative computing platforms: GPU, FPGA, and emerging nanoscale computing systems, respectively.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yiran",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yiran Chen",
   "pi_email_addr": "yiran.chen@duke.edu",
   "nsf_id": "000575362",
   "pi_start_date": "2017-07-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Hai",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hai Li",
   "pi_email_addr": "hai.li@duke.edu",
   "nsf_id": "000538107",
   "pi_start_date": "2017-07-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "Hudson Hall",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277080001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 520000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We developed Ula! - an integrated Deep Neural Network (DNN) acceleration framework with advanced learning capability. Ula! consists of three vertically integrated components at the levels of algorithm, hardware, and application as: 1) Reinforcement/Supervised based enhanced unsupervised learning; 2) Novel DNN computing paradigm; 3) Technology applications and realizations.</p>\n<p>Generative Adversarial Networks (GANs) have recently drawn tremendous attention as a powerful tool to improve the performance of unsupervised or semi-supervised DNN models. While GANs deliver state-of-the-art performance on these AI tasks, it comes at the cost of high computational complexity. We proposed ReGAN - a novel ReRAM-based Process-In-Memory (PIM) accelerator that can efficiently reduce off-chip memory accesses. Two techniques, namely, Spatial Parallelism and Computation Sharing are particularly proposed to further enhance training efficiency of GANs.</p>\n<p>Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. To alleviate the computational cost of RNN, we proposed to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. In the training process, removing a component of Intrinsic Sparse Structures (ISS) in the LSTMs simultaneously decreases the sizes of all basic structures by one and thereby always maintain the dimension consistency.</p>\n<p>Process-in-memory (PIM) architecture such as Hybrid Memory Cube (HMC) has been used to improve the data locality for efficient DNN executions. However, it is still hard to efficiently deploy largescale matrix computation in DNN on HMC because of its coarse-grained packet protocol. We proposed NeuralHMC, the first HMCbased accelerator tailored for efficient DNN executions.</p>\n<p>Existing nonvolatile memory-based machine learning accelerators could not support the computational needs required by GAN training. Specifically, the generator utilizes a new operator, called transposed convolution, which introduces significant resource underutilization when executed on conventional neural network accelerators as it inserts massive zeros in its input before a convolution operation. We proposed ZARA - A Novel Zero-free Dataflow Accelerator for Generative Adversarial Networks in 3D ReRAM.</p>\n<p>Stochastic Gradient Descent (SGD) is a popular training method of DNNs because of its efficiency. However, large-batch SGD tends to converge to sharp minima in the DNNs. We propose the SmoothOut framework to smooth out sharp minima in DNNs and thereby improve generalization of the DNNs. In particular, SmoothOut perturbs multiple copies of the DNN by noise injection and averages these copies. Our experimental results showed that SmoothOut improves generalization in both small-batch and large-batch training on the top of state-of-the-art solutions.</p>\n<p>Previous works on Convolutional Neural Network (CNN) acceleration utilize low-rank approximation of the original convolution layers to reduce computation cost. However, these methods are very difficult to conduct upon sparse models, which limits execution speedup since redundancies within the CNN model are not fully exploited. We argue that kernel granularity decomposition can be conducted with low-rank assumption while exploiting the redundancy within the remaining compact coefficients. Based on this observation, we propose PENNI, a CNN model compression framework that is able to achieve model compactness and hardware efficiency simultaneously.</p>\n<p>Depth is a key component of DNNs. However, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. Our experiments showed that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets.</p>\n<p>In modern GPUs, on-chip memory capacity keeps increasing to support thousands of chip-resident threads. The on-chip memory capacity of GPUs, however, is highly constrained by the large memory cell area and high static power consumption of conventional SRAM implementation. We propose to utilize the emerging multi-level cell (MLC) spin-transfer torque RAM (STT-RAM) technology to implement register files and shared memory in GPUs.</p>\n<p>Federated learning (FL) is a popular distributed learning framework that trains a global model through iterative communications between a central server and edge devices. We empirically show that under extremely strong poisoning attacks, the existing defensive methods fail to guarantee the robustness of FL. More importantly, we observed that as long as the global model is polluted, the impact of attacks on the global model will remain in subsequent rounds even if there are no subsequent attacks. We proposed a client-based defense, named White Blood Cell for Federated Learning (FL-WBC), which can mitigate model poisoning attacks that have already polluted the global model.</p>\n<p>This 5-year research project supported 7 graduate students and publications of 20 conference papers and 6 journal papers in total.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/23/2022<br>\n\t\t\t\t\tModified by: Yiran&nbsp;Chen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWe developed Ula! - an integrated Deep Neural Network (DNN) acceleration framework with advanced learning capability. Ula! consists of three vertically integrated components at the levels of algorithm, hardware, and application as: 1) Reinforcement/Supervised based enhanced unsupervised learning; 2) Novel DNN computing paradigm; 3) Technology applications and realizations.\n\nGenerative Adversarial Networks (GANs) have recently drawn tremendous attention as a powerful tool to improve the performance of unsupervised or semi-supervised DNN models. While GANs deliver state-of-the-art performance on these AI tasks, it comes at the cost of high computational complexity. We proposed ReGAN - a novel ReRAM-based Process-In-Memory (PIM) accelerator that can efficiently reduce off-chip memory accesses. Two techniques, namely, Spatial Parallelism and Computation Sharing are particularly proposed to further enhance training efficiency of GANs.\n\nModel compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. To alleviate the computational cost of RNN, we proposed to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. In the training process, removing a component of Intrinsic Sparse Structures (ISS) in the LSTMs simultaneously decreases the sizes of all basic structures by one and thereby always maintain the dimension consistency.\n\nProcess-in-memory (PIM) architecture such as Hybrid Memory Cube (HMC) has been used to improve the data locality for efficient DNN executions. However, it is still hard to efficiently deploy largescale matrix computation in DNN on HMC because of its coarse-grained packet protocol. We proposed NeuralHMC, the first HMCbased accelerator tailored for efficient DNN executions.\n\nExisting nonvolatile memory-based machine learning accelerators could not support the computational needs required by GAN training. Specifically, the generator utilizes a new operator, called transposed convolution, which introduces significant resource underutilization when executed on conventional neural network accelerators as it inserts massive zeros in its input before a convolution operation. We proposed ZARA - A Novel Zero-free Dataflow Accelerator for Generative Adversarial Networks in 3D ReRAM.\n\nStochastic Gradient Descent (SGD) is a popular training method of DNNs because of its efficiency. However, large-batch SGD tends to converge to sharp minima in the DNNs. We propose the SmoothOut framework to smooth out sharp minima in DNNs and thereby improve generalization of the DNNs. In particular, SmoothOut perturbs multiple copies of the DNN by noise injection and averages these copies. Our experimental results showed that SmoothOut improves generalization in both small-batch and large-batch training on the top of state-of-the-art solutions.\n\nPrevious works on Convolutional Neural Network (CNN) acceleration utilize low-rank approximation of the original convolution layers to reduce computation cost. However, these methods are very difficult to conduct upon sparse models, which limits execution speedup since redundancies within the CNN model are not fully exploited. We argue that kernel granularity decomposition can be conducted with low-rank assumption while exploiting the redundancy within the remaining compact coefficients. Based on this observation, we propose PENNI, a CNN model compression framework that is able to achieve model compactness and hardware efficiency simultaneously.\n\nDepth is a key component of DNNs. However, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. Our experiments showed that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets.\n\nIn modern GPUs, on-chip memory capacity keeps increasing to support thousands of chip-resident threads. The on-chip memory capacity of GPUs, however, is highly constrained by the large memory cell area and high static power consumption of conventional SRAM implementation. We propose to utilize the emerging multi-level cell (MLC) spin-transfer torque RAM (STT-RAM) technology to implement register files and shared memory in GPUs.\n\nFederated learning (FL) is a popular distributed learning framework that trains a global model through iterative communications between a central server and edge devices. We empirically show that under extremely strong poisoning attacks, the existing defensive methods fail to guarantee the robustness of FL. More importantly, we observed that as long as the global model is polluted, the impact of attacks on the global model will remain in subsequent rounds even if there are no subsequent attacks. We proposed a client-based defense, named White Blood Cell for Federated Learning (FL-WBC), which can mitigate model poisoning attacks that have already polluted the global model.\n\nThis 5-year research project supported 7 graduate students and publications of 20 conference papers and 6 journal papers in total.\n\n\t\t\t\t\tLast Modified: 11/23/2022\n\n\t\t\t\t\tSubmitted by: Yiran Chen"
 }
}