{
 "awd_id": "1704656",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: Large: Collaborative Research: Nonconvex Methods and Models for Learning: Towards Algorithms with Provable and Interpretable Guarantees",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2017-06-01",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 499998.0,
 "awd_amount": 507998.0,
 "awd_min_amd_letter_date": "2017-05-16",
 "awd_max_amd_letter_date": "2021-06-08",
 "awd_abstract_narration": "Artificial Intelligence along with Machine Learning are perhaps the most dominant research themes of our times - with far reaching implications for society and our current life style. While the possibilities are many, there are also doubts about how far these methods will go - and what new theoretical foundations may be required to take them to the next level overcoming possible hurdles. Recently, machine learning has undergone a paradigm shift with increasing reliance on  stochastic optimization to train highly non-convex models -- including but not limited to deep nets. Theoretical understanding has lagged behind, primarily because most problems in question are provably intractable on worst-case instances. Furthermore, traditional machine learning theory is mostly concerned with classification, whereas much practical success is driven by unsupervised learning and representation learning. Most past theory of representation learning was focused on simple models such as k-means clustering and PCA, whereas  practical work uses vastly more complicated models like autoencoders, restricted Boltzmann machines and deep generative models. The proposal presents an ambitious agenda for extending theory to embrace and support these practical trends, with hope of influencing practice. Theoretical foundations will be provided for the next generation of machine learning methods and optimization algorithms. \r\n\r\nThe project may end up having significant impact on  practical machine learning, and even cause a cultural change in the field -- theory as well as practice -- with long-term ramifications. Given the ubiquity as well as  economic and scientific implications of machine learning today, such impact will extend into other disciplines, especially in (ongoing) collaborations with researchers in neuroscience. The project will train a new generation of machine learning researchers, through an active teaching and mentoring plan at all levels, from undergrad to postdoc. This new generation will be at ease combining cutting edge theory and applications. There is a pressing need for such people today, and the senior PIs played a role in training/mentoring several existing ones.\r\n \r\nTechnical contributions will include new theoretical models of knowledge representation and semantics, and also frameworks for proving convergence of nonconvex optimization routines. Theory will be developed to explain and exploit the interplay between representation learning and supervised learning that has proved so empirically successful in deep learning, and seems to underlie new learning paradigms such as domain adaptation, transfer learning, and interactive learning. Attempts will be made to replace neural models with models with more \"interpretable\"  attributes and performance curves.  All PIs have a track record of combining theory with practice. They  are also devoted to a heterodox research approach, borrowing from all the past phases of machine learning: interpretable representations from the earlier phases (which relied on logical representations, or probabilistic models), provable guarantees from the middle phase (convex optimization, kernels etc.), and an embrace of nonconvex methods from the latest deep net phase. Such eclecticism is uncommon in machine learning, and may give rise to new paradigms and new kinds of science.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rong",
   "pi_last_name": "Ge",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rong Ge",
   "pi_email_addr": "rongge@cs.duke.edu",
   "nsf_id": "000699594",
   "pi_start_date": "2017-05-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "2200 W. Main St, Suite 710",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 194246.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 98689.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 101884.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 105179.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project addresses a gap between theory and practice of state-of-the-art machine learning. As part of our investigation, the PIs have set to develop theoretical foundations and algorithms for representation learning, unsupervised learning, reinforcement learning and control. The na&iuml;ve formulation of many problems in these domains is NP-hard, and thus novel approaches are necessary.</p>\n<p>Throughout this project, research has been performed mostly in 3 topics: understanding optimization landscape of neural networks, robust statistics using nonconvex methods and theory for representation learning.</p>\n<p>For the optimization landscape of neural networks, the project started by looking at simpler theoretical models of shallow networks - for example, showing that 2-layer neural networks have many spurious local minima and how to change the objective function to get a better landscape. The project then extended its scope to look at properties of optimization landscape in practice, giving theoretical understanding to mode connectivity phenomenon (where different minimizers of neual networks can be connected by simple curves) that was observed in practice.</p>\n<p>Robust statistics seeks to understand the problem of getting reliable estimation in the case of a strong adversary: if the adversary is allowed to arbitrarily change a small fraction of the data. This is closely related to data-poisoning attacks for neural networks. Existing algorithms for robust statistics are often quite complicated and require tuning of hyperparameters. This developed new algorithms and frameworks for designing robust statistics algorithms using faster algorithms and nonconvex optimization - including the first near-linear time algorithm for robust mean estimation and a simple nonconvex objective that gives practical algorithms.&nbsp;</p>\n<p>Representation learning is a broad area. This project works on models of text and reinforcement learning, with a focus on linear-algebraic structures. This led to new approaches in understanding compositions in word embeddings, as well as a new characterization of forward and backward objectives for reinforcement learning.</p>\n<p>All the papers that this project supported were available online and presented in related conferences. The project supported PhD students and post-docs who have contributed to machine learning in both academia and industry. The project also supported undergraduate researchers (some of the results mentioned above were done mostly by undergraduate students) who are now PhD students in theory of machine learning. Many workshops were organized to discuss and disseminate the work.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/29/2022<br>\n\t\t\t\t\tModified by: Rong&nbsp;Ge</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project addresses a gap between theory and practice of state-of-the-art machine learning. As part of our investigation, the PIs have set to develop theoretical foundations and algorithms for representation learning, unsupervised learning, reinforcement learning and control. The na&iuml;ve formulation of many problems in these domains is NP-hard, and thus novel approaches are necessary.\n\nThroughout this project, research has been performed mostly in 3 topics: understanding optimization landscape of neural networks, robust statistics using nonconvex methods and theory for representation learning.\n\nFor the optimization landscape of neural networks, the project started by looking at simpler theoretical models of shallow networks - for example, showing that 2-layer neural networks have many spurious local minima and how to change the objective function to get a better landscape. The project then extended its scope to look at properties of optimization landscape in practice, giving theoretical understanding to mode connectivity phenomenon (where different minimizers of neual networks can be connected by simple curves) that was observed in practice.\n\nRobust statistics seeks to understand the problem of getting reliable estimation in the case of a strong adversary: if the adversary is allowed to arbitrarily change a small fraction of the data. This is closely related to data-poisoning attacks for neural networks. Existing algorithms for robust statistics are often quite complicated and require tuning of hyperparameters. This developed new algorithms and frameworks for designing robust statistics algorithms using faster algorithms and nonconvex optimization - including the first near-linear time algorithm for robust mean estimation and a simple nonconvex objective that gives practical algorithms. \n\nRepresentation learning is a broad area. This project works on models of text and reinforcement learning, with a focus on linear-algebraic structures. This led to new approaches in understanding compositions in word embeddings, as well as a new characterization of forward and backward objectives for reinforcement learning.\n\nAll the papers that this project supported were available online and presented in related conferences. The project supported PhD students and post-docs who have contributed to machine learning in both academia and industry. The project also supported undergraduate researchers (some of the results mentioned above were done mostly by undergraduate students) who are now PhD students in theory of machine learning. Many workshops were organized to discuss and disseminate the work.\n\n\t\t\t\t\tLast Modified: 09/29/2022\n\n\t\t\t\t\tSubmitted by: Rong Ge"
 }
}