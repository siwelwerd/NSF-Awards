{
 "awd_id": "1655839",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Postdoctoral Fellowship: The Changing Interface between Data, Theories and Communities in Neuroimaging Research",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Frederick Kronz",
 "awd_eff_date": "2017-04-01",
 "awd_exp_date": "2019-09-30",
 "tot_intn_awd_amt": 188400.0,
 "awd_amount": 188400.0,
 "awd_min_amd_letter_date": "2017-01-26",
 "awd_max_amd_letter_date": "2017-01-26",
 "awd_abstract_narration": "General Audience Summary   \r\n\r\nThis Postdoctoral Fellowship supports a research project focused on philosophical questions associated with neuroimaging technologies. The specific questions to be addressed include the following. How are new technologies for sharing, organizing, and analyzing data and theories changing the norms of evidence in neuroimaging research? How are these technologies bringing research communities, theories, data, and analysis techniques together in novel ways? What role do different data analysis techniques play in using data as evidence for claims about phenomena it was not produced to investigate? How does the use of new technologies change the way cognitive scientists interpret their data? The postdoctoral fellow will engage in participatory experience in the practice of neuroscience to obtain valuable insight into the use and structure of these technologies, which will then be used to address these philosophical questions. More specifically, the fellow will be situated in a neuroscience lab that uses and develops tools such as the Cognitive Atlas, a community driven-knowledge base, and OpenfMRI, a database of minimally processed neuroimaging data. The postdoc will contribute to ongoing research using these tools. He will gain participatory experience regarding their use and development, and while doing so engage in philosophical analysis to addresses his questions. The analysis will in turn serve to enhance the use and development of these technologies, as well as the practice of neuroscience. By situating the fellow, a philosopher of science, within the active community of cognitive neuroscientists, the project will provide members of that community with cross-disciplinary expertise. More broadly, the project will demonstrate the value of engaged philosophy of neuroscience for both advancing the practice of neuroscience, and improving the richness and quality of philosophical analyses that take neuroscience to be its subject.\r\n\r\nTechnical Summary     \r\n\r\nThe proposed research project is focused on addressing philosophical issues in the epistemology of science, by approaching them from the perspective of the practices that give rise these issues. Over the last decade, neuroimaging research has involved new technologies and tools for sharing, organizing, and analyzing data and theories. This has gone hand in hand with two movements in cognitive neuroscience. The first is the growing pressure to make research practices reproducible and transparent, with the aim of improving the standards of evidence and quality of research. The second is the realization that newly developed analysis techniques that could be used to investigate hypotheses and theories previously beyond the scope of available evidence require large collections of organized and annotated data to produce meaningful results. The proposed project involves simultaneously engaging in the practice of using these technologies and studying how they are changing the evidential and theoretical landscape of cognitive neuroscience. The research fellow will use philosophical concepts to evaluate the strength of scientific inferences and to identify assumptions implicit in aspects of the technology. For example, he will explore the relations between concepts allowed by the Cognitive Atlas or its use, such as the decisions that are made during the analysis of data. This application of philosophical frameworks will also provide feedback on their descriptive and normative adequacy. In this way, this project will contribute to philosophical discussions about the epistemology of data-intensive science and neuroscience, and bring insight from those contributions to bear on the use and development of technologies (such as the Cognitive Atlas).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jessey",
   "pi_last_name": "Wright",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jessey Wright",
   "pi_email_addr": "jesseywright@gmail.com",
   "nsf_id": "000727899",
   "pi_start_date": "2017-01-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Helen",
   "pi_last_name": "Longino",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Helen E Longino",
   "pi_email_addr": "hlongino@stanford.edu",
   "nsf_id": "000476559",
   "pi_start_date": "2017-01-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Russell",
   "pi_last_name": "Poldrack",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Russell Poldrack",
   "pi_email_addr": "poldrack@stanford.edu",
   "nsf_id": "000587419",
   "pi_start_date": "2017-01-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "450 Serra Mall; Bldg 420",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052130",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760300",
   "pgm_ele_name": "STS-Sci, Tech & Society"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1353",
   "pgm_ref_txt": "Hist & Philosophy of SET"
  },
  {
   "pgm_ref_code": "7137",
   "pgm_ref_txt": "POSTDOCTORAL FELLOWSHIPS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 188400.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project had a philosopher observe and collaborate with a team of neuroscientists in order to evaluate how the development and uptake of new techniques for handling and manipulating data affects how scientists frame, present, and recognize data as evidence. Throughout the project the investigator contributed philosophical analysis and reasoning to a variety of neuroscience projects, and shared the findings of this research to the community of tool developers in neuroimaging research.</p>\n<p>This work revealed how new techniques for handling data can alter the range of hypotheses and theories that data is brought to bear as evidence on. For example, innovations in multivariate methods of analyzing neuroimaging data are part of what lead some neuroscientists to use imaging data as evidence for claims about the information represented within the brain's signalling patterns.</p>\n<p>Less strikingly, but no less important, have been the development of tools that allow neuroscientists to share their data, such as databases like OpenNeuro, and tools for cleaning data and evaluating its quality, such as fMRIPrep. These technologies are increasing the standards of evidenice in neuroimaging research by improving the overall depth of understanding researchers have of how data transformations change data. FMRIPrep, for example, is a tool that transforms neuroimaging data by removing noise such as head motion so that&nbsp;it is possible to analyze it. As it transforms the data it generates visualizations that depict the data being changed at key steps in the process. These visualizations can be used to evaluate if the algorithm funcitoned properly, detect faults in the data themselves, and help users to develop a better understanding of how the algorithm alters their data.</p>\n<p>Tools are often created to solve problems, such as fMRIPrep being developed to make it easier to perform good quality pre-processing on imaging data. When creating a new tool,&nbsp;developers often adapt and retrofit methods and techniques developed in other domains to fit with the unique characteristics, features and challenges of their research. This process of adaptation ensures that the new tool can actually solve the problems researchers are dealing with. Because they have to tinker with the tool to get it to work, developers come to have a deeper understanding of the tool and its limitations. This understanding&nbsp;allows&nbsp;them to usee tools they make as sophisticated techniques. That is, they use their tools in a skillful manner that is sensitive to many of the limitations and advantages that the tool affords. Many research applications do not require modifications to the tools used. Indeed, analysis tools are often developd so that the average user doesn't have to make adjustments to its workings in order to apply it. If the tool isn't heavily modifier, then it is used as a template.</p>\n<p>This project found, perhaps unsurprisingly, that tools are most productive when used like techniques. When a tool is both powerful and widely used as a template, progress in a scientific domain can actually be slowed down even if publication rates are increasing. This is because easy to use and powerful tools can 'take over' a research field, leading scientists to focus on questions that the new tools are well-tuned to answering, and not on questions that may be more productive or insightful to answer.</p>\n<p>The final finding of this project was that, while tool developers strive to make tools easy to use, the easier tools are to use the less depth of understanding the average tool user obtains. This increases the liklihood of 'well-intentioned' misuse, which is when a tool user performs an analysis or interprets a result in a way that is inconsistent with how the analysis tool actually transforms the data. This misinterpretation is not out of malice or intent to deceive. Well intentioned misuse is caused by a misunderstanding of how the tool acts as an interface between them and the data.</p>\n<p>Cases of well intentioned misuse can be prevented by introducing 'friction' into the tool. That is, by making the tool difficult to use in circumstances where it would be easy to make a mistake. This is not to say friction in tools makes them unuable, but only that it makes it so users need to obtain a sufficient level of expertise with the tool in order to apply the tool. In this way friction helps users move from using the tool as a template to using it as a technique. Tool developers, such as those who made fMRIPrep, can also include visualizations and other features to assist with data quality control into their tool as a way to addess the underlying cause of well-intentioned misuse. Visualizations in fMRIPrep, for example, were found to help users of the tool to develop a deeper understanding of data preprocessing without extra effort on the users part.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/29/2020<br>\n\t\t\t\t\tModified by: Jessey&nbsp;Wright</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project had a philosopher observe and collaborate with a team of neuroscientists in order to evaluate how the development and uptake of new techniques for handling and manipulating data affects how scientists frame, present, and recognize data as evidence. Throughout the project the investigator contributed philosophical analysis and reasoning to a variety of neuroscience projects, and shared the findings of this research to the community of tool developers in neuroimaging research.\n\nThis work revealed how new techniques for handling data can alter the range of hypotheses and theories that data is brought to bear as evidence on. For example, innovations in multivariate methods of analyzing neuroimaging data are part of what lead some neuroscientists to use imaging data as evidence for claims about the information represented within the brain's signalling patterns.\n\nLess strikingly, but no less important, have been the development of tools that allow neuroscientists to share their data, such as databases like OpenNeuro, and tools for cleaning data and evaluating its quality, such as fMRIPrep. These technologies are increasing the standards of evidenice in neuroimaging research by improving the overall depth of understanding researchers have of how data transformations change data. FMRIPrep, for example, is a tool that transforms neuroimaging data by removing noise such as head motion so that it is possible to analyze it. As it transforms the data it generates visualizations that depict the data being changed at key steps in the process. These visualizations can be used to evaluate if the algorithm funcitoned properly, detect faults in the data themselves, and help users to develop a better understanding of how the algorithm alters their data.\n\nTools are often created to solve problems, such as fMRIPrep being developed to make it easier to perform good quality pre-processing on imaging data. When creating a new tool, developers often adapt and retrofit methods and techniques developed in other domains to fit with the unique characteristics, features and challenges of their research. This process of adaptation ensures that the new tool can actually solve the problems researchers are dealing with. Because they have to tinker with the tool to get it to work, developers come to have a deeper understanding of the tool and its limitations. This understanding allows them to usee tools they make as sophisticated techniques. That is, they use their tools in a skillful manner that is sensitive to many of the limitations and advantages that the tool affords. Many research applications do not require modifications to the tools used. Indeed, analysis tools are often developd so that the average user doesn't have to make adjustments to its workings in order to apply it. If the tool isn't heavily modifier, then it is used as a template.\n\nThis project found, perhaps unsurprisingly, that tools are most productive when used like techniques. When a tool is both powerful and widely used as a template, progress in a scientific domain can actually be slowed down even if publication rates are increasing. This is because easy to use and powerful tools can 'take over' a research field, leading scientists to focus on questions that the new tools are well-tuned to answering, and not on questions that may be more productive or insightful to answer.\n\nThe final finding of this project was that, while tool developers strive to make tools easy to use, the easier tools are to use the less depth of understanding the average tool user obtains. This increases the liklihood of 'well-intentioned' misuse, which is when a tool user performs an analysis or interprets a result in a way that is inconsistent with how the analysis tool actually transforms the data. This misinterpretation is not out of malice or intent to deceive. Well intentioned misuse is caused by a misunderstanding of how the tool acts as an interface between them and the data.\n\nCases of well intentioned misuse can be prevented by introducing 'friction' into the tool. That is, by making the tool difficult to use in circumstances where it would be easy to make a mistake. This is not to say friction in tools makes them unuable, but only that it makes it so users need to obtain a sufficient level of expertise with the tool in order to apply the tool. In this way friction helps users move from using the tool as a template to using it as a technique. Tool developers, such as those who made fMRIPrep, can also include visualizations and other features to assist with data quality control into their tool as a way to addess the underlying cause of well-intentioned misuse. Visualizations in fMRIPrep, for example, were found to help users of the tool to develop a deeper understanding of data preprocessing without extra effort on the users part.\n\n\t\t\t\t\tLast Modified: 01/29/2020\n\n\t\t\t\t\tSubmitted by: Jessey Wright"
 }
}