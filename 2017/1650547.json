{
 "awd_id": "1650547",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "I/UCRC: Center for Unmanned Aircraft Systems, Phase II Site",
 "cfda_num": "47.041, 47.070",
 "org_code": "05050000",
 "po_phone": "7032927408",
 "po_email": "mokumar@nsf.gov",
 "po_sign_block_name": "Mohan Kumar",
 "awd_eff_date": "2017-03-15",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 2164805.0,
 "awd_min_amd_letter_date": "2017-03-15",
 "awd_max_amd_letter_date": "2021-12-20",
 "awd_abstract_narration": "The Center for Unmanned Aircraft Systems (C-UAS) addresses the issues common to the unmanned aircraft system (UAS) industry that limit widespread application across national security, scientific, civil, and commercial domains. Research within the UAS industry is driven by both technical gaps existing for specific high-value applications and the current under-developed regulatory framework that is needed for integration of UAS into the national airspace. The full value of unmanned aircraft systems, especially for a broad range of scientific and civil applications, cannot be realized without significant multidisciplinary research efforts such as those proposed here. Toward that goal, C-UAS investigates and develops new algorithms, architectures, and operational procedures for unmanned aircraft systems. The center contributes to the advancement of the state of the art for UAS through its research at the center's universities and by training graduate students in areas supporting the advancement of UAS. \r\n\r\nThe research pursued in C-UAS has potential application to unmanned aircraft of all sizes. The primary focus of research activities, however, is on small unmanned aircraft systems (SUAS), which feature aircraft with wingspans in the 1 ft to 8 ft range. C-UAS university sites have distinguished themselves with their experimental flight test demonstrations on these smaller platforms. The research interests and needs of industry in the area of UAS align well with the skills, knowledge, and background of the university participants in the center. Research focus areas for the Brigham Young University site can be described in terms of (1) technical areas and (2) application areas. Technical topic areas in which the Brigham Young University has particular strength and interest include: (i) Aerodynamics and dynamics of UAS with multiple propellers in close proximity, (ii) Anomaly detection from airborne imagery, (iii) Target tracking algorithms utilizing visual features and dynamic models, (iv) Cooperative control for teams of heterogeneous vehicles, (v) Probabilistic programming for perceptually driven autonomous agents, and (vi) Navigation in cluttered environments with intermittent or degraded GPS. Application topic areas in which the Brigham Young University has particular strength and interest include: (i) Autopilot innovation for small UAS, (ii) Radar-based detection and avoidance of other aircraft, (iii) Robust tracking of ground targets, and (iv) Monitoring of large-scale infrastructure and post-earthquake damage assessment. Specific research projects proposed by Brigham Young University faculty members in these technical and application areas are selected annually by the Industry Advisory Board (IAB).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Timothy",
   "pi_last_name": "McLain",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Timothy W McLain",
   "pi_email_addr": "mclain@byu.edu",
   "nsf_id": "000365157",
   "pi_start_date": "2017-03-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Randal",
   "pi_last_name": "Beard",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Randal W Beard",
   "pi_email_addr": "beard@byu.edu",
   "nsf_id": "000441886",
   "pi_start_date": "2017-03-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Franke",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Kevin W Franke",
   "pi_email_addr": "kfranke@et.byu.edu",
   "nsf_id": "000613744",
   "pi_start_date": "2017-03-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Hedengren",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "John Hedengren",
   "pi_email_addr": "john.hedengren@byu.edu",
   "nsf_id": "000641351",
   "pi_start_date": "2017-03-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Cammy",
   "pi_last_name": "Peterson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Cammy Peterson",
   "pi_email_addr": "cammypeterson@gmail.com",
   "nsf_id": "000726734",
   "pi_start_date": "2017-03-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brigham Young University",
  "inst_street_address": "A-153 ASB",
  "inst_street_address_2": "",
  "inst_city_name": "PROVO",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8014223360",
  "inst_zip_code": "846021128",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "UT03",
  "org_lgl_bus_name": "BRIGHAM YOUNG UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JWSYC7RUMJD1"
 },
 "perf_inst": {
  "perf_inst_name": "Brigham Young University",
  "perf_str_addr": "290 FB",
  "perf_city_name": "Provo",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "846021231",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "UT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "150400",
   "pgm_ele_name": "GOALI-Grnt Opp Acad Lia wIndus"
  },
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "576100",
   "pgm_ele_name": "IUCRC-Indust-Univ Coop Res Ctr"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "018Z",
   "pgm_ref_txt": "Grad Prep APG:Enhan. Activities"
  },
  {
   "pgm_ref_code": "019Z",
   "pgm_ref_txt": "Grad Prep APG:Enhan. Experience"
  },
  {
   "pgm_ref_code": "043Z",
   "pgm_ref_txt": "PPSR- Public Participation in Scientific"
  },
  {
   "pgm_ref_code": "1504",
   "pgm_ref_txt": "GRANT OPP FOR ACAD LIA W/INDUS"
  },
  {
   "pgm_ref_code": "5761",
   "pgm_ref_txt": "INDUSTRY/UNIV COOP RES CENTERS"
  },
  {
   "pgm_ref_code": "8039",
   "pgm_ref_txt": "Information, Communication & Computing"
  },
  {
   "pgm_ref_code": "8046",
   "pgm_ref_txt": "REV Supplements"
  },
  {
   "pgm_ref_code": "8237",
   "pgm_ref_txt": "CISE Interagency Agreements"
  },
  {
   "pgm_ref_code": "8808",
   "pgm_ref_txt": "Veterans Research Supplements"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 682759.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 481540.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 395825.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 268849.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 283332.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 52500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This Project Outcomes Report for the general public is displayed verbatim as submitted by the principal investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in the report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.&nbsp;</p>\n<p>The Center for Unmanned Aircraft Systems (C-UAS) was established in 2012 involving Brigham Young University, the University of Colorado Boulder, Virginia Tech, Texas A&amp;M, and the University of Michigan. The core purpose of C-UAS is to provide innovative solutions to key technical challenges and superb training for future leaders in the unmanned aircraft systems industry.&nbsp;</p>\n<p>We created cooperative path planning algorithms that optimize learning information about the environment.&nbsp;&nbsp;Unmanned aerial vehicles (UAVs) then share information learned to create a global understanding of the environment.&nbsp;&nbsp;The information is shared in a decentralized manner so that it is robust to vehicle attrition and insertion.&nbsp;&nbsp;The algorithms are also robust to communication and hardware constraints (e.g., limited communication ranges, bandwidth restrictions, or memory limitations).</p>\n<p>We also&nbsp;derived a new set of governing equations for vortex particle methods (VPM), reformulating the VPM as a meshless large eddy simulation (LES) in a scheme that is numerically stable, without increasing its computational cost.&nbsp;&nbsp;In addition to the VPM reformulation, a new anisotropic dynamic model of subfilter-scale vortex stretching was developed.&nbsp;&nbsp;Extensive validation was performed, demonstrating the stability, accuracy, and speed (100x faster than existing LES approaches) of the methodology for turbulent wake flow.</p>\n<p>As an application, we applied the analysis tools to the analysis of a fixed-wing, on-board generation, wind harvesting aircraft (or windcraft for short). Specifically, we explored variations in the rotation directions and vertical and horizontal spacing of the rotors to understand the complex interactional aerodynamics of a multi-rotor windcraft design.</p>\n<p>The Automated sUAV Infrastructure Monitoring project improves the optimization of a drone flight path to develop high-resolution, high-accuracy 3D models of infrastructure using structure from motion (SfM) algorithms. Conventional approaches for building high-resolution, high-accuracy point cloud models typically involve the use of laser-based methods (e.g., LiDAR, TLS). The potential for low-cost sUAVs configured with optical sensors to develop 3D point cloud models of sufficient resolution and accuracy offers several advantages over conventional laser-based approaches. Project outcomes include several new assessments and algorithms to optimize the view planning and path optimization for uncertain and cluttered environments. It includes the automatic identification of 3D point cloud model deficiencies and iterative replanning and improvement of the models. The work is a step-change improvement in infrastructure monitoring with UAVs. A significant outcome of the project is a campus-wide model that with an average accuracy of 1 cm accessible from&nbsp;<a href=\"https://3dbyu.byu.edu/\">https://3dbyu.byu.edu</a></p>\n<p>The Vision-based Tracking and Following&nbsp;project&nbsp;developed a suite of guidance and control algorithms that enable UAVs to visually track air- and ground-based objects, and then follow those objects so that the object remains in the field of view of the camera.&nbsp; The core algorithms in this suite include the visual front end, the recursive randomly sampled consensus (R-RANSAC) filter, and the target following algorithm.&nbsp; The system is architected to allow a variety of different approaches for each of these elements.&nbsp; The generic approach is to track and follow any target that moves in the image plane, but recent results use machine learning algorithms to detect salient targets.&nbsp; If multiple targets are detected, then a user may select the most relevant target to follow with the UAV.&nbsp; Key challenges solved in the project include the detection of moving targets from a moving camera, efficient data association and tracking using visual images with significant clutter and following algorithms that use only visual and inertial data.&nbsp; Recent results have extended these algorithms to targets that are modeled on Lie groups like SE(2) and SE(3).</p>\n<p>Our research in GPS-denied navigation has created a keyframe-based relative navigation approach with the following advantages: (1) It uses relative measurements to produce relative state estimates that are observable by design; (2) Keyframe-based relative navigation produces consistent, unbiased estimates. (3) Global map updates such as those from loop closures or GPS measurements do not cause undesired jumps in the global pose estimate; (4) The relative navigation front-end implementation is computationally efficient using common approaches such as the extended Kalman Filter or moving horizon estimation; and (5) The keyframe-based delta pose formulation of relative navigation enables real-time back-end optimization requiring only small amounts of data to be communicated. This is especially effective for cooperative navigation problems involving multiple vehicles. Using the relative navigation approach, we demonstrated the capability to produce accurate location results for a small multirotor aircraft flying indoor/outdoor paths varying levels of GPS degradation. We also demonstrated with flight tests significant performance benefits using the relative navigation approach for teams of aircraft localizing cooperatively.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/19/2022<br>\n\t\t\t\t\tModified by: Randal&nbsp;W&nbsp;Beard</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213348358_windcraft--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213348358_windcraft--rgov-800width.jpg\" title=\"Vorticity Contours for an 8-rotor Wind Harvesting Aircraft\"><img src=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213348358_windcraft--rgov-66x44.jpg\" alt=\"Vorticity Contours for an 8-rotor Wind Harvesting Aircraft\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Vorticity contours for an 8-rotor wind harvesting aircraft flying in a circular pattern, with all rotor, wing, and wake aerodynamics all coupled.</div>\n<div class=\"imageCredit\">Andrew Ning</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Randal&nbsp;W&nbsp;Beard</div>\n<div class=\"imageTitle\">Vorticity Contours for an 8-rotor Wind Harvesting Aircraft</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213172058_B-SPlinePathPlanning--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213172058_B-SPlinePathPlanning--rgov-800width.jpg\" title=\"Cooperative Path Planning using B-Splines\"><img src=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213172058_B-SPlinePathPlanning--rgov-66x44.jpg\" alt=\"Cooperative Path Planning using B-Splines\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The paths of two vehicles are optimized to follow a trajectory that senses high uncertainty in the domain.  The uncertainty is characterized by the background color map with brighter colors indicating larger uncertainty.</div>\n<div class=\"imageCredit\">PhD student Grant Stagg</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Randal&nbsp;W&nbsp;Beard</div>\n<div class=\"imageTitle\">Cooperative Path Planning using B-Splines</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213044014_Beard--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213044014_Beard--rgov-800width.jpg\" title=\"System Architecture\"><img src=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213044014_Beard--rgov-66x44.jpg\" alt=\"System Architecture\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">System architecture for vision-based tracking and following.  The visual front-end converts monochrome images to moving features.  The R-RANSAC MTT produces tracks and covariances.  The tracks are then used to command the UAS.</div>\n<div class=\"imageCredit\">Randal Beard</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Randal&nbsp;W&nbsp;Beard</div>\n<div class=\"imageTitle\">System Architecture</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213442154_Hedengren-Franke--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213442154_Hedengren-Franke--rgov-800width.jpg\" title=\"High-Accuracy 3-D Model\"><img src=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213442154_Hedengren-Franke--rgov-66x44.jpg\" alt=\"High-Accuracy 3-D Model\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Campus-wide model that with an average accuracy of 1 cm</div>\n<div class=\"imageCredit\">Brigham Young University</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Randal&nbsp;W&nbsp;Beard</div>\n<div class=\"imageTitle\">High-Accuracy 3-D Model</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213757055_McLain-page-001--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213757055_McLain-page-001--rgov-800width.jpg\" title=\"MAV Position Estimate for an Outdoor/Indoor Flight Path\"><img src=\"/por/images/Reports/POR/2022/1650547/1650547_10477349_1666213757055_McLain-page-001--rgov-66x44.jpg\" alt=\"MAV Position Estimate for an Outdoor/Indoor Flight Path\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Multirotor localization results flying indoors and outdoors with varying levels of GPS degradation.</div>\n<div class=\"imageCredit\">James Jackson</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Randal&nbsp;W&nbsp;Beard</div>\n<div class=\"imageTitle\">MAV Position Estimate for an Outdoor/Indoor Flight Path</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis Project Outcomes Report for the general public is displayed verbatim as submitted by the principal investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in the report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content. \n\nThe Center for Unmanned Aircraft Systems (C-UAS) was established in 2012 involving Brigham Young University, the University of Colorado Boulder, Virginia Tech, Texas A&amp;M, and the University of Michigan. The core purpose of C-UAS is to provide innovative solutions to key technical challenges and superb training for future leaders in the unmanned aircraft systems industry. \n\nWe created cooperative path planning algorithms that optimize learning information about the environment.  Unmanned aerial vehicles (UAVs) then share information learned to create a global understanding of the environment.  The information is shared in a decentralized manner so that it is robust to vehicle attrition and insertion.  The algorithms are also robust to communication and hardware constraints (e.g., limited communication ranges, bandwidth restrictions, or memory limitations).\n\nWe also derived a new set of governing equations for vortex particle methods (VPM), reformulating the VPM as a meshless large eddy simulation (LES) in a scheme that is numerically stable, without increasing its computational cost.  In addition to the VPM reformulation, a new anisotropic dynamic model of subfilter-scale vortex stretching was developed.  Extensive validation was performed, demonstrating the stability, accuracy, and speed (100x faster than existing LES approaches) of the methodology for turbulent wake flow.\n\nAs an application, we applied the analysis tools to the analysis of a fixed-wing, on-board generation, wind harvesting aircraft (or windcraft for short). Specifically, we explored variations in the rotation directions and vertical and horizontal spacing of the rotors to understand the complex interactional aerodynamics of a multi-rotor windcraft design.\n\nThe Automated sUAV Infrastructure Monitoring project improves the optimization of a drone flight path to develop high-resolution, high-accuracy 3D models of infrastructure using structure from motion (SfM) algorithms. Conventional approaches for building high-resolution, high-accuracy point cloud models typically involve the use of laser-based methods (e.g., LiDAR, TLS). The potential for low-cost sUAVs configured with optical sensors to develop 3D point cloud models of sufficient resolution and accuracy offers several advantages over conventional laser-based approaches. Project outcomes include several new assessments and algorithms to optimize the view planning and path optimization for uncertain and cluttered environments. It includes the automatic identification of 3D point cloud model deficiencies and iterative replanning and improvement of the models. The work is a step-change improvement in infrastructure monitoring with UAVs. A significant outcome of the project is a campus-wide model that with an average accuracy of 1 cm accessible from https://3dbyu.byu.edu\n\nThe Vision-based Tracking and Following project developed a suite of guidance and control algorithms that enable UAVs to visually track air- and ground-based objects, and then follow those objects so that the object remains in the field of view of the camera.  The core algorithms in this suite include the visual front end, the recursive randomly sampled consensus (R-RANSAC) filter, and the target following algorithm.  The system is architected to allow a variety of different approaches for each of these elements.  The generic approach is to track and follow any target that moves in the image plane, but recent results use machine learning algorithms to detect salient targets.  If multiple targets are detected, then a user may select the most relevant target to follow with the UAV.  Key challenges solved in the project include the detection of moving targets from a moving camera, efficient data association and tracking using visual images with significant clutter and following algorithms that use only visual and inertial data.  Recent results have extended these algorithms to targets that are modeled on Lie groups like SE(2) and SE(3).\n\nOur research in GPS-denied navigation has created a keyframe-based relative navigation approach with the following advantages: (1) It uses relative measurements to produce relative state estimates that are observable by design; (2) Keyframe-based relative navigation produces consistent, unbiased estimates. (3) Global map updates such as those from loop closures or GPS measurements do not cause undesired jumps in the global pose estimate; (4) The relative navigation front-end implementation is computationally efficient using common approaches such as the extended Kalman Filter or moving horizon estimation; and (5) The keyframe-based delta pose formulation of relative navigation enables real-time back-end optimization requiring only small amounts of data to be communicated. This is especially effective for cooperative navigation problems involving multiple vehicles. Using the relative navigation approach, we demonstrated the capability to produce accurate location results for a small multirotor aircraft flying indoor/outdoor paths varying levels of GPS degradation. We also demonstrated with flight tests significant performance benefits using the relative navigation approach for teams of aircraft localizing cooperatively.\n\n \n\n\t\t\t\t\tLast Modified: 10/19/2022\n\n\t\t\t\t\tSubmitted by: Randal W Beard"
 }
}