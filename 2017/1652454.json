{
 "awd_id": "1652454",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Goal-Guided Self-Reflective Control Interface in Teleoperation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2017-06-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 520729.0,
 "awd_amount": 520729.0,
 "awd_min_amd_letter_date": "2017-01-31",
 "awd_max_amd_letter_date": "2021-05-31",
 "awd_abstract_narration": "The focus of this project is on human-robot interactions (HRI) during teleoperation, for both the fundamental research and the educational activities.  The research derives from the observation that object grasping and manipulation using conventional teleoperation approaches places a significant control burden on the human operator and reduces task performance.  This is because indirect manipulation of an object through a robot hand may cause inaccurate or undesired robot motion, while the limited control inputs available to the operator (e.g., joystick, data glove) make direct kinematic mapping challenging for complex object manipulation tasks.  So the human operator must mentally and physically transform (e.g., rotate, translate, scale, deform) the desired robot actions to required inputs at the interface; these transformations significantly increase control difficulty.  The primary research goal of this project is to develop a novel goal-guided self-reflective control interface (GSRCI), which will enable the robot to understand the operator's high-level objective during an object-grasping operation and to conform to task constraints in order to reduce control difficulties and ensure the success of subsequent manipulation.  The primary educational activity derives from the common deficiency of distance learning programs supported by existing teleconferencing technologies, namely that they offer limited or no opportunities for hands-on learning.  To address this problem, an interactive distance learning system (IDLS) will be developed that immerses remote students in the classroom environment through student tele-controlling of a robot's arms and hands for object manipulation and/or interaction with other classmates, thereby enabling remote users to feel present in the classroom and engaged in class activities.  The task modeling technology as well as the goal-guided control interface technology from the research work will be coupled to develop an easy and intuitive teleoperation-based distance learning system for K-12 students.  The GSRCI represents transformative technology with the potential to provide a new paradigm of HRI that will significantly improve the power and quality of teleoperation and broadly impact applications related to diverse domains including assistance for the elderly and disabled, minimally invasive surgery, space and underwater explorations, military reconnaissance, nuclear servicing, and urban search and rescue.  The IDLS will foster engagement for remote students and allow them to successfully participate in STEM activities, offering disadvantaged groups the potential to learn regardless of their ability to physically attend a class setting.\r\n\r\nTo achieve these goals, a cognitive interface will be created that enables a robot to flexibly reproduce actions that accommodate the operator's motion inputs as well as autonomously regulate these actions in a self-reflective manner to compensate task constraints that facilitate subsequent manipulations.  A novel goal-achievement indicator will predict the level of goal accomplishment for the planned action.  Additionally, a goal-guided remedial planner will regulate this action to accomplish the goal by relaxing the constraint bound of the operator's motion inputs using an adaptive local search strategy.  New models for human and robot task-based grasp behaviors will also be developed, which will provide a knowledge base to infer human goals in a task and to conduct goal-guided robot-grasp planning.  To link quantified task constraints with symbolic tasks, account for uncertainty in task modeling, and allow task reasoning from partially observed data, a directed probabilistic Bayesian model will encode the statistical dependence among the task goal, object attributes, actions, and task constraints.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiaoli",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaoli Zhang",
   "pi_email_addr": "xlzhang@mines.edu",
   "nsf_id": "000654357",
   "pi_start_date": "2017-01-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Colorado School of Mines",
  "inst_street_address": "1500 ILLINOIS ST",
  "inst_street_address_2": "",
  "inst_city_name": "GOLDEN",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3032733000",
  "inst_zip_code": "804011887",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "CO07",
  "org_lgl_bus_name": "TRUSTEES OF THE COLORADO SCHOOL OF MINES",
  "org_prnt_uei_num": "JW2NGMP4NMA3",
  "org_uei_num": "JW2NGMP4NMA3"
 },
 "perf_inst": {
  "perf_inst_name": "Colorado School of Mines",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "804011887",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "CO07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 71478.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 129888.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 137224.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 88441.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 93698.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focuses on the development of a goal-guided self-reflective control interface (GSRCI) which enables the robot to perform self-reflection in order to flexibly reproduce actions that accommodate the operator&rsquo;s motion inputs as well as autonomously regulate the actions to compensate task constraints that facilitate subsequent manipulation. In particular, the emphasis was placed on developing: 1) human manipulation intent inference models for grasping tasks, 2) robot task-based grasp models and planning, 3) goal-guided self-reflective control architecture which guides the semi-autonomous actions that accommodate motion inputs and compensate task constraints.</p>\n<p>In the first phase of the research, we developed a novel multi-label classification system able to decipher natural human motion for intended manipulation tasks. To effectively achieve this, the models used real object interactions to evaluate subtle motion and extended this to virtual object interaction needed for teleoperation. Further, we developed Reinforcement Learning strategies to determine an operator&rsquo;s grasping preference when multiple candidate solutions are possible. Likewise, we also developed learning strategies for a robot to assess the intended goal an operator is trying to accomplish and whether we should have the robot learn the specific operator first, or the intended task first.</p>\n<p>In the second phase, we sought to develop grasp models based on tasks. The grasp models have been developed where manipulation intent inference was used as an input to determine a desired output grasp posture. These postures aim to handle the uncertainty that is inherent in ambiguous human motion. Further, autonomous grasp strategies were developed to incorporate goal-guided in-hand manipulation. First, we developed a policy which determines the placement of contact points on an object, even for a moving object. Then, we determined grasp strategies which utilized a multi-agent approach that allowed for objects to be manipulated in desired trajectories and patterns.</p>\n<p>In the third phase we developed self-reflective shared control schemes where operator actions guided the robot to adjusted desired grasp configurations. The shared control schemes looked at both blending and filter-based strategies. For the blending strategy, the control allocation was redefined to provide dimension-specific assistance and provides more assistance in dimensions the operator is actively moving focusing on. For the filter-based strategy, an adaptive self-reflective acceptance criterion was designed. The acceptance criteria allowed operators to make refined adjustments needed for manipulation tasks which would otherwise be rejected.</p>\n<p>In addition to the scientific contributions of the project described above an interactive distance learning system (IDLS) was developed to immerse remote students and instructors with a robotic system. Towards this effort robust full body motion mapping has been developed and used for enhanced telepresence. This effort utilized a VR headset where operators could directly see the robot&rsquo;s environment. To interact with the environment, two separate body tracking systems were used, a RGB-D camera and another that relies on the handheld joystick control. The outcome was the IDLS system had high engagement in controlled settings.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/25/2023<br>\n\t\t\t\t\tModified by: Xiaoli&nbsp;Zhang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project focuses on the development of a goal-guided self-reflective control interface (GSRCI) which enables the robot to perform self-reflection in order to flexibly reproduce actions that accommodate the operator\u2019s motion inputs as well as autonomously regulate the actions to compensate task constraints that facilitate subsequent manipulation. In particular, the emphasis was placed on developing: 1) human manipulation intent inference models for grasping tasks, 2) robot task-based grasp models and planning, 3) goal-guided self-reflective control architecture which guides the semi-autonomous actions that accommodate motion inputs and compensate task constraints.\n\nIn the first phase of the research, we developed a novel multi-label classification system able to decipher natural human motion for intended manipulation tasks. To effectively achieve this, the models used real object interactions to evaluate subtle motion and extended this to virtual object interaction needed for teleoperation. Further, we developed Reinforcement Learning strategies to determine an operator\u2019s grasping preference when multiple candidate solutions are possible. Likewise, we also developed learning strategies for a robot to assess the intended goal an operator is trying to accomplish and whether we should have the robot learn the specific operator first, or the intended task first.\n\nIn the second phase, we sought to develop grasp models based on tasks. The grasp models have been developed where manipulation intent inference was used as an input to determine a desired output grasp posture. These postures aim to handle the uncertainty that is inherent in ambiguous human motion. Further, autonomous grasp strategies were developed to incorporate goal-guided in-hand manipulation. First, we developed a policy which determines the placement of contact points on an object, even for a moving object. Then, we determined grasp strategies which utilized a multi-agent approach that allowed for objects to be manipulated in desired trajectories and patterns.\n\nIn the third phase we developed self-reflective shared control schemes where operator actions guided the robot to adjusted desired grasp configurations. The shared control schemes looked at both blending and filter-based strategies. For the blending strategy, the control allocation was redefined to provide dimension-specific assistance and provides more assistance in dimensions the operator is actively moving focusing on. For the filter-based strategy, an adaptive self-reflective acceptance criterion was designed. The acceptance criteria allowed operators to make refined adjustments needed for manipulation tasks which would otherwise be rejected.\n\nIn addition to the scientific contributions of the project described above an interactive distance learning system (IDLS) was developed to immerse remote students and instructors with a robotic system. Towards this effort robust full body motion mapping has been developed and used for enhanced telepresence. This effort utilized a VR headset where operators could directly see the robot\u2019s environment. To interact with the environment, two separate body tracking systems were used, a RGB-D camera and another that relies on the handheld joystick control. The outcome was the IDLS system had high engagement in controlled settings.\n\n \n\n\t\t\t\t\tLast Modified: 07/25/2023\n\n\t\t\t\t\tSubmitted by: Xiaoli Zhang"
 }
}