{
 "awd_id": "1741162",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Reliable Inference with Big Data: Reproducibility, Data Sharing, Heterogeneity",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 650000.0,
 "awd_amount": 650000.0,
 "awd_min_amd_letter_date": "2017-09-01",
 "awd_max_amd_letter_date": "2017-09-01",
 "awd_abstract_narration": "Over the last decade, 'big data' technologies have allowed the acquisition of vast amount of data (e.g. through smartphones) and their accumulation into large scale databases. Powerful hardware and software systems have been developed to crunch these data and extract statistical models. For instance, the outcome of a certain medical procedure can be modeled in terms of the features of the patient, thus in principle providing a personalized risk score for that procedure. Unfortunately, the increasing complexity of  these data and of the algorithms used has made statistical models significantly less transparent. How certain are we of these statistical predictions? What is their limit of validity? How biased is the resulting model?\r\n\r\nThis project focuses on four main challenges that are ubiquitous in big-data, and are crucial to extract reliable insights: reproducibility; data sharing; missing data; data heterogeneity. \r\n\r\n(1) Reproducibility requires being able to compare two models extracted from different data sets (e.g. after additional data have been accumulated). This is in turn impossible unless we have reliable procedures to quantify uncertainty and confidence in complex high-dimensional models. Recently proposed ideas in this direction are still insufficient to cope with realistic large-scale applications.\r\n\r\n(2) Data sharing is a key feature of modern data analysis, whereby a single massive data set is being studied by  hundreds of independent researchers. Unguarded statistical inference by such a population of researchers unavoidably leads to large numbers of false discoveries. The project builds on false discovery rate-controlling methods to propose safe approaches for decentralized data analysis.\r\n\r\n(3) Missing data are ubiquitous in big data. While several methods have been developed in the past to deal with missing data, it is unclear to what extent they are applicable to modern scenarios. The project aims at developing principled guidelines based on a rigorous comparison of various approaches,  and developing new algorithms based on maximum likelihood.\r\n\r\n(4) Data heterogeneity. Big data are often produced by the aggregation of multiple data sources. How can we prevent standard statistical procedures to be critically affected by such heterogeneities? The project uses new regularization schemes to fusion information across multiple sources.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrea",
   "pi_last_name": "Montanari",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andrea Montanari",
   "pi_email_addr": "montanari@stanford.edu",
   "nsf_id": "000107366",
   "pi_start_date": "2017-09-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "450 Serra Mall",
  "perf_city_name": "Palo Alto",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 650000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Statistical methodologies have largely been developed to construct simple modelsthat capture the variability of large homogeneous datasets.&nbsp;The surprising success of data science and machine learning largely concernscenarios to which those original ideas do not apply: data are heterogeneous or incomplete,&nbsp;models are extremely complex, and distinguishing between statistical uncertainty&nbsp;and systematic uncertainty is very difficult.<br />This project aimed at developing mathematical tools and concepts to understand the success&nbsp;of existing statistical methodologies in these new settings, and to develop new methodologies.Three example illustrate the project's results in the context of linear, regression,which is arguably one of the the most used and useful tools in the hands of the statistician.<br />First as mentioned above, data are often incomplete. The most popular approach&nbsp;to alleviate this problem is to produce estimates of the missing entries and use them insteadof the actual values. The project developed the first analysis of this procedure in&nbsp;modern applications in which the number of parameters is comparable or larger than the number of samples.<br />Second, assessing uncertainties on estimated parameters is particularly challenging in&nbsp;these applications. This problem can be overcome by a debiasing procedure when the distribution ofthe covariates is known or its covariance structure can be estimated accurately.&nbsp;This is however not the case in most applications. This project developed the first&nbsp;procedure that accounts for the uncertainty in the covariance estimate.<br />Third, carefully adjusting model's complexity to the sample size has long been consideredcrucial to the success of nonparametric and techniques and a cornerstone of statistical&nbsp;learning theory. Oblivious to this prescription, machine learning practitioners have opted for&nbsp;a relatively loose control of model complexity: richer, overparametrized models appear to be simplerto optimize when the data fitting criterion is non-convex. Can this practice be reconciledwith statistical theory? This project too several steps in this direction by showing&nbsp;&nbsp;by developing a sharp analysis of overparametrized linear models, and showing thatminimal or vanishing regularization can be optimal, for instance when covariates present a strong latent&nbsp;space structure.<br /><br />Intellectual merit: This project addressed some core problems in modern data science,which require rethinking commonly accepted prescriptions based on classical theory.Statistical methodology is fundamentally based on the assumption of observing a large&nbsp;sample from an homogeneous population. Uniform convergence theory conveniently leverages thisassumption. This project had to bypass this standard approach and develop new mathematical tools&nbsp;and new algorithms to understand these challenging regimes.</p>\n<p><br />Broader impact: Statistical estimation theory and statistical learning theory lie at the&nbsp;crossroad of multiple research communities: statistics, optimization, machine learning, information theory.The results developed in this project spurred follow up work in several of these areas.Methodology developed by this project is useful to perform statistical estimation (e.g. via imputation&nbsp;techniques in generalized linear models) and statistical inference (e.g. via computation ofconfidence intervals in high-dimensional linear regression). These are fundamental statistical tasksthat need to be carried out in science and engineering applications.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/17/2022<br>\n\t\t\t\t\tModified by: Andrea&nbsp;Montanari</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nStatistical methodologies have largely been developed to construct simple modelsthat capture the variability of large homogeneous datasets. The surprising success of data science and machine learning largely concernscenarios to which those original ideas do not apply: data are heterogeneous or incomplete, models are extremely complex, and distinguishing between statistical uncertainty and systematic uncertainty is very difficult.\nThis project aimed at developing mathematical tools and concepts to understand the success of existing statistical methodologies in these new settings, and to develop new methodologies.Three example illustrate the project's results in the context of linear, regression,which is arguably one of the the most used and useful tools in the hands of the statistician.\nFirst as mentioned above, data are often incomplete. The most popular approach to alleviate this problem is to produce estimates of the missing entries and use them insteadof the actual values. The project developed the first analysis of this procedure in modern applications in which the number of parameters is comparable or larger than the number of samples.\nSecond, assessing uncertainties on estimated parameters is particularly challenging in these applications. This problem can be overcome by a debiasing procedure when the distribution ofthe covariates is known or its covariance structure can be estimated accurately. This is however not the case in most applications. This project developed the first procedure that accounts for the uncertainty in the covariance estimate.\nThird, carefully adjusting model's complexity to the sample size has long been consideredcrucial to the success of nonparametric and techniques and a cornerstone of statistical learning theory. Oblivious to this prescription, machine learning practitioners have opted for a relatively loose control of model complexity: richer, overparametrized models appear to be simplerto optimize when the data fitting criterion is non-convex. Can this practice be reconciledwith statistical theory? This project too several steps in this direction by showing  by developing a sharp analysis of overparametrized linear models, and showing thatminimal or vanishing regularization can be optimal, for instance when covariates present a strong latent space structure.\n\nIntellectual merit: This project addressed some core problems in modern data science,which require rethinking commonly accepted prescriptions based on classical theory.Statistical methodology is fundamentally based on the assumption of observing a large sample from an homogeneous population. Uniform convergence theory conveniently leverages thisassumption. This project had to bypass this standard approach and develop new mathematical tools and new algorithms to understand these challenging regimes.\n\n\nBroader impact: Statistical estimation theory and statistical learning theory lie at the crossroad of multiple research communities: statistics, optimization, machine learning, information theory.The results developed in this project spurred follow up work in several of these areas.Methodology developed by this project is useful to perform statistical estimation (e.g. via imputation techniques in generalized linear models) and statistical inference (e.g. via computation ofconfidence intervals in high-dimensional linear regression). These are fundamental statistical tasksthat need to be carried out in science and engineering applications.\n\n\t\t\t\t\tLast Modified: 04/17/2022\n\n\t\t\t\t\tSubmitted by: Andrea Montanari"
 }
}