{
 "awd_id": "1724341",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: INT: Autonomous Multi-Robot Visual Monitoring for Urban, Agricultural, and Natural Resource Management",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 1000000.0,
 "awd_amount": 1000000.0,
 "awd_min_amd_letter_date": "2017-08-17",
 "awd_max_amd_letter_date": "2018-11-09",
 "awd_abstract_narration": "This project develops technologies to improve natural resource monitoring systems.  Specifically, this project investigates how teams of smart and autonomous aerial robots equipped with visual sensors can monitor vegetation health over extended spatial and temporal scales.  It addresses the fundamental issues of i) multi-robot collaboration, ii) adaptation of robot trajectories according to the quality of collected visual data, iii) networked decision making under several conflicting constraints, and iv) learning planning and control at real-world dynamic environments.  The project further investigates how these four tasks can work seamlessly with each other, under autonomous and real-time operation.  In addition to improving vegetation monitoring, the outcomes of this project may benefit several other application domains, including habitat monitoring, border control, forest fire tracking and search-and-rescue operations. Additionally, the project involves several hands-on procedures that are expected to attract undergraduate student participation and enable outreach to K-12 students.\r\n\r\nThis research addresses the theoretical and technical challenges in order to enable autonomous multi-robot visual monitoring of critical natural resources.  A critical step in this effort is to introduce novel algorithmic frameworks that can enable networks of smart and autonomous aerial robots to detect and adapt to observed phenomena, while satisfying multiple - often conflicting - time-varying constraints and mission specifications.  To achieve this, the project contributes to three main areas. i) Adaptive visual sensing with focus on the analysis of visual data that can automatically adapt to the environment and the computational constraints. ii) Autonomous aerial robot navigation, developing reliable and energy-efficient autonomous aerial robot navigation strategies under resource constraints. iii) System-level decision-making and adaptation, which will focus on general reasoning, decision-making and control schemes for adaptive resource allocation under the real-time constraints.  The project seamlessly merges individual contributions into the proposed Adaptive Resource Monitoring (ARM) system.  ARM is cognizant of its environmental, technical and ethical constraints, and issues goal-oriented tasks for each agent.  ARM possesses the ability to reflect upon past decisions and reinforce advantageous ones, and is driven by knowledge-rich models and methods. The individual components and the overall system are tested through a rigorous evaluation plan, starting with individual modules and leading up to a system-level evaluation in the application domain.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Amit",
   "pi_last_name": "Roy-Chowdhury",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Amit K Roy-Chowdhury",
   "pi_email_addr": "amitrc@ece.ucr.edu",
   "nsf_id": "000309390",
   "pi_start_date": "2017-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "George",
   "pi_last_name": "Jenerette",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "George D Jenerette",
   "pi_email_addr": "darrel.jenerette@ucr.edu",
   "nsf_id": "000253529",
   "pi_start_date": "2017-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anastasios",
   "pi_last_name": "Mourikis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anastasios Mourikis",
   "pi_email_addr": "mourikis@ee.ucr.edu",
   "nsf_id": "000514830",
   "pi_start_date": "2017-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Qi",
   "pi_last_name": "Zhu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qi Zhu",
   "pi_email_addr": "qzhu@northwestern.edu",
   "nsf_id": "000600127",
   "pi_start_date": "2017-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Konstantinos",
   "pi_last_name": "Karydis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Konstantinos Karydis",
   "pi_email_addr": "kkarydis@ece.ucr.edu",
   "nsf_id": "000737288",
   "pi_start_date": "2017-08-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Riverside",
  "inst_street_address": "200 UNIVERSTY OFC BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "RIVERSIDE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9518275535",
  "inst_zip_code": "925210001",
  "inst_country_name": "United States",
  "cong_dist_code": "39",
  "st_cong_dist_code": "CA39",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA AT RIVERSIDE",
  "org_prnt_uei_num": "",
  "org_uei_num": "MR5QC5FCAVH5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Riverside",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "925210001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "39",
  "perf_st_cong_dist": "CA39",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  },
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 1000000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Title:&nbsp; S&amp;AS:INT: Autonomous Multi-Robot Visual Monitoring for Urban, Agricultural, and Natural Resource Management</strong></p>\n<p>The information collected by existing natural resource monitoring systems is often limited due to the challenges of acquiring high-quality data over large space-time horizons, which leads to inefficient decision-making. Satellite imagery is useful at large spatial scales, but unfortunately yields very coarse results. For example, it does not provide sufficient resolution to determine the health of individual trees. Aerial robots can be used to improve resolution, but current systems lack autonomous decision-making capabilities, and thus cannot adapt to varying environmental and system conditions. They rely on a human field expert in the loop to make decisions such as determination of unhealthy vegetation patches.&nbsp; This procedure is time-consuming, and scales poorly as collected datasets increase both spatially and temporally.</p>\n<p>The focus of this project was to develop automated solutions to complement the human field expert. &nbsp;Specifically, this project investigated how teams of smart and autonomous aerial robots equipped with visual sensors can monitor vegetation health over extended spatial and temporal scales.&nbsp; It addressed the fundamental issues of i) multi-robot collaboration, ii) adaptation of robot trajectories according to the quality of collected visual data, iii) networked decision making under several conflicting constraints, and iv) learning planning and control at real-world dynamic environments.&nbsp; The project further investigated how these four tasks can work seamlessly with each other, under autonomous and real-time operation.&nbsp; In addition to improving vegetation monitoring, the outcomes of this project may benefit several other application domains, including habitat monitoring, border control, forest fire tracking and search-and-rescue operations.</p>\n<p>The research objective of this project was to address the theoretical and technical challenges in order to enable autonomous multi-robot visual monitoring of critical natural resources. &nbsp;A critical step in this effort was to introduce novel algorithmic frameworks that can enable networks of smart and autonomous aerial robots to detect and adapt to observed phenomena, while satisfying multiple&mdash;often conflicting&mdash;time-varying constraints and mission specifications.&nbsp; To achieve this, the project contributed to three main areas.</p>\n<p>i)&nbsp; Adaptive visual sensing with focus on the analysis of visual data that can automatically adapt to the environment and the computational constraints.</p>\n<p>ii)&nbsp; Autonomous aerial robot navigation, developing reliable and energy-efficient autonomous aerial robot navigation strategies under resource constraints.</p>\n<p>iii) System-level decision-making and adaptation, which focuses on general reasoning, decision-making and control schemes for adaptive resource allocation under the real-time constraints.&nbsp;</p>\n<p>The project seamlessly merges individual contributions into the proposed Adaptive Resource Monitoring (ARM) system.&nbsp; ARM is <em>cognizant</em> of its environmental, technical and ethical constraints, and issues <em>goal-oriented tasks</em> for each agent.&nbsp; ARM possesses the ability to <em>reflect</em> upon past decisions and reinforce advantageous ones, and is driven by <em>knowledge-rich</em> models and methods. The individual components and the overall system were tested through a rigorous evaluation plan, starting with individual modules and leading up to a system-level evaluation in the application domain.</p>\n<p>The project supported over 15 PhD students across the various topics of interest. Many of them have graduated and are working in research and development positions. The students have published in the top-most conferences and journals in their areas of interest and have been trained to be independent researchers. The project has led to over 70 publications.&nbsp;The PIs have integrated the research results in their courses. Some of them have offered Senior Design projects which have directly benefited from the graduate student research.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/13/2023<br>\nModified by: Amit&nbsp;K&nbsp;Roy-Chowdhury</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1724341/1724341_10515823_1702489282372_Project_Picture--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1724341/1724341_10515823_1702489282372_Project_Picture--rgov-800width.png\" title=\"Multi-modal Data via Aerial and Ground Robots (ARCS Lab @ UCR)\"><img src=\"/por/images/Reports/POR/2023/1724341/1724341_10515823_1702489282372_Project_Picture--rgov-66x44.png\" alt=\"Multi-modal Data via Aerial and Ground Robots (ARCS Lab @ UCR)\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">(a) Multi-modal data from aerial robot flight (RGB, NIR, Thermal)\n\n(b) Multi-modal dataset for robot localization, mapping and crop monitoring in citrus tree farms.</div>\n<div class=\"imageCredit\">Konstantinos Karydis</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Amit&nbsp;K&nbsp;Roy-Chowdhury\n<div class=\"imageTitle\">Multi-modal Data via Aerial and Ground Robots (ARCS Lab @ UCR)</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nTitle: S&AS:INT: Autonomous Multi-Robot Visual Monitoring for Urban, Agricultural, and Natural Resource Management\n\n\nThe information collected by existing natural resource monitoring systems is often limited due to the challenges of acquiring high-quality data over large space-time horizons, which leads to inefficient decision-making. Satellite imagery is useful at large spatial scales, but unfortunately yields very coarse results. For example, it does not provide sufficient resolution to determine the health of individual trees. Aerial robots can be used to improve resolution, but current systems lack autonomous decision-making capabilities, and thus cannot adapt to varying environmental and system conditions. They rely on a human field expert in the loop to make decisions such as determination of unhealthy vegetation patches. This procedure is time-consuming, and scales poorly as collected datasets increase both spatially and temporally.\n\n\nThe focus of this project was to develop automated solutions to complement the human field expert. Specifically, this project investigated how teams of smart and autonomous aerial robots equipped with visual sensors can monitor vegetation health over extended spatial and temporal scales. It addressed the fundamental issues of i) multi-robot collaboration, ii) adaptation of robot trajectories according to the quality of collected visual data, iii) networked decision making under several conflicting constraints, and iv) learning planning and control at real-world dynamic environments. The project further investigated how these four tasks can work seamlessly with each other, under autonomous and real-time operation. In addition to improving vegetation monitoring, the outcomes of this project may benefit several other application domains, including habitat monitoring, border control, forest fire tracking and search-and-rescue operations.\n\n\nThe research objective of this project was to address the theoretical and technical challenges in order to enable autonomous multi-robot visual monitoring of critical natural resources. A critical step in this effort was to introduce novel algorithmic frameworks that can enable networks of smart and autonomous aerial robots to detect and adapt to observed phenomena, while satisfying multipleoften conflictingtime-varying constraints and mission specifications. To achieve this, the project contributed to three main areas.\n\n\ni) Adaptive visual sensing with focus on the analysis of visual data that can automatically adapt to the environment and the computational constraints.\n\n\nii) Autonomous aerial robot navigation, developing reliable and energy-efficient autonomous aerial robot navigation strategies under resource constraints.\n\n\niii) System-level decision-making and adaptation, which focuses on general reasoning, decision-making and control schemes for adaptive resource allocation under the real-time constraints.\n\n\nThe project seamlessly merges individual contributions into the proposed Adaptive Resource Monitoring (ARM) system. ARM is cognizant of its environmental, technical and ethical constraints, and issues goal-oriented tasks for each agent. ARM possesses the ability to reflect upon past decisions and reinforce advantageous ones, and is driven by knowledge-rich models and methods. The individual components and the overall system were tested through a rigorous evaluation plan, starting with individual modules and leading up to a system-level evaluation in the application domain.\n\n\nThe project supported over 15 PhD students across the various topics of interest. Many of them have graduated and are working in research and development positions. The students have published in the top-most conferences and journals in their areas of interest and have been trained to be independent researchers. The project has led to over 70 publications.The PIs have integrated the research results in their courses. Some of them have offered Senior Design projects which have directly benefited from the graduate student research.\n\n\n\t\t\t\t\tLast Modified: 12/13/2023\n\n\t\t\t\t\tSubmitted by: AmitKRoy-Chowdhury\n"
 }
}