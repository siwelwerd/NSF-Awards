{
 "awd_id": "1704189",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "NeTS: Medium: Collaborative Research: Diagnosing Datacenter Networks with Quantitative Provenance",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Darleen Fisher",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 343520.0,
 "awd_amount": 343520.0,
 "awd_min_amd_letter_date": "2017-07-22",
 "awd_max_amd_letter_date": "2022-07-12",
 "awd_abstract_narration": "The increasing complexity of data center networks has made it considerably more difficult to identify the source of a networking problem when something goes wrong.  However, a set of new diagnostic tools can help diagnose subtle bugs that would be difficult to find with existing tools.\r\nOne promising approach is based on data provenance, a concept that was originally developed by the database community but is now increasingly being applied in the networking domain. In this approach, the network keeps track of causality as data flows through the system -- for instance, by noting a router's configuration state that contributed to a particular forwarding decision. This information can then be used later to determine a\r\ncomprehensive explanation of an observed networking problem.\r\n\r\nThis project will develop a quantitative equivalent of provenance for data networking that can be used to reason about properties such as time or probability.  The key idea is to use this provenance to improve root-cause analysis of network events.  The proposed effort will develop the scientific foundations of quantitative provenance, as well as practical techniques for capturing, storing, and reasoning about it. The investigators will add several quantitative metrics to provenance: temporal, probabilistic and influence; three research thrusts will be considered, one corresponding to each of these metrics. The project will explore efficient and reusable implementations of new diagnostic tools, which will be applied to several concrete case studies.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Wenchao",
   "pi_last_name": "Zhou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wenchao Zhou",
   "pi_email_addr": "wzhou@cs.georgetown.edu",
   "nsf_id": "000636942",
   "pi_start_date": "2017-07-22",
   "pi_end_date": "2021-09-02"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Ujcich",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin E Ujcich",
   "pi_email_addr": "bu31@georgetown.edu",
   "nsf_id": "000859312",
   "pi_start_date": "2021-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgetown University",
  "inst_street_address": "MAIN CAMPUS",
  "inst_street_address_2": "",
  "inst_city_name": "WASHINGTON",
  "inst_state_code": "DC",
  "inst_state_name": "District of Columbia",
  "inst_phone_num": "2026250100",
  "inst_zip_code": "20057",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DC00",
  "org_lgl_bus_name": "GEORGETOWN UNIVERSITY",
  "org_prnt_uei_num": "TF2CMKY1HMX9",
  "org_uei_num": "TF2CMKY1HMX9"
 },
 "perf_inst": {
  "perf_inst_name": "Georgetown University",
  "perf_str_addr": "37th & O St N W",
  "perf_city_name": "Washington",
  "perf_st_code": "DC",
  "perf_st_name": "District of Columbia",
  "perf_zip_code": "200571789",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DC00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736300",
   "pgm_ele_name": "Networking Technology and Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 107660.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 76440.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 78598.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 80822.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In many fields that require computation, there has been a paradigm shift towards probabilistic and statistical reasoning. There has been increasing use of machine learning libraries whose outputs are intrinsically probabilistic, tied to the precision of machine learning models. In other domains, probabilistic reasoning has been used as a basis to tradeoff performance and accuracy, for example, when collecting and aggregating readings from sensors, or when propagating messages in a lossy bandwidth-constrained wireless channel. We have seen an emergence of probabilistic programming languages being proposed, yet such tools have lacked debugging capabilities.<br /><br />In this project, we developed a novel provenance model and system, called Provenance for Probabilistic logic Programs (P3) for analyzing programs written in probabilistic logic programming (PLP) languages. P3 enables four types of provenance queries: traditional explanation queries, queries for finding the set of most important derivations within an approximate error, top-K most influential queries, and modification queries that enable us to modify tuple probabilities with fewest modifications to program or input data. We applied these queries with real-world scenarios and developed practical algorithms for such queries. We developed a prototype of P3, and our evaluation on real-world data demonstrated that the system can support a wide-range of provenance queries with explainable results. Moreover, the system maintains provenance and executes queries efficiently with low overhead.<br /><br />This project supported 5 PhD students at Georgetown University.</p><br>\n<p>\n Last Modified: 01/12/2024<br>\nModified by: Benjamin&nbsp;E&nbsp;Ujcich</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn many fields that require computation, there has been a paradigm shift towards probabilistic and statistical reasoning. There has been increasing use of machine learning libraries whose outputs are intrinsically probabilistic, tied to the precision of machine learning models. In other domains, probabilistic reasoning has been used as a basis to tradeoff performance and accuracy, for example, when collecting and aggregating readings from sensors, or when propagating messages in a lossy bandwidth-constrained wireless channel. We have seen an emergence of probabilistic programming languages being proposed, yet such tools have lacked debugging capabilities.\n\nIn this project, we developed a novel provenance model and system, called Provenance for Probabilistic logic Programs (P3) for analyzing programs written in probabilistic logic programming (PLP) languages. P3 enables four types of provenance queries: traditional explanation queries, queries for finding the set of most important derivations within an approximate error, top-K most influential queries, and modification queries that enable us to modify tuple probabilities with fewest modifications to program or input data. We applied these queries with real-world scenarios and developed practical algorithms for such queries. We developed a prototype of P3, and our evaluation on real-world data demonstrated that the system can support a wide-range of provenance queries with explainable results. Moreover, the system maintains provenance and executes queries efficiently with low overhead.\n\nThis project supported 5 PhD students at Georgetown University.\t\t\t\t\tLast Modified: 01/12/2024\n\n\t\t\t\t\tSubmitted by: BenjaminEUjcich\n"
 }
}