{
 "awd_id": "1648466",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I: Design, deployment, and algorithmic optimization of zoomorphic, interactive robot companions",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Muralidharan Nair",
 "awd_eff_date": "2016-12-15",
 "awd_exp_date": "2018-05-31",
 "tot_intn_awd_amt": 225000.0,
 "awd_amount": 225000.0,
 "awd_min_amd_letter_date": "2016-12-13",
 "awd_max_amd_letter_date": "2017-09-27",
 "awd_abstract_narration": "The broader impact/commercial potential of this project spans the near-term and many years of future development. Over both phases, this proposal covers research and development (R&D) to create a robot pet companions with the potential to sell millions of units in the U.S. toy industry. The proposal also supports the development of R&D infrastructure that will be a critical component of the expansion in subsequent years to the U.S. pet industry (as a robot companion), which is larger and has less direct competition for robotic entrants. The first-generation robot product, a result of the Phase I project, will support science, technology, engineering, and math (STEM) education and robot hobbyists of all ages by facilitating user-friendly modification of its hardware and software as well as the creation of users? own robots and behavioral programs. Given the interactive nature of these modifiable robots, they are likely to have a strong appeal to females, who are underrepresented in STEM fields. The project?s ultimate goal is to develop and market interactive robots that can improve the quality of life for anyone through companionship.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project proposes to develop first-generation robot pets that will be ready to place in thousands of customers? hands and to situate the awarded company to grow to deliver millions of sophisticated robot pets across the world, including to the many people who cannot have pets. Towards these outcomes, the following innovations will be pursued in Phase I: a puppeteer platform that wirelessly controls robot characters; specification of a mobile, social robot character through machine learning; perception of robots and their environment; reliable autonomous recharging; and a simple cloud-based infrastructure for gathering usage data and conducting field experiments on versions of robot characters.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "William",
   "pi_last_name": "Knox",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "William B Knox",
   "pi_email_addr": "bradknox@cs.utexas.edu",
   "nsf_id": "000713426",
   "pi_start_date": "2016-12-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Emoters, Inc.",
  "inst_street_address": "1701 Maple Ave",
  "inst_street_address_2": "",
  "inst_city_name": "Austin",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5125423333",
  "inst_zip_code": "787021433",
  "inst_country_name": "United States",
  "cong_dist_code": "37",
  "st_cong_dist_code": "TX37",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": "KZGBJQDTYKU7"
 },
 "perf_inst": {
  "perf_inst_name": "Emoters, Inc.",
  "perf_str_addr": "9800 N Lamar",
  "perf_city_name": "Ste 310",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787534160",
  "perf_ctry_code": "US",
  "perf_cong_dist": "17",
  "perf_st_cong_dist": "TX17",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "6840",
   "pgm_ref_txt": "ROBOTICS"
  },
  {
   "pgm_ref_code": "8034",
   "pgm_ref_txt": "Hardware Components"
  },
  {
   "pgm_ref_code": "8035",
   "pgm_ref_txt": "Hardware Devices"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 225000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This NSF SBIR Phase I grant was used to develop the Bots Alive software platform. This platform has extensive and effective functionality in the areas of mobile object localization, robot control, machine learning of robot behavior, and more. More specifically, the software platform contains the following as a result of this grant:</p>\n<p style=\"padding-left: 30px;\">-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Novel computer vision algorithms built upon OpenCV for tracking 3D pose of small robots and objects of interest using a smart device camera or an external camera. These algorithms achieve low-latency, accuracy, and robustness that to our knowledge was not available previously.</p>\n<p style=\"padding-left: 30px;\">-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A framework for predicting future robot poses using each robot&rsquo;s forward model, which helps account for latency in sensing, decision-making, and communication to the robot&rsquo;s actuators.</p>\n<p style=\"padding-left: 30px;\">&nbsp;-&nbsp; &nbsp; &nbsp; A machine learning framework and algorithms to allow users or developers to program the behavior of a robot or digital character through demonstrations of what control/actions they desire in various contexts, a technique often called &ldquo;learning from demonstration&rdquo;, &ldquo;programming by demonstration&rdquo;, or &ldquo;imitation learning&rdquo;.</p>\n<p style=\"padding-left: 30px;\">-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Broad debugging visualizations for pose detection and tracking, path planning, and autonomous probabilistic decision-making.</p>\n<p>We believe that the Bots Alive software platform is poised to have a large and positive impact, both through (1) providing video-game-style play in the physical world and (2) via the largely untapped potential of user-teachable robotic and video-game characters.</p>\n<p>Regarding video-game-style play, Bots Alive provides localization and selective control of elements of a play scene, by which tried-and-true video-game mechanics can be enforced on physical objects in the user&rsquo;s own setting. The resultant play combines the tangible, tactile play of toys with richer motor-skill-based and strategic play that video games can facilitate. It also allows users to play with or against autonomous non-player characters&mdash;as they do in video games, but instead with physical toys.</p>\n<p>Regarding user-teachable characters, toys and video games that are human-teachable present a form of play that is a simpler form of programming than writing code and therefore could aid in STEM education efforts. Additionally, the large user base of toys and video games makes it a useful domain for developing human-teachable machine learning algorithms in general; such algorithms promise to make the power and economic gains of machine learning more accessible to non-expert people.<strong>&nbsp;</strong></p>\n<p>&nbsp;</p>\n<p>The Bots Alive software platform has been responsible for several significant commercial impacts: a successfully marketed and shipped Kickstarter product, being honored as an Emerging Innovator award finalist from one of the largest toy companies, and ongoing interest from multiple large toy companies in potential products build upon the platform. Additionally, largely on the promise of Bots Alive, Emoters was acquired by the toy startup Dash Robotics, where the PI continues to work. Dash Robotics is in a strong position to expand the platform through further research, development, and commercialization that will amplify its commercial impact.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/29/2018<br>\n\t\t\t\t\tModified by: William&nbsp;B&nbsp;Knox</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535585831794_ScreenShot2018-08-29at6.25.10PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535585831794_ScreenShot2018-08-29at6.25.10PM--rgov-800width.jpg\" title=\"Robot pose detection on smart phone\"><img src=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535585831794_ScreenShot2018-08-29at6.25.10PM--rgov-66x44.jpg\" alt=\"Robot pose detection on smart phone\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Robot pose detection on smart phone. The white ring around the robot indicates the inferred ground plane.</div>\n<div class=\"imageCredit\">William Bradley Knox</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">William&nbsp;B&nbsp;Knox</div>\n<div class=\"imageTitle\">Robot pose detection on smart phone</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535585772959_ScreenShot2018-08-29at6.17.37PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535585772959_ScreenShot2018-08-29at6.17.37PM--rgov-800width.jpg\" title=\"Visualization of inferred 3D pose\"><img src=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535585772959_ScreenShot2018-08-29at6.17.37PM--rgov-66x44.jpg\" alt=\"Visualization of inferred 3D pose\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Unity \"Scene\" display of the 3D pose of a robot and other objects, shown from a perspective different than the camera's.</div>\n<div class=\"imageCredit\">William Bradley Knox</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">William&nbsp;B&nbsp;Knox</div>\n<div class=\"imageTitle\">Visualization of inferred 3D pose</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535586037826_ScreenShot2018-08-29at6.26.38PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535586037826_ScreenShot2018-08-29at6.26.38PM--rgov-800width.jpg\" title=\"Computer vision visualizations for debugging\"><img src=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535586037826_ScreenShot2018-08-29at6.26.38PM--rgov-66x44.jpg\" alt=\"Computer vision visualizations for debugging\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Some of the visualizations implemented to show intermediate steps of computer vision and tracking of objects, which greatly improve the ability to debug these algorithms.</div>\n<div class=\"imageCredit\">William Bradley Knox</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">William&nbsp;B&nbsp;Knox</div>\n<div class=\"imageTitle\">Computer vision visualizations for debugging</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535586484400_BotsAlive-LightingConditions--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535586484400_BotsAlive-LightingConditions--rgov-800width.jpg\" title=\"Illustration of robustness to lighting changes\"><img src=\"/por/images/Reports/POR/2018/1648466/1648466_10467417_1535586484400_BotsAlive-LightingConditions--rgov-66x44.jpg\" alt=\"Illustration of robustness to lighting changes\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Screenshots taken seconds apart, after lights in a room were turned off, showing re-detection of all objects.</div>\n<div class=\"imageCredit\">William Bradley Knox</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">William&nbsp;B&nbsp;Knox</div>\n<div class=\"imageTitle\">Illustration of robustness to lighting changes</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis NSF SBIR Phase I grant was used to develop the Bots Alive software platform. This platform has extensive and effective functionality in the areas of mobile object localization, robot control, machine learning of robot behavior, and more. More specifically, the software platform contains the following as a result of this grant:\n-       Novel computer vision algorithms built upon OpenCV for tracking 3D pose of small robots and objects of interest using a smart device camera or an external camera. These algorithms achieve low-latency, accuracy, and robustness that to our knowledge was not available previously.\n-       A framework for predicting future robot poses using each robot?s forward model, which helps account for latency in sensing, decision-making, and communication to the robot?s actuators.\n -      A machine learning framework and algorithms to allow users or developers to program the behavior of a robot or digital character through demonstrations of what control/actions they desire in various contexts, a technique often called \"learning from demonstration\", \"programming by demonstration\", or \"imitation learning\".\n-       Broad debugging visualizations for pose detection and tracking, path planning, and autonomous probabilistic decision-making.\n\nWe believe that the Bots Alive software platform is poised to have a large and positive impact, both through (1) providing video-game-style play in the physical world and (2) via the largely untapped potential of user-teachable robotic and video-game characters.\n\nRegarding video-game-style play, Bots Alive provides localization and selective control of elements of a play scene, by which tried-and-true video-game mechanics can be enforced on physical objects in the user?s own setting. The resultant play combines the tangible, tactile play of toys with richer motor-skill-based and strategic play that video games can facilitate. It also allows users to play with or against autonomous non-player characters&mdash;as they do in video games, but instead with physical toys.\n\nRegarding user-teachable characters, toys and video games that are human-teachable present a form of play that is a simpler form of programming than writing code and therefore could aid in STEM education efforts. Additionally, the large user base of toys and video games makes it a useful domain for developing human-teachable machine learning algorithms in general; such algorithms promise to make the power and economic gains of machine learning more accessible to non-expert people. \n\n \n\nThe Bots Alive software platform has been responsible for several significant commercial impacts: a successfully marketed and shipped Kickstarter product, being honored as an Emerging Innovator award finalist from one of the largest toy companies, and ongoing interest from multiple large toy companies in potential products build upon the platform. Additionally, largely on the promise of Bots Alive, Emoters was acquired by the toy startup Dash Robotics, where the PI continues to work. Dash Robotics is in a strong position to expand the platform through further research, development, and commercialization that will amplify its commercial impact.\n\n\t\t\t\t\tLast Modified: 08/29/2018\n\n\t\t\t\t\tSubmitted by: William B Knox"
 }
}