{
 "awd_id": "1642406",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SSE: Development of a High-Performance Parallel Gibbs Ensemble Monte Carlo Simulation Engine",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Seung-Jong Park",
 "awd_eff_date": "2017-05-01",
 "awd_exp_date": "2022-04-30",
 "tot_intn_awd_amt": 499886.0,
 "awd_amount": 499886.0,
 "awd_min_amd_letter_date": "2017-04-28",
 "awd_max_amd_letter_date": "2017-04-28",
 "awd_abstract_narration": "The use of molecular simulation to study complex physical phenomena at the atomic level has grown exponentially over the last decade with increasing CPU power and the development of parallel molecular dynamics codes that scale efficiently over thousands of processors.  Molecular dynamics codes that utilize parallel computation on CPUs and GPUs are relatively well developed, however, there are a number of problems that cannot be simulated with this methodology. Specifically, problems that require the simulation of an open system, such as adsorption in porous materials, require an alternative methodology that allows for fluctuation in the number of molecules in the system.  In addition, there are a number of systems where the presence of large free energy barriers and slow diffusion preclude the use of standard molecular dynamics.  Notable examples include the prediction of phase equilibria in multi-component lipid bilayers, or polymers.  For these types of problems, Monte Carlo or hybrid Monte Carlo/molecular dynamics simulations have the potential to significantly improve computational efficiency.  This project is focused on the development of the open-source Monte Carlo simulation engine, GOMC, which is able to use low cost graphics processing units (GPUs) and multi-core processors (CPUs) to significantly reduce computational time.  This effort will enable Monte Carlo simulations to be performed with higher fidelity and for larger systems than is currently accessible with standard Monte Carlo simulation codes, enabling the accelerated development of new materials by domain scientists. In addition, this project will provide training for graduate and undergraduate students in Monte Carlo simulation, design of efficient algorithms for parallel computation on a variety of hardware architectures, and software development.  Tutorials and other educational materials will be created to support the use of GOMC for teaching Monte Carlo simulation of molecular systems to students in undergraduate and graduate courses at Wayne State as well as other universities. The free distribution of GOMC, along with the tutorials for using the software, will enable other research groups to solve important research problems quickly and accurately. This project, supported by the Office of Advanced Cyberinfrastructure (OAC), and the divisions of Material Research and Chemistry in the Directorate of Mathematical and Physical Sciences, and the Division of Chemical, Bioengineering, Environmental and Transport Systems (CBET) in the Directorate of Engneering, will result in software that enables new and better science. It also serves the educational mission of the National, through its active involvement of graduate and undergraduate students.\r\n\r\nParallelization of Monte Carlo is complicated by the inherently sequential nature of the algorithm, which limits the reuse of code from molecular dynamics, and necessitates the development of new approaches.  The team's previous efforts have shown that despite the sequential nature of Monte Carlo, graphics processors (GPU) and multi-core CPUs can be used to yield significant reductions in wall-clock time required for a given calculation compared to a traditional serial CPU Monte Carlo code.  This effort led to the creation of the open-source Monte Carlo simulation engine GPU Optimized Monte Carlo (GOMC).  This work will add significant functional and computational enhancements to be added to GOMC.  These enhancements include: (1) support for polarizable force fields based on the Drude oscillator and AMOEBA models, (2) advanced configurational bias moves, such as concerted rotation, double bridging, and aggregation-volume bias (3) multi-molecule moves (4) hybrid Monte Carlo/molecular dynamics simulations (5) new optimizations for multi-core and GPU architectures. The project will enable the simulation of large systems (>100,000 atoms) at constant chemical potential, providing insight into a broad array of problems such as polymer, lipid and ionic liquid phase behavior, molecular self-assembly, the stabilization of nano and micro particle dispersions for drug delivery, and membrane fusion under physiologically relevant conditions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Potoff",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey J Potoff",
   "pi_email_addr": "jpotoff@wayne.edu",
   "nsf_id": "000319533",
   "pi_start_date": "2017-04-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Loren",
   "pi_last_name": "Schwiebert",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Loren J Schwiebert",
   "pi_email_addr": "loren@wayne.edu",
   "nsf_id": "000310239",
   "pi_start_date": "2017-04-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Wayne State University",
  "inst_street_address": "5700 CASS AVE STE 4900",
  "inst_street_address_2": "",
  "inst_city_name": "DETROIT",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "3135772424",
  "inst_zip_code": "482023692",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "MI13",
  "org_lgl_bus_name": "WAYNE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M6K6NTJ2MNE5"
 },
 "perf_inst": {
  "perf_inst_name": "Wayne State University",
  "perf_str_addr": "5050 Anthony Wayne Dr",
  "perf_city_name": "Detroit",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "482023902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "MI13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "140300",
   "pgm_ele_name": "Proc Sys, Reac Eng & Mol Therm"
  },
  {
   "pgm_ele_code": "171200",
   "pgm_ele_name": "DMR SHORT TERM SUPPORT"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7237",
   "pgm_ref_txt": "NANO NON-SOLIC SCI & ENG AWD"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7569",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE/SCIENCE"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  },
  {
   "pgm_ref_code": "8005",
   "pgm_ref_txt": "Scientific Software Elements"
  },
  {
   "pgm_ref_code": "8249",
   "pgm_ref_txt": "Sustainable Materials"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 499886.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The use of molecular simulation to study complex physical phenomena at the atomic level has grown exponentially over the last decade with increasing CPU power and the development of parallel molecular dynamics codes that scale efficiently over thousands of processors.&nbsp; While molecular dynamics codes that utilize parallel computation on CPUs and GPUs are relatively well developed, the development of Monte Carlo codes have lagged that of molecular dynamics, both in terms of parallel code development and features necessary for the simulation of state of the art models describing interactions between molecules.&nbsp; Despite these issues, the Monte Carlo method continues to see widespread use in science and engineering, and is the method of choice for simulations that require the use of an open system (changing number of molecules).</p>\n<p>Parallelization of Monte Carlo is complicated by the inherently sequential nature of the algorithm, which limits the reuse of code from molecular dynamics, and necessitates the development of new approaches.&nbsp; Despite the sequential nature of Monte Carlo, our work has shown that graphics processors (GPU) and multicore CPUs can be used to yield significant reductions in wall-clock time required for a given calculation compared to a traditional serial CPU Monte Carlo code.&nbsp;</p>\n<p>This effort supported the continued development of our open-source software: GPU Optimized Monte Carlo (GOMC).&nbsp; GOMC supports simulations in a variety of ensembles, which include canonical, isothermal-isobaric, grand canonical, and Gibbs ensemble. GOMC can be used to study vapor-liquid equilibria, adsorption in porous materials, surfactant self-assembly, and condensed phase structure for complex molecules. GOMC supports a variety of all-atom, united atom, and coarse grained force fields such as OPLS, TraPPE, Mie, and Martini.</p>\n<p>New Monte Carlo sampling algorithms were developed, such as Molecular Exchange Monte Carlo (MEMC) and Non-Equilibrium Molecular Transfer Monte Carlo (NEMTMC). &nbsp;These algorithms improve sampling efficiency for molecule transfer moves by 1-2 orders of magnitude over naive approaches.&nbsp; Additional sampling algorithms were added to GOMC, including configurational-bias moves for molecules that contain rings, crankshaft moves, multi-particle displacement and rotation moves, using either force/torque bias, or sampling using Brownian dynamics.&nbsp; Substantial effort was expended on the optimization of the performance of multi-particle moves on GPUs.&nbsp; The current version of GOMC uses GPUs for acceleration of expensive energy calculations.&nbsp; Ongoing work is focused on the development of a GPU-resident version of GOMC, which is expected to produce substantial performance gains.</p>\n<p>A number of new features were added to GOMC, including the ability to simulate systems that contain proteins, the ability to read and write trajectory files that are compatible with the molecular dynamics simulation engine, NAMD, and support for free energy calculations.&nbsp;</p>\n<p>To date, this project has produced seven peer reviewed publications, with additional papers in development.&nbsp; Eight new versions of GOMC have been released, with version 2.75a being the most recent. Three PhD students were trained in Monte Carlo algorithms,&nbsp;high performance computing, and software development.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/09/2022<br>\n\t\t\t\t\tModified by: Jeffrey&nbsp;J&nbsp;Potoff</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe use of molecular simulation to study complex physical phenomena at the atomic level has grown exponentially over the last decade with increasing CPU power and the development of parallel molecular dynamics codes that scale efficiently over thousands of processors.  While molecular dynamics codes that utilize parallel computation on CPUs and GPUs are relatively well developed, the development of Monte Carlo codes have lagged that of molecular dynamics, both in terms of parallel code development and features necessary for the simulation of state of the art models describing interactions between molecules.  Despite these issues, the Monte Carlo method continues to see widespread use in science and engineering, and is the method of choice for simulations that require the use of an open system (changing number of molecules).\n\nParallelization of Monte Carlo is complicated by the inherently sequential nature of the algorithm, which limits the reuse of code from molecular dynamics, and necessitates the development of new approaches.  Despite the sequential nature of Monte Carlo, our work has shown that graphics processors (GPU) and multicore CPUs can be used to yield significant reductions in wall-clock time required for a given calculation compared to a traditional serial CPU Monte Carlo code. \n\nThis effort supported the continued development of our open-source software: GPU Optimized Monte Carlo (GOMC).  GOMC supports simulations in a variety of ensembles, which include canonical, isothermal-isobaric, grand canonical, and Gibbs ensemble. GOMC can be used to study vapor-liquid equilibria, adsorption in porous materials, surfactant self-assembly, and condensed phase structure for complex molecules. GOMC supports a variety of all-atom, united atom, and coarse grained force fields such as OPLS, TraPPE, Mie, and Martini.\n\nNew Monte Carlo sampling algorithms were developed, such as Molecular Exchange Monte Carlo (MEMC) and Non-Equilibrium Molecular Transfer Monte Carlo (NEMTMC).  These algorithms improve sampling efficiency for molecule transfer moves by 1-2 orders of magnitude over naive approaches.  Additional sampling algorithms were added to GOMC, including configurational-bias moves for molecules that contain rings, crankshaft moves, multi-particle displacement and rotation moves, using either force/torque bias, or sampling using Brownian dynamics.  Substantial effort was expended on the optimization of the performance of multi-particle moves on GPUs.  The current version of GOMC uses GPUs for acceleration of expensive energy calculations.  Ongoing work is focused on the development of a GPU-resident version of GOMC, which is expected to produce substantial performance gains.\n\nA number of new features were added to GOMC, including the ability to simulate systems that contain proteins, the ability to read and write trajectory files that are compatible with the molecular dynamics simulation engine, NAMD, and support for free energy calculations. \n\nTo date, this project has produced seven peer reviewed publications, with additional papers in development.  Eight new versions of GOMC have been released, with version 2.75a being the most recent. Three PhD students were trained in Monte Carlo algorithms, high performance computing, and software development. \n\n \n\n\t\t\t\t\tLast Modified: 08/09/2022\n\n\t\t\t\t\tSubmitted by: Jeffrey J Potoff"
 }
}