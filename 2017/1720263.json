{
 "awd_id": "1720263",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "I-Corps: Customizable and scalable high-performance microprocessor",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Steven Konsek",
 "awd_eff_date": "2017-01-15",
 "awd_exp_date": "2017-12-31",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2017-01-09",
 "awd_max_amd_letter_date": "2017-01-09",
 "awd_abstract_narration": "The broader impact/commercial potential of this I-Corps project is to enable superior performance and energy efficiency for cost-sensitive embedded processors.  As computing devices have become ubiquitous and permeated every aspect of life, the need for greater performance has also followed.  However, traditional design techniques to enable high performance microprocessors incur resource costs and typically suffer from poor energy efficiency.  These overheads come at a considerable cost as most embedded designs are energy constrained and highly cost sensitive.  The byproduct of these competing design constraints has been stagnating progress due to the ineffectiveness of traditional techniques. Through the use of this project's novel processor microarchitecture, commercial designs should observe approximately 30% greater energy efficiency and superior performance without incurring resource overhead. Applications such as networking and storage domains are initial candidates for adopting the design.\r\n\r\nThis I-Corps project focuses on the creation of an energy-efficient microcontroller through the use of a novel processor microarchitecture.  At the time most fundamental microprocessor techniques were developed, power was not a problem and transistors were scarce.  To satisfy these constraints, designs focused on maximizing utilization of the few execution resources present.  As transistors became more plentiful and performance of greater importance, designs utilized these additional transistors to enable complex organizational and scheduling techniques to maximize the operation of the scarce execution resources.  This project's architecture reverses this trend by allocating additional transistors to execution resources and greatly simplifying organizational and scheduling overheads.  Initial research suggests that considerable energy and area efficiency gains are possible while maintaining a high level of performance in comparison to current commercial designs. Additionally, the novel structure of this microprocessor enables many optimizations that are impractical in conventional designs due to their complex organization and scheduling.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mikko",
   "pi_last_name": "Lipasti",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Mikko H Lipasti",
   "pi_email_addr": "mikko@engr.wisc.edu",
   "nsf_id": "000290802",
   "pi_start_date": "2017-01-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "21 North Park St Suite 6401",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537151218",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802300",
   "pgm_ele_name": "I-Corps"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- \t\t@page { margin: 0.79in } \t\tp { margin-bottom: 0.1in; direction: ltr; line-height: 120%; text-align: left; orphans: 2; widows: 2 } \t -->\n<p>Machine learning algorithms, including neural networks, have proven useful in solving problems from a broad range of applications such as image recognition, machine translation, and targeted marketing. Due to this widespread success, deep neural networks are being used to address increasingly computationally complex and data intensive tasks. The scale of these tasks is now pushing the computational abilities and memory requirements of modern systems.&nbsp; To solve these increasingly large problems, the use of sparse data representations has emerged. Sparse representations exploit high frequencies of zeros within datasets to drastically reduce the memory footprint and operations required for many operations.&nbsp; However, sparse representations suffer from a lack of regularity in data access patterns and program control flow, resulting in poor compute efficiency on both modern microprocessors and graphic processors.&nbsp; Similarly, alternative machine learning algorithms, including random forests and other tree-based approaches, suffer from intense and unpredictable control flow. The Revolution Architecture represents a novel way to construct a general-purpose microprocessor that excels on irregular and heavily-threaded sparse machine learning applications in both mobile and server environments.</p>\n<p><br />As part of the NSF I-Corps program, the team conducted 100 customer interviews to evaluate and understand the customer needs and commercial potential of a microprocessor architecture that excels at irregular and heavily-multithreaded compute.&nbsp; Furthermore, since the conclusion of the NSF I-Corps program, Revolution Computing has continued interactions and interviews with senior management personnel and engineers at Intel, Advanced Micro Devices, Tesla Motors, and Samsung Semiconductor.&nbsp; Through these discussions, the desire for a commercially licensable microprocessor that excels at emerging sparse machine learning was revealed.&nbsp; Customers highlighted that current microprocessors, although generally more efficient than graphics processors at such workloads, suffered from great inefficiency due to the irregular computation of such workloads.&nbsp; These needs span a spectrum of power constrained devices ranging from next generation mobile phones to training neural networks at the datacenter level.</p>\n<p>Sparse machine learning is a prominent area of industrial and academic research with many new algorithms and customized accelerators recently being proposed.&nbsp; Revolution Computing&rsquo;s value proposition is the ability to use a new general purpose microprocessor design to accelerate these applications by over three times and at twice the energy efficiency within an equivalent silicon area of conventional processor designs.&nbsp; Unlike dedicated accelerators, the Revolution Architecture runs the commercial RISC-V instruction set architecture giving it the ability to execute conventional programs and readily adapt as machine learning algorithms evolve.&nbsp; This general-purpose design greatly mitigates customer risk by allowing large algorithmic changes not possible with dedicated accelerators that only represent the current state of knowledge in the rapidly progressing field of machine learning.</p>\n<p>Although modern microprocessors benefit from decades of optimization, their fundamental structures were defined in a time when transistors were expensive and energy efficiency was not a primary design constraint. The Revolution Architecture reinvents the microprocessor by relying on a principle of in-place execution. Rather than delivering instructions and operands to a small set of heavily-pipelined execution units and constantly evaluating complex scheduling logic, instructions retain simple dedicated execution resources and simply wait for their operands to arrive.&nbsp; The use of in-place execution removes the need for commonly employed register renaming techniques, allowing dedicated decoded instruction caches to be placed directly alongside execution units.&nbsp; For codes like sparse machine learning with few, but unpredictable control flow paths these caches enable greater energy efficiency, the ability to service the instruction bandwidth required by many threads, and single-cycle branch misprediction recovery.</p>\n<p>The use of machine learning to automate tasks and solve complicated problems is spreading rapidly to business and consumer applications.&nbsp; However, for these applications to scale and address larger problems, more efficient means must be found to accelerate machine learning as the performance of modern compute is not increasing as it once was.&nbsp; The proposed architecture will enable these larger problems to be efficiently solved using fewer resources, resulting in energy and monetary savings.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/02/2018<br>\n\t\t\t\t\tModified by: Mikko&nbsp;H&nbsp;Lipasti</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nMachine learning algorithms, including neural networks, have proven useful in solving problems from a broad range of applications such as image recognition, machine translation, and targeted marketing. Due to this widespread success, deep neural networks are being used to address increasingly computationally complex and data intensive tasks. The scale of these tasks is now pushing the computational abilities and memory requirements of modern systems.  To solve these increasingly large problems, the use of sparse data representations has emerged. Sparse representations exploit high frequencies of zeros within datasets to drastically reduce the memory footprint and operations required for many operations.  However, sparse representations suffer from a lack of regularity in data access patterns and program control flow, resulting in poor compute efficiency on both modern microprocessors and graphic processors.  Similarly, alternative machine learning algorithms, including random forests and other tree-based approaches, suffer from intense and unpredictable control flow. The Revolution Architecture represents a novel way to construct a general-purpose microprocessor that excels on irregular and heavily-threaded sparse machine learning applications in both mobile and server environments.\n\n\nAs part of the NSF I-Corps program, the team conducted 100 customer interviews to evaluate and understand the customer needs and commercial potential of a microprocessor architecture that excels at irregular and heavily-multithreaded compute.  Furthermore, since the conclusion of the NSF I-Corps program, Revolution Computing has continued interactions and interviews with senior management personnel and engineers at Intel, Advanced Micro Devices, Tesla Motors, and Samsung Semiconductor.  Through these discussions, the desire for a commercially licensable microprocessor that excels at emerging sparse machine learning was revealed.  Customers highlighted that current microprocessors, although generally more efficient than graphics processors at such workloads, suffered from great inefficiency due to the irregular computation of such workloads.  These needs span a spectrum of power constrained devices ranging from next generation mobile phones to training neural networks at the datacenter level.\n\nSparse machine learning is a prominent area of industrial and academic research with many new algorithms and customized accelerators recently being proposed.  Revolution Computing?s value proposition is the ability to use a new general purpose microprocessor design to accelerate these applications by over three times and at twice the energy efficiency within an equivalent silicon area of conventional processor designs.  Unlike dedicated accelerators, the Revolution Architecture runs the commercial RISC-V instruction set architecture giving it the ability to execute conventional programs and readily adapt as machine learning algorithms evolve.  This general-purpose design greatly mitigates customer risk by allowing large algorithmic changes not possible with dedicated accelerators that only represent the current state of knowledge in the rapidly progressing field of machine learning.\n\nAlthough modern microprocessors benefit from decades of optimization, their fundamental structures were defined in a time when transistors were expensive and energy efficiency was not a primary design constraint. The Revolution Architecture reinvents the microprocessor by relying on a principle of in-place execution. Rather than delivering instructions and operands to a small set of heavily-pipelined execution units and constantly evaluating complex scheduling logic, instructions retain simple dedicated execution resources and simply wait for their operands to arrive.  The use of in-place execution removes the need for commonly employed register renaming techniques, allowing dedicated decoded instruction caches to be placed directly alongside execution units.  For codes like sparse machine learning with few, but unpredictable control flow paths these caches enable greater energy efficiency, the ability to service the instruction bandwidth required by many threads, and single-cycle branch misprediction recovery.\n\nThe use of machine learning to automate tasks and solve complicated problems is spreading rapidly to business and consumer applications.  However, for these applications to scale and address larger problems, more efficient means must be found to accelerate machine learning as the performance of modern compute is not increasing as it once was.  The proposed architecture will enable these larger problems to be efficiently solved using fewer resources, resulting in energy and monetary savings.\n\n\t\t\t\t\tLast Modified: 04/02/2018\n\n\t\t\t\t\tSubmitted by: Mikko H Lipasti"
 }
}