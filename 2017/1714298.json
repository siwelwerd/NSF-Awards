{
 "awd_id": "1714298",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Fellowship Award",
 "awd_titl_txt": "Decision Making in Autonomous Machines",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Josie S. Welkom",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 138000.0,
 "awd_amount": 138000.0,
 "awd_min_amd_letter_date": "2017-07-06",
 "awd_max_amd_letter_date": "2017-07-06",
 "awd_abstract_narration": "This award was provided as part of NSF's Social, Behavioral and Economic Sciences Postdoctoral Research Fellowships (SPRF) program. The goal of the SPRF program is to prepare promising, early career doctoral-level scientists for scientific careers in academia, industry or private sector, and government. SPRF awards involve two years of training under the sponsorship of established scientists and encourage Postdoctoral Fellows to perform independent research. NSF seeks to promote the participation of scientists from all segments of the scientific community, including those from underrepresented groups, in its research programs and activities; the postdoctoral period is considered to be an important level of professional development in attaining this goal. Each Postdoctoral Fellow must address important scientific questions that advance their respective disciplinary fields. The Directorate of Social, Behavioral and Economic Sciences offers postdoctoral research fellowships to provide opportunities for recent doctoral graduates to obtain additional training, to gain research experience under the sponsorship of established scientists, and to broaden their scientific horizons beyond their undergraduate and graduate training. This postdoctoral fellowship award supports a rising scholar in the field of psychology investigating how people perceive the morality of autonomous machines (AM), and whether people are willing to delegate moral responsibility to AM. For thousands of years, morality was believed to be unique to humans. However, the rise of AM challenges this uniqueness. As autonomous machines become more sophisticated, they are able to make decisions with moral importance in medicine, military and driving. This research investigates whether people are willing to extend moral agency to machines and allow them to make decisions when lives are at stake. This research advances social science to keep pace with technological advancement, investigates the roots of moral judgment, and also helps reveal whether morality can be shared amongst humans and autonomous machines.\r\n\r\nThis research project has three objectives. The first objective is assessing the perceived moral status of autonomous machines. Are robots seen as morally responsible in the same way as people, and if not, in what ways are they morally deficient? The second objective is assessing the role of perceived mind in the moral status of autonomous machines. What kind of minds are autonomous robots seen to have, and does this mind determine their moral status? This objective is relevant for human agents as well as for autonomous machines. Doing so, we investigate for the first time which mental capacities people want moral agents to have. The third objective is assessing whether acceptance of robots as moral deciders depends on their advantage as deciders and on whether or not they are fully autonomous. The research investigates these objectives, using both large-scale surveys in which participants rate their reaction to humans and AM making moral decisions, and laboratory studies providing further validation and using measures other than self-report. Finally, the research investigates how professionals, such as medical practitioners, respond to the possibility of AM making some of the hard decisions they usually make.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yochanan",
   "pi_last_name": "Bigman",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Yochanan E Bigman",
   "pi_email_addr": "",
   "nsf_id": "000736095",
   "pi_start_date": "2017-07-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kurt",
   "pi_last_name": "Gray",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kurt Gray",
   "pi_email_addr": "",
   "nsf_id": "000686935",
   "pi_start_date": "2017-07-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Bigman                  Yochanan       E",
  "inst_street_address": "",
  "inst_street_address_2": "",
  "inst_city_name": "Jerusalem",
  "inst_state_code": "",
  "inst_state_name": "RI REQUIRED",
  "inst_phone_num": "",
  "inst_zip_code": "",
  "inst_country_name": "Israel",
  "cong_dist_code": "",
  "st_cong_dist_code": "",
  "org_lgl_bus_name": "",
  "org_prnt_uei_num": "",
  "org_uei_num": ""
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Chapel Hill",
  "perf_str_addr": null,
  "perf_city_name": "Chapel Hill",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275993270",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "040Y00",
   "pgm_ele_name": "(SPRF-FR) SBE Postdoctoral Res"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 138000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Humans are increasing sharing the road with autonomous vehicles, sharing hospital wards with autonomous surgery robots, and making joint decisions with autonomous algorithms. As we adapt to the increasing presence of automated machines in our society, it is important to understand how people make sense of robots making moral decisions.</p>\n<p>&nbsp;</p>\n<p>In this project we examined how people make sense of autonomous machines making moral decisions and the involved psychological processes. Our studies show that people generally do not want machines to make moral decisions. Examining various scenarios such as driving decisions, parole decisions, medical decisions and military decisions, we found that people prefer humans over autonomous machines to be the ones making moral decisions. We also investigated why people are averse to machines making moral decisions. We found that people believe that making moral decision requires certain mental capacities. Specifically, the ability to think rationally and the ability to experience emotions. Because autonomous machines lack these mental capacities people are against them making moral decisions.</p>\n<p>&nbsp;</p>\n<p>These studies led us to create a theoretical framework to better understand when people will see robots as morally responsible. According to our framework, the more robots are seen as aware of the situation, the more their behavior seems intentional, the more they look like humans and the more they are seen as having free will, the more people will see them as morally responsible.</p>\n<p>Our research on machine morality possesses both clear intellectual merit and broader impacts. In terms of intellectual merit, our research answers two important and related theoretical questions. The first is -&nbsp;Do people want machines to make moral decisions? We revealed that the answer is no, people do not want autonomous machines to make moral decisions. The second is - What mental abilities do people see as required from those making moral decisions? We find that people think that certain mental capacities, specifically thinking and feeling are required for moral decision making. This provides insight into the very nature of morality and the importance of emotions in moral decision making.</p>\n<p>This research has obvious broader impacts. Understanding how people want autonomous machines to be integrated into human society can inform policy makers and regulators. For example, people&rsquo;s aversion from algorithms making parole decisions suggests that it might make sense to require those decisions to be made by people. Understanding the psychology of how people make sense of machines making moral decisions can shape the way we as a society harness this new and powerful technology to improve our lives.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/13/2019<br>\n\t\t\t\t\tModified by: Yochanan&nbsp;E&nbsp;Bigman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHumans are increasing sharing the road with autonomous vehicles, sharing hospital wards with autonomous surgery robots, and making joint decisions with autonomous algorithms. As we adapt to the increasing presence of automated machines in our society, it is important to understand how people make sense of robots making moral decisions.\n\n \n\nIn this project we examined how people make sense of autonomous machines making moral decisions and the involved psychological processes. Our studies show that people generally do not want machines to make moral decisions. Examining various scenarios such as driving decisions, parole decisions, medical decisions and military decisions, we found that people prefer humans over autonomous machines to be the ones making moral decisions. We also investigated why people are averse to machines making moral decisions. We found that people believe that making moral decision requires certain mental capacities. Specifically, the ability to think rationally and the ability to experience emotions. Because autonomous machines lack these mental capacities people are against them making moral decisions.\n\n \n\nThese studies led us to create a theoretical framework to better understand when people will see robots as morally responsible. According to our framework, the more robots are seen as aware of the situation, the more their behavior seems intentional, the more they look like humans and the more they are seen as having free will, the more people will see them as morally responsible.\n\nOur research on machine morality possesses both clear intellectual merit and broader impacts. In terms of intellectual merit, our research answers two important and related theoretical questions. The first is - Do people want machines to make moral decisions? We revealed that the answer is no, people do not want autonomous machines to make moral decisions. The second is - What mental abilities do people see as required from those making moral decisions? We find that people think that certain mental capacities, specifically thinking and feeling are required for moral decision making. This provides insight into the very nature of morality and the importance of emotions in moral decision making.\n\nThis research has obvious broader impacts. Understanding how people want autonomous machines to be integrated into human society can inform policy makers and regulators. For example, people\u2019s aversion from algorithms making parole decisions suggests that it might make sense to require those decisions to be made by people. Understanding the psychology of how people make sense of machines making moral decisions can shape the way we as a society harness this new and powerful technology to improve our lives.\n\n\t\t\t\t\tLast Modified: 12/13/2019\n\n\t\t\t\t\tSubmitted by: Yochanan E Bigman"
 }
}