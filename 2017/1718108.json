{
 "awd_id": "1718108",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Towards a Formal Theory of Blameworthiness, Intention, and Moral Responsibility",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Roger Mailler",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 426998.0,
 "awd_amount": 426998.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2017-07-27",
 "awd_abstract_narration": "As we move to an era of self-driving cars, autonomous drones, and robots helping people in work and everyday tasks, there is an increasing need to develop a theory of moral or ethical behavior.  Policy-makers are already grappling with some of the issues.  Jurisdictions around the world are beginning to formulate highway codes that take into consideration the presence of driverless cars.  The goal of this project is to provide the foundations for a theory of blameworthiness, intention, and moral responsibility for such agents and applications.  The study will evaluate this theory against the moral judgments that people make, and take the first steps to applying resulting policies in practice.  \r\n  \r\nA good theory will have to take into consideration tradeoffs among many competing and sometimes conflicting goals.  This suggests that probability and utility will be involved.  The theory will also have to deal with counterfactuals--that is, reasoning about what would have happened had circumstances or actions differed.  Issues of causality will reveal whether an agent's action was responsible for an outcome, or if the action was coincidental to the outcome.  Yet another relevant issue is determining intent: that is, determining whether an agent intended an outcome when performing an act. Pearl's structural-equations framework can model counterfactuals (interventions) and has been used as the basis of a definition of causality.  In this project, the framework will be extended to capture degree of blameworthiness and intention.  These definitions are given relative to an agent's epistemic state, which describes the agent's beliefs and preferences.  Initially, the beliefs will be specified by a probability measure and the preferences by a utility function.  The framework will then be extended to consider more qualitative representations of beliefs and preferences, to take into account issues of normality and typicality (e.g., to capture societal conventions), and to settings with multiple agents (e.g., multiple communicating driverless cars or robots), where game-theoretic concerns and issues of group responsibility vs. individual responsibility arise.  The definitions developed under this framework will be tested to see if they capture people's intuitions about these issues, and  will be applied to situations of practical interest.   The theory will then be refined to take into account the outcomes of these experiments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Joseph",
   "pi_last_name": "Halpern",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "Joseph Y Halpern",
   "pi_email_addr": "halpern@cs.cornell.edu",
   "nsf_id": "000107830",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "107 Hoy Road",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148507501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 426998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The main goal of this project was to&nbsp;provide the foundations for a theory of blameworthiness, intention,&nbsp;</span><span>and moral responsibility.&nbsp; Preliminary work on these topics had been done by Joe Halpern, the PI, and Max Kleiman-Weiner, providing definitions for the extent to which an individual was blameworthy for an outcome (e.g., to what extent was the autonomus vehicle's action to blame for the accident) and for whether an agent intended a particular outcome o when they performed an action a, using <em>causal models.</em>&nbsp; They focused on the single-agent case.&nbsp; But in many cases of interest, we have groups of agents, who together bring about an outcome.&nbsp; For example, if the fish in a lake die out due to overfishing by the villagers in a village, to what extent is each individual in the village to blame for the outcome?&nbsp; Halpern and his Ph.D. student Meir Friedenberg gave definitions that first assigned a degree of blameworthiness to each subgroup, taking the difficulty of coordination into account, and then assigned a degree of blameworthiness to individuals in a way that satisfied certain natural axiomatic properties, for which the only solution is the well-known <em>Shapley value from economics.&nbsp;&nbsp;</em></span></p>\n<p>Friedenberg and Halpern are currently working on cleaning up some problems in the Halpern-Kleiman-Weiner definition of intention, with the ultimate goal of extending it to dynamic settings of the type that come up in planning, and to multi-agent scenarios.</p>\n<p>In a different line of work, Halpern, together with Sander Beckers (his former postdoc), and Frederick Eberhardt, considered a notion of <em>abstraction </em>in causal models.&nbsp; We can and typically do analyze problems at different levels of abstraction.&nbsp;&nbsp;For example, we can try to understand human behavior by thinking at the level of neurons firing in the brain or at the level of beliefs,desires, and intentions.&nbsp; But what does it mean for a high-level causal model to be an abstraction (or approximate abstraction) of a low-level causal model?&nbsp; Intuitively, it should mean that for every result of an intervention in the high-level model, there shoiuld be a corresponding result in the low-level model.&nbsp; However, the formal definitions are surprisingly subtle.&nbsp;&nbsp;</p>\n<p>This is related to the work on blameworthiness and intention, because it suggests a mechanism whereby humans can determine blameworthiness and intention even in large complicated models: they find a much smaller abstraction of the model, and do the determination there.</p>\n<p>Clearly issues of blameworthiness, intention, and moral responsibility are of widespread, not just in computer science, but in philosophy, law, psychology, and economics.&nbsp; &nbsp;This work was widely disseminated&nbsp; via publications (so far, in computer science and psychology), and in public lectures by the PI.&nbsp; In addition, the PI organized and moderated a session on Ethical Issues in AI at the 2020 Annual Meeting of the AAAS.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><span><em><br /></em></span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/16/2021<br>\n\t\t\t\t\tModified by: Joseph&nbsp;Y&nbsp;Halpern</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe main goal of this project was to provide the foundations for a theory of blameworthiness, intention, and moral responsibility.  Preliminary work on these topics had been done by Joe Halpern, the PI, and Max Kleiman-Weiner, providing definitions for the extent to which an individual was blameworthy for an outcome (e.g., to what extent was the autonomus vehicle's action to blame for the accident) and for whether an agent intended a particular outcome o when they performed an action a, using causal models.  They focused on the single-agent case.  But in many cases of interest, we have groups of agents, who together bring about an outcome.  For example, if the fish in a lake die out due to overfishing by the villagers in a village, to what extent is each individual in the village to blame for the outcome?  Halpern and his Ph.D. student Meir Friedenberg gave definitions that first assigned a degree of blameworthiness to each subgroup, taking the difficulty of coordination into account, and then assigned a degree of blameworthiness to individuals in a way that satisfied certain natural axiomatic properties, for which the only solution is the well-known Shapley value from economics.  \n\nFriedenberg and Halpern are currently working on cleaning up some problems in the Halpern-Kleiman-Weiner definition of intention, with the ultimate goal of extending it to dynamic settings of the type that come up in planning, and to multi-agent scenarios.\n\nIn a different line of work, Halpern, together with Sander Beckers (his former postdoc), and Frederick Eberhardt, considered a notion of abstraction in causal models.  We can and typically do analyze problems at different levels of abstraction.  For example, we can try to understand human behavior by thinking at the level of neurons firing in the brain or at the level of beliefs,desires, and intentions.  But what does it mean for a high-level causal model to be an abstraction (or approximate abstraction) of a low-level causal model?  Intuitively, it should mean that for every result of an intervention in the high-level model, there shoiuld be a corresponding result in the low-level model.  However, the formal definitions are surprisingly subtle.  \n\nThis is related to the work on blameworthiness and intention, because it suggests a mechanism whereby humans can determine blameworthiness and intention even in large complicated models: they find a much smaller abstraction of the model, and do the determination there.\n\nClearly issues of blameworthiness, intention, and moral responsibility are of widespread, not just in computer science, but in philosophy, law, psychology, and economics.   This work was widely disseminated  via publications (so far, in computer science and psychology), and in public lectures by the PI.  In addition, the PI organized and moderated a session on Ethical Issues in AI at the 2020 Annual Meeting of the AAAS.\n\n \n\n \n\n \n\n\n\n\n\t\t\t\t\tLast Modified: 08/16/2021\n\n\t\t\t\t\tSubmitted by: Joseph Y Halpern"
 }
}