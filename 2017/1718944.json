{
 "awd_id": "1718944",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Integrative, Semantic-Aware, Speech-Driven Models for Believable Conversational Agents with Meaningful Behaviors",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 494116.0,
 "awd_amount": 526116.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2021-05-19",
 "awd_abstract_narration": "This project will analyze, model and synthesize human behaviors to create a believable Conversational Agent (CA). A CA is a virtual agent that interacts with a user, displaying human-like behaviors not only through speech but also through facial expressions and head movements. Replicating or representing human behavior includes generating gestures that are synchronized with speech, convey appropriate meaning in the message, and respond to the behaviors displayed by the user. An appealing approach to synthesize human-like behaviors is the use of data-driven methods, which have the potential of capturing naturalistic variations of the behaviors. Modeling the dependencies between speech and gestures brings insights about verbal and nonverbal communication, underlying the production and coordination mechanisms used during natural human interactions. CAs can be used in a variety of health care applications, such as helping hearing impaired individuals and teaching social skills to autistic children. Tutoring systems that display human-like behaviors to communicate and acknowledge active listening will engage better with the students, helping them in their learning. The project promises a fertile ground for interdisciplinary training of graduate and undergraduate students. The models will be evaluated with an assistive agent (CA or embodied robot) interacting with UT Dallas students, serving as a platform to reach out students from all majors, especially woman and underrepresented minorities.\r\n\r\nThe project will take an integrative, cross-disciplinary approach to generate believable and meaningful behaviors by exploring the intrinsic relation between speech, head motion, and facial expressions, constrained by important aspects of spoken language. The planned research leverages some of the latest developments in the field of deep learning in an integrative fashion, pulling together acoustic features and semantic language structure, to build models that are able to account for the correlation between various facial and head movements. The speech-driven approach will capture the variability of human behavior in a manner that is not easily possible with rule-based approaches. Dialog acts and emotions will be inferred and used to constrain the speech driven models, capturing the relation between high-level conversational functions and facial gestures. The project will offer novel, principled methods to generate behaviors driven by synthesized speech, opening new application domain when only text is available. The approach will capture the acoustic variability in synthesized speech, while maintaining the temporal dependency between gestures and speech. The project will also explore schemes to modify the behaviors of the user by displaying carefully designed gestures generated with our data-driven framework. By tracking the behaviors of the user, the system will provide appropriate responses, closing the loop in the interaction.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Carlos",
   "pi_last_name": "Busso",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Carlos Busso",
   "pi_email_addr": "cbusso@andrew.cmu.edu",
   "nsf_id": "000544291",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Dallas",
  "inst_street_address": "800 WEST CAMPBELL RD.",
  "inst_street_address_2": "SP2.25",
  "inst_city_name": "RICHARDSON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9728832313",
  "inst_zip_code": "750803021",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "TX24",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT DALLAS",
  "org_prnt_uei_num": "",
  "org_uei_num": "EJCVPNN1WFS5"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Texas at Dallas",
  "perf_str_addr": "800 W. Campbell Rd.",
  "perf_city_name": "Richardson",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "750803021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "TX24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 494116.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Research Objective and Significance:&nbsp;</p>\n<p>&nbsp;</p>\n<p>This project analyzed, modeled, and synthesized human behaviors to create a believable Conversational Agent (CA). A CA is a virtual agent that interacts with a user, displaying human-like behaviors not only through speech by also through facial expressions and head movements. Replicating human behavior includes generating gestures that are synchronized with speech, convey appropriate meaning in the message, and respond to the behaviors displayed by the user. An appealing approach to synthesize human-like behaviors is the use of data-driven methods, which have the potential of capturing naturalistic variations of the behaviors. CAs can be used in a variety of healthcare applications, such as helping hearing-impaired individuals and teaching social skills to autistic children. Tutoring systems that display human-like behaviors to communicate and acknowledge active listening will engage better with the students, helping them in their learning.&nbsp;</p>\n<p>&nbsp;</p>\n<p>The project took an integrative, cross-disciplinary approach to generate believable and meaningful behaviors by exploring the intrinsic relation between speech, head motion, and facial expressions, constrained by important aspects of spoken language. The speech-driven approach captures the variability of human behavior in a manner that is not easily possible with rule-based approaches. The project explored novel methods to generate behaviors driven by synthesized speech, opening new application domains when only text is available. The approach captures the acoustic variability in synthesized speech, while maintaining the temporal dependency between gestures and speech. The project also explored schemes to modify the behaviors of the user by displaying carefully designed gestures generated with our data-driven framework.</p>\n<p>&nbsp;</p>\n<p>The project&rsquo;s outcomes were published in top venues: 7 journals and 11 conference papers. Eight graduate and nine undergraduate students were involved in this project. Three databases were created: MSP-Face, MSP-Avatar and MSP-Entraiment corpora.</p>\n<p>&nbsp;</p>\n<p>We highlight some of the methods developed during this project.</p>\n<p>&nbsp;</p>\n<p>(a) Generative lip motion</p>\n<p>&nbsp;</p>\n<p>We proposed a conditional generative adversarial network, which learns the relationship between emotion, lexical content and lip movements. The model learns the distribution of the orofacial movements conditioned on speech features. A key feature of the adversarial training is to teach the model to capture the temporal relationship between acoustic features and lip motion by creating fake sequences with mismatched speech and lip motion trajectories. The resulting lip motion sequences capture the temporal coupling between speech and lip movements, creating realistic sequences, which convey the underlying lexical content. The proposed framework can be easily extended to consider emotions.</p>\n<p>&nbsp;</p>\n<p>b) Generating meaningful behaviors</p>\n<p>&nbsp;</p>\n<p>We proposed to bridge the gap between rule-based methods and data-driven approaches overcoming their limitations. The approach builds a dynamic Bayesian network (DBN), where a discrete variable is added to constrain the behaviors on the underlying constraint. By constraining the discourse functions (e.g., questions), the model learns the characteristic behaviors associated with a given discourse class learning the rules from the data. By constraining on prototypical behaviors (e.g., head nods), the approach can be embedded in a rule-based system as a behavior realizer creating trajectories that are timely synchronized with speech.</p>\n<p>&nbsp;</p>\n<p>c) Head-movements driven by synthetic speech</p>\n<p>&nbsp;</p>\n<p>Speech-driven methods for head movements require all the potential utterances of the CA to be recorded, which limits their applications. The use of text-to-speech (TTS) systems provides the flexibility of using text instead of pre-recorded speech. However, simply training speech-driven models with natural speech, and testing them with synthetic speech creates a mismatch affecting the performance of the system. We proposed a novel strategy to solve this mismatch by creating a parallel corpus either with neutral or emotional synthetic speech timely aligned with the original speech for which we have the motion capture recordings. This parallel corpus is used to retrain the models from scratch, or adapt the models originally built with natural speech. We proposed to extract dialog acts directly from the text and use this information to directly constrain our models. The model generates head motion sequences that are closer to the statistical patterns of the original head movements, and are perceived as more natural and appropriate.</p>\n<p>&nbsp;</p>\n<p>(d) Predicting Behaviors in Dyadic Interactions</p>\n<p>&nbsp;</p>\n<p>CA can greatly benefit from a model that predicts future behaviors of the users (e.g., smile, raising the fundamental frequency) by intentionally increasing the entrainment. We proposed a novel autoregressive transformer model that predicts people's behavior during dyadic interactions. It uses causal masks on the attention mechanisms to prevent the model from using future behaviors in the predictions. The first block in the model processes the interaction over the time dimension with attention mechanisms. The weight associated with this model indicates which turns in the conversation are relevant for the prediction of the next behaviors. The second block in the model indicates how the behaviors of the participants are related to each other. The model predicts the behaviors with a precision rate of 65.32%, outperforming the baseline models.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/26/2022<br>\n\t\t\t\t\tModified by: Carlos&nbsp;Busso</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nResearch Objective and Significance: \n\n \n\nThis project analyzed, modeled, and synthesized human behaviors to create a believable Conversational Agent (CA). A CA is a virtual agent that interacts with a user, displaying human-like behaviors not only through speech by also through facial expressions and head movements. Replicating human behavior includes generating gestures that are synchronized with speech, convey appropriate meaning in the message, and respond to the behaviors displayed by the user. An appealing approach to synthesize human-like behaviors is the use of data-driven methods, which have the potential of capturing naturalistic variations of the behaviors. CAs can be used in a variety of healthcare applications, such as helping hearing-impaired individuals and teaching social skills to autistic children. Tutoring systems that display human-like behaviors to communicate and acknowledge active listening will engage better with the students, helping them in their learning. \n\n \n\nThe project took an integrative, cross-disciplinary approach to generate believable and meaningful behaviors by exploring the intrinsic relation between speech, head motion, and facial expressions, constrained by important aspects of spoken language. The speech-driven approach captures the variability of human behavior in a manner that is not easily possible with rule-based approaches. The project explored novel methods to generate behaviors driven by synthesized speech, opening new application domains when only text is available. The approach captures the acoustic variability in synthesized speech, while maintaining the temporal dependency between gestures and speech. The project also explored schemes to modify the behaviors of the user by displaying carefully designed gestures generated with our data-driven framework.\n\n \n\nThe project\u2019s outcomes were published in top venues: 7 journals and 11 conference papers. Eight graduate and nine undergraduate students were involved in this project. Three databases were created: MSP-Face, MSP-Avatar and MSP-Entraiment corpora.\n\n \n\nWe highlight some of the methods developed during this project.\n\n \n\n(a) Generative lip motion\n\n \n\nWe proposed a conditional generative adversarial network, which learns the relationship between emotion, lexical content and lip movements. The model learns the distribution of the orofacial movements conditioned on speech features. A key feature of the adversarial training is to teach the model to capture the temporal relationship between acoustic features and lip motion by creating fake sequences with mismatched speech and lip motion trajectories. The resulting lip motion sequences capture the temporal coupling between speech and lip movements, creating realistic sequences, which convey the underlying lexical content. The proposed framework can be easily extended to consider emotions.\n\n \n\nb) Generating meaningful behaviors\n\n \n\nWe proposed to bridge the gap between rule-based methods and data-driven approaches overcoming their limitations. The approach builds a dynamic Bayesian network (DBN), where a discrete variable is added to constrain the behaviors on the underlying constraint. By constraining the discourse functions (e.g., questions), the model learns the characteristic behaviors associated with a given discourse class learning the rules from the data. By constraining on prototypical behaviors (e.g., head nods), the approach can be embedded in a rule-based system as a behavior realizer creating trajectories that are timely synchronized with speech.\n\n \n\nc) Head-movements driven by synthetic speech\n\n \n\nSpeech-driven methods for head movements require all the potential utterances of the CA to be recorded, which limits their applications. The use of text-to-speech (TTS) systems provides the flexibility of using text instead of pre-recorded speech. However, simply training speech-driven models with natural speech, and testing them with synthetic speech creates a mismatch affecting the performance of the system. We proposed a novel strategy to solve this mismatch by creating a parallel corpus either with neutral or emotional synthetic speech timely aligned with the original speech for which we have the motion capture recordings. This parallel corpus is used to retrain the models from scratch, or adapt the models originally built with natural speech. We proposed to extract dialog acts directly from the text and use this information to directly constrain our models. The model generates head motion sequences that are closer to the statistical patterns of the original head movements, and are perceived as more natural and appropriate.\n\n \n\n(d) Predicting Behaviors in Dyadic Interactions\n\n \n\nCA can greatly benefit from a model that predicts future behaviors of the users (e.g., smile, raising the fundamental frequency) by intentionally increasing the entrainment. We proposed a novel autoregressive transformer model that predicts people's behavior during dyadic interactions. It uses causal masks on the attention mechanisms to prevent the model from using future behaviors in the predictions. The first block in the model processes the interaction over the time dimension with attention mechanisms. The weight associated with this model indicates which turns in the conversation are relevant for the prediction of the next behaviors. The second block in the model indicates how the behaviors of the participants are related to each other. The model predicts the behaviors with a precision rate of 65.32%, outperforming the baseline models.\n\n \n\n\t\t\t\t\tLast Modified: 12/26/2022\n\n\t\t\t\t\tSubmitted by: Carlos Busso"
 }
}