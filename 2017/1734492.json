{
 "awd_id": "1734492",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: INT: COLLAB: Integrated Modeling and Learning for Robust Grasping and Dexterous Manipulation with Adaptive Hands",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 867729.0,
 "awd_amount": 867729.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2017-07-27",
 "awd_abstract_narration": "Robots need to effectively interact with a large variety of objects\r\nthat appear in warehouses and factories as well as homes and offices.\r\nThis requires robust grasping and dexterous manipulation of everyday\r\nobjects through low cost robots and low complexity solutions.\r\nTraditionally, robots use rigid hands and analytical models for such\r\ntasks, which often fail in the presence of even small errors. New\r\ncompliant hands promise improved performance, while minimizing\r\ncomplexity, and increased robustness. Nevertheless, they are\r\ninherently difficult to sense and model. This project combines ideas\r\nfrom different robotics sub-fields to address this limitation. It\r\nutilizes progress in machine learning and builds on a strong tradition\r\nin robot modeling. The objective is to provide adaptive, compliant\r\nrobots that are better in grasping objects in the presence of multiple\r\nunknown contact points and sliding or rolling objects in-hand. The\r\nbroader impact will be strengthened by the open release of new or\r\nmodified robot hand designs, improved control algorithms and software,\r\nas well as corresponding data sets. Furthermore, academic\r\ndissemination will be accompanied by educational outreach to\r\nundergraduate and high school students.\r\n\r\nTowards the above objective, the first step will be the definition of\r\nnew hybrid models appropriate for adaptive, compliant hands.  This\r\nwill happen by improving analytical solutions and extending them to\r\nallow adaptation based on data via novel, time-efficient learning\r\nmethods. The objective is to capture model uncertainty inherent in\r\nreal-world interactions; a process that suffers from data scarcity.\r\nIn order to reduce the amount of data required for learning, different\r\nmodels will be tailored to specific tasks through an automated\r\ndiscovery of these tasks and of underlying motion primitives for each\r\none of them. This task identification process will operate iteratively\r\nwith learning and utilize improved models to discover new tasks. It\r\ncan also provide feedback for improved hand design. Once these\r\nlearning-based and task-focused models are available, they will be\r\nused to learn and synthesize controllers for grasping and in-hand\r\nmanipulation. To learn controllers, this work will consider a\r\nmodel-based, reinforcement learning approach, which will be evaluated\r\nagainst alternatives. For controller synthesis, existing tools for\r\nthis purpose will be integrated with task planning primitives and\r\nextended through learning processes to identify the preconditions\r\nunder which different controllers can be chained together. The project\r\ninvolves extensive evaluation on a variety of novel adaptive hands and\r\nrobotic arms designed in the PIs' labs. Modern vision-based solutions\r\nwill be used to track grasped objects and provide feedback for\r\nlearning and closed-loop control.  The evaluation will measure whether\r\nthe developed hybrid models can significantly improve robustness of\r\ngrasping and the effectiveness of dexterous manipulation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kostas",
   "pi_last_name": "Bekris",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kostas Bekris",
   "pi_email_addr": "kostas.bekris@cs.rutgers.edu",
   "nsf_id": "000520262",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Abdeslam",
   "pi_last_name": "Boularias",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Abdeslam Boularias",
   "pi_email_addr": "abdeslam.boularias@rutgers.edu",
   "nsf_id": "000717794",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "617 Bowser Road",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548019",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 867729.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-4eb4930a-7fff-771d-807c-abe9839219cd\" style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Low-cost underactuated hands are cheap and robust but lack-analytical models. To address this, we showed that stochastic models can be automatically learned from data by identifying dominant, sensible features that express hand state transitions. Experiments showed that using Diffusion Maps with a feature space composed of the object position, actuator angles, and actuator loads, sufficiently expresses the hand-object system configuration and can provide accurate enough predictions. This was the first learned transition model for such hands with this level of predictability. The model generalizes to new objects of varying sizes. It can also identify states on the verge of failure that should be avoided. Figure 1 shows the integration of the model with closed-loop control. This work also provided the first large public dataset of real within-hand manipulation. We also released a ROS-based physics-engine model of such hands for independent data collection, experimentation, and sim-to-reality transfer.</span></p>\n<p><span id=\"docs-internal-guid-c9fb904b-7fff-a70b-e2f2-2f2db9fcfbf1\" style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">While the learned transition models of adaptive hands successfully predict short-term motions, errors accumulate over long horizon predictions.&nbsp; For this reason, the project explored robust, long-term manipulation by maintaining a probability distribution expressing the uncertainty of the learned model and to identify the safest manipulation, i.e., without dropping the object. The approach trains a &ldquo;critic&rdquo; model that estimates the error of the &ldquo;actor&rdquo; transition model. Then a motion planner directs paths to less erroneous regions in the state</span><span id=\"docs-internal-guid-c9fb904b-7fff-a70b-e2f2-2f2db9fcfbf1\" style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> space. Figure 2 highlights experiments with physically simulated and real adaptive hands, which showed increased robustness and reduced data requirements.</span></p>\n<p><span id=\"docs-internal-guid-90d198df-7fff-e0f7-e5f9-c5ac29f82a1a\" style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To empower the manipulation of everyday objects with adaptive hands for critical applications, we developed perception tools that reliably detect and track the pose (position and orientation) of objects under manipulation using image and depth data. Figure 3 illustrates the capabilities of these tools where a significantly occluded and texture-less object is correctly detected. This line of work resulted in multiple open-source software packages and datasets: SE(3)-TrackNet allows the high-speed tracking of object poses given a 3D model, while BundleTrack tracks the relative pose of an object as it is being manipulated even without access to a 3D model.</span></p>\n<p><span id=\"docs-internal-guid-6b15e307-7fff-fe03-88de-9c57fbccc29a\" style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">By integrating these perception capabilities with our Yale collaborator&rsquo;s learned motion primitives for reorienting objects with adaptive hands, we developed a complete system for precision manipulation tasks under tight, industrially-relevant tolerances (&lt; 0.25mm). Contrary to previous efforts, the method did not require expensive force sensors, precision manipulators, or time-consuming, online, data-hungry learning. Instead, it leveraged mechanical compliance, object-agnostic models of the hand learned offline, and our object pose trackers trained solely in simulation. This allowed the system to easily generalize and transfer to new insertion tasks for objects of varying geometries and open-world environments as highlighted in Figure 4.</span></p>\n<p><span id=\"docs-internal-guid-e486bce3-7fff-1d0f-b4e5-40e669477a38\" style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To manipulate deformable, unknown objects, we developed the first, unified, real-time 3D reconstruction system that simultaneously supports semantic fusion, fast motion tracking, non-rigid object deformation, and topological change. The system solves an optimization problem to deal with fast motion. Experiments demonstrated that the system significantly outperforms existing state-of-the-art. We also introduced a new technique that enables open-vocabulary 3D instance retrieval without using any 3D data for training, which can be used by the reconstruction tool. Given a human language query, the method returns a ranked set of 3D object instance segments based on the feature similarity of the instance and the text query, as shown in Figure 5. The process is performed in real-time during fusion of a 3D point cloud and does not require additional training. The source code for both the reconstruction and the instance retrieval were made publicly available.</span></p>\n<p id=\"docs-internal-guid-4eabb89b-7fff-43e5-2a95-eb741090ae7d\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The 3D perception, grasping and manipulation techniques described above were combined to perform complex manipulation tasks involving sequential tool use. We proposed a new approach for teaching a robot to perform complex tasks composed of several, consecutively executed low-level sub-tasks, given as input a few visual human demonstrations. The sub-tasks consist of moving the robot&rsquo;s hand until it reaches a sub-goal, performing an action, and triggering the next sub-task when a pre-condition is met. A key feature is that the policies are learned directly from raw videos of task demonstrations, without any manual annotation or postprocessing of the data. We extended this work to successfully learn category-level manipulation, which acquires skills that can generalize to new objects as illustrated in Figure 6.</span></p><br>\n<p>\n Last Modified: 01/19/2024<br>\nModified by: Kostas&nbsp;Bekris</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705692821574_Screenshot_2024_01_19_at_10.42.24_AM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705692821574_Screenshot_2024_01_19_at_10.42.24_AM--rgov-800width.png\" title=\"2. Two-finger adaptive hands manipulating a cylinder\"><img src=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705692821574_Screenshot_2024_01_19_at_10.42.24_AM--rgov-66x44.png\" alt=\"2. Two-finger adaptive hands manipulating a cylinder\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">(left) The yellow region depicts the x-y projection of the hand\ufffds workspace in Gazebo; (right) a real hand experiment to manipulate a cylindrical object in the interior of the red horseshoe obstacle.</div>\n<div class=\"imageCredit\">Sintov et al. ICRA 2020</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Kostas&nbsp;Bekris\n<div class=\"imageTitle\">2. Two-finger adaptive hands manipulating a cylinder</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705693426541_Screenshot_2024_01_19_at_2.42.12_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705693426541_Screenshot_2024_01_19_at_2.42.12_PM--rgov-800width.png\" title=\"6. Learning from demonstrations\"><img src=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705693426541_Screenshot_2024_01_19_at_2.42.12_PM--rgov-66x44.png\" alt=\"6. Learning from demonstrations\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The system is trained with unlabeled visual demonstrations to pick up a paint-brush in a specific manner, and place it in a specific configuration relative to an object. The robot repeats the demonstrated behavior on a new scene containing novel objects with different features.</div>\n<div class=\"imageCredit\">Liang et al. ICRA 2023</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Kostas&nbsp;Bekris\n<div class=\"imageTitle\">6. Learning from demonstrations</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705692524544_Screenshot_2024_01_19_at_2.26.04_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705692524544_Screenshot_2024_01_19_at_2.26.04_PM--rgov-800width.png\" title=\"1. Manipulation primitives that use the learned transition models\"><img src=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705692524544_Screenshot_2024_01_19_at_2.26.04_PM--rgov-66x44.png\" alt=\"1. Manipulation primitives that use the learned transition models\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">1. (a) Rotating actuators in the same direction with equal velocities. Circular arrows show the counter-clockwise motion. (b) Rotating the actuators in the opposite direction. The actuators? rotation is switched twice from clockwise to counter-clockwise. Resulting trajectories are non-linear.</div>\n<div class=\"imageCredit\">Sintov et al. RAL 2019</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Kostas&nbsp;Bekris\n<div class=\"imageTitle\">1. Manipulation primitives that use the learned transition models</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705693292762_Screenshot_2024_01_19_at_2.38.43_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705693292762_Screenshot_2024_01_19_at_2.38.43_PM--rgov-800width.png\" title=\"5. Open-Vocabulary 3D Instance Retrieval\"><img src=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705693292762_Screenshot_2024_01_19_at_2.38.43_PM--rgov-66x44.png\" alt=\"5. Open-Vocabulary 3D Instance Retrieval\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">5. Segmentation results using the proposed Open-Vocabulary 3D Instance Retrieval method without training on 3D data.</div>\n<div class=\"imageCredit\">Lu et al. CoRL 2023</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Kostas&nbsp;Bekris\n<div class=\"imageTitle\">5. Open-Vocabulary 3D Instance Retrieval</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705692931835_Screenshot_2024_01_19_at_11.06.01_AM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705692931835_Screenshot_2024_01_19_at_11.06.01_AM--rgov-800width.png\" title=\"3. Pose estimation of objects occluded by adaptive hand\"><img src=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705692931835_Screenshot_2024_01_19_at_11.06.01_AM--rgov-66x44.png\" alt=\"3. Pose estimation of objects occluded by adaptive hand\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">3. (left) original image showing the adaptive hand grasping a severely occluded texture-less object. (middle) Point-cloud data available as input. (right) Scene reconstruction given the detected position and orientation of the object.</div>\n<div class=\"imageCredit\">Wen et al. ICRA 2020</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Kostas&nbsp;Bekris\n<div class=\"imageTitle\">3. Pose estimation of objects occluded by adaptive hand</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705693103015_Screenshot_2024_01_19_at_10.59.37_AM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705693103015_Screenshot_2024_01_19_at_10.59.37_AM--rgov-800width.png\" title=\"4. Tight insertion with adaptive hands\"><img src=\"/por/images/Reports/POR/2024/1734492/1734492_10507302_1705693103015_Screenshot_2024_01_19_at_10.59.37_AM--rgov-66x44.png\" alt=\"4. Tight insertion with adaptive hands\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">4. (a) Experimental setup; (b) Tight tolerance insertion of 5 different peg geometries; (c) System ablations; (d) Open-world tasks, showcasing the sequence of actions taken from grasp to insertion (e) Five other open world tasks.</div>\n<div class=\"imageCredit\">Morgan et al. RSS 2021</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Kostas&nbsp;Bekris\n<div class=\"imageTitle\">4. Tight insertion with adaptive hands</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nLow-cost underactuated hands are cheap and robust but lack-analytical models. To address this, we showed that stochastic models can be automatically learned from data by identifying dominant, sensible features that express hand state transitions. Experiments showed that using Diffusion Maps with a feature space composed of the object position, actuator angles, and actuator loads, sufficiently expresses the hand-object system configuration and can provide accurate enough predictions. This was the first learned transition model for such hands with this level of predictability. The model generalizes to new objects of varying sizes. It can also identify states on the verge of failure that should be avoided. Figure 1 shows the integration of the model with closed-loop control. This work also provided the first large public dataset of real within-hand manipulation. We also released a ROS-based physics-engine model of such hands for independent data collection, experimentation, and sim-to-reality transfer.\n\n\nWhile the learned transition models of adaptive hands successfully predict short-term motions, errors accumulate over long horizon predictions. For this reason, the project explored robust, long-term manipulation by maintaining a probability distribution expressing the uncertainty of the learned model and to identify the safest manipulation, i.e., without dropping the object. The approach trains a critic model that estimates the error of the actor transition model. Then a motion planner directs paths to less erroneous regions in the state space. Figure 2 highlights experiments with physically simulated and real adaptive hands, which showed increased robustness and reduced data requirements.\n\n\nTo empower the manipulation of everyday objects with adaptive hands for critical applications, we developed perception tools that reliably detect and track the pose (position and orientation) of objects under manipulation using image and depth data. Figure 3 illustrates the capabilities of these tools where a significantly occluded and texture-less object is correctly detected. This line of work resulted in multiple open-source software packages and datasets: SE(3)-TrackNet allows the high-speed tracking of object poses given a 3D model, while BundleTrack tracks the relative pose of an object as it is being manipulated even without access to a 3D model.\n\n\nBy integrating these perception capabilities with our Yale collaborators learned motion primitives for reorienting objects with adaptive hands, we developed a complete system for precision manipulation tasks under tight, industrially-relevant tolerances (\n\n\nTo manipulate deformable, unknown objects, we developed the first, unified, real-time 3D reconstruction system that simultaneously supports semantic fusion, fast motion tracking, non-rigid object deformation, and topological change. The system solves an optimization problem to deal with fast motion. Experiments demonstrated that the system significantly outperforms existing state-of-the-art. We also introduced a new technique that enables open-vocabulary 3D instance retrieval without using any 3D data for training, which can be used by the reconstruction tool. Given a human language query, the method returns a ranked set of 3D object instance segments based on the feature similarity of the instance and the text query, as shown in Figure 5. The process is performed in real-time during fusion of a 3D point cloud and does not require additional training. The source code for both the reconstruction and the instance retrieval were made publicly available.\n\n\nThe 3D perception, grasping and manipulation techniques described above were combined to perform complex manipulation tasks involving sequential tool use. We proposed a new approach for teaching a robot to perform complex tasks composed of several, consecutively executed low-level sub-tasks, given as input a few visual human demonstrations. The sub-tasks consist of moving the robots hand until it reaches a sub-goal, performing an action, and triggering the next sub-task when a pre-condition is met. A key feature is that the policies are learned directly from raw videos of task demonstrations, without any manual annotation or postprocessing of the data. We extended this work to successfully learn category-level manipulation, which acquires skills that can generalize to new objects as illustrated in Figure 6.\t\t\t\t\tLast Modified: 01/19/2024\n\n\t\t\t\t\tSubmitted by: KostasBekris\n"
 }
}