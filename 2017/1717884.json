{
 "awd_id": "1717884",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Collaborative Research: Error Correction with Natural Redundancy",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 166666.0,
 "awd_amount": 166666.0,
 "awd_min_amd_letter_date": "2017-07-17",
 "awd_max_amd_letter_date": "2017-07-17",
 "awd_abstract_narration": "Part 1: Nontechnical description of the project\r\n\r\nThis project studies the fundamental problem of removing errors from data by using internal structures of data. It shows that the vast amount of data stored in current data-storage systems possess very rich structures; therefore, by fully exploiting them for error correction, the reliability of data-storage systems can be improved significantly. The project studies several fundamental aspects of this technology, including how to discover and characterize the highly complex structures of various types of data, how to use them to correct errors in data efficiently to improve the reliability of data-storage systems, how to combine the technology with existing error-correction techniques that are based on adding external redundancy to data, and how to implement the technology in practical data-storage systems. \r\n\r\nThis project addresses a critical issue of the modern society: how to ensure that data can be stored reliably at large scale and over a long time. The new technology has the potential to substantially improve the dependability of information infrastructure, which accesses vast amounts of data frequently for scientific and industrial computing. The project is interdisciplinary in nature: it combines multiple scientific fields including information theory, machine learning, big data analysis and algorithm design, and aims to educate students and contribute to workforce development for next-generation storage systems. The project conjugates rigorous theoretical analysis and significant practical applications, to foster collaboration between academia and industry, and create new scientific advances with combined efforts.\r\n\r\nPart 2: Technical description of the project\r\n\r\nThis project studies how to use the inherent redundancy in big data for error correction. Examples of big data include languages, images, databases, and others. The inherent redundancy is integrated with error-correcting codes (ECC) for effective error correction. The objective is to elevate data reliability in storage systems to the next level. To achieve this goal, new techniques will be developed to discover various types of inherent redundancy in both compressed and uncompressed data. New approaches will be explored to combine inherent-redundancy decoders and ECC decoders for effective error correction. Fundamental limits of both capacity and computational complexity will be studied for error correction using inherent redundancy.\r\n\r\nThis project combines error correction with machine learning and is interdisciplinary in nature. It will expand the current knowledge on error correction in multiple ways. First, it uses techniques in natural language processing and deep learning to discover new types of redundancy in big data that are suitable for error correction, and which extend beyond current knowledge in joint source-channel coding. This includes redundancy discovery techniques for data already compressed by various compression algorithms. Second, it explores decoding algorithms for ECCs with not only regular ECC-imposed redundancy, but also irregular inherent redundancy. It extends existing error correction schemes to cast the fundamental limits of inherent redundancy for error correction, in terms of both capacity and computational complexity. Third, by integrating a theoretical study with practical systems, a foundation can be laid for next-generation systems that store and transmit big data.\r\n\r\nModern society relies increasingly heavily on digital data. With the explosive amount of data generated each day, it is essential to make advances in error correction that can catch the speed of data explosion. This project aims at improving data reliability significantly to the next level, and improvements in this direction can be highly beneficial to the daily work and life of the modern society. This project, being interdisciplinary between coding theory and machine learning, can foster collaboration between the information theory and computer science communities. The project combines rigorous theoretical analysis with significant practical applications, to foster collaboration between academia and industry, and create new scientific advances with combined efforts. The proposed research will be integrated with engineering education by developing new courses for graduate and undergraduate students, and involving under-represented, domestic and international students in advanced research. The results will be actively publicized in national/international conferences and journals.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jehoshua",
   "pi_last_name": "Bruck",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jehoshua Bruck",
   "pi_email_addr": "bruck@paradise.caltech.edu",
   "nsf_id": "000465142",
   "pi_start_date": "2017-07-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "California Institute of Technology",
  "inst_street_address": "1200 E CALIFORNIA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "PASADENA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6263956219",
  "inst_zip_code": "911250001",
  "inst_country_name": "United States",
  "cong_dist_code": "28",
  "st_cong_dist_code": "CA28",
  "org_lgl_bus_name": "CALIFORNIA INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "U2JMKHNS5TG4"
 },
 "perf_inst": {
  "perf_inst_name": "California Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "911250001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "28",
  "perf_st_cong_dist": "CA28",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 166666.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Big data have lots of complex dependencies among its elements. For example, pixels in images use complex dependencies to form meaningful scenes, and the weights in deep-learning models use complex dependencies to carry out complex artificial-intelligence (AI) tasks. More dependencies exist in various types of data stored, transmitted and computed daily (such as Internet data, work files, etc.). Such dependencies are called redundancy in data. One classic application enabled by redundancy is data compression. This project explores a new research direction enabled by such redundancy, which is to use the redundancy to further improve the reliability of data, even if the data are already partially compressed. The definition of \"reliability\" is also broadened, which includes not only removing errors from noisy data, but also ensuring the computing performance when the data are used in computing (such as in AI applications).</p>\n<p>&nbsp;</p>\n<p>This project has developed various new algorithms for improving the reliability of data in storage, transmission and computing, all based on the redundancy in data. They include algorithms that can recognize which type of file that a segment of noisy bits are from, and then use specialized error correction schemes to remove the errors in the noisy bits. They include new schemes that enhance existing error-correction codes in substantial ways. They include new ways to protect the parameters of AI models selectively, such that AI performance can be optimized at the minimum storage cost. They include error-correction schemes for different types of errors in data, including binary and analog errors, burst errors and erasures, etc. They include joint source-channel coding schemes, which combine information theory with deep learning to achieve better data reliability. And they include new paradigms to design AI models that better utilize the redundancy in data. These new schemes together provide new ways to improve data reliability in storage, communication and computing.</p>\n<p>&nbsp;</p>\n<p>This project has bridged two research areas, information theory and machine learning, and utilized the integration to enhance data reliability for the big-data era. It has educated students in this interdisciplinary field, which has seen rapid progress in recent years, and prepared them as future work force. The project's outcome has led to new topics that can be widely useful for information technology systems.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/15/2021<br>\n\t\t\t\t\tModified by: Jehoshua&nbsp;Bruck</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nBig data have lots of complex dependencies among its elements. For example, pixels in images use complex dependencies to form meaningful scenes, and the weights in deep-learning models use complex dependencies to carry out complex artificial-intelligence (AI) tasks. More dependencies exist in various types of data stored, transmitted and computed daily (such as Internet data, work files, etc.). Such dependencies are called redundancy in data. One classic application enabled by redundancy is data compression. This project explores a new research direction enabled by such redundancy, which is to use the redundancy to further improve the reliability of data, even if the data are already partially compressed. The definition of \"reliability\" is also broadened, which includes not only removing errors from noisy data, but also ensuring the computing performance when the data are used in computing (such as in AI applications).\n\n \n\nThis project has developed various new algorithms for improving the reliability of data in storage, transmission and computing, all based on the redundancy in data. They include algorithms that can recognize which type of file that a segment of noisy bits are from, and then use specialized error correction schemes to remove the errors in the noisy bits. They include new schemes that enhance existing error-correction codes in substantial ways. They include new ways to protect the parameters of AI models selectively, such that AI performance can be optimized at the minimum storage cost. They include error-correction schemes for different types of errors in data, including binary and analog errors, burst errors and erasures, etc. They include joint source-channel coding schemes, which combine information theory with deep learning to achieve better data reliability. And they include new paradigms to design AI models that better utilize the redundancy in data. These new schemes together provide new ways to improve data reliability in storage, communication and computing.\n\n \n\nThis project has bridged two research areas, information theory and machine learning, and utilized the integration to enhance data reliability for the big-data era. It has educated students in this interdisciplinary field, which has seen rapid progress in recent years, and prepared them as future work force. The project's outcome has led to new topics that can be widely useful for information technology systems.\n\n \n\n\t\t\t\t\tLast Modified: 10/15/2021\n\n\t\t\t\t\tSubmitted by: Jehoshua Bruck"
 }
}