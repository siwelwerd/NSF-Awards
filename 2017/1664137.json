{
 "awd_id": "1664137",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SI2-SSI: FAMII: High Performance and Scalable Fabric Analysis, Monitoring and Introspection Infrastructure for HPC and Big Data",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rob Beverly",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2020-06-30",
 "tot_intn_awd_amt": 800000.0,
 "awd_amount": 800000.0,
 "awd_min_amd_letter_date": "2017-05-04",
 "awd_max_amd_letter_date": "2017-11-21",
 "awd_abstract_narration": "As the computing, networking, heterogeneous hardware, and storage\r\ntechnologies continue to evolve in High-End Computing (HEC) platforms,\r\nit becomes increasingly essential and challenging to understand the\r\ninteractions between time-critical High-Performance Computing (HPC)\r\nand Big Data applications, the software infrastructures upon which\r\nthey rely for achieving high-performing portable solutions, the\r\nunderlying communication fabric these high-performance middlewares\r\ndepend on and the schedulers that manage HPC clusters.  Such\r\nunderstanding will enable all involved parties (application\r\ndevelopers/users, system administrators, and middleware developers) to\r\nmaximize the efficiency and performance of the individual components\r\nthat comprise a modern HPC system and solve different grand challenge\r\nproblems. There is a clear need and unfortunate lack of a high-performance and\r\nscalable tool that is capable of analyzing and correlating the\r\ncommunication on the fabric with the behavior of HPC/Big Data\r\napplications, underlying middleware and the job scheduler on existing\r\nlarge HPC systems.  The proposed synergistic and collaborative effort,\r\nundertaken by a team of computer and computational scientists from OSU\r\nand OSC, aims to create an integrated software infrastructure \r\nfor high-performance and scalable Fabric Analysis, Monitoring and\r\nIntrospection for HPC and Big Data. This tool will achieve the\r\nfollowing objectives: 1) be portable, easy to use and easy to\r\nunderstand, 2) have high performance and scalable rendering and\r\nstorage techniques and, 3) be applicable to the different\r\ncommunication fabrics and programming models that are likely to be\r\nused on existing large HPC systems and emerging exascale systems.  The\r\ntransformative impact of the proposed research and development effort\r\nis to design a comprehensive analysis and performance monitoring tool\r\nfor applications of current and next generation multi\r\npetascale/exascale systems to harness the maximum performance and\r\nscalability.\r\n\r\nThe proposed research and the associated infrastructure will have a\r\nsignificant impact on enabling optimizations of HPC and Big Data\r\napplications that have previously been difficult to provide. These\r\npotential outcomes will be demonstrated by using the proposed\r\nframework to validate a variety of HPC and Big Data benchmarks and\r\napplications under multiple scenarios.  The integrated middleware and\r\ntools will be made publicly available to the community through public\r\nrepositories and publications in the top forums, enabling other MPI\r\nand Big Data stacks to adopt the designs.  Research results will also\r\nbe disseminated to the collaborating organizations of the\r\ninvestigators to impact their HPC software products and\r\napplications. The proposed research directions and their solutions\r\nwill be used in the curriculum of the PIs to train undergraduate and\r\ngraduate students, including under-represented minorities and female\r\nstudents. The technical challenges addressed by the proposal include: 1)\r\nScalable visualization of large and complex HEC networks so as to\r\nprovide a near instant rendering to end users, 2) A generalized data\r\ngathering scheme which is easily portable to multiple communication\r\nfabrics, novel compute architectures and high-performance middleware,\r\n3) Enhanced data storage performance through optimized database\r\nschemas and the use of memory-backed key value stores/databases, 4)\r\nSupport in MPI, PGAS, and Big Data libraries to enable the proposed\r\nmonitoring, analysis, and introspection framework, and 5) Enabling\r\ndeeper introspection of particular regions of application.  The\r\nresearch will also be driven by a set of HPC and Big Data\r\napplications. The transformative impact of the proposed research and\r\ndevelopment effort is to design a comprehensive analysis and\r\nperformance monitoring tool for applications of current and next\r\ngeneration multi petascale/exascale systems to harness the maximum\r\nperformance and scalability.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dhabaleswar",
   "pi_last_name": "Panda",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Dhabaleswar K Panda",
   "pi_email_addr": "panda.2@osu.edu",
   "nsf_id": "000487085",
   "pi_start_date": "2017-05-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Karen",
   "pi_last_name": "Tomko",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Karen A Tomko",
   "pi_email_addr": "ktomko@osc.edu",
   "nsf_id": "000330142",
   "pi_start_date": "2017-05-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Xiaoyi",
   "pi_last_name": "Lu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaoyi Lu",
   "pi_email_addr": "xiaoyi.lu@ucmerced.edu",
   "nsf_id": "000696718",
   "pi_start_date": "2017-05-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Hari",
   "pi_last_name": "Subramoni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hari Subramoni",
   "pi_email_addr": "subramoni.1@osu.edu",
   "nsf_id": "000704577",
   "pi_start_date": "2017-05-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Manalo",
   "pi_mid_init": "",
   "pi_sufx_name": "PhD",
   "pi_full_name": "Kevin Manalo",
   "pi_email_addr": "kmanalo@osc.edu",
   "nsf_id": "000714469",
   "pi_start_date": "2017-05-04",
   "pi_end_date": "2017-11-21"
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101206",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  },
  {
   "pgm_ref_code": "8009",
   "pgm_ref_txt": "Scientifc Software Integration"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 800000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As the computing, networking, heterogeneous hardware, and storage<br />technologies continue to evolve in high-end computing platforms, it<br />becomes increasingly essential and challenging to understand the<br />interactions between time-critical High-Performance Computing (HPC),<br />Big Data, and AI applications, the software infrastructures upon which<br />they rely for achieving high-performing portable solutions, and the<br />underlying communication fabric (network between computer in an HPC<br />system). We refer to these high-performance software layers between<br />the applications and the hardware as middleware.&nbsp; It is critical for<br />the users and administrators of HPC installations as well as<br />developers of high-performance middleware that run on these HPC<br />installations to clearly understand this interaction. Such<br />understanding will enable all the involved parties (application<br />developers/users, system administrators, and MPI/PGAS/Big Data/AI<br />programming language and middleware developers) to maximize the<br />efficiency and performance of the various individual components that<br />comprise a modern HPC system and solve the various \"grand challenge\"<br />problems.<br /><br />One of the most common questions HPC scientists tend to have is: \"Why<br />is my application running slower than usual now?\" Interaction with a<br />concurrent job in the network, a fragmented allocation of nodes, or<br />issues with the network-based parallel file system are the most common<br />causes for such unexplained behavior.&nbsp; Several tools exist in the<br />literature and as the products that allow the system administrators to<br />analyze and inspect the communication.&nbsp; However, due to the lack of<br />interaction with, and knowledge about the communication software<br />stack, no existing HPC fabric monitoring tool can correlate network<br />level and middleware level behavior to classify traffic as belonging<br />to or being generated by particular communication operations (e.g.:<br />pairwise between two computers, remote memory access of a computer by<br />another computer, global sum between all computers solving an AI<br />problem).</p>\n<p>To address the above outlined challenges, in this project, we have<br />adopted a multi-year and multi-tiered approach to utilize the<br />underlying system architecture to improve MPI, Big Data and AI<br />communication and application performance and scalability.&nbsp; Challenges<br />have been addressed along the following directions:<br /><br />1. Designed and developed Optimized network graph display for<br />InfiniBand and Omni-Path fabric and MPI/AI application analysis<br /><br />2. Designed and developed interfaces between software components to<br />support monitoring communication between CPU within a computer and<br />between different computers in the HPC system.<br /><br />3. Designed FAMII data collection interface for fabric health and load<br />monitoring, collecting detailed information from MPI, Big Data and AI<br />application runs.<br /><br />4. Designed visualization interface to profile, monitor and understand<br />CPU- and GPU-based communication traffic.<br /><br />5. Designs have been made available to the HPC community through multiple releases.<br /><br />6. Deployment of the framework on various HPC systems at the Ohio<br />Supercomputer Center and partner institutions and continuous<br />engagement with the users to improve the design.<br /><br />Some highlights of the FAMII tool are as follows:<br /><br />* Running on multiple HPC systems with more than 2,000 computers in a stable manner for more than two years.<br /><br />* Discovering fabric topology within 5 minutes for ~1,500 computer system.<br /><br />* Remote read and store of all fabric metrics at sub-second granularity.<br /><br />* End-to-end overhead of less than 2 percent on application performance.<br /><br />* Display network traffic on HPC systems up to 2,000 computers in sub-second granularity<br /><br />* Capability to visualize application-based communication with 5 second granularity<br /><br />The results of this research (new designs, performance results,<br />benchmarks, etc.) have been made available to the community through<br />the FAMII/INAM software tool together with the enhanced MVAPICH2-x<br />library software. Multiple releases of these two software packages<br />have been made during the project period. More than 6,500 copies of<br />the INAM tool and the enhanced MVAPICH2-X library have been downloaded<br />from the project's web site. In each of these releases, features,<br />performance numbers and scalability information have been shared with<br />the MVAPICH user community through mailing lists and the project's web<br />site.&nbsp; In addition to the software distribution, the results have been<br />presented at various conferences and events through Keynote talks,<br />invited talks, tutorials, and hands-on sessions.&nbsp; The research has<br />also led to thesis for several M.S. and Ph.D. students.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/13/2020<br>\n\t\t\t\t\tModified by: Dhabaleswar&nbsp;K&nbsp;Panda</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAs the computing, networking, heterogeneous hardware, and storage\ntechnologies continue to evolve in high-end computing platforms, it\nbecomes increasingly essential and challenging to understand the\ninteractions between time-critical High-Performance Computing (HPC),\nBig Data, and AI applications, the software infrastructures upon which\nthey rely for achieving high-performing portable solutions, and the\nunderlying communication fabric (network between computer in an HPC\nsystem). We refer to these high-performance software layers between\nthe applications and the hardware as middleware.  It is critical for\nthe users and administrators of HPC installations as well as\ndevelopers of high-performance middleware that run on these HPC\ninstallations to clearly understand this interaction. Such\nunderstanding will enable all the involved parties (application\ndevelopers/users, system administrators, and MPI/PGAS/Big Data/AI\nprogramming language and middleware developers) to maximize the\nefficiency and performance of the various individual components that\ncomprise a modern HPC system and solve the various \"grand challenge\"\nproblems.\n\nOne of the most common questions HPC scientists tend to have is: \"Why\nis my application running slower than usual now?\" Interaction with a\nconcurrent job in the network, a fragmented allocation of nodes, or\nissues with the network-based parallel file system are the most common\ncauses for such unexplained behavior.  Several tools exist in the\nliterature and as the products that allow the system administrators to\nanalyze and inspect the communication.  However, due to the lack of\ninteraction with, and knowledge about the communication software\nstack, no existing HPC fabric monitoring tool can correlate network\nlevel and middleware level behavior to classify traffic as belonging\nto or being generated by particular communication operations (e.g.:\npairwise between two computers, remote memory access of a computer by\nanother computer, global sum between all computers solving an AI\nproblem).\n\nTo address the above outlined challenges, in this project, we have\nadopted a multi-year and multi-tiered approach to utilize the\nunderlying system architecture to improve MPI, Big Data and AI\ncommunication and application performance and scalability.  Challenges\nhave been addressed along the following directions:\n\n1. Designed and developed Optimized network graph display for\nInfiniBand and Omni-Path fabric and MPI/AI application analysis\n\n2. Designed and developed interfaces between software components to\nsupport monitoring communication between CPU within a computer and\nbetween different computers in the HPC system.\n\n3. Designed FAMII data collection interface for fabric health and load\nmonitoring, collecting detailed information from MPI, Big Data and AI\napplication runs.\n\n4. Designed visualization interface to profile, monitor and understand\nCPU- and GPU-based communication traffic.\n\n5. Designs have been made available to the HPC community through multiple releases.\n\n6. Deployment of the framework on various HPC systems at the Ohio\nSupercomputer Center and partner institutions and continuous\nengagement with the users to improve the design.\n\nSome highlights of the FAMII tool are as follows:\n\n* Running on multiple HPC systems with more than 2,000 computers in a stable manner for more than two years.\n\n* Discovering fabric topology within 5 minutes for ~1,500 computer system.\n\n* Remote read and store of all fabric metrics at sub-second granularity.\n\n* End-to-end overhead of less than 2 percent on application performance.\n\n* Display network traffic on HPC systems up to 2,000 computers in sub-second granularity\n\n* Capability to visualize application-based communication with 5 second granularity\n\nThe results of this research (new designs, performance results,\nbenchmarks, etc.) have been made available to the community through\nthe FAMII/INAM software tool together with the enhanced MVAPICH2-x\nlibrary software. Multiple releases of these two software packages\nhave been made during the project period. More than 6,500 copies of\nthe INAM tool and the enhanced MVAPICH2-X library have been downloaded\nfrom the project's web site. In each of these releases, features,\nperformance numbers and scalability information have been shared with\nthe MVAPICH user community through mailing lists and the project's web\nsite.  In addition to the software distribution, the results have been\npresented at various conferences and events through Keynote talks,\ninvited talks, tutorials, and hands-on sessions.  The research has\nalso led to thesis for several M.S. and Ph.D. students.\n\n\t\t\t\t\tLast Modified: 10/13/2020\n\n\t\t\t\t\tSubmitted by: Dhabaleswar K Panda"
 }
}