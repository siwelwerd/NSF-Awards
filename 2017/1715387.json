{
 "awd_id": "1715387",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small:  ConfigV: Automated Verification of Configuration Files",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927991",
 "po_email": "namla@nsf.gov",
 "po_sign_block_name": "Nina Amla",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2017-08-30",
 "awd_max_amd_letter_date": "2017-08-30",
 "awd_abstract_narration": "Configuration files allow programmers to easily control many key software settings, but this variety of settings creates a large surface for potential errors, with impacts as severe as performance degradation or system-wide failure. These configuration errors have affected many software-based services, from social networking to emergency dispatch call systems. The fundamental issue this project addresses is the need to detect these errors, before they are released in production, by automatically checking configuration files against a set of rules that describe safe configurations. Since there are many different types of configuration languages, all with too many complex rules to be manually written, configuration file verification must automatically learn rules from existing examples of configuration files. This project will have broader impact in the field, expanding the verification beyond just traditional programs, and allowing for ensuring the safety of both configuration files and other complex and unstructured objects.\r\n\r\nThe goal of this proposal is to develop a fully automated verification framework for general software configurations. To do this, the user must provide a set of example configuration files, from which we learn rules that describe various properties that hold on the given example set. These rules, in general, specify which properties the keywords in a configuration file need to satisfy. A key challenge in the process of inferring such a specification is that configuration files are generally an untyped, unstructured sequence of assignments - making the application of existing formal methods approaches difficult. To add structure to these files, the PI uses a probabilistic type inference algorithm to assign each keyword a type. The learning process then relies on matching the inferred types to a set of very general templates, which describe the keywords and their relations. This project further extends the areas where formal verification can be applied and develops a tool set for configuration file verification that can increase the productivity of software practitioners.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ruzica",
   "pi_last_name": "Piskac",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ruzica Piskac",
   "pi_email_addr": "ruzica.piskac@yale.edu",
   "nsf_id": "000655841",
   "pi_start_date": "2017-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "AK Watson Lab",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208285",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8206",
   "pgm_ref_txt": "Formal Methods and Verification"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Configuration errors (also known as misconfigurations) have become a major cause of system failures, resulting in security vulnerabilities, application outages, and incorrect program executions. Every day the popular news media reports about data leakage and service unavailability caused by errors in configuration files. These critical system failures are not rare -- a study on the sources of software system failures found that about 31% were caused by configuration errors. This is even higher than the percentage resulting from program bugs (20%).<br /><br />The difficulty with configuration files is that they are mostly simple text files of keywords and values, and there is no traditional sense of a specification. With no formal specification of correctness or program semantics, verifying configuration files is far outside the scope of existing technologies. <br /><br />The goal of this award was to develop techniques and tools for configuration verification. We developed several verification approaches, each extending the class of configuration bugs that we can detect -- or verify the absence of. We first showed how to generalize the well-known synthesis paradigm of programming-by-example beyond its traditional application, where a user provides simple illustrative examples of a program's desired operation. We use actual configuration files as examples to automatically learn specifications that describe properties of correct configuration. We then use these learned specifications to verify other configuration files. In addition, we also addressed the problem of configuration verification and repair within the domain of continuous integration testing. Continuous integration (CI) allows users to automatically build and run their code on a suite of virtual environments as they develop a codebase, in order to test compatibility with different operating systems, hardware configurations, library versions, and other infrastructure. While this technique helps to ensure cross-platform functionality, correctly configuring the many permutations of test environments is a complicated process and can only be debugged by rerunning many slow test cases. In some cases, CI builds can take upwards of 24 hours to complete - and if there is a misconfiguration, the tests themselves cannot even run. In order to provide users with static, compile time feedback, we analyze large databases of code to learn models for correct CI configurations. We leverage the fact that it is often the small, incremental changes users make that break or fix builds. By combining association rule learning and with an SMT solver to resolve conflicting rules, we can efficiently learn models that capture the causes of failing CI builds. These models can be used to preemptively warn users of misconfigured test environments and suggest repairs to their configuration settings. Finally, we introduced the concept of \"silent misconfigurations\" that appear when the user changes the configuration file in a manner that does not influence the system?s runtime behavior. These silent misconfigurations are prohibitively hard to identify due to (1) lack of any feedback from systems and (2) complex dependencies between the configuration parameters and the source code. The main challenge in detecting such errors are complex connections and interactions between configuration variables. To overcome this challenge, we designed a comprehensive analysis to capture and analyze the interactions between code blocks related to configurations. We empirically evaluated all our tools on publicly available datasets across Apache, vsftpd and PostgreSQL, as well as on problems reported on user forums by real-world users. Across the whole project we received positive and affirmative feedback from those users. In addition, using our tools we found security issues in updates to the configuration in Infrastructure as Code for two major cloud vendors.<br /><br />From a broad impact perspective, this award supported several PhD students. Mark Santolucito has now graduated and is an assistant professor at Barnard College. Jialu Zhang is now a fifth year PhD student and his work on configuration files qualified him to work as an intern at Microsoft Research. As a part of this project, we have brought several undergraduate high school students into research. As an example, we published a paper at a major AI conference with high school students as co-authors.&nbsp; We also wrote a paper about our experiences involving these students with research, entitled \"Formal Methods and Computing Identity-based Mentorship for Early Stage Researchers\".<br /><br />Finally, one of the goals of this award was community building and raising the awareness about misconfigurations. We submitted a proposal for a Dagstuhl seminar (Resilient Software Configuration and Infrastructure Code Analysis), and the proposal has been accepted, though the seminar has been postponed, due to the pandemic. The PI also gave the invited talk at the Facebook Testing and Verification Symposium in 2018 and the keynote at the 20th IEEE International Working Conference on Source Code Analysis and Manipulation.<br /><br />Overall, we believe that this award established the solid foundations for the next steps in the verification of configuration files.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/31/2021<br>\n\t\t\t\t\tModified by: Ruzica&nbsp;Piskac</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nConfiguration errors (also known as misconfigurations) have become a major cause of system failures, resulting in security vulnerabilities, application outages, and incorrect program executions. Every day the popular news media reports about data leakage and service unavailability caused by errors in configuration files. These critical system failures are not rare -- a study on the sources of software system failures found that about 31% were caused by configuration errors. This is even higher than the percentage resulting from program bugs (20%).\n\nThe difficulty with configuration files is that they are mostly simple text files of keywords and values, and there is no traditional sense of a specification. With no formal specification of correctness or program semantics, verifying configuration files is far outside the scope of existing technologies. \n\nThe goal of this award was to develop techniques and tools for configuration verification. We developed several verification approaches, each extending the class of configuration bugs that we can detect -- or verify the absence of. We first showed how to generalize the well-known synthesis paradigm of programming-by-example beyond its traditional application, where a user provides simple illustrative examples of a program's desired operation. We use actual configuration files as examples to automatically learn specifications that describe properties of correct configuration. We then use these learned specifications to verify other configuration files. In addition, we also addressed the problem of configuration verification and repair within the domain of continuous integration testing. Continuous integration (CI) allows users to automatically build and run their code on a suite of virtual environments as they develop a codebase, in order to test compatibility with different operating systems, hardware configurations, library versions, and other infrastructure. While this technique helps to ensure cross-platform functionality, correctly configuring the many permutations of test environments is a complicated process and can only be debugged by rerunning many slow test cases. In some cases, CI builds can take upwards of 24 hours to complete - and if there is a misconfiguration, the tests themselves cannot even run. In order to provide users with static, compile time feedback, we analyze large databases of code to learn models for correct CI configurations. We leverage the fact that it is often the small, incremental changes users make that break or fix builds. By combining association rule learning and with an SMT solver to resolve conflicting rules, we can efficiently learn models that capture the causes of failing CI builds. These models can be used to preemptively warn users of misconfigured test environments and suggest repairs to their configuration settings. Finally, we introduced the concept of \"silent misconfigurations\" that appear when the user changes the configuration file in a manner that does not influence the system?s runtime behavior. These silent misconfigurations are prohibitively hard to identify due to (1) lack of any feedback from systems and (2) complex dependencies between the configuration parameters and the source code. The main challenge in detecting such errors are complex connections and interactions between configuration variables. To overcome this challenge, we designed a comprehensive analysis to capture and analyze the interactions between code blocks related to configurations. We empirically evaluated all our tools on publicly available datasets across Apache, vsftpd and PostgreSQL, as well as on problems reported on user forums by real-world users. Across the whole project we received positive and affirmative feedback from those users. In addition, using our tools we found security issues in updates to the configuration in Infrastructure as Code for two major cloud vendors.\n\nFrom a broad impact perspective, this award supported several PhD students. Mark Santolucito has now graduated and is an assistant professor at Barnard College. Jialu Zhang is now a fifth year PhD student and his work on configuration files qualified him to work as an intern at Microsoft Research. As a part of this project, we have brought several undergraduate high school students into research. As an example, we published a paper at a major AI conference with high school students as co-authors.  We also wrote a paper about our experiences involving these students with research, entitled \"Formal Methods and Computing Identity-based Mentorship for Early Stage Researchers\".\n\nFinally, one of the goals of this award was community building and raising the awareness about misconfigurations. We submitted a proposal for a Dagstuhl seminar (Resilient Software Configuration and Infrastructure Code Analysis), and the proposal has been accepted, though the seminar has been postponed, due to the pandemic. The PI also gave the invited talk at the Facebook Testing and Verification Symposium in 2018 and the keynote at the 20th IEEE International Working Conference on Source Code Analysis and Manipulation.\n\nOverall, we believe that this award established the solid foundations for the next steps in the verification of configuration files.\n\n\t\t\t\t\tLast Modified: 12/31/2021\n\n\t\t\t\t\tSubmitted by: Ruzica Piskac"
 }
}