{
 "awd_id": "1721445",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Automatic Video Interpretation and Description",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Christopher Stark",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 79999.0,
 "awd_amount": 79999.0,
 "awd_min_amd_letter_date": "2017-08-20",
 "awd_max_amd_letter_date": "2017-08-20",
 "awd_abstract_narration": "Digital information processing has become an essential part of modern life. It is nowadays often expressed in a form of multimedia, involving videos accompanied with images, captions, and audio. Given the explosive growth of such multimedia data, it is extremely critical that it is accurately summarized and organized for automatic processing in artificial intelligence. One important yet challenging problem is automatic interpretation and summarization of video content, having enormous applications in video advertisements, online video searching and browsing, movie recommendation based on personal preference, and essentially any electronic commerce platform. In this project, the research team plans to develop statistical tools to raise our capacity of processing digital information to respond to a rapid growth of video content in real-world applications. The primary objective is to create a learning system to decipher the meaning of visual expressions as perceived by the audience, with a focus on understanding semantic meaning conveyed by a video.\r\n\r\nThis project aims to develop methods of automatic video interpretation and description, which understands visual thoughts expressed by a video and generates semantic expressions of the content of a video. Particularly, it will utilize conditional dependence structures of entities as well as between entities and their pertinent actions, in a framework of multi-label and hierarchical classification. It will focus on three areas: 1) entity and action learning, 2) semantic learning for long videos and content-based segmentation, and 3) automatic video description generation, each of which develops techniques in novel ways. In each area, classification will be performed collaboratively based on pairwise conditional label dependencies and temporal dependencies of video frames, characterized by graphical and hidden Markov models. Special effort will be devoted to learning from multiple sources and extracting latent structures corresponding to scenes of a video. The PIs also plan to release the software developed as open source and build a user community around the language by ensuring that interested researchers are able to contribute to the codebase of the software developed. This will allow a wider growth of the  project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yunzhang",
   "pi_last_name": "Zhu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yunzhang Zhu",
   "pi_email_addr": "Zhu.219@osu.edu",
   "nsf_id": "000707004",
   "pi_start_date": "2017-08-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "1928 Neil Avenue, Cockins Hall",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "689200",
   "pgm_ele_name": "CI REUSE"
  },
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1253",
   "pgm_ref_txt": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 79999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<p>&nbsp; Digital information processing has become an essential part of modern life, where</p>\n<p>the relevant information is often embedded in multimedia, for example, images, captions, and&nbsp;</p>\n<p>audios. The project supported by this award studies methods and algorithms&nbsp;</p>\n<p>for automatic image interpretation and description, which understands visual expressions</p>\n<p>of an image or a video and generates semantic interpretations.</p>\n<p>The results of the project gave us some new insights into how the content of an image can be</p>\n<p>extracted through learning by examples and accurately described.</p>\n<p>On this ground, we develop methods to train an image model and a language model jointly</p>\n<p>for extracting and describing the content. As a result of our effort, human intelligence is&nbsp;</p>\n<p>integrated with the processing capacity of the machine to develop an intelligent machine to</p>\n<p>understand and describe visual expressions. Moreover, we also learned about the importance of&nbsp;</p>\n<p>the dependence structures of different image frames for recognition of various actions.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>The collaborative project has incorporated research results in teaching to create an exciting opportunity&nbsp;</p>\n<p>for students in learning state-of-art technology. Moreover, during the project period, the PIs have</p>\n<p>mentored several undergraduates, four M.S. students, and six Ph.D. students. Concerning</p>\n<p>the dissemination of the research results, the project has produced several image&nbsp;</p>\n<p>processing platforms for end-users. The PI has also delivered more than 40 invited presentations at national&nbsp;</p>\n<p>and international conferences, workshops, and institutional colloquia.&nbsp; &nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/12/2022<br>\n\t\t\t\t\tModified by: Yunzhang&nbsp;Zhu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \n\n  Digital information processing has become an essential part of modern life, where\n\nthe relevant information is often embedded in multimedia, for example, images, captions, and \n\naudios. The project supported by this award studies methods and algorithms \n\nfor automatic image interpretation and description, which understands visual expressions\n\nof an image or a video and generates semantic interpretations.\n\nThe results of the project gave us some new insights into how the content of an image can be\n\nextracted through learning by examples and accurately described.\n\nOn this ground, we develop methods to train an image model and a language model jointly\n\nfor extracting and describing the content. As a result of our effort, human intelligence is \n\nintegrated with the processing capacity of the machine to develop an intelligent machine to\n\nunderstand and describe visual expressions. Moreover, we also learned about the importance of \n\nthe dependence structures of different image frames for recognition of various actions. \n\n \n\n \n\nThe collaborative project has incorporated research results in teaching to create an exciting opportunity \n\nfor students in learning state-of-art technology. Moreover, during the project period, the PIs have\n\nmentored several undergraduates, four M.S. students, and six Ph.D. students. Concerning\n\nthe dissemination of the research results, the project has produced several image \n\nprocessing platforms for end-users. The PI has also delivered more than 40 invited presentations at national \n\nand international conferences, workshops, and institutional colloquia.   \n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/12/2022\n\n\t\t\t\t\tSubmitted by: Yunzhang Zhu"
 }
}