{
 "awd_id": "1655014",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Sociolinguistic Perception in Real Time",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tyler Kendall",
 "awd_eff_date": "2017-06-01",
 "awd_exp_date": "2021-11-30",
 "tot_intn_awd_amt": 254961.0,
 "awd_amount": 254961.0,
 "awd_min_amd_letter_date": "2017-02-27",
 "awd_max_amd_letter_date": "2017-02-27",
 "awd_abstract_narration": "Listeners can learn a lot of social information about a person from small details of speech, even details that they are not consciously aware of hearing. How do people put such complex information together so quickly over the course of an utterance?  Sociolinguists have begun recently to study moment-to-moment reactions to each individual meaningful instance of variation. Efforts have been hampered, however, by a lack of methodological consistency and an inadequate theoretical understanding of the relationship between such moment-to-moment reactions and the end-of-stimulus judgments on which most work on sociolinguistic perception is based. Research on people's perceptions of positive and negative experiences suggests that this relationship is likely to be more complex that sociolinguists have assumed.\r\n\r\nThe first phase of the project will test possible methods for tracking sociolinguistic perception moment-to-moment, including both explicit methods such as button presses and implicit methods such as eye-tracking. These will be tested on the same stimuli, which will be constructed to include specific linguistic differences, to identify the methods which are best at detecting these known differences. The successful methods will be used in the second phase to explore how listeners respond to multiple tokens of the same variable, to tokens of different but socially related variables and to variables combined with extra-linguistic information like pictures of the speaker. Finally, phase three will use the, by now, well-tested stimuli to explore the effects of important points in sociolinguistic perception on linguistic processing. The research will be conducted at the Ohio State University's Language Science Research Lab, which is embedded in the science museum COSI, inviting museum visitors to participate in the research. In addition to the core research, the project will develop more effective techniques for integrating research and outreach. Successful techniques will be shared with other institutions. Successful techniques will be shared with other institutions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kathryn",
   "pi_last_name": "Campbell-Kibler",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kathryn Campbell-Kibler",
   "pi_email_addr": "Campbell-Kibler.1@osu.edu",
   "nsf_id": "000508267",
   "pi_start_date": "2017-02-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101298",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 254961.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div>This project set out to learn more about how people perceive each other's voices socially and specifically how that happens at a moment to moment level as they listen to someone they don't know. We know a fair bit about language cues (individual details of pronunciation or phrasing that influence our social perception of each other) and how they shape social perceptions, but we don't know very much about how our minds add each individual piece of information to a developing social picture. We do know that it's probably pretty complicated because in earlier work researchers have seen that the same cue can contribute different social meanings depending on what else is happening in the speech stream or the wider context. Some work has explored tools to record people's reactions while they listen to voices and we wanted to test some of these tools to see how well they work. We had three research goals:</div>\n<ol>\n<li>Develop and test some tools for tracking people's social reactions to voices as they listen</li>\n<li>Use these tools to explore how multiple cues interact with each other</li>\n<li>Test whether important social cues show an effect on the speed of linguistic processing</li>\n</ol>\n<div>We also had an outreach goal of testing different ways of explaining the project to the participants, but due to the pandemic we didn't get as far with that goal as we had hoped we would.</div>\n<div><span>&nbsp;</span></div>\n<div>We tested a few different tools, some of them explicit, where participants had to make a decision to indicate their response, for example by pushing a button, and others implicit, where participants have less control over the measure, like eyetracking. For each, we tested some variations in the instructions and the details of the task, like using a slider vs. a joystick vs. a keyboard. We found one  explicit and one implicit method that were easy for participants to understand and that captured their responses reliably. The papers published from this project describe these methods, what we know about their strengths and weaknesses, and what we've learned about social perception of language based on the data we collected. Some of the data was collected in a linguistics lab in the science museum COSI, in Columbus, Ohio. The rest was collected online using a crowdsourcing platform.</div>\n<div><span>&nbsp;</span></div>\n<div>The explicit method asked participants to move a slider continuously while they listened to a minute-long recording, shifting its position whenever their perception of the speaker changed. The slider was labeled at each end, for example, \"Not at all educated\" to \"Very educated\", with the adjective changing based on which language cues were being examined. We found that this method was reliable at capturing social responses to big stylistic shifts, like the change between someone sounding very fluid and comfortable and then suddenly sounding very uncertain and stumbling. It was also somewhat effective at capturing responses to individual cues like (ING) (as in helping vs. helpin') and innovative \"like\" (as in \"He's, like, really hungry.\"). The effects were quite slow, around 4-7 seconds after the linguistic change or cue and also had a lot of variability, so that hundreds of participants were needed in order to see the effect. We concluded that the method works, but has drawbacks that should make researchers cautious using it.</div>\n<div><span>&nbsp;</span></div>\n<div>The eyetracking method had participants listen to shorter recordings, while looking at a display of pictures. After the recording finished, they were asked to click on the picture of the person most likely to have been the speaker they heard. During the recording, we tracked their eye movements to see when the individual cues influenced which speakers they looked at. We used language cues with well known effects so that we could test the linking hypothesis that where someone looks indicates who they think the speaker might be. We did see the predicted effects, showing that people perceived the race of the speaker within the first 1-2 seconds of speech (as Purnell, Idsardi &amp; Baugh 1999 found using other methods) and that they showed a similar 1-2 second response to different pronunciation of /s/ influencing their perception of the masculinity of the speaker they heard.</div>\n<div><span>&nbsp;</span></div>\n<div>Alongside the methods themselves, the results show some interesting things about sociolinguistic processing. We found that the in-the-moment slider ratings correlated with after-the-fact ratings, but not perfectly, in a way that suggests that the slider in-the-moment ratings might result from a different process or different mix of processes. Reports of the perceptions while they were forming were more heavily influenced by recent information than those that came afterwards, even very shortly afterwards. The more differences between the two tasks, the bigger this disconnect. Overall, we're excited for the field to continue to explore in-the-moment or real time reactions, but we urge caution and attention to the drawbacks of the available methods.</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/07/2022<br>\n\t\t\t\t\tModified by: Kathryn&nbsp;Campbell-Kibler</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "This project set out to learn more about how people perceive each other's voices socially and specifically how that happens at a moment to moment level as they listen to someone they don't know. We know a fair bit about language cues (individual details of pronunciation or phrasing that influence our social perception of each other) and how they shape social perceptions, but we don't know very much about how our minds add each individual piece of information to a developing social picture. We do know that it's probably pretty complicated because in earlier work researchers have seen that the same cue can contribute different social meanings depending on what else is happening in the speech stream or the wider context. Some work has explored tools to record people's reactions while they listen to voices and we wanted to test some of these tools to see how well they work. We had three research goals:\n\nDevelop and test some tools for tracking people's social reactions to voices as they listen\nUse these tools to explore how multiple cues interact with each other\nTest whether important social cues show an effect on the speed of linguistic processing\n\nWe also had an outreach goal of testing different ways of explaining the project to the participants, but due to the pandemic we didn't get as far with that goal as we had hoped we would.\n \nWe tested a few different tools, some of them explicit, where participants had to make a decision to indicate their response, for example by pushing a button, and others implicit, where participants have less control over the measure, like eyetracking. For each, we tested some variations in the instructions and the details of the task, like using a slider vs. a joystick vs. a keyboard. We found one  explicit and one implicit method that were easy for participants to understand and that captured their responses reliably. The papers published from this project describe these methods, what we know about their strengths and weaknesses, and what we've learned about social perception of language based on the data we collected. Some of the data was collected in a linguistics lab in the science museum COSI, in Columbus, Ohio. The rest was collected online using a crowdsourcing platform.\n \nThe explicit method asked participants to move a slider continuously while they listened to a minute-long recording, shifting its position whenever their perception of the speaker changed. The slider was labeled at each end, for example, \"Not at all educated\" to \"Very educated\", with the adjective changing based on which language cues were being examined. We found that this method was reliable at capturing social responses to big stylistic shifts, like the change between someone sounding very fluid and comfortable and then suddenly sounding very uncertain and stumbling. It was also somewhat effective at capturing responses to individual cues like (ING) (as in helping vs. helpin') and innovative \"like\" (as in \"He's, like, really hungry.\"). The effects were quite slow, around 4-7 seconds after the linguistic change or cue and also had a lot of variability, so that hundreds of participants were needed in order to see the effect. We concluded that the method works, but has drawbacks that should make researchers cautious using it.\n \nThe eyetracking method had participants listen to shorter recordings, while looking at a display of pictures. After the recording finished, they were asked to click on the picture of the person most likely to have been the speaker they heard. During the recording, we tracked their eye movements to see when the individual cues influenced which speakers they looked at. We used language cues with well known effects so that we could test the linking hypothesis that where someone looks indicates who they think the speaker might be. We did see the predicted effects, showing that people perceived the race of the speaker within the first 1-2 seconds of speech (as Purnell, Idsardi &amp; Baugh 1999 found using other methods) and that they showed a similar 1-2 second response to different pronunciation of /s/ influencing their perception of the masculinity of the speaker they heard.\n \nAlongside the methods themselves, the results show some interesting things about sociolinguistic processing. We found that the in-the-moment slider ratings correlated with after-the-fact ratings, but not perfectly, in a way that suggests that the slider in-the-moment ratings might result from a different process or different mix of processes. Reports of the perceptions while they were forming were more heavily influenced by recent information than those that came afterwards, even very shortly afterwards. The more differences between the two tasks, the bigger this disconnect. Overall, we're excited for the field to continue to explore in-the-moment or real time reactions, but we urge caution and attention to the drawbacks of the available methods.\n\n \n\n\t\t\t\t\tLast Modified: 07/07/2022\n\n\t\t\t\t\tSubmitted by: Kathryn Campbell-Kibler"
 }
}