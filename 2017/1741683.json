{
 "awd_id": "1741683",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Compiler and Runtime Support for Irregular Applications on Many-core Processors",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2017-01-01",
 "awd_exp_date": "2022-01-31",
 "tot_intn_awd_amt": 424999.0,
 "awd_amount": 424999.0,
 "awd_min_amd_letter_date": "2017-05-11",
 "awd_max_amd_letter_date": "2019-02-25",
 "awd_abstract_narration": "Many-core processors (such as GPUs) have been used to accelerate a wide variety of applications: molecular dynamics, image processing, data mining, option pricing and linear algebra, among others. Despite their widespread adoption, these devices are still considered relatively difficult to use, in that they require the programmer to be familiar both with parallel programming and with the operation of the hardware. In particular, the effective deployment of irregular applications on many-core devices is still far from understood. However, many established and emerging applications (from social and computer networking, electrical circuit modeling, discrete event simulation, compilers, and computational sciences) are irregular in nature, being based on data structures such as graphs and trees. \r\nThis research proposes compiler and runtime techniques to support the deployment of graph and other irregular applications on many-core processors, while hiding from the programmer the complexity and heterogeneity of the underlying hardware and software stack. Since the degree of parallelism within irregular applications is heavily data dependent, the proposed compiler techniques aim to generate multiple platform-specific code variants starting from high-level platform-agnostic algorithmic descriptions. The runtime techniques focus on the selection of the most appropriate code variant and its tuning to the hardware and the input datasets. More specifically, this research covers three important issues related to irregular applications: (i) the effective handling of nested parallelism (in the form of parallelizable nested loops and recursive functions) within irregular applications; (ii) the design of a dynamic memory allocation library that can scale to the degree of multithreading offered by many-core devices, and of graph encoding schemes suitable for applications operating on dynamic datasets; and (iii) the effective handling of synchronization on many-core devices.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michela",
   "pi_last_name": "Becchi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michela Becchi",
   "pi_email_addr": "mbecchi@ncsu.edu",
   "nsf_id": "000573363",
   "pi_start_date": "2017-05-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276957003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 148872.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 90591.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 89905.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 95630.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Many-core processors (such as Graphics Processing Units) have been used to accelerate a wide variety of applications: image and video processing, machine learning, data mining, weather prediction, genomics, astrophysics, and option pricing, among others. Despite their widespread adoption, these devices are still considered relatively difficult to use, in that they require the programmer to be familiar both with parallel programming and with the operation of the hardware. Many established and emerging applications are based on tree and graph data structures. Notable examples include applications from social and computer networking, electrical circuit modeling, compilers, bioinformatics, neuroscience, and other computational sciences. Applications in this category are often characterized by complex computational and memory usage patterns. As a result, accelerating those applications on massively parallel platforms is challenging.</p>\n<p><br />This research has proposed code transformation and runtime techniques to support the deployment of applications based on complex data structures (such as graphs and trees) on massively parallel processors, while hiding from the programmer the complexity and heterogeneity of the underlying hardware and software stack. To achieve this goal, we have identified problematic computational and memory access patterns at the core of the considered applications, and we have proposed several methods to map and execute these codes efficiently on different parallel platforms. We have generated multiple code variants for the same algorithm and devised mechanisms to select the code variant more suited to the characteristics of the input dataset and the target platform. We have developed software components to retarget code optimized for a specific device (for example, a graphics processing unit) to other parallel platforms with different hardware architectures.</p>\n<p>&nbsp;</p>\n<p>From a broader impact standpoint, this project has aimed to lower the barrier to the use of highly parallel processors for applications relying on trees and graphs and exhibiting complex parallelism, computational and memory access patterns. In addition, this project has allowed the training of undergraduate and graduate students in parallel computing, parallel computer architecture and graph applications.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/05/2022<br>\n\t\t\t\t\tModified by: Michela&nbsp;Becchi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany-core processors (such as Graphics Processing Units) have been used to accelerate a wide variety of applications: image and video processing, machine learning, data mining, weather prediction, genomics, astrophysics, and option pricing, among others. Despite their widespread adoption, these devices are still considered relatively difficult to use, in that they require the programmer to be familiar both with parallel programming and with the operation of the hardware. Many established and emerging applications are based on tree and graph data structures. Notable examples include applications from social and computer networking, electrical circuit modeling, compilers, bioinformatics, neuroscience, and other computational sciences. Applications in this category are often characterized by complex computational and memory usage patterns. As a result, accelerating those applications on massively parallel platforms is challenging.\n\n\nThis research has proposed code transformation and runtime techniques to support the deployment of applications based on complex data structures (such as graphs and trees) on massively parallel processors, while hiding from the programmer the complexity and heterogeneity of the underlying hardware and software stack. To achieve this goal, we have identified problematic computational and memory access patterns at the core of the considered applications, and we have proposed several methods to map and execute these codes efficiently on different parallel platforms. We have generated multiple code variants for the same algorithm and devised mechanisms to select the code variant more suited to the characteristics of the input dataset and the target platform. We have developed software components to retarget code optimized for a specific device (for example, a graphics processing unit) to other parallel platforms with different hardware architectures.\n\n \n\nFrom a broader impact standpoint, this project has aimed to lower the barrier to the use of highly parallel processors for applications relying on trees and graphs and exhibiting complex parallelism, computational and memory access patterns. In addition, this project has allowed the training of undergraduate and graduate students in parallel computing, parallel computer architecture and graph applications. \n\n \n\n\t\t\t\t\tLast Modified: 05/05/2022\n\n\t\t\t\t\tSubmitted by: Michela Becchi"
 }
}