{
 "awd_id": "1712957",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Investigation of Bayes Procedures: Theory, Modeling, and Computation",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2020-06-30",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2017-05-19",
 "awd_max_amd_letter_date": "2017-05-19",
 "awd_abstract_narration": "Bayesian analysis is a widely used technique in data science for estimation, prediction, and interpretation. The new era of complex and big data imposes unprecedented challenges to Bayesian statistics. This research project addresses these new challenges from three different perspectives. First, the investigator will study the relation between prior knowledge and scientific conclusion by conducting rigorous mathematical analysis in the framework of Bayesian statistics. Second, the investigator aims to find novel ways of modeling data sets that can take into account new features of modern big data. Finally, the investigator intends to push the boundary of Bayesian computation by inventing new algorithms that are both fast and theoretically sound. The results of the research are expected to have a positive impact in areas that apply Bayesian statistics on a routine basis, including population genetics, astronomy, computer vision, political science, social science, and animal science.\r\n\r\nBayesian analysis is an important statistical framework for both modeling and computation. However, applying Bayesian procedures correctly when encountering a specific problem is non-trivial. The selections of prior, likelihood, and algorithm all influence the final conclusion drawn from a posterior distribution. Despite many successful applications of Bayesian analysis in various scientific areas, solid theoretical foundations on how to perform Bayesian inference are still lacking. The goal of this project is to develop a coherent theory on optimal Bayesian inference. Specifically, the investigator will study: 1) Bayesian theory: optimal posterior contraction in parametric, nonparametric and high-dimensional models; 2) Bayesian modeling: likelihood functions that are free of nuisance parameters and Bayesian edge-exchangeable network analysis; 3) Bayesian computation: algorithmic and statistical properties of variational inference; and 4) applications to single-cell RNA sequencing analysis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chao",
   "pi_last_name": "Gao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chao Gao",
   "pi_email_addr": "chaogao@galton.uchicago.edu",
   "nsf_id": "000732656",
   "pi_start_date": "2017-05-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Chicago",
  "perf_str_addr": "5747 South Ellis Avenue",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606375418",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Bayesian analysis is a widely used technique in data science for estimation, prediction and interpretation. The new era of complex and big data imposes unprecedented challenges to Bayesian statistics. This project aims to address these new challenges from three different perspectives.</p>\n<p>In terms of Bayesian theory, the PI studied convergence rates of variational posterior distributions for nonparametric and high-dimensional inference. The PI and his student Fengshuo Zhang formulate general conditions on prior, likelihood, and variational class that characterize the convergence rates. Under similar &ldquo;prior mass and testing&rdquo; conditions considered in the literature, the rate is found to be the sum of two terms. The first term stands for the convergence rate of the true posterior distribution, and the second term is contributed by the variational approximation error. The PI and his student Fengshuo Zhang also studied the convergence rates of empirical Bayes posterior distributions for nonparametric and high-dimensional inference. They show that as long as the hyperparameter set is discrete, the empirical Bayes posterior distribution induced by the maximum marginal likelihood estimator can be regarded as a variational approximation to a hierarchical Bayes posterior distribution. This connection between empirical Bayes and variational Bayes allows us to leverage the recent results in the variational Bayes literature, and directly obtains the convergence rates of empirical Bayes posterior distributions from a variational perspective.</p>\n<p>For Bayesian modeling, high dimensional statistics deals with the challenge of extracting structured information from complex model settings. Compared with a large number of frequentist methodologies, there are rather few theoretically optimal Bayes methods for high dimensional models. The PI has studied a unified approach to both Bayes high dimensional statistics and Bayes nonparametrics in a general framework of structured linear models. With a proposed two-step prior, the PI proved a general oracle inequality for posterior contraction under an abstract setting that allows model misspecification. The general result can be used to derive new results on optimal posterior contraction under many complex model settings including recent works for stochastic block model, graphon estimation and dictionary learning.</p>\n<p>In terms of Bayesian computation, the PI and his student Bumeng Zhuo study the computational complexity of a Metropolis-Hastings algorithm for Bayesian community detection. They first establish a posterior strong consistency result for a natural prior distribution on stochastic block models under the optimal signal-to-noise ratio condition in the literature. They then give a set of conditions that guarantee rapid mixing of a simple Metropolis-Hastings algorithm. The mixing time analysis is based on a careful study of posterior ratios and a canonical path argument to control the spectral gap of the Markov chain. With another student Youngseok Kim, the PI proposed a general algorithmic framework for Bayesian model selection. A spike-and-slab Laplacian prior is introduced to model the underlying structural assumption. Using the notion of effective resistance, they derive an EM-type algorithm with closed-form iterations to efficiently explore possible candidates for Bayesian model selection.</p>\n<p>The outcomes of the project have been published in journals and presented in various conferences.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2020<br>\n\t\t\t\t\tModified by: Chao&nbsp;Gao</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nBayesian analysis is a widely used technique in data science for estimation, prediction and interpretation. The new era of complex and big data imposes unprecedented challenges to Bayesian statistics. This project aims to address these new challenges from three different perspectives.\n\nIn terms of Bayesian theory, the PI studied convergence rates of variational posterior distributions for nonparametric and high-dimensional inference. The PI and his student Fengshuo Zhang formulate general conditions on prior, likelihood, and variational class that characterize the convergence rates. Under similar \"prior mass and testing\" conditions considered in the literature, the rate is found to be the sum of two terms. The first term stands for the convergence rate of the true posterior distribution, and the second term is contributed by the variational approximation error. The PI and his student Fengshuo Zhang also studied the convergence rates of empirical Bayes posterior distributions for nonparametric and high-dimensional inference. They show that as long as the hyperparameter set is discrete, the empirical Bayes posterior distribution induced by the maximum marginal likelihood estimator can be regarded as a variational approximation to a hierarchical Bayes posterior distribution. This connection between empirical Bayes and variational Bayes allows us to leverage the recent results in the variational Bayes literature, and directly obtains the convergence rates of empirical Bayes posterior distributions from a variational perspective.\n\nFor Bayesian modeling, high dimensional statistics deals with the challenge of extracting structured information from complex model settings. Compared with a large number of frequentist methodologies, there are rather few theoretically optimal Bayes methods for high dimensional models. The PI has studied a unified approach to both Bayes high dimensional statistics and Bayes nonparametrics in a general framework of structured linear models. With a proposed two-step prior, the PI proved a general oracle inequality for posterior contraction under an abstract setting that allows model misspecification. The general result can be used to derive new results on optimal posterior contraction under many complex model settings including recent works for stochastic block model, graphon estimation and dictionary learning.\n\nIn terms of Bayesian computation, the PI and his student Bumeng Zhuo study the computational complexity of a Metropolis-Hastings algorithm for Bayesian community detection. They first establish a posterior strong consistency result for a natural prior distribution on stochastic block models under the optimal signal-to-noise ratio condition in the literature. They then give a set of conditions that guarantee rapid mixing of a simple Metropolis-Hastings algorithm. The mixing time analysis is based on a careful study of posterior ratios and a canonical path argument to control the spectral gap of the Markov chain. With another student Youngseok Kim, the PI proposed a general algorithmic framework for Bayesian model selection. A spike-and-slab Laplacian prior is introduced to model the underlying structural assumption. Using the notion of effective resistance, they derive an EM-type algorithm with closed-form iterations to efficiently explore possible candidates for Bayesian model selection.\n\nThe outcomes of the project have been published in journals and presented in various conferences.\n\n\t\t\t\t\tLast Modified: 10/29/2020\n\n\t\t\t\t\tSubmitted by: Chao Gao"
 }
}