{
 "awd_id": "1737846",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Establishing a ground truth for focus placement in naturally-occurring speech",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 105894.0,
 "awd_amount": 121894.0,
 "awd_min_amd_letter_date": "2017-06-08",
 "awd_max_amd_letter_date": "2021-07-21",
 "awd_abstract_narration": "By emphasizing words acoustically, people can convey the information about which concepts they wish to contrast.  This feature of speech, known as focus, is pervasive in English, yet is inadequately modeled in state-of-the-art speech technologies.  The challenge, which this Early Grant for Exploratory Research addresses, is that it is often difficult to identify phonetic emphasis  independently of semantic contrast:  words whose meanings are focused are usually realized with increased acoustic prominence, but not all cases of increased acoustic prominence are due to focus.  The project is innovative in its use both of speech that has been recorded in a laboratory under controlled conditions, and also of speech that occurs naturally, such as in podcasts and videos.  Judgments of focus location in laboratory speech and in naturally-occurring speech are collected from ordinary, non-expert listeners using online crowd-sourcing.  Using the comparative construction (for example, \"He liked it better than I did\" or \"I like it better now than I did\") in which focus can be independently verified, computational procedures are developed to mimic the judgment of subjects who read but do not listen to the utterance being investigated.  The findings will inform research in speech synthesis and in automatic speech recognition.  Commercial applications may include aids for the deaf and hearing impaired, robot assistants for the elderly, language instruction and speech therapy.\r\n\r\nIn a previous proof-of-concept study, the researcher collected utterances of \"than I did\" in laboratory experiments and from transcribed podcasts available on the web.  Machine learning classifiers (using linear discriminant analysis and support vector machines) were trained to detect focus from acoustic features alone, including measures of fundamental frequency, duration and intensity.  Location of focus can be determined independently from prosody in the comparative construction by observing the presence or absence of co-reference between subjects in the main and comparative clauses.  This research generalizes that study to variations of the comparative with different pronouns and auxiliaries and also introduces updated methods of acoustic extraction and classification.  Then, a verification dataset is created in order to reject annotations from participants who annotate non-focal prominence or who mark focus location incorrectly.  Finally, classifiers are trained to detect focus on pronouns and auxiliaries in contexts other than the comparative, using the crowd-sourced annotation data to infer correct location of focus independently from prosody.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jonathan",
   "pi_last_name": "Howell",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jonathan Howell",
   "pi_email_addr": "howellj@mail.montclair.edu",
   "nsf_id": "000723718",
   "pi_start_date": "2017-06-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Montclair State University",
  "inst_street_address": "1 NORMAL AVE",
  "inst_street_address_2": "",
  "inst_city_name": "MONTCLAIR",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "9736556923",
  "inst_zip_code": "070431624",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "NJ11",
  "org_lgl_bus_name": "MONTCLAIR STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "CM4TTRKFCLF9"
 },
 "perf_inst": {
  "perf_inst_name": "Montclair State University",
  "perf_str_addr": "1 Normal Avenue",
  "perf_city_name": "Montclair",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "070431624",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "NJ11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 105894.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>By emphasizing words acoustically, people can convey the information about which concepts they wish to contrast. This feature of speech, known as focus, is pervasive in English: literally every utterance an English speaker makes includes at least one focused word or phrase.&nbsp; Yet focus is inadequately modeled in state-of-the-art speech technologies.&nbsp; One challenge is that focus cannot be predicted from the structure of a sentence alone.&nbsp; Focus is dependent on the context of the conversation, and the mental model of the speaker, among other factors.</p>\n<p>Clearly, speech technology cannot hope to read the mind of speakers and listeners. In this project, we pursued a next best solution, using the judgement of non-expert listeners as a \"ground truth\". We adopted a crowd-sourcing methodology developed by other researchers for all kinds of acoustic emphasis, and applied it specifically to focus, even when listeners could not need hear audio of the text.&nbsp; We applied it to naturally-occuring language (from podcasts), as well as language collected under controlled conditions in a laboratory.</p>\n<p>We anticipated that not all non-expert annotators would perform well on the task, and so we prepared a validation dataset using a grammatical construction in which focus can be reasonably predicted.&nbsp; In a comparative construction, we can predict focus by observing whether the two subjects refer to the same individual. For example, <em>He liked it better than I did</em> has different subjects so therefore <em>I</em> is focused;&nbsp; <em>I like it better now than I did</em> has the same subject, so <em>I </em>is not focused.&nbsp; The judgements of non-expert annotators who did not perform well with the comparative construction could therefore be excluded.</p>\n<p>We found that the performance of our best-performing non-expert annotators fell short of the performance by expert annotators published by other researchers. Unlike expert annotators, however, our non-expert annotators agreed at a statistically significant rate without the benefit of audio or visual representations of the acoustics. Moreover, crowd-sourced annotations are orders of magnitude less costly in time and resources.&nbsp; At the time of this report, we are exploring ways of using these non-expert listener annotation data to improve existing protocols for the automatic detection and prediction of focus.</p>\n<p>A total of 4 graduate students and 10 undergraduate had some involvement in the project, either paid or volunteer.&nbsp; These students, several from underreprented minority populations, had the opportunity to contribute to various aspects of the work, including experimental design and implementation, development of computational tools, statistical analysis, project management, linguistic annotation and data wrangling.&nbsp; All of these students have graduated or are on track to graduate, and graduates have successfully secured career and educational opportunities in fields of language and speech.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/03/2023<br>\n\t\t\t\t\tModified by: Jonathan&nbsp;Howell</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nBy emphasizing words acoustically, people can convey the information about which concepts they wish to contrast. This feature of speech, known as focus, is pervasive in English: literally every utterance an English speaker makes includes at least one focused word or phrase.  Yet focus is inadequately modeled in state-of-the-art speech technologies.  One challenge is that focus cannot be predicted from the structure of a sentence alone.  Focus is dependent on the context of the conversation, and the mental model of the speaker, among other factors.\n\nClearly, speech technology cannot hope to read the mind of speakers and listeners. In this project, we pursued a next best solution, using the judgement of non-expert listeners as a \"ground truth\". We adopted a crowd-sourcing methodology developed by other researchers for all kinds of acoustic emphasis, and applied it specifically to focus, even when listeners could not need hear audio of the text.  We applied it to naturally-occuring language (from podcasts), as well as language collected under controlled conditions in a laboratory.\n\nWe anticipated that not all non-expert annotators would perform well on the task, and so we prepared a validation dataset using a grammatical construction in which focus can be reasonably predicted.  In a comparative construction, we can predict focus by observing whether the two subjects refer to the same individual. For example, He liked it better than I did has different subjects so therefore I is focused;  I like it better now than I did has the same subject, so I is not focused.  The judgements of non-expert annotators who did not perform well with the comparative construction could therefore be excluded.\n\nWe found that the performance of our best-performing non-expert annotators fell short of the performance by expert annotators published by other researchers. Unlike expert annotators, however, our non-expert annotators agreed at a statistically significant rate without the benefit of audio or visual representations of the acoustics. Moreover, crowd-sourced annotations are orders of magnitude less costly in time and resources.  At the time of this report, we are exploring ways of using these non-expert listener annotation data to improve existing protocols for the automatic detection and prediction of focus.\n\nA total of 4 graduate students and 10 undergraduate had some involvement in the project, either paid or volunteer.  These students, several from underreprented minority populations, had the opportunity to contribute to various aspects of the work, including experimental design and implementation, development of computational tools, statistical analysis, project management, linguistic annotation and data wrangling.  All of these students have graduated or are on track to graduate, and graduates have successfully secured career and educational opportunities in fields of language and speech.\n\n\t\t\t\t\tLast Modified: 01/03/2023\n\n\t\t\t\t\tSubmitted by: Jonathan Howell"
 }
}