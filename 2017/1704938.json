{
 "awd_id": "1704938",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Incorporating Biologically-Motivated Circuit Motifs into Large-Scale Deep Neural Network Models of the Brain",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 525000.0,
 "awd_amount": 525000.0,
 "awd_min_amd_letter_date": "2017-08-16",
 "awd_max_amd_letter_date": "2017-08-16",
 "awd_abstract_narration": "This project studies the effects of incorporating, into deep neural networks for visual processing, several heretofore unincorporated features of biological visual cortical circuits. Deep neural networks are artificial circuits loosely inspired by the brain's cerebral cortex. Their abilities to solve complex problems, such as recognizing objects in visual scenes, have revolutionized artificial intelligence and machine learning in recent years. The hierarchy of layers in a deep network trained for visual object recognition also provides the best existing models of the hierarchy of areas in the visual cortex implicated in object recognition (the \"ventral stream\"). This project seeks to understand whether and how incorporating additional features of brain circuits may (1) improve machine learning performance, particularly on tasks that are more challenging than those typically studied; and (2) yield improved models of visual cortex. Improving the performance of deep networks would yield great benefits across wide swaths of society and industry that are impacted by advances in artificial intelligence. Improved models of visual cortex will advance understanding of cortical function, which may lead to significant further benefits for understanding normal mental functioning and perception and their potential enhancement, as well as mental illness and perceptual and cognitive deficits. \r\n\r\nDeep networks currently achieve their success using almost purely feedforward processing. Yet the visual cortical ventral stream that helped inspire deep networks also uses massive recurrent processing within each area as well as feedback connections from higher areas to lower areas and \"bypass\" connections from lower areas to areas multiple steps higher in the hierarchy. Deep networks also use \"neurons\" that can either excite or inhibit different neurons that they project to, whereas biological neurons are exclusively excitatory or inhibitory. This project will incorporate feedback and bypass connections into deep networks, as well as local recurrent processing in networks of separate excitatory and inhibitory neurons. Recent work by the investigators has shown how local recurrent processing explains a number of nonlinear visual cortical operations often summarized as \"normalization.\" Simple forms of normalization currently used in deep networks maintain activities in an appropriate dynamic range, but the biological forms of normalization involve interactions between different stimulus features and locations in determining neural responses, which may have important computational roles e.g. in parsing visual scenes. The performance of deep networks incorporating these features will be assayed on a variety of visual tasks and as models of ventral stream neural data and human psychophysical data, and compared to performance of existing deep net models.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kenneth",
   "pi_last_name": "Miller",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Kenneth D Miller",
   "pi_email_addr": "kdm2103@columbia.edu",
   "nsf_id": "000115497",
   "pi_start_date": "2017-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "3220 Broadway",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100323702",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 525000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project we studied the interface between the neuroscience of the circuits of cerebral cortex that perform vision, and the ?deep networks? trained for visual object recognition that are at the forefront of artificial intelligence and computer vision. We have three main sets of results:<br />(1) We studied how incorporation of features of biological circuits can impact the structure and performance of deep networks. We first studied the effects of incorporating separate excitatory and inhibitory neurons, as found biologically. We found that training networks for object recognition while enforcing this separation and including several other known biological features ? rectified power-law neuronal input/output functions, varying contrasts, output from one layer to the next only from excitatory cells, cross-channel interactions of limited range rather than all-to-all ? led to a network that shared many additional features with cortical circuits, suggesting these biological features that emerged in training have a functional importance. We then focused on a particular computational operation, divisive normalization, that seems to characterize visual cortical circuits and in particular characterizes the behavior of a model we have developed of those circuits, the ?stabilized supralinear network? (SSN). We found that incorporation of divisive normalization improved the performance of deep networks on object recognition by a few percentage points, and caused a number of interesting changes in representations whose causal role in the improved performance needs further investigation. These changes in representation included increased low-spatial-frequency power in first-layer filters, decorrelation or anticorrelation of different channels within a layer, a decreased capacity for separating multiple objects in earlier layers that yields an increased capacity in later layers, changes in sparseness of activity patterns, and improved robustness to corruption of images by Gaussian white noise.</p>\n<p>(2) We used deep networks as a model of biological attention, to address how and why attention may improve behavioral performance. We designed a binary detection task ? presence or absence of a given object category in an image involving a number of categories ? that was difficult for the deep networks to perform, allowing room for attention to improve performance. We modeled attention to a category as an enhancement of the activity of neurons that responded best to that category. This could improve performance by increasing true positive detections or reduce performance by increasing false positive detections. We found that it improved performance over a reasonable range of attention strengths, indicating that the false negatives were closer to detection than true negatives, so that attention could preferentially transform false rather than true negatives into positives. However, we found that attention in early layers was much less effective than in later layers. In contrast, enhancing neural activities according to their influence on the final categorization ? their gradient for detecting the proper category ? worked well at all layers. This raised the question whether biological attention might be ?smart? enough to attend according to final effect rather than to tuning, and we designed an experiment that could be performed to determine the answer.&nbsp;</p>\n<p>(3) We incorporated biological circuit motifs into the deep network models of attention, showing that they could improve behavioral performance similarly to more artificial models of attention. Rather than increase by hand the activity of neurons that were most selective for the given category, we inserted an SSN circuit and added excitatory input to the excitatory cells representing the given category. This had the same effect as manipulating activity by hand, showing that a biological implementation of attention could enhance behavioral performance.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/31/2022<br>\n\t\t\t\t\tModified by: Kenneth&nbsp;D&nbsp;Miller</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this project we studied the interface between the neuroscience of the circuits of cerebral cortex that perform vision, and the ?deep networks? trained for visual object recognition that are at the forefront of artificial intelligence and computer vision. We have three main sets of results:\n(1) We studied how incorporation of features of biological circuits can impact the structure and performance of deep networks. We first studied the effects of incorporating separate excitatory and inhibitory neurons, as found biologically. We found that training networks for object recognition while enforcing this separation and including several other known biological features ? rectified power-law neuronal input/output functions, varying contrasts, output from one layer to the next only from excitatory cells, cross-channel interactions of limited range rather than all-to-all ? led to a network that shared many additional features with cortical circuits, suggesting these biological features that emerged in training have a functional importance. We then focused on a particular computational operation, divisive normalization, that seems to characterize visual cortical circuits and in particular characterizes the behavior of a model we have developed of those circuits, the ?stabilized supralinear network? (SSN). We found that incorporation of divisive normalization improved the performance of deep networks on object recognition by a few percentage points, and caused a number of interesting changes in representations whose causal role in the improved performance needs further investigation. These changes in representation included increased low-spatial-frequency power in first-layer filters, decorrelation or anticorrelation of different channels within a layer, a decreased capacity for separating multiple objects in earlier layers that yields an increased capacity in later layers, changes in sparseness of activity patterns, and improved robustness to corruption of images by Gaussian white noise.\n\n(2) We used deep networks as a model of biological attention, to address how and why attention may improve behavioral performance. We designed a binary detection task ? presence or absence of a given object category in an image involving a number of categories ? that was difficult for the deep networks to perform, allowing room for attention to improve performance. We modeled attention to a category as an enhancement of the activity of neurons that responded best to that category. This could improve performance by increasing true positive detections or reduce performance by increasing false positive detections. We found that it improved performance over a reasonable range of attention strengths, indicating that the false negatives were closer to detection than true negatives, so that attention could preferentially transform false rather than true negatives into positives. However, we found that attention in early layers was much less effective than in later layers. In contrast, enhancing neural activities according to their influence on the final categorization ? their gradient for detecting the proper category ? worked well at all layers. This raised the question whether biological attention might be ?smart? enough to attend according to final effect rather than to tuning, and we designed an experiment that could be performed to determine the answer. \n\n(3) We incorporated biological circuit motifs into the deep network models of attention, showing that they could improve behavioral performance similarly to more artificial models of attention. Rather than increase by hand the activity of neurons that were most selective for the given category, we inserted an SSN circuit and added excitatory input to the excitatory cells representing the given category. This had the same effect as manipulating activity by hand, showing that a biological implementation of attention could enhance behavioral performance.\n\n \n\n\t\t\t\t\tLast Modified: 01/31/2022\n\n\t\t\t\t\tSubmitted by: Kenneth D Miller"
 }
}