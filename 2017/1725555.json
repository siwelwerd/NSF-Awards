{
 "awd_id": "1725555",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: Collaborative Research: Asynchronous, Parallel-Adaptive Solution of Extreme Multiscale Problems in Seismology",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Seung-Jong Park",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 178680.0,
 "awd_amount": 178680.0,
 "awd_min_amd_letter_date": "2017-08-30",
 "awd_max_amd_letter_date": "2021-06-08",
 "awd_abstract_narration": "This collaborative project among scientists at the University of Illinois and the University of Tennessee will develop parallel software for efficient earthquake simulations on exascale supercomputers. State-of-the-art systems now resolve seismic response at frequencies up to 1 Hz, but design engineers require resolution up to 10 Hz. Synchronization barriers may limit performance on exascale systems. This project will develop scalable, barrier-free asynchronous simulation tools and space-time adaptive meshing to meet the seismic resolution requirements on exascale platforms. It will develop improved fault-gouge physics models and extend asynchronous hyperbolic solvers to address elliptic (and eventually parabolic) systems. These extensions will enable the first regional, full-cycle seismic simulations, covering fast earthquake events and much slower crustal motion between earthquakes, as well as the use of asynchronous exascale solvers in most PDE-based scientific and engineering applications. The asynchronous solution technology will support more reliable earthquake hazard maps and the design of safer, more economical earthquake-resistant buildings and infrastructure. In view of its broad applicability, the unprecedented simulation power afforded by this research could trigger numerous breakthroughs in the commercial and defense sectors. Four graduate research assistants will receive cross-disciplinary training, and undergraduate students will participate through the National Center for Supercomputing Application?s SPIN (Students Pushing INnovation) program.\r\n\r\n\r\nThis collaborative project among scientists at the University of Illinois and the University of Tennessee will develop parallel software for efficient earthquake simulations on exascale supercomputers. State-of-the-art systems now resolve seismic response at frequencies up to 1 Hz, but design engineers require resolution up to 10 Hz. Synchronization barriers and load balancing across subdomains may limit performance on exascale systems. This project will replace the standard bulk synchronous parallel model and Domain Decomposition Method (DDM) with scalable, barrier-free asynchronous solvers and space-time adaptive meshing without DDM to meet the seismic resolution requirements on exascale platforms. It will develop Shear Transition Zone models for fault-gouge physics and use pseudo-time methods to extend asynchronous hyperbolic solvers to address elliptic (and eventually parabolic) systems. These extensions will enable the first regional, full-cycle seismic simulations, covering fast earthquake events and much slower crustal motion between earthquakes, as well as the use of asynchronous exascale solvers in most PDE-based scientific and engineering applications. The asynchronous solution technology will support more reliable earthquake hazard maps and the design of safer, more economical earthquake-resistant buildings and infrastructure. In view of its broad applicability, the unprecedented simulation power afforded by this research could trigger numerous breakthroughs in the commercial and defense sectors. Four graduate research assistants will receive cross-disciplinary training, and undergraduate students will participate through the National Center for Supercomputing Application?s SPIN (Students Pushing INnovation) program",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Reza",
   "pi_last_name": "Abedi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Reza Abedi",
   "pi_email_addr": "rabedi@utsi.edu",
   "nsf_id": "000677759",
   "pi_start_date": "2017-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Space Institute",
  "inst_street_address": "411 BH GOETHERT PKWY",
  "inst_street_address_2": "",
  "inst_city_name": "TULLAHOMA",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "9313937212",
  "inst_zip_code": "373889700",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "TN04",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "R7UDMNZJNFU5",
  "org_uei_num": "R7UDMNZJNFU5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Space Institute",
  "perf_str_addr": "411 B.H. Goethert Parkway",
  "perf_city_name": "Tullahoma",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "373889700",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "TN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 178680.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Many problems of interest in science, engineering, and medicine involve wave propagation: ranging from gravitational waves emitted by black-hole collisions at astronomical scales to tsunami waves at global scales to siesmic waves at continental scales to mechanical waves causing concussions and traumatic brain injuries at human scales. Hyperbolic systems are a common class of partial differential equations that model these and all other wave propagation phenomena which also share fundamental physical properties. These include the <em>causallity principal </em>which requires that only earlier events can influence a later event a constraint that waves must propagate at finite (non-infinite) speeds. Given points A and B separated by a finite distance in space, the finite wave-speed constrian implies that a wave emanating from point A cannot reach or influence point B until a definite time interval has passed. Accordingly, the response at A at a given time cannot influence the response at B at the same time. The finite wave-speed and causality constraints are built into the mathematical structure of hyperbolic systems</p>\n<p>Numerical simulations play an important role in advancing scientific research, optimizing engineering designs, and improving health outcomes in applications involving wave propagation. Many of these applications encompass extreme ranges of temporal and length scales that exceed the capabilities of the most advanced computational methods running on the fastest high-performance computing platforms. For example, a seismic fault system can range over thousands of kilometers while the diameter of an earthquake process zone (the moving region in which intact rock fails as a rupture advances) is on the order of a centimeter. Resolutions down to millimeter scale may be required to capture the critical details of response within the process zone that ultimately determine macroscopic rupture dynamics. Wave propagation speeds in rock imply a similar extreme range of time scales. Since the most advanced models running on today's largest high-performance computing platforms struggle to reach meter-scale spatial resolution, a thousandfold increase in spatial resolution (and a much larger increase in computational problems size) is required to generate predictive computational models based on measurable properties of rock. Extensions of conventional numerical methods are unlikely to attain this goal in the foreseeable future.</p>\n<p>With few exceptions, most existing numerical methods for solving hyperbolic systems begin with a grid that covers the problem's spatial domain at the initial time and assigns solution degrees of freedom to the spatial mesh's grid points or cells. Starting with prescribed initial data, the solver advances the solution through a series of uniform time steps. This synchronous time-marching strategy couples the solution at each time between all points in the domain ?? an expensive artificial coupling that is not required by either the physics of wave propagation or the mathematics of hyperbolic systems. The ubiquitous Domain Decomposition Method (DDM) attempts to mitigate the communication cost of this global coupling by decomposing the spatial domain into coherent supdomains that can be held in shared memory.</p>\n<p>We replaced time marching and parallelization by the DDM with a powerful hyperbolic solution scheme called the causal Spacetime Discontinuous Galerkin (cSDG) method. cSDG solvers construct unstructured spacetime meshes that incorporate the causality principal and local wave-speed data. cSDG solutions advance asynchronously via localized problems defined on small spacetime meshes called <em>patches</em>. Localized adaptive spacetime meshing executes at the same patch-wise granularity to produce intense dynamic refinement that delivers much higher resolution than previous adaptive refinement methods. Although patch-wise solutions and adaptive meshing offer a natural structure for parallelization, previous data and load-balancing technologies cannot keep pace with cSDG dynamic refinement. Thus, adaptive cSDG solvers were previously restricted to serial implementations. The lack of any causal adaptive meshing</p>\n<p>scheme for 3d x time (three space dimensions + time ==&gt; 4-dimensional meshes) restricts previous cSDG solvers to 2d x time.</p>\n<p>We developed <em>ParaSDG</em>, the first parallel-adaptive cSDG solver with capabilites for causal adaptive meshing in up to 3d x time, to remove these restrictions. Several theoretical and algoritmic breakthroughs supported this effort:</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A novel software framework for distributed, parallel-adaptive cSDG solvers in which patch-wise meshing/solution processes execute as asynchronous, isolated, and embarrassingly parallel tasks</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; New probabilistic schemes for data and load-balancing that execute at the same granularity and keep pace with localized cSDG adaptive meshing</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advances in adaptive causal meshing led to our creation of the first working platform for adaptive causal meshing in up to 3d x time</p>\n<p>We implemented a branched-fault seismic rupture model in a serial cSDG code and pushed intense adaptive refinement to achieve maximum resolution in the 1-10mm range. This revealed a new dipole-like stress feature at the branch point that can trigger fault opening and much higher slip rates with potential for greater damage to structures along the fault</p>\n<p>ParaSDG is a general-purpose hyperbolic solver that can deliver new high-resolution capabilities to numerous applications in science, engineering, and healthcare</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/24/2023<br>\n\t\t\t\t\tModified by: Reza&nbsp;Abedi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany problems of interest in science, engineering, and medicine involve wave propagation: ranging from gravitational waves emitted by black-hole collisions at astronomical scales to tsunami waves at global scales to siesmic waves at continental scales to mechanical waves causing concussions and traumatic brain injuries at human scales. Hyperbolic systems are a common class of partial differential equations that model these and all other wave propagation phenomena which also share fundamental physical properties. These include the causallity principal which requires that only earlier events can influence a later event a constraint that waves must propagate at finite (non-infinite) speeds. Given points A and B separated by a finite distance in space, the finite wave-speed constrian implies that a wave emanating from point A cannot reach or influence point B until a definite time interval has passed. Accordingly, the response at A at a given time cannot influence the response at B at the same time. The finite wave-speed and causality constraints are built into the mathematical structure of hyperbolic systems\n\nNumerical simulations play an important role in advancing scientific research, optimizing engineering designs, and improving health outcomes in applications involving wave propagation. Many of these applications encompass extreme ranges of temporal and length scales that exceed the capabilities of the most advanced computational methods running on the fastest high-performance computing platforms. For example, a seismic fault system can range over thousands of kilometers while the diameter of an earthquake process zone (the moving region in which intact rock fails as a rupture advances) is on the order of a centimeter. Resolutions down to millimeter scale may be required to capture the critical details of response within the process zone that ultimately determine macroscopic rupture dynamics. Wave propagation speeds in rock imply a similar extreme range of time scales. Since the most advanced models running on today's largest high-performance computing platforms struggle to reach meter-scale spatial resolution, a thousandfold increase in spatial resolution (and a much larger increase in computational problems size) is required to generate predictive computational models based on measurable properties of rock. Extensions of conventional numerical methods are unlikely to attain this goal in the foreseeable future.\n\nWith few exceptions, most existing numerical methods for solving hyperbolic systems begin with a grid that covers the problem's spatial domain at the initial time and assigns solution degrees of freedom to the spatial mesh's grid points or cells. Starting with prescribed initial data, the solver advances the solution through a series of uniform time steps. This synchronous time-marching strategy couples the solution at each time between all points in the domain ?? an expensive artificial coupling that is not required by either the physics of wave propagation or the mathematics of hyperbolic systems. The ubiquitous Domain Decomposition Method (DDM) attempts to mitigate the communication cost of this global coupling by decomposing the spatial domain into coherent supdomains that can be held in shared memory.\n\nWe replaced time marching and parallelization by the DDM with a powerful hyperbolic solution scheme called the causal Spacetime Discontinuous Galerkin (cSDG) method. cSDG solvers construct unstructured spacetime meshes that incorporate the causality principal and local wave-speed data. cSDG solutions advance asynchronously via localized problems defined on small spacetime meshes called patches. Localized adaptive spacetime meshing executes at the same patch-wise granularity to produce intense dynamic refinement that delivers much higher resolution than previous adaptive refinement methods. Although patch-wise solutions and adaptive meshing offer a natural structure for parallelization, previous data and load-balancing technologies cannot keep pace with cSDG dynamic refinement. Thus, adaptive cSDG solvers were previously restricted to serial implementations. The lack of any causal adaptive meshing\n\nscheme for 3d x time (three space dimensions + time ==&gt; 4-dimensional meshes) restricts previous cSDG solvers to 2d x time.\n\nWe developed ParaSDG, the first parallel-adaptive cSDG solver with capabilites for causal adaptive meshing in up to 3d x time, to remove these restrictions. Several theoretical and algoritmic breakthroughs supported this effort:\n\n-        A novel software framework for distributed, parallel-adaptive cSDG solvers in which patch-wise meshing/solution processes execute as asynchronous, isolated, and embarrassingly parallel tasks\n\n-        New probabilistic schemes for data and load-balancing that execute at the same granularity and keep pace with localized cSDG adaptive meshing\n\n-        Advances in adaptive causal meshing led to our creation of the first working platform for adaptive causal meshing in up to 3d x time\n\nWe implemented a branched-fault seismic rupture model in a serial cSDG code and pushed intense adaptive refinement to achieve maximum resolution in the 1-10mm range. This revealed a new dipole-like stress feature at the branch point that can trigger fault opening and much higher slip rates with potential for greater damage to structures along the fault\n\nParaSDG is a general-purpose hyperbolic solver that can deliver new high-resolution capabilities to numerous applications in science, engineering, and healthcare\n\n \n\n\t\t\t\t\tLast Modified: 05/24/2023\n\n\t\t\t\t\tSubmitted by: Reza Abedi"
 }
}