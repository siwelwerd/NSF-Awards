{
 "awd_id": "1709727",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CDS&E: Space-Time Parallel Algorithms for Solving PDE-Constrained Optimization Problems",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rob Beverly",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2017-08-24",
 "awd_max_amd_letter_date": "2017-08-24",
 "awd_abstract_narration": "Many fields of science and engineering, from atmospheric science to aeronautics, and from material science to cosmology, rely on complex models built from \"first principles\" in order to study the phenomena of interest; the fundamental physical laws are often described by time-dependent partial differential equations (PDEs). These models are implemented in complex software that performs vast amount of computations and processes large data sets in order to simulate the physical reality. PDE-based models typically run long times on parallel computers, using large numbers of cores.  A central problem in these fields of science and engineering is that of optimizing the system of interest according to specific design criteria. For example, in aeronautics, one wants not only to simulate the flight of an airplane, but also to design the best aircraft using shape optimization. In numerical weather prediction, one needs not only simulate the evolution of the atmosphere, but also to optimally utilize the information coming from satellite, aircraft, and ground based measurements in order to keep the forecasts accurate. All these applications seek to optimize systems governed by PDEs. This is an extremely challenging quest, since solving a PDE-constrained optimization problem is one-two orders of magnitude costlier than the underlying forward PDE simulation. There is considerable need for novel highly-parallel solution methodologies. This project develops the algorithmic infrastructure to support large-scale optimization of systems governed by time-dependent PDEs. New ideas will be used to unravel and exploit the inherent parallelism. First, we seek to parallelize the computations in both space and time. The space is divided in subdomains, the time in subintervals, and sub-models on each time subinterval and on each spatial subdomain are run concurrently on different sets of processors. Next, to further increase computational effectiveness, we will build surrogate models, i.e., inexpensive approximate models that capture the main dynamical characteristics of the full PDE-based models. Parallel construction of new surrogates is proposed using local-in-space-and-time information. The main idea is to perform optimization using the inexpensive surrogate models, transferring the improved design to the full PDE-model, re-computing a surrogate for the new configuration, and iterating. Enormous computational savings can be realized this way. Lastly, the new algorithms will be laid on solid theoretical foundations, and will be applied to speed up the incorporation of measurement data in a numerical weather prediction model. The tools developed in this project will enable leap developments in many fields in science and engineering where time-dependent PDE-constrained optimization problems are central. Important examples include aircraft shape optimization, seismic imaging, medical imaging, optimal control of fabrication processes, and inverse problems. The project will directly train one doctoral student and one postdoctoral researcher, will involve undergraduates in research, will develop graduate level educational materials, and will attract students from under-represented groups in parallel computing and large-scale simulations of the physical world.\r\n\r\n\r\nThis project develops the algorithmic infrastructure to support large-scale optimization of systems governed by time-dependent partial differential equations (PDEs). PDE optimization problems are central to many fields in science and engineering.  They are considerably more complex, and costlier to solve, than the underlying PDE simulations. There is considerable need for highly-parallel solution methodologies. In order to address this, the project proposes a space-time parallel formalism, and new reduced order modeling techniques, that have the potential to speed up the PDE-constrained optimization solution process by several orders of magnitude. (1) Intellectual merit: This work develops a space-time parallel formalism for the solution of large scale PDE-constrained optimization problems. The space is divided in subdomains, the time in subintervals, and the forward and adjoint models are run in parallel on each time subinterval and on each spatial subdomain. Solution continuity equations are imposed more stringently as the optimization process advances. This work formulates reduced-order PDE-constrained optimization problems using local-in-space-and-time reduced order models. Such models can represent the system dynamics much better than traditional global approaches. Moreover, both the off-line construction of local reduced order models and the on-line reduced order simulations can be carried out concurrently on each time subinterval and on each spatial subdomain, resulting in considerable speed-ups. A trust region framework is employed for provably convergent reduced order optimization algorithms. The new methodologies are demonstrated on large atmospheric data assimilation applications. (2) Broader impact: The tools developed in this project will enable leap developments in many fields in science and engineering where time-dependent PDE-constrained optimization problems are central. Important examples include aircraft shape optimization, seismic imaging, medical imaging, optimal control of fabrication processes, and inverse problems. One doctoral student and one postdoctoral researcher are directly trained in PDE-constrained optimization, reduced order modeling, high performance computing, and science applications. Graduate level educational materials are developed.\r\n\r\n\r\nThis project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Adrian",
   "pi_last_name": "Sandu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Adrian Sandu",
   "pi_email_addr": "sandu@cs.vt.edu",
   "nsf_id": "000388914",
   "pi_start_date": "2017-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "2202 Kraft Drive",
  "perf_city_name": "Blacksburg",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "240600001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "VA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  },
  {
   "pgm_ele_code": "808400",
   "pgm_ele_name": "CDS&E"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8084",
   "pgm_ref_txt": "CDS&E"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project is to develop the algorithmic infrastructure needed for the optimization and the solution of inverse problems with large-scale time-dependent dynamical systems.</p>\n<p>The project has advanced the field of statistical inverse problems. Data assimilation, a subset of statistical inverse problems, seeks to fuse information from measurements of the physical world and computer models to learn more about a real system of interest. Two families of computational methods are being widely used: ensemble Kalman filters and variational approaches. This project has developed the new multifidelity ensemble Kalman framework that allows to perform inference (data assimilation) using not just one model, but a hierarchy of models of different resolutions, coupled in a mathematically coherent way. A small ensemble of expensive high-fidelity model runs is accompanied by a large ensemble of inexpensive low fidelity model runs, resulting in an increase of analysis quality and a decrease in the overall computational time. Other novel approaches developed in the project include linear and nonlinear ensemble filters based on shrinkage covariance approximations, and a variational Fokker Planck particle filter based on a dynamical system approach. These developments have advanced the field of data assimilation with potential impact on fields such as numerical weather prediction and oceanography, and in general the field of statistical inverse problems where the constraints are posed by dynamical models.</p>\n<p>Variational approaches for optimization and inference with&nbsp;large-scale time-dependent dynamical systems require adjoint models.&nbsp;The project developed discrete adjoint methodologies for sensitivity analysis and optimization of multiphysics systems in the formalism of general structure additive Runge-Kutta methods. We investigated discrete adjoints of exponential methods, and have applied methods of this family to compute gradients in an optimization for parameter estimation problems. We also developed direct and adjoint sensitivity analysis algorithms for the optimization of dynamical systems with non-smooth trajectories, such as mechanical systems undergoing impact events.</p>\n<p>The project has advanced the use of machine learning techniques to aid large scale computing and the solution of inverse problems.&nbsp;&nbsp;We developed novel machine learning approaches to analyze structural (model-form) uncertainty in physical models. This type of uncertainty is due to physical processes in nature that are not fully captured by the model. Our approach is construct dynamical models of model errors based on the discrepancies between model solutions and measurements of the physical reality. We studied machine learning methods to guide the configuration of multi-physics simulations such as to decrease forecast errors, and to tune ensemble-based data assimilation algorithms (localization radii). We developed the first machine learning approach to aid the construction of local parametric reduced order models, and developed nonlinear model order reduction technique through physics-informed convolutional autoencoders, which could be used as surrogates in large scale optimization and inversion.&nbsp;&nbsp;We developed the first machine learning approaches to adaptive covariance localization in the context of inverse problems such as data assimilation.&nbsp;&nbsp;We constructed basis-agnostic polynomial chaos expansions via a modified neural network architecture. This approach learns the form of the polynomial bases together with the base coefficients from data, unlike the classical approach where the basis is predefined and the coefficients are learned.</p>\n<p>&nbsp;Software developed includes&nbsp;DATES, the highly-extensible data assimilation testing suite, a Python package that implements methodologies for the solution of inverse problems, and DATools, a Matlab package counterpart. The project helped train four doctoral students and two master of science students.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/26/2022<br>\n\t\t\t\t\tModified by: Adrian&nbsp;Sandu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project is to develop the algorithmic infrastructure needed for the optimization and the solution of inverse problems with large-scale time-dependent dynamical systems.\n\nThe project has advanced the field of statistical inverse problems. Data assimilation, a subset of statistical inverse problems, seeks to fuse information from measurements of the physical world and computer models to learn more about a real system of interest. Two families of computational methods are being widely used: ensemble Kalman filters and variational approaches. This project has developed the new multifidelity ensemble Kalman framework that allows to perform inference (data assimilation) using not just one model, but a hierarchy of models of different resolutions, coupled in a mathematically coherent way. A small ensemble of expensive high-fidelity model runs is accompanied by a large ensemble of inexpensive low fidelity model runs, resulting in an increase of analysis quality and a decrease in the overall computational time. Other novel approaches developed in the project include linear and nonlinear ensemble filters based on shrinkage covariance approximations, and a variational Fokker Planck particle filter based on a dynamical system approach. These developments have advanced the field of data assimilation with potential impact on fields such as numerical weather prediction and oceanography, and in general the field of statistical inverse problems where the constraints are posed by dynamical models.\n\nVariational approaches for optimization and inference with large-scale time-dependent dynamical systems require adjoint models. The project developed discrete adjoint methodologies for sensitivity analysis and optimization of multiphysics systems in the formalism of general structure additive Runge-Kutta methods. We investigated discrete adjoints of exponential methods, and have applied methods of this family to compute gradients in an optimization for parameter estimation problems. We also developed direct and adjoint sensitivity analysis algorithms for the optimization of dynamical systems with non-smooth trajectories, such as mechanical systems undergoing impact events.\n\nThe project has advanced the use of machine learning techniques to aid large scale computing and the solution of inverse problems.  We developed novel machine learning approaches to analyze structural (model-form) uncertainty in physical models. This type of uncertainty is due to physical processes in nature that are not fully captured by the model. Our approach is construct dynamical models of model errors based on the discrepancies between model solutions and measurements of the physical reality. We studied machine learning methods to guide the configuration of multi-physics simulations such as to decrease forecast errors, and to tune ensemble-based data assimilation algorithms (localization radii). We developed the first machine learning approach to aid the construction of local parametric reduced order models, and developed nonlinear model order reduction technique through physics-informed convolutional autoencoders, which could be used as surrogates in large scale optimization and inversion.  We developed the first machine learning approaches to adaptive covariance localization in the context of inverse problems such as data assimilation.  We constructed basis-agnostic polynomial chaos expansions via a modified neural network architecture. This approach learns the form of the polynomial bases together with the base coefficients from data, unlike the classical approach where the basis is predefined and the coefficients are learned.\n\n Software developed includes DATES, the highly-extensible data assimilation testing suite, a Python package that implements methodologies for the solution of inverse problems, and DATools, a Matlab package counterpart. The project helped train four doctoral students and two master of science students.\n\n \n\n\t\t\t\t\tLast Modified: 04/26/2022\n\n\t\t\t\t\tSubmitted by: Adrian Sandu"
 }
}