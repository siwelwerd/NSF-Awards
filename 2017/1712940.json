{
 "awd_id": "1712940",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:   Higher-Order Asymptotics and Accurate Inference for Post-Selection",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Pena Edsel",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 79955.0,
 "awd_amount": 79955.0,
 "awd_min_amd_letter_date": "2017-08-02",
 "awd_max_amd_letter_date": "2017-08-02",
 "awd_abstract_narration": "Many statistical analyses utilize a model selection procedure. Perhaps the most common model selection problem is that of variable selection in linear regression. Principled motivations for selection include the desire for interpretability, prevention of over-fitting, and concerns about statistical power. A practical motivation arises when the data are high-dimensional, with more explanatory variables than observations. Relevant applications span the entire domain of scientific inquiry, from neuroscience, medicine and physics, to economics, sociology, and psychology. A large catalogue of variable selection procedures is now available, and the statistics community has turned its focus to the question of inference after selection. Standard methods of statistical inference are no longer valid when the same data are used to both select a model and make inferences about that model. It is fundamentally important to have accurate post-selection inference procedures that are also powerful enough to detect observed departures from scientific hypotheses and that avoid the strong distributional assumptions needed for exact inference in finite samples. This research aims to develop post-selection inference methodology that is both accurate and powerful, with particular emphasis on reducing statistical errors that depend on the sample size. \r\n\r\nThe goal of this project is to further understanding of the asymptotic theory of post-selection inference, particularly selective inference based on the CovTest and truncated Gaussian (TG) statistic, as well as simultaneous inference using the post-selection intervals (PoSI) procedure. The first two procedures can be motivated by selective error control, i.e., error control for the selected model parameters. The PoSI method seeks to control family-wise error rates for all possible sub-model parameters. While these procedures yield valid post-selection inference, without strong assumptions they are particularly vulnerable to the effects of violation of key assumptions in the realistic setting of small to moderate sample sizes, such as overly-conservative or inaccurate confidence intervals, and low power. In this project, asymptotic expansions, saddle-point approximations, the bootstrap, and related techniques from higher-order asymptotics will be employed to improve accuracy and power for these post-selection inference procedures.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Todd",
   "pi_last_name": "Kuffner",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Todd A Kuffner",
   "pi_email_addr": "kuffner@math.wustl.edu",
   "nsf_id": "000708162",
   "pi_start_date": "2017-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington University",
  "inst_street_address": "1 BROOKINGS DR",
  "inst_street_address_2": "",
  "inst_city_name": "SAINT LOUIS",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "3147474134",
  "inst_zip_code": "631304862",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MO01",
  "org_lgl_bus_name": "WASHINGTON UNIVERSITY, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "L6NFUM28LQM5"
 },
 "perf_inst": {
  "perf_inst_name": "Washington University",
  "perf_str_addr": "1 Brookings Drive",
  "perf_city_name": "Saint Louis",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "631304899",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 79955.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project produced numerous published papers, and also supported a Ph.D. student who is now a postdoctoral researcher. The results of this project also led to the creation of a new course on post-selection inference, as well as incorporation of post-selection inference material into a statistical learning course which I teach each year.&nbsp;</p>\n<p>Higher-order asymptotics is concerned with studying accuracy and sources of error in distributional approximations, with the goal of obtaining more accurate inferences in the settings of small or moderate sample sizes. Kolassa &amp; Kuffner (2020) solves an open problem in Bayesian inference regarding the accuracy of approxiimate inferences, as well as providing the first known results concerning the orders of magnitude of posterior cumulants, which enables a new set of tools to be used in studying and designing accurate Bayesian inference procedures, post-model-selection or otherwise.&nbsp;</p>\n<p>Another paper provides a insight into the connections between overfitting when performing variable selection, and the effects on subsequent uncertainty quantification.&nbsp;</p>\n<p>Kuffner &amp; Young (2018) elucidates the connections between selective inference and Fisherian conditional inference, which is of great value for understanding the foundations of statistical reasoning in the post-model-selection regime.&nbsp;</p>\n<p>Other papers supported by this grant are aimed at practitioners, providing clear presentations, examples, and solutions for the problem of inference after variable selection in linear regression.&nbsp;</p>\n<p>The results of this project further our understanding of the effects of variable selection on subsequent inference, and will be useful to other researchers as well as any practitioner concerned with accurate inference after variable selection.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/10/2023<br>\n\t\t\t\t\tModified by: Todd&nbsp;A&nbsp;Kuffner</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project produced numerous published papers, and also supported a Ph.D. student who is now a postdoctoral researcher. The results of this project also led to the creation of a new course on post-selection inference, as well as incorporation of post-selection inference material into a statistical learning course which I teach each year. \n\nHigher-order asymptotics is concerned with studying accuracy and sources of error in distributional approximations, with the goal of obtaining more accurate inferences in the settings of small or moderate sample sizes. Kolassa &amp; Kuffner (2020) solves an open problem in Bayesian inference regarding the accuracy of approxiimate inferences, as well as providing the first known results concerning the orders of magnitude of posterior cumulants, which enables a new set of tools to be used in studying and designing accurate Bayesian inference procedures, post-model-selection or otherwise. \n\nAnother paper provides a insight into the connections between overfitting when performing variable selection, and the effects on subsequent uncertainty quantification. \n\nKuffner &amp; Young (2018) elucidates the connections between selective inference and Fisherian conditional inference, which is of great value for understanding the foundations of statistical reasoning in the post-model-selection regime. \n\nOther papers supported by this grant are aimed at practitioners, providing clear presentations, examples, and solutions for the problem of inference after variable selection in linear regression. \n\nThe results of this project further our understanding of the effects of variable selection on subsequent inference, and will be useful to other researchers as well as any practitioner concerned with accurate inference after variable selection.\n\n\t\t\t\t\tLast Modified: 05/10/2023\n\n\t\t\t\t\tSubmitted by: Todd A Kuffner"
 }
}