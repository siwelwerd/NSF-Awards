{
 "awd_id": "1734183",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Topics in stochastic analysis and Malliavin calculus",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924885",
 "po_email": "tbartosz@nsf.gov",
 "po_sign_block_name": "Tomek Bartoszynski",
 "awd_eff_date": "2016-07-07",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 55509.0,
 "awd_amount": 55509.0,
 "awd_min_amd_letter_date": "2017-02-06",
 "awd_max_amd_letter_date": "2017-02-06",
 "awd_abstract_narration": "The central limit theorem (CLT) is a universality result for independent and identically distributed trials on which is based much statistical analysis in the sociological and natural sciences. The CLT's main conclusion is that aggregated data follows the so-called Gaussian law, also known as the normal or \"bell\" curve. But scientists in many fields from seismology to computer science to quantitative finance are finding that their data series have long-range correlations, which means that the CLT may or may not be a valid way of looking at how such data aggregates. The PI's work on correlated data sequences, and related questions, would show that the Gaussian-law behavior afforded by the CLT persists up to very long correlation lengths, with some quantitative differences with the standard CLT, such as an increase in how spread out averages tend to get. For instance, one of the PI's theoretical conjectures is that if correlation is long enough, it would take too much data in practice to be able to observe a CLT-type aggregation. The PI will study the effect of even longer-range correlations, showing that instead of bell-curve behavior, data could involve much higher levels of uncertainty (a.k.a. heavy tails), with an extremely slow rate of aggregation. This could be of some significance when applied to financial risk in the housing market: tools could be developed for sellers of institutional mortgage insurance products for highly correlated mortgages; they would help avoid errors in risk calculations, such as those made by the American International Group (AIG) in the years preceding the world financial crisis of 2008, which resulted in a taxpayer-funded bailout upwards of $ 180 billion. The PI also plans to study the implications of long-range correlations in so-called spin models which are useful in the physics of random media, where, unlike the example of mortgage-based financial derivatives, long-range correlations and heavy tails could have little or no influence on the average large-scale behavior. The PI's Ph.D. students will take part in both theoretical and applied aspects of the research, working with the PI to prove theorems and test their results in practice using numerics. Involving students in fundamental research with real-world applications will broadly disseminate scientific understanding. The PI systematically encourages students from underrepresented groups to join the research program.\r\n\r\n   The PI proposes a three-year research program in stochastic analysis, with two groups of topics. First, the complexity of asymptotic laws for variations of Gaussian processes with long-range correlations will be evidenced by searching for conditions implying normal, non-normal, and conditionally normal limits in general situations, including sharp convergence rates. Second, the PI will analyze densities, tails, and convex functionals, spin systems, and hitting probabilities, for general Malliavin-differentiable non-Gaussian processes and fields. A main set of tools is the new use of the Malliavin calculus for quantitative estimates of various distances between laws of random variables on Wiener space. This includes the PI's formula for the density of general random variables on Wiener space, proved with I. Nourdin in 2009. Another tool is the PI's comparison of convex functionals for random vectors and fields on Wiener space, proved in 2013 with I. Nourdin and G. Peccati. Yet another is the first sharp estimates of distances to the normal law on Wiener space, proved in 2012 and 2013 by Bierme, Bonami, Nourdin, and Peccati. The PI will forego power-scale model assumptions such as self-similarity and/or stationarity whenever possible, using instead assumptions which are intrinsic to general covariance structures. One of the consequence of the work will be to show that well-known behaviors in so-called critical cases for power variations can be artefacts of the chosen model classes. Another will be to find out the extend of the so-called Sherrington-Kirkpatrick universality class for spin systems in random media, and to determine behaviors when heavy tails and long-range correlations cause spin systems to exit this class. A third consequence should be to understand the critical cases for hitting probabilities of fractional Brownian motion.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Frederi",
   "pi_last_name": "Viens",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Frederi G Viens",
   "pi_email_addr": "viens@rice.edu",
   "nsf_id": "000490283",
   "pi_start_date": "2017-02-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Michigan State University",
  "inst_street_address": "426 AUDITORIUM RD RM 2",
  "inst_street_address_2": "",
  "inst_city_name": "EAST LANSING",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "5173555040",
  "inst_zip_code": "488242600",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MI07",
  "org_lgl_bus_name": "MICHIGAN STATE UNIVERSITY",
  "org_prnt_uei_num": "VJKZC4D1JN36",
  "org_uei_num": "R28EKN92ZTZ9"
 },
 "perf_inst": {
  "perf_inst_name": "Michigan State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "488242600",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MI07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126300",
   "pgm_ele_name": "PROBABILITY"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 55509.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Stochastic analysis is a mathematical topic in the theory of probability which help understand the structure of random fluctuations. It is important because most apparently random phenomena actually follow well-prescribed rules. A particularly telling example of this is that when one analyzes natural or sociological quantities over a &ldquo;large-enough&rdquo; number of independent outcomes or samples, deviations from the average values never fluctuate very far, and we can measure how large typical fluctuations are. But what if the samples are not independent, either because they follow a complex time evolution, like climate variables for instance, or because there are tenuous but real long-range connections between them over time, like the intensity of stock price fluctuations in a bull market? And how large is &ldquo;large enough&rdquo;? In this project, the PI and his many collaborators in the US and a number of other countries, have concentrated on understanding the theory and applications of random fluctuations under these non-standard conditions, thanks to new tools in stochastic analysis.</p>\n<p>&nbsp;</p>\n<p>They have published nearly three dozen mathematical research papers technical peer-reviewed papers on these topics. In about ten mathematical papers, some of which are on statistical theory, they show precisely how to calculate how large is large enough, and how to measure the effect of time evolutions or of long-range memory.&nbsp; By working with objects which are familiar to statisticians, including so-called sums of squares, their estimates are proved to be sharp (meaning that they cannot be improved), which is a first for mathematical statistics.</p>\n<p>&nbsp;</p>\n<p>Among the applications that the PI and his teams of collaborators have produced, one counts a deep study in investment finance, which introduces the sharpest study yet of random fluctuations in volatility. An evaluation of all public US investments in agriculture science in the 20<sup>th</sup> century, and how it has driven ag productivity since the 1950s. A study of on-farm practices among smallholder farmers in the impoverished African nation of Malawi, and how weed prevention and control are key to food security and household stability there. A estimation of how nuclear physicists ought to plan their experiments when going after exotic nuclei, thanks to a predictive analysis of what heavy particles should exist. And various studies related to risk analytics in insurance and finance.</p>\n<p>&nbsp;</p>\n<p>The overall impact of the project is in its mathematical novelty in being able to measure with precision the size of random fluctuations, how much data is really needed to see them clearly, and how many different applications can be informed by these new tools in mathematical analysis and probability theory.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/17/2020<br>\n\t\t\t\t\tModified by: Frederi&nbsp;G&nbsp;Viens</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nStochastic analysis is a mathematical topic in the theory of probability which help understand the structure of random fluctuations. It is important because most apparently random phenomena actually follow well-prescribed rules. A particularly telling example of this is that when one analyzes natural or sociological quantities over a \"large-enough\" number of independent outcomes or samples, deviations from the average values never fluctuate very far, and we can measure how large typical fluctuations are. But what if the samples are not independent, either because they follow a complex time evolution, like climate variables for instance, or because there are tenuous but real long-range connections between them over time, like the intensity of stock price fluctuations in a bull market? And how large is \"large enough\"? In this project, the PI and his many collaborators in the US and a number of other countries, have concentrated on understanding the theory and applications of random fluctuations under these non-standard conditions, thanks to new tools in stochastic analysis.\n\n \n\nThey have published nearly three dozen mathematical research papers technical peer-reviewed papers on these topics. In about ten mathematical papers, some of which are on statistical theory, they show precisely how to calculate how large is large enough, and how to measure the effect of time evolutions or of long-range memory.  By working with objects which are familiar to statisticians, including so-called sums of squares, their estimates are proved to be sharp (meaning that they cannot be improved), which is a first for mathematical statistics.\n\n \n\nAmong the applications that the PI and his teams of collaborators have produced, one counts a deep study in investment finance, which introduces the sharpest study yet of random fluctuations in volatility. An evaluation of all public US investments in agriculture science in the 20th century, and how it has driven ag productivity since the 1950s. A study of on-farm practices among smallholder farmers in the impoverished African nation of Malawi, and how weed prevention and control are key to food security and household stability there. A estimation of how nuclear physicists ought to plan their experiments when going after exotic nuclei, thanks to a predictive analysis of what heavy particles should exist. And various studies related to risk analytics in insurance and finance.\n\n \n\nThe overall impact of the project is in its mathematical novelty in being able to measure with precision the size of random fluctuations, how much data is really needed to see them clearly, and how many different applications can be informed by these new tools in mathematical analysis and probability theory.\n\n \n\n\t\t\t\t\tLast Modified: 02/17/2020\n\n\t\t\t\t\tSubmitted by: Frederi G Viens"
 }
}