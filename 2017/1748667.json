{
 "awd_id": "1748667",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER:  Spatial Audio Data Immersive Experience (SADIE)",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2017-08-15",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 149930.0,
 "awd_amount": 149930.0,
 "awd_min_amd_letter_date": "2017-08-07",
 "awd_max_amd_letter_date": "2017-08-07",
 "awd_abstract_narration": "Although there has been much recent interest in visualization to support data analysis, sonification -- the rendering of non-auditory information as sound -- represents a relatively unexplored but rich space that could map onto many data analysis problems, especially when the data has a natural spatial and temporal element.  In contrast to headphone-based sonification approaches, this project will explore the potential of \"exocentric\" sound environments that completely encompass the user and allow them to interact with the sonified data by moving in space.  The hypothesis is that compared to existing methods of data analysis, the coupling of spatial data with spatial representations, the naturalness of interacting with the data through motion, the leveraging of humans' ability to hear patterns and localize them in 3D, and the avoidance of artifacts introduced by headphone-based sonification strategies will all help people perceive patterns and causal relationships in data.  To test this, the team will develop a set of primitives for mapping spatio-temporal data to sound parameters such as volume, pitch, and spectral filtering.  They will refine these primitives through a series of increasingly complex data analysis experiments, including specific analysis tasks in the domain of geospace science.  If successful, the work could have implications in a variety of applications, from enhancing visualizations to developing better virtual reality systems, while developing interdisciplinary bridges between scientific communities from music to computing to the physical sciences.\r\n\r\nThe project will be developed using an immersive sound studio that includes motion tracking capabilities and a high-density loudspeaker array, driven by algorithms and open source sound libraries developed by the team to support embodied, rich exploration of sonified data that is not subject to audio deviations introduced by headphone-based strategies such as Head Related Transfer Functions.  The specific sonification strategies for individual data streams will be based on the primitives described earlier, focusing on sounds rich in spectra that are easier for people to localize.  Strategies for representing multiple data streams will include layering multiple non-masking sounds and combining streams to modulate different aspects of the same sound (e.g., pitch and volume).  To develop and validate the strategies, the team will conduct a series of experiments that gradually increase the complexity of the analysis tasks: from basic ability to perceive and interpret single data primitives, to perceiving and inferring relationships between multiple data streams, to measuring subjects' ability to perceive known causes between multiple data streams in a series of geospatial model scenarios.  In these studies the team will vary the strength of relationships in the data, the size of the parameter manipulations of sounds, and the pairing of different sounds and parameterizations in order to determine perceptual properties and limitations of sonficiation strategies (similar in some ways to perception-based foundations of visualization); they will also compare both analysis performance and qualitative reactions of participants using both the exocentric environment and a headphone-based egocentric environment as a control.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ivica",
   "pi_last_name": "Bukvic",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Ivica I Bukvic",
   "pi_email_addr": "ico@vt.edu",
   "nsf_id": "000284700",
   "pi_start_date": "2017-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Earle",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory D Earle",
   "pi_email_addr": "earle@vt.edu",
   "nsf_id": "000298307",
   "pi_start_date": "2017-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "240610001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "VA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 149930.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The spatial-audio data immersive experience project (SADIE) focuses on the use of sound to explore complex, multi-faceted datasets.&nbsp; The essential idea is to develop infrastructure and knowledge necessary to analyze data by assigning specific sounds to a group of variables, and then immersing users in a room equipped to simultaneously play these sounds so that users can let their auditory systems detect correlations and other relationships between the variables.&nbsp; The SADIE project is developing new foundational knowledge in this research domain, which has been found to offer unique advantages relative to headphone-based sound studies.&nbsp; To date we have developed the infrastructure required to do this. and also to allow users to interact with the data.&nbsp; Further, we have run a study with over 100 participants and have uncovered foundational knowledge that sets the stage for the follow-on studies in order to be able to expand our sonification methodologies to include multidimensional datasets, with primary focus on inherently spatial geospatial data. The study has resulted in 3 peer-reviewed publications at international conferences and has in part served as an inspiration for a book chapter.&nbsp; Further, the underlying technologies used in this study have paved way towards potential commercialization opportunities.&nbsp; The facilities and techniques used in this study do more than enable research; they also allow users to interact with the sound fields by selecting and modifying specific sounds, with potential future use scenarios in education and assistive technologies scenarios.&nbsp; This capability has also led to new forms of artistic expression, including performances for the general public.&nbsp;&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/12/2019<br>\n\t\t\t\t\tModified by: Ivica&nbsp;I&nbsp;Bukvic</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe spatial-audio data immersive experience project (SADIE) focuses on the use of sound to explore complex, multi-faceted datasets.  The essential idea is to develop infrastructure and knowledge necessary to analyze data by assigning specific sounds to a group of variables, and then immersing users in a room equipped to simultaneously play these sounds so that users can let their auditory systems detect correlations and other relationships between the variables.  The SADIE project is developing new foundational knowledge in this research domain, which has been found to offer unique advantages relative to headphone-based sound studies.  To date we have developed the infrastructure required to do this. and also to allow users to interact with the data.  Further, we have run a study with over 100 participants and have uncovered foundational knowledge that sets the stage for the follow-on studies in order to be able to expand our sonification methodologies to include multidimensional datasets, with primary focus on inherently spatial geospatial data. The study has resulted in 3 peer-reviewed publications at international conferences and has in part served as an inspiration for a book chapter.  Further, the underlying technologies used in this study have paved way towards potential commercialization opportunities.  The facilities and techniques used in this study do more than enable research; they also allow users to interact with the sound fields by selecting and modifying specific sounds, with potential future use scenarios in education and assistive technologies scenarios.  This capability has also led to new forms of artistic expression, including performances for the general public.  \n\n\t\t\t\t\tLast Modified: 08/12/2019\n\n\t\t\t\t\tSubmitted by: Ivica I Bukvic"
 }
}