{
 "awd_id": "1652491",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Beyond Worst-Case Analysis: New Approaches in Approximation Algorithms and Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2017-03-15",
 "awd_exp_date": "2024-02-29",
 "tot_intn_awd_amt": 505251.0,
 "awd_amount": 505251.0,
 "awd_min_amd_letter_date": "2017-03-10",
 "awd_max_amd_letter_date": "2021-03-30",
 "awd_abstract_narration": "Combinatorial optimization problems such as clustering, and unsupervised learning of probabilistic models are important computational problems that arise in diverse areas including machine learning, computer vision, operations research and data analysis. However, there is a large disconnect between our theoretical and practical understanding of these problems -- while theory tells us that many interesting computational problems in combinatorial optimization and machine learning are intractable in the worst case, practitioners in areas such as machine learning and computer vision have made significant progress in solving such theoretically hard problems. This project focuses on bridging the fundamental gap between theory and practice by developing paradigms and machinery that will allow us to reason about the performance of algorithms on real-world instances. This research has the potential to have broad impact on both theory and practice of computational problems across different areas of computer science, machine learning and statistics. The project will involve students at all levels of research, will integrate aspects of average-case analysis in both graduate and undergraduate courses, and will include outreach activities in high schools in Evanston and the broader Chicago area.\r\n\r\nThe PI will study several problems in machine learning and combinatorial optimization by using realistic average-case models and smoothed analysis. Broad goals include designing new model-independent algorithms with provable guarantees for realistic average-case models of graph partitioning and clustering and challenging average-case settings where there is no unique or planted solution. These algorithms will also lead to new algorithmic techniques for learning probabilistic models such as mixtures of Gaussians and stochastic block models that are robust to various kinds of modeling errors and noise. Another focus of the project is on developing new efficient algorithms for learning latent variable models and for reasoning about the performance of algorithms using smoothed analysis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aravindan",
   "pi_last_name": "Vijayaraghavan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aravindan Vijayaraghavan",
   "pi_email_addr": "aravindv@northwestern.edu",
   "nsf_id": "000692489",
   "pi_start_date": "2017-03-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "2133 N Sheridan Rd, 3-207",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602083109",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 55436.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 100269.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 106032.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 106795.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 136719.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focused on developing new theories and techniques for reasoning about algorithmic problems that arise in machine learning, and optimization though a beyond worst-case analysis lens. Traditional paradigms in theoretical computer science have focused on worst-case analysis of algorithms which can be pessimistic. Real-world instances of computational problems are unlikely to be in degenerate or pathological configurations. On the research side, the project led to the development of new models of real-world instances, new techniques to prove guarantees, and efficient algorithms that were also robust to different modeling errors and corruptions. These research outcomes were complemented with new courses and educational material on related topics, and the training of several PhD students and undergraduate students on beyond worst-case paradigms. &nbsp;</p>\n<p>On the research front, one of the highlights was the development of new smoothed analysis techniques for many basic problems in machine learning and high-dimensional data analysis that are computationally intractable in the worst-case. In smoothed analysis, the algorithm is analyzed on a small random perturbation of an arbitrary input. Smoothed analysis guarantees show that the bad instances for the algorithm are isolated &ndash; qualitatively they represent the strongest possible algorithmic guarantees in the absence of worst-case guarantees. In a sequence of works that also involved PhD students (funded by the project) and undergraduate students at Northwestern, we developed new techniques that are broadly useful for proving smoothed analysis guarantees (Bhaskara et al. FOCS 2019, Math Programming 2021, and STOC 2024). We developed new probabilistic ideas for establishing lower bounds on the least singular value of random matrices with limited randomness. Least singular value bounds often involve showing strong anti-concentration inequalities that are intricate and much less understood compared to concentration (or large deviation) bounds. These techniques allowed us to resolve open questions in smoothed analysis for well-studied problems like learning hidden Markov models, tensor decompositions, subspace clustering, learning mixtures of non-spherical decompositions and several other problems. Moreover, we used these ideas to also develop new that gave the first polynomial time algorithmic guarantees (through smoothed analysis) for learning depth-2 neural networks with general ReLU activation (Awasthi et al., NeurIPS 2021). Finally, in a sequence of recent projects (Johnston, Lovitz and Vijayaraghavan QIP 2022, FOCS 2023), we studied a broad class of algorithmic problems in algebraic geometry that capture important problems in quantum information and on low-rank tensor decompositions. While problem is known to be NP-hard in the worst-case, we showed that we could design efficient algorithms with smoothed analysis guarantees.</p>\n<p>Another highlight of this project was the development of more realistic average-case models called semi-random models for common machine learning tasks. Semi-random models provide a framework to reason about robustness of algorithms to modeling errors by incorporating both adversarial and random choices in instance generation. In two different works (Awasthi and Vijayaraghvan FOCS 2018, NeurIPS 2018) we studied new semi-random models for two important problems in unsupervised learning: dictionary learning, and clustering mixtures of Gaussians, and designed efficient algorithms with robustness guarantees for these problems. This focus on robustness also led to the development of machine learning algorithms with provable guarantees in the presence of adversarial attacks and perturbations.&nbsp; Adversarial robustness of machine learning methods is important from a reliability and security standpoint. In a sequence of works (Awasthi et al. COLT 2020, NeurIPS 2020, COLT 2021) we designed algorithms for basic machine learning primitives like finding low-dimensional data representations that are robust to adversarial perturbations. We gave polynomial time algorithms with approximately optimal guarantees, and also gave similar guarantees for downstream tasks. This approach was also used to get better adversarial robustness for deep neural networks in image classification.&nbsp;</p>\n<p>The project involved overall 6 PhD students (3 of whom graduated) and 3 undergraduate students (all went on to do PhDs). There were several graduate courses on related topics that were developed, along with a new undergraduate course on mathematical foundations that covered linear algebra, probability and optimization foundations for computer science. Finally, the PI was also involved in outreach activities in the broader Chicago area that targeted K-12 students, particularly from underrepresented minorities.</p>\n<p>&nbsp;</p>\n<p>The project also supported organized activities and workshops for the broader scientific community. One of these workshops was the Junior Theorists Workshop, which provided opportunities for some of the best junior researchers (PhD students and postdoctoral researchers, who are not in faculty positions) to showcase their research. The research that came out of the project has also been published in several conferences and journals for dissemination, and in publicly accessible repositories like ArXiv. The PI also authored a publicly-accessible survey on smoothed analysis for tensor decompositions that also became a chapter in the book on Beyond the Worst-Case Analysis of Algorithms. &nbsp;Finally, videos of many of the seminar talks and workshop talks have been made available publicly.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/02/2024<br>\nModified by: Aravindan&nbsp;Vijayaraghavan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project focused on developing new theories and techniques for reasoning about algorithmic problems that arise in machine learning, and optimization though a beyond worst-case analysis lens. Traditional paradigms in theoretical computer science have focused on worst-case analysis of algorithms which can be pessimistic. Real-world instances of computational problems are unlikely to be in degenerate or pathological configurations. On the research side, the project led to the development of new models of real-world instances, new techniques to prove guarantees, and efficient algorithms that were also robust to different modeling errors and corruptions. These research outcomes were complemented with new courses and educational material on related topics, and the training of several PhD students and undergraduate students on beyond worst-case paradigms. \n\n\nOn the research front, one of the highlights was the development of new smoothed analysis techniques for many basic problems in machine learning and high-dimensional data analysis that are computationally intractable in the worst-case. In smoothed analysis, the algorithm is analyzed on a small random perturbation of an arbitrary input. Smoothed analysis guarantees show that the bad instances for the algorithm are isolated  qualitatively they represent the strongest possible algorithmic guarantees in the absence of worst-case guarantees. In a sequence of works that also involved PhD students (funded by the project) and undergraduate students at Northwestern, we developed new techniques that are broadly useful for proving smoothed analysis guarantees (Bhaskara et al. FOCS 2019, Math Programming 2021, and STOC 2024). We developed new probabilistic ideas for establishing lower bounds on the least singular value of random matrices with limited randomness. Least singular value bounds often involve showing strong anti-concentration inequalities that are intricate and much less understood compared to concentration (or large deviation) bounds. These techniques allowed us to resolve open questions in smoothed analysis for well-studied problems like learning hidden Markov models, tensor decompositions, subspace clustering, learning mixtures of non-spherical decompositions and several other problems. Moreover, we used these ideas to also develop new that gave the first polynomial time algorithmic guarantees (through smoothed analysis) for learning depth-2 neural networks with general ReLU activation (Awasthi et al., NeurIPS 2021). Finally, in a sequence of recent projects (Johnston, Lovitz and Vijayaraghavan QIP 2022, FOCS 2023), we studied a broad class of algorithmic problems in algebraic geometry that capture important problems in quantum information and on low-rank tensor decompositions. While problem is known to be NP-hard in the worst-case, we showed that we could design efficient algorithms with smoothed analysis guarantees.\n\n\nAnother highlight of this project was the development of more realistic average-case models called semi-random models for common machine learning tasks. Semi-random models provide a framework to reason about robustness of algorithms to modeling errors by incorporating both adversarial and random choices in instance generation. In two different works (Awasthi and Vijayaraghvan FOCS 2018, NeurIPS 2018) we studied new semi-random models for two important problems in unsupervised learning: dictionary learning, and clustering mixtures of Gaussians, and designed efficient algorithms with robustness guarantees for these problems. This focus on robustness also led to the development of machine learning algorithms with provable guarantees in the presence of adversarial attacks and perturbations. Adversarial robustness of machine learning methods is important from a reliability and security standpoint. In a sequence of works (Awasthi et al. COLT 2020, NeurIPS 2020, COLT 2021) we designed algorithms for basic machine learning primitives like finding low-dimensional data representations that are robust to adversarial perturbations. We gave polynomial time algorithms with approximately optimal guarantees, and also gave similar guarantees for downstream tasks. This approach was also used to get better adversarial robustness for deep neural networks in image classification.\n\n\nThe project involved overall 6 PhD students (3 of whom graduated) and 3 undergraduate students (all went on to do PhDs). There were several graduate courses on related topics that were developed, along with a new undergraduate course on mathematical foundations that covered linear algebra, probability and optimization foundations for computer science. Finally, the PI was also involved in outreach activities in the broader Chicago area that targeted K-12 students, particularly from underrepresented minorities.\n\n\n\n\n\nThe project also supported organized activities and workshops for the broader scientific community. One of these workshops was the Junior Theorists Workshop, which provided opportunities for some of the best junior researchers (PhD students and postdoctoral researchers, who are not in faculty positions) to showcase their research. The research that came out of the project has also been published in several conferences and journals for dissemination, and in publicly accessible repositories like ArXiv. The PI also authored a publicly-accessible survey on smoothed analysis for tensor decompositions that also became a chapter in the book on Beyond the Worst-Case Analysis of Algorithms. Finally, videos of many of the seminar talks and workshop talks have been made available publicly.\n\n\n\t\t\t\t\tLast Modified: 07/02/2024\n\n\t\t\t\t\tSubmitted by: AravindanVijayaraghavan\n"
 }
}