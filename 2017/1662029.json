{
 "awd_id": "1662029",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Integrated Computer Vision System Based on Human Eye Motion",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Eva Kanso",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 332659.0,
 "awd_amount": 399020.0,
 "awd_min_amd_letter_date": "2017-05-16",
 "awd_max_amd_letter_date": "2020-03-24",
 "awd_abstract_narration": "The goal of this project is to build a robotic vision system that mimics human eye motions. In particular, this research will lead to an enhanced vision system that can achieve quick scanning capability (saccadic eye motion) and smooth object following capability (smooth-pursuit) with minimal computational overhead and using an inexpensive generic hardware.  Current state-of-the-art systems require significant computer post-processing and expensive hardware.  This project will lead to a novel camera positioning system (hardware) and real-time vision (software) system that does not require any post-processing and produces ready-to-use images in real-time. To achieve this goal, the project draws upon the biomechanical and cognitive principles of human vision. The innovative methods and algorithms that can produce motions with speed, accuracy, and smoothness to mimic human eye can greatly enhance capabilities of autonomous vehicles used in a wide variety of applications such as agriculture, military, security, and traffic monitoring. The project will integrate research and outreach activities for K-12 and undergraduate students. \r\n\r\nThis project studies a bio-inspired dynamics-based method for coordinating motion control and image processing where the system can mechanically displace the field of view by a large angle between frames. The concept behind this research is to merge the system dynamics area and image processing area while previous studies focused solely on either mechanical design or image processing.  The method is motivated by the principles of human vision for effective image de-blurring and panoramic image stitching. In coordination with inherently discrete and rapid ocular movements, the developed image processing methods inspired by ocular physiology will mimic saccades and smooth-pursuit in a fast-moving robotic eye.  A piezoelectrically driven robotic camera positioning mechanism will be employed to demonstrate the effectiveness of this ocular physiology-inspired approach. Hardware architecture that can synchronize image processing and real-time motion control will be configured and tested. A panoramic image of a scene will be generated from multiple images acquired during saccades by removing motion blur and stitching images within a single frame rate. The system architecture will be optimized to best coordinate hardware and software for real-time motion control.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jun",
   "pi_last_name": "Ueda",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jun Ueda",
   "pi_email_addr": "jun.ueda@me.gatech.edu",
   "nsf_id": "000508370",
   "pi_start_date": "2017-05-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164200",
   "pgm_ele_name": "Special Initiatives"
  },
  {
   "pgm_ele_code": "756900",
   "pgm_ele_name": "Dynamics, Control and System D"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "030E",
   "pgm_ref_txt": "CONTROL SYSTEMS"
  },
  {
   "pgm_ref_code": "034E",
   "pgm_ref_txt": "Dynamical systems"
  },
  {
   "pgm_ref_code": "091Z",
   "pgm_ref_txt": "Data Initiatives"
  },
  {
   "pgm_ref_code": "8024",
   "pgm_ref_txt": "Complex Systems"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 332659.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 66361.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to develop a robotic vision system and optimization method that combine motion control and image processing for a motion controlled robotic vision system. Such vision systems can be found in biological systems as well, where eye motions are controlled based on the desired visual task. The benefit of such a system is that the controlled motion of the camera sensor enables control over the resulting motion blur, while having prior knowledge of the motion for image processing. Given this prior knowledge, images can be processed in real-time without significant computational resources. Real-time processing is essential for autonomous systems to successfully interpret visual information and make decisions in a dynamic environment. In contrast, current state-of-the-art systems either require more computational time due to iterative serial processes or more computational hardware to handle many parallel processes. This project developed trajectory optimization and motion control algorithms for a novel camera positioning system for performing visual tasks in real-time with minimal computational overhead. The motion trajectory for a dynamic camera system was numerically generated and optimized to maximize the object recognition rate while meeting endpoint constraints. Spectral analysis confirmed that chosen trajectories successfully preserved text edges in a large image dataset. These findings may be applied to a wide variety of controlled mobile camera platforms, such as autonomous automobiles or unmanned aerial vehicles, to improve their ability to gather information from their environment.</p>\n<p>A total of four graduate students have been involved in the project for the development of control algorithms and testbed device building. Findings have been presented at domestic and international conferences and journal articles.&nbsp; Demonstrations have been given for high school students during National Robotics Week events from 2017 to 2020. Graduate students have also as well as mentored undergraduate research. Through the Data Science Research Supplement, the project expanded research to motion beblur suppression for a camera system mounted on a bipedal robotic device and its application to image-based manipulation of fruit. The research results have been introduced in the PI&rsquo;s undergraduate and graduate mechanical and robotics courses.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/18/2021<br>\n\t\t\t\t\tModified by: Jun&nbsp;Ueda</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to develop a robotic vision system and optimization method that combine motion control and image processing for a motion controlled robotic vision system. Such vision systems can be found in biological systems as well, where eye motions are controlled based on the desired visual task. The benefit of such a system is that the controlled motion of the camera sensor enables control over the resulting motion blur, while having prior knowledge of the motion for image processing. Given this prior knowledge, images can be processed in real-time without significant computational resources. Real-time processing is essential for autonomous systems to successfully interpret visual information and make decisions in a dynamic environment. In contrast, current state-of-the-art systems either require more computational time due to iterative serial processes or more computational hardware to handle many parallel processes. This project developed trajectory optimization and motion control algorithms for a novel camera positioning system for performing visual tasks in real-time with minimal computational overhead. The motion trajectory for a dynamic camera system was numerically generated and optimized to maximize the object recognition rate while meeting endpoint constraints. Spectral analysis confirmed that chosen trajectories successfully preserved text edges in a large image dataset. These findings may be applied to a wide variety of controlled mobile camera platforms, such as autonomous automobiles or unmanned aerial vehicles, to improve their ability to gather information from their environment.\n\nA total of four graduate students have been involved in the project for the development of control algorithms and testbed device building. Findings have been presented at domestic and international conferences and journal articles.  Demonstrations have been given for high school students during National Robotics Week events from 2017 to 2020. Graduate students have also as well as mentored undergraduate research. Through the Data Science Research Supplement, the project expanded research to motion beblur suppression for a camera system mounted on a bipedal robotic device and its application to image-based manipulation of fruit. The research results have been introduced in the PI\u2019s undergraduate and graduate mechanical and robotics courses.\n\n\t\t\t\t\tLast Modified: 11/18/2021\n\n\t\t\t\t\tSubmitted by: Jun Ueda"
 }
}