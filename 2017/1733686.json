{
 "awd_id": "1733686",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AitF: Collaborative Research: Efficient High-Dimensional Integration using Error-Correcting Codes",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 360000.0,
 "awd_amount": 360000.0,
 "awd_min_amd_letter_date": "2017-08-11",
 "awd_max_amd_letter_date": "2017-08-11",
 "awd_abstract_narration": "Efficiently estimating integrals of high-dimensional functions is a fundamental and largely unsolved computational problem, manifesting in scientific areas from biology and physics to economics. In particular, in Artificial Intelligence and Machine Learning, a wide array of methods are computationally limited precisely because they require the computation of high-dimensional integrals. While computing such integrals exactly is highly intractable, approximations suffice for many applications. Currently, approximation is attempted using two main classes of algorithms: Markov Chain Monte Carlo (MCMC) sampling methods and variational inference techniques. The former are asymptotically accurate, but their computational budget is inflexible and often prohibitive. The latter have manageable computational budget, but typically come with no accuracy guarantees. This project will investigate a new family of computationally efficient approximation methods which reduce the task of integration to the much better studied task of optimization, thus leveraging decades of research and engineering in combinatorial optimization methods and technology. A key goal of the project is to develop an open-source software library of efficient tools for high-dimensional integration.\r\n\r\nThe reduction of integration to optimization builds on the probabilistic reduction of decision problems to uniqueness promise problems developed in the mid-80s. Specifically, the idea is to use systems of random parity equations in order to specify random subsets of the function's domain, and relate integration to the task of optimization over these subsets. In general, the capacity for efficient optimization fundamentally stems from the capacity to summarily dispense large parts of the domain as uninteresting. The key question to be addressed by the project is whether it is possible to define random subsets over which optimization is both tractable and informative for integration. To that end, the project will employ random systems of linear equations corresponding to Low Density Parity Check (LDPC) matrices for error-correcting codes. The energy landscape, i.e., the number of violated equations, of such systems is far smoother than that of the generic (dense) random systems of linear equations that underlie the original mid-80s technique, thus being far more amenable to optimization. The project will also build upon the deep understanding gained in the last two decades for LDPC codes in the field of communications, with the goal of integrating a priori knowledge about the energy landscape in the optimization strategy. This will provide a fundamentally new use for error-correcting codes, creating a bridge between the areas of optimization and information theory.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stefano",
   "pi_last_name": "Ermon",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefano Ermon",
   "pi_email_addr": "ermon@cs.stanford.edu",
   "nsf_id": "000693004",
   "pi_start_date": "2017-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943055008",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "723900",
   "pgm_ele_name": "Algorithms in the Field"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 360000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focused on the problem of developing efficient algorithms for probabilistic inference. This is a key computational problem arising in many fields, including Artificial Intelligence (AI) and Machine Learning (ML), where it is required for example to update a prior probabilistic belief based on new evidence.&nbsp; Fundamentally, the problem is hard because it requires the computation of an integral to \"marginalize\" or \"average\" over all possible models or parameter choices (a potentially enormous set). This project investigated a variety of efficient approximation algorithms based on the overarching idea of reducing the task of integration to the much better studied task of optimization. The resulting algorithms achieve strong guarantees of accuracy while being efficient in practice.</p>\n<p>These algorithms have been tested and rigorously evaluated on a variety of tasks including (1) model counting, where the goal is to count the number of distinct solutions to a system of equations, (2) estimating the partition function of graphical models, (3) computing the permanent of a matrix, (4) multi-target tracking, and (5) learning generative models of data.The research resulted in over 10 conference papers accepted for publications in leading AI and ML venues.</p>\n<p>These results have been disseminated through teaching, invited seminars, publications in machine learning and artificial intelligence conferences, and blog posts. The research constituted the bulk of the thesis of a Computer Science PhD student, and provided opportunities for several undergraduate students to gain research experience in machine learning. Additionally, open-source implementations of the algorithms have been publicly released.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/28/2021<br>\n\t\t\t\t\tModified by: Stefano&nbsp;Ermon</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project focused on the problem of developing efficient algorithms for probabilistic inference. This is a key computational problem arising in many fields, including Artificial Intelligence (AI) and Machine Learning (ML), where it is required for example to update a prior probabilistic belief based on new evidence.  Fundamentally, the problem is hard because it requires the computation of an integral to \"marginalize\" or \"average\" over all possible models or parameter choices (a potentially enormous set). This project investigated a variety of efficient approximation algorithms based on the overarching idea of reducing the task of integration to the much better studied task of optimization. The resulting algorithms achieve strong guarantees of accuracy while being efficient in practice.\n\nThese algorithms have been tested and rigorously evaluated on a variety of tasks including (1) model counting, where the goal is to count the number of distinct solutions to a system of equations, (2) estimating the partition function of graphical models, (3) computing the permanent of a matrix, (4) multi-target tracking, and (5) learning generative models of data.The research resulted in over 10 conference papers accepted for publications in leading AI and ML venues.\n\nThese results have been disseminated through teaching, invited seminars, publications in machine learning and artificial intelligence conferences, and blog posts. The research constituted the bulk of the thesis of a Computer Science PhD student, and provided opportunities for several undergraduate students to gain research experience in machine learning. Additionally, open-source implementations of the algorithms have been publicly released.\n\n \n\n\t\t\t\t\tLast Modified: 12/28/2021\n\n\t\t\t\t\tSubmitted by: Stefano Ermon"
 }
}