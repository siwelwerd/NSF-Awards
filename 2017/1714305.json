{
 "awd_id": "1714305",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF:Small:Information-theoretic and Computational Thresholds in Statistical Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2017-06-28",
 "awd_max_amd_letter_date": "2017-06-28",
 "awd_abstract_narration": "Advanced algorithms are an increasingly powerful tool to extract information from vast amount of data\r\nthat are gathered over the Internet, by smartphones, sensors networks, or high-throughput scientific studies.\r\nAs these methods become ubiquitous, it is crucial to understand their full potential. What kind of\r\ninformation can we hope to extract from a certain type of data? Viceversa, how much data should\r\nwe accumulate in order to be able to infer a certain piece of information? What is the bottleneck that prevents us from extracting more information? These questions have been studied within classical statistics, but modern applications pose entirely new challenges and classical concepts are only partially useful.\r\nIn particular, computational resources become a crucial bottleneck for modern datasets. In many cases,\r\nalthough the data contain in principle the information of interest, finding it is a needle-in-haystack problem, and cannot be done on human timescales. This project aims at characterizing these fundamental limitations in several central problems, and develop algorithms that can achieve those limits.\r\n\r\nBoth information theory and complexity theory fall short of capturing the fundamental limitations to statistical learning tasks. This project follows a different approach which aims at analyzing broad classes of algorithms, and draw connections between their behavior. More precisely, the project considers three such classes that essentially encompass most algorithms used nowadays: empirical risk minimization;\r\nsemidefinite programming hierarchies; and local algorithms. The focus is on two concrete statistical estimation problems that are relevant for a number of applications: group synchronization on graphs; non-linear high-dimensional regression and classification.  In these and analogous problems, the behavior of seemingly different types of algorithms is often surprisingly similar.  Understanding the origin of this similarity and its implications is a key focus of this research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrea",
   "pi_last_name": "Montanari",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andrea Montanari",
   "pi_email_addr": "montanari@stanford.edu",
   "nsf_id": "000107366",
   "pi_start_date": "2017-06-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "3160 Porter Drive, Suite 100",
  "perf_city_name": "Palo Alto",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943041212",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Computational complexity theory characterizes the hardness of ensembles of computational problemsin the worst case. On the other hand, data analysis methodologies are typically based onthe premise that data are generated according to a probabilistic model.&nbsp;Faced with the growing challenge of rapidly processing vast amount of data,&nbsp;it is important to understand what are the fundamental computational limits to&nbsp;such data-analysis tasks.With the aim of reconciling computational complexity and statistical applications, this&nbsp;project studied the computational complexity of a number of canonical estimation and&nbsp;optimization problems.</p>\n<p>A specific example is the one of estimating a latent low-dimensional structurein a large data matrix. This problem arises --for instance-- in data clustering,where we are given a large number of&nbsp; high-dimensional vectors, each representing&nbsp;a measurement (e.g. the genomic sequence of a patient), and we would like to partition these vectorsin a small number of clusters. For certain regimes in the dimensions, number of samples,and number of clusters, this problem appears to be solvable from a purely statistical viewpoint butintractable in polynomial time.</p>\n<p>Other examples include estimating latent structures in large data tensors,and fitting high-dimensional regression models.&nbsp;</p>\n<p><br />Intellectual merit:For this and several of this problems, the project developed new efficient algorithmsthat are believed to achieve optimal accuracy among polynomial time methods. It&nbsp; provided&nbsp;rigorous evidence that no polynomial-time algorithm can outperform them.Among other problems, it developed a new classes of algorithms to optimize mean field spin glasses,which are a class of canonical random high-dimensional impact.</p>\n<p>Broader impact:This research had implications in statistics, computer science, statistical physics,and spurred follow up work in each of these areas. This research direction has attracteda large number of brilliant young researchers over the last few years, as witnessed&nbsp;by several semester research programs at leading institutes on this topic.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/27/2021<br>\n\t\t\t\t\tModified by: Andrea&nbsp;Montanari</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nComputational complexity theory characterizes the hardness of ensembles of computational problemsin the worst case. On the other hand, data analysis methodologies are typically based onthe premise that data are generated according to a probabilistic model. Faced with the growing challenge of rapidly processing vast amount of data, it is important to understand what are the fundamental computational limits to such data-analysis tasks.With the aim of reconciling computational complexity and statistical applications, this project studied the computational complexity of a number of canonical estimation and optimization problems.\n\nA specific example is the one of estimating a latent low-dimensional structurein a large data matrix. This problem arises --for instance-- in data clustering,where we are given a large number of  high-dimensional vectors, each representing a measurement (e.g. the genomic sequence of a patient), and we would like to partition these vectorsin a small number of clusters. For certain regimes in the dimensions, number of samples,and number of clusters, this problem appears to be solvable from a purely statistical viewpoint butintractable in polynomial time.\n\nOther examples include estimating latent structures in large data tensors,and fitting high-dimensional regression models. \n\n\nIntellectual merit:For this and several of this problems, the project developed new efficient algorithmsthat are believed to achieve optimal accuracy among polynomial time methods. It  provided rigorous evidence that no polynomial-time algorithm can outperform them.Among other problems, it developed a new classes of algorithms to optimize mean field spin glasses,which are a class of canonical random high-dimensional impact.\n\nBroader impact:This research had implications in statistics, computer science, statistical physics,and spurred follow up work in each of these areas. This research direction has attracteda large number of brilliant young researchers over the last few years, as witnessed by several semester research programs at leading institutes on this topic.\n\n\t\t\t\t\tLast Modified: 11/27/2021\n\n\t\t\t\t\tSubmitted by: Andrea Montanari"
 }
}