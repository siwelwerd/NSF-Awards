{
 "awd_id": "1720268",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative: BystanderBots: Automated Bystander Intervention for Cyberbullying Mitigation",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Balakrishnan Prabhakaran",
 "awd_eff_date": "2017-08-15",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2017-08-07",
 "awd_max_amd_letter_date": "2018-01-18",
 "awd_abstract_narration": "Bullying has lasting negative psychological and physical effects on victims, bystanders, and bullies alike; online settings can magnify both the scale and impact of these effects, as anonymity can embolden people to make hostile posts about individuals or groups.  This project aims to reduce the prevalence of such posts through the design of active, automated \"bystander interventions\" in online comment threads.  Bystander interventions, in which one or more witnesses to a bullying incident pressures the bully to stop, are often effective in schoolyards, but people are often reluctant to intervene in online scenarios.  Instead, a computer program could post comments that contain these interventions, potentially reducing follow-on aggression from the original poster or others who might pile on -- if bullies perceive these posts as coming from human bystanders, and if bullies under the cover of pseudonyms react to bystander interventions as they do in in-person confrontations.   The project will proceed in three main stages.  The first stage involves improving cyberbullying detection through better detection of non-standard language use associated with bullying in a particular commenting system.  The second stage involves developing a dialogue system that acts like a human bystander, creating messages that look appropriate in the context of given a comment thread and that contain psychologically-valid bystander interventions.  The third stage involves deploying the tool in a large video sharing site and monitoring its ability to detect and, through interventions, mitigate further bullying.  If successful, the project could have real impacts in reducing online aggression in social media systems while reducing the need for (and possible harms to) human moderators; the tools will also be released to the community to support other kinds of research around how chatbots and humans might interact in online comments.\r\n\r\nThe work on detection aims to advance natural language processing (NLP) and computational pragmatics, particularly around non-canonical language use, because state-of-the-art bullying detection schemes typically use bag-of-words approaches that do not consider the linguistic and structural features of cyberbullying.  The team will explore how to identify both explicit indicators of bullying, by developing topic models based on complex features where particular topics are more often associated with bullying, and implicit indicators, through looking for words whose use in a given context diverges from their location in other contexts. The context will be represented as a subspace of words, where the words themselves occur as low-dimensional word embeddings. The dialogue generation portion of the project will characterize and represent properties of effective bystander interventions from the psychology literature.  This representation will drive a dialogue manager designed to generate bystander responses automatically so that the responses contain features that are both believable and are known to be effective in reducing bullying online.  These components will first be evaluated through offline testing, using comment data labeled for bullying content and human ratings of the generated dialogue.  Once a reasonably effective pipeline has been built, it will be evaluated in a series of online experiments in which comment threads are monitored and automated bystander responses generated for some, but not all, threads detected as containing bullying.  The software will log the monitored threads and any generated responses, along with behavior both before and after the automated bystander response in a particular thread; these data will allow the team to evaluate the impact of the bystander intervention on bullying incidents later in the thread.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Suma",
   "pi_last_name": "Bhat",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Suma Bhat",
   "pi_email_addr": "spbhat2@illinois.edu",
   "nsf_id": "000688650",
   "pi_start_date": "2017-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Giulia",
   "pi_last_name": "Fanti",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Giulia Fanti",
   "pi_email_addr": "gfanti@andrew.cmu.edu",
   "nsf_id": "000734916",
   "pi_start_date": "2017-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618207473",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "065Z",
   "pgm_ref_txt": "Human factors for security research"
  },
  {
   "pgm_ref_code": "114Z",
   "pgm_ref_txt": "SaTC-CISE-SBE New Collabs"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8225",
   "pgm_ref_txt": "SaTC Special Projects"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Bullying can have serious psychological repercussions, particularly for the bullied. Studies in social psychology have found that an active bystander (a bystander that replies to the bully in a prosocial way in support of the victim) can effectively reduce the harm caused by the bully's words. 'Cyberbullying' refers to bullying that occurs in online forums and is linked with the same psychological effects observed in traditional bullying. We chose to study a more general form of cyberbullying called abusive language, which refers to strongly&nbsp;impolite and harmful language used to hurt and control a person or a group by way of harassment, insults, threats, bullying and/or trolling.&nbsp;This study explores one possible countermeasure for abusive language in anonymous online forums using automatic bots that serve as bystanders, bringing a computational lens to the active bystander effect.</p>\n<p>The outcomes of this study are summarized below.</p>\n<p>a) We publicly released a large dataset of abusive language with over 11,000 YouTube comments from comments related to feminism, annotated according to (a) whether they constitute abusive language, and (b) the category of abusive language (e.g., religion, sexuality). There is only one other such English-language dataset from YouTube (of limited scope in comparison to ours). We used this dataset to build an automated detector of abusive language. The intuition for this detector was that a given comment was an abusive instance depending on whether or not it had a sentence or two abusive instances. This suggested a context-dependent modeling approach, considering the surrounding sentences of an abusive sentence. We experimented with different neural network architectures, comparing their performances on the task and found our method significantly outperforms previously proposed methods&nbsp;that diregard the context for abusive language detection.</p>\n<p>b) During abusive language annotation, we found that some of the comments were natural attempts to mitigate bullying, leading us to think that analyzing such behaviors may help us understand its effect, and to adopt them to effectively create the needed automated pro-social response. Next, we conducted an experiment, where we manually monitored a set of YouTube channels and, upon detecting an abusive comment, we posted a prosocial response from a list of prosocial responses compiled based on related literature. We monitored the prevalence of abusive language two weeks prior to the prosocial comment being posted, compared to the successive two weeks. Our initial hypothesis that prosocial intervention may reduce abusive language in online forums appeared not to hold. We found no significant change in the amount or severity of abusive language after posting prosocial comments. We hypothesize that because we were not able to directly target individual cyberbullies (owing to ethical obligations), our prosocial comments tended to be more general (e.g., \"no reason to get worked up\"). Worse still, in a few instances, we saw the cyberbullies actually responded to our comment and got even angrier; in one case, the bully became abusive toward our prosocial account, suggesting that potentially a naive prosocial intervention may not be effective for mitigating online abusive language.</p>\n<p>c) We experimented with 3 natural language processing algorithms to generate counter-comments to abusive language. Doing this in a controlled setting, we used a publicly available collection with aligned hate speech-counter speech instances that were manually created. Our proposed approach was found to outperform competitive methods generating counterspeech that was not only fluent but also diverse, and conformed to the style of expert-generated counterspeech.</p>\n<p>d) Finally, we studied the detection of euphemisms in text, inspired by the fact that hate speech is often couched in euphemisms that represent hateful concepts---a challenge for existing automatic filters. Considering their evolving nature, we designed a novel algorithm that relies on the context in which the euphemistic word occurs without the need for explicit labeling. This included euphemistic speech, where the dominant meaning of a word or phrase is not the meaning meant by the author. Specifically, given a corpus of text and a list of banned or sensitive words (e.g., illegal drug names), we asked how to identify (in a fully unsupervised way) euphemisms for these words. Our main finding was that it&nbsp;<em>is</em>&nbsp;possible to detect euphemistic speech in an unsupervised fashion. Innovating over prior approaches that do not work well for this task, we transitioned to large state-of-the-art methods, that were re-designed using novel methods for detecting and classifying euphemistic speech. without manual annotation. Our proposed algorithms obtained significant improvements over state-of-the-art algorithms (in some cases as much as 400% higher labeling accuracy!).&nbsp;Going beyond euphemistic words, we also proposed a solution to detect and classify euphemistic phrases. We recently learned that euphemistic speech is also being used to circumvent vaccine misinformation filters on Facebook, which gives additional relevance to our work. Although we do not have access to relevant Facebook data, our proposed approach could easily be used to tackle this problem.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/30/2021<br>\n\t\t\t\t\tModified by: Suma&nbsp;Bhat</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nBullying can have serious psychological repercussions, particularly for the bullied. Studies in social psychology have found that an active bystander (a bystander that replies to the bully in a prosocial way in support of the victim) can effectively reduce the harm caused by the bully's words. 'Cyberbullying' refers to bullying that occurs in online forums and is linked with the same psychological effects observed in traditional bullying. We chose to study a more general form of cyberbullying called abusive language, which refers to strongly impolite and harmful language used to hurt and control a person or a group by way of harassment, insults, threats, bullying and/or trolling. This study explores one possible countermeasure for abusive language in anonymous online forums using automatic bots that serve as bystanders, bringing a computational lens to the active bystander effect.\n\nThe outcomes of this study are summarized below.\n\na) We publicly released a large dataset of abusive language with over 11,000 YouTube comments from comments related to feminism, annotated according to (a) whether they constitute abusive language, and (b) the category of abusive language (e.g., religion, sexuality). There is only one other such English-language dataset from YouTube (of limited scope in comparison to ours). We used this dataset to build an automated detector of abusive language. The intuition for this detector was that a given comment was an abusive instance depending on whether or not it had a sentence or two abusive instances. This suggested a context-dependent modeling approach, considering the surrounding sentences of an abusive sentence. We experimented with different neural network architectures, comparing their performances on the task and found our method significantly outperforms previously proposed methods that diregard the context for abusive language detection.\n\nb) During abusive language annotation, we found that some of the comments were natural attempts to mitigate bullying, leading us to think that analyzing such behaviors may help us understand its effect, and to adopt them to effectively create the needed automated pro-social response. Next, we conducted an experiment, where we manually monitored a set of YouTube channels and, upon detecting an abusive comment, we posted a prosocial response from a list of prosocial responses compiled based on related literature. We monitored the prevalence of abusive language two weeks prior to the prosocial comment being posted, compared to the successive two weeks. Our initial hypothesis that prosocial intervention may reduce abusive language in online forums appeared not to hold. We found no significant change in the amount or severity of abusive language after posting prosocial comments. We hypothesize that because we were not able to directly target individual cyberbullies (owing to ethical obligations), our prosocial comments tended to be more general (e.g., \"no reason to get worked up\"). Worse still, in a few instances, we saw the cyberbullies actually responded to our comment and got even angrier; in one case, the bully became abusive toward our prosocial account, suggesting that potentially a naive prosocial intervention may not be effective for mitigating online abusive language.\n\nc) We experimented with 3 natural language processing algorithms to generate counter-comments to abusive language. Doing this in a controlled setting, we used a publicly available collection with aligned hate speech-counter speech instances that were manually created. Our proposed approach was found to outperform competitive methods generating counterspeech that was not only fluent but also diverse, and conformed to the style of expert-generated counterspeech.\n\nd) Finally, we studied the detection of euphemisms in text, inspired by the fact that hate speech is often couched in euphemisms that represent hateful concepts---a challenge for existing automatic filters. Considering their evolving nature, we designed a novel algorithm that relies on the context in which the euphemistic word occurs without the need for explicit labeling. This included euphemistic speech, where the dominant meaning of a word or phrase is not the meaning meant by the author. Specifically, given a corpus of text and a list of banned or sensitive words (e.g., illegal drug names), we asked how to identify (in a fully unsupervised way) euphemisms for these words. Our main finding was that it is possible to detect euphemistic speech in an unsupervised fashion. Innovating over prior approaches that do not work well for this task, we transitioned to large state-of-the-art methods, that were re-designed using novel methods for detecting and classifying euphemistic speech. without manual annotation. Our proposed algorithms obtained significant improvements over state-of-the-art algorithms (in some cases as much as 400% higher labeling accuracy!). Going beyond euphemistic words, we also proposed a solution to detect and classify euphemistic phrases. We recently learned that euphemistic speech is also being used to circumvent vaccine misinformation filters on Facebook, which gives additional relevance to our work. Although we do not have access to relevant Facebook data, our proposed approach could easily be used to tackle this problem. \n\n \n\n\t\t\t\t\tLast Modified: 09/30/2021\n\n\t\t\t\t\tSubmitted by: Suma Bhat"
 }
}