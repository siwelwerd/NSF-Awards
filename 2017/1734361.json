{
 "awd_id": "1734361",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Mutually Aware Social Navigation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924702",
 "po_email": "cye@nsf.gov",
 "po_sign_block_name": "Cang Ye",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 743549.0,
 "awd_amount": 743549.0,
 "awd_min_amd_letter_date": "2017-08-29",
 "awd_max_amd_letter_date": "2022-06-15",
 "awd_abstract_narration": "This project seeks to provide robots with the social intelligence to be aware of the mutual\r\ndependency between their movements and the movements of humans around them. To this\r\nend, the work will focus on (1) improving the way robots reason about spatial behavior, and (2)\r\ndeveloping navigation methods that lead to understandable and appropriate motion patterns in\r\nsocial environments. This project will build upon prior work in robot perception and social\r\nbehavior in crowds and groups. This work will impact the future use of robots in many application domains, \r\nespecially for those where people untrained in robotics are present (e.g., delivery robots, guide robots, etc.). \r\nAlmost all robots that move near people will need to behave appropriately, so it is necessary to discover\r\nsocially intelligent navigation techniques, thereby increasing human acceptance and market\r\nsuccess. The team will also continue established and successful efforts in fostering diversity,\r\nintegrating education with research, disseminating new knowledge to the general public,\r\nindustry stakeholders, and other researchers.\r\n\r\nPrior work has identified the importance of human-aware navigation, and has developed\r\nmethods to incorporate the social norms that govern human physical space into aspects of robot\r\npath planning. Building on this foundational work, the team will address three main social\r\nintelligence tasks: (1) enabling robots to reason jointly about nearby human spatial behavior and\r\ntheir own, (2) enabling robots to communicate their intentions as they navigate so that their\r\nmotion is understandable by nearby humans, and (3) giving robots the ability to decide when it is\r\nacceptable to violate pre-established social conventions. Research in these areas is\r\nincomplete since most efforts do not include awareness or reasoning about mutual dependency.\r\nThis makes it difficult for a robot to reason intelligently on how to alter crowd motions in a\r\nsocially appropriate manner. Methods discovered by the team will also support the case where\r\nmultiple robots must mix with multiple humans.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aaron",
   "pi_last_name": "Steinfeld",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aaron Steinfeld",
   "pi_email_addr": "steinfeld@cmu.edu",
   "nsf_id": "000158622",
   "pi_start_date": "2017-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 743549.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project provided robots with increased ability to move in socially appropriate manner around humans. To this end, the intellectual merit aspects of this work focused on (1) improving the way robots reason about spatial behavior and (2) developing advances in navigation methods that lead to understandable and appropriate motion patterns in social environments. This project built upon prior work in robot perception and social behavior in crowds and groups. Key advances under the project center on reasoning about human motion from a group-based perspective, rather than individual human trajectories. This conceptual shift led to significant performance improvements and mitigation of scenarios where robots freeze in place during crowded scenes. The broader impacts of this work will support future robots in many application domains, especially for those where people untrained in robotics are present. Almost all robots that move near people will need to behave appropriately, so it is necessary to advance socially intelligent navigation techniques, thereby increasing human acceptance and market success. The project has also supported the creation of new evaluation benchmarks, software tools, and data for use by other researchers. The latter includes the largest, to date, publicly available dataset of pedestrian motion that includes (1) ground-truth labels in metric space of pedestrian positions and trajectories, (2) both overhead and an ego-centric views, and (3) natural data collected in a public setting.&nbsp;</p><br>\n<p>\n Last Modified: 12/30/2023<br>\nModified by: Aaron&nbsp;Steinfeld</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1734361/1734361_10519192_1703962535405_set2_combined--rgov-214x142.jpeg\" original=\"/por/images/Reports/POR/2023/1734361/1734361_10519192_1703962535405_set2_combined--rgov-800width.jpeg\" title=\"TBD Pedestrian Dataset\"><img src=\"/por/images/Reports/POR/2023/1734361/1734361_10519192_1703962535405_set2_combined--rgov-66x44.jpeg\" alt=\"TBD Pedestrian Dataset\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Three views from a public dataset on natural pedestrian motion. (a) An overhead camera view with traces showing pedestrian motion. (b) An overhead tracking views using lidar and camera data. (c) Fused forward views showing RBG and depth data, with pedestrian locations labelled.</div>\n<div class=\"imageCredit\">Carnegie Mellon University</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Aaron&nbsp;Steinfeld\n<div class=\"imageTitle\">TBD Pedestrian Dataset</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project provided robots with increased ability to move in socially appropriate manner around humans. To this end, the intellectual merit aspects of this work focused on (1) improving the way robots reason about spatial behavior and (2) developing advances in navigation methods that lead to understandable and appropriate motion patterns in social environments. This project built upon prior work in robot perception and social behavior in crowds and groups. Key advances under the project center on reasoning about human motion from a group-based perspective, rather than individual human trajectories. This conceptual shift led to significant performance improvements and mitigation of scenarios where robots freeze in place during crowded scenes. The broader impacts of this work will support future robots in many application domains, especially for those where people untrained in robotics are present. Almost all robots that move near people will need to behave appropriately, so it is necessary to advance socially intelligent navigation techniques, thereby increasing human acceptance and market success. The project has also supported the creation of new evaluation benchmarks, software tools, and data for use by other researchers. The latter includes the largest, to date, publicly available dataset of pedestrian motion that includes (1) ground-truth labels in metric space of pedestrian positions and trajectories, (2) both overhead and an ego-centric views, and (3) natural data collected in a public setting.\t\t\t\t\tLast Modified: 12/30/2023\n\n\t\t\t\t\tSubmitted by: AaronSteinfeld\n"
 }
}