{
 "awd_id": "1744111",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Centaur: A Bio-inspired Ultra Low-Power Hybrid Embedded Computing Engine Beyond One TeraFlops/Watt",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2017-05-01",
 "awd_exp_date": "2018-05-31",
 "tot_intn_awd_amt": 246649.0,
 "awd_amount": 246648.0,
 "awd_min_amd_letter_date": "2017-06-29",
 "awd_max_amd_letter_date": "2017-09-19",
 "awd_abstract_narration": "The objective of the research is to innovate an embedded computing engine named ?Centaur? to achieve ultra-high power efficiency by adopting the bio-inspired computation model and the advanced memristor technology. \r\nThree constituent elements are included to address the major technical obstacles: (1) The power-efficient hybrid computing system that integrates memristor-based synapse network and crossbar structure, targeting the flexible and intensive data processing, respectively. (2) The robust design methodology for Centaur, including the circuit and algorithm enhancements as well as the necessary EDA flow. (3) The integration of Centaur into modern heterogeneous systems and the prototype demonstration. Creative applications of critical importance to nowadays mobile and embedded systems by taking the full advantages of Centaur, including pattern recognition and video and image processing, will be also explored.\r\nThe research can benefit the embedded system community by the revolutions in computing architecture and hardware design for functional variety, power-efficiency, and cost. The results can further benefit the semiconductor and neuromorphic societies at large by stimulating the interaction between the advances in device engineering and computing models. The developed techniques will be transferred to mainstream practices under the close collaborations with several industry partners, and directly impact the future embedded systems. The activities in the collaboration also include the tutorials in the major conferences on the technical aspects of the projects and new course development. The educational plan will enhance the existing standard curricula by integrating new modules on emerging memristor-based computing architecture and the relevant neuromorphic computing model.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yiran",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yiran Chen",
   "pi_email_addr": "yiran.chen@duke.edu",
   "nsf_id": "000575362",
   "pi_start_date": "2017-06-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 49631.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 96722.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 100296.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The major goals of the projects are three folded: 1)Designing a novel hybrid embedded computing engine that integrates the memristor-based synapse network and crossbar structure for flexible and intensive data processing, respectively; 2) Investigating the robust design methodology for the proposed computing engine, including both hardware and algorithm enhancements; and 3) Exploiting the approaches to seamlessly integrate the computing engine into modern heterogeneous systems and demonstrate the prototypes.</p>\n<p>Convolutional neural networks (CNNs) are the heart of deep learning applications. Recent works demonstrated the promise of using resistive random access memory (ReRAM, a.k.a. memristor) to perform neural computations in memory. We found that training cannot be efficiently supported with the current schemes. For example, the existing techniques do not consider weight update and complex data dependency in training procedure. Also, the deep pipeline is vulnerable to pipeline bubbles and execution stall. Therefore, we developed PipeLayer - a ReRAM-based process-in-memory (PIM) accelerator for CNNs that support both training and testing in a manner of pipelining.</p>\n<p>Although ReRAM-based convolutional neural network (CNN) accelerators like PipeLayer have been widely studied, state-of-the-art solutions suffer from either incapability of training (e.g., ISSAC) or inefficiency of inference (e.g., PipeLayer) due to the pipeline design. To solve this problem, we further propose AtomLayer - a universal ReRAM-based accelerator to support both training and inference of CNN.</p>\n<p>Most of these systems are composed of a single crossbar layer, in many of which system training is done off-chip, i.e., using computer based simulations then the trained weights are pre-programmed to the memristor crossbar array. However, as AtomLayer demonstrated, multi-layered on-chip trained systems become crucial for handling massive amount of data and to overcome the resistance shift that occurs to memristors overtime. Hence, we propose a spiking-based multi-layered neuromorphic computing system capable of online training, and evaluate the effectiveness and robustness of the proposed technique.</p>\n<p>However, the currently low manufacturing reliability of nano-devices and the voltage IR-drop along metal wires and memristors arrays severely limits the scale of memristor crossbar based neuromorphic computing systems (NCS) and hinders the design scalability. We propose a novel system reduction scheme that significantly lowers the required dimension of the memristor crossbars in NCS while maintaining high computing accuracy. An IR-drop compensation technique is also proposed to overcome the adverse impacts of the wire resistance and the sneak path problem in large memristor crossbar designs.</p>\n<p>Training methods have been proposed to program the memristors in a crossbar by following existing training algorithms in neural network models. However, the robustness of these training methods has not been well investigated by taking into account the limits imposed by realistic hardware implementations. We perform a quantitative analysis on the impact of device imperfections and circuit design constraints on the robustness of two popular training methods - &ldquo;close-loop on-device&rdquo; (CLD) and &ldquo;open-loop off-device&rdquo; (OLD). A novel variation-aware training scheme, namely, Vortex, is then invented to enhance the training robustness of memristor crossbar-based NCS by actively compensating the impact of device variations and optimizing the mapping scheme from computations to crossbars.</p>\n<p>As a spintronic type of memristor devices, skyrmions racetrack memory (SKM) has been identified as a promising candidate for future on-chip cache. Similar to many other nanoscale technologies, process variations adversely impact the reliability and performance of SKM cache. To overcome this design issue, we propose a holistic solution for employing SKM as last-level caches by leveraging several architecture and circuit-level techniques.</p>\n<p>Based on our robust designs of memristor crossbar-based neuromorphic computing accelerators (NCAs), we propose Harmonica &ndash; a framework of heterogeneous computing systems enhanced with memristor-based NCAs. In Harmonica, a conventional pipeline is augmented with a NCA which is designed to speedup artificial neural network (ANN) relevant executions. An inline calibration scheme is proposed to guarantee the computation accuracy degradation incurred by the memristor resistance shifting during the NCA executions within an acceptable range. A thorough comparison between Harmonica vs. a pure digital neural processing unit (D-NPU) and a mixed-signal design with MBC arrays cooperating through a digital interconnection network is also conducted to show the substantial advantages of Harmonica in improving computational efficiency and system robustness.</p>\n<p>Finally, we introduce several security concerns in cognitive system designs. Some real examples are then used to demonstrate how the attackers can potentially access the confidential user data, replicate a sensitive data processing model without being granted the access to the details of the model, and obtain some key features of the training data by using the services publically accessible to a normal user. Based on the analysis of these security challenges, we also discuss several possible memristor-based solutions that can protect the information privacy and security of cognitive systems during different stages of the usage.</p>\n<p>This 5-year research project supported 16 graduate students and publications of 20 conference papers and 6 journal papers in total.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/09/2018<br>\n\t\t\t\t\tModified by: Yiran&nbsp;Chen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe major goals of the projects are three folded: 1)Designing a novel hybrid embedded computing engine that integrates the memristor-based synapse network and crossbar structure for flexible and intensive data processing, respectively; 2) Investigating the robust design methodology for the proposed computing engine, including both hardware and algorithm enhancements; and 3) Exploiting the approaches to seamlessly integrate the computing engine into modern heterogeneous systems and demonstrate the prototypes.\n\nConvolutional neural networks (CNNs) are the heart of deep learning applications. Recent works demonstrated the promise of using resistive random access memory (ReRAM, a.k.a. memristor) to perform neural computations in memory. We found that training cannot be efficiently supported with the current schemes. For example, the existing techniques do not consider weight update and complex data dependency in training procedure. Also, the deep pipeline is vulnerable to pipeline bubbles and execution stall. Therefore, we developed PipeLayer - a ReRAM-based process-in-memory (PIM) accelerator for CNNs that support both training and testing in a manner of pipelining.\n\nAlthough ReRAM-based convolutional neural network (CNN) accelerators like PipeLayer have been widely studied, state-of-the-art solutions suffer from either incapability of training (e.g., ISSAC) or inefficiency of inference (e.g., PipeLayer) due to the pipeline design. To solve this problem, we further propose AtomLayer - a universal ReRAM-based accelerator to support both training and inference of CNN.\n\nMost of these systems are composed of a single crossbar layer, in many of which system training is done off-chip, i.e., using computer based simulations then the trained weights are pre-programmed to the memristor crossbar array. However, as AtomLayer demonstrated, multi-layered on-chip trained systems become crucial for handling massive amount of data and to overcome the resistance shift that occurs to memristors overtime. Hence, we propose a spiking-based multi-layered neuromorphic computing system capable of online training, and evaluate the effectiveness and robustness of the proposed technique.\n\nHowever, the currently low manufacturing reliability of nano-devices and the voltage IR-drop along metal wires and memristors arrays severely limits the scale of memristor crossbar based neuromorphic computing systems (NCS) and hinders the design scalability. We propose a novel system reduction scheme that significantly lowers the required dimension of the memristor crossbars in NCS while maintaining high computing accuracy. An IR-drop compensation technique is also proposed to overcome the adverse impacts of the wire resistance and the sneak path problem in large memristor crossbar designs.\n\nTraining methods have been proposed to program the memristors in a crossbar by following existing training algorithms in neural network models. However, the robustness of these training methods has not been well investigated by taking into account the limits imposed by realistic hardware implementations. We perform a quantitative analysis on the impact of device imperfections and circuit design constraints on the robustness of two popular training methods - \"close-loop on-device\" (CLD) and \"open-loop off-device\" (OLD). A novel variation-aware training scheme, namely, Vortex, is then invented to enhance the training robustness of memristor crossbar-based NCS by actively compensating the impact of device variations and optimizing the mapping scheme from computations to crossbars.\n\nAs a spintronic type of memristor devices, skyrmions racetrack memory (SKM) has been identified as a promising candidate for future on-chip cache. Similar to many other nanoscale technologies, process variations adversely impact the reliability and performance of SKM cache. To overcome this design issue, we propose a holistic solution for employing SKM as last-level caches by leveraging several architecture and circuit-level techniques.\n\nBased on our robust designs of memristor crossbar-based neuromorphic computing accelerators (NCAs), we propose Harmonica &ndash; a framework of heterogeneous computing systems enhanced with memristor-based NCAs. In Harmonica, a conventional pipeline is augmented with a NCA which is designed to speedup artificial neural network (ANN) relevant executions. An inline calibration scheme is proposed to guarantee the computation accuracy degradation incurred by the memristor resistance shifting during the NCA executions within an acceptable range. A thorough comparison between Harmonica vs. a pure digital neural processing unit (D-NPU) and a mixed-signal design with MBC arrays cooperating through a digital interconnection network is also conducted to show the substantial advantages of Harmonica in improving computational efficiency and system robustness.\n\nFinally, we introduce several security concerns in cognitive system designs. Some real examples are then used to demonstrate how the attackers can potentially access the confidential user data, replicate a sensitive data processing model without being granted the access to the details of the model, and obtain some key features of the training data by using the services publically accessible to a normal user. Based on the analysis of these security challenges, we also discuss several possible memristor-based solutions that can protect the information privacy and security of cognitive systems during different stages of the usage.\n\nThis 5-year research project supported 16 graduate students and publications of 20 conference papers and 6 journal papers in total.\n\n \n\n\t\t\t\t\tLast Modified: 07/09/2018\n\n\t\t\t\t\tSubmitted by: Yiran Chen"
 }
}