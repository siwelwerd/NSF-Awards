{
 "awd_id": "1741312",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research: Malleable Media to Support Interaction through Bi-Directional Touch Displays",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 73794.0,
 "awd_amount": 73794.0,
 "awd_min_amd_letter_date": "2017-05-31",
 "awd_max_amd_letter_date": "2021-08-11",
 "awd_abstract_narration": "Interaction with touchscreens is compelling because, in part, it replicates interaction with objects in our environment; you can use a touchscreen to visually apprehend, reach toward, touch and manipulate an object.  Of course, this is only partly true because, at the last moment, your hand contacts glass and the rest of the manipulation is simulated.  For many applications, the glass barrier is an acceptable compromise because the simulated response of the on-screen application to your action (delivered visually) is sufficient to close a perceptual gap, to fool you into believing that you are actually touching a button or icon; the visual feedback serves as proxy for the missing tactile interaction with the object.  However, in some situations, and for some users, the presence of the glass barrier causes the interaction to fail completely.  For blind users in particular, for whom closing a perceptual loop through vision is not an option, other means of interaction with computationally mediated environments must be developed.  The primary objective of this exploratory project is to develop a full-page interactive tactile display that can render what the PIs term \"malleable media\" by combining touch sensing with their existing microfluidic actuators to create a full-page responsive surface.  Such a display could provide an immediate tangible response in reaction to how it is being touched.  By transcending the glass barrier of the screen, such a display would quite literally support a direct manipulation paradigm by making both interface elements, such as buttons and icons, and application content touchable.  Thus, project outcomes are likely to provide greater accessibility to digital media by blind users.  But the sighted community also stands to benefit.  Both students and professionals working in STEM fields require access to increasingly high dimensional data that cannot be easily accessed via speech. An interactive tactile display such as that which is the focus of this work would provide such access, to blind computer users and their sighted counterparts alike.\r\n\r\nThe intellectual merit of this exploratory project derives from combining two strands of existing research by the PIs.  The first is their recent work on a microfluidic tactile display, while the second is their established record in applying principles of enactive cognition to the design of human-computer interfaces.  Enactive cognition posits a tight coupling between our actions on the environment and our perception of how the environment is responding.  With the new technology the PIs have the means of creating such an interactive surface, but it is not yet known what such interactions should \"feel\" like.  What would it be like to sculpt an object by molding the surface of a tablet computer with one's hands?  For blind users, this would provide a means for not just feeling tactile objects, but also for creating those objects.  Such objects might be used as new ways of representing, for example, patterns present in complex data sets or of interacting with mathematical models.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Coughlan",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "James M Coughlan",
   "pi_email_addr": "coughlan@ski.org",
   "nsf_id": "000117895",
   "pi_start_date": "2017-05-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Smith-Kettlewell Eye Research Foundation",
  "inst_street_address": "2318 FILLMORE ST",
  "inst_street_address_2": "",
  "inst_city_name": "SAN FRANCISCO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "4153452035",
  "inst_zip_code": "941151813",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "CA11",
  "org_lgl_bus_name": "THE SMITH-KETTLEWELL EYE RESEARCH INSTITUTE",
  "org_prnt_uei_num": "VT22KLLMNQL6",
  "org_uei_num": "VT22KLLMNQL6"
 },
 "perf_inst": {
  "perf_inst_name": "Smith-Kettlewell Eye Research Foundation",
  "perf_str_addr": "2318 Fillmore Street",
  "perf_city_name": "San Francisco",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "941151813",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "CA11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 73794.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our work is aimed at supporting greater access to digital media for blind computer users, in particular through refreshable braille and interactive tactile graphics that complements speech and audio.&nbsp; Our approaches tightly couple technology development and interaction science in the domain of touchscreens featuring shape display and surface haptics.&nbsp; Using recently developed technology (including our own novel refreshable braille and tactile graphic devices) for supporting bi-directional interaction through touch, we have developed tactile and audio-tactile interaction paradigms that promise to revolutionize access to media for tablets and portable computing devices.&nbsp; The work carried out within this Eager is continuing through currently funded projects to ensure that the design approaches we developed here are immediately available to support new modes of interaction for blind computer users.&nbsp; This work has allowed us to apply established principles of human-computer interaction design to develop an entirely new repertoire of interaction techniques for an emerging class of devices, full-page tactile arrays for the display of braille and tactile graphics.</p>\n<p>&nbsp;</p>\n<p>This project will significantly reduce the burden of accessing information for the 1.5 million blind people in the United States by making a full page of braille text and tactile graphics available in a device resembling a tablet computer.&nbsp; Such a display will provide increased access to braille and enable blind students to read digitized spatial content including mathematical equations, graphs, and figures with their fingers, creating parity with their sighted counterparts interested in STEM fields.&nbsp; The products and interaction techniques that developed from the work carried out in this project will improve braille literacy, opportunities in STEM fields, and ultimately the employment success and independence of blind Americans.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/19/2022<br>\n\t\t\t\t\tModified by: James&nbsp;M&nbsp;Coughlan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur work is aimed at supporting greater access to digital media for blind computer users, in particular through refreshable braille and interactive tactile graphics that complements speech and audio.  Our approaches tightly couple technology development and interaction science in the domain of touchscreens featuring shape display and surface haptics.  Using recently developed technology (including our own novel refreshable braille and tactile graphic devices) for supporting bi-directional interaction through touch, we have developed tactile and audio-tactile interaction paradigms that promise to revolutionize access to media for tablets and portable computing devices.  The work carried out within this Eager is continuing through currently funded projects to ensure that the design approaches we developed here are immediately available to support new modes of interaction for blind computer users.  This work has allowed us to apply established principles of human-computer interaction design to develop an entirely new repertoire of interaction techniques for an emerging class of devices, full-page tactile arrays for the display of braille and tactile graphics.\n\n \n\nThis project will significantly reduce the burden of accessing information for the 1.5 million blind people in the United States by making a full page of braille text and tactile graphics available in a device resembling a tablet computer.  Such a display will provide increased access to braille and enable blind students to read digitized spatial content including mathematical equations, graphs, and figures with their fingers, creating parity with their sighted counterparts interested in STEM fields.  The products and interaction techniques that developed from the work carried out in this project will improve braille literacy, opportunities in STEM fields, and ultimately the employment success and independence of blind Americans.\n\n\t\t\t\t\tLast Modified: 12/19/2022\n\n\t\t\t\t\tSubmitted by: James M Coughlan"
 }
}