{
 "awd_id": "1725657",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: Enabling Scalable Synchronizations for General Purpose GPUs",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 850000.0,
 "awd_amount": 850000.0,
 "awd_min_amd_letter_date": "2017-09-08",
 "awd_max_amd_letter_date": "2017-09-08",
 "awd_abstract_narration": "Today?s GPUs have been successfully used for general purpose processing including high-performance computing, machine learning, graph analytics, to name a few applications. However, general purpose computing has different complexity and characteristics from those of graphics processing. A crucial characteristic is the need for coordination and synchronization among parallel threads, which occurs more often in general purpose applications than in graphics rendering. However, unlike common CPU designs, current GPU designs are missing full-fledged support for synchronization primitives that are convenient for implementing transparent scalability and achieving high performance for many parallel applications.\r\n\r\nThis project develops truly effective, efficient, easy-to-use and easy-to-implement synchronization mechanisms for GPGPU while overcoming major obstacles pertaining to the GPU architecture. The first thrust will identify GPGPU synchronization primitives that are deemed sufficient to handle a large set of applications with static and dynamic dependencies. The second thrust will provide solutions and methodologies on how to use the new primitives in the most efficient way to suit the characteristics of applications. The third thrust addresses the main challenges in implementing those primitives in efficiency, possible context switching overhead, and memory capacity limitations. The success in this project will help fuel the spread of accelerator architectures for high-performance computing, cloud and mobile systems. It will increase efficiency for improved performance and reduced energy/power consumption, leading to a greener IT industry. Trained students will become future taskforce to continue the revolution of extending computing into every realm of science, life, commerce and culture.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jun",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jun Yang",
   "pi_email_addr": "juy9@pitt.edu",
   "nsf_id": "000106419",
   "pi_start_date": "2017-09-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Rami",
   "pi_last_name": "Melhem",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rami Melhem",
   "pi_email_addr": "melhem@cs.pitt.edu",
   "nsf_id": "000214915",
   "pi_start_date": "2017-09-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pittsburgh",
  "inst_street_address": "4200 FIFTH AVENUE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4126247400",
  "inst_zip_code": "152600001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "MKAGLD59JRL1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pittsburgh",
  "perf_str_addr": "",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152132303",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 850000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>GPGPUs are widely used in various fields to speed up large tasks, but they inherit limitations from their origins as graphics processors, lacking some CPU features. This project aims to bridge the gap between GPGPUs and CPUs by developing efficient, hardware-supported synchronization mechanisms for parallel applications. We create effective synchronization primitives for widespread application use, optimize their usage across different scenarios, and enhance hardware support to address GPU-specific challenges. These advancements will also benefit other GPU aspects like context switching and memory management.</p>\n<p>We initially aimed to develop synchronization primitives for both global and local contexts. The first year focused on creating _globalSync(), a global synchronization barrier usable by all threads to synchronize simultaneously. We explored its syntax, semantics, the limitations of existing solutions, and its application in various scenarios. We also developed strategies to avoid event overload with large datasets, devised architectural implementations, and tackled challenges like context switch overheads. Techniques to minimize the impact of these overheads on performance were also developed, enhancing kernel preemption and scalability.</p>\n<p>In our further investigation, we discovered that mismanagement of GPGPU memory significantly impacts overall performance, particularly when running large applications across multiple GPUs. We first observed that GPGPU&rsquo;s memory capacity often limits its potential to support large or multiple tasks simultaneously. Modern GPGPUs support unified memory and demand paging. However, when application working sets exceed memory capacity, the resulting data movement can cause great performance loss. We developed a memory management framework that enhances GPU performance under memory oversubscription by combining techniques to manage eviction latency, reduce thrashing, and expand effective memory without needing more physical capacity. Our ETC framework&mdash;encompassing proactive eviction, memory-aware throttling, and capacity compression&mdash;adapts dynamically to different workloads, offering tailored memory management solutions. Through online application characterization, we categorize applications to apply the most suitable ETC techniques, resulting in significant performance improvements over current methods in both real and simulated GPGPU environments.</p>\n<p>Unified Virtual Memory (UVM) automates data paging and migration, addressing memory oversubscription by replacing pages as needed. Yet, straightforward page fault management significantly slows performance. We found that while prefetching data improves latency issues, its efficiency varies, often becoming ineffective when GPU memory reaches capacity. As GPU memory reaches capacity, traditional prefetching becomes less effective due to a lack of eviction policy awareness. To address this, we developed new eviction policies attuned to hardware prefetcher semantics, introducing two innovative, locality-aware pre-eviction strategies. These strategies work seamlessly with existing hardware prefetchers, enhancing performance without additional costs or implementation efforts.</p>\n<p>To address memory constraints in data-intensive workloads, we furthered our research on adaptive memory management. We noted that performance under memory oversubscription varies with the workload's memory access pattern, affecting both regular and irregular applications differently. Our advancements include leveraging remote zero-copy access and utilizing hardware access counters to mitigate page migration and memory thrashing. We developed a runtime that automatically adjusts memory strategies based on access patterns, aiming for a balance between low-latency and high-bandwidth access, ensuring our approach enhances performance without affecting workloads fitting within device-local memory.</p>\n<p>In our deep dive into parallel applications on GPGPU systems, we've pinpointed critical factors influencing performance. Previously focusing on memory capacity, we've found that memory access latencies, especially address-translation lookups, significantly affect performance. Large memory footprints lead to TLB misses, causing latencies that worsen in multi-GPU systems due to IOMMU contention. Our studies revealed severe thrashing in remote IOMMU TLBs, leading to significant performance drops. We developed optimization techniques improving TLB hit rates and reducing IOMMU access latencies by leveraging translation sharing and reducing redundancy.</p>\n<p>Amid growing GPU memory demands, multi-GPU systems like NVIDIA DGX and Intel Xe, utilizing unified virtual memory, simplify application deployment but struggle with address translation latency and NUMA overheads. We introduce Trans-FW to enhance multi-GPU efficiency by addressing three latency sources: queuing for page table walk threads, cache miss-induced memory accesses, and page fault handling. Additionally, we propose IDYLL, a software-hardware co-design reducing page table invalidation overheads through a managed directory and hardware buffer, streamlining translation coherence and boosting performance.</p>\n<p>In summary, this project advances GPGPU systems by addressing key performance challenges. Initially focusing on synchronization limitations, we developed global and local synchronization primitives, notably _globalSync(), to enhance parallel application performance. Subsequent research identified memory management as a critical bottleneck, leading to the creation of the ETC framework for dynamic memory management and the introduction of novel eviction policies in response to prefetch inefficiencies. We further tackled memory access latency and address translation issues, culminating in the Trans-FW and IDYLL solutions to optimize multi-GPU system efficiency. These innovations collectively target the inherent weaknesses of GPGPUs, offering significant performance improvements across various applications.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/25/2024<br>\nModified by: Jun&nbsp;Yang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nGPGPUs are widely used in various fields to speed up large tasks, but they inherit limitations from their origins as graphics processors, lacking some CPU features. This project aims to bridge the gap between GPGPUs and CPUs by developing efficient, hardware-supported synchronization mechanisms for parallel applications. We create effective synchronization primitives for widespread application use, optimize their usage across different scenarios, and enhance hardware support to address GPU-specific challenges. These advancements will also benefit other GPU aspects like context switching and memory management.\n\n\nWe initially aimed to develop synchronization primitives for both global and local contexts. The first year focused on creating _globalSync(), a global synchronization barrier usable by all threads to synchronize simultaneously. We explored its syntax, semantics, the limitations of existing solutions, and its application in various scenarios. We also developed strategies to avoid event overload with large datasets, devised architectural implementations, and tackled challenges like context switch overheads. Techniques to minimize the impact of these overheads on performance were also developed, enhancing kernel preemption and scalability.\n\n\nIn our further investigation, we discovered that mismanagement of GPGPU memory significantly impacts overall performance, particularly when running large applications across multiple GPUs. We first observed that GPGPUs memory capacity often limits its potential to support large or multiple tasks simultaneously. Modern GPGPUs support unified memory and demand paging. However, when application working sets exceed memory capacity, the resulting data movement can cause great performance loss. We developed a memory management framework that enhances GPU performance under memory oversubscription by combining techniques to manage eviction latency, reduce thrashing, and expand effective memory without needing more physical capacity. Our ETC frameworkencompassing proactive eviction, memory-aware throttling, and capacity compressionadapts dynamically to different workloads, offering tailored memory management solutions. Through online application characterization, we categorize applications to apply the most suitable ETC techniques, resulting in significant performance improvements over current methods in both real and simulated GPGPU environments.\n\n\nUnified Virtual Memory (UVM) automates data paging and migration, addressing memory oversubscription by replacing pages as needed. Yet, straightforward page fault management significantly slows performance. We found that while prefetching data improves latency issues, its efficiency varies, often becoming ineffective when GPU memory reaches capacity. As GPU memory reaches capacity, traditional prefetching becomes less effective due to a lack of eviction policy awareness. To address this, we developed new eviction policies attuned to hardware prefetcher semantics, introducing two innovative, locality-aware pre-eviction strategies. These strategies work seamlessly with existing hardware prefetchers, enhancing performance without additional costs or implementation efforts.\n\n\nTo address memory constraints in data-intensive workloads, we furthered our research on adaptive memory management. We noted that performance under memory oversubscription varies with the workload's memory access pattern, affecting both regular and irregular applications differently. Our advancements include leveraging remote zero-copy access and utilizing hardware access counters to mitigate page migration and memory thrashing. We developed a runtime that automatically adjusts memory strategies based on access patterns, aiming for a balance between low-latency and high-bandwidth access, ensuring our approach enhances performance without affecting workloads fitting within device-local memory.\n\n\nIn our deep dive into parallel applications on GPGPU systems, we've pinpointed critical factors influencing performance. Previously focusing on memory capacity, we've found that memory access latencies, especially address-translation lookups, significantly affect performance. Large memory footprints lead to TLB misses, causing latencies that worsen in multi-GPU systems due to IOMMU contention. Our studies revealed severe thrashing in remote IOMMU TLBs, leading to significant performance drops. We developed optimization techniques improving TLB hit rates and reducing IOMMU access latencies by leveraging translation sharing and reducing redundancy.\n\n\nAmid growing GPU memory demands, multi-GPU systems like NVIDIA DGX and Intel Xe, utilizing unified virtual memory, simplify application deployment but struggle with address translation latency and NUMA overheads. We introduce Trans-FW to enhance multi-GPU efficiency by addressing three latency sources: queuing for page table walk threads, cache miss-induced memory accesses, and page fault handling. Additionally, we propose IDYLL, a software-hardware co-design reducing page table invalidation overheads through a managed directory and hardware buffer, streamlining translation coherence and boosting performance.\n\n\nIn summary, this project advances GPGPU systems by addressing key performance challenges. Initially focusing on synchronization limitations, we developed global and local synchronization primitives, notably _globalSync(), to enhance parallel application performance. Subsequent research identified memory management as a critical bottleneck, leading to the creation of the ETC framework for dynamic memory management and the introduction of novel eviction policies in response to prefetch inefficiencies. We further tackled memory access latency and address translation issues, culminating in the Trans-FW and IDYLL solutions to optimize multi-GPU system efficiency. These innovations collectively target the inherent weaknesses of GPGPUs, offering significant performance improvements across various applications.\n\n\n\t\t\t\t\tLast Modified: 02/25/2024\n\n\t\t\t\t\tSubmitted by: JunYang\n"
 }
}