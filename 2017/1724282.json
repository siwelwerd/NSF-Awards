{
 "awd_id": "1724282",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: INT: Inference, Reasoning, and Learning for Robust Autonomous Driving",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Corman",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 1398587.0,
 "awd_amount": 1398587.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2021-06-24",
 "awd_abstract_narration": "While research in autonomous driving has made great strides in recent years, fully autonomous cars are still a distant goal, primarily because of a lack of robustness. Current autonomous cars cannot drive on new roads, or roads that have changed substantially (such as after an earthquake), or when there is a GPS or data outage such as in parking garages, urban cities and tunnels. Importantly, humans are good at all of this: Humans can drive without detailed maps or high precision GPS/IMU sensors, and typically require only a small amount of sparse information for guidance, and their performance typically gets better over time through learning. Using the \"intelligent\" human driver as a guide, the planned research will develop algorithms that can perceive and make predictions about a scene in real time with measurable confidence, particularly as the scene is closer to the car. New robustness characteristics will be achieved through the ability to detect and overcome mistakes, both in the near term (real time) and long term (learning). The planned algorithms will be designed and validated in a way to enable an inherent robustness not currently available in autonomous driving, and fast adoption by the community. This project is aligned with NSF's Intelligent Physical Systems (IPS) because the algorithms will require cognizant and reflective capabilities in a knowledge-rich environment. Additionally, outputs of this project will impact robotics, machine learning and cyber-physical systems. Educationally, data logs will be disseminated to enable open ended student projects in the community, and undergrad and high school students will collaborate with the research team to integrate sensors, perform experiments and data collection, and disseminate data logs to the community. \r\n\r\nLed by researchers in Mechanical and Aerospace Engineering, and Computer Science at Cornell University, the goal of this research is to develop, integrate and validate theory and algorithms to enable robust and persistent autonomous driving. This project is aligned with NSF's Intelligent Physical Systems (IPS) because the algorithms will require cognizant and reflective capabilities in a knowledge-rich environment. The technical approach will develop a robust perceptual pipeline for detection, scene estimation, prediction, and anomaly/mistake detection and learning; integrate the algorithms into Cornell's autonomous car software framework and validate the components and system in a series of experimental scenarios to enable their faster adoption by the community. Key component level algorithms to be developed include anytime deep learning detectors with quantifiable performance; multiple hypothesis reasoning with memory attributes; generalized probabilistic anticipation algorithms to mimic a human's mental model of a dynamic scene; and anomaly/mistake detection coupled with online learning. Outcomes will include open source algorithms and data logs; publications, conferences, workshops; data logs for open ended projects in courses and across the community; and undergrad and high school education and diversity programs in the interdisciplinary area of autonomous driving.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Campbell",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Mark E Campbell",
   "pi_email_addr": "mc288@cornell.edu",
   "nsf_id": "000214501",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Kilian",
   "pi_last_name": "Weinberger",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kilian Weinberger",
   "pi_email_addr": "kilianweinberger@cornell.edu",
   "nsf_id": "000576980",
   "pi_start_date": "2017-07-27",
   "pi_end_date": "2020-05-05"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kilian",
   "pi_last_name": "Weinberger",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kilian Weinberger",
   "pi_email_addr": "kilianweinberger@cornell.edu",
   "nsf_id": "000576980",
   "pi_start_date": "2021-06-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Bharath",
   "pi_last_name": "Hariharan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bharath Hariharan",
   "pi_email_addr": "bh497@cornell.edu",
   "nsf_id": "000760500",
   "pi_start_date": "2020-05-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "550 Upson Hall",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148537501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 1398587.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Research in autonomous driving has made great strides recently, but their full deployment is still a distant goal, primarily because of a lack of robustness. Key challenges still exist including driving where there is no GPS or no data coverage (e.g., parking garages, urban cities), and in poor weather conditions (e.g., snow, rain, at night). Importantly, humans are good at all of this &ndash; without the requirement of detailed maps or high precision sensors. Using the intelligent human driver as a guide, this research project developed algorithms that can perceive and make reliable predictions about a scene in real time with measurable confidence. New robustness characteristics are achieved through the ability to detect and overcome mistakes, both in the near term (real time) and long term (learning). The following summarizes specific outcomes of the project:</span><span>&nbsp;</span></p>\n<p><strong><span>Pseudo-lidar:</span></strong><span> One of the most impactful research contributions of the project was the development of Pseudo-LIDAR, which drastically improved performance of camera/vision detectors. Specifically, by analyzing state-of-the-art approaches using LIDAR and vision, a key conclusion was that the difference is not the quantity of the sensory data, but instead the internal representation used for the deep networks that perform the object detection. By first creating 3D point clouds from the vision data that mimic LiDAR input data, and then performing 3D object detection as if the data was obtained from a LiDAR sensor, the approach immediately became the new state-of-the-art and image-based 3D object detection from 22% to 74% accuracy. The work was immediately leveraged by other universities in research and companies in their pipelines.</span></p>\n<p><strong><span>Ithaca365 Dataset:</span></strong><span> The project developed and published a dataset, called Ithaca 365, which uniquely includes repeated traversals of the same route and varying scene and weather conditions. This work is impactful to the community both because of the repeated traversals create new avenues for research, and most other datasets are in nice weather conditions (without snow, rain, or poor lighting). Data is repeatedly recorded along a 15 km route under diverse scene (urban, highway, rural, campus), weather (snow, rain, sun), time (day/night), and traffic conditions (pedestrians, cyclists and cars). The dataset includes images and point clouds from cameras and LiDAR sensors, along with high-precision GPS/INS to establish correspondence across routes. A conference paper with benchmarks, and interface code were all open sourced to the community.</span></p>\n<p><strong><span>Repeated traversals:</span></strong><span> The project leveraged the repeated traversals in the Ithaca365 dataset in a series of contributions in the area of scene perception and object detection, particularly in unsupervised domain adaptation. Specifically, repeated traversals are very common &ndash; with cars driving the same routes via commuting or on the same roads/highways. Key is that this data is unlabeled, and thus unsupervised domain adaptation is required. Several approaches were developed in this project, such as creating maps from unlabeled data, developing better detectors in similar scenes, and leveraging statistics within the scene to improve subsequent prediction functions. The key outcome of this line of work is the ability of a self-driving car to reliably operate its perceptual system in the end-user&rsquo;s environment without additional annotation efforts.</span></p>\n<p><strong><span>Dreaming car:</span></strong><span> The project developed a novel approach - called Dreaming Cars - to reduce over-fit domain idiosyncrasies in deep learning detectors by fine-tuning the detector on high-quality pseudo-labels in the target domain &ndash; pseudo-labels that are automatically generated after driving based on replays of previously recorded driving sequences (i.e., at night, when the car is not moving, but `dreaming&rsquo;). In these replays, object tracks are smoothed forward and backward in time, and detections are interpolated and extrapolated&mdash; crucially, leveraging future information to catch hard cases such as missed detections due to occlusions or far ranges. Results show strong improvements detection reliability and accuracy.</span></p>\n<p><strong><span>Uncertainty quantification:</span></strong><span> The project developed an uncertainty quantification approach for deep learning architectures, with application to visual localization (although the approach is generalizable to other applications). Specifically, the quantification of uncertainties from prediction models (e.g., neural networks) is crucial for many robotics applications, especially for those in safety-critical domains such as self-driving cars. By mapping an internal output of the prediction model to the uncertainty, the deep learning outputs can be used similar to other sensors such as GPS and IMUs.</span></p>\n<p><strong><span>Transition and Training:</span></strong><span> In addition to the generation of algorithms and data, a total of 19 conference/journal papers were published. We open sourced the Ithaca365 dataset to the community, and for most papers, open sourced the original code with links within papers. A total of eight PhD students were funded under this project, of which six graduated with their PhD degrees. We have mentored two postdocs on this project, one of which transitioned to a faculty position at another university. We have made an explicit effort to incorporate undergraduates and students across multiple departments in the research, data collections and validation experiments.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/31/2023<br>\nModified by: Mark&nbsp;E&nbsp;Campbell</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nResearch in autonomous driving has made great strides recently, but their full deployment is still a distant goal, primarily because of a lack of robustness. Key challenges still exist including driving where there is no GPS or no data coverage (e.g., parking garages, urban cities), and in poor weather conditions (e.g., snow, rain, at night). Importantly, humans are good at all of this  without the requirement of detailed maps or high precision sensors. Using the intelligent human driver as a guide, this research project developed algorithms that can perceive and make reliable predictions about a scene in real time with measurable confidence. New robustness characteristics are achieved through the ability to detect and overcome mistakes, both in the near term (real time) and long term (learning). The following summarizes specific outcomes of the project:\n\n\nPseudo-lidar: One of the most impactful research contributions of the project was the development of Pseudo-LIDAR, which drastically improved performance of camera/vision detectors. Specifically, by analyzing state-of-the-art approaches using LIDAR and vision, a key conclusion was that the difference is not the quantity of the sensory data, but instead the internal representation used for the deep networks that perform the object detection. By first creating 3D point clouds from the vision data that mimic LiDAR input data, and then performing 3D object detection as if the data was obtained from a LiDAR sensor, the approach immediately became the new state-of-the-art and image-based 3D object detection from 22% to 74% accuracy. The work was immediately leveraged by other universities in research and companies in their pipelines.\n\n\nIthaca365 Dataset: The project developed and published a dataset, called Ithaca 365, which uniquely includes repeated traversals of the same route and varying scene and weather conditions. This work is impactful to the community both because of the repeated traversals create new avenues for research, and most other datasets are in nice weather conditions (without snow, rain, or poor lighting). Data is repeatedly recorded along a 15 km route under diverse scene (urban, highway, rural, campus), weather (snow, rain, sun), time (day/night), and traffic conditions (pedestrians, cyclists and cars). The dataset includes images and point clouds from cameras and LiDAR sensors, along with high-precision GPS/INS to establish correspondence across routes. A conference paper with benchmarks, and interface code were all open sourced to the community.\n\n\nRepeated traversals: The project leveraged the repeated traversals in the Ithaca365 dataset in a series of contributions in the area of scene perception and object detection, particularly in unsupervised domain adaptation. Specifically, repeated traversals are very common  with cars driving the same routes via commuting or on the same roads/highways. Key is that this data is unlabeled, and thus unsupervised domain adaptation is required. Several approaches were developed in this project, such as creating maps from unlabeled data, developing better detectors in similar scenes, and leveraging statistics within the scene to improve subsequent prediction functions. The key outcome of this line of work is the ability of a self-driving car to reliably operate its perceptual system in the end-users environment without additional annotation efforts.\n\n\nDreaming car: The project developed a novel approach - called Dreaming Cars - to reduce over-fit domain idiosyncrasies in deep learning detectors by fine-tuning the detector on high-quality pseudo-labels in the target domain  pseudo-labels that are automatically generated after driving based on replays of previously recorded driving sequences (i.e., at night, when the car is not moving, but `dreaming). In these replays, object tracks are smoothed forward and backward in time, and detections are interpolated and extrapolated crucially, leveraging future information to catch hard cases such as missed detections due to occlusions or far ranges. Results show strong improvements detection reliability and accuracy.\n\n\nUncertainty quantification: The project developed an uncertainty quantification approach for deep learning architectures, with application to visual localization (although the approach is generalizable to other applications). Specifically, the quantification of uncertainties from prediction models (e.g., neural networks) is crucial for many robotics applications, especially for those in safety-critical domains such as self-driving cars. By mapping an internal output of the prediction model to the uncertainty, the deep learning outputs can be used similar to other sensors such as GPS and IMUs.\n\n\nTransition and Training: In addition to the generation of algorithms and data, a total of 19 conference/journal papers were published. We open sourced the Ithaca365 dataset to the community, and for most papers, open sourced the original code with links within papers. A total of eight PhD students were funded under this project, of which six graduated with their PhD degrees. We have mentored two postdocs on this project, one of which transitioned to a faculty position at another university. We have made an explicit effort to incorporate undergraduates and students across multiple departments in the research, data collections and validation experiments.\n\n\n\t\t\t\t\tLast Modified: 12/31/2023\n\n\t\t\t\t\tSubmitted by: MarkECampbell\n"
 }
}