{
 "awd_id": "1718306",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI:  Small:  Collaborative Research:  Hidden Parameter Markov Decision Processes: Exploiting Structure in Families of Tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 242000.0,
 "awd_amount": 242000.0,
 "awd_min_amd_letter_date": "2017-08-04",
 "awd_max_amd_letter_date": "2017-08-04",
 "awd_abstract_narration": "Part 1\r\nMachine learning has the potential to automate many complex, real-life tasks. However, learning algorithms typically require a substantial amount of data from each specific task they are asked to solve, requiring repeated interactions with the world, each of which take time and effort. Many real-life learning scenarios involve repeated interactions with tasks that are similar, but not identical. For example, an immunologist may encounter HIV patients with different comorbid conditions and latent viral reservoirs - each has a similar disease but a different progression, requiring individualized treatment; a robot may have to manipulate objects of different size and weight - each requiring similar but not identical grasping strategies. In such cases treating all of the tasks as the same results in poor performance, but learning to solve each as if they were completely different takes far too long. This project will develop intelligent agents that can use knowledge gained when solving prior tasks to much more rapidly learn new tasks that are similar but not quite the same.\r\n\r\nThe principal technical component of this project will lie in rigorously defining what it means for tasks to be related and in producing algorithms for leveraging that definition to enable rapid learning. To do so, the project will introduce the Hidden-Parameter Markov Decision Process, which models a family of tasks through a parameter which describes variation through the family but is hidden from the learner. The project will investigate methods that exploit this structure by learning a model of task variation and then seeking to identify the parameter value for each specific task. The planned work will focus on healthcare applications, where families of related but distinct tasks are common (i.e. each patient will have unique characteristics).  However, the project aims to produce foundational learning algorithms applicable to many application areas, ranging from robotics to systems design. This research will also be integrated into the courses taught by the PIs at Harvard and Brown and made available online; the PIs will include a diverse population, including REUs, both in these classes and in their research groups.\r\n\r\nPart 2\r\nMany real-life learning scenarios involve repeated interactions with tasks that have similar, but not identical, dynamics.  For example, an immunologist may encounter HIV patients with different comorbid conditions and latent viral reservoirs; a robot may have to manipulate objects of different size and weight.  These cases describe a family of related tasks, each of which is similar but not quite the same. An intelligent agent should be able to transfer knowledge learned during previous experiences to rapidly solve new tasks in the same family. However, while many algorithms have been developed to transfer knowledge, the lack of a model of task relatedness inhibits our ability to formally understand the benefits of such algorithms or the structure they exploit.\r\n\r\nThe planned work will model such scenarios by embedding the tasks on a low dimensional manifold that captures relevant variation between instances.  Each location on this manifold (unobserved by the agent) describes a task instance, forming a sufficient statistic for solving the task in the context of the task family.  Preliminary work by the PIs has shown that it is possible to learn such a manifold after solving just a few individual task instances and enable the rapid optimization of policies for new task instances.  Building on these promising initial results, the PIs plan to: 1) Develop methods for task family characterization, by determining whether a collection of tasks can be modeled via a single manifold or consists of several clusters; whether a new task belongs to an existing cluster or manifold; and if so, and whether or not transfer is worthwhile. 2) Scale inference by adapting recent results from machine learning to deal with large state and action spaces. 3) Generate policies using Bayesian reinforcement learning algorithms, and by exploiting formal links between state and policy representations.\r\n\r\nIn addition to synthetic domains, progress on these directions will be applied to problems of treatment optimization for patients with HIV, sepsis, and depression via clinical collaborations that the PIs have with world-experts in these diseases.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Finale",
   "pi_last_name": "Doshi-Velez",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Finale Doshi-Velez",
   "pi_email_addr": "finale@seas.harvard.edu",
   "nsf_id": "000599894",
   "pi_start_date": "2017-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "33 Oxford St",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382933",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 242000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to advance rapid personalization across distinct but related reinforcement learning tasks.&nbsp; We were particularly motivated by use cases in healthcare, where, even within the same disease, each patient may have distinct patterns of progression and treatment response.</p>\n<p>Our strategy for tackling this question was by introducing a hidden parameter for every task (e.g. patient).&nbsp; We assume that if this hidden parameter were known, then the task could be modeled as a Markov Decision Process.&nbsp;&nbsp;</p>\n<p>In prior work, we had taken a model-based approach: the hidden parameter was treated as a static, unobserved part of the current state that fed into the model's dynamics.&nbsp; In this project, we first demonstrated that an alternative approach, which uses the hidden parameters to directly inform the policy, often performs significantly better.&nbsp; Understanding why and when proved to be quite a challenge, and ultimately we were able to link when policy-based approaches do better than model-based approaches to the relative smoothness of the policies and the dynamics.</p>\n<p>Along the way, we also made other important contributions.&nbsp; We developed a new modeling approach using horseshoe priors to prevent Bayesian neural networks (the model we used to model the task's dynamics) from overfitting when they had little data and demonstrated how this benefited task performance.&nbsp; We also identified ways to personalize to an individual with provably low regret while also maintaining enough exploration to perform statistical analyses for main treatment effects (important if one is running a trial).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/06/2022<br>\n\t\t\t\t\tModified by: Finale&nbsp;P&nbsp;Doshi-Velez</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to advance rapid personalization across distinct but related reinforcement learning tasks.  We were particularly motivated by use cases in healthcare, where, even within the same disease, each patient may have distinct patterns of progression and treatment response.\n\nOur strategy for tackling this question was by introducing a hidden parameter for every task (e.g. patient).  We assume that if this hidden parameter were known, then the task could be modeled as a Markov Decision Process.  \n\nIn prior work, we had taken a model-based approach: the hidden parameter was treated as a static, unobserved part of the current state that fed into the model's dynamics.  In this project, we first demonstrated that an alternative approach, which uses the hidden parameters to directly inform the policy, often performs significantly better.  Understanding why and when proved to be quite a challenge, and ultimately we were able to link when policy-based approaches do better than model-based approaches to the relative smoothness of the policies and the dynamics.\n\nAlong the way, we also made other important contributions.  We developed a new modeling approach using horseshoe priors to prevent Bayesian neural networks (the model we used to model the task's dynamics) from overfitting when they had little data and demonstrated how this benefited task performance.  We also identified ways to personalize to an individual with provably low regret while also maintaining enough exploration to perform statistical analyses for main treatment effects (important if one is running a trial).\n\n\t\t\t\t\tLast Modified: 12/06/2022\n\n\t\t\t\t\tSubmitted by: Finale P Doshi-Velez"
 }
}