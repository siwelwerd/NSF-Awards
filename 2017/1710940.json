{
 "awd_id": "1710940",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Low-power Neuromorphic Chip Architecture with in situ Deep Learning",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": "7032927360",
 "po_email": "jenlin@nsf.gov",
 "po_sign_block_name": "Jenshan Lin",
 "awd_eff_date": "2017-07-15",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 330000.0,
 "awd_amount": 330000.0,
 "awd_min_amd_letter_date": "2017-07-06",
 "awd_max_amd_letter_date": "2023-06-26",
 "awd_abstract_narration": "As ultra-low-power sensing devices are being deployed expansively in all-pervasive wireless computing and consumer electronic products that now profoundly impact our everyday life, enormous amount of data are continuously amassed from these ubiquitous sensors. To exploit these data efficiently for elevating application-specific objectives, built-in intelligences are warranted in these sensor platforms to endow transitions from the traditional rule-based computing paradigm to emerging data-driven computing methods. Deep learning on multi-layered networks of neurons has provided such an opportunity to achieve intelligent computing through learning from the abundant data collected by sensing devices. How to make the current energy-intensive deep learning that mainly runs on clusters of general-purpose central processing units (CPUs) and graphics processing units (GPUs) amenable to various low-power systems remains to be a formidable challenge. This project envisages inventing hardware-friendly learning techniques and designing customized low-power deep learning hardware capable of real-time in-situ learning. The innovative hardware will be implemented on various low-power platforms to significantly accelerate the deployment of the nascent Internet-of-Things (IoT) technology. The low-power deep learning chip will enable energy-constraint systems to sense, process, organize, and utilize the data more intelligently. Useful information associated with the collected data can be extracted and exploited at the front end, thereby reducing the system response time and the energy consumption in wireless communications.\r\n\r\nIn this research project, energy-efficient spiking neural networks (SNNs) will be used to construct deep learning networks. By leveraging the sparsity in multi-dimensional input data such as image, audio and video, the use of event-triggered SNNs can potentially result in significant energy savings. Building deep neural networks in SNNs also has the advantage of good scalability as CMOS technology advances beyond 10 nm. Through address-event representation, hundreds of sub-SNNs can be interconnected to build large SNNs solving disparate types of large-scale problems. Underlying difficulties owing to lack of effective learning algorithms in SNNs will be addressed by formulating the modulated spike-timing-dependent plasticity (STDP) learning rules. Through these bio-inspired on-line new learning rules, hardware-based SNNs can be designed for the event-triggered computation and deep learning. The neural hardware will be at first prototyped and validated on commercial FPGA boards, before realizing digitally by using nano-scale CMOS technology. Finally, in order to further reduce energy dissipation as warranted in ultra-low-power wearable systems, emergent memristor, i.e., analog resistive memory technology will be co-integrated into CMOS chip to mimic high-density artificial synapses. Variation-resilient architectures and algorithms will be developed to fully exploit the density of the memristor arrays, while tolerating their concomitant conductance variations due to several manufacturing limitations. Further, the integration of research and education will train future engineering workforce encompassing minority and female. Instructive materials developed under this project will be disseminated to research communities and practicing engineers by leveraging the NSF supported nanoHub repository.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Pinaki",
   "pi_last_name": "Mazumder",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Pinaki Mazumder",
   "pi_email_addr": "mazum@eecs.umich.edu",
   "nsf_id": "000309312",
   "pi_start_date": "2017-07-06",
   "pi_end_date": "2019-08-09"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mikhail",
   "pi_last_name": "Erementchouk",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mikhail Erementchouk",
   "pi_email_addr": "merement@umich.edu",
   "nsf_id": "000761630",
   "pi_start_date": "2019-08-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "2260 Hayward",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "105E",
   "pgm_ref_txt": "RF/Microwave & mm-wave tech"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 330000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project pursued developing novel computing architectures that meet the demands of applications requiring short decision-making time and a tight energy budget. The development evolved in two directions:&nbsp;</p>\n<p>1. Theoretical investigation of adapting biologically plausible spike-timing-dependent plasticity (STDP) learning rules to hardware-friendly learning algorithms and developing a novel hardware architecture for spiking neural networks (SNN) equipped with the proposed algorithm.</p>\n<p>2. Theoretical investigation of incorporating gradient descent-based optimization into event-driven architectures and developing analog and digital architectures implementing the dissipative dynamics driven by a Lyapunov function capturing the optimization objective.</p>\n<p>Within the project, the following significant results were obtained:</p>\n<p>1. The actor-critic type reinforcement learning and an example of temporal difference (TD) learning with offline policy updating captured on VLSI&nbsp; chips. Spiking correlation-based synaptic plasticity similar to the&nbsp; Hodgkin-Huxley model commonly used in biological unsupervised learning applications is adapted for hardware implementations. The spike-timing-dependent plasticity (STDP) is adapted to silicon to minimize energy dissipation and tackle the process-voltage-temperature (PVT) variations associated with chip manufacturing.</p>\n<p>2. A hardware accelerator VLSI chip for actor-critic networks is developed. The accelerator was applied to selected control-theoretic benchmark problems by emulating adaptive dynamic programming (ADP), which is at the heart of reinforcement learning (RL) software programs. Compared to traditional RL software running on a general-purpose processor, the&nbsp; VLSI chip accelerator operating at 175 MHz reaches the solution 270&nbsp; times faster while consuming merely 25 mW.</p>\n<p>3. Hardware-friendly spike-timing-dependent plasticity learning rules are formulated and implemented in a multilayered spiking neural network&nbsp; (SNN) deep learning chip, which achieved classification rates of 97.2%&nbsp; and 97.8% for the one-hidden-layer and two-hidden-layer neural networks,&nbsp; respectively. The architecture is tested against a Modified National&nbsp; Institute of Standards and Technology (MNIST) database benchmark to verify the newly proposed learning algorithm, which also provided high-energy efficiency and good scalability due to the SNN&nbsp; implementation.</p>\n<p>4. To adapt solving optimization problems to hardware, an almost-linear adaptation of rank-2 semidefinite programming (SDP)&nbsp; relaxation is developed. This adaptation significantly simplifies required arithmetic operations while producing in polynomial time good quality solutions comparable to state-of-the-art heuristic solvers. This feature drastically distinguishes the relaxation-based approach from&nbsp; \"conventional\" Ising machines based on converging to binary states.</p>\n<p>5. A hybrid analog-digital implementation of an Ising machine based on the almost-linear adaptation of rank-2 SDP is developed, delivering the proof-of-concept demonstration of relaxation-based Ising machines. The implementation supported networks with all-to-all connectivity and used a shared computational unit, which, on the one hand, enabled straightforward extension to other dynamical models and, on the other hand, reduced the impact of PVT variations.</p>\n<p>6. The main challenge for adapting the relaxation-based approach is the non-binary structure of the produced solutions requiring separate post-processing. To address this challenge, a dynamical realization of optimal rounding of terminal states of the relaxed models of the combinatorial optimization problems is developed. This realization made the relaxation-based Ising machines self-contained and enabled using arbitrary dynamical models that target particular optimization problems.7. A proof-of-concept digital implementation of the relaxation-based Ising machine on the FPGA platform is developed utilizing the inherent parallelism of dynamical Ising machines. The architectural design accommodates arbitrary coupling between the relaxed spins. This approach is principally different from the FPGA implementations of the \"conventional\" Ising machines that rely on matrix-vector multiplication&nbsp; (MVM) and, therefore, are dynamically locked. The developed implementation delivers good quality solutions in polynomial time and demonstrates the linear speedup compared to single-thread&nbsp; PC implementations: O(N) vs. O(N^2), where N is the number of variables.The theoretical framework and hardware design principles developed within the project can be applied to a broad class of computationally expensive problems. The direct application of the obtained results is solving various combinatorial optimization problems. The results obtained within the project open ways for efficient hardware implementations of the Ising model of computation, with applications greatly extending the immediate objective of combinatorial optimization.</p>\n<p>The project's results will impact immersing combinatorial optimization and machine learning techniques into low-power CMOS chips for portable electronics and wearable devices. Enabling a large degree of integration of spin-representing nodes, our results open ways for solving optimization problems that are beyond the capabilities of general-purpose computing architectures.&nbsp;The developed theoretical description of the Ising machines of the new generation revealed classes of problems underexplored within the combinatorial optimization theory. Besides identifying such problems, the developed theory exposed techniques for their investigation.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/17/2024<br>\nModified by: Mikhail&nbsp;Erementchouk</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project pursued developing novel computing architectures that meet the demands of applications requiring short decision-making time and a tight energy budget. The development evolved in two directions:\n\n\n1. Theoretical investigation of adapting biologically plausible spike-timing-dependent plasticity (STDP) learning rules to hardware-friendly learning algorithms and developing a novel hardware architecture for spiking neural networks (SNN) equipped with the proposed algorithm.\n\n\n2. Theoretical investigation of incorporating gradient descent-based optimization into event-driven architectures and developing analog and digital architectures implementing the dissipative dynamics driven by a Lyapunov function capturing the optimization objective.\n\n\nWithin the project, the following significant results were obtained:\n\n\n1. The actor-critic type reinforcement learning and an example of temporal difference (TD) learning with offline policy updating captured on VLSI chips. Spiking correlation-based synaptic plasticity similar to the Hodgkin-Huxley model commonly used in biological unsupervised learning applications is adapted for hardware implementations. The spike-timing-dependent plasticity (STDP) is adapted to silicon to minimize energy dissipation and tackle the process-voltage-temperature (PVT) variations associated with chip manufacturing.\n\n\n2. A hardware accelerator VLSI chip for actor-critic networks is developed. The accelerator was applied to selected control-theoretic benchmark problems by emulating adaptive dynamic programming (ADP), which is at the heart of reinforcement learning (RL) software programs. Compared to traditional RL software running on a general-purpose processor, the VLSI chip accelerator operating at 175 MHz reaches the solution 270 times faster while consuming merely 25 mW.\n\n\n3. Hardware-friendly spike-timing-dependent plasticity learning rules are formulated and implemented in a multilayered spiking neural network (SNN) deep learning chip, which achieved classification rates of 97.2% and 97.8% for the one-hidden-layer and two-hidden-layer neural networks, respectively. The architecture is tested against a Modified National Institute of Standards and Technology (MNIST) database benchmark to verify the newly proposed learning algorithm, which also provided high-energy efficiency and good scalability due to the SNN implementation.\n\n\n4. To adapt solving optimization problems to hardware, an almost-linear adaptation of rank-2 semidefinite programming (SDP) relaxation is developed. This adaptation significantly simplifies required arithmetic operations while producing in polynomial time good quality solutions comparable to state-of-the-art heuristic solvers. This feature drastically distinguishes the relaxation-based approach from \"conventional\" Ising machines based on converging to binary states.\n\n\n5. A hybrid analog-digital implementation of an Ising machine based on the almost-linear adaptation of rank-2 SDP is developed, delivering the proof-of-concept demonstration of relaxation-based Ising machines. The implementation supported networks with all-to-all connectivity and used a shared computational unit, which, on the one hand, enabled straightforward extension to other dynamical models and, on the other hand, reduced the impact of PVT variations.\n\n\n6. The main challenge for adapting the relaxation-based approach is the non-binary structure of the produced solutions requiring separate post-processing. To address this challenge, a dynamical realization of optimal rounding of terminal states of the relaxed models of the combinatorial optimization problems is developed. This realization made the relaxation-based Ising machines self-contained and enabled using arbitrary dynamical models that target particular optimization problems.7. A proof-of-concept digital implementation of the relaxation-based Ising machine on the FPGA platform is developed utilizing the inherent parallelism of dynamical Ising machines. The architectural design accommodates arbitrary coupling between the relaxed spins. This approach is principally different from the FPGA implementations of the \"conventional\" Ising machines that rely on matrix-vector multiplication (MVM) and, therefore, are dynamically locked. The developed implementation delivers good quality solutions in polynomial time and demonstrates the linear speedup compared to single-thread PC implementations: O(N) vs. O(N^2), where N is the number of variables.The theoretical framework and hardware design principles developed within the project can be applied to a broad class of computationally expensive problems. The direct application of the obtained results is solving various combinatorial optimization problems. The results obtained within the project open ways for efficient hardware implementations of the Ising model of computation, with applications greatly extending the immediate objective of combinatorial optimization.\n\n\nThe project's results will impact immersing combinatorial optimization and machine learning techniques into low-power CMOS chips for portable electronics and wearable devices. Enabling a large degree of integration of spin-representing nodes, our results open ways for solving optimization problems that are beyond the capabilities of general-purpose computing architectures.The developed theoretical description of the Ising machines of the new generation revealed classes of problems underexplored within the combinatorial optimization theory. Besides identifying such problems, the developed theory exposed techniques for their investigation.\n\n\n\t\t\t\t\tLast Modified: 04/17/2024\n\n\t\t\t\t\tSubmitted by: MikhailErementchouk\n"
 }
}