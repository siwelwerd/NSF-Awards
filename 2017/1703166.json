{
 "awd_id": "1703166",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Medium: Learning Multimodal Knowledge about Entities and Events",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Wei Ding",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 700000.0,
 "awd_amount": 700000.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2018-09-15",
 "awd_abstract_narration": "Everyday knowledge about the world is a necessary condition for intelligent information processing and reasoning. People can read between the lines in text and see beyond what are visible in images because of everyday functional knowledge about how the world works. The primary goal of this research is to develop learning algorithms that can automatically acquire such knowledge, centered around entities and events, from large-scale multimodal web data. Entity knowledge includes a broad range of physical and conceptual knowledge about objects and people, including their attributes, their relative differences, and logical relations among them. Event knowledge focuses on structural knowledge about everyday events in people's lives organized through hierarchical and temporal relations among sub-events and the event participants. Together, the resulting knowledge will be a critical step forward to enable robust AI systems at the intersection between natural language processing and computer vision that can understand and reason about unstructured multimodal information. The potential impact of this research includes interactive assistive systems for the visually-impaired and multimodal educational interfaces. \r\n\r\nThis project investigates multimodal knowledge extraction as a new research paradigm drawing connections between relevant methods in natural language processing such as information extraction, textual entailments, and frame semantics with recent advances in computer vision. One of the critical challenges in commonsense knowledge acquisition is to overcome reporting bias, i.e., people do not state the obvious. Therefore, this project develops new learning algorithms based on a graph-based collective inference that can reason about unspoken knowledge that systematically influences the way people describe the world in language, images, and videos. In addition, this project develops new models for visual semantic parsing and event recognition, which generalize existing studies on activity recognition by specifying various structural components of events such as actors, objects, locations, tools, intents, and goals. The learned knowledge and representation will be validated through several applications including multimodal question answering and grounded language understanding.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Yejin",
   "pi_last_name": "Choi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yejin Choi",
   "pi_email_addr": "yejin@cs.washington.edu",
   "nsf_id": "000584106",
   "pi_start_date": "2017-07-27",
   "pi_end_date": "2017-08-24"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hanna",
   "pi_last_name": "Hajishirzi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hanna Hajishirzi",
   "pi_email_addr": "hannaneh@uw.edu",
   "nsf_id": "000634179",
   "pi_start_date": "2017-08-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yejin",
   "pi_last_name": "Choi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yejin Choi",
   "pi_email_addr": "yejin@cs.washington.edu",
   "nsf_id": "000584106",
   "pi_start_date": "2018-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ali",
   "pi_last_name": "Farhadi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ali Farhadi",
   "pi_email_addr": "afarhad2@gmail.com",
   "nsf_id": "000611236",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Hanna",
   "pi_last_name": "Hajishirzi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hanna Hajishirzi",
   "pi_email_addr": "hannaneh@uw.edu",
   "nsf_id": "000634179",
   "pi_start_date": "2017-07-27",
   "pi_end_date": "2017-08-24"
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "185 Stevens Way",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 700000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-6421481b-7fff-9968-b6ee-acd9fb728bdd\"> </span></p>\n<p dir=\"ltr\"><span>This project addressed fundamental problems at the boundary of natural language processing, computer vision and artificial intelligence. The inferred knowledge repositories explained in this proposal has enabled new research directions in semantic understanding of the world and reasoning. Incorporating this knowledge improved state of the art object and action recognition by constraining visual by the inferred knowledge of the world. The algorithms, datasets, and knowledge repositories helped in several NLP tasks such as grounded language understanding, instructional language understanding, and multimodal storyline generation. Also, the outcomes of the project provided valuable input for several reasoning algorithms in AI. The project was a tightly collaborative research program between Natural Language Processing and Computer Vision researchers and made a substantial impact on both communities.</span></p>\n<p dir=\"ltr\"><span>The project provided a substantial list of datasets, benchmarks, and leaderboards to evaluate different textual and visual reasoning capabilities of current machine learning models. In addition, it provided pioneering and state-of-the-art models, software, and tools in improving textual and visual understanding and reasoning.&nbsp;<br /></span></p>\n<p dir=\"ltr\"><span>Some of the most key findings of the project included: 1)&nbsp;</span>MERLOT and MERLOT-Reserve, a multimodal neural script knowledge through vision, langauge, and speech modalities, 2) methods for connecting the dots between audio and text without parallel data through visual knowledge transfer, 3) The Abduction of Sherlock Holmes, a visual dataset for abductive visual reasoning, 4) WISE-FT, a robust fine-tuning method for zero-shot task transfer, 5) VCR, the most prominent dataset for visual commonsense reasoning, 6) a method for benchmarking hierarchical script knowledge, 7) ESPNet and ESPNet V2,<span>&nbsp;Light-weight, Power Efficient, and General Purpose Convolutional Neural Network, 8) DyGIE, a general framework for information extraction from text, 9) Neural process networks by simulating action dynamics, and 10) Neural motifs, a method for scenge graph parsing in images.&nbsp;</span></p>\n<p><span>Research results were disseminated through several publications at top conferednces, invited talks, and popular media. Importantly, this project facilitated opportunities for training and mentoring graduate and undergraduate students and collaborations with industry.</span></p>\n<p><span><br /></span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/07/2022<br>\n\t\t\t\t\tModified by: Hanna&nbsp;Hajishirzi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis project addressed fundamental problems at the boundary of natural language processing, computer vision and artificial intelligence. The inferred knowledge repositories explained in this proposal has enabled new research directions in semantic understanding of the world and reasoning. Incorporating this knowledge improved state of the art object and action recognition by constraining visual by the inferred knowledge of the world. The algorithms, datasets, and knowledge repositories helped in several NLP tasks such as grounded language understanding, instructional language understanding, and multimodal storyline generation. Also, the outcomes of the project provided valuable input for several reasoning algorithms in AI. The project was a tightly collaborative research program between Natural Language Processing and Computer Vision researchers and made a substantial impact on both communities.\nThe project provided a substantial list of datasets, benchmarks, and leaderboards to evaluate different textual and visual reasoning capabilities of current machine learning models. In addition, it provided pioneering and state-of-the-art models, software, and tools in improving textual and visual understanding and reasoning. \n\nSome of the most key findings of the project included: 1) MERLOT and MERLOT-Reserve, a multimodal neural script knowledge through vision, langauge, and speech modalities, 2) methods for connecting the dots between audio and text without parallel data through visual knowledge transfer, 3) The Abduction of Sherlock Holmes, a visual dataset for abductive visual reasoning, 4) WISE-FT, a robust fine-tuning method for zero-shot task transfer, 5) VCR, the most prominent dataset for visual commonsense reasoning, 6) a method for benchmarking hierarchical script knowledge, 7) ESPNet and ESPNet V2, Light-weight, Power Efficient, and General Purpose Convolutional Neural Network, 8) DyGIE, a general framework for information extraction from text, 9) Neural process networks by simulating action dynamics, and 10) Neural motifs, a method for scenge graph parsing in images. \n\nResearch results were disseminated through several publications at top conferednces, invited talks, and popular media. Importantly, this project facilitated opportunities for training and mentoring graduate and undergraduate students and collaborations with industry.\n\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/07/2022\n\n\t\t\t\t\tSubmitted by: Hanna Hajishirzi"
 }
}