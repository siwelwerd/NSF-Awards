{
 "awd_id": "1723128",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:  Connecting Submodularity and Restricted Strong Convexity",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Christopher Stark",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 160000.0,
 "awd_amount": 160000.0,
 "awd_min_amd_letter_date": "2017-08-18",
 "awd_max_amd_letter_date": "2019-08-28",
 "awd_abstract_narration": "Structured estimation problems arise in a variety of contexts including astronomy, genomics, and computer vision. This project aims to develop methods that can use the additional structure in order to estimate statistical models effectively, while also using the structure for computational improvements. This work seeks to connect ideas in combinatorial optimization and statistical estimation to develop computationally tractable methods for performing structured statistical estimation.\r\n\r\nThis project provides an integrated program to explore and connect combinatorial optimization and statistical estimation. Modern statistical challenges have become increasingly dependent on understanding both the computational and statistical issues. Many modern statistical estimation problems rely on imposing additional structure in order to reduce the statistical complexity and provide interpretability. Unfortunately, these structures often are combinatorial in nature and result in computationally challenging problems. In parallel, the combinatorial optimization community has placed significant effort in developing algorithms that can approximately solve such optimization problems in a computationally efficient manner. The focus of this project is to expand upon ideas that arise in combinatorial optimization and connect those algorithms and ideas to statistical questions. The research directions of this project are split into three main thrusts unified by the concept of weak submodularity: (a) cardinality constrained optimization and its applications to general statistical optimization problems; (b) matrix estimation problems including low-rank matrix estimation and semi-definite programming problems as well as problems in sparse dictionary learning; and (c) a general theoretical understanding of weak submodularity and specifically analyzing how to develop algorithms in this regime that work well for large-scale datasets.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sahand",
   "pi_last_name": "Negahban",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sahand Negahban",
   "pi_email_addr": "sahand.negahban@yale.edu",
   "nsf_id": "000675090",
   "pi_start_date": "2017-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "24 Hillhouse Avenue",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065116814",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 54079.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 52203.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 53718.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As modern datasets become larger and more complex so too do the models used with this data. Often, the models used have higher complexity than the datasets allow as there may not be enough data to fit the model in full generality. Model selection is a broad method for overcoming this challenge. Such problems can yield non-convex optimization procedures that may be computationally intractable. For instance, in feature selection for linear models the natural task is to enumerate all possible subsets of features. Two techniques often used to overcome the intractability are convex relaxations (eg the Lasso) and greedy model selection approaches (eg Orthogonal Matching Pursuit).<br />This project aimed to connect ideas from combinatorial optimization and model selection to understand and develop computationally tractable methods with good statistical properties. Furthermore, the goal was to ensure these methods applied to general objectives beyond quadratic optimization problems. Two key properties were explored in this project: Restricted Strong Convexity (RSC) and weak submodularity.<br />The main thrust of this project was to study greedy algorithms used for model selection through the lens of submodular optimization. The main finding was to establish a general connection between an idea that was first developed in the statistics literature, known as RSC, to a property called weak submodularity. Specifically, the project demonstrated that when an optimization objective satisfies RSC, then it also satisfies weak submodularity. The property of RSC has been shown to be satisfied for general objective functions. Furthermore, this property has been used to demonstrate an estimation procedure has good statistical properties. On the other hand, submodularity is a property that has been applied to showing certain greedy algorithms obtain good optimization performance. This project sought to connect the statistical and computational sides of the greedy optimization problem. Applications of this connection yielded algorithmic results with good statistical properties for general models in greedy feature selection, greedy low-rank matrix estimation, and distributed greedy optimization.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/20/2023<br>\n\t\t\t\t\tModified by: Sahand&nbsp;Negahban</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAs modern datasets become larger and more complex so too do the models used with this data. Often, the models used have higher complexity than the datasets allow as there may not be enough data to fit the model in full generality. Model selection is a broad method for overcoming this challenge. Such problems can yield non-convex optimization procedures that may be computationally intractable. For instance, in feature selection for linear models the natural task is to enumerate all possible subsets of features. Two techniques often used to overcome the intractability are convex relaxations (eg the Lasso) and greedy model selection approaches (eg Orthogonal Matching Pursuit).\nThis project aimed to connect ideas from combinatorial optimization and model selection to understand and develop computationally tractable methods with good statistical properties. Furthermore, the goal was to ensure these methods applied to general objectives beyond quadratic optimization problems. Two key properties were explored in this project: Restricted Strong Convexity (RSC) and weak submodularity.\nThe main thrust of this project was to study greedy algorithms used for model selection through the lens of submodular optimization. The main finding was to establish a general connection between an idea that was first developed in the statistics literature, known as RSC, to a property called weak submodularity. Specifically, the project demonstrated that when an optimization objective satisfies RSC, then it also satisfies weak submodularity. The property of RSC has been shown to be satisfied for general objective functions. Furthermore, this property has been used to demonstrate an estimation procedure has good statistical properties. On the other hand, submodularity is a property that has been applied to showing certain greedy algorithms obtain good optimization performance. This project sought to connect the statistical and computational sides of the greedy optimization problem. Applications of this connection yielded algorithmic results with good statistical properties for general models in greedy feature selection, greedy low-rank matrix estimation, and distributed greedy optimization.\n\n\t\t\t\t\tLast Modified: 03/20/2023\n\n\t\t\t\t\tSubmitted by: Sahand Negahban"
 }
}