{
 "awd_id": "1657155",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: III: Robust Machine Learning Methods for Messy Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2017-05-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 174999.0,
 "awd_amount": 174999.0,
 "awd_min_amd_letter_date": "2017-03-01",
 "awd_max_amd_letter_date": "2017-03-01",
 "awd_abstract_narration": "Messy data is ubiquitous in modern science. Data come from heterogeneous sources; there are many latent confounding factors; and it is often unclear what are the relevant questions to ask and models to use. This reality is in sharp contrast with the usual modeling assumptions of machine learning and statistics, where data are assumed to come from well-specified models and the hypotheses to test are clearly laid out. The glaring gap between standard theory and the actual practice of messy data is a major contributor to the reproducibility crises across science and prevents researchers from harnessing the full insights from data. This project will develop rigorous mathematical foundations and robust machine learning algorithms to address the core challenges of messy data. The PI will explore novel techniques to quantify and reduce different types of selection biases that arise from exploratory data analysis. The PI will also investigate algorithms to perform statistical inference when the model is mis- or under-specified. The project will apply these new methods to tackle challenging problems in human population genomics. \r\n\r\nThe PI recently initialized a framework based on information usage to quantify the magnitude of over-fitting and bias arising from data exploration. This project will significantly expand this framework. In particular the PI will apply this information usage approach to quantify and reduce bias in data generated from adaptive experimentation, such as online A/B testing and more general multi-arm bandits. Related to over-fitting is the problem of mis- and under-specified statistical models. The PI has recently developed method-of-cumulant approaches to learn probabilistic models when the observations are perturbed by unknown and arbitrary interference. A promising direction of research is to extend this approach to more general settings that allow for nonlinear interference and to develop software tools for the broad data science community. Genomics exemplify many of the challenges of messy data-genomic data typically requires substantial exploratory analysis and faces modeling uncertainty. This makes genomics a high impact domain to apply the new messy data algorithms developed here. Bio-medical databases are interactively analyzed by many researchers and thus are particularly prone to exploration bias and overfitting. The PI will explore piloting the information usage framework on the bio-medical data hubs being created at Stanford in order to quantify and reduce exploration bias. As a part of the project, PI is also developing courses, workshops and tutorials to bring together researchers and practitioners across machine learning, statistics, information theory and bio-medical data science to address the ubiquitous challenge of messy data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Zou",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "James Y Zou",
   "pi_email_addr": "jamesz@stanford.edu",
   "nsf_id": "000728417",
   "pi_start_date": "2017-03-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "365 Lasuen Street 3rd fl.",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943055005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "026Y00",
   "pgm_ele_name": "CRII CISE Research Initiation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 174999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 11.0px Helvetica} -->\n<p class=\"p1\">Modern data science and machine learning is often challenging because the data come from heterogeneous sources and are often high dimensional. In this project we developed several tools to address these challenges and to enable practitioners to more easily work with such challenging data. Here we summarize the key outcomes and contributions of the project with two examples.&nbsp;</p>\n<p class=\"p1\">Visualization and exploration of high-dimensional data is a ubiquitous challenge across disciplines. Widely used techniques such as principal component analysis (PCA) aim to identify dominant trends in one dataset. However, in many settings we have datasets collected under different conditions, e.g., a treatment and a control experiment, and we are interested in visualizing and exploring patterns that are speci&#64257;c to one dataset. In this project, we developed a new method, contrastive principal component analysis (cPCA), which identi&#64257;es low-dimensional structures that are enriched in a dataset relative to comparison data. In a wide variety of experiments, we demonstrated that cPCA with a background dataset enables us to visualize dataset-speci&#64257;c patterns missed by PCA and other standard methods. We further provided geometric interpretation of cPCA and strong mathematical guarantees. An implementation of cPCA is publicly available on Github, and can be used for exploratory data analysis in many applications where PCA is currently used.</p>\n<p class=\"p1\"><span>The second problem we tackled is multiple hypothesis testing, which is an essential component of modern data science. In many settings, in addition to the&nbsp;</span><em>p</em><span>-value, additional covariates for each hypothesis are available, e.g., functional annotation of variants in genome-wide association studies. Such information is ignored by popular multiple testing approaches such as the Benjamini-Hochberg procedure (BH). In this project, we introduced&nbsp;</span><span class=\"u-monospace\">AdaFDR</span><span>, a fast and flexible method that adaptively learns the optimal&nbsp;</span><em>p</em><span>-value threshold from covariates to significantly improve detection power. On expression quantitative trait loci (eQTL) analysis of the GTEx data,&nbsp;</span><span class=\"u-monospace\">AdaFDR</span><span>&nbsp;discovers 32% more associations than BH at the same false discovery rate. We prove that&nbsp;</span><span class=\"u-monospace\">AdaFDR</span><span>&nbsp;controls false discovery proportion and show that it makes substantially more discoveries while controlling false discovery rate (FDR) in extensive experiments.&nbsp;</span><span class=\"u-monospace\">AdaFDR</span><span>&nbsp;is computationally efficient and allows multi-dimensional covariates with both numeric and categorical values, making it broadly useful across many applications. We have also released adaFDR as an open source Python package.&nbsp;</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/19/2019<br>\n\t\t\t\t\tModified by: James&nbsp;Y&nbsp;Zou</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModern data science and machine learning is often challenging because the data come from heterogeneous sources and are often high dimensional. In this project we developed several tools to address these challenges and to enable practitioners to more easily work with such challenging data. Here we summarize the key outcomes and contributions of the project with two examples. \nVisualization and exploration of high-dimensional data is a ubiquitous challenge across disciplines. Widely used techniques such as principal component analysis (PCA) aim to identify dominant trends in one dataset. However, in many settings we have datasets collected under different conditions, e.g., a treatment and a control experiment, and we are interested in visualizing and exploring patterns that are speci&#64257;c to one dataset. In this project, we developed a new method, contrastive principal component analysis (cPCA), which identi&#64257;es low-dimensional structures that are enriched in a dataset relative to comparison data. In a wide variety of experiments, we demonstrated that cPCA with a background dataset enables us to visualize dataset-speci&#64257;c patterns missed by PCA and other standard methods. We further provided geometric interpretation of cPCA and strong mathematical guarantees. An implementation of cPCA is publicly available on Github, and can be used for exploratory data analysis in many applications where PCA is currently used.\nThe second problem we tackled is multiple hypothesis testing, which is an essential component of modern data science. In many settings, in addition to the p-value, additional covariates for each hypothesis are available, e.g., functional annotation of variants in genome-wide association studies. Such information is ignored by popular multiple testing approaches such as the Benjamini-Hochberg procedure (BH). In this project, we introduced AdaFDR, a fast and flexible method that adaptively learns the optimal p-value threshold from covariates to significantly improve detection power. On expression quantitative trait loci (eQTL) analysis of the GTEx data, AdaFDR discovers 32% more associations than BH at the same false discovery rate. We prove that AdaFDR controls false discovery proportion and show that it makes substantially more discoveries while controlling false discovery rate (FDR) in extensive experiments. AdaFDR is computationally efficient and allows multi-dimensional covariates with both numeric and categorical values, making it broadly useful across many applications. We have also released adaFDR as an open source Python package. \n\n\t\t\t\t\tLast Modified: 12/19/2019\n\n\t\t\t\t\tSubmitted by: James Y Zou"
 }
}