{
 "awd_id": "1703678",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CIF:Medium:Collaborative Research: Foundations of Coding for Modern Distributed Computing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2017-05-01",
 "awd_exp_date": "2022-04-30",
 "tot_intn_awd_amt": 700000.0,
 "awd_amount": 855850.0,
 "awd_min_amd_letter_date": "2017-04-11",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Coding and information theory provide a very rich body of knowledge from theory to concepts to constructions for creating, leveraging, and removing ?redundancy\" in ways that have revolutionized the digital era. This project brings these concepts and techniques to bear in a new field: large-scale distributed computing. The modern paradigm for large-scale distributed computing systems is driven by \u00a0\"scaling out\" of computations across clusters consisting of as many as tens or hundreds of thousands of machines. As such, there is an abundance of resource redundancy that can be exploited. This project develops a foundation for \"coded computing\", a new framework that combines coding with distributed computing to overcome several fundamental challenges limiting the performance of today's large-scale distributed computing platforms. The research outcomes of the project will be integrated into education and will be disseminated broadly.\u00a0\r\n\r\nThis project takes a principled and foundational approach to providing a unified coding framework to tackle three key challenges in large-scale distributed computing: significant delays due to straggling nodes; large communication loads between computing nodes; and massive input data-sets. In particular, three novel coding concepts are proposed for distributed computing: coding for injecting computation redundancy to mitigate straggler issues; coding to trade local computation with global communication; and coding for statistically principled data sketching. The unified role of codes in both doing fast sketching and in providing robustness to straggler node delays and communication bottlenecks is also studied.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kannan",
   "pi_last_name": "Ramchandran",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kannan Ramchandran",
   "pi_email_addr": "kannanr@eecs.berkeley.edu",
   "nsf_id": "000173259",
   "pi_start_date": "2017-04-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Mahoney",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Mahoney",
   "pi_email_addr": "mmahoney@icsi.berkeley.edu",
   "nsf_id": "000661349",
   "pi_start_date": "2017-04-11",
   "pi_end_date": "2020-03-18"
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "269 Cory Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947201770",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7797",
   "pgm_ref_txt": "COMM & INFORMATION FOUNDATIONS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 142255.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 413868.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 159877.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 139850.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The goal of this project is to take a foundational approach to providing a unified coding framework to tackle several key challenges in large-scale distributed computing. Here, we summarize the most recent progress on the project.</span></p>\n<p>In our&nbsp;<strong>first work</strong>, we develop Dynamic Communication Thresholding (DCT), a method which employs a hybrid form of data parallelism and model parallelism to scale distributed training of large scale recommender systems. DCT employs a simple hard-thresholding function to filter the entities to be communicated across the network allowing for passage of only essential information. For data parallelism, we compress the parameter gradiens sent to the parameter server during model synchronization and for model parallelism&nbsp;we compress the activations and gradients sent across the network during the forward and backward propagation.</p>\n<p>In our&nbsp;<strong>second work</strong>, we develop a communication-efficient distributed learning algorithm that is robust against Byzantine worker machines. To mitigate&nbsp;Byzantine failures, we employ a simple&nbsp;thresholding based on gradient norms and analyze the resulting&nbsp;distributed gradient-descent algorithm. We show the (statistical) error-rate of our algorithm matches previous work which uses more complicated schemes (coordinate-wise median, trimmed mean). We show that, in a certain range of the compression factor, the (order-wise) rate of convergence is not affected by the compression operation. Moreover, we analyze the compressed gradient descent algorithm with error feedback in a distributed setting and in the presence of Byzantine worker machines.</p>\n<p>In our&nbsp;<strong>third work</strong>, and in the context of federated learning, we develop a&nbsp;'secure aggregation' protocol that enables the server to aggregate clients' models in a privacy-preserving manner. The existing secure aggregation protocols incur high computation/communication costs, especially when the number of model parameters is larger than the number of clients participating in an iteration -- a typical scenario in federated learning. In this work, we propose a secure aggregation protocol, FastSecAgg, that is efficient in terms of computation and communication, and robust to client dropouts. The main building block of FastSecAgg is a novel multi-secret sharing scheme, FastShare, based on the Fast Fourier Transform (FFT).&nbsp;FastShare is information-theoretically secure, and achieves a trade-off between the number of secrets, privacy threshold, and dropout tolerance.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/30/2022<br>\n\t\t\t\t\tModified by: Kannan&nbsp;Ramchandran</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project is to take a foundational approach to providing a unified coding framework to tackle several key challenges in large-scale distributed computing. Here, we summarize the most recent progress on the project.\n\nIn our first work, we develop Dynamic Communication Thresholding (DCT), a method which employs a hybrid form of data parallelism and model parallelism to scale distributed training of large scale recommender systems. DCT employs a simple hard-thresholding function to filter the entities to be communicated across the network allowing for passage of only essential information. For data parallelism, we compress the parameter gradiens sent to the parameter server during model synchronization and for model parallelism we compress the activations and gradients sent across the network during the forward and backward propagation.\n\nIn our second work, we develop a communication-efficient distributed learning algorithm that is robust against Byzantine worker machines. To mitigate Byzantine failures, we employ a simple thresholding based on gradient norms and analyze the resulting distributed gradient-descent algorithm. We show the (statistical) error-rate of our algorithm matches previous work which uses more complicated schemes (coordinate-wise median, trimmed mean). We show that, in a certain range of the compression factor, the (order-wise) rate of convergence is not affected by the compression operation. Moreover, we analyze the compressed gradient descent algorithm with error feedback in a distributed setting and in the presence of Byzantine worker machines.\n\nIn our third work, and in the context of federated learning, we develop a 'secure aggregation' protocol that enables the server to aggregate clients' models in a privacy-preserving manner. The existing secure aggregation protocols incur high computation/communication costs, especially when the number of model parameters is larger than the number of clients participating in an iteration -- a typical scenario in federated learning. In this work, we propose a secure aggregation protocol, FastSecAgg, that is efficient in terms of computation and communication, and robust to client dropouts. The main building block of FastSecAgg is a novel multi-secret sharing scheme, FastShare, based on the Fast Fourier Transform (FFT). FastShare is information-theoretically secure, and achieves a trade-off between the number of secrets, privacy threshold, and dropout tolerance.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/30/2022\n\n\t\t\t\t\tSubmitted by: Kannan Ramchandran"
 }
}