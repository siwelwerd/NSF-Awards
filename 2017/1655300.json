{
 "awd_id": "1655300",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Discovering Hierarchical Representations for Action Understanding",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 555792.0,
 "awd_amount": 555792.0,
 "awd_min_amd_letter_date": "2017-04-05",
 "awd_max_amd_letter_date": "2017-04-05",
 "awd_abstract_narration": "A major issue in the psychological sciences is understanding how people can infer the intentions of others. Humans are remarkably adept at predicting the actions of other people and making inferences about their intention and goals. The present investigation examines how humans make such inferences from the physical movements of others. The work is guided by a computational theory of biological motion understanding that quantifies what aspects of actions allow observers to make inferences about the meaning of actions and what might come next. The larger goal is to explain how perception and reasoning operate synergistically to infer hidden goals and intentions. These findings will guide development of the next generation of intelligent machine-vision systems, useful in forensic sciences as well as many other real-world applications. Such systems will need to perform challenging tasks that currently are difficult and time-consuming for humans (for example, automated interpretation of human actions recorded in low-resolution surveillance video). The project will also help to identify individual differences in action understanding, potentially revealing the nature of the impairments in action understanding observed in people with autism disorder. In addition, the project will provide a unique training opportunity for students who are interested in interdisciplinary research at the interface between cognitive science and artificial intelligence and will provide an in-depth international research experience for a graduate student and postdoctoral fellow.\r\n\r\nThe research will integrate advanced psychophysical methods with sophisticated computational approaches. A key aim is to develop a unified theory based on a hierarchical non-parametric Bayesian framework, specifying the fundamental computational mechanisms involved in perception of human actions and reasoning about them. More generally, the project will use human body movements as an underutilized approach to understanding general problems in learning: how to construct, use and transform hierarchical representations to support human perception and cognition. Three aims are particularly noteworthy. First, the project will integrate computational modeling approaches with behavioral experiments to investigate the critical connection between perceptual and cognitive systems. Second, the project uses action stimuli derived from motion capture data in the real world as the visual input (CCTV images collected in the UK and secured at the University of Glasgow). By avoiding the limitations of studies that use restricted examples and constrained environments, the investigators maximize the likelihood that the findings will generalize to real-world situations. Third, the project will develop significant extensions of Bayesian approaches in order to study complex visual processes by combining generative models with probabilistic constraints.  This award is co-funded by the Perception, Action, and Cognition Program and the Office of International Science and Engineering.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hongjing",
   "pi_last_name": "Lu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hongjing Lu",
   "pi_email_addr": "hongjing@ucla.edu",
   "nsf_id": "000498799",
   "pi_start_date": "2017-04-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "UCLA Psychology Department",
  "perf_str_addr": "BOX 951563, 6552 Franz Hall",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951563",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "054Y00",
   "pgm_ele_name": "GVF - Global Venture Fund"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5946",
   "pgm_ref_txt": "UNITED KINGDOM"
  },
  {
   "pgm_ref_code": "5980",
   "pgm_ref_txt": "WESTERN EUROPE PROGRAM"
  },
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 555792.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Human actions are more than mere body movements. To fully understand actions, humans must be able to address the fundamental questions of ?<em>who did what, how and why?</em>? But it remains unclear how action perception is translated into understanding and inference of the hidden causes that elicit human actions. This project systematically examined visual representations and reasoning about actions. Our goal was to identify the key computational components and visual processes that connect action perception with reasoning. With the support of the current grant, our research group published 15 peer-reviewed papers, including 10 journal articles and 5 conference proceedings papers, as well as supporting about 10 conference presentations. The funded project also provided training opportunities for 3 postdoctoral researchers (one female scholar), 5 graduate students (4 female students), and about 40 undergraduate students (majority are female students) who were involved in research activities.</p>\n<p>The project produced several major results that our research group has reported. Our experience of actions depends not only on spatiotemporal features of dynamic stimuli, but also on causal understanding of actions. We identified critical features in body movements signaling awkward social interactions between people. We found that people judge body movements to be smoother when the observed action is a natural outcome causing by a social interaction (e.g., a person raising up their arms appears to move smoothly if the person is trying to catch a ball thrown towards them). We also extended the experimental paradigm to show how to provide effective explanations of actions performed by AI and robot systems for fostering human trust in robotic systems.</p>\n<p>Based on our empirical findings, we developed a new computational model to show that capturing latent representations in motion dynamics is essential in accounting for human judgments. We compared our model with deep learning models for various visual recognition tasks, and found that in comparison with human perception, deep learning models lack generalization and deep understanding of objects and actions. Modeling work based on latent representations is likely to aid in developing more sophisticated AI systems for understanding actions, going beyond pattern recognition.</p>\n<p>Finally, our research group extended the experimental investigations beyond typical populations. We found that young infants are sensitive to causal consistency in actions. We also found individual differences in self-action recognition that were related to degree of autistic traits for people, and intact ability in recognizing action by schizophrenia patients. These findings from the project shed light on potential mechanisms in development. The paradigms developed in our project could be adopted to enable transformative research on mental disorders such as autism and schizophrenia.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2022<br>\n\t\t\t\t\tModified by: Hongjing&nbsp;Lu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHuman actions are more than mere body movements. To fully understand actions, humans must be able to address the fundamental questions of ?who did what, how and why?? But it remains unclear how action perception is translated into understanding and inference of the hidden causes that elicit human actions. This project systematically examined visual representations and reasoning about actions. Our goal was to identify the key computational components and visual processes that connect action perception with reasoning. With the support of the current grant, our research group published 15 peer-reviewed papers, including 10 journal articles and 5 conference proceedings papers, as well as supporting about 10 conference presentations. The funded project also provided training opportunities for 3 postdoctoral researchers (one female scholar), 5 graduate students (4 female students), and about 40 undergraduate students (majority are female students) who were involved in research activities.\n\nThe project produced several major results that our research group has reported. Our experience of actions depends not only on spatiotemporal features of dynamic stimuli, but also on causal understanding of actions. We identified critical features in body movements signaling awkward social interactions between people. We found that people judge body movements to be smoother when the observed action is a natural outcome causing by a social interaction (e.g., a person raising up their arms appears to move smoothly if the person is trying to catch a ball thrown towards them). We also extended the experimental paradigm to show how to provide effective explanations of actions performed by AI and robot systems for fostering human trust in robotic systems.\n\nBased on our empirical findings, we developed a new computational model to show that capturing latent representations in motion dynamics is essential in accounting for human judgments. We compared our model with deep learning models for various visual recognition tasks, and found that in comparison with human perception, deep learning models lack generalization and deep understanding of objects and actions. Modeling work based on latent representations is likely to aid in developing more sophisticated AI systems for understanding actions, going beyond pattern recognition.\n\nFinally, our research group extended the experimental investigations beyond typical populations. We found that young infants are sensitive to causal consistency in actions. We also found individual differences in self-action recognition that were related to degree of autistic traits for people, and intact ability in recognizing action by schizophrenia patients. These findings from the project shed light on potential mechanisms in development. The paradigms developed in our project could be adopted to enable transformative research on mental disorders such as autism and schizophrenia.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/30/2022\n\n\t\t\t\t\tSubmitted by: Hongjing Lu"
 }
}