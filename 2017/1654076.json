{
 "awd_id": "1654076",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Nonconvex Optimization and Identifiability with Applications to Medical Imaging",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2017-03-13",
 "awd_max_amd_letter_date": "2021-08-10",
 "awd_abstract_narration": "Modern large-scale data sets arising in the physical and biological sciences often exhibit complex features that do not fit into the framework of existing methodologies, preventing the information in the gathered data from being fully utilized. In medical imaging, computed tomography (CT) scans and positron emission tomography (PET) scans are often best represented with models that are complex, beyond the scope of most available computational approaches, as existing methodology and theoretical analysis are mostly restricted to simpler classes of optimization problems. Since CT and PET scans come with a cost of a small radiation dose to the patient, better models for the data obtained by these imaging devices would result in a better tradeoff between the risk due to radiation and the benefit of obtaining a precise image for effective diagnosis and treatment. This research project will study a broad framework for complex optimization problems, applicable to medical imaging and across a range of problems in the physical and biological sciences, providing concrete methods and guarantees for many problems arising in these fields. The developed tools will be implemented on specific image reconstruction problems in CT and PET imaging, through collaborations with medical imaging researchers who will provide actual scan data, with the goal of enabling greater diagnostic accuracy for these popular clinical tools. Methods and code developed under this project will all be made publicly available. Throughout, the investigator will mentor students interested in working at the intersection of high-dimensional statistics, optimization, and medical imaging, and will increase interaction and communication across these fields through new courses and new collaborations.\r\n\r\nStatistical problems arising in many modern applied fields often exhibit a range of challenging features, including non-convexity and non-differentiability, that pose significant challenges for high dimensional optimization and theoretical analysis. The proposed research will explore complex non-convex optimization and identifiability problems to develop methodology and theory for a broad range of problems facing applied researchers in practice. The research will study and develop algorithms that adapt techniques such as sparse or low-rank optimization, primal/dual methods, and alternating minimization or alternating descent, with the aim of achieving efficient empirical performance and broad theoretical convergence guarantees. The resulting methods will be adapted to address concrete problems in medical imaging, where noisy side information must be incorporated into the reconstructed image, and where the image representation is confounded by additional parameters modeling the imaging device.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rina",
   "pi_last_name": "Barber",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rina Barber",
   "pi_email_addr": "rina@uchicago.edu",
   "nsf_id": "000602367",
   "pi_start_date": "2017-03-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Chicago",
  "perf_str_addr": "5747 S. Ellis Ave",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606375418",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  },
  {
   "pgm_ele_code": "804800",
   "pgm_ele_name": "Division Co-Funding: CAREER"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 76269.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 78080.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 79944.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 81865.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 83842.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to develop and analyze algorithms for signal estimation in challenging structured statistical settings, particularly for problems arising in image reconstruction. Taking computed tomography (CT or CAT) imaging as an example, the question of image reconstruction refers to an algorithm whose input is comprised of the measurements from the scanner (along with various calibration parameters describing the scanner's performance), and returning an estimated image that reveals the anatomical structure of the part of the body being imaged (and also, if applicable, the&nbsp; location within the body of the injected contrast agent). Solving this problem is challenging for a broad range of reasons, including (1) the problem of undersampling (i.e., relatively few measurements are taken, compared to a high desired resolution of the output image) and (2) the mathematical complexity of the \"forward model\" that describes how the scanner produces its measurements, in addition to possible errors or oversimplifications in this model.<br />With regard to problem (1), the issue of undersampling, it has been known for many years that image reconstruction in an undersampled regime can greatly improve in accuracy through the use of regularization, meaning that the algorithm should simultaneously seek to minimize both the \"data discrepancy\" (the discrepancy between the actual observed measurements, versus the measurements that the model tells us to expect if the true anatomy were equal to the estimated image) and the complexity of the estimated image. This project has also encompassed work on improving time resolution of magnetic resonance imaging (MRI), where higher time resolution means that the data is undersampled; to allow for this, we have developed a locally adaptive smoothness regularization scheme that allows for extremely efficient optimization while increasing time resolution. In other settings, a useful measure of complexity is the total variation (TV) norm, which measures the amount of local variation in tissue density; in typical naturally-occurring images, including anatomical images, we often expect to see most pairs of neighboring pixels being equal or approximately equal, with a large change from one pixel to the next occurring only if we are moving across an edge between one tissue type and another, such as when one pixel is part of a bone and the next pixel is part of the soft tissue. While TV regularization has been studied and successfully used in imaging for several decades, it continues to pose challenges in optimization particularly when combined with more complex measurement models, and addressing this has been one of the aims of our work for both CT and PET (positron emission tomography) imaging.&nbsp;<br />A main challenge addressed in this project is that of problem (2), particularly with regard to the issue of convexity: the property where if we average two potential estimated images, the assessed quality of the averaged image (in terms of data discrepancy and regularization) will be at least as good as the averaged quality of each of the two images. Without convexity, optimization is substantially more challenging, with many standard approaches failing both in theory and often empirically. For both CT and PET imaging, the underlying measurement model leads to a highly nonconvex optimization problem, but the medical imaging literature has often avoided this issue by either taking a convex approximation (which can then lead to noticeable systematic errors, called artifacts, in the recovered images), or (in the case of PET) by avoiding some of the unknowns via combining with additional scans, e.g., PET and CT scans taken simultaneously. A key aim of this project has been to develop and analyze algorithms that can directly address this nonconvex optimization problem to avoid these issues of accuracy or cost, and our work has developed algorithms that have shown promising success in simulated data (for CT and PET) and real data (so far, for CT only), as well as theoretical results that support these positive empirical findings and show that the algorithm strategy has potential applications to other statistical estimation problems as well.<br />In addition to these questions in optimization and image reconstruction, the project has also developed a range of methodological and theoretical results in the area of distribution-free predictive inference, which is the field of statistics that aims to provide predictive statements with assumption-free guarantees of validity (e.g., giving a range of possible values for an observed response variable, with 90% confidence). Results from this project include methods for producing prediction intervals offering distribution-free validity guarantees while balancing statistical and computational efficiency (the jackknife+), as well as theoretical characterizations of the types of inference targets and guarantees that are compatible or incompatible with the distribution-free framework. These results have the potential for application to problems in image reconstruction and related fields where we seek ways to quantify uncertainty in our estimates without relying too much on model assumptions.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/20/2023<br>\n\t\t\t\t\tModified by: Rina&nbsp;Barber</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project aims to develop and analyze algorithms for signal estimation in challenging structured statistical settings, particularly for problems arising in image reconstruction. Taking computed tomography (CT or CAT) imaging as an example, the question of image reconstruction refers to an algorithm whose input is comprised of the measurements from the scanner (along with various calibration parameters describing the scanner's performance), and returning an estimated image that reveals the anatomical structure of the part of the body being imaged (and also, if applicable, the  location within the body of the injected contrast agent). Solving this problem is challenging for a broad range of reasons, including (1) the problem of undersampling (i.e., relatively few measurements are taken, compared to a high desired resolution of the output image) and (2) the mathematical complexity of the \"forward model\" that describes how the scanner produces its measurements, in addition to possible errors or oversimplifications in this model.\nWith regard to problem (1), the issue of undersampling, it has been known for many years that image reconstruction in an undersampled regime can greatly improve in accuracy through the use of regularization, meaning that the algorithm should simultaneously seek to minimize both the \"data discrepancy\" (the discrepancy between the actual observed measurements, versus the measurements that the model tells us to expect if the true anatomy were equal to the estimated image) and the complexity of the estimated image. This project has also encompassed work on improving time resolution of magnetic resonance imaging (MRI), where higher time resolution means that the data is undersampled; to allow for this, we have developed a locally adaptive smoothness regularization scheme that allows for extremely efficient optimization while increasing time resolution. In other settings, a useful measure of complexity is the total variation (TV) norm, which measures the amount of local variation in tissue density; in typical naturally-occurring images, including anatomical images, we often expect to see most pairs of neighboring pixels being equal or approximately equal, with a large change from one pixel to the next occurring only if we are moving across an edge between one tissue type and another, such as when one pixel is part of a bone and the next pixel is part of the soft tissue. While TV regularization has been studied and successfully used in imaging for several decades, it continues to pose challenges in optimization particularly when combined with more complex measurement models, and addressing this has been one of the aims of our work for both CT and PET (positron emission tomography) imaging. \nA main challenge addressed in this project is that of problem (2), particularly with regard to the issue of convexity: the property where if we average two potential estimated images, the assessed quality of the averaged image (in terms of data discrepancy and regularization) will be at least as good as the averaged quality of each of the two images. Without convexity, optimization is substantially more challenging, with many standard approaches failing both in theory and often empirically. For both CT and PET imaging, the underlying measurement model leads to a highly nonconvex optimization problem, but the medical imaging literature has often avoided this issue by either taking a convex approximation (which can then lead to noticeable systematic errors, called artifacts, in the recovered images), or (in the case of PET) by avoiding some of the unknowns via combining with additional scans, e.g., PET and CT scans taken simultaneously. A key aim of this project has been to develop and analyze algorithms that can directly address this nonconvex optimization problem to avoid these issues of accuracy or cost, and our work has developed algorithms that have shown promising success in simulated data (for CT and PET) and real data (so far, for CT only), as well as theoretical results that support these positive empirical findings and show that the algorithm strategy has potential applications to other statistical estimation problems as well.\nIn addition to these questions in optimization and image reconstruction, the project has also developed a range of methodological and theoretical results in the area of distribution-free predictive inference, which is the field of statistics that aims to provide predictive statements with assumption-free guarantees of validity (e.g., giving a range of possible values for an observed response variable, with 90% confidence). Results from this project include methods for producing prediction intervals offering distribution-free validity guarantees while balancing statistical and computational efficiency (the jackknife+), as well as theoretical characterizations of the types of inference targets and guarantees that are compatible or incompatible with the distribution-free framework. These results have the potential for application to problems in image reconstruction and related fields where we seek ways to quantify uncertainty in our estimates without relying too much on model assumptions.\n\n\t\t\t\t\tLast Modified: 07/20/2023\n\n\t\t\t\t\tSubmitted by: Rina Barber"
 }
}