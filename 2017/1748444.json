{
 "awd_id": "1748444",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Constrained Statistical Estimation and Inference: Theory, Algorithms and Applications",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 144973.0,
 "awd_amount": 144973.0,
 "awd_min_amd_letter_date": "2017-07-26",
 "awd_max_amd_letter_date": "2017-07-26",
 "awd_abstract_narration": "This project lies at the boundary of statistics and machine learning. The underlying theme is to exploit constraints that are present in complex scientific data analysis problems, but that have not been thoroughly studied in traditional approaches. The project will explore theory, algorithms, and applications of statistical procedures, with constraints imposed on the storage, runtime, shape, energy or physics of the estimators and applications. The overall goal of the research is to develop theory and tools that can help scientists to conduct more effective data analysis.\r\n\r\nMany statistical methods are purely \"data driven\" and only place smoothness or regularity restrictions on the underlying model. In particular, classical statistical theory studies estimators without regard to their computational requirements. In modern data analysis settings, including astronomy, cloud computing, and embedded devices, computational demands are often central. The project will develop minimax theory and algorithms for nonparametric estimation and detection problems under constraints on storage, computation, and energy. Other constraints to be studied include shape restrictions such as convexity and monotonicity for high dimensional data. The project will also investigate the incorporation of physical constraints through the use of PDEs and models of physical dynamics and mechanics, focusing on both algorithms and theoretical bounds.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Lafferty",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "John D Lafferty",
   "pi_email_addr": "john.lafferty@yale.edu",
   "nsf_id": "000092106",
   "pi_start_date": "2017-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "24 Hillhouse Ave.",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065116814",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 144973.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The underlying theme of this project was to exploit constraints that are present in complex scientific data analysis problems, but that have not been thoroughly studied in traditional approaches. The research was at the interface of statistics and machine learning. The project explored theory, algorithms, and applications of statistical procedures, with constraints imposed on the storage, runtime, shape, energy or physics of the estimators and applications.&nbsp; Many statistical methods are purely \"data driven\" and only place smoothness or regularity restrictions on the underlying model. In particular, classical statistical theory studies estimators without regard to their computational requirements. In modern data analysis settings, including astronomy, cloud computing, and embedded devices, computational demands are often central.</p>\n<p>The project developed new theory and algorithms for nonparametric estimation and detection problems under constraints on storage, computation, and energy. In particular, new algorithms were developed in the setting where data are obtained from from multiple distributed sensors, and a signal is to be estimated by aggregating the information. Sharp analysis was carried out for this setting under smoothness assumptions commonly made in nonparametric regression.&nbsp; Placing limits on the number of bits that each machine can use to transmit information to the central machine, the results obtained included both asymptotic lower bounds and matching upper bounds on the statistical risk under various settings. Three regimes were identified, depending on the relationship among the number of machines, the size of data available at each machine, and the communication budget.&nbsp;</p>\n<p>Other constraints studied included shape restrictions such as convexity and monotonicity for high dimensional data. In particular, new algorithms were developed for imposing monotonicity constraints in machine learning procedures for handling high dimensional data. These methods reshape pre-trained prediction rules to satisfy shape constraints. One such method can be applied to any pre-trained prediction rule, while the second method deals specifically with random forests. In both cases, efficient algorithms were developed for computing the estimators, and experiments were performed to demonstrate their performance.</p>\n<p>In another line of research in this project, procedures for numerical optimization were studied, and a new mathematical framework was developed for characterizing the complexity of optimizing individual convex functions. In particular, function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize the function were obtained, in a form that relates to the curvature of the function at the optimum. A new computational invariant was studied that can be seen as a computational analogue of the Fisher information in classical statistical estimation.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/10/2018<br>\n\t\t\t\t\tModified by: John&nbsp;D&nbsp;Lafferty</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe underlying theme of this project was to exploit constraints that are present in complex scientific data analysis problems, but that have not been thoroughly studied in traditional approaches. The research was at the interface of statistics and machine learning. The project explored theory, algorithms, and applications of statistical procedures, with constraints imposed on the storage, runtime, shape, energy or physics of the estimators and applications.  Many statistical methods are purely \"data driven\" and only place smoothness or regularity restrictions on the underlying model. In particular, classical statistical theory studies estimators without regard to their computational requirements. In modern data analysis settings, including astronomy, cloud computing, and embedded devices, computational demands are often central.\n\nThe project developed new theory and algorithms for nonparametric estimation and detection problems under constraints on storage, computation, and energy. In particular, new algorithms were developed in the setting where data are obtained from from multiple distributed sensors, and a signal is to be estimated by aggregating the information. Sharp analysis was carried out for this setting under smoothness assumptions commonly made in nonparametric regression.  Placing limits on the number of bits that each machine can use to transmit information to the central machine, the results obtained included both asymptotic lower bounds and matching upper bounds on the statistical risk under various settings. Three regimes were identified, depending on the relationship among the number of machines, the size of data available at each machine, and the communication budget. \n\nOther constraints studied included shape restrictions such as convexity and monotonicity for high dimensional data. In particular, new algorithms were developed for imposing monotonicity constraints in machine learning procedures for handling high dimensional data. These methods reshape pre-trained prediction rules to satisfy shape constraints. One such method can be applied to any pre-trained prediction rule, while the second method deals specifically with random forests. In both cases, efficient algorithms were developed for computing the estimators, and experiments were performed to demonstrate their performance.\n\nIn another line of research in this project, procedures for numerical optimization were studied, and a new mathematical framework was developed for characterizing the complexity of optimizing individual convex functions. In particular, function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize the function were obtained, in a form that relates to the curvature of the function at the optimum. A new computational invariant was studied that can be seen as a computational analogue of the Fisher information in classical statistical estimation. \n\n \n\n\t\t\t\t\tLast Modified: 12/10/2018\n\n\t\t\t\t\tSubmitted by: John D Lafferty"
 }
}