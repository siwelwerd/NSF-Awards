{
 "awd_id": "1652294",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER:  In-Situ Compute Memories for Accelerating Data Parallel Applications",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2017-02-01",
 "awd_exp_date": "2024-01-31",
 "tot_intn_awd_amt": 573554.0,
 "awd_amount": 573554.0,
 "awd_min_amd_letter_date": "2017-01-24",
 "awd_max_amd_letter_date": "2021-05-11",
 "awd_abstract_narration": "As computing today is dominated by Big Data, there is a strong impetus for specialization for this important domain. Performance of these data-centric applications depends critically on efficient access and processing of data. These applications tend to be highly data-parallel and deal with large amounts. Recent studies show that by the year 2020, data production from individuals and corporations is expected to grow to 73.5 zetabytes, a 4.4\u00d7 increase from the year 2015. In addition, they tend to expend disproportionately large fraction of time and energy in moving data from storage to compute units, and in instruction processing, when compared to the actual computation. This research seeks to design specialized data-centric computing systems that dramatically reduce these overheads. \r\n \r\nIn a general-purpose computing system, the majority of the aggregate die area (over 90%) is devoted for storing and retrieving information at several levels in the memory hierarchy: on-chip caches, main memory (DRAM), and non-volatile memory (NVM). The central vision of this research is to create in-situ compute memories, which re-purpose the elements used in these storage structures and transform them into active computational units. In contrast to prior processing in memory approaches, which augment logic outside the memory arrays, the underpinning principle behind in-situ compute memories is to enable computation in-place within each memory array, without transferring the data in or out of it. Such a transformation could unlock massive data-parallel compute capabilities (up to 100\u00d7), and reduce energy spent in data movement through various levels of memory hierarchy (up to 20\u00d7), thereby directly address the needs of data-centric applications. This work develops in-situ compute memory technology, adapts the system software stack and re-designs data-centric applications to take advantage of those memories.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Reetuparna",
   "pi_last_name": "Das",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Reetuparna Das",
   "pi_email_addr": "reetudas@umich.edu",
   "nsf_id": "000750892",
   "pi_start_date": "2017-01-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "3003 S. State Street",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091274",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 154326.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 108341.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 100463.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 103593.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 106831.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-3958148c-7fff-bfb8-9d6f-1e95615fd422\"> </span></p>\n<p dir=\"ltr\"><span>Computer designers have traditionally separated the role of storage and compute units. Memories stored data. Processors&rsquo; logic units computed them. Is this separation necessary? A human brain does not separate the two so distinctly. Why should a processor? Over two-thirds of a processor die (caches) and all of main memory is devoted to temporary storage. Today, none of these memory elements can compute. But, could they?&nbsp; Our research, with this NSF CAREER Award, addresses this fundamental question regarding the role of memory and proposes to impose a dual responsibility on them: store and compute data.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Data stored in memory arrays share wires (bit-lines) and signal sensing apparatus (sense-amps). We observe that logic operations can be computed over these shared structures which allows us to re-purpose thousands of cache memory arrays into over a million of bit-serial arithmetic-logic units. Thus, we morph existing memory into massive vector compute units, providing parallelism several orders of magnitude higher than a contemporary GPU.&nbsp; Additionally, it saves energy spent shuffling data between storage and compute units -- a significant concern in BigData applications. In-memory computing is a significant departure from processing-in-memory (PIM) technologies which do not reuse memory structures for computing, but simply move conventional compute units near memory.</span></p>\n<p dir=\"ltr\"><span>Caches that compute can be a game changer for Artificial Intelligence (AI). They can add accelerator capabilities to general-purpose processors, without the significant die area cost of a dedicated accelerator like Google&rsquo;s TPU. For example, we showed that compute-enabled caches in Intel Xeon can improve processor efficiency by 629 times for convolutional neural networks (CNNs). This result received significant attention from the industry.&nbsp;</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span>Emerging non-volatile memories such as Resistive Memory (ReRAM) can also be repurposed into very large vector parallel units. We asked, could in-memory computing units be used as general-purpose data-parallel accelerators, much like GPUs? Our research showed that the answer is a resounding yes. We built novel compiler technology that can transform arbitrary data-parallel computation expressed in Google&rsquo;s TensorFlow into binaries that can run directly on ReRAM. We showed that in-memory computing can be over 700x more efficient than GPUs.&nbsp;&nbsp;</p>\n<p dir=\"ltr\"><span>&nbsp;</span>Finite state automata (FSA) are widely used as a computation model for approximate/exact regular expression matching in several application domains such as data analytics, network security, bioinformatics, and computational finance. We proposed a new way to repurpose DRAM memory as a Finite State Automata (FSA) accelerator. It addressed a very hard problem: parallelization of inherently sequential FSA. Our follow-up work extended this solution to turn processor caches in the FSA accelerator. To achieve this, we developed novel SRAM-based interconnects for automata processing. Our solution provides a speedup of 25x compared to state-of-the-art custom accelerators and more than 3000x compared to off-the-shelf stock processors.&nbsp;&nbsp;</p>\n<p dir=\"ltr\"><span>The automata research led us to Genomics, which was a killer application for FSA acceleration. Genomics can transform precision health over the next decade. We can detect cancer several years earlier through simple blood tests, without invasive biopsies. We can identify infectious pathogens and avoid the indiscriminate use of broad-spectrum antibiotics. In the last few years,&nbsp; partially supported by this award, we have made significant advancements in accelerating whole genome sequencing and ultra-rapid real-time sequencing for cancer diagnostics.&nbsp; We have demonstrated that hardware-software co-design approaches can provide orders of magnitude acceleration in some of the most time-consuming steps in Genomics.</span></p>\n<p dir=\"ltr\"><span>This project yielded several notable outcomes with broad impacts. Firstly, it supported two students in earning their PhD degrees in Computer Science and Engineering from the University of Michigan. It also partially supported several other students, including undergraduate research efforts. Secondly, the PI collaborated closely with Intel Corp. for technology transfer and also co-founded a precision health startup. Third, the PI created a permanent EECS undergraduate parallel programming class being taught to students at the University of Michigan.&nbsp; Lastly, the research was widely disseminated through numerous conferences and journal publications, seminars at universities and industrial research labs, and the creation of benchmarks and artifacts to facilitate further research by the broader community.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/09/2024<br>\nModified by: Reetuparna&nbsp;Das</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nComputer designers have traditionally separated the role of storage and compute units. Memories stored data. Processors logic units computed them. Is this separation necessary? A human brain does not separate the two so distinctly. Why should a processor? Over two-thirds of a processor die (caches) and all of main memory is devoted to temporary storage. Today, none of these memory elements can compute. But, could they? Our research, with this NSF CAREER Award, addresses this fundamental question regarding the role of memory and proposes to impose a dual responsibility on them: store and compute data.\n\n\nData stored in memory arrays share wires (bit-lines) and signal sensing apparatus (sense-amps). We observe that logic operations can be computed over these shared structures which allows us to re-purpose thousands of cache memory arrays into over a million of bit-serial arithmetic-logic units. Thus, we morph existing memory into massive vector compute units, providing parallelism several orders of magnitude higher than a contemporary GPU. Additionally, it saves energy spent shuffling data between storage and compute units -- a significant concern in BigData applications. In-memory computing is a significant departure from processing-in-memory (PIM) technologies which do not reuse memory structures for computing, but simply move conventional compute units near memory.\n\n\nCaches that compute can be a game changer for Artificial Intelligence (AI). They can add accelerator capabilities to general-purpose processors, without the significant die area cost of a dedicated accelerator like Googles TPU. For example, we showed that compute-enabled caches in Intel Xeon can improve processor efficiency by 629 times for convolutional neural networks (CNNs). This result received significant attention from the industry.\n\n\nEmerging non-volatile memories such as Resistive Memory (ReRAM) can also be repurposed into very large vector parallel units. We asked, could in-memory computing units be used as general-purpose data-parallel accelerators, much like GPUs? Our research showed that the answer is a resounding yes. We built novel compiler technology that can transform arbitrary data-parallel computation expressed in Googles TensorFlow into binaries that can run directly on ReRAM. We showed that in-memory computing can be over 700x more efficient than GPUs.\n\n\nFinite state automata (FSA) are widely used as a computation model for approximate/exact regular expression matching in several application domains such as data analytics, network security, bioinformatics, and computational finance. We proposed a new way to repurpose DRAM memory as a Finite State Automata (FSA) accelerator. It addressed a very hard problem: parallelization of inherently sequential FSA. Our follow-up work extended this solution to turn processor caches in the FSA accelerator. To achieve this, we developed novel SRAM-based interconnects for automata processing. Our solution provides a speedup of 25x compared to state-of-the-art custom accelerators and more than 3000x compared to off-the-shelf stock processors.\n\n\nThe automata research led us to Genomics, which was a killer application for FSA acceleration. Genomics can transform precision health over the next decade. We can detect cancer several years earlier through simple blood tests, without invasive biopsies. We can identify infectious pathogens and avoid the indiscriminate use of broad-spectrum antibiotics. In the last few years, partially supported by this award, we have made significant advancements in accelerating whole genome sequencing and ultra-rapid real-time sequencing for cancer diagnostics. We have demonstrated that hardware-software co-design approaches can provide orders of magnitude acceleration in some of the most time-consuming steps in Genomics.\n\n\nThis project yielded several notable outcomes with broad impacts. Firstly, it supported two students in earning their PhD degrees in Computer Science and Engineering from the University of Michigan. It also partially supported several other students, including undergraduate research efforts. Secondly, the PI collaborated closely with Intel Corp. for technology transfer and also co-founded a precision health startup. Third, the PI created a permanent EECS undergraduate parallel programming class being taught to students at the University of Michigan. Lastly, the research was widely disseminated through numerous conferences and journal publications, seminars at universities and industrial research labs, and the creation of benchmarks and artifacts to facilitate further research by the broader community.\n\n\n\t\t\t\t\tLast Modified: 07/09/2024\n\n\t\t\t\t\tSubmitted by: ReetuparnaDas\n"
 }
}