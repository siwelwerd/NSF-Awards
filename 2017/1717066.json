{
 "awd_id": "1717066",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Optical Skin For Robots: Tactile Sensing and Whole Body Vision",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Roger Mailler",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 440000.0,
 "awd_amount": 440000.0,
 "awd_min_amd_letter_date": "2017-07-25",
 "awd_max_amd_letter_date": "2017-07-25",
 "awd_abstract_narration": "This project will enable robots to feel what they touch. The key idea\r\nis to put cameras inside the body of the robot, looking outward at the\r\nrobot skin as it deforms, and also through the robot skin to see nearby objects as they are grasped or avoided.  This approach addresses several challenges: 1) achieving close to human resolution (a million biological sensors) using millions of pixels, 2) reducing occlusion during grasping and manipulation, and detecting obstacles before impact, and 3) protecting expensive electronics and wiring while allowing replacement of worn out or damaged inexpensive skin. Humans replace the outer layer of our skin every month.  One theory as to why current robots are so clumsy is that they have little or no feeling in their skin.  Robots that can feel when they touch will make better servants and be more useful, especially when taking care of older adults and others needing help walking, dressing, cleaning, or feeding. A soft touch is needed in many tasks in the home and workplace. Teaching robots to do new tasks, and robot learning,\r\nwill be much easier if robots can feel what they are doing. Robots will also be safer, and safer to work with, if they can feel accidental contact or the size of forces they are applying. Possible applications of this approach include aware furniture and car interiors that feel what a human is doing or wants to do, aware\r\nclothes, aware tools, and aware floors, walls, and ceilings. Optical skin-based sensing is a practical, affordable, manufacturable, and maintainable approach to sensing for touching and helping people.\r\n\r\nTechnical goals for the project include first building and then installing on a robot a network of about 100 off-the-shelf small cameras (less than 1 cubic centimeter) that is capable of collecting information, deciding what video streams to pay attention to, and processing the video streams to estimate forces, slip, and object shape. The wiring, solder joints, and other connections in the sensing system don't have to repeatedly deform or face cyclic or variable stresses--major sources of failure. The materials that do deform or are stressed (the outer layer of the skin) can be optimized for sensing, grasping and manipulation, and mechanical robustness, and can be easily and cheaply replaced when worn or damaged.  The researchers will evaluate their work by co-developing an \"optical skin\" for hands and a synergistic soft hand. A transformative idea is to aggressively distribute high resolution imaging over the entire robot body. This reduces occlusion, a major issue in perception for manipulation. Given the low cost of imaging sensors, there is no longer a need to restrict optical sensing to infrared range finders (single pixel depth\r\ncameras), line cameras, or low resolution area cameras.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Atkeson",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher G Atkeson",
   "pi_email_addr": "cga@cs.cmu.edu",
   "nsf_id": "000477583",
   "pi_start_date": "2017-07-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Akihiko",
   "pi_last_name": "Yamaguchi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Akihiko Yamaguchi",
   "pi_email_addr": "akihikoy@andrew.cmu.edu",
   "nsf_id": "000712058",
   "pi_start_date": "2017-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "CMU Robotics Institute",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 440000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We developed a multi-spectral multi-modal system for high resolution tactile  sensing and proximity sense for covering an entire robot body in  addition to fingers and hands. Sensing includes visible light and  far-infrared (thermal) cameras, radar, vibration and sound sensing. We further developed the original and new versions of FingerVision,  cameras in a finger used for both tactile and proximity sensing. Both  tactile and proximity sensing are in the form of high resolution  imaging. One key idea is to turn tactile sensing into a computer vision  problem, and take advantage of recent progress in computer vision. A  second key idea is to develop proximity imaging as an important form of  body-based vision that complements sensors located in the head. This  provides more complete visual input and avoids much occlusion and  self-occlusion.</p>\n<p>We found that proximity imaging is very useful, perhaps more useful than tactile  sensing. It helps the robot get in the right place, while grasping is  often open loop with respect to tactile sensing for compliant objects or  compliant hands. Proximity vision provides useful priors for object location, pose, shape, and identity for tactile sensing. We used the point of visual expansion to predict the contact location.  To do this accurately we had to integrate optical flow over about 10cm.  We were able to predict contact location within about 1cm.</p>\n<p>Optical sensing of delicate object deformation is more sensitive than current ways to measure skin deformation, so being able to see what is being manipulated supports &ldquo;force control&rdquo; at very low forces that are hard to measure using strain of some material, as well as being able to detect slip, and incipient slips (when slipping is about to happen). Being able to see the manipulated object helps with letting go, where sometimes robot fingers knock an object over when releasing it, especially if there is any adhesion. The prior on contact location from proximity imaging resolved ambiguous contact predictions  based on only tactile sensing. The prior made using tactile information  to estimate contact location possible in many cases.</p>\n<p>We demonstrated the utility of short-range chip-based radar for robots. Ultra-wide-band pulse radar is the simplest and cheapest to use (for  example Acconeer.com). Radar works best when it can  be moved around (for example synthetic aperture radar). Translation of  the radar can resolve correspondence ambiguities.</p>\n<p>We demonstrated the utility of thermal imaging to provide thermal sensing over an area of robot skin. Thermal imaging from inside the robot can be used to implement  thermal sensing across an area of skin as well as provide thermal imaging of nearby objects. Thermal imaging provides useful  information about where humans are, and when humans are in contact with  the robot, as well as during processes involving heating and cooling,  such as cooking.</p>\n<p>We developed FingerVision II in which different cameras were collocated: a camera for tactile sensing,  and two types of cameras for proximity vision (an RGB camera and a  Time-of-Flight (TOF) camera for depth). This version only covered the  tactile sensing camera with a transparent elastomer, while the RGB and  depth cameras were not obstructed. This resulted in better images for  the proximity cameras, avoiding distortion by the elastomer as well as  wear and tear (scratches etc.) on the elastomer.</p>\n<p>We developed SearchLight, a vision system in which controlled illumination is used to  highlight depth and texture. This system is able to recover normal vector, depth, and  texture information much more accurately than traditional vision  approaches.</p>\n<p>We developed BodyVision, applying FingerVision to the entire body of a robot, in  which a multi-spectral multi-modal system suitable for covering an  entire robot is used for tactile, thermal, vibration, and proximity  sensing. Proximity sensing was performed using visible light, infrared  light, and radar.</p>\n<p>Technology is being transferred to industry through a collaboration  with the Toyota Research Institute (TRI), a technology transfer  organization of Toyota. TRI is funding the continuation and further  development of this work.</p>\n<p>Technology is also being transferred to industry through a new  company, FingerVision Inc., founded by Akihiko Yamaguchi. Further  information is available at https://www.fingervision.jp/en</p>\n<p>A transformative paradigm shift that is supported by this work is  from robotics that is currently too focused on head-based sensing and  the biological model for skin-based sensing, which focuses on skin  deformation, to using a much wider range of sensing modalities, sensors,  and sensing modalities. There are a wide range of non-biological or  \"superhuman\" sensing modalities that complement biologically inspired  sensing as well as other forms of superhuman sensing. Development of  these sensors is partly driven by the cell phone industry.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/28/2022<br>\n\t\t\t\t\tModified by: Christopher&nbsp;Atkeson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWe developed a multi-spectral multi-modal system for high resolution tactile  sensing and proximity sense for covering an entire robot body in  addition to fingers and hands. Sensing includes visible light and  far-infrared (thermal) cameras, radar, vibration and sound sensing. We further developed the original and new versions of FingerVision,  cameras in a finger used for both tactile and proximity sensing. Both  tactile and proximity sensing are in the form of high resolution  imaging. One key idea is to turn tactile sensing into a computer vision  problem, and take advantage of recent progress in computer vision. A  second key idea is to develop proximity imaging as an important form of  body-based vision that complements sensors located in the head. This  provides more complete visual input and avoids much occlusion and  self-occlusion.\n\nWe found that proximity imaging is very useful, perhaps more useful than tactile  sensing. It helps the robot get in the right place, while grasping is  often open loop with respect to tactile sensing for compliant objects or  compliant hands. Proximity vision provides useful priors for object location, pose, shape, and identity for tactile sensing. We used the point of visual expansion to predict the contact location.  To do this accurately we had to integrate optical flow over about 10cm.  We were able to predict contact location within about 1cm.\n\nOptical sensing of delicate object deformation is more sensitive than current ways to measure skin deformation, so being able to see what is being manipulated supports \"force control\" at very low forces that are hard to measure using strain of some material, as well as being able to detect slip, and incipient slips (when slipping is about to happen). Being able to see the manipulated object helps with letting go, where sometimes robot fingers knock an object over when releasing it, especially if there is any adhesion. The prior on contact location from proximity imaging resolved ambiguous contact predictions  based on only tactile sensing. The prior made using tactile information  to estimate contact location possible in many cases.\n\nWe demonstrated the utility of short-range chip-based radar for robots. Ultra-wide-band pulse radar is the simplest and cheapest to use (for  example Acconeer.com). Radar works best when it can  be moved around (for example synthetic aperture radar). Translation of  the radar can resolve correspondence ambiguities.\n\nWe demonstrated the utility of thermal imaging to provide thermal sensing over an area of robot skin. Thermal imaging from inside the robot can be used to implement  thermal sensing across an area of skin as well as provide thermal imaging of nearby objects. Thermal imaging provides useful  information about where humans are, and when humans are in contact with  the robot, as well as during processes involving heating and cooling,  such as cooking.\n\nWe developed FingerVision II in which different cameras were collocated: a camera for tactile sensing,  and two types of cameras for proximity vision (an RGB camera and a  Time-of-Flight (TOF) camera for depth). This version only covered the  tactile sensing camera with a transparent elastomer, while the RGB and  depth cameras were not obstructed. This resulted in better images for  the proximity cameras, avoiding distortion by the elastomer as well as  wear and tear (scratches etc.) on the elastomer.\n\nWe developed SearchLight, a vision system in which controlled illumination is used to  highlight depth and texture. This system is able to recover normal vector, depth, and  texture information much more accurately than traditional vision  approaches.\n\nWe developed BodyVision, applying FingerVision to the entire body of a robot, in  which a multi-spectral multi-modal system suitable for covering an  entire robot is used for tactile, thermal, vibration, and proximity  sensing. Proximity sensing was performed using visible light, infrared  light, and radar.\n\nTechnology is being transferred to industry through a collaboration  with the Toyota Research Institute (TRI), a technology transfer  organization of Toyota. TRI is funding the continuation and further  development of this work.\n\nTechnology is also being transferred to industry through a new  company, FingerVision Inc., founded by Akihiko Yamaguchi. Further  information is available at https://www.fingervision.jp/en\n\nA transformative paradigm shift that is supported by this work is  from robotics that is currently too focused on head-based sensing and  the biological model for skin-based sensing, which focuses on skin  deformation, to using a much wider range of sensing modalities, sensors,  and sensing modalities. There are a wide range of non-biological or  \"superhuman\" sensing modalities that complement biologically inspired  sensing as well as other forms of superhuman sensing. Development of  these sensors is partly driven by the cell phone industry.\n\n \n\n\t\t\t\t\tLast Modified: 06/28/2022\n\n\t\t\t\t\tSubmitted by: Christopher Atkeson"
 }
}