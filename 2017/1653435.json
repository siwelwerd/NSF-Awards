{
 "awd_id": "1653435",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Stochastic Nested Composition Optimization: Theory and Algorithms",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922443",
 "po_email": "gaklutke@nsf.gov",
 "po_sign_block_name": "Georgia-Ann Klutke",
 "awd_eff_date": "2017-02-01",
 "awd_exp_date": "2023-01-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2017-01-19",
 "awd_max_amd_letter_date": "2017-01-19",
 "awd_abstract_narration": "The objective of this Faculty Early Career Development (CAREER) award is to develop foundational theory and efficient computational tools for an important class of data-driven stochastic optimization problems, stochastic nested composition optimization. These nested composition problems arise in many areas such as risk management, machine learning, and online decision making, and are not amenable to classical methods of stochastic optimization. On the theory and methodology side, the project will advance both optimization and data analytics. On the education side, the project will result in curricular innovation that integrates optimization and data analytics in a unified way and offers new case studies on practical applications. The project will promote underrepresented minority students by involving them in frontier research. The research will produce new algorithms and analysis tools that will be useful for many data-intensive applications.  These methods will be empirically tested in a collaborative project with a local healthcare system, with the goal of improving healthcare delivery by reducing cost and improving quality of service. \r\n\r\nStochastic nested composition optimization constitutes a new class of stochastic optimization problems that involve nested nonlinear composition of multiple expectations and multi-level random variables. This project will (a) establish the basic complexity theory for two-level and multi-level stochastic nested composition optimization and their generalizations; (b) develop efficient algorithms that process streaming data with theoretical guarantees; (c) investigate several special cases of the problem and apply the results to modeling and optimizing healthcare decisions based on real clinical data (obtained through the collaboration with a NJ-based hospital chain); and (d) develop innovative curricula and research projects that can bring students at various levels to frontier technology. The nested composition provides a rich modeling tool for applications that require data-driven decision-making and optimization under uncertainty. A critical challenge is that the objective is no longer a linear functional of the data distribution, and thus existing theory and methods are inappropriate. The nonlinearity with respect to the distribution of data makes the problem fundamentally more difficult than most of the classical problems. Overcoming the analytical challenge calls for an integration of mathematical programming and stochastic analysis. If successful, the research will make a substantial contribution by expanding the scope of stochastic optimization. Theoretically, it will strengthen our mathematical understanding of stochastic optimization and establish foundational sample complexity bounds. Methodologically, it will provide new algorithms and analysis tools for several important problems in data-driven optimization and online learning. The results will establish important connections among several areas in mathematical programming, statistics, and machine learning.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mengdi",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mengdi Wang",
   "pi_email_addr": "mengdiw@princeton.edu",
   "nsf_id": "000712266",
   "pi_start_date": "2017-01-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "226 Sherrerd Hall",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "006Y00",
   "pgm_ele_name": "OE Operations Engineering"
  },
  {
   "pgm_ele_code": "104500",
   "pgm_ele_name": "CAREER: FACULTY EARLY CAR DEV"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "077E",
   "pgm_ref_txt": "SIMULATION MODELS"
  },
  {
   "pgm_ref_code": "078E",
   "pgm_ref_txt": "ENTERPRISE DESIGN & LOGISTICS"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "8023",
   "pgm_ref_txt": "Health Care Enterprise Systems"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div>\n<p><span>The goal of this project was to address a new class of optimization problems, known as Stochastic Nested Composition Optimization. This type of problem involves optimizing the nested compositions of multiple expected-value functions over a path of random variables. The resulting model has proven to be extremely useful in a variety of fields, including operations management and machine learning. However, the problem is fundamentally more difficult than classical stochastic optimization, as the objective is no longer linear with respect to the joint probability distribution of the multi-level random variables.</span></p>\n<p>&nbsp;</p>\n<p><span>To tackle this challenge, the team developed a class of Stochastic Compositional Gradient Descent (SCGD) algorithms. These algorithms update the solutions based on noisy sample gradients of the inner and outer functions and use an auxiliary variable to track the unknown expectation of the inner function value. The methods converge almost surely to an optimal solution for convex problems or a stationary solution for nonconvex problems. Additionally, the team sought to accelerate the algorithm by using a combination of variance reduction and extrapolation techniques.</span></p>\n<p><span>The team's research has yielded several important results. First, under some smoothness assumptions, they showed that solving the compositional optimization problem has similar complexity as single-level optimization. Additionally, the team applied the SCGD algorithm to the field of meta machine learning, where they developed a method to tune hyperparameters to maximize out-of-sample performance of a trained machine learning model. The team also extended their method to federated machine learning, a popular approach for machine learning on distributed data. The results generated by this project have been published in top optimization journals and international machine learning conferences.</span></p>\n<p><span>The research conducted by this project has had a significant impact on the field of machine learning optimization. The team's algorithm solutions have been followed by many in the field, and their research has been integrated into classroom teaching in both undergraduate and graduate curricula at Princeton University.</span></p>\n<p><span>In summary, this project has focused on the development of Stochastic Compositional Gradient Descent algorithms to optimize the nested compositions of multiple expected-value functions. Results of the project have led to significant advancements in machine learning optimization and have the potential to unlock new capabilities in a variety of fields.</span></p>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/06/2023<br>\n\t\t\t\t\tModified by: Mengdi&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1653435/1653435_10469917_1683328196163_PastedGraphic--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1653435/1653435_10469917_1683328196163_PastedGraphic--rgov-800width.jpg\" title=\"compositionopt\"><img src=\"/por/images/Reports/POR/2023/1653435/1653435_10469917_1683328196163_PastedGraphic--rgov-66x44.jpg\" alt=\"compositionopt\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of Stochastic Nested CompositionOptimization. Top left: the classical one-level problem. Topright: the two-level composition problem. Bottom: Multi-leveloptimization over a random path.</div>\n<div class=\"imageCredit\">PI Mengdi Wang</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mengdi&nbsp;Wang</div>\n<div class=\"imageTitle\">compositionopt</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n\nThe goal of this project was to address a new class of optimization problems, known as Stochastic Nested Composition Optimization. This type of problem involves optimizing the nested compositions of multiple expected-value functions over a path of random variables. The resulting model has proven to be extremely useful in a variety of fields, including operations management and machine learning. However, the problem is fundamentally more difficult than classical stochastic optimization, as the objective is no longer linear with respect to the joint probability distribution of the multi-level random variables.\n\n \n\nTo tackle this challenge, the team developed a class of Stochastic Compositional Gradient Descent (SCGD) algorithms. These algorithms update the solutions based on noisy sample gradients of the inner and outer functions and use an auxiliary variable to track the unknown expectation of the inner function value. The methods converge almost surely to an optimal solution for convex problems or a stationary solution for nonconvex problems. Additionally, the team sought to accelerate the algorithm by using a combination of variance reduction and extrapolation techniques.\n\nThe team's research has yielded several important results. First, under some smoothness assumptions, they showed that solving the compositional optimization problem has similar complexity as single-level optimization. Additionally, the team applied the SCGD algorithm to the field of meta machine learning, where they developed a method to tune hyperparameters to maximize out-of-sample performance of a trained machine learning model. The team also extended their method to federated machine learning, a popular approach for machine learning on distributed data. The results generated by this project have been published in top optimization journals and international machine learning conferences.\n\nThe research conducted by this project has had a significant impact on the field of machine learning optimization. The team's algorithm solutions have been followed by many in the field, and their research has been integrated into classroom teaching in both undergraduate and graduate curricula at Princeton University.\n\nIn summary, this project has focused on the development of Stochastic Compositional Gradient Descent algorithms to optimize the nested compositions of multiple expected-value functions. Results of the project have led to significant advancements in machine learning optimization and have the potential to unlock new capabilities in a variety of fields.\n\n\n \n\n\t\t\t\t\tLast Modified: 06/06/2023\n\n\t\t\t\t\tSubmitted by: Mengdi Wang"
 }
}