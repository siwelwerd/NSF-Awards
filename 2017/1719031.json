{
 "awd_id": "1719031",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "PFI:BIC: iWork, a Modular Multi-Sensing Adaptive Robot-Based Service for Vocational Assessment, Personalized Worker Training and Rehabilitation.",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032927795",
 "po_email": "jsoriano@nsf.gov",
 "po_sign_block_name": "Jesus Soriano Molla",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 999638.0,
 "awd_amount": 1031638.0,
 "awd_min_amd_letter_date": "2017-07-25",
 "awd_max_amd_letter_date": "2022-04-26",
 "awd_abstract_narration": "Automation, foreign competition, and the increasing use of robots replacing human jobs, stress the need for a major shift in vocational training practices to training for intelligent manufacturing environments, so-called \"Industry 4.0\".  In particular, vocational safety training using the latest robot and other technologies is imperative, as thousands of workers lose their job or die on the job each year due to accidents, unforeseen injuries, and lack of appropriate assessment and training.  The objective of this Partnerships for Innovation: Building Innovation Capacity (PFI:BIC) project is to develop iWork, a smart robot-based vocational assessment and intervention system to assess the physical, cognitive and collaboration skills of an industry worker while he/she performs a manufacturing tasks in a simulated industry setting and collaborating with a robot to do the task. The aim is to transform traditional vocational training and rehabilitation practices to an evidence-based and personalized system that can be used to (re)train, retain, and prepare workers for robotic factories of the future. The need for personalized vocational training, rehabilitation and accurate job-matching is essential to ensuring a strong manufacturing sector, vital to America's economic development and ability to innovate. The iWork service is \"smart\" because it can adjust and adapt to the individual's abilities as it assesses him/her and help decide on the type of tasks needed to test and train, based on the job's complexity, difficulty or familiarity to the worker.  The iWork system integrates human expert knowledge to overcome or compensate for detected worker constraints. \r\nResearch has shown that robot trainers can increase motivation and sustain interest, increase compliance and learning, and provide training for specific and individual needs. The iWork system aims to assess and train both the human and the work-assistive robot, as they collaborate on a manufacturing job.  The projected outcome is low-cost vocational training solutions that can have substantial economic and societal benefits to diverse economic sectors. Most importantly, if successful, projected outcomes could impact how millions of persons seeking a manufacturing job are trained, including those facing a type of learning, physical or aging disability. The system's mobile, low cost methods accelerate recognizing a worker's specific needs and improve the ability of the vocational expert to make correlations between cognitive and physical assessments, thus empowering traditional practices with user-centric targeted training methods. In addition, the project's robot-based emphasis on safety and risk assessment, can reduce liability costs and productivity setbacks faced by industry, due to manufacturing accidents. \r\nThe iWork system uses computational methods in reinforcement (machine) learning, data mining, collaborative filtering and human robot interaction to collect and analyze multi-sensing worker data during a manufacturing human-robot collaboration simulation. Data collected and analyzed come from sensors, wearables, and explicit user feedback measuring worker movements, eye gazes, errors made, performance delays, human-robot interactions, physiological metrics, and others, depending on the task. The system has a closed loop architecture composed of four phases: assessment, recommendation, intervention (or adjustment), and evaluation, with a human expert in the loop. The system generates recommendations for personalized interventions to the expert, at different loop intervals. Use of the latest developments in sensing technologies, robotics and intelligent communications, assess the ability to enhance the intelligence of a robot co-worker with more human-like learning and collaboration abilities to support the human in achieving a task.  The system is modular and customizable to a particular manufacturing task, domain or worker robot. Two types of robots are used, socially assistive robots that provide non-contact user assistance through feedback and physically assistive robots that provide cognitive, physical and collaboration skill training. To predict risks of injury due to inattention, age, vision, or physical and mental issues, motion analysis and kinematics experiments are conducted to determine the type of safety training needed, to assess how well a human interacts with a collaborative robot, and how best to train the robot to help the human overcome identified physical and other deficiencies in performing a given task. The project integrates three main areas of expertise, engineered service system design, where assistive robots interact with and train each other to collaborate; computing, sensing, and information technologies, where machine learning, data mining and recommender algorithms are used to identify behavioral patterns of interest, and recommend targeted interventions; and human factors and cognitive engineering that deploy methods from the team's expertise in workplace assessment, personalized psychiatric intervention, and evaluation methods of vocational satisfaction, work habits, work quality, etc., as they relate to job preparation and retention.\r\nThe project has an interdisciplinary team of experts from two collaborating universities, University of Texas Arlington (UTA) and Yale University, representing several fields, including human factors, psychology, computing, and industrial organization. The project deploys two primary industry partners, SoftBank Robotics (San Francisco, CA) manufacturer of humanoid service robots, and InteraXon (Canada), producing mobile EEG devices, who provides hardware, software and know-how to enhance iWork's functionality in cognitive activity monitoring. The broader context partners include, C8Sciences (USA), Assistive Technology Resources (USA), Barrett Technologies Inc. (USA), and the Dallas Veteran Affairs Research Corp. (USA).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Fillia",
   "pi_last_name": "Makedon",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Fillia S Makedon",
   "pi_email_addr": "makedon@cse.uta.edu",
   "nsf_id": "000191699",
   "pi_start_date": "2017-07-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vassilis",
   "pi_last_name": "Athitsos",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vassilis Athitsos",
   "pi_email_addr": "athitsos@uta.edu",
   "nsf_id": "000308836",
   "pi_start_date": "2017-07-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Nicolette",
   "pi_last_name": "Hass",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nicolette Hass",
   "pi_email_addr": "nphass@uta.edu",
   "nsf_id": "000557679",
   "pi_start_date": "2017-07-25",
   "pi_end_date": "2018-06-14"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Morris",
   "pi_last_name": "Bell",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Morris Bell",
   "pi_email_addr": "morris.bell@yale.edu",
   "nsf_id": "000685730",
   "pi_start_date": "2017-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Arlington",
  "inst_street_address": "701 S NEDDERMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "ARLINGTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "8172722105",
  "inst_zip_code": "760199800",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT ARLINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "LMLUKUPJJ9N3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Arlington",
  "perf_str_addr": "",
  "perf_city_name": "Arlington",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "760190145",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "166200",
   "pgm_ele_name": "PFI-Partnrships for Innovation"
  },
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "116E",
   "pgm_ref_txt": "RESEARCH EXP FOR UNDERGRADS"
  },
  {
   "pgm_ref_code": "1662",
   "pgm_ref_txt": "PARTNRSHIPS FOR INNOVATION-PFI"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 999638.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 32000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Project Title:&nbsp;PFI:BIC: iWork, a Modular Multi-Sensing Adaptive Robot-Based Service for Vocational Assessment, Personalized Worker Training and Rehabilitation.</p>\n<p>PI: Fillia Makedon</p>\n<p>Awardee: University of Texas at Arlington</p>\n<p>Award Number:&nbsp;1719031</p>\n<p>Award Ends:&nbsp;08/31/2023</p>\n<p class=\"Normal1\"><strong>Motivation and objectives</strong></p>\n<p>The objective of this research&nbsp;is to develop iWork, a smart robot-based vocational assessment and intervention service system to assess physical, cognitive and robot-collaboration skills of a worker while she/he performs simulated manufacturing tasks. To achieve this, multimodal human-robot interaction data are collected and analyzed. This is a modular closed loop data flow system withfour phases, assessment, recommendation, intervention, and evaluation, with a human-factors expert in the loop.</p>\n<p class=\"Normal1\"><strong>Summary of Activities</strong></p>\n<p class=\"Normal1\">A testbed system was designed from the collected multisensing data while the user performed certain tasks. Manufacturing simulation tasks were used to provide vocational assessment and intervention service that can assess physical, cognitive and collaboration skills of a potential worker. Designed algorithms to collect multisensing data while the user performs simulated workplace tasks. Advanced machine learning algorithms were designed and used to analyze and evaluate the collected multisensing human-robot collaboration data. Experiments were designed using assistive robots to train the user and to assess work habits such as, work quality, cooperativeness, social skills and personal presentation as they relate to job preparation, engagement and retention. Different engineering methods were explored to provide workplace assessments. New software products and user interfaces were developed.</p>\n<p class=\"Normal1\"><strong>Project Outcomes and Findings</strong></p>\n<p class=\"Normal1\">New methods were developed to assess cognitive and physical fatigue of users engaged in simulated vocational tasks. A multisensory system was developed to monitor and assess the performance of veterans with breathing/dyspnea issues. A smart wearable shirt called PNEUMON was developed to monitor persons with dyspnea issues that can cause fatigue and other problems relevant to vocational performance.<strong>&nbsp;</strong>An assistive robot called MINA was developed to explore gait issues of persons in a clinical setting and to assess the ability of working nurses to use such an assistive robot effectively. The ATEC system (Activate Test for Embodied Cognition) was developed to assess executive function capabilities of children and young adults through embodied cognition tasks/games/exercises, using metrics that relate to performance in a vocational setting. The project resulted in the graduation of 5 Ph.D. students, the graduation of 4 REU students, and one MS thesis student. They received competitive positions in industry and government, based on the work they did on the project (General Motors, SalesForce, Hewlett Packard and FDA). Additionally, 6 REU students were trained and mentored. The project was used to mentor a female postdoc for 2 years, who then became assistant professor at the University of Santa Clara, CA. The PI?S team received Best Technical Paper award on a paper describing project results and also best poster paper award at the 2021 PETRA conference. The project team applied for a patent for the analysis of multisensory data related to vocational simulation experiments. Project researchers published many papers in peer review conferences and journals. User interfaces were developed for the PNEUMON dyspnea system, for the ATEC assessment system and for monitoring the Human Robot Interaction with the MINA robotic assistant.&nbsp;</p>\n<p><strong>Intellectual Merit:&nbsp;</strong></p>\n<p>This is an interdisciplinary project that integrated three areas of expertise, (1)&nbsp;<strong>engineered service system design</strong>, where socially and rehabilitation assistive robots interact with and train the user to collaborate. 2)<strong>&nbsp;</strong><strong>Computing, sensing, and information technologies,</strong>&nbsp;where machine learning, data mining and recommender algorithms are used to identify patterns of interest, (e.g., errors, delays, etc.) and recommend targeted interventions for the user. (3)&nbsp;<strong>Human Factors and cognitive engineering</strong>&nbsp;that deploy methods from the team?s Intelligent Interactive Learning and Adaptation Framework.&nbsp;&nbsp;Expertise in workplace assessment, personalized psychiatric intervention, and evaluation methods of vocational satisfaction, guided experiments on Work Habits, Work Quality, Cooperativeness, Personal Presentation and Social (interpersonal) Skills, as they relate to job preparation and retention. Innovative new methods were developed for human-robot collaboration and training.<strong>&nbsp;</strong></p>\n<p><strong>&nbsp;Broader Impacts:</strong></p>\n<p>The iWork vocational assessment system produces personalized vocational training solutions that have great economic and societal impact and affect economic sectors such as, energy, transportation, mining, electronics, robotics, and others.&nbsp;&nbsp;System outcomes impact traditional disciplines such as education, social sciences, psychology, computer science, rehabilitation, robotics and others. Additionally, project outcomes potentially impact millions of persons seeking a manufacturing job, including those facing a type of learning or aging disability. The system?s mobile, low-cost methods can accelerate recognizing a worker?s ability, improve correlations between cognitive and physical assessments and elevate traditional Vocational Rehabilitation practices to new levels of efficient, user-centric training. Furthermore, the project?s robot-based training emphasis on safety and risk assessment, can reduce liability costs and productivity setbacks due to manufacturing accidents.&nbsp;The project?s educational plan engaged student programmers and interns throughout the system?s development, organized industrial internships and encouraged industry-university collaborations through the iPerform IUCRC NSF center.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/07/2023<br>\n\t\t\t\t\tModified by: Fillia&nbsp;S&nbsp;Makedon</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nProject Title: PFI:BIC: iWork, a Modular Multi-Sensing Adaptive Robot-Based Service for Vocational Assessment, Personalized Worker Training and Rehabilitation.\n\nPI: Fillia Makedon\n\nAwardee: University of Texas at Arlington\n\nAward Number: 1719031\n\nAward Ends: 08/31/2023\nMotivation and objectives\n\nThe objective of this research is to develop iWork, a smart robot-based vocational assessment and intervention service system to assess physical, cognitive and robot-collaboration skills of a worker while she/he performs simulated manufacturing tasks. To achieve this, multimodal human-robot interaction data are collected and analyzed. This is a modular closed loop data flow system withfour phases, assessment, recommendation, intervention, and evaluation, with a human-factors expert in the loop.\nSummary of Activities\nA testbed system was designed from the collected multisensing data while the user performed certain tasks. Manufacturing simulation tasks were used to provide vocational assessment and intervention service that can assess physical, cognitive and collaboration skills of a potential worker. Designed algorithms to collect multisensing data while the user performs simulated workplace tasks. Advanced machine learning algorithms were designed and used to analyze and evaluate the collected multisensing human-robot collaboration data. Experiments were designed using assistive robots to train the user and to assess work habits such as, work quality, cooperativeness, social skills and personal presentation as they relate to job preparation, engagement and retention. Different engineering methods were explored to provide workplace assessments. New software products and user interfaces were developed.\nProject Outcomes and Findings\nNew methods were developed to assess cognitive and physical fatigue of users engaged in simulated vocational tasks. A multisensory system was developed to monitor and assess the performance of veterans with breathing/dyspnea issues. A smart wearable shirt called PNEUMON was developed to monitor persons with dyspnea issues that can cause fatigue and other problems relevant to vocational performance. An assistive robot called MINA was developed to explore gait issues of persons in a clinical setting and to assess the ability of working nurses to use such an assistive robot effectively. The ATEC system (Activate Test for Embodied Cognition) was developed to assess executive function capabilities of children and young adults through embodied cognition tasks/games/exercises, using metrics that relate to performance in a vocational setting. The project resulted in the graduation of 5 Ph.D. students, the graduation of 4 REU students, and one MS thesis student. They received competitive positions in industry and government, based on the work they did on the project (General Motors, SalesForce, Hewlett Packard and FDA). Additionally, 6 REU students were trained and mentored. The project was used to mentor a female postdoc for 2 years, who then became assistant professor at the University of Santa Clara, CA. The PI?S team received Best Technical Paper award on a paper describing project results and also best poster paper award at the 2021 PETRA conference. The project team applied for a patent for the analysis of multisensory data related to vocational simulation experiments. Project researchers published many papers in peer review conferences and journals. User interfaces were developed for the PNEUMON dyspnea system, for the ATEC assessment system and for monitoring the Human Robot Interaction with the MINA robotic assistant. \n\nIntellectual Merit: \n\nThis is an interdisciplinary project that integrated three areas of expertise, (1) engineered service system design, where socially and rehabilitation assistive robots interact with and train the user to collaborate. 2) Computing, sensing, and information technologies, where machine learning, data mining and recommender algorithms are used to identify patterns of interest, (e.g., errors, delays, etc.) and recommend targeted interventions for the user. (3) Human Factors and cognitive engineering that deploy methods from the team?s Intelligent Interactive Learning and Adaptation Framework.  Expertise in workplace assessment, personalized psychiatric intervention, and evaluation methods of vocational satisfaction, guided experiments on Work Habits, Work Quality, Cooperativeness, Personal Presentation and Social (interpersonal) Skills, as they relate to job preparation and retention. Innovative new methods were developed for human-robot collaboration and training. \n\n Broader Impacts:\n\nThe iWork vocational assessment system produces personalized vocational training solutions that have great economic and societal impact and affect economic sectors such as, energy, transportation, mining, electronics, robotics, and others.  System outcomes impact traditional disciplines such as education, social sciences, psychology, computer science, rehabilitation, robotics and others. Additionally, project outcomes potentially impact millions of persons seeking a manufacturing job, including those facing a type of learning or aging disability. The system?s mobile, low-cost methods can accelerate recognizing a worker?s ability, improve correlations between cognitive and physical assessments and elevate traditional Vocational Rehabilitation practices to new levels of efficient, user-centric training. Furthermore, the project?s robot-based training emphasis on safety and risk assessment, can reduce liability costs and productivity setbacks due to manufacturing accidents. The project?s educational plan engaged student programmers and interns throughout the system?s development, organized industrial internships and encouraged industry-university collaborations through the iPerform IUCRC NSF center. \n\n \n\n\t\t\t\t\tLast Modified: 10/07/2023\n\n\t\t\t\t\tSubmitted by: Fillia S Makedon"
 }
}