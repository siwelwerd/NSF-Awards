{
 "awd_id": "1712554",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Statistical Inference Using Random Forests and Related Methods",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 215078.0,
 "awd_amount": 215078.0,
 "awd_min_amd_letter_date": "2017-07-19",
 "awd_max_amd_letter_date": "2017-07-19",
 "awd_abstract_narration": "This project seeks to develop methods to quantify uncertainty in machine learning algorithms and \r\nto incorporate machine learning and statistical inference. Machine learning has been enormously \r\nsuccessful at using data to make predictions; it is used in an extensive range of applications \r\nfrom handwriting recognition to high frequency trading to driverless cars and personalized medicine. \r\nHowever, while machine learning algorithms make good predictions, they tell humans very little \r\nabout how those predictions were arrived at: What were the important factors? How did they affect \r\nthe prediction? They also don't distinguish predictions for which there is a lot of information \r\nabout the probability of different outcomes (even if that covers a wide range) from those where \r\nvery little information is available. For example, a machine learning algorithm may very accurately \r\npredict whether a person is likely to develop diabetes, but provides little if any information \r\nregarding how that person might lower his or her risk. This project will build on initial \r\nmathematical theory to develop methods to explain how Random Forests arrive at their predictions \r\nand how statistically confident those predictions are, and produce ways to link machine learning \r\nmethods to other statistical models.\r\n\r\nThis project seeks to develop methods to quantify uncertainty in machine learning algorithms \r\nand to incorporate machine learning and statistical inference. The project will extend on a \r\ntheoretical framework representing Random Forests as U-statistics to produce a practical \r\nimplementation of statistical uncertainty quantification in machine learning. In particular, \r\nit will improve on methods to estimate sample variability in Random Forest predictions, develop \r\ncomputationally efficient screening tools for covariate and interaction selection, and incorporate \r\nensemble methods as non-parametric terms in partially-linear models while retaining statistical \r\ninference via a modified boosting algorithm. These methods will be demonstrated on a citizen \r\nscience data base in ornithology and in various biomedical applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Giles",
   "pi_last_name": "Hooker",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Giles Hooker",
   "pi_email_addr": "ghooker@wharton.upenn.edu",
   "nsf_id": "000489242",
   "pi_start_date": "2017-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "1186 Comstock Hall, BSCB",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148532601",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "118200",
   "pgm_ele_name": "POP & COMMUNITY ECOL PROG"
  },
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  },
  {
   "pgm_ele_code": "745400",
   "pgm_ele_name": "MSPA-INTERDISCIPLINARY"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8007",
   "pgm_ref_txt": "BioMaPS"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 215078.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focussed on developing inferential tools for random forests and related methods. Over the past two decades, random forests have become a workhorse of machine learning: exhibiting strong predictive ability for a very general set of problems. Because of their predictive success, and their ability to \"just work\" with a wide variety of data sets, random forests have been used as a default baseline, for example in the kaggle.com competition website. <br /><br />Despite this success, we have only recently started to provide the sort of uncertainty quantification that usually accompanies statistical models, expressed roughly as \"If we repeated the analysis on new data, how different might our answers be?\"&nbsp; This project builds upon recent theoretical developments to both improve our understanding of random forests and our ability to use them to gain knowledge from data. In particular, we have<br /><br />1. Developed improved estimates of the variability of random forests that are more precise at lower computational cost. <br /><br />2. Developed uncertainty quantification tools for boosting methods in which models are built successively, the most recent model correcting the errors of the current ensemble. <br /><br />3. Investigated the stability of commonly used methods to explain the predictions of machine learning functions and demonstrated that these are often fragile; potentially providing different answers when asked to explain the same prediction. Very large computational effort is needed to stabilize them. <br /><br />4. Elucidated more features of the behavior of random forests. In particular, providing additional columns of noise to random forests can paradoxically improve their ability to predict. This calls into question a number of proposals to test for the significance of features. <br /><br />Together, these move us a long way towards providing valid statistical uncertainty quantification using random forests and other ensemble methods, allowing the incorporation of very powerful machine learning tools into more classical statistical models.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/07/2021<br>\n\t\t\t\t\tModified by: Giles&nbsp;Hooker</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project focussed on developing inferential tools for random forests and related methods. Over the past two decades, random forests have become a workhorse of machine learning: exhibiting strong predictive ability for a very general set of problems. Because of their predictive success, and their ability to \"just work\" with a wide variety of data sets, random forests have been used as a default baseline, for example in the kaggle.com competition website. \n\nDespite this success, we have only recently started to provide the sort of uncertainty quantification that usually accompanies statistical models, expressed roughly as \"If we repeated the analysis on new data, how different might our answers be?\"  This project builds upon recent theoretical developments to both improve our understanding of random forests and our ability to use them to gain knowledge from data. In particular, we have\n\n1. Developed improved estimates of the variability of random forests that are more precise at lower computational cost. \n\n2. Developed uncertainty quantification tools for boosting methods in which models are built successively, the most recent model correcting the errors of the current ensemble. \n\n3. Investigated the stability of commonly used methods to explain the predictions of machine learning functions and demonstrated that these are often fragile; potentially providing different answers when asked to explain the same prediction. Very large computational effort is needed to stabilize them. \n\n4. Elucidated more features of the behavior of random forests. In particular, providing additional columns of noise to random forests can paradoxically improve their ability to predict. This calls into question a number of proposals to test for the significance of features. \n\nTogether, these move us a long way towards providing valid statistical uncertainty quantification using random forests and other ensemble methods, allowing the incorporation of very powerful machine learning tools into more classical statistical models.\n\n\t\t\t\t\tLast Modified: 10/07/2021\n\n\t\t\t\t\tSubmitted by: Giles Hooker"
 }
}