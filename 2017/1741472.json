{
 "awd_id": "1741472",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Audio-Visual Scene Understanding",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 650000.0,
 "awd_amount": 666000.0,
 "awd_min_amd_letter_date": "2017-08-24",
 "awd_max_amd_letter_date": "2019-05-08",
 "awd_abstract_narration": "Understanding scenes around us, i.e., recognizing objects, human actions and events, and inferring their spatial, temporal, correlative and causal relations, is a fundamental capability in human intelligence. Similarly, designing computer algorithms that can understand scenes is a fundamental problem in artificial intelligence. Humans consciously or unconsciously use all five senses (vision, audition, taste, smell, and touch) to understand a scene, as different senses provide complimentary information. For example, watching a movie with the sound muted makes it very difficult to understand the movie; walking on a street with eyes closed without other guidance can be dangerous. Existing machine scene understanding algorithms, however, are designed to rely on just a single modality. Take the two most commonly used senses, vision and audition, as an example, there are scene understanding algorithms designed to deal with each single modality. However, no systematic investigations have been conducted to integrate these two modalities towards more comprehensive audio-visual scene understanding. Designing algorithms that jointly model audio and visual modalities towards a complete audio-visual scene understanding is important, not only because this is how humans understand scenes, but also because it will enable novel applications in many fields. These fields include multimedia (video indexing and scene editing), healthcare (assistive devices for visually and aurally impaired people), surveillance security (comprehensive monitoring of the suspicious activities), and virtual and augmented reality (generation and alternation of visuals and/or sound tracks). In addition, the investigators will involve graduate and undergraduate students in the research activities, integrate research results into the teaching curriculum, and conduct outreach activities to local schools and communities with an aim to broader participation in computer science. \r\n\r\nThis project aims to achieve human-like audio-visual scene understanding that overcomes the limitations of single-modality approaches through big data analysis of Internet videos. The core idea is to learn to parse a scene into elements and infer their relations, i.e., forming an audio-visual scene graph. Specifically, an element of the audio-visual scene can be a joint audio-visual component of an event when the event shows correlated audio and visual features. It can also be an audio component or a visual component if the event only appears in one modality. The relations between the elements include spatial and temporal relations at a lower level, as well as correlative and causal relations at a higher level. Through this scene graph, information across the two modalities can be extracted, exchanged and interpreted. The investigators propose three main research thrusts: (1) Learning joint audio-visual representations of scene elements; (2) Learning a scene graph to organize scene elements; and (3) Cross-modality scene completion. Each of the three research thrusts explores a dimension in the space of audio-visual scene understanding, yet they are also inter-connected. For example, the audio-visual scene elements are nodes in the scene graph, and the scene graph, in turn, guides the learning of relations among scene elements with structured information; the cross-modality scene completion generates missing data in the scene graph and is necessary for good audio-visual understanding of the scene. Expected outcomes of this proposal include: a software package for learning joint audio-visual representations of various scene elements; a web-deployed system for audio-visual scene understanding utilizing the learned scene elements and scene graphs, illustrated with text generation; a software package for cross-modality scene completion based on scene understanding; and a large-scale video dataset with annotations for audio-visual association, text generation and scene completion. Datasets, software and demos will be hosted on the project website.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chenliang",
   "pi_last_name": "Xu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chenliang Xu",
   "pi_email_addr": "chenliang.xu@rochester.edu",
   "nsf_id": "000728261",
   "pi_start_date": "2017-08-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Zhiyao",
   "pi_last_name": "Duan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhiyao Duan",
   "pi_email_addr": "zhiyao.duan@rochester.edu",
   "nsf_id": "000662423",
   "pi_start_date": "2017-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146270140",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 650000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-3ecdfe42-7fff-b9be-091d-d1f7bf292afb\"> </span></p>\n<p dir=\"ltr\"><span>Designing computer algorithms that can understand audio-visual scenes is a fundamental problem in artificial intelligence. Humans consciously or unconsciously use multiple senses to understand a scene, as different senses provide complementary information. Existing machine scene understanding algorithms, however, were designed to rely on just a single modality. No systematic investigations had been conducted to integrate the two predominant modalities---audio and visual---toward a more comprehensive audio-visual scene understanding.</span></p>\n<p dir=\"ltr\"><span>This project aimed to achieve human-like audio-visual scene understanding that overcomes the limitations of single-modality approaches through big data analysis of Internet videos.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We developed technologies for understanding generic web images/videos, including&nbsp;<br /> </span></p>\n<ul>\n<li>Algorithms for audio-visual event localization and audio-visual video parsing&nbsp;</li>\n<li>Algorithms for studying explainability and robustness in audio-visual video understanding</li>\n<li>Algorithms for sounding object visual grounding and visually-indicated sound separation to build audio-visual scene elements</li>\n<li>Algorithms for scene graph generation from weak, natural language supervision&nbsp;</li>\n<li>Algorithms for answering questions in dynamic audio-visual scenarios.&nbsp;</li>\n</ul>\n<p dir=\"ltr\"><span>We developed technologies for analyzing music and speech audios/videos, including&nbsp;<br /> </span></p>\n<ul>\n<li>Algorithms for audio-visual speech separation and audio-visual singing voice separation</li>\n<li>Algorithms for audio-visual active speaker detection and audio-visual speech diarization&nbsp;</li>\n<li>Algorithms for audio-visual music performance analysis, including vibrato analysis and source association</li>\n<li>Algorithms for audio signal analysis, including source separation, speaker embedding, speaker verification, and anti-spoofing.&nbsp;</li>\n</ul>\n<ul>\n</ul>\n<p dir=\"ltr\"><span>Furthermore, we developed technologies for cross-modal audio-visual generation, including&nbsp;<br /> </span></p>\n<ul>\n<li>Algorithms for speech-driven talking lip/face/head generation, including lip movement generation, face landmark generation, face video generation, emotional talking face generation, and rhythmic talking head generation</li>\n<li>Algorithms for music-driven pianist skeleton generation.&nbsp;</li>\n</ul>\n<p dir=\"ltr\"><span>We created many useful datasets and benchmarks to facilitate future research, including&nbsp;<br /> </span></p>\n<ul>\n<li>An audio-visual event dataset named AVE (over 4,000 videos)</li>\n<li>An audio-visual video parsing dataset named LLP (over 11,000 videos totaling 32.9 video hours)</li>\n<li>An audio-visual question-answering dataset named MUSIC-AVQA (over 9,000 videos totaling over 150 hours and over 45,000 question-answer pairs)</li>\n<li>An in-house recorded audio-visual music performance dataset of 44 multi-track music pieces covering 14 kinds of instruments with isolated source recordings named URMP&nbsp;</li>\n<li>An in-house recorded audio-visual singing dataset of 65 songs with isolated singing recordings named URSing.</li>\n</ul>\n<p dir=\"ltr\"><span>The project graduated four Ph.D. students and provided training opportunities to a dozen more graduate and undergraduate students, among whom nearly half were women and URM students. We have provided educational opportunities to more than two dozen high schoolers via established outreach programs by the University of Rochester (UR)'s David T. Kearns Center. The results of this project have been featured as permanent course modules in both PIs' courses and presented as a half-day tutorial at ISMIR 2019, a half-day tutorial at WACV 2021, and a full-day tutorial at CVPR 2021. </span><span>Through this project, we established collaborations with researchers in different disciplines including computer science, electrical engineering, neural science, biomedical engineering, healthcare, and music.</span></p>\n<p dir=\"ltr\"><span>This research is intellectually transformative as it ties computer vision and computer audition and advances both that were developed independently. The methodologies and techniques developed in learning audio-visual bimodal representations can be applied to multi-modal modeling with data from various sources such as tactile perception, GPS, radar, and lasers, leading to a coherent robot perception system. Furthermore, learning from the vast amount of Internet videos requires scalable models and efficient optimization techniques.</span></p>\n<p dir=\"ltr\"><span>This work has a broad impact beyond the above-mentioned educational opportunities because it could also enable many applications that can greatly benefit society, including multimedia (video indexing and scene editing), healthcare (assistive devices for visually and aurally impaired people), surveillance security (comprehensive monitoring of the suspicious activities), and virtual and augmented reality (generation and alternation of visuals or soundtracks).</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/29/2022<br>\n\t\t\t\t\tModified by: Chenliang&nbsp;Xu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nDesigning computer algorithms that can understand audio-visual scenes is a fundamental problem in artificial intelligence. Humans consciously or unconsciously use multiple senses to understand a scene, as different senses provide complementary information. Existing machine scene understanding algorithms, however, were designed to rely on just a single modality. No systematic investigations had been conducted to integrate the two predominant modalities---audio and visual---toward a more comprehensive audio-visual scene understanding.\nThis project aimed to achieve human-like audio-visual scene understanding that overcomes the limitations of single-modality approaches through big data analysis of Internet videos. \nWe developed technologies for understanding generic web images/videos, including \n \n\nAlgorithms for audio-visual event localization and audio-visual video parsing \nAlgorithms for studying explainability and robustness in audio-visual video understanding\nAlgorithms for sounding object visual grounding and visually-indicated sound separation to build audio-visual scene elements\nAlgorithms for scene graph generation from weak, natural language supervision \nAlgorithms for answering questions in dynamic audio-visual scenarios. \n\nWe developed technologies for analyzing music and speech audios/videos, including \n \n\nAlgorithms for audio-visual speech separation and audio-visual singing voice separation\nAlgorithms for audio-visual active speaker detection and audio-visual speech diarization \nAlgorithms for audio-visual music performance analysis, including vibrato analysis and source association\nAlgorithms for audio signal analysis, including source separation, speaker embedding, speaker verification, and anti-spoofing. \n\n\n\nFurthermore, we developed technologies for cross-modal audio-visual generation, including \n \n\nAlgorithms for speech-driven talking lip/face/head generation, including lip movement generation, face landmark generation, face video generation, emotional talking face generation, and rhythmic talking head generation\nAlgorithms for music-driven pianist skeleton generation. \n\nWe created many useful datasets and benchmarks to facilitate future research, including \n \n\nAn audio-visual event dataset named AVE (over 4,000 videos)\nAn audio-visual video parsing dataset named LLP (over 11,000 videos totaling 32.9 video hours)\nAn audio-visual question-answering dataset named MUSIC-AVQA (over 9,000 videos totaling over 150 hours and over 45,000 question-answer pairs)\nAn in-house recorded audio-visual music performance dataset of 44 multi-track music pieces covering 14 kinds of instruments with isolated source recordings named URMP \nAn in-house recorded audio-visual singing dataset of 65 songs with isolated singing recordings named URSing.\n\nThe project graduated four Ph.D. students and provided training opportunities to a dozen more graduate and undergraduate students, among whom nearly half were women and URM students. We have provided educational opportunities to more than two dozen high schoolers via established outreach programs by the University of Rochester (UR)'s David T. Kearns Center. The results of this project have been featured as permanent course modules in both PIs' courses and presented as a half-day tutorial at ISMIR 2019, a half-day tutorial at WACV 2021, and a full-day tutorial at CVPR 2021. Through this project, we established collaborations with researchers in different disciplines including computer science, electrical engineering, neural science, biomedical engineering, healthcare, and music.\nThis research is intellectually transformative as it ties computer vision and computer audition and advances both that were developed independently. The methodologies and techniques developed in learning audio-visual bimodal representations can be applied to multi-modal modeling with data from various sources such as tactile perception, GPS, radar, and lasers, leading to a coherent robot perception system. Furthermore, learning from the vast amount of Internet videos requires scalable models and efficient optimization techniques.\nThis work has a broad impact beyond the above-mentioned educational opportunities because it could also enable many applications that can greatly benefit society, including multimedia (video indexing and scene editing), healthcare (assistive devices for visually and aurally impaired people), surveillance security (comprehensive monitoring of the suspicious activities), and virtual and augmented reality (generation and alternation of visuals or soundtracks).\n\n\t\t\t\t\tLast Modified: 12/29/2022\n\n\t\t\t\t\tSubmitted by: Chenliang Xu"
 }
}