{
 "awd_id": "1734938",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NCS-FO: Neuroimaging to Advance Computer Vision, NLP, and AI",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2017-08-15",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 1000000.0,
 "awd_amount": 1000000.0,
 "awd_min_amd_letter_date": "2017-08-07",
 "awd_max_amd_letter_date": "2022-06-30",
 "awd_abstract_narration": "It is often said that a picture is worth a thousand words. Frequently, to search for what is needed, whether images or objects in those images, words are needed instead. Getting accurate labels for efficient searches is a longstanding goal of computer vision, but progress has been slow. This project employs new methods to significantly change how picture-word labeling is accomplished by taking advantage of the best picture recognizer available, the human brain. Through functional magnetic resonance imaging and electroencephalography, brain activity of humans looking at pictures/videos is recorded and then used to improve performance on artificial intelligence (AI) tasks involving computer vision and natural language processing. Current systems use machine learning to train computers to recognize objects (nouns) and activities (verbs) in images/video, which are then used to describe events. Reasoning tasks (e.g., solving math problems) can then be done. These systems are trained on specially prepared datasets with samples of nouns for objects, verbs for activities, sentences describing events, and exam questions and answers. A novel paradigm using humans to perform the same tasks while their brains are scanned allows determination of neural patterns associated with those tasks. The brain activity patterns, in turn, are used to train better computer systems.\r\n\r\nThe central hypothesis is that understanding human processing of grounded language involving predication and its use during reasoning will materially improve engineered computer vision, natural language processing, and AI systems that perform image/video captioning, visual question answering, and problem solving.  Scientific and engineering goals include developing models of human language grounding and reasoning consistent with neuroimaging, to improve engineered systems integrating language and vision that support automated reasoning.  The main scientific question is to understand mechanisms by which predicates and arguments are identified, linked, and used for reasoning by the human brain.  The hypothesis, that predicate-argument linking in visual and linguistic representations are accomplished similarly, and that this then supports reasoning and problem solving, will be tested using multiple neuroimaging modalities, and machine learning algorithms to decode \"who did what to whom\" from brain scans of subjects processing linguistic and visual stimuli.  The iterative approach will involve understanding information integration at the neural level, to improve machine learning performance on AI tasks by training computers to perform increasingly complex tasks with neuroimaging data from stimuli derived from large-scale natural tasks.  Using identical datasets for human and machine performance will support translation of scientific advances to engineering practice involving integration of computer vision and natural language processing.\r\n\r\nThis award is cofunded by the Office of International Science and Engineering.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Siskind",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey M Siskind",
   "pi_email_addr": "qobi@purdue.edu",
   "nsf_id": "000169494",
   "pi_start_date": "2017-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ronnie",
   "pi_last_name": "Wilbur",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Ronnie B Wilbur",
   "pi_email_addr": "wilbur@purdue.edu",
   "nsf_id": "000163081",
   "pi_start_date": "2017-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Evguenia",
   "pi_last_name": "Malaia",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Evguenia Malaia",
   "pi_email_addr": "eamalaia@ua.edu",
   "nsf_id": "000595605",
   "pi_start_date": "2017-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "465 Northwestern Ave.",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072035",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "054Y00",
   "pgm_ele_name": "GVF - Global Venture Fund"
  },
  {
   "pgm_ele_code": "798000",
   "pgm_ele_name": "ECR-EDU Core Research"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5946",
   "pgm_ref_txt": "UNITED KINGDOM"
  },
  {
   "pgm_ref_code": "5980",
   "pgm_ref_txt": "WESTERN EUROPE PROGRAM"
  },
  {
   "pgm_ref_code": "6869",
   "pgm_ref_txt": "US/IRELAND R & D"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "8551",
   "pgm_ref_txt": "IntgStrat Undst Neurl&Cogn Sys"
  },
  {
   "pgm_ref_code": "8817",
   "pgm_ref_txt": "STEM Learning & Learning Environments"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0417",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001718DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 1000000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We studied how the human brain processes and represents people and objects (nouns) and activity (verbs) across different modalities, including vision (video) and language (both spoken and written). We particularly studied how that brain activity allows forming compound concepts by combining simpler concepts like nouns and verbs into sentence structure (subject-verb-object). We developed methods for computer programs to decode both simple and compound concepts from neuroimaging data (EEG and fMRI) recorded from subjects viewing short video clips, hearing spoken presentation of sentences, and reading sentences, all depicting compound concepts. Our methods demonstrate two key findings: first, the pattern of brain activity recorded from the same concepts (stimuli) is similar, but not identical, across different people, and second, the pattern of brain activity recordied from the same concepts (stimuli) is similar, but not identical, across different modalities. The pattern of brain activity associated&nbsp;with combining simple concepts into compound concepts is also similar, but not identical, across different people and modalities.</p>\n<p>In another major study, we discovered flaws in the experimental method used to collect data in a large number of prominent published papers. This flaw means their conclusions and claims are not valid (and cannot be used). We published our findings is the same places so that others would be aware of the problem.</p><br>\n<p>\n Last Modified: 01/27/2024<br>\nModified by: Jeffrey&nbsp;M&nbsp;Siskind</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nWe studied how the human brain processes and represents people and objects (nouns) and activity (verbs) across different modalities, including vision (video) and language (both spoken and written). We particularly studied how that brain activity allows forming compound concepts by combining simpler concepts like nouns and verbs into sentence structure (subject-verb-object). We developed methods for computer programs to decode both simple and compound concepts from neuroimaging data (EEG and fMRI) recorded from subjects viewing short video clips, hearing spoken presentation of sentences, and reading sentences, all depicting compound concepts. Our methods demonstrate two key findings: first, the pattern of brain activity recorded from the same concepts (stimuli) is similar, but not identical, across different people, and second, the pattern of brain activity recordied from the same concepts (stimuli) is similar, but not identical, across different modalities. The pattern of brain activity associatedwith combining simple concepts into compound concepts is also similar, but not identical, across different people and modalities.\n\n\nIn another major study, we discovered flaws in the experimental method used to collect data in a large number of prominent published papers. This flaw means their conclusions and claims are not valid (and cannot be used). We published our findings is the same places so that others would be aware of the problem.\t\t\t\t\tLast Modified: 01/27/2024\n\n\t\t\t\t\tSubmitted by: JeffreyMSiskind\n"
 }
}