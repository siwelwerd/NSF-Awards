{
 "awd_id": "1737419",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Holistic Scene Understanding with Multiple Hypotheses from Vision Modules",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2017-02-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 435122.0,
 "awd_amount": 435122.0,
 "awd_min_amd_letter_date": "2017-04-05",
 "awd_max_amd_letter_date": "2018-06-12",
 "awd_abstract_narration": "This project develops algorithms and techniques for holistic scene understanding from images. The key barrier to building the next generation of vision systems is ambiguity. For example, a patch from an image may look like a face but may simply be an incidental arrangement of tree branches and shadows. Thus, a vision module operating in isolation often produces nonsensical results, such as hallucinating faces floating in thin air. This project develops a visual system that jointly reasons about multiple plausible hypotheses from different vision modules such as 3D scene layout, object layout, and pose estimation. The developed technologies have the potential to improve vision systems and make fundamental impact - from self-driving cars bringing mobility to the physically impaired, to unmanned aircrafts helping law enforcement with search and rescue in disasters. The project involves research tightly integrated with education and outreach to train the next generation of young scientists and researchers.  \r\n\r\nThis research addresses the fundamental challenge in joint reasoning by extracting and leveraging a small set of diverse plausible hypotheses or guesses from computer vision modules (e.g. a patch may be a {sky or a vertical surface} x {face or tree branches}). This project generates new knowledge and techniques for (1) generating a small set of diverse plausible hypotheses from different vision modules, (2) joint reasoning over all modules to pick a single hypothesis from each module, and (3) reducing human annotation effort by actively soliciting user feedback only on the small set of plausible hypotheses. \r\n\r\nProject Webpage: http://computing.ece.vt.edu/~dbatra",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dhruv",
   "pi_last_name": "Batra",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dhruv Batra",
   "pi_email_addr": "dbatra@vt.edu",
   "nsf_id": "000611210",
   "pi_start_date": "2017-04-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "Office of Sponsored Programs",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320420",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 28339.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 96350.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 99808.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 103425.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 107200.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main goal of this proposal is holistic scene understanding, i.e. inferring all properties of the scene (Who, what, where, doing what?) that produced this image.</p>\n<p>&nbsp;</p>\n<p>The key questions being considered are:<br />&mdash; how can vision systems hedge against ambiguity by jointly reasons about different vision &ldquo;modules&rdquo; or sub- components (3D scene layout, object layout, pose and activity recognition)?<br />&mdash; Idealized probabilistic models that reason about all variables of interest are typically computationally intractable even for a single module. Jointly reasoning about multiple modules simply seems out of reach due to the combinatorial explosion of search space (= {all-3D-layouts}&times;{all-segmentations}&times; . . . &times;{all-object-poses}). How can we overcome this combinatorial explosion of state space?</p>\n<p>&nbsp;</p>\n<p>Major Activities:</p>\n<p>1. To address these questions, we have developed a technique to extract and leverage a small set of diverse plausible hypotheses or guesses from vision modules. This technique (called DivMBest) leverages a line of work in the PI&rsquo;s lab for finding a diverse set of highly probable solutions under a discrete probabilistic model.<br />2. Moreover, we have been working on several multi&shy;modal AI problems at the intersection of vision, language, reasoning.</p>\n<p>&nbsp;</p>\n<p>Significant Results: <br />1. Resolving Language and Vision Ambiguities Together: Joint Segmentation &amp; Prepositional Attachment Resolution in Captioned Scenes.<br />In our EMNLP 2016 paper, we published the flagship result of this grant &mdash; that maintaining multiple diverse plausible hypotheses from a vision system (semantic segmentation) and from an NLP system (parse trees) can help improve joint segmentation and prepositional phrase attachment in captions. This paper has been very well received by the natural language community, and demonstrates the broad applicability of the ideas proposed in this grant (vision + language + AI).</p>\n<p>2. Visual Question Answering<br />In collaboration with Prof. Devi Parikh and her lab, PI Batra and his lab (partially supported by this grant) have undertaken an ambitious large&shy;scale project called Visual Question Answering (VQA). Given an image and a natural language question about the image (&ldquo;What kind of store is this?&rdquo;, &ldquo;How many people are waiting in the queue?&rdquo;, &ldquo;Is it safe to cross the street?&rdquo;), the machine&rsquo;s task is to automatically produce a concise, accurate, natural language answer (&ldquo;bakery&rdquo;, &ldquo;5&rdquo;, &ldquo;Yes&rdquo;). Answering any possible question about an image is one of the &lsquo;holy grails&rsquo; of Computer Vision and Artificial Intelligence requiring vision, language, and reasoning. In a limited sense, this task is the visual analogue of the famous Turing Test.</p>\n<p>Scientific Impact: <br />Our work on Visual Question Answering has been *highly impactful*. In a short period of time since our first paper (ICCV &rsquo;15) on the topic, this work has already attracted significant interest by the machine learning, vision, and language research communities, and created an entirely new AI sub-field &ndash; of visually- grounded dialog with AI agents. The ICCV&rsquo;15 paper has been cited over 2000 times, with thousands of downloads of the dataset, and thousands of submissions to the annual challenge (running since 2016). The project website (visualqa.org) routinely receives over 10k visits per month. There are over 200 papers on arXiv on the topic, and top-tier machine learning conferences (ICLR, NeurIPS) having VQA as a sub-topic that authors can pick when submitting their papers.</p>\n<p>&nbsp;</p>\n<p>Broader Impact:</p>\n<p>Improved vision systems have the potential to fundamentally change the way we live. One specific benefit is autonomous systems to improve the lives of visually and physically impaired. If the pedestrian detector (running on an autonomous car) fires, it could be due to a real person on the road or a picture of a person on a billboard. The two hypotheses lead to very different recommendations and it is important to consider both. Always focusing on just the worst-case scenario makes the system useless due to unacceptable false positives that prevent the car from ever being able to drive forward.</p>\n<p><br />Visual Question Answering and Dialog are directly applicable to a variety of applications of high societal impact that involve humans eliciting situationally-relevant information from visual data; where humans and machines must collaborate to extract information from pictures. Examples include aiding visually-impaired users in understanding their surroundings (&ldquo;What temperature is this oven set to?&rdquo;), analysts in making decisions based on large quantities of surveillance data (&ldquo;What kind of car did the man in the red shirt drive away in?&rdquo;), and interacting with a robot or autonomous system (&ldquo;Is my laptop in my bedroom upstairs?&rdquo;).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/18/2020<br>\n\t\t\t\t\tModified by: Dhruv&nbsp;Batra</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe main goal of this proposal is holistic scene understanding, i.e. inferring all properties of the scene (Who, what, where, doing what?) that produced this image.\n\n \n\nThe key questions being considered are:\n&mdash; how can vision systems hedge against ambiguity by jointly reasons about different vision \"modules\" or sub- components (3D scene layout, object layout, pose and activity recognition)?\n&mdash; Idealized probabilistic models that reason about all variables of interest are typically computationally intractable even for a single module. Jointly reasoning about multiple modules simply seems out of reach due to the combinatorial explosion of search space (= {all-3D-layouts}&times;{all-segmentations}&times; . . . &times;{all-object-poses}). How can we overcome this combinatorial explosion of state space?\n\n \n\nMajor Activities:\n\n1. To address these questions, we have developed a technique to extract and leverage a small set of diverse plausible hypotheses or guesses from vision modules. This technique (called DivMBest) leverages a line of work in the PI\u2019s lab for finding a diverse set of highly probable solutions under a discrete probabilistic model.\n2. Moreover, we have been working on several multi&shy;modal AI problems at the intersection of vision, language, reasoning.\n\n \n\nSignificant Results: \n1. Resolving Language and Vision Ambiguities Together: Joint Segmentation &amp; Prepositional Attachment Resolution in Captioned Scenes.\nIn our EMNLP 2016 paper, we published the flagship result of this grant &mdash; that maintaining multiple diverse plausible hypotheses from a vision system (semantic segmentation) and from an NLP system (parse trees) can help improve joint segmentation and prepositional phrase attachment in captions. This paper has been very well received by the natural language community, and demonstrates the broad applicability of the ideas proposed in this grant (vision + language + AI).\n\n2. Visual Question Answering\nIn collaboration with Prof. Devi Parikh and her lab, PI Batra and his lab (partially supported by this grant) have undertaken an ambitious large&shy;scale project called Visual Question Answering (VQA). Given an image and a natural language question about the image (\"What kind of store is this?\", \"How many people are waiting in the queue?\", \"Is it safe to cross the street?\"), the machine\u2019s task is to automatically produce a concise, accurate, natural language answer (\"bakery\", \"5\", \"Yes\"). Answering any possible question about an image is one of the \u2018holy grails\u2019 of Computer Vision and Artificial Intelligence requiring vision, language, and reasoning. In a limited sense, this task is the visual analogue of the famous Turing Test.\n\nScientific Impact: \nOur work on Visual Question Answering has been *highly impactful*. In a short period of time since our first paper (ICCV \u201915) on the topic, this work has already attracted significant interest by the machine learning, vision, and language research communities, and created an entirely new AI sub-field &ndash; of visually- grounded dialog with AI agents. The ICCV\u201915 paper has been cited over 2000 times, with thousands of downloads of the dataset, and thousands of submissions to the annual challenge (running since 2016). The project website (visualqa.org) routinely receives over 10k visits per month. There are over 200 papers on arXiv on the topic, and top-tier machine learning conferences (ICLR, NeurIPS) having VQA as a sub-topic that authors can pick when submitting their papers.\n\n \n\nBroader Impact:\n\nImproved vision systems have the potential to fundamentally change the way we live. One specific benefit is autonomous systems to improve the lives of visually and physically impaired. If the pedestrian detector (running on an autonomous car) fires, it could be due to a real person on the road or a picture of a person on a billboard. The two hypotheses lead to very different recommendations and it is important to consider both. Always focusing on just the worst-case scenario makes the system useless due to unacceptable false positives that prevent the car from ever being able to drive forward.\n\n\nVisual Question Answering and Dialog are directly applicable to a variety of applications of high societal impact that involve humans eliciting situationally-relevant information from visual data; where humans and machines must collaborate to extract information from pictures. Examples include aiding visually-impaired users in understanding their surroundings (\"What temperature is this oven set to?\"), analysts in making decisions based on large quantities of surveillance data (\"What kind of car did the man in the red shirt drive away in?\"), and interacting with a robot or autonomous system (\"Is my laptop in my bedroom upstairs?\").\n\n\t\t\t\t\tLast Modified: 09/18/2020\n\n\t\t\t\t\tSubmitted by: Dhruv Batra"
 }
}