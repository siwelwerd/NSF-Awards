{
 "awd_id": "1652950",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Blending Deep Reinforcement Learning and Probabilistic Programming",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2017-03-01",
 "awd_exp_date": "2024-02-29",
 "tot_intn_awd_amt": 509715.0,
 "awd_amount": 525715.0,
 "awd_min_amd_letter_date": "2017-02-28",
 "awd_max_amd_letter_date": "2021-03-22",
 "awd_abstract_narration": "In reinforcement learning (RL), autonomous agents (such as disaster recovery robots, self-driving cars or unmanned aerial vehicles) must simultaneously learn about an unknown environment while acting in that environment.  Deep reinforcement learning is a variant of RL that leverages the power of deep neural networks (DNNs) to learn both to extract information from sensors and how to transform that information into optimal actions.  The combination of RL and deep learning has generated impressive advances, but it is expensive, both in terms of data and in terms of computation: typical agents only learn after tens or hundreds of millions of interactions with an environment, making most algorithms unusable in anything but a fast simulator.  This stands in stark contrast to humans attempting the same tasks, who can perform well after only a few minutes of practice (or even merely watching another human practice for a few minutes).\r\n \r\nBy its nature, this type of machine learning research builds tools for others to use. To connect these tools with disciplines outside of theoretical computer science and to improve outreach and education, this work integrates a new student exchange program, intended to both export results and to import technical challenges from other fields. The plan is rounded out with a mix of undergraduate research opportunities, competitions, and project-oriented classes focused on real-world systems and data -- all intended to spark excitement and communicate a hope for a better world through improved technology.\r\n\r\nThis work seeks to improve deep RL by addressing two fundamental issues: first, how to reduce the amount of data and computation needed by deep RL, and second, how to improve deep RL's ability to solve complex tasks by incorporating model-based prior knowledge.  The technical strategy builds on ideas from cognitive science, mimicing three key human cognitive capabilities lacking in current deep RL algorithms: (1) humans' native ability to build models of the world, which allows them to (2) transfer knowledge from previous experience via abstraction, and (3) reason explicitly about their own uncertainty.\r\n \r\nTo accomplish this, this work combines the strengths of two frameworks: deep neural networks and Bayesian models.  The DNNs provide low-level signal processing, flexible and learnable model components, and powerful building blocks for discriminative inference, while the Bayesian models provide high-level reasoning about objects, causality, and theory of mind in a coherent probabilistic framework that can deal explicitly with uncertainty.  These capabilities are delivered by improved probabilistic programming frameworks that enable both the necessary models and algorithms: probabilistic programming permits the construction complex probabilistic models, and provides natural opportunities for integration with DNNs through automated inference compilers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Wingate",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Wingate",
   "pi_email_addr": "wingated@cs.byu.edu",
   "nsf_id": "000700514",
   "pi_start_date": "2017-02-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brigham Young University",
  "inst_street_address": "A-153 ASB",
  "inst_street_address_2": "",
  "inst_city_name": "PROVO",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8014223360",
  "inst_zip_code": "846021128",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "UT03",
  "org_lgl_bus_name": "BRIGHAM YOUNG UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JWSYC7RUMJD1"
 },
 "perf_inst": {
  "perf_inst_name": "Brigham Young University",
  "perf_str_addr": "3332 TMCB",
  "perf_city_name": "Provo",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "846021231",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "UT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 115440.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 96512.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 99163.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 109894.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 104706.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Deep learning is a critical component of many of today's most successful AI systems. While it has enjoyed considerable success, it has a significant drawback: it requires tremendous amounts of data.&nbsp; This project aims to improve both the data efficiency of deep learning, as well as improve the representations learned in deep reinforcement learning, by combining deep learning with structured, hierarchical Bayesian models.&nbsp; Bayesian models allow researchers to specify rich priors over data, which can result in strong inductive biases that can learn quickly from limited data.</p>\n<p>This project explored several aspects of how deep learning and hierarchical Bayesian model could be combined, especially in the context of reinforcement learning. Some notable thrusts from the project include the development of the Holodeck (an RL simulator based on the Unreal Engine) and corresponding public code repository; the idea of \"policy autoencoders\"; an exploration of whether large language models could be combined with RL algorithms; using logical specifications to aid in RL; and the construction of sophisticated models that involve theory of mind (implemented as recursive probabilistic programs) for competing autonomous agents.</p>\n<p>In terms of broader impacts, this project supported disadvantaged students through a summer student swap program, and actively recruited and mentored women research assistants.</p>\n<p>This project supported 3 PhD students, 4 MS students, and 10 undergraduates.</p><br>\n<p>\n Last Modified: 07/15/2024<br>\nModified by: David&nbsp;Wingate</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nDeep learning is a critical component of many of today's most successful AI systems. While it has enjoyed considerable success, it has a significant drawback: it requires tremendous amounts of data. This project aims to improve both the data efficiency of deep learning, as well as improve the representations learned in deep reinforcement learning, by combining deep learning with structured, hierarchical Bayesian models. Bayesian models allow researchers to specify rich priors over data, which can result in strong inductive biases that can learn quickly from limited data.\n\n\nThis project explored several aspects of how deep learning and hierarchical Bayesian model could be combined, especially in the context of reinforcement learning. Some notable thrusts from the project include the development of the Holodeck (an RL simulator based on the Unreal Engine) and corresponding public code repository; the idea of \"policy autoencoders\"; an exploration of whether large language models could be combined with RL algorithms; using logical specifications to aid in RL; and the construction of sophisticated models that involve theory of mind (implemented as recursive probabilistic programs) for competing autonomous agents.\n\n\nIn terms of broader impacts, this project supported disadvantaged students through a summer student swap program, and actively recruited and mentored women research assistants.\n\n\nThis project supported 3 PhD students, 4 MS students, and 10 undergraduates.\t\t\t\t\tLast Modified: 07/15/2024\n\n\t\t\t\t\tSubmitted by: DavidWingate\n"
 }
}