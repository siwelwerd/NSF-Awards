{
 "awd_id": "1718637",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: The Impacts of Human Decision-Making on Security and Robustness of Interdependent Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2017-08-15",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 476037.0,
 "awd_amount": 492037.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2021-05-19",
 "awd_abstract_narration": "There is a substantial body of work in behavioral economics and psychology showing that people are only partially rational, and thus consistently deviate from classical economic theory. People's perceptions of risks, rewards, and losses can differ substantially from their true values, and these perceptions can have a significant impact on the investments made to protect the systems that the individuals are managing.  The objective of this research is to understand the decisions people make to protect their computer systems using realistic models of behavioral decision-making. The research encompasses formal theory to rigorously analyze and predict the outcomes that should be expected under alternative models of behavioral decision-making, and laboratory experiments with human subjects to evaluate the predictions made by the theory and to identify new behavioral models. The research will tackle two specific classes of problems. First, it will identify the impact of behavioral decision-making in settings where different components of a large interconnected cyber-physical system are owned by different stakeholders, each deciding how much to invest in securing their owned assets. Second, it will characterize how decision-makers choose among different security technologies, open source and public versus closed source and proprietary, based on their perceived risks and rewards. The research will lead to a more complete understanding of the vulnerabilities that arise in large-scale interconnected systems, and guide us to the design of more secure systems, with corresponding societal benefits. \r\n \r\nThis research systematically and rigorously characterizes the impact of behavioral deviations from optimal and unbounded rational choice in security settings. The work includes models of decision-making under risk and uncertainty, such as prospect theory, and how such models affect the behavior of agents who manage interdependent systems. The research brings together game-theoretic analysis to predict outcomes based on models of interacting humans and systems, computer security concepts to model how vulnerabilities are exploited and how attacks spread, and behavioral economics experiments to test the theoretical predictions and refine the models. The research is organized in two parts. The first part considers a class of interdependent security games on networks, where each player chooses security investments to protect nodes under her control; this work models applications such as multi-stakeholder SCADA systems. The research will encompass general formulations of attack probabilities, epidemic risks, attack graph models of system interdependencies, and the optimal design of networks to mitigate security vulnerabilities introduced by humans' decision-making.  The second part considers a general class of common-pool resource management games, whereby players choose to split their utilization among multiple resources, each of which provides a certain rate-of-return and has a certain probability of failure. This class of games represents conditions in which decision-makers must choose between different public and proprietary security technologies. The research will characterize the impacts of prospect-theoretic decision-making and how users react to incentives provided by the resource operators or vendors. In both parts of the work, the research will identify how Nash equilibrium security investments and resource utilizations are affected by skewed perceptions of risks and rewards. Both parts include controlled behavioral economics experiments using human subjects that will evaluate the theoretical predictions and potentially yield new models of decision-making.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shreyas",
   "pi_last_name": "Sundaram",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shreyas Sundaram",
   "pi_email_addr": "sundara2@purdue.edu",
   "nsf_id": "000682911",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Timothy",
   "pi_last_name": "Cason",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Timothy N Cason",
   "pi_email_addr": "cason@purdue.edu",
   "nsf_id": "000293627",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Saurabh",
   "pi_last_name": "Bagchi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Saurabh Bagchi",
   "pi_email_addr": "sbagchi@purdue.edu",
   "nsf_id": "000309372",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "465 Northwestern Ave",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072035",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "065Z",
   "pgm_ref_txt": "Human factors for security research"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 476037.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The security and robustness of large-scale interdependent and shared systems (such as computer networks, intelligent transportation systems, and the power grid) depend critically on the decisions made by the humans who manage those systems.&nbsp; Different portions of those large systems are typically managed by different stakeholders, each of whom is deciding how much to invest in protecting their subsystems based on their perceived risks and costs of being attacked.&nbsp; Decisions made by one stakeholder may also impact the decisions made by other stakeholders, due to the interdependencies in their subsystems. While game theory provides a mathematical framework to reason about the outcomes in such settings, traditional game-theoretic analysis predominantly treats the players as behaving according to classical models of decision-making. In contrast, there is a substantial body of work in behavioral economics showing that humans consistently deviate from these classical models. These deviations and misperceptions of risks can have significant implications for how the human stakeholders invest in protecting their resources.&nbsp; Thus, the objective of this project was to rigorously characterize the security impacts of such behavioral decision-making by the humans who manage interdependent and shared systems, bringing together mathematical analysis and laboratory experiments to better understand vulnerabilities in complex systems.&nbsp;</p>\n<p>We addressed this objective in several ways. First, we investigated a class of interdependent security games where each stakeholder is managing a subset of assets (or nodes) in a network.&nbsp; An attacker wishes to compromise certain nodes in the network, and must do so by utilizing a \"stepping stone\" attack, where they proceed through the network by compromising one node at a time.&nbsp; The stakeholders can invest in improving the security of their assets in order to reduce the probability of the attacker succeeding in attacking them.&nbsp; In this setting, we characterized the impacts of behavioral misperceptions of the probability of successful attack; specifically, we analyzed scenarios where the stakeholders overweight low probabilities and underweight high probabilities of attack (aligning with findings from behavioral and experimental economics). Our analysis showed that such behavioral probability weighting causes stakeholders to move their security investments away from critical portions of the network, thereby increasing the overall vulnerability of the system.&nbsp; We demonstrated this both theoretically and through experiments. Based on these findings, we formulated approaches for central regulators to incentivize stakeholders to overcome their behavioral biases and make better security investments. We also showed that our ideas can be applied to other settings where nodes in a network can invest to protect themselves against undesirable effects, including epidemics spreading through populations (where the investments correspond to vaccinations).</p>\n<p>In addition to the networked settings described above, we also studied common-pool resource games, where the set of stakeholders are investing in a set of shared resources, with the probability of successful attack on a resource depending on the total investment on that resource by all stakeholders. In this setting, we once again characterized the impacts of behavioral probability weighting.&nbsp; We showed that overweighting of low attack probabilities (and underweighting of high attack probabilities) causes the stakeholders to shift their investments to higher valued assets, leading to suboptimal outcomes. We considered settings where the stakeholders and attackers make their decisions simultaneously, as well as settings where the attackers can choose which resources to attack after observing the actions of the stakeholders.&nbsp; Finally, we showed how incentive schemes (such as taxes) can be used to guide stakeholders to make better decisions and overcome their behavioral biases.</p>\n<p>In total, our research has established tools, frameworks, and insights to better understand how to design secure systems, accounting for human biases in the operation and management of those systems.&nbsp; In addition to the scientific contributions described above, the project has trained two PhD students in computer security, control, and human decision-making, and has exposed multiple undergraduate students to research.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/14/2022<br>\n\t\t\t\t\tModified by: Shreyas&nbsp;Sundaram</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe security and robustness of large-scale interdependent and shared systems (such as computer networks, intelligent transportation systems, and the power grid) depend critically on the decisions made by the humans who manage those systems.  Different portions of those large systems are typically managed by different stakeholders, each of whom is deciding how much to invest in protecting their subsystems based on their perceived risks and costs of being attacked.  Decisions made by one stakeholder may also impact the decisions made by other stakeholders, due to the interdependencies in their subsystems. While game theory provides a mathematical framework to reason about the outcomes in such settings, traditional game-theoretic analysis predominantly treats the players as behaving according to classical models of decision-making. In contrast, there is a substantial body of work in behavioral economics showing that humans consistently deviate from these classical models. These deviations and misperceptions of risks can have significant implications for how the human stakeholders invest in protecting their resources.  Thus, the objective of this project was to rigorously characterize the security impacts of such behavioral decision-making by the humans who manage interdependent and shared systems, bringing together mathematical analysis and laboratory experiments to better understand vulnerabilities in complex systems. \n\nWe addressed this objective in several ways. First, we investigated a class of interdependent security games where each stakeholder is managing a subset of assets (or nodes) in a network.  An attacker wishes to compromise certain nodes in the network, and must do so by utilizing a \"stepping stone\" attack, where they proceed through the network by compromising one node at a time.  The stakeholders can invest in improving the security of their assets in order to reduce the probability of the attacker succeeding in attacking them.  In this setting, we characterized the impacts of behavioral misperceptions of the probability of successful attack; specifically, we analyzed scenarios where the stakeholders overweight low probabilities and underweight high probabilities of attack (aligning with findings from behavioral and experimental economics). Our analysis showed that such behavioral probability weighting causes stakeholders to move their security investments away from critical portions of the network, thereby increasing the overall vulnerability of the system.  We demonstrated this both theoretically and through experiments. Based on these findings, we formulated approaches for central regulators to incentivize stakeholders to overcome their behavioral biases and make better security investments. We also showed that our ideas can be applied to other settings where nodes in a network can invest to protect themselves against undesirable effects, including epidemics spreading through populations (where the investments correspond to vaccinations).\n\nIn addition to the networked settings described above, we also studied common-pool resource games, where the set of stakeholders are investing in a set of shared resources, with the probability of successful attack on a resource depending on the total investment on that resource by all stakeholders. In this setting, we once again characterized the impacts of behavioral probability weighting.  We showed that overweighting of low attack probabilities (and underweighting of high attack probabilities) causes the stakeholders to shift their investments to higher valued assets, leading to suboptimal outcomes. We considered settings where the stakeholders and attackers make their decisions simultaneously, as well as settings where the attackers can choose which resources to attack after observing the actions of the stakeholders.  Finally, we showed how incentive schemes (such as taxes) can be used to guide stakeholders to make better decisions and overcome their behavioral biases.\n\nIn total, our research has established tools, frameworks, and insights to better understand how to design secure systems, accounting for human biases in the operation and management of those systems.  In addition to the scientific contributions described above, the project has trained two PhD students in computer security, control, and human decision-making, and has exposed multiple undergraduate students to research.\n\n\t\t\t\t\tLast Modified: 11/14/2022\n\n\t\t\t\t\tSubmitted by: Shreyas Sundaram"
 }
}