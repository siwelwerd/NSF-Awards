{
 "awd_id": "1724237",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: FND: COLLAB: Learning Manipulation Skills Using Deep Reinforcement Learning with Domain Transfer",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 316000.0,
 "awd_min_amd_letter_date": "2017-08-16",
 "awd_max_amd_letter_date": "2019-06-25",
 "awd_abstract_narration": "This project develops new methods of using deep reinforcement learning to solve real world robotics problems. The project focuses on robotic manipulation tasks such as grasping, opening doors, helping out in the home, performing repairs aboard Navy ships, etc. The key operation in all of the above is the ability for the robot to reliably manipulate objects, parts, or tools with its hands in order to perform a task. The project leverages deep reinforcement learning: a new approach to robotic learning that is capable of learning both perceptual features and control policies simultaneously. This project could have important benefits for a variety of practical applications including: explosive ordnance disposal for our military, materials handling aboard Navy ships, dexterous robotic assistants for NASA astronauts in space, assistive technologies that could help seniors age in place longer, better capabilities for handling radioactive materials during nuclear cleanup, assistance for ergonomically challenging tasks in manufacturing, and general assistance in the office and the home.\r\n\r\nThis research investigates novel deep reinforcement learning approaches for robotic grasping and manipulation that work well in previously unseen, unstructured environments and compose end-to-end tasks from simpler sub-task controllers. The research is built on two main results from research team's recent work, the deep learning approach to grasping and domain adaptation methods for deep neural networks. The research is guided by the following three key ideas: 1) learning in simulation and then using domain transfer techniques to adapt the solutions to reality; 2) simplifying learning for visuomotor control by using planning to estimate the value function; and 3) using symbolic task and motion planning to perform end-to-end tasks by sequencing learned controllers and planned arm/hand motions. The research team performs extensive evaluations to ensure that the system is able to perform novel instances of a task, e.g., those in a context that the robot has not seen before.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kate",
   "pi_last_name": "Saenko",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kate Saenko",
   "pi_email_addr": "saenko@bu.edu",
   "nsf_id": "000601038",
   "pi_start_date": "2017-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151300",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 300000.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-c06ab02a-7fff-de68-25d9-2a4cea51159f\"> </span></p>\n<p dir=\"ltr\"><span>This project was a collaboration between robotics and computer vision researchers to create new machine learning approaches allowing robots to learn to act and manipulate objects. The goal was to create algorithms that enable robots to act or perform manipulation tasks autonomously and consistently over time, regardless of the environment, and transitioning smoothly from simulation to real situations. The project's ultimate goal was to develop controllers that enable robots to perform a variety of tasks robustly and generalize well across different application scenarios.</span></p>\n<p dir=\"ltr\"><span>The outcome of the project was a series of new techniques that improved the efficiency and robustness of reinforcement learning in various tasks. As one notable example, we introduced \"Hierarchical Reinforcement Learning with Hindsight\" and \"Hierarchical Actor-Critic\", where we developed a new approach to help robots learn tasks more efficiently using abstractions. We discovered that by teaching robots temporally abstract actions, which are a series of actions that can be used across multiple tasks, the robot can learn more quickly. For example, when applied to the pole balancing task, our approach learns pole balancing primitives that make it easier to solve the overall task. What's really exciting is that our method learns both an abstraction and a policy faster than the baseline method would be able to learn the task by itself. Normally, learning an abstraction slows down learning initially to make it easier later, but our new approach speeds up learning from the beginning.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We also introduced a key technique called \"Conditioning for Action Policy Smoothness (CAPS)\" to address the lack of smoothness in the actions learned by deep Reinforcement Learning (RL) policies. Lack of smoothness can result in poor control, high power consumption, and system wear. It is particularly pronounced in continuous control, and can be further accentuated by the domain gap when policies are learned in simulation and transferred to real robots. We focused on deep RL-based low-level attitude controllers for high-performance quadrotor drones, training controllers to follow a pilot's input. We showed that CAPS is an effective regularization technique that consistently improves the smoothness of the learned state-to-action mappings of neural network controllers. The technique was tested on a quadrotor drone and resulted in an almost 80% reduction in power consumption while consistently training flight-worthy controllers. The CAPS project website is available at http://ai.bu.edu/caps.</span></p>\n<p dir=\"ltr\"><span>Overall, this collaboration has led to new machine learning approaches that improve robots' ability to act autonomously and consistently over time, regardless of the environment. The project outcomes have the potential to improve the development of autonomous robots and impact many industries, including manufacturing, healthcare, and transportation.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/07/2023<br>\n\t\t\t\t\tModified by: Kate&nbsp;Saenko</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis project was a collaboration between robotics and computer vision researchers to create new machine learning approaches allowing robots to learn to act and manipulate objects. The goal was to create algorithms that enable robots to act or perform manipulation tasks autonomously and consistently over time, regardless of the environment, and transitioning smoothly from simulation to real situations. The project's ultimate goal was to develop controllers that enable robots to perform a variety of tasks robustly and generalize well across different application scenarios.\nThe outcome of the project was a series of new techniques that improved the efficiency and robustness of reinforcement learning in various tasks. As one notable example, we introduced \"Hierarchical Reinforcement Learning with Hindsight\" and \"Hierarchical Actor-Critic\", where we developed a new approach to help robots learn tasks more efficiently using abstractions. We discovered that by teaching robots temporally abstract actions, which are a series of actions that can be used across multiple tasks, the robot can learn more quickly. For example, when applied to the pole balancing task, our approach learns pole balancing primitives that make it easier to solve the overall task. What's really exciting is that our method learns both an abstraction and a policy faster than the baseline method would be able to learn the task by itself. Normally, learning an abstraction slows down learning initially to make it easier later, but our new approach speeds up learning from the beginning. \nWe also introduced a key technique called \"Conditioning for Action Policy Smoothness (CAPS)\" to address the lack of smoothness in the actions learned by deep Reinforcement Learning (RL) policies. Lack of smoothness can result in poor control, high power consumption, and system wear. It is particularly pronounced in continuous control, and can be further accentuated by the domain gap when policies are learned in simulation and transferred to real robots. We focused on deep RL-based low-level attitude controllers for high-performance quadrotor drones, training controllers to follow a pilot's input. We showed that CAPS is an effective regularization technique that consistently improves the smoothness of the learned state-to-action mappings of neural network controllers. The technique was tested on a quadrotor drone and resulted in an almost 80% reduction in power consumption while consistently training flight-worthy controllers. The CAPS project website is available at http://ai.bu.edu/caps.\nOverall, this collaboration has led to new machine learning approaches that improve robots' ability to act autonomously and consistently over time, regardless of the environment. The project outcomes have the potential to improve the development of autonomous robots and impact many industries, including manufacturing, healthcare, and transportation.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/07/2023\n\n\t\t\t\t\tSubmitted by: Kate Saenko"
 }
}