{
 "awd_id": "1704337",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium:  Collaborative Research: Understanding and Editing Visual Sentiments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 301873.0,
 "awd_amount": 301873.0,
 "awd_min_amd_letter_date": "2017-06-21",
 "awd_max_amd_letter_date": "2017-09-07",
 "awd_abstract_narration": "The project develops computer vision and pattern recognition technologies for visual sentiment understanding and visual sentiment editing. The interdisciplinary research team investigates the problem of understanding how images and video convey emotion. The project develops methods to infer, edit, and synthesize visual sentimental content in image/videos, addition to their semantic contents. The project applies developed technologies to reduce violence from multimedia materials for children, and negative psychological impacts from social media for posttraumatic stress disorder (PTSD) patients. The project integrates research and education by creating new interdisciplinary courses and training graduate students.  The project builds connection with the veteran academic resource center on the campus to help PTSD patients to recover from mental health problems. The research team also shares collected data with research communities.\r\n\r\nThis research develops visual sentiment understanding algorithms through joint extraction of sentiments and semantics, in order to advance the understanding of how semantic entities substantiate and carry sentiments at a fine-grained object or pixel level. Computer vision algorithms and psychometric assessment techniques are combined to automatically analyze visual and recognize sentiments and emotions from multimedia materials and social media contents posted and shared by veterans. The research also explores methods of visual sentiment editing to reduce violence from multimedia materials and social media contents. The research can help (1) to protect children from accessing violent multimedia materials, and (2) to provide appropriate social media contents for applications of automatically detecting violent contents from veteran-shared multimedia.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jiebo",
   "pi_last_name": "Luo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jiebo Luo",
   "pi_email_addr": "jluo@cs.rochester.edu",
   "nsf_id": "000601671",
   "pi_start_date": "2017-06-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146270140",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 301873.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The overall goal of the project aims to develop computer vision and pattern recognition technologies for visual sentiment understanding and visual sentiment editing. We first investigate the problem of understanding how images and video convey emotion. The project then develops methods to infer, edit, and synthesize visual sentimental content in image/videos, as well as their semantic contents. The overall Collaborative Medium&nbsp;project (with UCF as the Lead Institution) also intends to apply the developed technologies to reducing violence from multimedia materials for children and negative psychological impacts from social media for post-traumatic stress disorder (PTSD) patients.</p>\n<p>Intellectual Merit: This project advanced the frontiers of computer vision research in the following aspects: 1) we developed a suite of visual sentiment understanding algorithms through joint extraction of sentiments and semantics from visual or multimedia contents, in order to advance the understanding of how visual sentiments interact with the semantic entities that substantiate sentiments at a fine-grained pixel level; 2) we developed several image editing algorithms to manipulate or create image content with semantics-based guidence, 3) we developed computational methods to synthesize visual sentiments based on a desired sentiment of a reference image, while retaining the global scene structures of the original images; and 4) we developed algorithms to synthesize desired sentiments by first modifying emotion-bearing descriptions and then using the modified descriptions to guide image synthesis, based on desired sentiments of multiple reference images and semantic map decomposition at the object level, providing a more flexible way to edit visual sentiments.</p>\n<p>Broader Impacts: The research outcomes have been integrated into our broader impact efforts through new course material development and public outreach. Specifically, the broader impacts include 1) 25 related publications in opt conferences and journals, 2) 2 related public datasets for image setiment understanding and image sentiment transfer, 3) 1 specific workshop keynote and 1 related conference keynote, 4) 2 supported PhD dissertations, 5) research experience for 3 undergraduate and 2 master students, 5) new course materials for image sentiment analysis and generation; and 6) related presentations on the course      homepage, Blackboard, and/or slideshare.net, where both enrolled students      and the public can access the content freely.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/27/2021<br>\n\t\t\t\t\tModified by: Jiebo&nbsp;Luo</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638069144690_senti1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638069144690_senti1--rgov-800width.jpg\" title=\"Object-level Image Sentiment Transfer\"><img src=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638069144690_senti1--rgov-66x44.jpg\" alt=\"Object-level Image Sentiment Transfer\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The pipeline of the proposed image sentiment transfer  framework. Given an input image, object mask extraction is first performed to extract objects and the corresponding masks. Image captioning and semantic image segmentation are utilized to obtain comprehensive objects and high-quality masks.</div>\n<div class=\"imageCredit\">Jiebo Luo</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Jiebo&nbsp;Luo</div>\n<div class=\"imageTitle\">Object-level Image Sentiment Transfer</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638068931549_gst--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638068931549_gst--rgov-800width.jpg\" title=\"Global Image Sentiment Transfer\"><img src=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638068931549_gst--rgov-66x44.jpg\" alt=\"Global Image Sentiment Transfer\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Framework of the proposed global image sentiment transfer algorithm. Our method consists of two parts: a reference image retrieve algorithm and a global sentiment transfer approach based on the retrieved reference image.</div>\n<div class=\"imageCredit\">Jiebo Luo</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Jiebo&nbsp;Luo</div>\n<div class=\"imageTitle\">Global Image Sentiment Transfer</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638070840054_captioning--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638070840054_captioning--rgov-800width.jpg\" title=\"Image Captioning with Sentiment and Style\"><img src=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638070840054_captioning--rgov-66x44.jpg\" alt=\"Image Captioning with Sentiment and Style\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The framework of our stylized image captioning model. In the adaptive learning block, the style-related matrices in the reference model (yellow) are frozen. It is designed to lead the real style-factual LSTM (blue) to learn from factual information selectively.</div>\n<div class=\"imageCredit\">Jiebo Luo</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Jiebo&nbsp;Luo</div>\n<div class=\"imageTitle\">Image Captioning with Sentiment and Style</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638071189039_exguide--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638071189039_exguide--rgov-800width.jpg\" title=\"Example-Guided Scene Image Synthesis\"><img src=\"/por/images/Reports/POR/2021/1704337/1704337_10494773_1638071189039_exguide--rgov-66x44.jpg\" alt=\"Example-Guided Scene Image Synthesis\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Example-guided scene synthesis is performed between two retrieved scenes (rows 1,2) and two arbitrary scenes (rows 3,4). Columns 1 to 3 (blue) depict depict the target label maps, exemplar labelmaps and the associated images, respectively. Columns 4 to 8 in (red) depict different methods.</div>\n<div class=\"imageCredit\">Jiebo Luo</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Jiebo&nbsp;Luo</div>\n<div class=\"imageTitle\">Example-Guided Scene Image Synthesis</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe overall goal of the project aims to develop computer vision and pattern recognition technologies for visual sentiment understanding and visual sentiment editing. We first investigate the problem of understanding how images and video convey emotion. The project then develops methods to infer, edit, and synthesize visual sentimental content in image/videos, as well as their semantic contents. The overall Collaborative Medium project (with UCF as the Lead Institution) also intends to apply the developed technologies to reducing violence from multimedia materials for children and negative psychological impacts from social media for post-traumatic stress disorder (PTSD) patients.\n\nIntellectual Merit: This project advanced the frontiers of computer vision research in the following aspects: 1) we developed a suite of visual sentiment understanding algorithms through joint extraction of sentiments and semantics from visual or multimedia contents, in order to advance the understanding of how visual sentiments interact with the semantic entities that substantiate sentiments at a fine-grained pixel level; 2) we developed several image editing algorithms to manipulate or create image content with semantics-based guidence, 3) we developed computational methods to synthesize visual sentiments based on a desired sentiment of a reference image, while retaining the global scene structures of the original images; and 4) we developed algorithms to synthesize desired sentiments by first modifying emotion-bearing descriptions and then using the modified descriptions to guide image synthesis, based on desired sentiments of multiple reference images and semantic map decomposition at the object level, providing a more flexible way to edit visual sentiments.\n\nBroader Impacts: The research outcomes have been integrated into our broader impact efforts through new course material development and public outreach. Specifically, the broader impacts include 1) 25 related publications in opt conferences and journals, 2) 2 related public datasets for image setiment understanding and image sentiment transfer, 3) 1 specific workshop keynote and 1 related conference keynote, 4) 2 supported PhD dissertations, 5) research experience for 3 undergraduate and 2 master students, 5) new course materials for image sentiment analysis and generation; and 6) related presentations on the course      homepage, Blackboard, and/or slideshare.net, where both enrolled students      and the public can access the content freely.\n\n\t\t\t\t\tLast Modified: 11/27/2021\n\n\t\t\t\t\tSubmitted by: Jiebo Luo"
 }
}