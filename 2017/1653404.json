{
 "awd_id": "1653404",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER:  Bayesian Generalized Shrinkage: An Encompassing Model Approach",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2017-02-23",
 "awd_max_amd_letter_date": "2021-07-01",
 "awd_abstract_narration": "With ever-increasing complexities of datasets collected in diverse application areas, statisticians are compelled to entertain a plethora of different statistical models with varying degrees of complexity. A primary motivation for this research is to broaden the scope of classical shrinkage approaches to model selection and inferential goals in new statistical problems by providing computationally efficient and statistically accurate solutions. The methodology developed in the project has broad applications ranging from genetic and epidemiological studies to communication networks. The principal investigator is committed to increased interactions and collaborations with the broader scientific community to maximize the impact of the statistical methodology developed through this project. The deliverables of the project include user-friendly software packages that enable users in the scientific community to analyze structured data sets in applications relevant to the project goals.\r\n\r\nThe principal investigator proposes a class of generalized shrinkage methods for model selection and inference in structured high-dimensional models. Operating in a Bayesian framework, the proposed approach shrinks the parameters of an encompassing model towards a family of sub-models. Given a class of statistical models, the principal investigator defines an encompassing model as one which assigns a positive prior probability to arbitrarily small neighborhoods of any model in the specified class, with the neighborhoods defined in terms of some general statistical divergence measure such as the Kullback-Leibler divergence. This general formulation enlarges the scope of traditional shrinkage mechanisms by employing shrinkage towards potentially non-nested models. Model selection is performed by post-processing the posterior summaries from the encompassing model. The principal investigator develops novel post-processing schemes for variable selection and clustering, tensor decompositions, tree-structured models and network topology identification as various applications of the general methodology. The principal investigator formulates information theoretical techniques for studying non-asymptotic frequentist behavior of the encompassing posterior distributions, with specific emphasis on providing theoretical support for the model selection procedures developed in this project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anirban",
   "pi_last_name": "Bhattacharya",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anirban Bhattacharya",
   "pi_email_addr": "anirbanb@stat.tamu.edu",
   "nsf_id": "000655820",
   "pi_start_date": "2017-02-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M University",
  "inst_street_address": "400 HARVEY MITCHELL PKY S STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778454375",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A & M UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JF6XLNB4CDJ5"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M University Main Campus",
  "perf_str_addr": "",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433143",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  },
  {
   "pgm_ele_code": "804800",
   "pgm_ele_name": "Division Co-Funding: CAREER"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 74789.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 76818.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 89313.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 109466.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 49614.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With rapid advances in data acquisition and storage techniques, modern scientific investigations in social, biological, physical, and environmental research routinely produces complex structured data. Shrinkage estimation has emerged as a key tool in the analysis of such structured high-dimensional data. A fundamental idea behind shrinkage, commonly referred to as the bias-variance trade-off, is to reduce the variability of a statistical estimator at the cost of systematically incurring some bias. As a simple illustration, consider fitting a curve to a noisy scatter plot. One can always find a polynomial function a + b*x + c*x^2 + &hellip; of sufficiently high degree that will perfectly interpolate the data, i.e., pass through each data point. However, this interpolating curve can produce highly variable predictions at nearby points thereby lacking stability and interpretability, and it would therefore be advisable to use a lower-degree polynomial which strikes a balance between fidelity to the data and stability. Shrinkage estimation can be thought of as a general set of prescriptions for arriving at such &lsquo;stable&rsquo; estimators.</p>\n<p>&nbsp;Bayesian inference, a statistical inference framework where a stochastic model for the data is combined with prior belief about model parameters in a principled probabilistic manner, naturally induces shrinkage on parameter estimates and thereby provides an attractive inferential engine for modern multi-level datasets with complex dependencies. The research conducted as part of this project has made significant advances in methodological, theoretical, and computational aspects of Bayesian shrinkage procedures. From a methodological perspective, the project has developed new statistical methodology enlarging the scope of Bayesian shrinkage procedures in hitherto unexplored domains. Computationally, the project has contributed to development of highly scalable algorithms for conducting Bayesian regression for large datasets with very many predictors. From a theoretical perspective, novel general-purpose technical tools have been developed to provide theoretical support for related methodology.</p>\n<p>&nbsp;The PI is passionate about teaching, and has undertaken substantial curriculum development to teach ideas related to the project to students at the undergraduate and graduate level at Texas A&amp;M University. Specific activities include developing a special topic course for advanced PhD students in Statistics, remodeling a core PhD course on Bayesian statistics for first year PhD students, modernizing a graduate-level statistical machine learning course, and teaching an undergraduate Bayesian statistics course. The courses have been well-attended and received highly positive feedback. The PI has strived to disseminate historically important and foundational ideas, as well as cutting-edge research developments to students from a diverse set of backgrounds.&nbsp;&nbsp;</p>\n<p>&nbsp;Publications resulting from this project have appeared in premier statistics journals and conference proceedings such as the Annals of Statistics, The Journal of the American Statistical Association, The Journal of Royal Statistical Society (Series B), The Journal of Machine Learning Research, and AISTATS, among others. Multiple graduate students have been partially supported by this project, who have successfully defended their dissertation and continued their academic endeavors at reputed institutions.&nbsp;The work conducted as part of this project has been disseminated through multiple departmental seminars and conference presentations, and code to implement the methodologies developed in this project have been made publicly available through GitHub.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/25/2023<br>\n\t\t\t\t\tModified by: Anirban&nbsp;Bhattacharya</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWith rapid advances in data acquisition and storage techniques, modern scientific investigations in social, biological, physical, and environmental research routinely produces complex structured data. Shrinkage estimation has emerged as a key tool in the analysis of such structured high-dimensional data. A fundamental idea behind shrinkage, commonly referred to as the bias-variance trade-off, is to reduce the variability of a statistical estimator at the cost of systematically incurring some bias. As a simple illustration, consider fitting a curve to a noisy scatter plot. One can always find a polynomial function a + b*x + c*x^2 + &hellip; of sufficiently high degree that will perfectly interpolate the data, i.e., pass through each data point. However, this interpolating curve can produce highly variable predictions at nearby points thereby lacking stability and interpretability, and it would therefore be advisable to use a lower-degree polynomial which strikes a balance between fidelity to the data and stability. Shrinkage estimation can be thought of as a general set of prescriptions for arriving at such \u2018stable\u2019 estimators.\n\n Bayesian inference, a statistical inference framework where a stochastic model for the data is combined with prior belief about model parameters in a principled probabilistic manner, naturally induces shrinkage on parameter estimates and thereby provides an attractive inferential engine for modern multi-level datasets with complex dependencies. The research conducted as part of this project has made significant advances in methodological, theoretical, and computational aspects of Bayesian shrinkage procedures. From a methodological perspective, the project has developed new statistical methodology enlarging the scope of Bayesian shrinkage procedures in hitherto unexplored domains. Computationally, the project has contributed to development of highly scalable algorithms for conducting Bayesian regression for large datasets with very many predictors. From a theoretical perspective, novel general-purpose technical tools have been developed to provide theoretical support for related methodology.\n\n The PI is passionate about teaching, and has undertaken substantial curriculum development to teach ideas related to the project to students at the undergraduate and graduate level at Texas A&amp;M University. Specific activities include developing a special topic course for advanced PhD students in Statistics, remodeling a core PhD course on Bayesian statistics for first year PhD students, modernizing a graduate-level statistical machine learning course, and teaching an undergraduate Bayesian statistics course. The courses have been well-attended and received highly positive feedback. The PI has strived to disseminate historically important and foundational ideas, as well as cutting-edge research developments to students from a diverse set of backgrounds.  \n\n Publications resulting from this project have appeared in premier statistics journals and conference proceedings such as the Annals of Statistics, The Journal of the American Statistical Association, The Journal of Royal Statistical Society (Series B), The Journal of Machine Learning Research, and AISTATS, among others. Multiple graduate students have been partially supported by this project, who have successfully defended their dissertation and continued their academic endeavors at reputed institutions. The work conducted as part of this project has been disseminated through multiple departmental seminars and conference presentations, and code to implement the methodologies developed in this project have been made publicly available through GitHub.\n\n \n\n\t\t\t\t\tLast Modified: 10/25/2023\n\n\t\t\t\t\tSubmitted by: Anirban Bhattacharya"
 }
}