{
 "awd_id": "1735752",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EXP: Development of Human Language Technologies to Improve Disciplinary Writing and Learning through Self-Regulated Revising",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 549555.0,
 "awd_amount": 557555.0,
 "awd_min_amd_letter_date": "2017-07-25",
 "awd_max_amd_letter_date": "2019-04-30",
 "awd_abstract_narration": "Writing and revising are essential parts of learning, yet many college students graduate without\r\ndemonstrating improvement or mastery of academic writing. This project explores the feasibility\r\nof improving students' academic writing through a revision environment that integrates natural\r\nlanguage processing methods, best practices in data visualization and user interfaces, and\r\ncurrent pedagogical theories. The environment will support and encourage students to develop\r\nself-regulation skills that are necessary for writing and revising, including goal-setting, selection\r\nof writing strategies, and self-monitoring of progress. As a learning technology, the environment\r\ncan be applied on a large scale, thereby improving the writing of diverse student populations,\r\nincluding English learners. Additionally, the project's multidisciplinary training of graduate\r\nstudents is focused on increasing diversity in cyberlearning research and development. \r\n\r\nThree stages of investigation are planned. First, to analyze data on students' revision behaviors,\r\na series of experiments are conducted to study interactions between students\r\nand variations of the revision writing environment. Second, the collected data forms the gold\r\nstandard for developing an end-to-end system that automatically extracts revisions between\r\nstudent drafts and identifies the goal for each revision. Multiple extraction algorithms are\r\nconsidered, including phrasal alignment based on semantic similarity metrics and deep learning\r\napproaches. To identify the goal of a revision, a supervised classifier is trained from the gold\r\nstandard. A diverse set of features and the representations of the identified goals (e.g.,\r\ngranularity, scope) are explored. In addition to the \"extract-then-classify\" pipeline, an alternative\r\njoint sequence labeling model is also developed. The labeling of sequences is used to\r\nrecognize revision goals and the sequences are mutated to generate possible corrections of\r\nsentence alignments for revision extraction. The writing environment is iteratively refined,\r\naugmenting the interface prototyping through frequent user studies. Third, a complete\r\nend-to-end system that integrates the most successful component models is deployed in\r\ncollege-level writing classes. Student progress is tracked across multiple assignments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Diane",
   "pi_last_name": "Litman",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Diane J Litman",
   "pi_email_addr": "litman@cs.pitt.edu",
   "nsf_id": "000233759",
   "pi_start_date": "2019-04-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Rebecca",
   "pi_last_name": "Hwa",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rebecca Hwa",
   "pi_email_addr": "rebecca.hwa@gwu.edu",
   "nsf_id": "000462757",
   "pi_start_date": "2017-07-25",
   "pi_end_date": "2019-04-30"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Diane",
   "pi_last_name": "Litman",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Diane J Litman",
   "pi_email_addr": "litman@cs.pitt.edu",
   "nsf_id": "000233759",
   "pi_start_date": "2017-07-25",
   "pi_end_date": "2019-04-30"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Amanda",
   "pi_last_name": "Godley",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Amanda J Godley",
   "pi_email_addr": "agodley@pitt.edu",
   "nsf_id": "000476837",
   "pi_start_date": "2017-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pittsburgh",
  "inst_street_address": "4200 FIFTH AVENUE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4126247400",
  "inst_zip_code": "152600001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "MKAGLD59JRL1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pittsburgh",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152132303",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "798000",
   "pgm_ele_name": "ECR-EDU Core Research"
  },
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "8841",
   "pgm_ref_txt": "Exploration Projects"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0417",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001718DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 549555.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explored whether an automated writing evaluation system designed to support students in argumentative writing could offer effective guidance for revising paper drafts. Specifically, the project examined whether using natural language processing to recognize and provide feedback on students' revision goals could support students in developing self-regulation skills necessary for writing and revising.</p>\n<p>Three stages of investigation were conducted.&nbsp; First, to understand how a revision assistant could best support students, four system interfaces were designed and implemented.&nbsp; The versions differed in the unit span of revision analysis (sentence versus sub-sentence) and the granularity of revision goals used to provide feedback (none, binary, or detailed).&nbsp; The efficacy of the different interfaces were examined through a Wizard of Oz experiment, where the \"natural language processing outputs\" were actually provided by trained humans.&nbsp; This separated the problem of developing good natural language processing models from the problem of understanding the interactions between students and interfaces.&nbsp; Second, the collected data from the Wizard of Oz experiment formed the gold standard for training and evaluating different natural language processing methods to automatically extract revisions between student drafts and to identify the goal for each extracted revision. A variety of features and models for predicting differing revision goal representations (e.g., based on different unit spans and goal granularities) were explored. Third,&nbsp;a completely automated end-to-end system bringing together the first two stages of investigation was deployed in two college-level writing classes.&nbsp; Natural language processing was used to extract revised sentences between paper drafts and to classify revision goals, while interfaces conveyed the natural language processing results. The deployment purpose was to examine students' interactions with the revision assistant in an authentic instructional context.</p>\n<p>The outcomes for each of the three stages were as follows.&nbsp; First, the Wizard of Oz experiment showed that while a simple interface with no revision feedback was easier to use, an interface that provided a detailed categorization of sentence-level revisions was the most helpful based on survey data, as well as the most effective based on improvement in writing outcomes. An additional outcome of the experiment was a corpus of revision data that has been made freely available to the research community. The corpus consists of three drafts written by 86 university students, with around 3.3K sentence and 2.5K sub-sentence revisions. &nbsp;Annotations are provided at different levels of revision goal granularity and unit span. The corpus also includes the revision feedback given to each student, essay scores, annotation verification, and pre- and post-experiment surveys collected from participants as meta-data. Second, a set of automatic classifiers that predicted the goal of a revision sentence pair were developed and evaluated (for all possible combinations of revision goal granularity and unit span). While classifiers were reasonably good at distinguishing between binary revision goals, classification using detailed categories was much more challenging. One obstacle was the lack of enough&nbsp;training data, although performance could be improved through data augmentation techniques.&nbsp; Finally, a cross-unit prediction experiment showed that models trained on sentence revisions performed better in predicting sub-sentence revision goals than the other way around.&nbsp; Third, a fully automated version of the system that recognized and provided feedback on students' binary goals for sentence revisions was deployed&nbsp;in an undergraduate cognitive psychology class that required three writing assignments involving multiple paper drafts.&nbsp; Results suggested that natural language processing could accurately analyze where and what kind of revisions students made across paper drafts, that students engaged in self-monitored revising, and that the interfaces for providing feedback based on the natural language processing results were perceived by students to be useful.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/22/2022<br>\n\t\t\t\t\tModified by: Diane&nbsp;J&nbsp;Litman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project explored whether an automated writing evaluation system designed to support students in argumentative writing could offer effective guidance for revising paper drafts. Specifically, the project examined whether using natural language processing to recognize and provide feedback on students' revision goals could support students in developing self-regulation skills necessary for writing and revising.\n\nThree stages of investigation were conducted.  First, to understand how a revision assistant could best support students, four system interfaces were designed and implemented.  The versions differed in the unit span of revision analysis (sentence versus sub-sentence) and the granularity of revision goals used to provide feedback (none, binary, or detailed).  The efficacy of the different interfaces were examined through a Wizard of Oz experiment, where the \"natural language processing outputs\" were actually provided by trained humans.  This separated the problem of developing good natural language processing models from the problem of understanding the interactions between students and interfaces.  Second, the collected data from the Wizard of Oz experiment formed the gold standard for training and evaluating different natural language processing methods to automatically extract revisions between student drafts and to identify the goal for each extracted revision. A variety of features and models for predicting differing revision goal representations (e.g., based on different unit spans and goal granularities) were explored. Third, a completely automated end-to-end system bringing together the first two stages of investigation was deployed in two college-level writing classes.  Natural language processing was used to extract revised sentences between paper drafts and to classify revision goals, while interfaces conveyed the natural language processing results. The deployment purpose was to examine students' interactions with the revision assistant in an authentic instructional context.\n\nThe outcomes for each of the three stages were as follows.  First, the Wizard of Oz experiment showed that while a simple interface with no revision feedback was easier to use, an interface that provided a detailed categorization of sentence-level revisions was the most helpful based on survey data, as well as the most effective based on improvement in writing outcomes. An additional outcome of the experiment was a corpus of revision data that has been made freely available to the research community. The corpus consists of three drafts written by 86 university students, with around 3.3K sentence and 2.5K sub-sentence revisions.  Annotations are provided at different levels of revision goal granularity and unit span. The corpus also includes the revision feedback given to each student, essay scores, annotation verification, and pre- and post-experiment surveys collected from participants as meta-data. Second, a set of automatic classifiers that predicted the goal of a revision sentence pair were developed and evaluated (for all possible combinations of revision goal granularity and unit span). While classifiers were reasonably good at distinguishing between binary revision goals, classification using detailed categories was much more challenging. One obstacle was the lack of enough training data, although performance could be improved through data augmentation techniques.  Finally, a cross-unit prediction experiment showed that models trained on sentence revisions performed better in predicting sub-sentence revision goals than the other way around.  Third, a fully automated version of the system that recognized and provided feedback on students' binary goals for sentence revisions was deployed in an undergraduate cognitive psychology class that required three writing assignments involving multiple paper drafts.  Results suggested that natural language processing could accurately analyze where and what kind of revisions students made across paper drafts, that students engaged in self-monitored revising, and that the interfaces for providing feedback based on the natural language processing results were perceived by students to be useful.\n\n \n\n\t\t\t\t\tLast Modified: 11/22/2022\n\n\t\t\t\t\tSubmitted by: Diane J Litman"
 }
}