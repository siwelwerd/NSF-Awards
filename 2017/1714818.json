{
 "awd_id": "1714818",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF:Small:Beyond Worst Case Running time: Algorithms for Routing, Scheduling and Matching",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 456793.0,
 "awd_amount": 456793.0,
 "awd_min_amd_letter_date": "2017-06-22",
 "awd_max_amd_letter_date": "2017-06-22",
 "awd_abstract_narration": "Algorithms are ubiquitous and influence many important aspects of our lives. They address decisions ranging from the mundane, e.g. which movie to watch, to the vitally important, e.g. how to manage the internet or to schedule some time-critical tasks in a navigation system. Even though computing power and network bandwidth increase at a rapid rate, users and applications increase their demand for computation and their use of networks at roughly the same pace. Thus, a critical bottleneck in many important technologies and applications is an algorithm.\r\n\r\nThe problems considered in this project, mainly matching, scheduling and routing are fundamental algorithmic problems. By looking at resources such as energy, machines needed, and reassignments, the project will significantly broaden and deepen our understanding of these basic algorithmic problems. It will also design simpler algorithms that will lead to easily implementable solutions. The PI will make progress both in theory and in practice and will disseminate his results. The project will also make significant contributions to education via PI's textbook and other new materials. The PI will continue his commitment to Ph.D. student diversity.\r\n\r\nIt is now well-understood that time, space and worst-case solution quality are not the only resources that need to be optimized. For the past few decades, there has been a growing emphasis on other concerns such as availability of information, use of cache, management of disk, etc. More recently, there has been a growing understanding that energy and power management are also resources that should be carefully managed. In addition, one may also be interested in features of solutions as they evolve over time, or one may be interested in the algorithm's use of resources such as machines, processing speed or updates. Finally, one may also be interested in not just the bounds that we improve, but also the real-life performance of important problems.\r\n\r\nThis project plans to study several algorithmic problems that arise in scheduling, routing and matchings. The PI intends to make progress on some traditional problems and their variants and is particularly interested in considering problems from novel perspectives, that is, considering metrics that go beyond the traditionally studied ones. In particular, the project will consider energy consumption in both computers and networks, and will also consider environments in which other resources must be managed, such as minimizing the number of changes to a solution over time, the amount of processing power needed to compute a solution, or the algorithm's response to a changing environment.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Clifford",
   "pi_last_name": "Stein",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Clifford S Stein",
   "pi_email_addr": "cliff@ieor.columbia.edu",
   "nsf_id": "000193678",
   "pi_start_date": "2017-06-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "2960 Broadway",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 456793.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The work done under this grant has advanced the theory and practice of algorithm design.&nbsp;&nbsp;</p>\n<p>One important&nbsp; area is scheduling for large systems.&nbsp; We make several advances in that area.&nbsp;&nbsp;</p>\n<p>A first area we study is throughput, which is the number of jobs that are completed in a certain amount of time.&nbsp; We make two major breakthroughs in this area.&nbsp; First, we study several different models of commitment, which is the point at which you irrevocably commit to finishing a particular job.&nbsp; You may have to do so when a job arrives, when it starts processing or at some later time.&nbsp; We give a unified framework to study such decisions, show that, assuming the system has a little bit of laxity,&nbsp; these different decision points can all be handled similarly.&nbsp; &nbsp;We then give algorithms which match or improve upon previous work.&nbsp; We also show how to generalize algorithms that previously only worked for one machine to situations where you have multiple machines, as in a datacenter.&nbsp;&nbsp;</p>\n<p>We also study the notion of uncertainty in scheduling.&nbsp; We consider situations where you have to make decisions about scheduling without knowing how many machines you will be scheduling on.&nbsp; This problem models a common scenario in computing in the cloud, where you may have to partition your work without knowing exactly where it will run.</p>\n<p>Another setting where we model uncertainty is in&nbsp; a problem that arises in ride share settings.&nbsp; Matching demand (riders) to supply (drivers) efficiently is a fundamental problem for ride-hailing platforms who need to match theriders (almost) as soon as the request arrives with only partial knowledge about future ride requests. A myopic approach that computes an optimal matching for current. We consider a two-stage robust optimization framework for this matching problem where future demand uncertainty is modeled using a set of demand scenarios.&nbsp; &nbsp;The goal is to match the current request to drivers (in the first stage) so that the cost of first stage matching and the worst-case cost over all scenarios for the second stage matching is minimized. We show that this two-stage robust matching is hard but then show that intelligent algorithms can improve significantly over standard greedy approaches.</p>\n<p>&nbsp;</p>\n<p>Another&nbsp; major problem areas is algorithms for big data. The scale of modern datasets requires that the computation is done in a more structured/restricted settings than on a standard single computer. One major such setting is the massive parallel computing model (MPC), which models cluster computing systems such as the influential MapReduce or many systems that followed, where a computational task is performed collaboratively by a number of machines.&nbsp; &nbsp;</p>\n<p>&nbsp;</p>\n<p>We study algorithms for massive graphs in this setting.&nbsp; The key point in designing algorithms for massive graphs is that each machine only \"sees\" a small fraction of the data.&nbsp; Thus, the communication between machines, an operation that is orders-of-magnitude slower than local computation, becomes the bottleneck.&nbsp; We have to design algorithms that are nothing like their sequential counterparts, and must work to minimize communication and total storage, in addition to time.</p>\n<p>In this work, we make significant progress on several important graph problems.&nbsp; These include</p>\n<p>- connectivity : figuring out if a graph is connected, that is can you reach every node from every other node</p>\n<p>- biconnectivity : finding components in a graph that are biconnected.&nbsp; Each pair of nodes in a biconnected component has two distinct paths between them.</p>\n<p>- matching:&nbsp; figuring out an optimal way to pair up nodes, or to assign entities from one set of objects to another</p>\n<p>- shortest paths : figuring out the path of least total distance between two points.</p>\n<p>&nbsp;For each of these problems, we advance the state of the art in MPC algorithms, and for several answer long-standing open questions about the complexity of such computations.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/08/2023<br>\n\t\t\t\t\tModified by: Clifford&nbsp;S&nbsp;Stein</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe work done under this grant has advanced the theory and practice of algorithm design.  \n\nOne important  area is scheduling for large systems.  We make several advances in that area.  \n\nA first area we study is throughput, which is the number of jobs that are completed in a certain amount of time.  We make two major breakthroughs in this area.  First, we study several different models of commitment, which is the point at which you irrevocably commit to finishing a particular job.  You may have to do so when a job arrives, when it starts processing or at some later time.  We give a unified framework to study such decisions, show that, assuming the system has a little bit of laxity,  these different decision points can all be handled similarly.   We then give algorithms which match or improve upon previous work.  We also show how to generalize algorithms that previously only worked for one machine to situations where you have multiple machines, as in a datacenter.  \n\nWe also study the notion of uncertainty in scheduling.  We consider situations where you have to make decisions about scheduling without knowing how many machines you will be scheduling on.  This problem models a common scenario in computing in the cloud, where you may have to partition your work without knowing exactly where it will run.\n\nAnother setting where we model uncertainty is in  a problem that arises in ride share settings.  Matching demand (riders) to supply (drivers) efficiently is a fundamental problem for ride-hailing platforms who need to match theriders (almost) as soon as the request arrives with only partial knowledge about future ride requests. A myopic approach that computes an optimal matching for current. We consider a two-stage robust optimization framework for this matching problem where future demand uncertainty is modeled using a set of demand scenarios.   The goal is to match the current request to drivers (in the first stage) so that the cost of first stage matching and the worst-case cost over all scenarios for the second stage matching is minimized. We show that this two-stage robust matching is hard but then show that intelligent algorithms can improve significantly over standard greedy approaches.\n\n \n\nAnother  major problem areas is algorithms for big data. The scale of modern datasets requires that the computation is done in a more structured/restricted settings than on a standard single computer. One major such setting is the massive parallel computing model (MPC), which models cluster computing systems such as the influential MapReduce or many systems that followed, where a computational task is performed collaboratively by a number of machines.   \n\n \n\nWe study algorithms for massive graphs in this setting.  The key point in designing algorithms for massive graphs is that each machine only \"sees\" a small fraction of the data.  Thus, the communication between machines, an operation that is orders-of-magnitude slower than local computation, becomes the bottleneck.  We have to design algorithms that are nothing like their sequential counterparts, and must work to minimize communication and total storage, in addition to time.\n\nIn this work, we make significant progress on several important graph problems.  These include\n\n- connectivity : figuring out if a graph is connected, that is can you reach every node from every other node\n\n- biconnectivity : finding components in a graph that are biconnected.  Each pair of nodes in a biconnected component has two distinct paths between them.\n\n- matching:  figuring out an optimal way to pair up nodes, or to assign entities from one set of objects to another\n\n- shortest paths : figuring out the path of least total distance between two points.\n\n For each of these problems, we advance the state of the art in MPC algorithms, and for several answer long-standing open questions about the complexity of such computations.\n\n \n\n \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/08/2023\n\n\t\t\t\t\tSubmitted by: Clifford S Stein"
 }
}