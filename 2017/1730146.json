{
 "awd_id": "1730146",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CompCog:  Collaborative Research:  Learning Visuospatial Reasoning Skills from Experiences",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927878",
 "po_email": "slim@nsf.gov",
 "po_sign_block_name": "Soo-Siang Lim",
 "awd_eff_date": "2017-08-15",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 99691.0,
 "awd_amount": 99691.0,
 "awd_min_amd_letter_date": "2017-08-16",
 "awd_max_amd_letter_date": "2017-08-16",
 "awd_abstract_narration": "This project uses methods from artificial intelligence (AI) to better understand how people learn visuospatial reasoning skills like mental rotation, which are a critical ingredient in the development of strong math and science abilities.  In particular, this project proposes a new approach to quantify the learning value contained in different visual experiences, using wearable cameras combined with a new AI system that learns visuospatial reasoning skills from video examples.  Results from this project will not only advance the state of the art in AI but also will enable researchers to measure how valuable different real-world visual experiences are in helping people to learn visuospatial reasoning skills.  For example, certain types of object play activities might be particularly valuable for helping a child to learn certain visuospatial reasoning skills.  Ultimately, this new measurement approach could be used to identify early signs of visuospatial reasoning difficulties in children and could also help in the design of new visuospatial training interventions to boost children?s early math and science development.\r\n\r\nThe core scientific question that this project aims to answer is: How are visuospatial reasoning skills learned from first-person visual experiences?  This question will be answered through computational experiments with a new AI system---the Mental Imagery Engine (MIME)---that learns visuospatial reasoning skills, like mental rotation, from video examples.  Training data will include first-person, wearable-camera videos from two different settings that are both important for human learning:  unstructured object manipulation by infants and visuospatial training interventions designed for children.  Results from experiments with the MIME AI system will advance the state of the art in both AI and the science of human learning by helping to explain how visuospatial reasoning skills can be learned from visual experiences, and, in particular, how having different kinds of visual experiences can affect the quality of a person?s learning outcomes in different ways.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Linda",
   "pi_last_name": "Smith",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Linda B Smith",
   "pi_email_addr": "smith4@indiana.edu",
   "nsf_id": "000085967",
   "pi_start_date": "2017-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "474013654",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IN09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "004Y00",
   "pgm_ele_name": "Science of Learning"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "059Z",
   "pgm_ref_txt": "Science of Learning"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 99691.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In artificial intelligence, commonsense reasoning is critical to virtually every area of robust intelligent behavior, including question answering, planning, and more (Davis 2014), but it remains a difficult challenge for the field. For example, humans are very good at predicting how an unseen side (or view) of an object &nbsp;would look by mentally rotating that object. Many psychological theories (and experiments) suggest that humans learn to do these internal simulations as a consequence of seeing their own real-world actions on actions.&nbsp; &nbsp;&nbsp;The larger collaborative project sought to teach a novel AI system to imagine spatial transformations of objects by using videos of human object manipulation.&nbsp; The subproject funded by this grant collected and analyzed videos collected by 1 to 2 year old children wearing head-cameras as they visually and manually explored objects as a potential training set in the AI larger project.&nbsp; Our analyses provide a lesson plan for teaching mental rotation to AI.&nbsp; Children during play slowly rotate objects and bias looking times to transformations between views in which one axis of the object is elongated, a pattern that systematically exposes and relates different views.&nbsp; In sum, the statistical properties of infants&rsquo; self-generated visual experiences provide insights as to how to optimize training data and learning algorithms for more human-like computer vision. &nbsp;The broader impacts of the project include bringing together&nbsp; a&nbsp; team&nbsp; of&nbsp; researchers&nbsp; &ndash;and&nbsp; their&nbsp; undergraduates,&nbsp; their&nbsp; graduate&nbsp; students,&nbsp; and&nbsp; their&nbsp; post- doctoral researchers &ndash; that cross disciplinary boundaries not commonly spanned from diverse background including individuals underrepresented in science and technology.<strong>&nbsp;</strong></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/01/2019<br>\n\t\t\t\t\tModified by: Linda&nbsp;B&nbsp;Smith</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn artificial intelligence, commonsense reasoning is critical to virtually every area of robust intelligent behavior, including question answering, planning, and more (Davis 2014), but it remains a difficult challenge for the field. For example, humans are very good at predicting how an unseen side (or view) of an object  would look by mentally rotating that object. Many psychological theories (and experiments) suggest that humans learn to do these internal simulations as a consequence of seeing their own real-world actions on actions.    The larger collaborative project sought to teach a novel AI system to imagine spatial transformations of objects by using videos of human object manipulation.  The subproject funded by this grant collected and analyzed videos collected by 1 to 2 year old children wearing head-cameras as they visually and manually explored objects as a potential training set in the AI larger project.  Our analyses provide a lesson plan for teaching mental rotation to AI.  Children during play slowly rotate objects and bias looking times to transformations between views in which one axis of the object is elongated, a pattern that systematically exposes and relates different views.  In sum, the statistical properties of infants? self-generated visual experiences provide insights as to how to optimize training data and learning algorithms for more human-like computer vision.  The broader impacts of the project include bringing together  a  team  of  researchers  &ndash;and  their  undergraduates,  their  graduate  students,  and  their  post- doctoral researchers &ndash; that cross disciplinary boundaries not commonly spanned from diverse background including individuals underrepresented in science and technology. \n\n \n\n\t\t\t\t\tLast Modified: 08/01/2019\n\n\t\t\t\t\tSubmitted by: Linda B Smith"
 }
}