{
 "awd_id": "1717950",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: Multi-Party High-dimensional Machine Learning with Privacy",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "James Joshi",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 498624.0,
 "awd_amount": 498624.0,
 "awd_min_amd_letter_date": "2017-08-18",
 "awd_max_amd_letter_date": "2019-02-26",
 "awd_abstract_narration": "Individuals and organizations can frequently benefit from combining their data to learn collective models. However, combining data to enable multi-party learning is often not possible.  It may not be permitted due to privacy policies, or may be considered too risky for a business to expose its own data to others. In addition, high-dimensional data are prevalent in modern data-driven applications. Learning from high-dimensional data owned by differential organizations is even more challenging, due to the bias introduced by the high-dimensional machine learning methods. The overarching goal of this project is to address these challenges by developing methods that enable a group of mutually distrusting parties to securely collaborate to apply high dimensional machine learning methods to produce a joint model without exposing their own data. This project enables owners of sensitive data to jointly learn models across their datasets without exposing that data and providing meaningful privacy guarantees.  It produces open source software tools and has many important societal applications, including its use in analyzing electronic health records across multiple hospitals to identify medical correlations what could not be found by any individual hospital.  \r\n\r\n\r\nThe key of multi-party high-dimensional machine learning is to find an efficient way to produce an accurate aggregate model that reflects all of the data, by combining local models that are developed independently based on individual data sets. The strategy of this project is to combine two emerging research directions: distributed machine learning, which seeks to distribute machine learning algorithms across hosts and produce an aggregate model by combining multiple local models; and secure multi-party computation, which enables a group of mutually distrusting parties to jointly compute a function without leaking information about their private inputs or any intermediate results. It also incorporates differential privacy-based mechanisms into multi-party high dimensional learning, which further protects the individual data points in each party. The results of this research have the potential to impact both the machine learning and security research communities. The education plan of this project includes developing open course materials that integrate privacy and machine learning, and provide research-based training opportunities for both undergraduate and graduate students in computer science, systems engineering, and medical informatics. It actively gets underrepresented groups involved in research projects, and trains a new generation of interdisciplinary researchers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Evans",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "David E Evans",
   "pi_email_addr": "evans@virginia.edu",
   "nsf_id": "000319935",
   "pi_start_date": "2019-02-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Quanquan",
   "pi_last_name": "Gu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Quanquan Gu",
   "pi_email_addr": "qgu@cs.ucla.edu",
   "nsf_id": "000694630",
   "pi_start_date": "2017-08-18",
   "pi_end_date": "2019-02-26"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Evans",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "David E Evans",
   "pi_email_addr": "evans@virginia.edu",
   "nsf_id": "000319935",
   "pi_start_date": "2017-08-18",
   "pi_end_date": "2019-02-26"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Quanquan",
   "pi_last_name": "Gu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Quanquan Gu",
   "pi_email_addr": "qgu@cs.ucla.edu",
   "nsf_id": "000694630",
   "pi_start_date": "2019-02-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia",
  "perf_str_addr": "P. O. Box 400195",
  "perf_city_name": "Charlottesville",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229044195",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 498624.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-ba2b6c96-7fff-90ef-a3a6-754b852ec31c\"> </span></p>\n<p dir=\"ltr\"><span>When machine learning is used to train models on sensitive data, there is a risk that the sensitive training data is exposed directly, especially when there is a need to train models on data from more than one data owner, and that the trained model which is then released reveals sensitive aspects of the training data. This project advances scientific understanding of privacy in machine learning settings where training is done on sensitive data owned by different organizations, and those organizations securely collaborate to jointly learn a model without exposing their own data.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>This project developed new methods for using cryptographic techniques to perform multi-party computation to enable joint models to be learned without needing to centralize the data. This project developed methods for incorporating noise directly within the secure computation to provide a formal privacy guarantee that bounds the risk that the learned joint model will reveal sensitive information about an individual?s training data. The results of the project enabled performance improvements in secure distributed learning, including in challenging non-convex learning settings, and resulted in multi-party learning algorithms that enable secure multi-party machine learning. The project also developed new empirical methods for analyzing the inference risks for a release model, providing better ways to estimate the risks of realistic inference attacks on release models and developing a new attack method that demonstrates that there are inference risks in settings where previous attacks would be ineffective. The tools developed for this project have been released as open source code and used by other researchers in academia and industry, and the scientific results of the project have been presented in research papers in top conferences in machine learning, security and privacy, and in keynote and invited talks at conferences and workshops, and to government policy-makers.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/22/2021<br>\n\t\t\t\t\tModified by: David&nbsp;E&nbsp;Evans</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nWhen machine learning is used to train models on sensitive data, there is a risk that the sensitive training data is exposed directly, especially when there is a need to train models on data from more than one data owner, and that the trained model which is then released reveals sensitive aspects of the training data. This project advances scientific understanding of privacy in machine learning settings where training is done on sensitive data owned by different organizations, and those organizations securely collaborate to jointly learn a model without exposing their own data. \n\n \nThis project developed new methods for using cryptographic techniques to perform multi-party computation to enable joint models to be learned without needing to centralize the data. This project developed methods for incorporating noise directly within the secure computation to provide a formal privacy guarantee that bounds the risk that the learned joint model will reveal sensitive information about an individual?s training data. The results of the project enabled performance improvements in secure distributed learning, including in challenging non-convex learning settings, and resulted in multi-party learning algorithms that enable secure multi-party machine learning. The project also developed new empirical methods for analyzing the inference risks for a release model, providing better ways to estimate the risks of realistic inference attacks on release models and developing a new attack method that demonstrates that there are inference risks in settings where previous attacks would be ineffective. The tools developed for this project have been released as open source code and used by other researchers in academia and industry, and the scientific results of the project have been presented in research papers in top conferences in machine learning, security and privacy, and in keynote and invited talks at conferences and workshops, and to government policy-makers.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/22/2021\n\n\t\t\t\t\tSubmitted by: David E Evans"
 }
}