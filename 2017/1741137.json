{
 "awd_id": "1741137",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Testing High Dimensional Distributions without the Curse of Dimensionality",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2017-12-01",
 "awd_exp_date": "2020-11-30",
 "tot_intn_awd_amt": 900000.0,
 "awd_amount": 900000.0,
 "awd_min_amd_letter_date": "2017-08-30",
 "awd_max_amd_letter_date": "2019-08-30",
 "awd_abstract_narration": "Scientists develop descriptions of models that explain their observations. But how many observations are needed to verify the validity of a model? When the model is probabilistic, the resulting question is this: How many samples from a distribution are needed to test whether it has a certain property? Arguably this problem lies at the foundations of scientific thought, and recent years have seen a tremendous body of work in the Computer Science literature trying to close in on the precise sample and time complexity needed to test distribution properties. Often data is high dimensional -- for example, medical records for patients have many entries. However, high dimensional distribution data is notoriously hard to deal with.  This project will find new ways of overcoming the difficulties of dealing with high dimensional data, by isolating properties of data occurring in practice that aid in simplifying the distribution testing problems.\r\n\r\nThe broader impact of this project includes advancing the interface of Computer Science, Statistics and Learning. Our methods will be tested on a healthcare dataset, including over 3.7 million patients, and will test the accuracy of common models used to improve healthcare outcomes. Broader impact of this project also includes engagement in Computer Science activities for elementary school children, MIT PRIMES mathematical research with high school students, and participation in activities for promoting women in research. \r\n\r\n\r\nCurrent work on distribution property testing has focused on properties of single-dimensional distributions such as uniformity, monotonicity, log-concavity, and others, with only a few results on testing properties of high-dimensional distributions. Unfortunately, testing properties of high-dimensional distributions quickly runs into exponential sample complexity lower bounds. The goal of the project is to develop new analysis frameworks for overcoming these lower bounds. Typically the lower bounds construct highly-complex distributions that do not possess a property but are really hard to distinguish from those that do. Our thesis is that such rich structure may not be present in many practical settings of interest. The overarching question of our research then is this: \r\nare there reasonable assumptions that one could make about the unknown distribution under which high-dimensional testing problems are more tractable? \r\n\r\nThis research will (1) explore how the expressive language of graphical models can be used to restrict the correlation structure of high-dimensional distributions in ways that can be leveraged for faster testing; and (2) develop analysis frameworks that allow testing generating models of combinatorial structures, such as social networks, from a single or a constant number of samples; this sounds like an oxymoron but it will be made possible with adequate assumptions about the model generating the combinatorial structure. (1) will reveal important connections to Bayesian networks and their use in healthcare decision making, as well as to computational biology and phylogenetics, while (2) will have connections to social network modeling.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ronitt",
   "pi_last_name": "Rubinfeld",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ronitt Rubinfeld",
   "pi_email_addr": "ronitt@csail.mit.edu",
   "nsf_id": "000322655",
   "pi_start_date": "2017-08-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Constantinos",
   "pi_last_name": "Daskalakis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Constantinos Daskalakis",
   "pi_email_addr": "costis@csail.mit.edu",
   "nsf_id": "000537704",
   "pi_start_date": "2017-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Ave",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 900000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Scientists develop descriptions of models that explain their  observations. But how many observations are needed to verify the  validity of a model? When the model is probabilistic, the resulting  question is this: How many samples from a distribution are needed to  test whether it has a certain property? Arguably this problem lies at  the foundations of scientific thought, and recent years have seen a  tremendous body of work in the CS literature and specifically the fields  of Sublinear Algorithms and Property Testing, trying to close in on the  precise sample and time complexity needed to test distribution  properties. Current work has focused on properties of single-dimensional  distributions such as uniformity, monotonicity, log-concavity, and  others, with only a few results on testing properties of  high-dimensional distributions. Unfortunately, testing properties of  high-dimensional distributions quickly runs into exponential sample  complexity lower bounds.</p>\n<p>This project&nbsp; develops new analysis frameworks for  overcoming these lower bounds. Typically the lower bounds construct  highly-correlated distributions that do not possess a property but are  really hard to distinguish from those that do. The thesis underlying  this work is that such strong correlations may not be present in many  practical settings of interest.</p>\n<p>The overarching question of this&nbsp; research then is this: are there  reasonable assumptions that one could make about the unknown  distribution under which high-dimensional testing problems are more  tractable?</p>\n<p>This project&nbsp; has (1) explored how the expressive language of  graphical models can be used to restrict the correlation structure of  high-dimensional distributions in ways that can be leveraged for faster  testing; (2) developed analysis frameworks that allow testing the  randomness of combinatorial structures from a single or a constant  number of samples;&nbsp; made  possible with adequate assumptions about the model generating the  combinatorial structure; (3) developed more powerful and efficient algorithms for hypothesis testing, and testing structural properties of high dimensional distributions -- such as to estimate the support size, determine whether the tail of the distribution is heavy or light,&nbsp; determine whether the distribution is monotone orwhether it is a mixture of more than one known distribution. We expand on two important lines of investigation that our work avanced.</p>\n<p>&nbsp;</p>\n<p><strong>Testing from a Single Sample:&nbsp;</strong>Classical distribution  testing assumes access to many independent and identically distributed  (i.i.d.) samples from the distribution that is being tested. However, in  many important applications the observer does not get to observe  multiple samples from the distribution of interest. In fact, in many  applications the observer only gets to observe a <em>single sample</em> from the distribution of interest, e.g. one social network, one  dissected brain etc. This project leads an effort to obtain statistical  tests that operate under a <em>single sample</em> from the distribution of interest. Progress has been made in several directions, including the following:</p>\n<p>- The study of Markov chain testing is initiated:&nbsp; given access to  a <em>single trajectory</em> of a Markov Chain the goal is to test whether the trajectory comes from a specific given Markov chain. &nbsp; The first algorithms are given for this problem and are shown to be nearly optimal.</p>\n<p>- The study of graph generative model testing is initiated: given a <em>single graph </em>the goal is to test whether it came from a specific graph generative model such as preferential attachment. We provide conditions under which this is possible.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Testing and learning monotone high-dimensional probability distributions:&nbsp;</strong>A probability distribution over the Boolean cube is&nbsp;<em>monotone</em>&nbsp;if  switching the value of a coordinate from zero to one can only increase  the probability of an element.&nbsp; Given samples of an unknown monotone  distribution over the Boolean cube, we give the first algorithm that learns an  approximation of the distribution in statistical distance using a number  of samples that is sublinear in the size of the domain.</p>\n<p>To do this, a new structural lemma describing  monotone&nbsp;probability distributions is given which allows for a more  succinct description of monotone distributions over the Boolean  cube.&nbsp;&nbsp;The structural lemma has further implications to the sample  complexity of other basic testing tasks for analyzing monotone  probability distributions over the Boolean cube: It is used to give  nontrivial upper bounds on the tasks of estimating the distance of a  monotone distribution to uniform and&nbsp;of estimating the support size of a  monotone distribution. In the setting of monotone probability  distributions over the Boolean cube,&nbsp;the new algorithms are the first to  have sample complexity lower than known lower bounds for the same  testing tasks on arbitrary (not necessarily monotone) probability  distributions.&nbsp;<br />&nbsp;&nbsp;&nbsp;&nbsp;<br />One further consequence of the new learning  algorithm is a significantly improved sample complexity for the task of  testing whether a&nbsp;distribution on the Boolean cube is monotone.</p>\n<p>To improve the dissemination of our results and our outreach, we have given multiple invited lectures, keynotes, and public lectures. We have also been interviewed for documentaries, wide-circulation news outlets and podcasts.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/06/2021<br>\n\t\t\t\t\tModified by: Constantinos&nbsp;Daskalakis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nScientists develop descriptions of models that explain their  observations. But how many observations are needed to verify the  validity of a model? When the model is probabilistic, the resulting  question is this: How many samples from a distribution are needed to  test whether it has a certain property? Arguably this problem lies at  the foundations of scientific thought, and recent years have seen a  tremendous body of work in the CS literature and specifically the fields  of Sublinear Algorithms and Property Testing, trying to close in on the  precise sample and time complexity needed to test distribution  properties. Current work has focused on properties of single-dimensional  distributions such as uniformity, monotonicity, log-concavity, and  others, with only a few results on testing properties of  high-dimensional distributions. Unfortunately, testing properties of  high-dimensional distributions quickly runs into exponential sample  complexity lower bounds.\n\nThis project  develops new analysis frameworks for  overcoming these lower bounds. Typically the lower bounds construct  highly-correlated distributions that do not possess a property but are  really hard to distinguish from those that do. The thesis underlying  this work is that such strong correlations may not be present in many  practical settings of interest.\n\nThe overarching question of this  research then is this: are there  reasonable assumptions that one could make about the unknown  distribution under which high-dimensional testing problems are more  tractable?\n\nThis project  has (1) explored how the expressive language of  graphical models can be used to restrict the correlation structure of  high-dimensional distributions in ways that can be leveraged for faster  testing; (2) developed analysis frameworks that allow testing the  randomness of combinatorial structures from a single or a constant  number of samples;  made  possible with adequate assumptions about the model generating the  combinatorial structure; (3) developed more powerful and efficient algorithms for hypothesis testing, and testing structural properties of high dimensional distributions -- such as to estimate the support size, determine whether the tail of the distribution is heavy or light,  determine whether the distribution is monotone orwhether it is a mixture of more than one known distribution. We expand on two important lines of investigation that our work avanced.\n\n \n\nTesting from a Single Sample: Classical distribution  testing assumes access to many independent and identically distributed  (i.i.d.) samples from the distribution that is being tested. However, in  many important applications the observer does not get to observe  multiple samples from the distribution of interest. In fact, in many  applications the observer only gets to observe a single sample from the distribution of interest, e.g. one social network, one  dissected brain etc. This project leads an effort to obtain statistical  tests that operate under a single sample from the distribution of interest. Progress has been made in several directions, including the following:\n\n- The study of Markov chain testing is initiated:  given access to  a single trajectory of a Markov Chain the goal is to test whether the trajectory comes from a specific given Markov chain.   The first algorithms are given for this problem and are shown to be nearly optimal.\n\n- The study of graph generative model testing is initiated: given a single graph the goal is to test whether it came from a specific graph generative model such as preferential attachment. We provide conditions under which this is possible. \n\n \n\nTesting and learning monotone high-dimensional probability distributions: A probability distribution over the Boolean cube is monotone if  switching the value of a coordinate from zero to one can only increase  the probability of an element.  Given samples of an unknown monotone  distribution over the Boolean cube, we give the first algorithm that learns an  approximation of the distribution in statistical distance using a number  of samples that is sublinear in the size of the domain.\n\nTo do this, a new structural lemma describing  monotone probability distributions is given which allows for a more  succinct description of monotone distributions over the Boolean  cube.  The structural lemma has further implications to the sample  complexity of other basic testing tasks for analyzing monotone  probability distributions over the Boolean cube: It is used to give  nontrivial upper bounds on the tasks of estimating the distance of a  monotone distribution to uniform and of estimating the support size of a  monotone distribution. In the setting of monotone probability  distributions over the Boolean cube, the new algorithms are the first to  have sample complexity lower than known lower bounds for the same  testing tasks on arbitrary (not necessarily monotone) probability  distributions. \n    \nOne further consequence of the new learning  algorithm is a significantly improved sample complexity for the task of  testing whether a distribution on the Boolean cube is monotone.\n\nTo improve the dissemination of our results and our outreach, we have given multiple invited lectures, keynotes, and public lectures. We have also been interviewed for documentaries, wide-circulation news outlets and podcasts. \n\n\t\t\t\t\tLast Modified: 06/06/2021\n\n\t\t\t\t\tSubmitted by: Constantinos Daskalakis"
 }
}