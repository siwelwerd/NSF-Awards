{
 "awd_id": "1758028",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Bayesian Nonparametric Learning for Large-Scale Structure Discovery",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-02-28",
 "tot_intn_awd_amt": 331349.0,
 "awd_amount": 331349.0,
 "awd_min_amd_letter_date": "2017-09-22",
 "awd_max_amd_letter_date": "2018-05-24",
 "awd_abstract_narration": "CAREER: Bayesian Nonparametric Learning for Large-Scale Structure Discovery\r\n\r\nThis CAREER project will advance the state-of-the-art for automated discovery of structure within data as diverse as images and video, natural language, audio sequences, and social and biological networks.  Contemporary applications of statistical machine learning are dominated by parametric models.  This approach constructs models of pre-determined size (with a finite-dimensional vector of parameters which) are tuned using training data.  To be effective, the underlying structure of such models must be manually specified by experts with application-specific knowledge.  This presumed structure imposes limits on what can possibly be learned even from very big datasets.\r\n\r\nBayesian nonparametric models instead define distributions on models of arbitrary size with infinite-dimensional spaces of functions, partitions, or other combinatorial structures.  They lead to flexible, data-driven unsupervised learning algorithms, and models whose internal structure continually grows and adapts to new observations.  Bayesian nonparametric models, while promising, are an incompletely-developed technology posing significant challenges to practice.  This CAREER project will increase the practical feasibility and impact of Bayesian nonparametric approaches by pursuing three interrelated themes:\r\n\r\n1) Nonparametric Model Design and Evaluation.  New families of models for data with hierarchical, spatial, temporal, or relational structure are investigated.  Quantitative validation of the statistical assumptions and biases inherent in these models will be emphasized, evaluating whether these align with the empirical statistics of significant application areas.\r\n\r\n2) Reliable Structure Discovery.  Statistical inference algorithms which move beyond the local moves of standard (and widely used) Monte Carlo and variational methods will be developed.  Compelling examples indicate that local optima are a significant issue for contemporary methods, so a family of novel algorithms is proposed, which dynamically adjust model complexity as learning proceeds.\r\n\r\n3) Scalable and Extensible Nonparametric Learning.  Common patterns across a wide range of popular nonparametric models are identified, which suggest a corresponding family of scalable and parallelizable online learning algorithms.  The \"memoized\" online variational inference algorithm avoids some practical instabilities and sensitivities of conventional methods, while allowing provably correct optimization of the nonparametric model structure and complexity.\r\n\r\nAn extensible \"BNPy: Bayesian Nonparametric Learning in Python\" software package is under development to allow easy application of the novel learning algorithms to a wide range of current and future BNP models.  The education and outreach plan of this CAREER project leverages this software to create interdisciplinary undergraduate research teams exploring applications in the natural and social sciences, and a week-long summer school on Bayesian nonparametrics to be held twice at Brown University's Institute for Computational and Experimental Research in Mathematics (ICERM).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Erik",
   "pi_last_name": "Sudderth",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Erik Sudderth",
   "pi_email_addr": "sudderth@uci.edu",
   "nsf_id": "000561453",
   "pi_start_date": "2017-09-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "School of Information and Computer Sciences",
  "perf_city_name": "Irvine",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926973425",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 17751.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 100986.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 104478.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 108134.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Probabilistic machine learning methods are widely used in science and engineering because they allow practitioners to incorporate prior knowledge (via Bayesian statistics), quantify uncertainty when making predictions based on data, and learn from data where expert annotations are sparse or entirely missing.&nbsp; Our focus on this project is on models that, with little to no expert supervision, can learn structured and interpretable data representations.&nbsp; Example applications we have explored include the discovery of topics discussed in large collections of text documents or medical records, the identification of typical patterns in images to improve algorithms for image processing and denoising, the segmentation of typical activities from large sequences of human motion data, and the discovery of the communities underlying observed social network links.<br />A challenge of such weakly supervised machine learning problems is that the number of topics/patterns/activities/communities is typically unknown before the data is analyzed.&nbsp; We address this issue via infinite-dimensional, Bayesian Nonparametric (BNP) models. BNP methods lead to flexible learning algorithms that allow model structure to continually grow and adapt to new observations.&nbsp; Our research has developed new families of hierarchical BNP models, and rigorously studied whether their statistical biases are consistent with real-world data; designed reliable structure discovery methods which avoid the local optima that plague previous approaches; and developed scalable online learning algorithms which allow our methods to be applied to complex data with hierarchical, spatial, temporal, and relational structure.&nbsp;&nbsp;<br />While BNP models are defined mathematically via probability distributions, inference algorithms are needed to learn model parameters from data.&nbsp; The inferred model structure may then be used for data visualization and exploration, as well as for predictions driven by the science or engineering application.&nbsp; Most prior work on BNP models has used simulation-based, \"Monte Carlo\" inference algorithms that are computationally expensive, and thus impractical for large datasets.&nbsp; Our work instead developed optimization-based, \"variational\" inference algorithms that are explicitly designed to support a broad range of models, and enable online learning from huge data streams.&nbsp; We demonstrated that local optima are a significant issue for most prior inference algorithms, and developed a family of novel variational methods that dynamically adjust model complexity as learning proceeds, and are insensitive to initialization.&nbsp; Successful applications of our variational inference algorithms have included improved methods for removing noise from images, the discovery of patterns in medical records that are predictive of antidepressant effectiveness, the removal of errors in crowd-sourced data annotations, and the suggestion of new potential links from social network data.<br />Our variational inference algorithms were implemented in open source statistical software to facilitate their use by engineers and scientists.&nbsp; We also integrated some of our variational inference algorithms with probabilistic programming languages, which allow less-expert users to specify models via high-level commands, and then have efficient inference code automatically generated.&nbsp; Interdisciplinary teams of graduate and undergraduate students worked throughout this project to develop our new BNP methods, and explored various applications in the natural and social sciences.&nbsp; Instructional materials regarding BNP models and inference algorithms were also developed, tested in courses taught by the principal investigator, and distributed freely online.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/29/2022<br>\n\t\t\t\t\tModified by: Erik&nbsp;B&nbsp;Sudderth</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nProbabilistic machine learning methods are widely used in science and engineering because they allow practitioners to incorporate prior knowledge (via Bayesian statistics), quantify uncertainty when making predictions based on data, and learn from data where expert annotations are sparse or entirely missing.  Our focus on this project is on models that, with little to no expert supervision, can learn structured and interpretable data representations.  Example applications we have explored include the discovery of topics discussed in large collections of text documents or medical records, the identification of typical patterns in images to improve algorithms for image processing and denoising, the segmentation of typical activities from large sequences of human motion data, and the discovery of the communities underlying observed social network links.\nA challenge of such weakly supervised machine learning problems is that the number of topics/patterns/activities/communities is typically unknown before the data is analyzed.  We address this issue via infinite-dimensional, Bayesian Nonparametric (BNP) models. BNP methods lead to flexible learning algorithms that allow model structure to continually grow and adapt to new observations.  Our research has developed new families of hierarchical BNP models, and rigorously studied whether their statistical biases are consistent with real-world data; designed reliable structure discovery methods which avoid the local optima that plague previous approaches; and developed scalable online learning algorithms which allow our methods to be applied to complex data with hierarchical, spatial, temporal, and relational structure.  \nWhile BNP models are defined mathematically via probability distributions, inference algorithms are needed to learn model parameters from data.  The inferred model structure may then be used for data visualization and exploration, as well as for predictions driven by the science or engineering application.  Most prior work on BNP models has used simulation-based, \"Monte Carlo\" inference algorithms that are computationally expensive, and thus impractical for large datasets.  Our work instead developed optimization-based, \"variational\" inference algorithms that are explicitly designed to support a broad range of models, and enable online learning from huge data streams.  We demonstrated that local optima are a significant issue for most prior inference algorithms, and developed a family of novel variational methods that dynamically adjust model complexity as learning proceeds, and are insensitive to initialization.  Successful applications of our variational inference algorithms have included improved methods for removing noise from images, the discovery of patterns in medical records that are predictive of antidepressant effectiveness, the removal of errors in crowd-sourced data annotations, and the suggestion of new potential links from social network data.\nOur variational inference algorithms were implemented in open source statistical software to facilitate their use by engineers and scientists.  We also integrated some of our variational inference algorithms with probabilistic programming languages, which allow less-expert users to specify models via high-level commands, and then have efficient inference code automatically generated.  Interdisciplinary teams of graduate and undergraduate students worked throughout this project to develop our new BNP methods, and explored various applications in the natural and social sciences.  Instructional materials regarding BNP models and inference algorithms were also developed, tested in courses taught by the principal investigator, and distributed freely online.\n\n\t\t\t\t\tLast Modified: 06/29/2022\n\n\t\t\t\t\tSubmitted by: Erik B Sudderth"
 }
}