{
 "awd_id": "1725743",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: CISIT: Computing In Situ and In Memory for Hierarchical Numerical Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2020-09-30",
 "tot_intn_awd_amt": 800000.0,
 "awd_amount": 800000.0,
 "awd_min_amd_letter_date": "2017-09-11",
 "awd_max_amd_letter_date": "2017-09-11",
 "awd_abstract_narration": "High performance computing holds an enormous promise for revolutionizing science, technology, and everyday life through modeling and simulation, statistical inference, and artificial intelligence.  Despite the numerous successes in software and hardware technologies, energy efficiency barriers have become a major hurdle towards more powerful computers -- from mobile devices all the way to supercomputers. The originally natural separation between the memory subsystem and the central processing unit (CPU) of a computer has emerged as one the main reasons for energy inefficiency. Data movement between the memory and the CPU requires orders of magnitude more energy than the computations themselves. To address these challenges, this project will consider novel architectural design paradigms and algorithms that are aimed at blurring these traditional boundaries between separated memory and computation subsystems and, by distributing computations to be performed directly in the memory or as part of the memory data transfers, achieve order of magnitude gains inenergy efficiency and performance. This project will investigate such novel approaches in the context of a class of methods in computational mathematics, which appear at the core of many problems in computational science, large-scale data analytics, and machine learning.\r\n\r\nSpecifically, this project will focus on data-driven rather than compute-driven co-design of algorithms and architectures for the construction, approximation, and factorization of hierarchical matrices. The end-goal of the project is the design of a novel architecture, CISIT (for ``Computing In Situ and In Transit''), that specifically aims to address acceleration of both computation and data movement in the context of hierarchical matrices. CISIT will uniquely combine traditional general-purpose CPU and GPU cores with: (1) acceleration of core algorithmic primitives using custom hardware; (2) in-situ computing capabilities that will comprise both processing in or near main memory as well as computing within on-chip caches and memory close to the cores; (3) novel in-transit compute capabilities that will enable cutting down on and in many cases completely eliminating unnecessary roundtrip data transfers by processing of data transparently as it is transferred between main memory and local compute cores across the cache hierarchies. Upon success, CISIT will influence future architectural implementations.  Along with the research activities, an educational and dissemination program will be designed to communicate the results of this work to both students and researchers, as well as a more general audience of computational and application scientists.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "George",
   "pi_last_name": "Biros",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "George Biros",
   "pi_email_addr": "gbiros@gmail.com",
   "nsf_id": "000209886",
   "pi_start_date": "2017-09-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Lizy",
   "pi_last_name": "John",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Lizy K John",
   "pi_email_addr": "ljohn@ece.utexas.edu",
   "nsf_id": "000378662",
   "pi_start_date": "2017-09-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Andreas",
   "pi_last_name": "Gerstlauer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andreas Gerstlauer",
   "pi_email_addr": "gerstl@ece.utexas.edu",
   "nsf_id": "000520598",
   "pi_start_date": "2017-09-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121532",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "9229",
   "pgm_ref_txt": "RES IN UNDERGRAD INST-RESEARCH"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 800000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project was on the of codesign numerical algorithms and computer architectures to&nbsp; minimize time-to-solution and energy-to-solution. We targeted a broad range of problems in scientific and high performance computing. In the course of the project, we studied N-body, nearest-neighbor, hierarchical matrix, neural networks and multigrid methods.&nbsp; Examples of computer&nbsp; architectures considered in our studies, include multi-accelerator systems, embedded systems, and application specific integrated circuit systems. Below, we give a brief summary of our contributions.&nbsp;</p>\n<p><br />First, we introduced several novel algorithms for machine learning, image analysis, and modeling and simulation of physical systems. These algorithms targeted heterogeneous architectures, in particular with respect scalability to larger problem sizes and avoiding communication and data movement. We demonstrated excellent scalability and, in certain cases, orders of magnitude speedups over the state of the art.&nbsp; Second, we confirmed that codesign of algorithms and architectures is of critical importance to enable performance and resource utilization.&nbsp; We considered the problem of placing an accelerator&nbsp; in the memory hierarchy, accelerator initiation and actuation overheads, specialized caching policies, and efficient fusion of machine learning inference tasks on edge devices. Several of our results were in collaboration with industrial partners.&nbsp; This award partially funded the design and development of three software packages that are now open sourced and used by several groups.</p>\n<p><br />This award partially supported seven graduate students (including two students from underrepresented groups), two postdocs, and a number of undergraduate students through summer internships and senior design projects. The results were disseminated using peer-reviewed journals and conferences,&nbsp; several talks by students and faculty, and through interactions with our industrial collaborations.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/18/2021<br>\n\t\t\t\t\tModified by: George&nbsp;Biros</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1725743/1725743_10521516_1613659298661_nsfReport20-SPX-Y3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1725743/1725743_10521516_1613659298661_nsfReport20-SPX-Y3--rgov-800width.jpg\" title=\"ASIC placement for HPC workloads\"><img src=\"/por/images/Reports/POR/2021/1725743/1725743_10521516_1613659298661_nsfReport20-SPX-Y3--rgov-66x44.jpg\" alt=\"ASIC placement for HPC workloads\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Integration an Application Specific Integrated Circuit (ASIC) to accelerate a state-of-the-art Fast Multipole  Method (FMM).  FMM is widely used in applications and is representative example of many HPC workloads. Details can be found in [https://doi.org/10.1109/TPDS.2021.3056045].</div>\n<div class=\"imageCredit\">George Biros</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">George&nbsp;Biros</div>\n<div class=\"imageTitle\">ASIC placement for HPC workloads</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project was on the of codesign numerical algorithms and computer architectures to  minimize time-to-solution and energy-to-solution. We targeted a broad range of problems in scientific and high performance computing. In the course of the project, we studied N-body, nearest-neighbor, hierarchical matrix, neural networks and multigrid methods.  Examples of computer  architectures considered in our studies, include multi-accelerator systems, embedded systems, and application specific integrated circuit systems. Below, we give a brief summary of our contributions. \n\n\nFirst, we introduced several novel algorithms for machine learning, image analysis, and modeling and simulation of physical systems. These algorithms targeted heterogeneous architectures, in particular with respect scalability to larger problem sizes and avoiding communication and data movement. We demonstrated excellent scalability and, in certain cases, orders of magnitude speedups over the state of the art.  Second, we confirmed that codesign of algorithms and architectures is of critical importance to enable performance and resource utilization.  We considered the problem of placing an accelerator  in the memory hierarchy, accelerator initiation and actuation overheads, specialized caching policies, and efficient fusion of machine learning inference tasks on edge devices. Several of our results were in collaboration with industrial partners.  This award partially funded the design and development of three software packages that are now open sourced and used by several groups.\n\n\nThis award partially supported seven graduate students (including two students from underrepresented groups), two postdocs, and a number of undergraduate students through summer internships and senior design projects. The results were disseminated using peer-reviewed journals and conferences,  several talks by students and faculty, and through interactions with our industrial collaborations. \n\n\t\t\t\t\tLast Modified: 02/18/2021\n\n\t\t\t\t\tSubmitted by: George Biros"
 }
}