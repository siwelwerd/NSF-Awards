{
 "awd_id": "1703161",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Incorporating Biological-Motivated Circuit Motifs into Large-Scale Deep Neural Network Models of the Brain",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2020-09-30",
 "tot_intn_awd_amt": 524779.0,
 "awd_amount": 524779.0,
 "awd_min_amd_letter_date": "2017-08-16",
 "awd_max_amd_letter_date": "2017-08-16",
 "awd_abstract_narration": "This project studies the effects of incorporating, into deep neural networks for visual processing, several heretofore unincorporated features of biological visual cortical circuits. Deep neural networks are artificial circuits loosely inspired by the brain's cerebral cortex. Their abilities to solve complex problems, such as recognizing objects in visual scenes, have revolutionized artificial intelligence and machine learning in recent years. The hierarchy of layers in a deep network trained for visual object recognition also provides the best existing models of the hierarchy of areas in the visual cortex implicated in object recognition (the \"ventral stream\"). This project seeks to understand whether and how incorporating additional features of brain circuits may (1) improve machine learning performance, particularly on tasks that are more challenging than those typically studied; and (2) yield improved models of visual cortex. Improving the performance of deep networks would yield great benefits across wide swaths of society and industry that are impacted by advances in artificial intelligence. Improved models of visual cortex will advance understanding of cortical function, which may lead to significant further benefits for understanding normal mental functioning and perception and their potential enhancement, as well as mental illness and perceptual and cognitive deficits. \r\n\r\nDeep networks currently achieve their success using almost purely feedforward processing. Yet the visual cortical ventral stream that helped inspire deep networks also uses massive recurrent processing within each area as well as feedback connections from higher areas to lower areas and \"bypass\" connections from lower areas to areas multiple steps higher in the hierarchy. Deep networks also use \"neurons\" that can either excite or inhibit different neurons that they project to, whereas biological neurons are exclusively excitatory or inhibitory. This project will incorporate feedback and bypass connections into deep networks, as well as local recurrent processing in networks of separate excitatory and inhibitory neurons. Recent work by the investigators has shown how local recurrent processing explains a number of nonlinear visual cortical operations often summarized as \"normalization.\" Simple forms of normalization currently used in deep networks maintain activities in an appropriate dynamic range, but the biological forms of normalization involve interactions between different stimulus features and locations in determining neural responses, which may have important computational roles e.g. in parsing visual scenes. The performance of deep networks incorporating these features will be assayed on a variety of visual tasks and as models of ventral stream neural data and human psychophysical data, and compared to performance of existing deep net models.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Yamins",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Yamins",
   "pi_email_addr": "yamins@stanford.edu",
   "nsf_id": "000733845",
   "pi_start_date": "2017-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Board of Trustees of the Leland Stanford Junior University",
  "perf_str_addr": "450 Serra Mall, Bldg. 420",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052130",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 524779.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project we investigated the role of adding biologically-plausiable within-layer and long-range recurrence to convolutional neural networks (CNNs) that have typically been feedforward in nature.&nbsp; Specifically, we build Convolutional Recurrent Neural Networks (ConvRNNs), augmenting CNNs with recurrence both locally and globally.&nbsp;<br /><br />There were two main outcomes of our project.&nbsp; One was essentially an artificial intelligence (AI) outcome. We found that with a particular form of local recurrent circuit, we could improve performance on neural network benchmarks relative to feedforward networks of the same or even substantially larger depth.&nbsp; In other words, with networks containing many fewer units, we could achieve equal performance by trading off depth for local recurrent and long-range feedback connections.&nbsp;&nbsp;<br />In achieving this result, it was critical to use a particular form of local recurrent circuit that difference in several ways from the typical recurrent circuit structures such as Vanilla RNNs or LSTMs. Specifically, we found that it was important that the circuit both have gating (like the LSTM) but also that it should have an initializable \"zero state\" in which it doesnt' interfere with the operation of the overall feedforward backbone of the network.&nbsp;&nbsp;The other outcome was a neuroscience result, showing that these task-optimized ConvRNNs improved our ability to predict neural response dynamics.&nbsp; Specifically, we compared these networks to the times at which real images become \"solvable\" by neurons, e.g. at which decoding object category for specific images from neural responses meets the actual animal behavioral performance level. We found that the task-optimized ConvRNNs better matched empirically-measured object solution times than either shallow CNNs (which have biologically plausiable depth but aren't good at solving categorization tasks) or much deeper CNNs (which are good at task performance but are implausibly deep).&nbsp; The ConvRNNs appear to have hit the sweetspot between being biologically reasonable in depth, but good at solving tasks.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/12/2021<br>\n\t\t\t\t\tModified by: Daniel&nbsp;Yamins</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this project we investigated the role of adding biologically-plausiable within-layer and long-range recurrence to convolutional neural networks (CNNs) that have typically been feedforward in nature.  Specifically, we build Convolutional Recurrent Neural Networks (ConvRNNs), augmenting CNNs with recurrence both locally and globally. \n\nThere were two main outcomes of our project.  One was essentially an artificial intelligence (AI) outcome. We found that with a particular form of local recurrent circuit, we could improve performance on neural network benchmarks relative to feedforward networks of the same or even substantially larger depth.  In other words, with networks containing many fewer units, we could achieve equal performance by trading off depth for local recurrent and long-range feedback connections.  \nIn achieving this result, it was critical to use a particular form of local recurrent circuit that difference in several ways from the typical recurrent circuit structures such as Vanilla RNNs or LSTMs. Specifically, we found that it was important that the circuit both have gating (like the LSTM) but also that it should have an initializable \"zero state\" in which it doesnt' interfere with the operation of the overall feedforward backbone of the network.  The other outcome was a neuroscience result, showing that these task-optimized ConvRNNs improved our ability to predict neural response dynamics.  Specifically, we compared these networks to the times at which real images become \"solvable\" by neurons, e.g. at which decoding object category for specific images from neural responses meets the actual animal behavioral performance level. We found that the task-optimized ConvRNNs better matched empirically-measured object solution times than either shallow CNNs (which have biologically plausiable depth but aren't good at solving categorization tasks) or much deeper CNNs (which are good at task performance but are implausibly deep).  The ConvRNNs appear to have hit the sweetspot between being biologically reasonable in depth, but good at solving tasks. \n\n\t\t\t\t\tLast Modified: 04/12/2021\n\n\t\t\t\t\tSubmitted by: Daniel Yamins"
 }
}