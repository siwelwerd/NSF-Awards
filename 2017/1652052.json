{
 "awd_id": "1652052",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Active and Action-Centric Visual Understanding",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 549999.0,
 "awd_amount": 549999.0,
 "awd_min_amd_letter_date": "2017-02-23",
 "awd_max_amd_letter_date": "2020-07-17",
 "awd_abstract_narration": "This project develops technologies for visual semantic planning; the problem of producing ordered sequences of actions that change the current world state from what is depicted in a given image or video to the state defined by a query task. The project bridges the gap between current levels of image understanding and what is needed to actively understand the visual world to the extent that an agent can plan and perform tasks. The project develops the technology for a crucial next step in recognition: active and action-centric image understanding by semantic understanding of actions, their preconditions and effects, and visual planning.  Doing so empowers several applications in healthcare, prospective memory failure care, visually impaired care, elderly care, robotics, entertainment, and education.\r\n\r\nThis research addresses the visual planning problem that entails knowing what actions are, how they change the world state, and which sequences of actions change the current state to a desired one. Successful active understanding of images requires addressing several fundamental and challenging problems at the intersection of computer vision and artificial intelligence. The research is focused on the development of a framework for active visual understanding, new scalable algorithms for joint detection of actions and their arguments, new datasets and representations for actions' preconditions and effects, new algorithms for predicting the consequences of actions with intuitive laws of physics, and visual semantic planning. The developed framework is designed for active and action-centric image understanding by large-scale, semantic action recognition, modeling actions' preconditions and effects, predicting consequences of actions, and visual planning. These resources not only enable new research directions in computer vision, robotics, and AI, but also bring together some of the independent efforts across these disciplines.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ali",
   "pi_last_name": "Farhadi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ali Farhadi",
   "pi_email_addr": "afarhad2@gmail.com",
   "nsf_id": "000611236",
   "pi_start_date": "2017-02-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn Ave NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 124002.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 105844.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 320153.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project we explored the role of action and interaction in visual understanding and how to act and interact in visual environment. As part of this projects we developed families of platforms around THOR, including RoboTHOR, ProcTHOR, etc. These platforms are now play a significant role in our community as enablers for researchers in these areas. Our platforms have been downloaded for more than 700.000 time. &nbsp;We also released Objaverse, the dataset of 10million 3D objects to be offer to our community large scale datasets for 3D objects and hopefully see the same range of improvements we have observed when we scaled up our 2D image datasets. We have published several papers including outstanding paper award at NeurIPS 2023, and Best paper honorable mention at CVPR 2023.&nbsp;</p>\n<p>When interacting with a changing environment, agents should be able to adjust their models at inference time. As part of this project we introduced Neural Priming and showed significant improvemrns in visual recognition tasks, Uncertaintiy will remain a key piece of interaction: the expected outcome of actions might differ from an actual outcome (due to mechanical issues or environmental changes). We also developed algorithms that can plan with broken actions and can take into account changes in the precondition or postcondition of actions. &nbsp;</p><br>\n<p>\n Last Modified: 06/28/2024<br>\nModified by: Ali&nbsp;Farhadi</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project we explored the role of action and interaction in visual understanding and how to act and interact in visual environment. As part of this projects we developed families of platforms around THOR, including RoboTHOR, ProcTHOR, etc. These platforms are now play a significant role in our community as enablers for researchers in these areas. Our platforms have been downloaded for more than 700.000 time. We also released Objaverse, the dataset of 10million 3D objects to be offer to our community large scale datasets for 3D objects and hopefully see the same range of improvements we have observed when we scaled up our 2D image datasets. We have published several papers including outstanding paper award at NeurIPS 2023, and Best paper honorable mention at CVPR 2023.\n\n\nWhen interacting with a changing environment, agents should be able to adjust their models at inference time. As part of this project we introduced Neural Priming and showed significant improvemrns in visual recognition tasks, Uncertaintiy will remain a key piece of interaction: the expected outcome of actions might differ from an actual outcome (due to mechanical issues or environmental changes). We also developed algorithms that can plan with broken actions and can take into account changes in the precondition or postcondition of actions. \t\t\t\t\tLast Modified: 06/28/2024\n\n\t\t\t\t\tSubmitted by: AliFarhadi\n"
 }
}