{
 "awd_id": "1733860",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AitF: Mechanism Design and Machine Learning for Peer Grading",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 700000.0,
 "awd_amount": 716000.0,
 "awd_min_amd_letter_date": "2017-09-06",
 "awd_max_amd_letter_date": "2021-12-08",
 "awd_abstract_narration": "This project explores the design and analysis of peer grading technology.  A peer grading system is an online tool that collects student submissions, assigns review tasks to the students and graders, and aggregates reviews to produce assessments of both the submissions and the peer reviews.  The PIs have developed a prototype system and have collected preliminary evidence that suggests that peer review has important potential benefits:\r\n\r\n1.  Learning by reviewing: Students learn from critical assessment of other students' work.  In the PIs' prototype at Northwestern, 60% of the students reported that peer review helped them learn course material and 55% of the students reported that peer review helped them to prepare better homework solutions themselves.\r\n\r\n2.  Reduced grading staff: Peer grading reduces the grading load on course staff and allows for effective teaching with larger classes.  This is especially important currently, as interest in computer science classes increases at a faster pace than teaching resources.  In the PIs' prototype at Northwestern, the course staff graded 1/5 of the student submissions.\r\n\r\n3.  Promptness of feedback: Reduced teacher grading enables prompt feedback to students.  In the PIs' prototype at Northwestern, peer reviews were available within three days and final assessment of both the submission and peer reviews were available within five days.  Prior to introducing peer review, assessments took one to two weeks.\r\n\r\nA peer grading system is comprised of three main components:\r\n\r\n1.  The review matching algorithm determines which peers should review which submissions and which submissions should be reviewed by the teacher.\r\n\r\n2.  The submission grading algorithm aggregates the reviews of the peers and the submissions and assigns grades to the submissions.\r\n\r\n3.  The review grading algorithm compares the peer reviews with the teacher reviews and assigns grades to the peer reviews.  Without this algorithm, peers may not put effort into providing quality reviews, and the reviews will be neither accurate for grading nor beneficial for the peer.\r\n\r\nThe details of these algorithms are crucial for the proper working of the peer review system.  A main research effort of this project is to identify the algorithms to use for each of these components.  The review matching algorithm affects the accuracy of the subsequent grading algorithms and the grading load of the teacher.  The submission grading algorithm determines which peer reviews are accurate and which are inaccurate and uses this understanding to assign grades to the submissions that are representative of the submission quality.  The review grading algorithm incentivizes the peers to put in sufficient effort to determine whether a submission is good or bad and it is calibrated so that good reviews and bad reviews get the appropriate review grades.  \r\n\r\nThe PIs have implemented prototypes of these algorithms as part of a peer grading system that has been prototyped in Northwestern computer science classes.  However, the space of possible algorithms is large and the PIs' work on the prototype has yet to determine the algorithms that combine to give the best education outcomes.  A main focus of this project will be improving the understanding of which algorithms lead to the best education outcomes.\r\n\r\nTheoretical work in algorithms and machine learning provides a starting point for the project's study of good algorithms for peer grading systems.  A key endeavor of the project is translating and applying these theoretical algorithms to the peer grading domain.  As one example, proper scoring rules are a natural approach for grading the peer reviews.  However, test runs of the PIs' prototype implementation suggest that these rules might not be so good in practice.  Both new models and algorithms are needed in theory, and these new algorithms need to work in practice.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Hartline",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Jason D Hartline",
   "pi_email_addr": "hartline@eecs.northwestern.edu",
   "nsf_id": "000500650",
   "pi_start_date": "2017-09-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Douglas",
   "pi_last_name": "Downey",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Douglas C Downey",
   "pi_email_addr": "dougd@allenai.org",
   "nsf_id": "000534948",
   "pi_start_date": "2017-09-06",
   "pi_end_date": "2021-12-08"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Eleanor",
   "pi_last_name": "O'Rourke",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Eleanor M O'Rourke",
   "pi_email_addr": "eorourke@northwestern.edu",
   "nsf_id": "000741884",
   "pi_start_date": "2017-09-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "2133 Sheridan Rd",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602083118",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "723900",
   "pgm_ele_name": "Algorithms in the Field"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7932",
   "pgm_ref_txt": "COMPUT GAME THEORY & ECON"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 700000.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explored the design and analysis of peer grading technology. A peer grading system is an online tool that collects student submissions, assigns review tasks to the students and graders, and aggregates reviews to produce assessments of both the submissions and the peer reviews. A major outcome of the project is the development of an open-source platform for peer grading.</p>\n<p>The main research findings of the project pertain to automated grading methods in such a peer grading platform.&nbsp; There are two main grading algorithms:</p>\n<p>1. The submission grading algorithm aggregates the reviews of the peers and the submissions and assigns grades to the submissions.</p>\n<p>2. The review grading algorithm compares the peer reviews with the teacher reviews and assigns grades to the peer reviews. Without this algorithm, peers may not put effort into providing quality reviews, and the reviews will be neither accurate for grading nor beneficial for the peer.</p>\n<p>The submission grading algorithm determines which peer reviews are accurate and which are inaccurate and uses this understanding to assign grades to the submissions that are representative of the submission quality.&nbsp; The prior literature on peer grading measures accuracy of a peer by comparing the peers scores to others' scores for the same submissions.&nbsp; In this project, a natural language processing method was developed to assess peer accuracy.&nbsp; This method analyzed the textual feedback of the peer reviews to determine whether or not the peer was well informed about the submission.&nbsp; This method was evaluated on data collected from the peer grading platform and improved on the error of the best score-based method by six percent.</p>\n<p>The review grading algorithm incentivizes the peers to put in sufficient effort to determine whether a submission is good or bad.&nbsp; A straightforward approach of using proper scoring rules to grade reviews, by comparing review scores of a peer to review scores of the instructor and penalizing peer scores that are far from the instructor scores.&nbsp; A preliminary empirical study showed that the incentives offered by classical scoring rules may be insufficient.&nbsp; The project formulated the problem of optimizing scoring rules for binary effort.&nbsp; The objective of such an optimization is to maximize the difference in expected scores for a peer that puts in effort versus a peer who does not put in effort and instead reports the class average.</p>\n<p>In this framework for optimizing scoring rules, the project has established a number of significant results.&nbsp; For these results scores collected from peers take the form of a score for each of several rubric elements.&nbsp;&nbsp;</p>\n<p>1. The optimal scoring rule for a single rubric element rewards the peer for identifying whether the submission is above or below average.&nbsp;&nbsp;</p>\n<p>2. A near optimal scoring rule for multiple rubric element rewards the peer according to the most surprising element.&nbsp; Here, \"most surprising\" means deviating most significantly from average.</p>\n<p>Importantly these simple to state and implement findings are potentially easy to make use of in automated grading of peer reviews.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/11/2023<br>\n\t\t\t\t\tModified by: Jason&nbsp;D&nbsp;Hartline</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project explored the design and analysis of peer grading technology. A peer grading system is an online tool that collects student submissions, assigns review tasks to the students and graders, and aggregates reviews to produce assessments of both the submissions and the peer reviews. A major outcome of the project is the development of an open-source platform for peer grading.\n\nThe main research findings of the project pertain to automated grading methods in such a peer grading platform.  There are two main grading algorithms:\n\n1. The submission grading algorithm aggregates the reviews of the peers and the submissions and assigns grades to the submissions.\n\n2. The review grading algorithm compares the peer reviews with the teacher reviews and assigns grades to the peer reviews. Without this algorithm, peers may not put effort into providing quality reviews, and the reviews will be neither accurate for grading nor beneficial for the peer.\n\nThe submission grading algorithm determines which peer reviews are accurate and which are inaccurate and uses this understanding to assign grades to the submissions that are representative of the submission quality.  The prior literature on peer grading measures accuracy of a peer by comparing the peers scores to others' scores for the same submissions.  In this project, a natural language processing method was developed to assess peer accuracy.  This method analyzed the textual feedback of the peer reviews to determine whether or not the peer was well informed about the submission.  This method was evaluated on data collected from the peer grading platform and improved on the error of the best score-based method by six percent.\n\nThe review grading algorithm incentivizes the peers to put in sufficient effort to determine whether a submission is good or bad.  A straightforward approach of using proper scoring rules to grade reviews, by comparing review scores of a peer to review scores of the instructor and penalizing peer scores that are far from the instructor scores.  A preliminary empirical study showed that the incentives offered by classical scoring rules may be insufficient.  The project formulated the problem of optimizing scoring rules for binary effort.  The objective of such an optimization is to maximize the difference in expected scores for a peer that puts in effort versus a peer who does not put in effort and instead reports the class average.\n\nIn this framework for optimizing scoring rules, the project has established a number of significant results.  For these results scores collected from peers take the form of a score for each of several rubric elements.  \n\n1. The optimal scoring rule for a single rubric element rewards the peer for identifying whether the submission is above or below average.  \n\n2. A near optimal scoring rule for multiple rubric element rewards the peer according to the most surprising element.  Here, \"most surprising\" means deviating most significantly from average.\n\nImportantly these simple to state and implement findings are potentially easy to make use of in automated grading of peer reviews. \n\n\t\t\t\t\tLast Modified: 03/11/2023\n\n\t\t\t\t\tSubmitted by: Jason D Hartline"
 }
}