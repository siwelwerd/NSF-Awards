{
 "awd_id": "1664554",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CRII: CHS: Human-Robot Collaboration in Special Education: A Robot that Learns Service Delivery from Teachers' Demonstrations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2016-09-13",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 126848.0,
 "awd_amount": 126849.0,
 "awd_min_amd_letter_date": "2017-01-23",
 "awd_max_amd_letter_date": "2022-04-22",
 "awd_abstract_narration": "This project focuses on improving the quality of life for children with special needs through the use of robot technologies.  In 2011, 13% of children enrolled in schools in the United States required some form of special education; in 2010, 1 in 68 children in the U.S. was diagnosed with some form of autism spectrum disorder (ASD), a developmental condition that almost always triggers the need for special education.  A significant amount of robotics research over the past decade has indicated that many children with ASD have a strong interest in robotic toys, and suggested that robots are potentially valuable tools in special education for these children.  But the clinical community and educators, who have the authority to approve the use of such devices in special education, remain unconvinced.  One reason for this gap is that the robotics research in this domain has not had a strong focus on effectiveness.  The PI recently completed a preliminary study involving children with ASD that confirmed the clinical feasibility of special education service delivery through human-robot interaction (HRI).  Her goal in the current project is to establish a research program that builds on these findings to develop a framework, including methods and algorithms, for robot-mediated special education service delivery to children with ASD and other disorders of a similar nature.  The PI's vision is to lay the foundations for a future where educators deliver services in collaboration with robots, thereby making a significant impact by increasing the effectiveness of special education services while reducing the burden on the educator during the service delivery process.\r\n\r\nThis research proposes a \"Learning from Demonstration\" (LfD) approach where the robot learns service delivery from a series of demonstrations by human educators in real special education scenarios.  When the human educator has sufficient confidence in the robot's ability, s/he will initiate a collaborative service delivery scenario in which the robot autonomously delivers some steps of a specific educational service but  asks for the educator's assistances in cases where it has low confidence in its perception and/or planned actions.  A multi-modal activity recognition framework will provide information about the responses and activities of children with special needs as they engage with the robot.  To facilitate educator/robot interaction, a focus group will be organized with educators and clinicians to accommodate their needs and expectations in the design of the control interface for the special education service delivery robot.  An HRI study will test the effectiveness of the proposed framework in real-world settings.  The design and implementation of robots that can deliver special education services in collaboration with a human educator will open up many opportunities to advance the state of the art in robot learning (especially high-level concept learning), robot vision and activity recognition, and HRI.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Momotaz",
   "pi_last_name": "Begum",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Momotaz Begum",
   "pi_email_addr": "mbegum@cs.unh.edu",
   "nsf_id": "000673054",
   "pi_start_date": "2017-01-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of New Hampshire",
  "inst_street_address": "51 COLLEGE RD",
  "inst_street_address_2": "BLDG 107",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NH",
  "inst_state_name": "New Hampshire",
  "inst_phone_num": "6038622172",
  "inst_zip_code": "038242620",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NH01",
  "org_lgl_bus_name": "UNIVERSITY SYSTEM OF NEW HAMPSHIRE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GBNGC495XA67"
 },
 "perf_inst": {
  "perf_inst_name": "University of New Hampshire",
  "perf_str_addr": "",
  "perf_city_name": "Durham",
  "perf_st_code": "NH",
  "perf_st_name": "New Hampshire",
  "perf_zip_code": "038242620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NH01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "026Y00",
   "pgm_ele_name": "CRII CISE Research Initiation"
  },
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 22900.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 103949.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The objective of this project was to design a robot learning framework that can learn goal-directed human activities from lay users' demonstrations. A target use case for the proposed LfD framework was special education where the robot will learn tasks from a therapist who has no knowledge of robotics or programming. At the first stage the project designed a framework that can learn simple motion related to therapeutic exercises - such as pronation, supination - using the motion information captured through sensors such as inertial measurement units. The effectiveness of the framework was limited by the accuracy of the sensor data.&nbsp; At the second stage, the project designed a framework to learn structures of simple turn-taking tasks after observing it through a camera. We tested the framework to learn an ABA-style structured intervention for children with developmental delays. Although the designed framework showed the potential of learning-enabled robots in practical applications such as special education classrooms, the technology however is far from being ready for deployment. This project helped us to idenfity some of the major technial roadblocks toward deploying learning-enabled robots in the field such as improving the perceptual ability of the robot, learning from errorneous demonstrations, continual learning. The team is currently exploring many of these challenges through various other federal grants.&nbsp;&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/22/2023<br>\n\t\t\t\t\tModified by: Momotaz&nbsp;Begum</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe objective of this project was to design a robot learning framework that can learn goal-directed human activities from lay users' demonstrations. A target use case for the proposed LfD framework was special education where the robot will learn tasks from a therapist who has no knowledge of robotics or programming. At the first stage the project designed a framework that can learn simple motion related to therapeutic exercises - such as pronation, supination - using the motion information captured through sensors such as inertial measurement units. The effectiveness of the framework was limited by the accuracy of the sensor data.  At the second stage, the project designed a framework to learn structures of simple turn-taking tasks after observing it through a camera. We tested the framework to learn an ABA-style structured intervention for children with developmental delays. Although the designed framework showed the potential of learning-enabled robots in practical applications such as special education classrooms, the technology however is far from being ready for deployment. This project helped us to idenfity some of the major technial roadblocks toward deploying learning-enabled robots in the field such as improving the perceptual ability of the robot, learning from errorneous demonstrations, continual learning. The team is currently exploring many of these challenges through various other federal grants.  \n\n\t\t\t\t\tLast Modified: 09/22/2023\n\n\t\t\t\t\tSubmitted by: Momotaz Begum"
 }
}