{
 "awd_id": "1704527",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SaTC: CORE: Large: Collaborative: Accountable Information Use: Privacy and Fairness in Decision-Making Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925177",
 "po_email": "asquicci@nsf.gov",
 "po_sign_block_name": "Anna Squicciarini",
 "awd_eff_date": "2017-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 899999.0,
 "awd_amount": 949983.0,
 "awd_min_amd_letter_date": "2017-05-18",
 "awd_max_amd_letter_date": "2021-06-09",
 "awd_abstract_narration": "Increasingly, decisions and actions affecting people's lives are determined by automated systems processing personal data.\u00a0 Excitement about these systems has been accompanied by serious concerns about their opacity and the threats that they pose to privacy, fairness, and other values.\u00a0 Recognizing these concerns, the investigators seek to make real-world automated decision-making systems accountable for privacy and fairness by enabling them to detect and explain violations of these values.\u00a0 The technical work is informed by, and applied to, online advertising, healthcare, and criminal justice, in collaboration with and as advised by domain experts.\u00a0\r\n\r\nAddressing privacy and fairness in decision systems requires providing formal definitional frameworks and practical system designs.\u00a0 The investigators provide new notions of privacy and fairness that deal with both protected information itself and proxies for it, while handling context-dependent, normative definitions of violations.\u00a0 A fundamental tension they address pits the access given to auditors of a system against the system owners' intellectual property protections and the confidentiality of the personal data used by the system.\u00a0 The investigators decompose such auditing into stages, where the level of access granted to an auditor is increased when potential (but not explainable) violations of privacy or fairness are detected. Workshops and public releases of code and data amplify the investigators' interactions with policy makers and other stakeholders.  Their partnerships with outreach organizations encourage diversity.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Helen",
   "pi_last_name": "Nissenbaum",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Helen Nissenbaum",
   "pi_email_addr": "hn288@cornell.edu",
   "nsf_id": "000471690",
   "pi_start_date": "2017-05-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Ristenpart",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas Ristenpart",
   "pi_email_addr": "ristenpart@cornell.edu",
   "nsf_id": "000573673",
   "pi_start_date": "2017-05-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell Tech",
  "perf_str_addr": "111 8th Ave, Ste 302",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100115204",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7556",
   "pgm_ref_txt": "CONFERENCE AND WORKSHOPS"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 100887.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 113631.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 300127.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 308148.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 127190.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The core aims of this project when awarded in 2017 were to make real-world systems accountable for privacy and fairness by enabling them to detect and explain violations. Little did our team of collaborators realize that the brewing of excitement over systems of automated decision and control was about to explode into a massive storm as AI, generative AI, and LLMs burst onto the public stage. Instead of ameliorating the worries about transparency, accountability, fairness, and privacy, however, these giant steps in AI technologies vastly amplified them. Accordingly, it was impossible not to expand the scope of our work to these extended capabilities, and, in the case of Cornell Tech, to respond particularly to platform accountability in the face of opaque models and privacy as contextual integrity, which, increasingly emerged as one of the few robust approaches to privacy in the face of increasingly frenetic data stockpiling. <br /><br />On ethics front, primarily led by PI Dr. Helen Nissenbaum<br /><br />On the issue of accountability, the project supported a multidisciplinary research effort on accountability in an \"algorithmic society,\" modeled after Dr. Nissenbaum's prior article on accountability in a \"computerized society.\" It warns that similar barriers exist today as then, some further exacerbated by the nature of the industry as well as the underlying technologies of data-driven algorithms. This work, which combined ethical analysis with a deep level of understanding of the ML pipeline, was published at the FAccT 2022 Conference. <br /><br />On the issue of transparency, completed projects (with Ph.D. advisee, Ben Laufer) included an ethical examination of strategic behavior when confronting algorithm-driven evaluation and decision systems and hidden values in the practices of optimization, at the foundation of many decision systems. One project drew attention to the problem of replacing traditional trust mechanisms that have evolved over centuries to achieve knowledge, for example, in book and news publishing, with data-driven algorithmic systems.<br /><br />On the issue or privacy, the project focused on developing theory and applications of contextual integrity (CI). Some of significant outcomes of this research include: a research study of sociotechnical systems claimed to be \"privacy enhancing\", with colleagues Dr. Vitaly Shmatikov and Dr. Kirsten Martin; an investigation (with Professors Kathy Strandburg and Salome Viljoen) of fertility apps reveals a phenomenon we have called \"regulatory dodge\" use personal data to evade the sectoral regulation one would expect would cover their practices; a study of privacy norms in online groups, via an interview-based study and a representative survey; a study of people&rsquo;s location privacy preferences, through surveys informed by contextual integrity (in collaboration with Dr. Kirsten Martin); studies using contextual integrity to analyze the vagueness, ambiguity, and complexity on privacy policies; and design specifications for VACCINE: a data leakage detection approach at the system level drawing on privacy as contextual integrity. <br /><br />On the technical front, primarily led by co-PI Dr. Tom Ristenpart<br /><br />We explored the ability of malicious machine learning algorithms to&#8232;purposefully exfiltrate sensitive training data via the parameters of models.&#8232;For example, our results showed that deep learning models have sufficient capacity&#8232;to both do well on standard benchmark tasks as well as simultaneously memorize sensitive information. That sensitive information can be later recovered by an &#8232;adversary given black-box access to the model (i.e., a prediction API).&#8232;This result was published in ACM CCS 2017. <br /><br />We worked on using ML techniques to study harassment against politicians online, using a new dataset that we collected consisting of ~400k Twitter users&rsquo; 1.2 million replies to political candidates during the 2018 midterm election in the United States. We were able to characterize observed properties of the users who interact in an adversarial manner with politicians, as well as understand differences in harassment behaviors as a function of the target. We developed new domain-specific algorithms for detecting adversarial content. This work resulted in papers at ICWSM and CHI 2020.<br /><br />We then built off this prior work on abuse/harassment in public settings to start thinking about abuse in private communication channels.&nbsp; We developed a privacy-preserving protocol to help scale detection of abusive content, so that private messaging clients can notify users should they be sent problematic content (e.g., misinformation). The work focused on how to scale privacy-preserving similarity checking to very large databases, via a new technique called similarity-based bucketization. The paper appeared at USENIX Security 2022. <br /><br />PhD Ben Laufer (with Niko Grupen), with guidance from PI Ristenpart, pursued research on the security and privacy implications of centralized reporting platforms like 311 and 911. A talk was given by Ben at an ICML workshop in 2022.<br /><br />Broader impacts<br />PI Nissenbaum and several trainees have consulted with commercial as well as governmental actors in their pursuit of privacy by design and privacy regulation. The grant helped support many academic research articles that were widely disseminated and training of PhD students and postdocs.</p><br>\n<p>\n Last Modified: 12/08/2024<br>\nModified by: Thomas&nbsp;Ristenpart</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe core aims of this project when awarded in 2017 were to make real-world systems accountable for privacy and fairness by enabling them to detect and explain violations. Little did our team of collaborators realize that the brewing of excitement over systems of automated decision and control was about to explode into a massive storm as AI, generative AI, and LLMs burst onto the public stage. Instead of ameliorating the worries about transparency, accountability, fairness, and privacy, however, these giant steps in AI technologies vastly amplified them. Accordingly, it was impossible not to expand the scope of our work to these extended capabilities, and, in the case of Cornell Tech, to respond particularly to platform accountability in the face of opaque models and privacy as contextual integrity, which, increasingly emerged as one of the few robust approaches to privacy in the face of increasingly frenetic data stockpiling. \n\nOn ethics front, primarily led by PI Dr. Helen Nissenbaum\n\nOn the issue of accountability, the project supported a multidisciplinary research effort on accountability in an \"algorithmic society,\" modeled after Dr. Nissenbaum's prior article on accountability in a \"computerized society.\" It warns that similar barriers exist today as then, some further exacerbated by the nature of the industry as well as the underlying technologies of data-driven algorithms. This work, which combined ethical analysis with a deep level of understanding of the ML pipeline, was published at the FAccT 2022 Conference. \n\nOn the issue of transparency, completed projects (with Ph.D. advisee, Ben Laufer) included an ethical examination of strategic behavior when confronting algorithm-driven evaluation and decision systems and hidden values in the practices of optimization, at the foundation of many decision systems. One project drew attention to the problem of replacing traditional trust mechanisms that have evolved over centuries to achieve knowledge, for example, in book and news publishing, with data-driven algorithmic systems.\n\nOn the issue or privacy, the project focused on developing theory and applications of contextual integrity (CI). Some of significant outcomes of this research include: a research study of sociotechnical systems claimed to be \"privacy enhancing\", with colleagues Dr. Vitaly Shmatikov and Dr. Kirsten Martin; an investigation (with Professors Kathy Strandburg and Salome Viljoen) of fertility apps reveals a phenomenon we have called \"regulatory dodge\" use personal data to evade the sectoral regulation one would expect would cover their practices; a study of privacy norms in online groups, via an interview-based study and a representative survey; a study of peoples location privacy preferences, through surveys informed by contextual integrity (in collaboration with Dr. Kirsten Martin); studies using contextual integrity to analyze the vagueness, ambiguity, and complexity on privacy policies; and design specifications for VACCINE: a data leakage detection approach at the system level drawing on privacy as contextual integrity. \n\nOn the technical front, primarily led by co-PI Dr. Tom Ristenpart\n\nWe explored the ability of malicious machine learning algorithms to&#8232;purposefully exfiltrate sensitive training data via the parameters of models.&#8232;For example, our results showed that deep learning models have sufficient capacity&#8232;to both do well on standard benchmark tasks as well as simultaneously memorize sensitive information. That sensitive information can be later recovered by an &#8232;adversary given black-box access to the model (i.e., a prediction API).&#8232;This result was published in ACM CCS 2017. \n\nWe worked on using ML techniques to study harassment against politicians online, using a new dataset that we collected consisting of ~400k Twitter users 1.2 million replies to political candidates during the 2018 midterm election in the United States. We were able to characterize observed properties of the users who interact in an adversarial manner with politicians, as well as understand differences in harassment behaviors as a function of the target. We developed new domain-specific algorithms for detecting adversarial content. This work resulted in papers at ICWSM and CHI 2020.\n\nWe then built off this prior work on abuse/harassment in public settings to start thinking about abuse in private communication channels. We developed a privacy-preserving protocol to help scale detection of abusive content, so that private messaging clients can notify users should they be sent problematic content (e.g., misinformation). The work focused on how to scale privacy-preserving similarity checking to very large databases, via a new technique called similarity-based bucketization. The paper appeared at USENIX Security 2022. \n\nPhD Ben Laufer (with Niko Grupen), with guidance from PI Ristenpart, pursued research on the security and privacy implications of centralized reporting platforms like 311 and 911. A talk was given by Ben at an ICML workshop in 2022.\n\nBroader impacts\nPI Nissenbaum and several trainees have consulted with commercial as well as governmental actors in their pursuit of privacy by design and privacy regulation. The grant helped support many academic research articles that were widely disseminated and training of PhD students and postdocs.\t\t\t\t\tLast Modified: 12/08/2024\n\n\t\t\t\t\tSubmitted by: ThomasRistenpart\n"
 }
}