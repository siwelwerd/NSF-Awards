{
 "awd_id": "1740225",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "E2CDA: Type I: Collaborative Research: Energy-Efficient Artificial Intelligence with Binary RRAM and Analog Epitaxial Synaptic Arrays",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2017-09-15",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 579066.0,
 "awd_amount": 579066.0,
 "awd_min_amd_letter_date": "2017-09-11",
 "awd_max_amd_letter_date": "2019-08-21",
 "awd_abstract_narration": "In recent years, deep learning and artificial neural networks have been very successful in large-scale recognition and classification tasks, some even surpassing human-level accuracy. However, state-of-the-art deep learning algorithms tend to present very large network models, which poses significant challenges for hardware, especially for memory. Emerging resistive devices have been proposed as an alternative solution for weight storage and parallel neural computing, but severe limitations still exist for applying resistive random access memories (RRAMs) for practical large-scale neural computing. This proposal targets on addressing limitations in resistive device based neural computing through novel device engineering, new bitcell designs, new neuron circuits, energy-aware architecture, and a new circuit-level benchmark simulator. A successful completion of this research is likely to have consequences to our society, enabling wide adoption of dense and energy-efficient intelligent hardware to power-/area-constrained local mobile/wearable devices. Furthermore, a self-learning chip that learns in near real-time and consumes very low-power can be integrated in smart biomedical devices, personalizing healthcare. This project will have a strong effort on integrating the research outcomes with education and outreach through summer outreach programs for high school students, undergraduate/graduate student training, and organization of tutorials and workshops at conferences for knowledge dissemination.\r\n\r\nThe proposal will perform innovative and interdisciplinary research to address many limitations in today?s resistive device based neural computing and make a leap progress towards energy-efficient intelligent computing. Severe limitations of applying resistive random access memories (RRAMs) for practical large-scale neural computing include: (1) device-level non-idealities, e.g., non-linearity, variability, selector, and endurance, (2) inefficiency in representing negative weights and neurons, and (3) limited demonstration on simpler networks, instead of cutting-edge convolutional and recurrent neural networks. To address these limitations, novel technologies from devices to architectures will be investigated. First, new bitcell circuits will be designed for today's binary resistive devices, efficiently mapping XNOR functionality with (+1, -1) weights and neurons. Second, a novel epitaxial resistive device (EpiRAM) that exhibits many idealistic properties will be investigated, including linear programming for analog weights, suppressed variability, self-selectivity, and high endurance. Third, new neuron circuits will be explored for integration with new resistive devices for feedforward/feedback deep neural networks. Finally, new data-mapping techniques that efficiently map state-of-the-art deep neural networks onto the hardware framework with RRAM arrays will be developed, and the overall energy-efficiency will be verified with a new benchmark simulator ?NeuroSim?. With vertical innovations across material, device, circuit and architecture, tremendous potential and research needs will be pursued towards energy-efficient artificial intelligence in ubiquitous resource-constrained hardware systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jae-sun",
   "pi_last_name": "Seo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jae-sun Seo",
   "pi_email_addr": "js3528@cornell.edu",
   "nsf_id": "000929203",
   "pi_start_date": "2017-09-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shimeng",
   "pi_last_name": "Yu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shimeng Yu",
   "pi_email_addr": "shimeng.yu@ece.gatech.edu",
   "nsf_id": "000656063",
   "pi_start_date": "2017-09-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "PO Box 876011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852876011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "015Y00",
   "pgm_ele_name": "Energy Efficient Computing: fr"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 193022.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 193022.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 193022.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project investigated novel RRAM device/bitcell/array design and system integration for energy-efficient artificial intelligence. First, for binary RRAM, we designed new bitcell circuits, architectures, and accelerators. Second, for analog epitaxial RRAM, we investigated new materials and devices and demonstrated successful array operation. Third, on the architecture and algorithm side, we mapped various deep neural networks (DNNs) onto RRAM based accelerators and developed algorithms that are aware of RRAM characteristics. Fourth, we developed NeuroSim, an open-source benchmark simulator, which can use conventional or emerging new devices? characteristics towards obtaining the corresponding DNN hardware?s projected accuracy and energy values. For each of these four research tasks, the project outcomes are described below.</p>\n<p>&nbsp;</p>\n<p>First, we experimented with binary HfO<sub>2</sub> RRAM for in-memory computing. We designed a prototype chip that monolithically integrated 90nm CMOS and RRAM arrays and performed RRAM based in-memory computing (IMC) for binary multiply-and-accumulate (MAC) functions. To address the low area-efficiency concern due to the peripheral circuits, we investigated using higher low resistance state (LRS) to lower current and reduce the column multiplexer size, while the RRAM on/off ratio and stability are lower. Also, we used input-splitting scheme that binarizes crossbar partial sum through special training techniques, and this enhanced the noise tolerance and enables lower power supply operation for the RRAM IMC chip. Overall, we achieved a high energy efficiency of up to 136 TOPS/W. Subsequently, we investigated multi-level RRAM, by re-purposing the same RRAM testchip for IMC with 2-bit weights. We programmed 2T2R cells in pairs of (G<sub>LOW </sub>, G<sub>HIGH</sub>) and (G<sub>HIGH</sub>?1/3<sub> </sub>, G<sub>HIGH</sub>?2/3), and the multi-level RRAM cells have been characterized for &gt;100 hours with respect to relaxation and temperature variation. By using 2-bit weights, we achieved 2.8-5.3% DNN accuracy improvement for iso-area, or achieved 2X reduction in the RRAM array area for iso-accuracy.</p>\n<p>&nbsp;</p>\n<p>Second, we worked on analog epitaxial RRAM called epiRAM with new materials and devices. We developed the epiRAM device with single crystal SiGe as switching medium and silver as active metal electrode. The silver movement is confined in the naturally formed one dimensional dislocation channels. As a result, very good spatial and temporal uniformity has been achieved. This unique device also provided very fine linear analog switching behaviors, fitting for neural network training tasks. One of the issues with this device was the retention at low conductance states because of the poor interaction between Ag and silicon. We further found that applying AgCu alloy as active metal could regulate and stabilize the silver movement, significantly improving retention and switching uniformity. With this new alloy material, we could demonstrate array level parallel operations in a 32x32 0T1R array for date storage and image processing applications.</p>\n<p>&nbsp;</p>\n<p>Third, we developed new architectures and algorithms for RRAM based in-memory computing. On the architecture side, we presented a RRAM-based recurrent neural network (RNN) accelerator that consists of RRAM crossbar, special function unit and element-wise multiplier. By employing a RNN friendly pipeline, 79X higher computing efficiency was achieved compared to GPU designs. On the algorithm side, we presented RRAM device variation aware (DVA) algorithms towards achieving high inference accuracy with RRAM in-memory computing hardware. Understanding that LRS and HRS device conductance has variation, we added such noise during DNN training, to find an optimal point that is more stable amid noise/variation. We evaluated DVA algorithm with the RRAM testchip measurement data, and the DVA showed noticeable accuracy improvement, especially for high noise.</p>\n<p>&nbsp;</p>\n<p>Fourth, we developed NeuroSim, a circuit-level benchmark simulator that estimates the area, latency, dynamic energy and leakage power to facilitate the design space exploration of neuro-inspired architectures with mainstream and emerging device technologies. NeuroSim provides flexible interface and a wide variety of design options at the circuit and device level. Therefore, NeuroSim can be used by neural networks as a supporting tool to provide circuit-level performance evaluation. With NeuroSim, an integrated framework can be built with hierarchical organization from the device level (synaptic device properties) to the circuit level (array architectures) and then to the algorithm level (neural network topology), enabling instruction-accurate evaluation on the learning/inference accuracy as well as the circuit-level performance metrics at the run-time of learning/inference. NeuroSim has been publicly available and actively used by many users from both academia and industry.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/04/2022<br>\n\t\t\t\t\tModified by: Jae-Sun&nbsp;Seo</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1740225/1740225_10521695_1641283548723_XNOR_RRAM_chip--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1740225/1740225_10521695_1641283548723_XNOR_RRAM_chip--rgov-800width.jpg\" title=\"Binary RRAM prototype chip design and operation\"><img src=\"/por/images/Reports/POR/2022/1740225/1740225_10521695_1641283548723_XNOR_RRAM_chip--rgov-66x44.jpg\" alt=\"Binary RRAM prototype chip design and operation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Schematic and prototype chip for binary RRAM based in-memory computing are shown, including the RRAM array and CMOS peripheral circuits that are monolithically integrated.</div>\n<div class=\"imageCredit\">IEEE Transactions on Electron Devices</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jae-Sun&nbsp;Seo</div>\n<div class=\"imageTitle\">Binary RRAM prototype chip design and operation</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1740225/1740225_10521695_1641283667272_NeuroSim--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1740225/1740225_10521695_1641283667272_NeuroSim--rgov-800width.jpg\" title=\"NeuroSim benchmark simulator\"><img src=\"/por/images/Reports/POR/2022/1740225/1740225_10521695_1641283667272_NeuroSim--rgov-66x44.jpg\" alt=\"NeuroSim benchmark simulator\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">NeuroSim, a circuit-level benchmark simulator, estimates the area, latency, dynamic energy and leakage power to facilitate the design space exploration of neuro-inspired architectures with mainstream and emerging device technologies.</div>\n<div class=\"imageCredit\">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jae-Sun&nbsp;Seo</div>\n<div class=\"imageTitle\">NeuroSim benchmark simulator</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project investigated novel RRAM device/bitcell/array design and system integration for energy-efficient artificial intelligence. First, for binary RRAM, we designed new bitcell circuits, architectures, and accelerators. Second, for analog epitaxial RRAM, we investigated new materials and devices and demonstrated successful array operation. Third, on the architecture and algorithm side, we mapped various deep neural networks (DNNs) onto RRAM based accelerators and developed algorithms that are aware of RRAM characteristics. Fourth, we developed NeuroSim, an open-source benchmark simulator, which can use conventional or emerging new devices? characteristics towards obtaining the corresponding DNN hardware?s projected accuracy and energy values. For each of these four research tasks, the project outcomes are described below.\n\n \n\nFirst, we experimented with binary HfO2 RRAM for in-memory computing. We designed a prototype chip that monolithically integrated 90nm CMOS and RRAM arrays and performed RRAM based in-memory computing (IMC) for binary multiply-and-accumulate (MAC) functions. To address the low area-efficiency concern due to the peripheral circuits, we investigated using higher low resistance state (LRS) to lower current and reduce the column multiplexer size, while the RRAM on/off ratio and stability are lower. Also, we used input-splitting scheme that binarizes crossbar partial sum through special training techniques, and this enhanced the noise tolerance and enables lower power supply operation for the RRAM IMC chip. Overall, we achieved a high energy efficiency of up to 136 TOPS/W. Subsequently, we investigated multi-level RRAM, by re-purposing the same RRAM testchip for IMC with 2-bit weights. We programmed 2T2R cells in pairs of (GLOW , GHIGH) and (GHIGH?1/3 , GHIGH?2/3), and the multi-level RRAM cells have been characterized for &gt;100 hours with respect to relaxation and temperature variation. By using 2-bit weights, we achieved 2.8-5.3% DNN accuracy improvement for iso-area, or achieved 2X reduction in the RRAM array area for iso-accuracy.\n\n \n\nSecond, we worked on analog epitaxial RRAM called epiRAM with new materials and devices. We developed the epiRAM device with single crystal SiGe as switching medium and silver as active metal electrode. The silver movement is confined in the naturally formed one dimensional dislocation channels. As a result, very good spatial and temporal uniformity has been achieved. This unique device also provided very fine linear analog switching behaviors, fitting for neural network training tasks. One of the issues with this device was the retention at low conductance states because of the poor interaction between Ag and silicon. We further found that applying AgCu alloy as active metal could regulate and stabilize the silver movement, significantly improving retention and switching uniformity. With this new alloy material, we could demonstrate array level parallel operations in a 32x32 0T1R array for date storage and image processing applications.\n\n \n\nThird, we developed new architectures and algorithms for RRAM based in-memory computing. On the architecture side, we presented a RRAM-based recurrent neural network (RNN) accelerator that consists of RRAM crossbar, special function unit and element-wise multiplier. By employing a RNN friendly pipeline, 79X higher computing efficiency was achieved compared to GPU designs. On the algorithm side, we presented RRAM device variation aware (DVA) algorithms towards achieving high inference accuracy with RRAM in-memory computing hardware. Understanding that LRS and HRS device conductance has variation, we added such noise during DNN training, to find an optimal point that is more stable amid noise/variation. We evaluated DVA algorithm with the RRAM testchip measurement data, and the DVA showed noticeable accuracy improvement, especially for high noise.\n\n \n\nFourth, we developed NeuroSim, a circuit-level benchmark simulator that estimates the area, latency, dynamic energy and leakage power to facilitate the design space exploration of neuro-inspired architectures with mainstream and emerging device technologies. NeuroSim provides flexible interface and a wide variety of design options at the circuit and device level. Therefore, NeuroSim can be used by neural networks as a supporting tool to provide circuit-level performance evaluation. With NeuroSim, an integrated framework can be built with hierarchical organization from the device level (synaptic device properties) to the circuit level (array architectures) and then to the algorithm level (neural network topology), enabling instruction-accurate evaluation on the learning/inference accuracy as well as the circuit-level performance metrics at the run-time of learning/inference. NeuroSim has been publicly available and actively used by many users from both academia and industry.\n\n\t\t\t\t\tLast Modified: 01/04/2022\n\n\t\t\t\t\tSubmitted by: Jae-Sun Seo"
 }
}