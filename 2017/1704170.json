{
 "awd_id": "1704170",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Variance and Invariance in Voice Quality: Implications for Machine and Human Speaker Identification",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 250907.0,
 "awd_amount": 250907.0,
 "awd_min_amd_letter_date": "2017-06-21",
 "awd_max_amd_letter_date": "2017-09-07",
 "awd_abstract_narration": "A talker's voice quality conveys many kinds of information, including word and utterance prosody, emotional state, and personal identity. Variations in both the voice source and the vocal tract affect voice quality and there can be significant inter- and intra-talker variability. Understanding what aspects of a voice are talker-specific should aid in understanding the human limits in perceiving speaker differences and in developing better speaker identification (SID) algorithms. Despite technological advances, the performance of current SID systems remains far from perfect, and degrades significantly when the training and testing conditions are mismatched especially in terms of speech style (conversational versus read for example), speaker's emotional status, when the utterances are short, and when the task is text-independent. The key questions that the project aims to answer are: under normal daily life variability, how often does a talker sound less like him- or herself and more like someone else? Which acoustic properties account for speaker similarity? Can automatic speaker identification (SID) algorithms be improved by knowledge of which properties are important for human perception of speaker similarity?\r\nThe project is a transformative one and  helps better understand and model variance and invariance in voice quality. It will inform several important issues in human speech perception, especially in the area of talker similarity. Understanding what aspects of the source signal, if any, are talker-specific, should aid in developing better speaker identification and verification algorithms that are able to handle short utterances and are robust to varying affect and styles of speaking. A model of voice quality variations could also improve the naturalness of text-to-speech (TTS) systems. If it were known how much a person could change his or her voice quality without compromising their vocal identity, this knowledge could also inform medical rehab applications and forensics. A better understanding of voice quality will thus be of significant impact scientifically, and for engineering, forensic, and medical applications. The project has strong outreach and dissemination programs and fosters interdisciplinary activities in Electrical Engineering, Linguistics, and Speech and Hearing Science at UCLA and the Center of Excellence at JHU. It trains undergraduate and graduate students in important cross-disciplinary activities of technological and scientific significance.  The results will be published in high-quality journals and presented at relevant international conferences. The research results - a set of databases, software tools, and publications will be disseminated freely.\r\n\r\n\r\nThe project analyzes and discovers how the speech signal varies within and across talkers under circumstances that introduce variability in everyday life situations. Specifically, it investigates whether an individual talker's speech varies significantly across recording sessions and speech tasks. Most importantly, it examines how intra-talker variability from all these sources of variability compares with inter-talker variability. Understanding these issues requires a high-quality speech database with multiple voice samples from many talkers (in this case 200) which are collected, annotated, and distributed to other researchers.   Acoustic analyses reveals inter- and intra-talker variability in the speech signal across different situations by generating a multi- dimensional acoustic profile of each talker that specifies the range of parameter values that are typical in the corpus for that talker, and the likelihood of deviations from that usual profile. Perceptual studies determine the extent to which parameter profiles predict perceived similarity, and how much variability in each parameter can be tolerated before talkers cease to sound like themselves. Insights from the acoustic and perceptual studies guide the development of robust text-dependent and text-independent SID algorithms that are anticipated to be robust to variations in affect, style, and for short utterances.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "McCree",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alan McCree",
   "pi_email_addr": "alan.mccree@jhu.edu",
   "nsf_id": "000710493",
   "pi_start_date": "2017-06-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Human Language Technology COE",
  "perf_str_addr": "810 Wyman Park Drive",
  "perf_city_name": "Baltimore, MD",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212112840",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 250907.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project worked with the UCLA Speaker Variability Database, which<br />provides multiple recordings of speakers in a variety of speech tasks<br />and on multiple occasions in order to systematically study both<br />within- and between speaker variability. Audio recordings were done in<br />a sound-attenuated booth with a sampling rate of 22 kHz.&nbsp; Speech tasks<br />from the database used for this study include reading sentences to<br />represent scripted speaking style (75 sec); narrating a recent<br />neutral, happy, or annoying conversation to represent unscripted<br />affective speech (30 sec each); making a telephone call to a familiar<br />person to represent unscripted conversational style (60&ndash;120 sec); and<br />talking aloud to pets in a video, providing pet-directed speech, which<br />typically has exaggerated prosody (60&ndash;120 sec).<br /><br />Key achievements on this project were in two efforts. First, working<br />jointly with UCLA, we investigated the effects of speaking-style<br />variability on automatic speaker identification using the UCLA Speaker<br />Variability database. A deep neural network (DNN) x-vector/PLDA<br />(probabilistic linear discriminant analysis) speaker identification<br />system was trained with the SRE and Switchboard databases with<br />standard augmentation techniques and evaluated with utterances from<br />the UCLA database. The equal error rate (EER) was low when enrollment<br />and test utterances were of the same style (e.g., 0.98% and 0.57% for<br />read and conversational speech, respectively), but it increased<br />substantially when styles were mismatched between enrollment and test<br />utterances. For instance, when enrolled with conversation utterances,<br />the EER increased to 3.03%, 2.96% and 22.12% when tested on read,<br />narrative, and pet-directed speech, respectively. To reduce the effect<br />of style mismatch, we proposed an entropy-based variable frame rate<br />technique to artificially generate style-normalized representations<br />for PLDA adaptation. The proposed system significantly improved<br />performance. In the aforementioned conditions, the EERs improved to<br />2.69% (conversation&ndash;read), 2.27% (conversation&ndash;narrative), and 18.75%<br />(pet-directed&ndash;read). Overall, the proposed technique performed<br />comparably to multi-style PLDA adaptation without the need for<br />training data in different speaking styles per speaker.<br /><br />Secondly, we continued to investigate the multi-stream paradigm for<br />robustness in speaker recognition systems. An ultimate goal of this<br />effort is to apply performance monitoring to deep speaker embeddings<br />in challenging conditions, which would determine if the derived<br />embeddings are in fact sufficiently reliable.&nbsp; However, the<br />performance monitoring research in this project focused instead on<br />automatic speech recognition (ASR) for simplicity in problem<br />definition. We developed a practical two-stage training strategy for<br />multi-stream end-to-end ASR.&nbsp; In Stage-1, a single-stream model is<br />trained using all data for better model generalization. The encoder<br />then acts as a Universal Feature Extractor (UFE) to process parallel<br />data individually to generate a set of high-level parallel<br />features. Initializing pre-trained components from Stage-1, Stage-2<br />training only optimizes the stream-fusion component operating directly<br />on UFE parallel features. The resulting memory and computation savings<br />greatly simplify training, potentially allowing for more<br />hyperparameter exploration or consideration of more complicated<br />architectures. This two-stage strategy remarkably alleviates the<br />burden of optimizing a massive multi-encoder model while still<br />substantially improving the ASR performance. Experiments were<br />conducted on two datasets, DIRHA and AMI, as a scenario with distant<br />multi-microphone arrays. Compared with our previous method, this<br />strategy achieves relative word error rate reductions of 8.2&ndash;32.4%,<br />while consistently outperforming several conventional combination<br />methods.&nbsp; Moreover, numerous directions remain for future<br />exploration. For instance, this approach could be explored in several<br />multi-stream scenarios, such as audio-visual ASR or utilizing speaker<br />embeddings in ASR.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/11/2021<br>\n\t\t\t\t\tModified by: Alan&nbsp;Mccree</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project worked with the UCLA Speaker Variability Database, which\nprovides multiple recordings of speakers in a variety of speech tasks\nand on multiple occasions in order to systematically study both\nwithin- and between speaker variability. Audio recordings were done in\na sound-attenuated booth with a sampling rate of 22 kHz.  Speech tasks\nfrom the database used for this study include reading sentences to\nrepresent scripted speaking style (75 sec); narrating a recent\nneutral, happy, or annoying conversation to represent unscripted\naffective speech (30 sec each); making a telephone call to a familiar\nperson to represent unscripted conversational style (60&ndash;120 sec); and\ntalking aloud to pets in a video, providing pet-directed speech, which\ntypically has exaggerated prosody (60&ndash;120 sec).\n\nKey achievements on this project were in two efforts. First, working\njointly with UCLA, we investigated the effects of speaking-style\nvariability on automatic speaker identification using the UCLA Speaker\nVariability database. A deep neural network (DNN) x-vector/PLDA\n(probabilistic linear discriminant analysis) speaker identification\nsystem was trained with the SRE and Switchboard databases with\nstandard augmentation techniques and evaluated with utterances from\nthe UCLA database. The equal error rate (EER) was low when enrollment\nand test utterances were of the same style (e.g., 0.98% and 0.57% for\nread and conversational speech, respectively), but it increased\nsubstantially when styles were mismatched between enrollment and test\nutterances. For instance, when enrolled with conversation utterances,\nthe EER increased to 3.03%, 2.96% and 22.12% when tested on read,\nnarrative, and pet-directed speech, respectively. To reduce the effect\nof style mismatch, we proposed an entropy-based variable frame rate\ntechnique to artificially generate style-normalized representations\nfor PLDA adaptation. The proposed system significantly improved\nperformance. In the aforementioned conditions, the EERs improved to\n2.69% (conversation&ndash;read), 2.27% (conversation&ndash;narrative), and 18.75%\n(pet-directed&ndash;read). Overall, the proposed technique performed\ncomparably to multi-style PLDA adaptation without the need for\ntraining data in different speaking styles per speaker.\n\nSecondly, we continued to investigate the multi-stream paradigm for\nrobustness in speaker recognition systems. An ultimate goal of this\neffort is to apply performance monitoring to deep speaker embeddings\nin challenging conditions, which would determine if the derived\nembeddings are in fact sufficiently reliable.  However, the\nperformance monitoring research in this project focused instead on\nautomatic speech recognition (ASR) for simplicity in problem\ndefinition. We developed a practical two-stage training strategy for\nmulti-stream end-to-end ASR.  In Stage-1, a single-stream model is\ntrained using all data for better model generalization. The encoder\nthen acts as a Universal Feature Extractor (UFE) to process parallel\ndata individually to generate a set of high-level parallel\nfeatures. Initializing pre-trained components from Stage-1, Stage-2\ntraining only optimizes the stream-fusion component operating directly\non UFE parallel features. The resulting memory and computation savings\ngreatly simplify training, potentially allowing for more\nhyperparameter exploration or consideration of more complicated\narchitectures. This two-stage strategy remarkably alleviates the\nburden of optimizing a massive multi-encoder model while still\nsubstantially improving the ASR performance. Experiments were\nconducted on two datasets, DIRHA and AMI, as a scenario with distant\nmulti-microphone arrays. Compared with our previous method, this\nstrategy achieves relative word error rate reductions of 8.2&ndash;32.4%,\nwhile consistently outperforming several conventional combination\nmethods.  Moreover, numerous directions remain for future\nexploration. For instance, this approach could be explored in several\nmulti-stream scenarios, such as audio-visual ASR or utilizing speaker\nembeddings in ASR.\n\n\t\t\t\t\tLast Modified: 09/11/2021\n\n\t\t\t\t\tSubmitted by: Alan Mccree"
 }
}