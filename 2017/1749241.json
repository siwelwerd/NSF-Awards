{
 "awd_id": "1749241",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Collaborative Research: Inference of Information Measures on Large Alphabets: Fundamental Limits, Fast Algorithims, and Applications",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 206012.0,
 "awd_amount": 206012.0,
 "awd_min_amd_letter_date": "2017-08-31",
 "awd_max_amd_letter_date": "2017-08-31",
 "awd_abstract_narration": "A key task in information theory is to characterize fundamental performance limits in compression, communication, and more general operational problems involving the storage, transmission and processing of information. Such characterizations are usually in terms of information measures, among the most fundamental of which are the Shannon entropy and the mutual information. In addition to their prominent operational roles in the traditional realms of information theory, information measures have found numerous applications in many statistical modeling and machine learning tasks. Various modern data-analytic applications deal with data sets naturally viewed as samples from a probability distribution over a large domain. Due to the typically large alphabet size and resource constraints, the practitioner contends with the difficulty of undersampling in applications ranging from corpus linguistics to neuroscience. One of the main goals of this project is the development of a general theory based on a new set of mathematical tools that will facilitate the construction and analysis of optimal estimation of information measures on large alphabets. The other major facet of this project is the incorporation of the new theoretical methodologies into machine learning algorithms, thereby significantly impacting current real-world learning practices.  Successful completion of this project will result in enabling technologies and practical schemes - in applications ranging from analysis of neural response data to learning graphical models - that are provably much closer to attaining the fundamental performance limits than existing ones. The findings of this project will enrich existing big data-analytic curricula.  A new course dedicated to high-dimensional statistical inference that addresses estimation for large-alphabet data in depth will be created and offered. Workshops on the themes and findings of this project will be organized and held at Stanford and UIUC. \r\n\r\nA comprehensive approximation-theoretic approach to estimating functionals of distributions on large alphabets will be developed via computationally efficient procedures based on best polynomial approximation, with provable essential optimality guarantees. Rooted in the high-dimensional statistics literature, our key observation is that while estimating the distribution itself requires the sample size to scale linearly with the alphabet size, it is possible to accurately estimate functionals of the distribution, such as entropy or mutual information, with sub-linear sample complexity. This requires going beyond the conventional wisdom by developing more sophisticated approaches than maximal likelihood (?plug-in?) estimation. The other major facet of this project is translating the new theoretical methodologies into highly scalable and efficient machine learning algorithms, thereby significantly impacting current real-world learning practices and significantly boosting the performance in several of the most prevalent machine learning applications, such as learning graphical models, that rely on mutual information estimation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yihong",
   "pi_last_name": "Wu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yihong Wu",
   "pi_email_addr": "yihong.wu@yale.edu",
   "nsf_id": "000656290",
   "pi_start_date": "2017-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "Office of Sponsored Projects P.O. Box 208327",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208327",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 206012.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Various modern data analytic applications deal with datasets naturally viewed as samples from a probability distribution over a large domain. From this perspective, many modern statistical modeling and machine learning tasks rest upon data-driven procedures for accurately estimating information measures of the data-generating distributions. Due to the typically large alphabet size and resource constraints, the practitioner contends with the difficulty of undersampling in applications ranging from corpus linguistics to neuroscience. This project has substantially progressed our understanding of statistical inference on large alphabets with insufficient samples and, in turn, of compression, prediction, classification and estimation in large-alphabet settings. Further, we've developed a general theory based on a new set of mathematical tools including, in particular, approximation-theoretic techniques, that guides the construction and analysis of optimal estimators of information measures on large alphabets.</p>\n<p><br />The other major thrust of this proposal was to incorporate the new theoretical methodologies into machine learning algorithms, thereby significantly impacting real-world learning practices. The procedures developed are highly scalable and provably approximately optimal. We've deployed these schemes in domains ranging from medicine and genomics through neuroscience to language modeling and compression, yielding considerable performance boosts and new insights into the respective domains. Beyond the scientific community, we've incorporated our results in lecture notes and communicated our findings to the general public through a series of outreach events.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/15/2019<br>\n\t\t\t\t\tModified by: Yihong&nbsp;Wu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nVarious modern data analytic applications deal with datasets naturally viewed as samples from a probability distribution over a large domain. From this perspective, many modern statistical modeling and machine learning tasks rest upon data-driven procedures for accurately estimating information measures of the data-generating distributions. Due to the typically large alphabet size and resource constraints, the practitioner contends with the difficulty of undersampling in applications ranging from corpus linguistics to neuroscience. This project has substantially progressed our understanding of statistical inference on large alphabets with insufficient samples and, in turn, of compression, prediction, classification and estimation in large-alphabet settings. Further, we've developed a general theory based on a new set of mathematical tools including, in particular, approximation-theoretic techniques, that guides the construction and analysis of optimal estimators of information measures on large alphabets.\n\n\nThe other major thrust of this proposal was to incorporate the new theoretical methodologies into machine learning algorithms, thereby significantly impacting real-world learning practices. The procedures developed are highly scalable and provably approximately optimal. We've deployed these schemes in domains ranging from medicine and genomics through neuroscience to language modeling and compression, yielding considerable performance boosts and new insights into the respective domains. Beyond the scientific community, we've incorporated our results in lecture notes and communicated our findings to the general public through a series of outreach events.\n\n \n\n\t\t\t\t\tLast Modified: 12/15/2019\n\n\t\t\t\t\tSubmitted by: Yihong Wu"
 }
}