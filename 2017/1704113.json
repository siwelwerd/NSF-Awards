{
 "awd_id": "1704113",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SaTC: CORE: Medium: Collaborative: A Linguistically-Informed Approach for Measuring and Circumventing Internet Censorship",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925177",
 "po_email": "asquicci@nsf.gov",
 "po_sign_block_name": "Anna Squicciarini",
 "awd_eff_date": "2017-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 500849.0,
 "awd_amount": 500849.0,
 "awd_min_amd_letter_date": "2017-08-14",
 "awd_max_amd_letter_date": "2022-03-25",
 "awd_abstract_narration": "Internet censorship consists of restrictions on what information can be publicized or viewed on the Internet. According to Freedom House's annual Freedom on the Net report, more than half the world's Internet users now live in a place where the Internet is censored or restricted. However, members of the Internet Freedom community lack comprehensive real-time awareness of where and how censorship is being imposed. The challenges to achieving such a solution include but are not limited to coverage, scalability, adoption, and safety. The project explores a linguistically-informed approach for measuring and circumventing Internet censorship.\r\n\r\nThe research takes a new perspective on the problem by investigating a hybrid method for censorship detection and evasion from the lens of linguistic analysis. The team develops new models to measure Internet censorship, investigates mechanisms to circumvent censorship using linguistic techniques, conducts communication and social network measurements of censored content. Active Sensing and natural language processing techniques, in conjunction with machine learning and optimization, invigorates new research directions in Internet Freedom and produces new high quality data and tools available for public use. This new allogamy between computer science, information security, network analysis and linguistics provides the foundation for evolution of anti-censorship technologies. The research contributes to a number of fields including Internet censorship, privacy and online information retrieval, as well as computational social science by modeling and analyzing the phenomenon of censorship using the signal available in language. The broader contribution includes wide dissemination of the research results via peer-reviewed publications, special topic courses and workshops. Additional benefits include providing graduate and undergraduate researchers with significant experience of highly practical work on a difficult interdisciplinary problem. Significant gains are obtained in recruitment of minority students through research training in computer science and linguistics.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chris",
   "pi_last_name": "Leberknight",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Chris S Leberknight",
   "pi_email_addr": "leberknightc@mail.montclair.edu",
   "nsf_id": "000566222",
   "pi_start_date": "2017-08-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anna",
   "pi_last_name": "Feldman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anna Feldman",
   "pi_email_addr": "feldmana@mail.montclair.edu",
   "nsf_id": "000277026",
   "pi_start_date": "2017-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Montclair State University",
  "inst_street_address": "1 NORMAL AVE",
  "inst_street_address_2": "",
  "inst_city_name": "MONTCLAIR",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "9736556923",
  "inst_zip_code": "070431624",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "NJ11",
  "org_lgl_bus_name": "MONTCLAIR STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "CM4TTRKFCLF9"
 },
 "perf_inst": {
  "perf_inst_name": "Montclair State University",
  "perf_str_addr": "1 Normal Avenue",
  "perf_city_name": "Montclair",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "070431624",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "NJ11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 329352.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 171497.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Summary</strong> &ndash; This project explored a linguistically-informed approach and system for measuring and mitigating Internet censorship.&nbsp; Much of our initial work on censorship analysis was based on limited publicly available historical datasets consisting of banned words.&nbsp; During the first year of this project, in 2018, our goal was to gain a better understanding of censorship enforcement practices across different topics and applications.&nbsp; Subsequent years focused on developing new metrics for predicting censored content based on the analysis of linguistic properties, content characteristics (topic analysis, controversy detection), and application specific features such as user demographics.</p>\n<p>The project outcomes provide a broad view of censorship&nbsp; policies and technologies through the examination of different categorical topics, applications, and languages.&nbsp; Our research activity has produced software tools, novel datasets, and proposed new metrics for predicting censored content.&nbsp; These new metrics are based on linguistic properties and content characteristics such as &ldquo;sensitivity&rdquo; and &ldquo;controversy&rdquo;.&nbsp;</p>\n<p><strong>Software Tools</strong> &ndash; Due to limited data and the need to examine recent<em> </em>censorship policies many new software tools were developed.&nbsp; The software system we developed to extract, archive, and analyze censored content has been integrated with a publicly available online censorship monitoring application.&nbsp; Our software, unlike the publicly available application and historical data, provides access to current and archived censored data.&nbsp; This will enable future researchers to conduct longitudinal studies to examine the evolving nature of censored topics and the language dynamics use to evade detection.&nbsp; All software developed for this project is publicly available on GitHub</p>\n<ul>\n<li><a href=\"https://github.com/leberkc/nlp-news-aggregator\">https://github.com/leberkc/nlp-news-aggregator</a></li>\n<li><a href=\"https://github.com/arregoitiaj1/censorship-lab\">https://github.com/arregoitiaj1/censorship-lab</a></li>\n<li><a href=\"https://github.com/matt-goldeck/apollo-pytho\">https://github.com/matt-goldeck/apollo-pytho</a></li>\n<li><a href=\"https://github.com/kenneywl/Controversy-Classification\">https://github.com/kenneywl/Controversy-Classification</a></li>\n</ul>\n<p><strong>Datasets</strong> &ndash; When we started working on linguistic fingerprints of censorship, we realized that censored language uses figurative language, in particular, sarcasm and euphemisms, and therefore, our linguistic explorations concentrated on these phenomena in multiple languages. We have created datasets in Mandarin, Yoruba, English, Spanish, and are currently working on Turkish, Russian, Hausa and Ingbo.&nbsp; We have also created datasets of censored posts extracted from Sina Weibo.&nbsp;</p>\n<p><strong>Content Characteristics and Metrics</strong> &ndash; We explored the accuracy of censored content in China, with publicly available censored keyword lists across several search engines.&nbsp; Results suggest that while the accuracy of censored content differed between applications, political content was censored most frequently compared to other categorical content.&nbsp; The differences in accuracy and type of content censored, demonstrate variations in enforcement practices, but it also sheds light on how citizens might utilize different applications to evade detection. This result while insightful required further experiments to explore not only the categories associated with the content, but the level of controversy associated with that content. Our work on controversy detection of English words is inspired by the expectation that controversial topics are more likely to be censored compared to non-controversial topics.&nbsp; Identifying controversial words in a message might signal the potential for the message to get censored.&nbsp; We developed a classifier for detecting controversial keywords and proposed new metrics to measure controversy such as variance of sentiment and information entropy.&nbsp; &nbsp;</p>\n<p><strong>Linguistic Properties and Metrics</strong> &ndash;This work was extended to evaluate the classification accuracy of detecting censored microblog posts on Sina Weibo.&nbsp; Our investigation reveals that the strongest linguistic indicator of censored content of our corpus is its readability.&nbsp; Text readability can be determined with the number of distinct semantic classes a sentence has, and the insufficiency of using word frequency or word count alone to decide readability, we combine the use of frequency and semantic classes to form a readability metric for our data.&nbsp; We build a neural network classifier to predict censorship based on &lsquo;sensitive&rsquo; topics or authored by &lsquo;sensitive&rsquo; users on Sina Weibo. Our model performs with an 88.50% accuracy using only linguistic features.&nbsp;</p>\n<p><strong>User Demographics and Application Specific Features</strong> &ndash; Furthermore, results from our user demographic studies on Sina Weibo suggest male users who are verified (pay for mobile and security features) are more likely to be censored than females or users who are not verified. In addition, users from provinces such as Hong Kong, Macao, and Beijing are more heavily censored compared to any other province in China over the same period.&nbsp; This work is currently being extended in two directions: (1) evaluation of stylometric properties for authorship identification and (2) examination user characteristics and content recency with Sina Weibo data from 2014 -2021with 14,000 censored posts.</p>\n<p><strong>Censorship Evasion Strategies</strong> &ndash; With regard to censorship evasion, ongoing work inspired by results from this project are focused on developing different censorship circumvention strategies based on Luby Transform Fountain codes and automated homophone generation.</p>\n<p>The ultimate aim of this project is to promote digital democracy for netizens faced with oppressive methods of censorship and information control to openly express their thoughts and ideas online.&nbsp; Understanding how to predict censored content is the primary goal of this project which is necessary for the next phase of this work focused on censorship evasion techniques.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/29/2023<br>\nModified by: Chris&nbsp;S&nbsp;Leberknight</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nSummary  This project explored a linguistically-informed approach and system for measuring and mitigating Internet censorship. Much of our initial work on censorship analysis was based on limited publicly available historical datasets consisting of banned words. During the first year of this project, in 2018, our goal was to gain a better understanding of censorship enforcement practices across different topics and applications. Subsequent years focused on developing new metrics for predicting censored content based on the analysis of linguistic properties, content characteristics (topic analysis, controversy detection), and application specific features such as user demographics.\n\n\nThe project outcomes provide a broad view of censorship policies and technologies through the examination of different categorical topics, applications, and languages. Our research activity has produced software tools, novel datasets, and proposed new metrics for predicting censored content. These new metrics are based on linguistic properties and content characteristics such as sensitivity and controversy.\n\n\nSoftware Tools  Due to limited data and the need to examine recent censorship policies many new software tools were developed. The software system we developed to extract, archive, and analyze censored content has been integrated with a publicly available online censorship monitoring application. Our software, unlike the publicly available application and historical data, provides access to current and archived censored data. This will enable future researchers to conduct longitudinal studies to examine the evolving nature of censored topics and the language dynamics use to evade detection. All software developed for this project is publicly available on GitHub\n\nhttps://github.com/leberkc/nlp-news-aggregator\nhttps://github.com/arregoitiaj1/censorship-lab\nhttps://github.com/matt-goldeck/apollo-pytho\nhttps://github.com/kenneywl/Controversy-Classification\n\n\n\nDatasets  When we started working on linguistic fingerprints of censorship, we realized that censored language uses figurative language, in particular, sarcasm and euphemisms, and therefore, our linguistic explorations concentrated on these phenomena in multiple languages. We have created datasets in Mandarin, Yoruba, English, Spanish, and are currently working on Turkish, Russian, Hausa and Ingbo. We have also created datasets of censored posts extracted from Sina Weibo.\n\n\nContent Characteristics and Metrics  We explored the accuracy of censored content in China, with publicly available censored keyword lists across several search engines. Results suggest that while the accuracy of censored content differed between applications, political content was censored most frequently compared to other categorical content. The differences in accuracy and type of content censored, demonstrate variations in enforcement practices, but it also sheds light on how citizens might utilize different applications to evade detection. This result while insightful required further experiments to explore not only the categories associated with the content, but the level of controversy associated with that content. Our work on controversy detection of English words is inspired by the expectation that controversial topics are more likely to be censored compared to non-controversial topics. Identifying controversial words in a message might signal the potential for the message to get censored. We developed a classifier for detecting controversial keywords and proposed new metrics to measure controversy such as variance of sentiment and information entropy. \n\n\nLinguistic Properties and Metrics This work was extended to evaluate the classification accuracy of detecting censored microblog posts on Sina Weibo. Our investigation reveals that the strongest linguistic indicator of censored content of our corpus is its readability. Text readability can be determined with the number of distinct semantic classes a sentence has, and the insufficiency of using word frequency or word count alone to decide readability, we combine the use of frequency and semantic classes to form a readability metric for our data. We build a neural network classifier to predict censorship based on sensitive topics or authored by sensitive users on Sina Weibo. Our model performs with an 88.50% accuracy using only linguistic features.\n\n\nUser Demographics and Application Specific Features  Furthermore, results from our user demographic studies on Sina Weibo suggest male users who are verified (pay for mobile and security features) are more likely to be censored than females or users who are not verified. In addition, users from provinces such as Hong Kong, Macao, and Beijing are more heavily censored compared to any other province in China over the same period. This work is currently being extended in two directions: (1) evaluation of stylometric properties for authorship identification and (2) examination user characteristics and content recency with Sina Weibo data from 2014 -2021with 14,000 censored posts.\n\n\nCensorship Evasion Strategies  With regard to censorship evasion, ongoing work inspired by results from this project are focused on developing different censorship circumvention strategies based on Luby Transform Fountain codes and automated homophone generation.\n\n\nThe ultimate aim of this project is to promote digital democracy for netizens faced with oppressive methods of censorship and information control to openly express their thoughts and ideas online. Understanding how to predict censored content is the primary goal of this project which is necessary for the next phase of this work focused on censorship evasion techniques.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 11/29/2023\n\n\t\t\t\t\tSubmitted by: ChrisSLeberknight\n"
 }
}