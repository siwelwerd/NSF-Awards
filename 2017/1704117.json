{
 "awd_id": "1704117",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium: Formal Methods for Program Fairness",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927991",
 "po_email": "namla@nsf.gov",
 "po_sign_block_name": "Nina Amla",
 "awd_eff_date": "2017-06-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 999987.0,
 "awd_amount": 1014107.0,
 "awd_min_amd_letter_date": "2017-05-31",
 "awd_max_amd_letter_date": "2017-07-21",
 "awd_abstract_narration": "As software permeates our personal lives, corporate world, and bureaucracy, more and more of our critical decisions are being delegated to opaque algorithms. These algorithms have thus become powerful arbitrators of a range of significant decisions with far-reaching societal impact.  However, an algorithm carrying out a sensitive task could potentially discriminate ? advertently or inadvertently ? against certain groups. The overarching goal of this project is to enable automated reasoning about fairness of such decision-making programs. The results from this project are integrated in teaching curricula, thus raising awareness about fairness in automated decision making. The project trains graduate students in a unique and inter-disciplinary environment. \r\n\r\nThis project investigates various notions of fairness, inspired by the law and recent works in the area, and casts them through the formal lens of program verification. The project develops techniques for certifying that a program is fair under a given population, and techniques for automatically repairing unfair programs to make them fair. From a technical viewpoint, this project develops novel probabilistic program verification and synthesis technologies that, while focusing on the problem of fairness, are general and expand the reach of current technologies. Specifically, the project develops novel probabilistic verification techniques based on volume computation. Furthermore, the project develops program repair and debugging techniques for probabilistic programs. \r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aws",
   "pi_last_name": "Albarghouthi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aws Albarghouthi",
   "pi_email_addr": "aws@cs.wisc.edu",
   "nsf_id": "000682630",
   "pi_start_date": "2017-05-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Xiaojin",
   "pi_last_name": "Zhu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaojin Zhu",
   "pi_email_addr": "jerryzhu@cs.wisc.edu",
   "nsf_id": "000211108",
   "pi_start_date": "2017-05-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shuchi",
   "pi_last_name": "Chawla",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shuchi Chawla",
   "pi_email_addr": "shuchi@cs.utexas.edu",
   "nsf_id": "000204205",
   "pi_start_date": "2017-05-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Loris",
   "pi_last_name": "DAntoni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Loris DAntoni",
   "pi_email_addr": "ldantoni@ucsd.edu",
   "nsf_id": "000701818",
   "pi_start_date": "2017-05-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061613",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "8206",
   "pgm_ref_txt": "Formal Methods and Verification"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 1014107.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Today algorithms are making decisions that affect people's lives. Algorithms sift through resumes, assign credit scores, predict health risks, and much more. There have been growing concerns about the potential unfairness of algorithmic decision making, particularly towards historically marginalized groups. Think, for example, about an algorithm that decides whether to grant someone a loan depending on their financial history. The algorithm may be less inclined to grant loans to minority applicants, even if they are eligible for a loan. This kind of bias, or unfairness, is not necessarily intentionally baked into the decision-making algorithm, but the bias can be a complex product of data and techniques used to construct the decision-making algorithm. To tackle unfairness in algorithmic decision-making, in this project we have developed theoretical and practical tools for understanding sources of algorithmic unfairness, auditing decision-making algorithms, and ensuring their fairness. For instance, we have developed techniques for monitoring fairness of large-scale decision-making algorithms, ensuring fairness in online advertising, and investigating biases in the data that is used to train decision-making algorithms.</p>\n<p><br />The project has trained a number of outstanding doctoral students, and has helped kickstart interdisciplinary collaborations at the University of Wicsonsin--Madison, involving computer science, law, philosophy, and beyond.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/30/2021<br>\n\t\t\t\t\tModified by: Aws&nbsp;Albarghouthi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nToday algorithms are making decisions that affect people's lives. Algorithms sift through resumes, assign credit scores, predict health risks, and much more. There have been growing concerns about the potential unfairness of algorithmic decision making, particularly towards historically marginalized groups. Think, for example, about an algorithm that decides whether to grant someone a loan depending on their financial history. The algorithm may be less inclined to grant loans to minority applicants, even if they are eligible for a loan. This kind of bias, or unfairness, is not necessarily intentionally baked into the decision-making algorithm, but the bias can be a complex product of data and techniques used to construct the decision-making algorithm. To tackle unfairness in algorithmic decision-making, in this project we have developed theoretical and practical tools for understanding sources of algorithmic unfairness, auditing decision-making algorithms, and ensuring their fairness. For instance, we have developed techniques for monitoring fairness of large-scale decision-making algorithms, ensuring fairness in online advertising, and investigating biases in the data that is used to train decision-making algorithms.\n\n\nThe project has trained a number of outstanding doctoral students, and has helped kickstart interdisciplinary collaborations at the University of Wicsonsin--Madison, involving computer science, law, philosophy, and beyond.\n\n\t\t\t\t\tLast Modified: 09/30/2021\n\n\t\t\t\t\tSubmitted by: Aws Albarghouthi"
 }
}