{
 "awd_id": "1717349",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF:Small: Fundamental High-Dimensional Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2017-08-31",
 "awd_max_amd_letter_date": "2017-08-31",
 "awd_abstract_narration": "The availability of high-dimensional data in important application areas has made efficient tools to handle such data the need of the century. This proposal addresses some of the most basic questions arising from this need. The topics targeted in this project are on the frontier of research in algorithms, targeting well-known open problems with promising new ideas. Progress on these problems is sure to unravel mathematical structure and is likely to yield new tools. As the field of algorithms continues to expand (and extend its reach beyond computer science), such tools have become indispensable.\r\n\r\nThe PI was the founding director of the Algorithms and Randomness Center (ARC) and continues in-depth collaborations with scientists from other fields to identify problems and ideas that could play a fundamental role in understanding the complexity of computation.  The topics and findings of this project will be used to design graduate courses and contribute to undergraduate ones. The graduate courses will be the basis for textbooks to benefit the research community. In addition, up-to-date surveys on these topics will be prepared by the PI and collaborators.\r\n\r\nSampling, Learning and Optimization in high dimension are intricately linked at many levels: reductions between problems from one topic to another, insights from one that apply to another, common analysis techniques and similar contexts (e.g., large, high-dimensional data). This project is motivated by quest for a theory of efficient algorithms, a theory that would include algorithmic tools, lower bounds and analysis techniques, in addition to questions that arise from the quest but are of independent mathematical interest and provide new ideas for classical fields.\r\n\r\nSpecifically, the project seeks to find efficient algorithms for sampling in the oracle model as well as for explicit polytopes, faster sampling and optimization using Riemannian geometry; algorithms for learning polyhedra, the analysis of neural networks with a single hidden layer, robust estimation and unsupervised learning in the presence of noise, and algorithmic considerations in the representation and analysis of very large (but not dense) graphs.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Santosh",
   "pi_last_name": "Vempala",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Santosh S Vempala",
   "pi_email_addr": "vempala@cc.gatech.edu",
   "nsf_id": "000215458",
   "pi_start_date": "2017-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The focus of this project was developing provably efficient algorithms for sampling and optimization in very high dimension, two general problems with many applications in Machine Learning broadly, as well as across the natural and applied sciences. Major findings include:</p>\n<p>1-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Isoperimetric inequalities. To sample a high-dimensional set or distribution, the general method is to design a random process that converges to the target distribution. The convergence of such a process depends on the isoperimetry of the distribution, i.e., how small a cut can divide the distribution into two large parts. This project made major progress on the KLS conjecture, showing that the isoperimetric ratio is bounded by the fourth root of the dimension, and introduced a proof strategy that was recently used [Chen 2020] to prove a sub-polynomial bound. These improvements have many consequences, including faster sampling of logconcave densities. Another finding was an optimal (tight) general inequality (called a log-Sobolev inequality) for such densities.&nbsp; &nbsp;</p>\n<p>2-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Non-Euclidean geometries for efficient sampling. Classical Markov processes for sampling, such as the ball walk and hit-and-run, generate the next point using Euclidean straight lines. This hits a quadratic (in dimension) barrier for known methods. An important finding of this project was that using curved paths (Riemannian Hamiltonian Monte Carlo) leads to a sub-quadratic bound on the number of steps needed to sample arbitrary polytopes.&nbsp;</p>\n<p>3-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Convex optimization with a membership oracle. The number of membership queries required for convex optimization grows roughly as at most quadratic in the dimension, significantly improving known bounds, and conjectured to be the best possible.</p>\n<p>4-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fair and distributed optimization. The project introduced two frameworks for optimization that capture contemporary challenges. The first is fair optimization in the presence of multiple subgroups. By viewing each subgroup as a different objective function, the goal is to minimize the maximum cost over the subgroups. The problem includes many well-known special cases such as fair PCA and fair dimensionality reduction. A polynomial-time algorithm was found for two groups, and an approximation algorithm for any number of groups.</p>\n<p>The second is optimization when the data/constraints are distributed across many servers. Optimal communication bounds were established for the core problem of solving linear systems, and new upper and lower bounds for solving linear programs.</p>\n<p>5-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Random models of sparse graphs. Very large graphs encountered in many fields are typically sparse. How to model them? This project introduced two methods, one to generate sparse graphs with a desired density of edges, and small cycles; the other to give a decomposition into simple blocks (cut matrices) with small error.</p>\n<p>6-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Models for brain computation. How does the brain compute all that it does? Introducing a simple primitive consistent with experimental neuroscience, called Random Projection and Cap, the project showed that assemblies of neurons are capable of memorizing patterns and increasing association between co-occurring stimuli, in a rigorous model.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/03/2021<br>\n\t\t\t\t\tModified by: Santosh&nbsp;S&nbsp;Vempala</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1717349/1717349_10520057_1609688705129_needle--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1717349/1717349_10520057_1609688705129_needle--rgov-800width.jpg\" title=\"Needle Decomposition\"><img src=\"/por/images/Reports/POR/2021/1717349/1717349_10520057_1609688705129_needle--rgov-66x44.jpg\" alt=\"Needle Decomposition\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Convex bodies can be partitioned into needles. It is conjectured that any such decomposition must have a large fraction of short needles.</div>\n<div class=\"imageCredit\">Santosh Vempala</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Santosh&nbsp;S&nbsp;Vempala</div>\n<div class=\"imageTitle\">Needle Decomposition</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1717349/1717349_10520057_1609688894136_walks--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1717349/1717349_10520057_1609688894136_walks--rgov-800width.jpg\" title=\"Walking in a Polytope\"><img src=\"/por/images/Reports/POR/2021/1717349/1717349_10520057_1609688894136_walks--rgov-66x44.jpg\" alt=\"Walking in a Polytope\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Hamiltonian walk avoids boundaries and takes longer steps.</div>\n<div class=\"imageCredit\">Santosh Vempala</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Santosh&nbsp;S&nbsp;Vempala</div>\n<div class=\"imageTitle\">Walking in a Polytope</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe focus of this project was developing provably efficient algorithms for sampling and optimization in very high dimension, two general problems with many applications in Machine Learning broadly, as well as across the natural and applied sciences. Major findings include:\n\n1-      Isoperimetric inequalities. To sample a high-dimensional set or distribution, the general method is to design a random process that converges to the target distribution. The convergence of such a process depends on the isoperimetry of the distribution, i.e., how small a cut can divide the distribution into two large parts. This project made major progress on the KLS conjecture, showing that the isoperimetric ratio is bounded by the fourth root of the dimension, and introduced a proof strategy that was recently used [Chen 2020] to prove a sub-polynomial bound. These improvements have many consequences, including faster sampling of logconcave densities. Another finding was an optimal (tight) general inequality (called a log-Sobolev inequality) for such densities.   \n\n2-      Non-Euclidean geometries for efficient sampling. Classical Markov processes for sampling, such as the ball walk and hit-and-run, generate the next point using Euclidean straight lines. This hits a quadratic (in dimension) barrier for known methods. An important finding of this project was that using curved paths (Riemannian Hamiltonian Monte Carlo) leads to a sub-quadratic bound on the number of steps needed to sample arbitrary polytopes. \n\n3-      Convex optimization with a membership oracle. The number of membership queries required for convex optimization grows roughly as at most quadratic in the dimension, significantly improving known bounds, and conjectured to be the best possible.\n\n4-      Fair and distributed optimization. The project introduced two frameworks for optimization that capture contemporary challenges. The first is fair optimization in the presence of multiple subgroups. By viewing each subgroup as a different objective function, the goal is to minimize the maximum cost over the subgroups. The problem includes many well-known special cases such as fair PCA and fair dimensionality reduction. A polynomial-time algorithm was found for two groups, and an approximation algorithm for any number of groups.\n\nThe second is optimization when the data/constraints are distributed across many servers. Optimal communication bounds were established for the core problem of solving linear systems, and new upper and lower bounds for solving linear programs.\n\n5-      Random models of sparse graphs. Very large graphs encountered in many fields are typically sparse. How to model them? This project introduced two methods, one to generate sparse graphs with a desired density of edges, and small cycles; the other to give a decomposition into simple blocks (cut matrices) with small error.\n\n6-      Models for brain computation. How does the brain compute all that it does? Introducing a simple primitive consistent with experimental neuroscience, called Random Projection and Cap, the project showed that assemblies of neurons are capable of memorizing patterns and increasing association between co-occurring stimuli, in a rigorous model.\n\n \n\n\t\t\t\t\tLast Modified: 01/03/2021\n\n\t\t\t\t\tSubmitted by: Santosh S Vempala"
 }
}