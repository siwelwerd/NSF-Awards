{
 "awd_id": "1737933",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Optimal Bayesian Concentration Rates from Double Empirical Priors",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 87369.0,
 "awd_amount": 87369.0,
 "awd_min_amd_letter_date": "2017-02-28",
 "awd_max_amd_letter_date": "2017-02-28",
 "awd_abstract_narration": "Statisticians frequently encounter problems that involve complicated models with high-dimensional parameters, particularly in \"big data\" settings. From a Bayesian perspective, it is imperative in these problems that the prior distribution be chosen to sit in a good position. Information about where is a good starting position can come from the data. There is a potential danger with this basic strategy, namely, that a double use of data might cause the model to track the data too closely, resulting on over fitting. To avoid this, the PIs introduce a regularization technique that suitably re-weights the likelihood, preventing the model from learning too quickly. This general \"double empirical Bayes\" strategy, where the prior is centered on the data and the likelihood is re-weighted, will be applied to several important and challenging high-dimensional problems, including estimation of sparse high-dimensional precision matrices, which is relevant to estimation of large complex networks. \r\n\r\nIn this project, the PIs will develop this new double empirical Bayes framework for inference on high-dimensional parameters with a relatively low \"complexity\" or \"effective dimension\". For example, in function estimation problems, posited smoothness on the function is a constraint on its complexity. The first step of the double empirical Bayes strategy is to use a prior, indexed by the complexity of the parameter, centered at a complexity-specific estimate of the parameter based on data. To prevent the posterior from tracking the data too closely, the second step is to re-weight the likelihood to be combined with the data-dependent prior. The result is a sort of posterior distribution on the parameter space, and the PIs will provide general conditions for this posterior to concentrate around the truth at optimal rates. An additional advantage of this new approach is that the complexity-specific priors, for suitable centering, can be taken of relatively simple form, which facilitates computation. The PIs will investigate the double empirical Bayes analysis of several important high-dimensional inference problems, including density and function estimation, variable selection problems in non-linear models, and estimation of sparse precision matrices. Software will be developed for each application.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ryan",
   "pi_last_name": "Martin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ryan Martin",
   "pi_email_addr": "rgmarti3@ncsu.edu",
   "nsf_id": "000548272",
   "pi_start_date": "2017-02-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276957907",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 87369.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>High- or even infinite-dimensional statistical inference problems are commonplace, so there is a need for methodology that is efficient both computationally and theoretically. &nbsp;A Bayesian approach to these problems is attractive for a variety of reasons, but the choice of prior distribution creates a dilemma: should one pick the theoretically good prior distributions that are difficult to compute with, or pick a computationally convenient prior and risk suboptimal theoretical properties? &nbsp;This project aimed to show that a choice between good computation and good theory is not necessary, that both can be achieved with the same prior, provided that one is willing to bend slightly the traditional definition of Bayesian inference. &nbsp;</p>\n<p><br />What creates the separation between computational and theoretical properties in the prior are its tails. &nbsp;The key realization motivating this project was that the tails of the prior distribution do not matter if its center is chosen appropriately. &nbsp;But an appropriate choice of center relies on the data, so our proposal is to let the data inform the prior center in a strategic way. &nbsp;Thus, we propose to work with an \"empirical prior.\" &nbsp;By letting the data control the prior center, the tails can still be chosen in a way that makes the computation convenient, hence the best of both worlds. &nbsp;</p>\n<p><br />To fully justify these claims, we investigated the theoretical properties of the posterior distributions obtained by working with the data-centered empirical prior. &nbsp;The aim was to show that the optimal asymptotic posterior concentration rate properties could be derived with these empirical priors. &nbsp;It is not obvious that such properties must hold since the double-use of the data might result in posterior distribution that is too tight and might miss its desired target. &nbsp;Starting with some practically relevant special cases, such as high-dimensional regression, we showed that the new empirical Bayes posterior achieves the optimal concentration rate and, moreover, leads to consistent feature selection. &nbsp;Building on this experience, we developed a very general framework for the construction of empirical prior distributions whose corresponding posterior distribution is guaranteed to achieve a target rate. &nbsp;This general result has been applied to several other important special cases, including shape-constrained estimation problems, and the results are impressive. &nbsp;Aside from the theoretical properties that have been established, the numerical results alone indicate that this is sound methodology. &nbsp;Efforts to extend these results, in particular, to address the uncertainty quantification problem, are ongoing. &nbsp; Beyond the specific technical goals of this project, five graduate students gained valuable training and research experience thanks to this award. &nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/10/2018<br>\n\t\t\t\t\tModified by: Ryan&nbsp;Martin</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHigh- or even infinite-dimensional statistical inference problems are commonplace, so there is a need for methodology that is efficient both computationally and theoretically.  A Bayesian approach to these problems is attractive for a variety of reasons, but the choice of prior distribution creates a dilemma: should one pick the theoretically good prior distributions that are difficult to compute with, or pick a computationally convenient prior and risk suboptimal theoretical properties?  This project aimed to show that a choice between good computation and good theory is not necessary, that both can be achieved with the same prior, provided that one is willing to bend slightly the traditional definition of Bayesian inference.  \n\n\nWhat creates the separation between computational and theoretical properties in the prior are its tails.  The key realization motivating this project was that the tails of the prior distribution do not matter if its center is chosen appropriately.  But an appropriate choice of center relies on the data, so our proposal is to let the data inform the prior center in a strategic way.  Thus, we propose to work with an \"empirical prior.\"  By letting the data control the prior center, the tails can still be chosen in a way that makes the computation convenient, hence the best of both worlds.  \n\n\nTo fully justify these claims, we investigated the theoretical properties of the posterior distributions obtained by working with the data-centered empirical prior.  The aim was to show that the optimal asymptotic posterior concentration rate properties could be derived with these empirical priors.  It is not obvious that such properties must hold since the double-use of the data might result in posterior distribution that is too tight and might miss its desired target.  Starting with some practically relevant special cases, such as high-dimensional regression, we showed that the new empirical Bayes posterior achieves the optimal concentration rate and, moreover, leads to consistent feature selection.  Building on this experience, we developed a very general framework for the construction of empirical prior distributions whose corresponding posterior distribution is guaranteed to achieve a target rate.  This general result has been applied to several other important special cases, including shape-constrained estimation problems, and the results are impressive.  Aside from the theoretical properties that have been established, the numerical results alone indicate that this is sound methodology.  Efforts to extend these results, in particular, to address the uncertainty quantification problem, are ongoing.   Beyond the specific technical goals of this project, five graduate students gained valuable training and research experience thanks to this award.  \n\n\t\t\t\t\tLast Modified: 11/10/2018\n\n\t\t\t\t\tSubmitted by: Ryan Martin"
 }
}