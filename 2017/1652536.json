{
 "awd_id": "1652536",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Robust and Secure Multi-Modal Learning for Library-Scale Text Collections",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2017-05-15",
 "awd_exp_date": "2024-04-30",
 "tot_intn_awd_amt": 550000.0,
 "awd_amount": 566000.0,
 "awd_min_amd_letter_date": "2017-05-10",
 "awd_max_amd_letter_date": "2021-05-05",
 "awd_abstract_narration": "The growth of social media and digitized libraries has made computational text analysis a vital tool for modern scholarship. But too often methods that work on standardized collections for expert users don't translate to real-world data analysis. In order to be useful, text mining methodologies need to balance theoretical power with practical application. Real data sets are noisy and complicated. More importantly, vast amounts of data cannot be shared directly due to copyright, including all published books after 1923. This project will develop tools that can be applied to limited, privatized views of documents. Algorithms will focus on reliability and efficiency, so that powerful techniques can be used by non-expert users on easily accessible hardware, such as the 10 million K-12 students using low-powered browser-based Chromebooks thereby increasing the societal impact of the work.\r\n\r\nUnsupervised text mining methods such as topic models and word embeddings have become popular outside of machine learning because they operate on simple, widely-available representations and identify latent variables that represent recognizable themes, events, or concepts. But standard algorithms do not scale well, require full access to potentially sensitive text collections, and cannot take advantage of non-textual data such as images. Although recent work in spectral inference has produced improvements in speed, current methods are plagued by sensitivity to noisy observations. This work will develop a unified approach to unsupervised text mining based on matrix and tensor factorization. The project will focus on data rectification methods for input matrices, enabling simple algorithms to work dramatically better, even in the presence of sparse and noisy observations, while also reducing model uncertainty. The project will develop new methods for learning from private and sensitive documents by creating public views of non-public data. These will include both noisy representations of individual documents as well as corpus-level summary matrices, and support both strong non-identifiability and weaker non-expressivity criteria. Finally, the project will develop new tools for modeling images and text optimized for the way images actually accompany text in real corpora, rather than short, artificial captions. By jointly modeling large volumes of text and semantically related images, the project will enable users to search for contextually related images, not just visually similar images, and identify topics that are grounded in the visual world, not just in text. For further information see the project web page: http://mimno.infosci.cornell.edu",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Mimno",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Mimno",
   "pi_email_addr": "dm655@cornell.edu",
   "nsf_id": "000667456",
   "pi_start_date": "2017-05-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "107 Hoy Road",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148537501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 103257.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 106826.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 126060.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 113270.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 116587.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project focused on three areas, inspired by the challenges of learning on library-scale digitized book collections. These results centered around statistical topic models, which were the dominant tool for dealing with large, unstructured text collections prior to the introduction of neural language models.</p>\n<p>Efficient, scalable inference. Previous theoretical results had indicated that there could exist algorithms for inferring topic model parameters that would be dramatically faster and more reliable than existing methods based on statistical sampling. However, initial implementations were brittle and performed poorly on real data. We discovered that in practice, data did not exactly fit the assumptions of the spectral algorithm, but that simple iterative methods could de-noise the data sufficiently to enforce the proper constraints. The project developed new methods such as PADD (primal aware dual decomposition) and on-the-fly rectification that enable inference of topic model parameters using only a single, highly-parallelizable pass through data of arbitrary size. While these models have small parameter counts and limited capabilities next to today's hundred-billion parameters language models, this work provides a promising alternative to GPU-intensive learning methods for LLMs.</p>\n<p>Privacy-preserving inference. We identified early that digitized book collections are both extremely valuable sources of language examples, factual knowledge, and argumentation, they are also highly sensitive due to legal constraints. These factors have been borne out in the LLM era even more significantly.&nbsp;</p>\n<p>Multi-modal data. Another thrust that has been borne out by subsequent developments is the interaction between text and images. We developed a new method for connecting visual and textual elements that is competitive with some purely neural methods but thousands of times less computationally expensive.</p>\n<p>Outreach. The project has had impact beyond machine learning. The PI, and many of the graduate assistants, have significant experience in humanities scholarship. Much of the work on algorithms for large-scale book collections directly translates to methodologies for bringing new capabilities and new insights to the study of the past. This impact included the creation of a new course, Text Mining for History and Literature, that combines contemporary NLP with cultural heritage documents, and is now taught yearly to more than a hundred students.</p>\n<p>Mentorship. The project has launched the careers of seven young researchers, who are now in faculty positions and industry leadership roles.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 10/14/2024<br>\nModified by: David&nbsp;Mimno</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project focused on three areas, inspired by the challenges of learning on library-scale digitized book collections. These results centered around statistical topic models, which were the dominant tool for dealing with large, unstructured text collections prior to the introduction of neural language models.\n\n\nEfficient, scalable inference. Previous theoretical results had indicated that there could exist algorithms for inferring topic model parameters that would be dramatically faster and more reliable than existing methods based on statistical sampling. However, initial implementations were brittle and performed poorly on real data. We discovered that in practice, data did not exactly fit the assumptions of the spectral algorithm, but that simple iterative methods could de-noise the data sufficiently to enforce the proper constraints. The project developed new methods such as PADD (primal aware dual decomposition) and on-the-fly rectification that enable inference of topic model parameters using only a single, highly-parallelizable pass through data of arbitrary size. While these models have small parameter counts and limited capabilities next to today's hundred-billion parameters language models, this work provides a promising alternative to GPU-intensive learning methods for LLMs.\n\n\nPrivacy-preserving inference. We identified early that digitized book collections are both extremely valuable sources of language examples, factual knowledge, and argumentation, they are also highly sensitive due to legal constraints. These factors have been borne out in the LLM era even more significantly.\n\n\nMulti-modal data. Another thrust that has been borne out by subsequent developments is the interaction between text and images. We developed a new method for connecting visual and textual elements that is competitive with some purely neural methods but thousands of times less computationally expensive.\n\n\nOutreach. The project has had impact beyond machine learning. The PI, and many of the graduate assistants, have significant experience in humanities scholarship. Much of the work on algorithms for large-scale book collections directly translates to methodologies for bringing new capabilities and new insights to the study of the past. This impact included the creation of a new course, Text Mining for History and Literature, that combines contemporary NLP with cultural heritage documents, and is now taught yearly to more than a hundred students.\n\n\nMentorship. The project has launched the careers of seven young researchers, who are now in faculty positions and industry leadership roles.\n\n\n\t\t\t\t\tLast Modified: 10/14/2024\n\n\t\t\t\t\tSubmitted by: DavidMimno\n"
 }
}