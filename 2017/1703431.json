{
 "awd_id": "1703431",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Medium: Collaborative Research: A Unified and Declarative Approach to Causal Analysis for Big Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 408000.0,
 "awd_amount": 408000.0,
 "awd_min_amd_letter_date": "2017-08-09",
 "awd_max_amd_letter_date": "2017-08-09",
 "awd_abstract_narration": "Observational data is available today in multi-relational form, often extracted from various sources, and stored in multiple flat and interrelated tables.  Standard statistical methods for conducting causal inference on observational data assume a very simple data model: a single table with independent units.  This research has the potential to significantly impact application domains where differentiating causality from correlation is essential, e.g., education policy and cancer genomics. The HUME project develops techniques for efficient causal analysis using a declarative approach, over complex views, and over large datasets that are integrated from disparate data sources.  HUME uses a SQL-like language and is integrated with a relational database system.\r\n\r\nThe project develops techniques for defining arbitrarily complex units, treatments, outcomes, and covariates, by combining joins, data mapping, and aggregates across multiple tables, and uses a causal network to choose a good set of covariates for causal inference.  The first part of the project develops scalable techniques for sub-classification and matching for large data sets obtained by declaratively integrating multiple data sources.  The second part of the project develops scalable methods for discovering causal relationships among the attributes in the views by constraint-based, search-based, and hybrid discovery processes. Finally, the third part of the project investigates interferences among units arising from the complex views by designing normal forms and automatic inference of underlying assumptions exploiting techniques from database theory.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sudeepa",
   "pi_last_name": "Roy",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sudeepa Roy",
   "pi_email_addr": "roy.sudeepa@gmail.com",
   "nsf_id": "000699595",
   "pi_start_date": "2017-08-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "308 RESEARCH DR",
  "perf_city_name": "DURHAM",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277080129",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 408000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-6f4ff204-7fff-a5d2-aad9-3785546b92ae\"> </span></p>\n<p dir=\"ltr\"><span>The main goal of the project was to develop declarative approaches to enable easy and efficient causal analysis by specifying units, treatment, outcome, and covariates, over the integrated data, and declaring any underlying assumptions required in causal analysis using database view definitions.&nbsp; Causality is usually studied over independent and uniform units.&nbsp; However, in many situations, the units are connected by relationships; these relationships can be links in a social network, or author-paper relationships, or employee-employer relationships.&nbsp; In that case the outcomes for one unit may be affected through its relationships by the outcomes of other units. &nbsp; This project has researched techniques and methods to allow causal inference over relational data by assuming a much simpler relational representation,&nbsp; as well as efficient and interpretable matching techniques for causal inference,&nbsp; The project had the following major outcomes.</span></p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">&nbsp;First, we developed a system of causal inference in observational relational data. Although causal inference methods for &ldquo;observed data&rdquo; (not from randomized controlled experiments) have been studied in Statistics and Artificial Intelligence, they rely on the critical assumption that the units of study are sampled from a population of homogeneous units; in other words, the data can be represented in a single flat table. However, many real-world data are available in &ldquo;relational&rdquo; form in multiple related tables. Basic notions used in causal analysis, such as &ldquo;units&rdquo;, no longer readily apply.&nbsp; For example the treatment may be applied to one table whereas the outcome may be observed in a different table.&nbsp; This makes causal inference in relational data more challenging. For this purpose, we developed a declarative framework, called CaRL (Causal Relational Language).&nbsp; The framework includes a declarative language to represent causal background knowledge and assumptions, a semantics for complex causal queries, and an algorithm for answering causal queries from the given relational data.</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Second, we studied hypothetical reasoning on relational data with a probabilistic causal approach. What-if (provisioning for an update to a database) and how-to (how to modify the database to achieve a goal) analyses provide insights to users who wish to examine hypothetical scenarios without making actual changes to a database and thereby help plan strategies in their fields. Typically, such analyses are done by testing the effect of an update in the existing database on a specific view created by a query of interest. In real-world scenarios, however, an update to a particular part of the database may affect tuples and attributes in a completely different part due to implicit semantic dependencies. To allow for hypothetical reasoning while accommodating such dependencies, we develop HypeR, a framework that supports what-if and how-to queries accounting for probabilistic dependencies among attributes captured by a probabilistic causal model. We developed a declarative framework for expressing these hypothetical queries where the user can express constraints on pre-intervention and post-intervention values of variables, define their semantics, devise efficient algorithms and optimizations to compute their results using concepts from causality and probabilistic databases.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Third, we developed several scalable and interpretable &ldquo;almost exact matching&rdquo; methods for causal inference to match treated and control units that are similar in terms of as many &ldquo;relevant&rdquo; covariates as possible. Along these methods two main algorithms are Dynamic Almost Matching Exactly (DAME) and Fast, Large-Scale Almost Matching Exactly (FLAME), which produce high-quality interpretable matched groups by using machine learning on a holdout training set to learn distance metrics and relevant covariates instead of relying on human inputs to specify these that can be prone to errors. DAME solves an optimization problem that matches units on as many covariates as possible, prioritizing matches on important covariates. FLAME approximates the solution found by DAME via a much faster backward feature selection procedure and has shown to be scalable for big datasets using database queries. In addition, we developed the Adaptive HyperBoxes (AHB) method for matching units with continuous covariate, the FLAME-Network method to apply the FLAME algorithm to scenarios where the units are connected in a network, and FLAME-IV to apply the FLAME algorithm to setups with &ldquo;instrumental variables&rdquo;.&nbsp; We have developed software packages for these algorithms that are available online with detailed documentations and examples.&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/10/2023<br>\n\t\t\t\t\tModified by: Sudeepa&nbsp;Roy</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe main goal of the project was to develop declarative approaches to enable easy and efficient causal analysis by specifying units, treatment, outcome, and covariates, over the integrated data, and declaring any underlying assumptions required in causal analysis using database view definitions.  Causality is usually studied over independent and uniform units.  However, in many situations, the units are connected by relationships; these relationships can be links in a social network, or author-paper relationships, or employee-employer relationships.  In that case the outcomes for one unit may be affected through its relationships by the outcomes of other units.   This project has researched techniques and methods to allow causal inference over relational data by assuming a much simpler relational representation,  as well as efficient and interpretable matching techniques for causal inference,  The project had the following major outcomes.\n \n First, we developed a system of causal inference in observational relational data. Although causal inference methods for \"observed data\" (not from randomized controlled experiments) have been studied in Statistics and Artificial Intelligence, they rely on the critical assumption that the units of study are sampled from a population of homogeneous units; in other words, the data can be represented in a single flat table. However, many real-world data are available in \"relational\" form in multiple related tables. Basic notions used in causal analysis, such as \"units\", no longer readily apply.  For example the treatment may be applied to one table whereas the outcome may be observed in a different table.  This makes causal inference in relational data more challenging. For this purpose, we developed a declarative framework, called CaRL (Causal Relational Language).  The framework includes a declarative language to represent causal background knowledge and assumptions, a semantics for complex causal queries, and an algorithm for answering causal queries from the given relational data.\n\n \nSecond, we studied hypothetical reasoning on relational data with a probabilistic causal approach. What-if (provisioning for an update to a database) and how-to (how to modify the database to achieve a goal) analyses provide insights to users who wish to examine hypothetical scenarios without making actual changes to a database and thereby help plan strategies in their fields. Typically, such analyses are done by testing the effect of an update in the existing database on a specific view created by a query of interest. In real-world scenarios, however, an update to a particular part of the database may affect tuples and attributes in a completely different part due to implicit semantic dependencies. To allow for hypothetical reasoning while accommodating such dependencies, we develop HypeR, a framework that supports what-if and how-to queries accounting for probabilistic dependencies among attributes captured by a probabilistic causal model. We developed a declarative framework for expressing these hypothetical queries where the user can express constraints on pre-intervention and post-intervention values of variables, define their semantics, devise efficient algorithms and optimizations to compute their results using concepts from causality and probabilistic databases.\n\n \nThird, we developed several scalable and interpretable \"almost exact matching\" methods for causal inference to match treated and control units that are similar in terms of as many \"relevant\" covariates as possible. Along these methods two main algorithms are Dynamic Almost Matching Exactly (DAME) and Fast, Large-Scale Almost Matching Exactly (FLAME), which produce high-quality interpretable matched groups by using machine learning on a holdout training set to learn distance metrics and relevant covariates instead of relying on human inputs to specify these that can be prone to errors. DAME solves an optimization problem that matches units on as many covariates as possible, prioritizing matches on important covariates. FLAME approximates the solution found by DAME via a much faster backward feature selection procedure and has shown to be scalable for big datasets using database queries. In addition, we developed the Adaptive HyperBoxes (AHB) method for matching units with continuous covariate, the FLAME-Network method to apply the FLAME algorithm to scenarios where the units are connected in a network, and FLAME-IV to apply the FLAME algorithm to setups with \"instrumental variables\".  We have developed software packages for these algorithms that are available online with detailed documentations and examples. \n\n \n\n\t\t\t\t\tLast Modified: 03/10/2023\n\n\t\t\t\t\tSubmitted by: Sudeepa Roy"
 }
}