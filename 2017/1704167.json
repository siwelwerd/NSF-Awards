{
 "awd_id": "1704167",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Variance and Invariance in Voice Quality: Implications for Machine and Human Speaker Identification",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 851552.0,
 "awd_amount": 867552.0,
 "awd_min_amd_letter_date": "2017-06-21",
 "awd_max_amd_letter_date": "2022-06-10",
 "awd_abstract_narration": "A talker's voice quality conveys many kinds of information, including word and utterance prosody, emotional state, and personal identity. Variations in both the voice source and the vocal tract affect voice quality and there can be significant inter- and intra-talker variability. Understanding what aspects of a voice are talker-specific should aid in understanding the human limits in perceiving speaker differences and in developing better speaker identification (SID) algorithms. Despite technological advances, the performance of current SID systems remains far from perfect, and degrades significantly when the training and testing conditions are mismatched especially in terms of speech style (conversational versus read for example), speaker's emotional status, when the utterances are short, and when the task is text-independent. The key questions that the project aims to answer are: under normal daily life variability, how often does a talker sound less like him- or herself and more like someone else? Which acoustic properties account for speaker similarity? Can automatic speaker identification (SID) algorithms be improved by knowledge of which properties are important for human perception of speaker similarity?\r\nThe project is a transformative one and  helps better understand and model variance and invariance in voice quality. It will inform several important issues in human speech perception, especially in the area of talker similarity. Understanding what aspects of the source signal, if any, are talker-specific, should aid in developing better speaker identification and verification algorithms that are able to handle short utterances and are robust to varying affect and styles of speaking. A model of voice quality variations could also improve the naturalness of text-to-speech (TTS) systems. If it were known how much a person could change his or her voice quality without compromising their vocal identity, this knowledge could also inform medical rehab applications and forensics. A better understanding of voice quality will thus be of significant impact scientifically, and for engineering, forensic, and medical applications. The project has strong outreach and dissemination programs and fosters interdisciplinary activities in Electrical Engineering, Linguistics, and Speech and Hearing Science at UCLA and the Center of Excellence at JHU. It trains undergraduate and graduate students in important cross-disciplinary activities of technological and scientific significance.  The results will be published in high-quality journals and presented at relevant international conferences. The research results - a set of databases, software tools, and publications will be disseminated freely.\r\n\r\n\r\nThe project analyzes and discovers how the speech signal varies within and across talkers under circumstances that introduce variability in everyday life situations. Specifically, it investigates whether an individual talker's speech varies significantly across recording sessions and speech tasks. Most importantly, it examines how intra-talker variability from all these sources of variability compares with inter-talker variability. Understanding these issues requires a high-quality speech database with multiple voice samples from many talkers (in this case 200) which are collected, annotated, and distributed to other researchers.   Acoustic analyses reveals inter- and intra-talker variability in the speech signal across different situations by generating a multi- dimensional acoustic profile of each talker that specifies the range of parameter values that are typical in the corpus for that talker, and the likelihood of deviations from that usual profile. Perceptual studies determine the extent to which parameter profiles predict perceived similarity, and how much variability in each parameter can be tolerated before talkers cease to sound like themselves. Insights from the acoustic and perceptual studies guide the development of robust text-dependent and text-independent SID algorithms that are anticipated to be robust to variations in affect, style, and for short utterances.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Abeer",
   "pi_last_name": "Alwan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Abeer Alwan",
   "pi_email_addr": "alwan@ee.ucla.edu",
   "nsf_id": "000090848",
   "pi_start_date": "2017-06-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Patricia",
   "pi_last_name": "Keating",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Patricia A Keating",
   "pi_email_addr": "keating@humnet.ucla.edu",
   "nsf_id": "000092027",
   "pi_start_date": "2017-06-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jody",
   "pi_last_name": "Kreiman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jody Kreiman",
   "pi_email_addr": "jkreiman@ucla.edu",
   "nsf_id": "000441064",
   "pi_start_date": "2017-06-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900952000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 851552.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>An individual's voice is shaped in part by the varying physiological, emotional, social, and linguistic states that humans experience in different contexts, and, hence, significant acoustic variability arises within an individual's voice. Both listeners and machines face the challenge of attributing the differences they hear across utterances to a change in talkers or a change within a single talker. This suggests that listeners need to learn how a given voice varies to recognize it accurately and efficiently and must have knowledge of how qualities are distributed across a population to successfully discriminate among unknown talkers. &nbsp;This project attempts to answer questions related to the extent of variability within and across talkers, and to discover acoustic properties in the signal that account for speaker similarity. &nbsp;In addition, we aim at improving automatic speaker identification and verification algorithms by knowledge of which properties are important for human perception of speaker similarity.</p>\n<p><strong>Outcomes: </strong></p>\n<ol>\n<li><strong>Transcribed and released a database of high-quality acoustic      recordings that we collected with EAGER funding. </strong>We transcribed the database, made it      public and<strong> </strong>used it for analysis, perceptual experiments, and      speaker identification experiments. The database can be obtained from the Linguistic      Data Consortium:&nbsp; <a href=\"https://catalog.ldc.upenn.edu/LDC2021S09\">https://catalog.ldc.upenn.edu/LDC2021S09</a>. The 'UCLA Speaker Variability Database' is comprised of approximately 34 hours of American English speech from 100 male and 100 female speakers. It was designed to sample variability in speaking style within individual speakers and across a large number of speakers. Participants took part in six different tasks: vowel sounds, reading sentences, giving instructions, neutral conversation, happy conversation, a phone conversation, annoyed conversation, and responding to a video. The database is the first of its kind to sample variability within and across speakers on various, simple to more difficult, tasks </li>\n<li><strong>Characterized inter- and intra-speaker variability in the voice source across different situations, sound classes, and genders</strong>. Some of the important findings include discovering that for both reading and spontaneous speech, the most acoustic variance within talkers, regardless of gender, was accounted for by variability in higher-frequency harmonic and inharmonic energy, which are often associated with the degree of perceived breathiness in the voice. Another important feature was formant frequency dispersion. These acoustic variables are associated with important biological and social traits across many species, including gender, body size, arousal, and dominance</li>\n<li><strong>Conducted perceptual studies</strong> to determine the extent to which parameter profiles predict perceived similarity by human listeners, and how much (and what kind of) variability in each parameter can be tolerated before speakers cease to sound like themselves. In one study, for example, we compared human speaker discrimination performance for read speech versus casual conversations and explored differences between unfamiliar voices that are &ldquo;easy&rdquo; versus &ldquo;hard&rdquo; to &ldquo;tell together&rdquo; versus &ldquo;tell apart.&rdquo; Listeners performed better when stimuli were style-matched, particularly in read speech&minus;read speech trials. When styles were matched, listeners' confidence was higher than when speakers were the same versus different; however, style variation caused decreases in listeners' confidence for the &ldquo;same speaker&rdquo; trials, suggesting a higher dependency of this task on within-speaker variability. Analysis of speaker acoustic spaces suggested that the difference observed in human approaches to &ldquo;same speaker&rdquo; and &ldquo;different speaker&rdquo; tasks depends primarily on listeners' different perceptual strategies when dealing with within- versus between-speaker acoustic variability. </li>\n<li><strong>Developed robust speaker identification and verification algorithms.</strong> Our perceptual experiments show that      humans and machines seem to employ different approaches to speaker      discrimination, especially in the presence of speaking style variability.      The experiments examined read versus conversational speech. Listeners      focused on speaker-specific idiosyncrasies while \"telling speakers      together\", and on relative distances in a shared acoustic space when      \"telling speakers apart\". However, automatic speaker      verification (ASV) systems use the same loss function irrespective of      target or non-target trials. To improve ASV performance in the presence of      style variability, insights learnt from human perception were used to      design a new training loss function that uses both speaker-specific      idiosyncrasies and relative acoustic distances between speakers to train      the ASV system. Results show significant performance improvements. </li>\n</ol>\n<p><strong>Broader Impact:</strong></p>\n<p>Understanding what aspects of the source signal are talker-specific should aid in developing better speaker identification and verification algorithms that are robust to varying affect and styles of speaking. A model of voice variations could also improve the naturalness of text-to-speech (TTS) systems. If we knew how much a person could change his or her voice quality, and in what ways, without compromising their vocal identity, this knowledge could also inform medical rehab applications and forensics.</p>\n<p>The project fostered interdisciplinary activities in Electrical and Computer Engineering, Linguistics, and the Head and Neck Surgery Departments at UCLA. It trained several undergraduate and graduate students in important cross-disciplinary activities of technological and scientific significance.</p>\n<p>All publications have been posted on our webpages and presented at leading conferences and journals in Engineering, Computer Science, Speech Science and Linguistics.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/02/2024<br>\nModified by: Abeer&nbsp;A&nbsp;Alwan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAn individual's voice is shaped in part by the varying physiological, emotional, social, and linguistic states that humans experience in different contexts, and, hence, significant acoustic variability arises within an individual's voice. Both listeners and machines face the challenge of attributing the differences they hear across utterances to a change in talkers or a change within a single talker. This suggests that listeners need to learn how a given voice varies to recognize it accurately and efficiently and must have knowledge of how qualities are distributed across a population to successfully discriminate among unknown talkers. This project attempts to answer questions related to the extent of variability within and across talkers, and to discover acoustic properties in the signal that account for speaker similarity. In addition, we aim at improving automatic speaker identification and verification algorithms by knowledge of which properties are important for human perception of speaker similarity.\n\n\nOutcomes: \n\nTranscribed and released a database of high-quality acoustic      recordings that we collected with EAGER funding. We transcribed the database, made it      public and used it for analysis, perceptual experiments, and      speaker identification experiments. The database can be obtained from the Linguistic      Data Consortium: https://catalog.ldc.upenn.edu/LDC2021S09. The 'UCLA Speaker Variability Database' is comprised of approximately 34 hours of American English speech from 100 male and 100 female speakers. It was designed to sample variability in speaking style within individual speakers and across a large number of speakers. Participants took part in six different tasks: vowel sounds, reading sentences, giving instructions, neutral conversation, happy conversation, a phone conversation, annoyed conversation, and responding to a video. The database is the first of its kind to sample variability within and across speakers on various, simple to more difficult, tasks \nCharacterized inter- and intra-speaker variability in the voice source across different situations, sound classes, and genders. Some of the important findings include discovering that for both reading and spontaneous speech, the most acoustic variance within talkers, regardless of gender, was accounted for by variability in higher-frequency harmonic and inharmonic energy, which are often associated with the degree of perceived breathiness in the voice. Another important feature was formant frequency dispersion. These acoustic variables are associated with important biological and social traits across many species, including gender, body size, arousal, and dominance\nConducted perceptual studies to determine the extent to which parameter profiles predict perceived similarity by human listeners, and how much (and what kind of) variability in each parameter can be tolerated before speakers cease to sound like themselves. In one study, for example, we compared human speaker discrimination performance for read speech versus casual conversations and explored differences between unfamiliar voices that are easy versus hard to tell together versus tell apart. Listeners performed better when stimuli were style-matched, particularly in read speechread speech trials. When styles were matched, listeners' confidence was higher than when speakers were the same versus different; however, style variation caused decreases in listeners' confidence for the same speaker trials, suggesting a higher dependency of this task on within-speaker variability. Analysis of speaker acoustic spaces suggested that the difference observed in human approaches to same speaker and different speaker tasks depends primarily on listeners' different perceptual strategies when dealing with within- versus between-speaker acoustic variability. \nDeveloped robust speaker identification and verification algorithms. Our perceptual experiments show that      humans and machines seem to employ different approaches to speaker      discrimination, especially in the presence of speaking style variability.      The experiments examined read versus conversational speech. Listeners      focused on speaker-specific idiosyncrasies while \"telling speakers      together\", and on relative distances in a shared acoustic space when      \"telling speakers apart\". However, automatic speaker      verification (ASV) systems use the same loss function irrespective of      target or non-target trials. To improve ASV performance in the presence of      style variability, insights learnt from human perception were used to      design a new training loss function that uses both speaker-specific      idiosyncrasies and relative acoustic distances between speakers to train      the ASV system. Results show significant performance improvements. \n\n\n\nBroader Impact:\n\n\nUnderstanding what aspects of the source signal are talker-specific should aid in developing better speaker identification and verification algorithms that are robust to varying affect and styles of speaking. A model of voice variations could also improve the naturalness of text-to-speech (TTS) systems. If we knew how much a person could change his or her voice quality, and in what ways, without compromising their vocal identity, this knowledge could also inform medical rehab applications and forensics.\n\n\nThe project fostered interdisciplinary activities in Electrical and Computer Engineering, Linguistics, and the Head and Neck Surgery Departments at UCLA. It trained several undergraduate and graduate students in important cross-disciplinary activities of technological and scientific significance.\n\n\nAll publications have been posted on our webpages and presented at leading conferences and journals in Engineering, Computer Science, Speech Science and Linguistics.\n\n\n\t\t\t\t\tLast Modified: 03/02/2024\n\n\t\t\t\t\tSubmitted by: AbeerAAlwan\n"
 }
}