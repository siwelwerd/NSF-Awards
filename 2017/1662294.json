{
 "awd_id": "1662294",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Taylor Expansion Approximations for Dynamic Programming Problems",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922443",
 "po_email": "gaklutke@nsf.gov",
 "po_sign_block_name": "Georgia-Ann Klutke",
 "awd_eff_date": "2017-06-01",
 "awd_exp_date": "2020-11-30",
 "tot_intn_awd_amt": 349971.0,
 "awd_amount": 349971.0,
 "awd_min_amd_letter_date": "2017-05-17",
 "awd_max_amd_letter_date": "2017-05-17",
 "awd_abstract_narration": "Operational decision-making in service and manufacturing environments often requires that decisions respond to real-time changes in available resources and system characteristics. Because these operating environments are generally quite complex, capturing the dynamic nature of the system is often very difficult. This project aims to help the decision maker manage dynamic complexity by offering a structured approach to the approximation of dynamic decision problems. The results of this project will advance operational methods in a variety of domains, including healthcare operations and production and distribution of goods and services. The project will educate graduate students engaged in a diverse set of industry-related programs.\r\n\r\nThis project will utilize Stein's method to create novel approximate solution techniques for stochastic dynamic programing (DP) problems. Stein's method has recently been used in the context of queueing models to bound the error when approximating the performance of a queuing system by that of a suitable Brownian model. This project will extend that approach to the study of controlled Markov processes, thus moving beyond performance analysis and into optimization. The research will result in a structured approximation approach that allows for explicit examination of the optimality gap between the true optimal solution (as captured by the Bellman equation) and the optimal solution of a Brownian control problem (as captured by a Hamilton-Jacobi-Bellman equation). If successful, the research will lead to computationally efficient approximation methods for DP problems with explicit guarantees of  \"near optimality\". The research will advance the mathematical understanding of the relationship between Markov decision processes and Brownian control problems beyond the context of queueing",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Itai",
   "pi_last_name": "Gurvich",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Itai Gurvich",
   "pi_email_addr": "i-gurvich@kellogg.northwestern.edu",
   "nsf_id": "000729844",
   "pi_start_date": "2017-05-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell Tech",
  "perf_str_addr": "111 8th Ave STE 302",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100115204",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "006Y00",
   "pgm_ele_name": "OE Operations Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "8023",
   "pgm_ref_txt": "Health Care Enterprise Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 349971.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Dynamic programming is the fundamental technique for solving sequential decision problems such as those prevalent in revenue, inventory, and workflow management. The key object of analysis is the Bellman optimality equation; this equation, when solved, produces a policy: specifying the optimal action to take at each possible state of the system. It also produced the value function---that specifies for each state the reward or cost collected if we start at that state and follow the optimal policy. As the dimension of the state space increases, then, the computational burden of solving the Bellman equation becomes prohibitive. Approximate dynamic programming (ADP) is a family of algorithms developed to address this computational challenge by reducing---through various mechanism---the computational burden.</p>\n<p>In the body of work funded by this award, we introduce a new mathematical and algorithmic framework for approximate dynamic programming. Mathematically, the framework is grounded in the connection between the Bellman equation, as originally specified on a discrete state space--- with a Partial Differential Equation (PDE) in which (in the spirit of the central limit theorem) the transition matrix is reduced to its &#64257;rst and second moments. The PDE prescribes, as well, a policy. We prove bounds on the optimality gap --- the loss in optimality from using the PDE-prescribed policy instead of the optimal one.</p>\n<p>Two controlled systems, whose transition dynamics share the same first and second moment (even if their transition matrix is different) translate to the same PDE and this fact provides the scaffolding for algorithm development. Our algorithm bypasses the solution of a PDE. Instead, it constructs a ``sister'' process whose two local transition moments are (approximately) identical with those of the original one. Because they share these moments, the original process and its ``sister'' are coupled through the PDE, a coupling that facilitates optimality guarantees.</p>\n<p>For the construction of the sister process we use the well-studied state-aggregation ADP method as a central building block. The parameters of aggregation&mdash;the so-called aggregation and disaggregation matrices---are tuned for moment matching. In this way, the computational strength of aggregation is combined with the mathematical underpinning of moment matching to develop an algorithm that is appealing in its implementation simplicity and whose performance can be apriori guaranteed.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/29/2021<br>\n\t\t\t\t\tModified by: Itai&nbsp;Gurvich</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDynamic programming is the fundamental technique for solving sequential decision problems such as those prevalent in revenue, inventory, and workflow management. The key object of analysis is the Bellman optimality equation; this equation, when solved, produces a policy: specifying the optimal action to take at each possible state of the system. It also produced the value function---that specifies for each state the reward or cost collected if we start at that state and follow the optimal policy. As the dimension of the state space increases, then, the computational burden of solving the Bellman equation becomes prohibitive. Approximate dynamic programming (ADP) is a family of algorithms developed to address this computational challenge by reducing---through various mechanism---the computational burden.\n\nIn the body of work funded by this award, we introduce a new mathematical and algorithmic framework for approximate dynamic programming. Mathematically, the framework is grounded in the connection between the Bellman equation, as originally specified on a discrete state space--- with a Partial Differential Equation (PDE) in which (in the spirit of the central limit theorem) the transition matrix is reduced to its &#64257;rst and second moments. The PDE prescribes, as well, a policy. We prove bounds on the optimality gap --- the loss in optimality from using the PDE-prescribed policy instead of the optimal one.\n\nTwo controlled systems, whose transition dynamics share the same first and second moment (even if their transition matrix is different) translate to the same PDE and this fact provides the scaffolding for algorithm development. Our algorithm bypasses the solution of a PDE. Instead, it constructs a ``sister'' process whose two local transition moments are (approximately) identical with those of the original one. Because they share these moments, the original process and its ``sister'' are coupled through the PDE, a coupling that facilitates optimality guarantees.\n\nFor the construction of the sister process we use the well-studied state-aggregation ADP method as a central building block. The parameters of aggregation&mdash;the so-called aggregation and disaggregation matrices---are tuned for moment matching. In this way, the computational strength of aggregation is combined with the mathematical underpinning of moment matching to develop an algorithm that is appealing in its implementation simplicity and whose performance can be apriori guaranteed.\n\n\t\t\t\t\tLast Modified: 03/29/2021\n\n\t\t\t\t\tSubmitted by: Itai Gurvich"
 }
}