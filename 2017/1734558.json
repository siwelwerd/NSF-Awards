{
 "awd_id": "1734558",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: INT: Learning-Enabled Robot Support of Daily Activities for Successful Activity Completion",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922568",
 "po_email": "wnilsen@nsf.gov",
 "po_sign_block_name": "Wendy Nilsen",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 999999.0,
 "awd_amount": 999999.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2018-03-26",
 "awd_abstract_narration": "The population is aging - the estimated number of individuals over the age of 85 will triple by 2050. Even in our current population an estimated 9% of adults age 65+ and 50% of adults age 85+ need assistance with everyday activities, and the annual cost for the United States is roughly $2 trillion. Given the economic and quality of life costs, there is a critical need to better use smart technologies so that individuals can live independently in their own homes, helping both individuals and society as a whole. This proposed work will create a novel multi-agent robot system, called Robotic Activity Support (RAS), that provides in-home activity support for older adults and others that need assistance to independently perform common activities of daily living. The system will rely on cooperation between a smart home and a mobile robot to learn activity routines for an individual. RAS will use this information to provide activity interventions that help smart home residents initiate and successfully complete important daily activities and improve functional independence. \r\n\r\nRather than explore co-robot systems with multiple identical platforms, the system will represent a collaboration between a mobile robot, a smart home agent with multiple heterogeneous sensors and multiple humans with distinct roles. For this collaboration, the project team will employ a custom robot that will partner with the team's CASAS smart home architecture.  RAS will incorporate caregiver-in-the-loop active learning to improve its models. the team will use an iterative user-centered development process to enhance the mobile robotic platform design. The RAS system will use active learning from both the resident and caregiver to learn common activities and how it can support such activities. For example, the robot may prompt a resident to eat breakfast if she does not initiate the task at the normal time, remind the resident where the cereal is, and notify a nearby caregiver if help is needed that is beyond the robot's capabilities. The team will evaluate RAS on historic smart home data (from their more than 100 deployments), in their on-campus smart home, and in a home with a healthy older adult caregiver, as well as an older adult exhibiting cognitive limitations.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Diane",
   "pi_last_name": "Cook",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Diane J Cook",
   "pi_email_addr": "cook@eecs.wsu.edu",
   "nsf_id": "000294757",
   "pi_start_date": "2018-03-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Taylor",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew Taylor",
   "pi_email_addr": "taylorm@eecs.wsu.edu",
   "nsf_id": "000560224",
   "pi_start_date": "2017-07-27",
   "pi_end_date": "2018-03-26"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Diane",
   "pi_last_name": "Cook",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Diane J Cook",
   "pi_email_addr": "cook@eecs.wsu.edu",
   "nsf_id": "000294757",
   "pi_start_date": "2017-07-27",
   "pi_end_date": "2018-03-26"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Maureen",
   "pi_last_name": "Schmitter-Edgecombe",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Maureen E Schmitter-Edgecombe",
   "pi_email_addr": "schmitter-e@wsu.edu",
   "nsf_id": "000207605",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Taylor",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew Taylor",
   "pi_email_addr": "taylorm@eecs.wsu.edu",
   "nsf_id": "000560224",
   "pi_start_date": "2018-03-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington State University",
  "inst_street_address": "240 FRENCH ADMINISTRATION BLDG",
  "inst_street_address_2": "",
  "inst_city_name": "PULLMAN",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "5093359661",
  "inst_zip_code": "991640001",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "WA05",
  "org_lgl_bus_name": "WASHINGTON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "XRJSGX384TD6"
 },
 "perf_inst": {
  "perf_inst_name": "Washington State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "991642752",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "WA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 999999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As older adults age, they may require assistance completing activities of daily living<br />(ADLs). Robotic assistance can offset healthcare costs and allow older adults to preserve their<br />autonomy. Memory impairment, the hallmark symptom of mild cognitive impairment (MCI), can<br />interfere with the successful completion of ADLs. All these robotic aids, while designed to<br />support OAs in their daily routines, lack one thing: they are not capable of helping OAs when<br />they are unable to remember how to complete a task without human intervention. To this end, we<br />created the Robot Activity Support (RAS) system and robot. Paired with a<br />smart home environment, the RAS robot is able to monitor a resident?s activity patterns,<br />recognize when an error is made in task execution and intervene to offer help in a multi-modal<br />fashion.</p>\n<p>The robot (RAS) is an amalgamation of elements. At the top of the robot is a RGBD camera<br />that allows the robot to detect both the objects involved in everyday tasks,<br />and the human in its environment. Located at the<br />bottom of the robot are the components of the navigation manager (e.g.,<br />the LiDAR and computational units) and the hardware required for the<br />robot to maneuver in a space (e.g., wheels and battery). The robot also has a mounted Android tablet, which serves as the<br />point of interaction for users. When users forget to perform a step in a<br />task,lack of interaction with the forgotten object?s attached accelerometer-based motion sensor. Immediately upon error detection, the<br />tablet plays an audio clip asking the participant if they need help (i.e.,<br />?Hello, how can I help you??) and begins approaching the person. The<br />screen display changes from a neutral facial expression to a surprised one<br />and displays the question ?Do you want help??. If the participant selects<br />?Yes?, the robot will offer to play a video of the entire task without errors, a video of the forgotten/incorrect step, or lead the user to the object that is needed for the forgotten/missed step.</p>\n<p>This technology requires a close partnership between many components including smart home-based activity monitoring and error detection, space mapping, robot navigation and control, object and human finding, tablet interface, and system connectivity. We demonstrated the success of the project for scripted activities and errors based on 54 participants in an on-campus smart home. We also let RAS detect and assist with errors for three continuous days in the home of one younger adult and one older adult.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/04/2021<br>\n\t\t\t\t\tModified by: Diane&nbsp;J&nbsp;Cook</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630761167244_f6--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630761167244_f6--rgov-800width.jpg\" title=\"RAS partnership\"><img src=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630761167244_f6--rgov-66x44.jpg\" alt=\"RAS partnership\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">RAS partners with a smart home to offer activity support. Passive infrared motion sensors installed on the ceiling of a smart home collect data to monitor and analyze activity performance. When needed, the RAS mobile robot steps in to offer activity assistance.</div>\n<div class=\"imageCredit\">Washington State University CASAS Lab</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Diane&nbsp;J&nbsp;Cook</div>\n<div class=\"imageTitle\">RAS partnership</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760833845_f2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760833845_f2--rgov-800width.jpg\" title=\"RAS cyber-physical system\"><img src=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760833845_f2--rgov-66x44.jpg\" alt=\"RAS cyber-physical system\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">RAS software, hardware, and communication components. Components are located within a CASAS smart home, a CASAS server, the RAS on-board computer (Joule or Jetson), robot hardware, or an Android tablet interface. Components communicate using RabbitMQ or ROS.</div>\n<div class=\"imageCredit\">Washington State University CASAS Lab</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Diane&nbsp;J&nbsp;Cook</div>\n<div class=\"imageTitle\">RAS cyber-physical system</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760906454_f3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760906454_f3--rgov-800width.jpg\" title=\"RAS activity monitoring\"><img src=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760906454_f3--rgov-66x44.jpg\" alt=\"RAS activity monitoring\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">RAS shows a passive avatar expression when it is partnering with the smart home in monitoring mode.</div>\n<div class=\"imageCredit\">Washington State University CASAS Lab</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Diane&nbsp;J&nbsp;Cook</div>\n<div class=\"imageTitle\">RAS activity monitoring</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760979573_f4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760979573_f4--rgov-800width.jpg\" title=\"RAS interface\"><img src=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760979573_f4--rgov-66x44.jpg\" alt=\"RAS interface\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">RAS shows an inquisitive expression while asking the resident if they want help with their current activity.</div>\n<div class=\"imageCredit\">Washington State University CASAS Lab</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Diane&nbsp;J&nbsp;Cook</div>\n<div class=\"imageTitle\">RAS interface</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630761067764_f5--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630761067764_f5--rgov-800width.jpg\" title=\"RAS activity assistance\"><img src=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630761067764_f5--rgov-66x44.jpg\" alt=\"RAS activity assistance\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">If requested, RAS will play a video of the missed or forgotten activity step to help individuals remember and complete important activities.</div>\n<div class=\"imageCredit\">Washington State University CASAS Lab</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Diane&nbsp;J&nbsp;Cook</div>\n<div class=\"imageTitle\">RAS activity assistance</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760786937_f1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760786937_f1--rgov-800width.jpg\" title=\"The RAS robot.\"><img src=\"/por/images/Reports/POR/2021/1734558/1734558_10507210_1630760786937_f1--rgov-66x44.jpg\" alt=\"The RAS robot.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">RAS provides activity support for individuals to promote functional independence.</div>\n<div class=\"imageCredit\">Washington State University CASAS Lab</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Diane&nbsp;J&nbsp;Cook</div>\n<div class=\"imageTitle\">The RAS robot.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nAs older adults age, they may require assistance completing activities of daily living\n(ADLs). Robotic assistance can offset healthcare costs and allow older adults to preserve their\nautonomy. Memory impairment, the hallmark symptom of mild cognitive impairment (MCI), can\ninterfere with the successful completion of ADLs. All these robotic aids, while designed to\nsupport OAs in their daily routines, lack one thing: they are not capable of helping OAs when\nthey are unable to remember how to complete a task without human intervention. To this end, we\ncreated the Robot Activity Support (RAS) system and robot. Paired with a\nsmart home environment, the RAS robot is able to monitor a resident?s activity patterns,\nrecognize when an error is made in task execution and intervene to offer help in a multi-modal\nfashion.\n\nThe robot (RAS) is an amalgamation of elements. At the top of the robot is a RGBD camera\nthat allows the robot to detect both the objects involved in everyday tasks,\nand the human in its environment. Located at the\nbottom of the robot are the components of the navigation manager (e.g.,\nthe LiDAR and computational units) and the hardware required for the\nrobot to maneuver in a space (e.g., wheels and battery). The robot also has a mounted Android tablet, which serves as the\npoint of interaction for users. When users forget to perform a step in a\ntask,lack of interaction with the forgotten object?s attached accelerometer-based motion sensor. Immediately upon error detection, the\ntablet plays an audio clip asking the participant if they need help (i.e.,\n?Hello, how can I help you??) and begins approaching the person. The\nscreen display changes from a neutral facial expression to a surprised one\nand displays the question ?Do you want help??. If the participant selects\n?Yes?, the robot will offer to play a video of the entire task without errors, a video of the forgotten/incorrect step, or lead the user to the object that is needed for the forgotten/missed step.\n\nThis technology requires a close partnership between many components including smart home-based activity monitoring and error detection, space mapping, robot navigation and control, object and human finding, tablet interface, and system connectivity. We demonstrated the success of the project for scripted activities and errors based on 54 participants in an on-campus smart home. We also let RAS detect and assist with errors for three continuous days in the home of one younger adult and one older adult.\n\n\t\t\t\t\tLast Modified: 09/04/2021\n\n\t\t\t\t\tSubmitted by: Diane J Cook"
 }
}