{
 "awd_id": "1723963",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: FND: Norm Processing for Autonomous Social Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 599998.0,
 "awd_amount": 599998.0,
 "awd_min_amd_letter_date": "2017-07-27",
 "awd_max_amd_letter_date": "2017-07-27",
 "awd_abstract_narration": "As intelligent systems continue to become more autonomous, it is important to imbue such systems with the types of social and moral norms that are deeply ingrained in human cognition and behavior.  Failing to abide by these norms typically causes social reactions from humans, from blame and reprimands, in simple cases, all the way to full-fledged legal consequences.  Given the fundamental social expectations humans have of each other, it is very likely that they will extend and apply those to artificial agents as well.  Hence, intelligent autonomous systems will need to be endowed with computational mechanisms that ensure their ethical behavior.\r\n\r\nThis project will develop explicit norm representations and planning algorithms for norm-conforming behavior that will address shortcomings of previous approaches, enabling explicit, temporally complex norm representations in stochastic worlds that allow for systematic handling of norm conflicts in ways that maximize the norms an agent can obey at any moment in time.  In particular, explicit norm specifications will  allow for generalization across unobserved states.  In addition, it is important for systems interacting with humans to make decisions that are transparent.  To that end, the approach will enable the generation of justifications in cases where an autonomous system is forced to violate some norms because not all can be satisfied simultaneously.  The approach and algorithms developed will be demonstrated on a fully autonomous cleaning robot with predefined norms in different scenarios, where conflicting norms are applicable and the robot must determine which norms to follow and which to suspend.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matthias",
   "pi_last_name": "Scheutz",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Matthias J Scheutz",
   "pi_email_addr": "matthias.scheutz@tufts.edu",
   "nsf_id": "000289027",
   "pi_start_date": "2017-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Tufts University",
  "inst_street_address": "80 GEORGE ST",
  "inst_street_address_2": "",
  "inst_city_name": "MEDFORD",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6176273696",
  "inst_zip_code": "021555519",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "TRUSTEES OF TUFTS COLLEGE",
  "org_prnt_uei_num": "WL9FLBRVPJJ7",
  "org_uei_num": "WL9FLBRVPJJ7"
 },
 "perf_inst": {
  "perf_inst_name": "Tufts University",
  "perf_str_addr": "200 Boston Ave",
  "perf_city_name": "Medford",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021554243",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 599998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Human societies are structured by norms. Members that violate normsare blamed and required to justify their actions.&nbsp; When robots areembedded in human contexts, they need to be aware of human normativeexpectations, as otherwise they will inevitably violate norms withoutknowing.&nbsp; The result can be a loss of trust in the artificial agentsand ultimately their complete rejection.&nbsp; Hence, it is important todevelop algorithms that allow artificial agents like robots to learnnorms and apply them in human-like ways when making decisions andcarrying out actions.&nbsp; The challenge, however, with learning and usinghuman norms is that they do not form a consistent set of rules, butare often mutually inconsistent, with one norm demanding a particularaction while a another prohibits it.<br />The central objective of this project was thus to develop algorithmsthat could adjudicate among different conflicting norms.&nbsp; Thisinvolved weighing the \"cost\" of violating one set of norms against thecosts of violating others.&nbsp; The algorithm then determined the smallestset of norms with the smallest \"violation cost\" that would have to beviolated when it is not possible to obey all norms simultaneously.&nbsp; Itthen selected actions to obey all other norms and only \"suspended\" theviolating ones for the shorted time possible.<br />In conjunction with the development of a norm conflict resolutionalgorithm, it was imperative to develop methods for artificial agentsto communicate their choices to human interactants, supervisors, andinstruction givers. These explanations, to be acceptable to humans,had to include recourse to the involved normative principles.&nbsp; Hencethe robot needed to have explicit knowledge of normative principles tobegin with (which is different from other approaches in AI that only\"implicitly\" follow norms, i.e., do the right thing without knowingwhy).<br />In addition to developing novel algorithms for norm conflictresolution and norm-based explanations, a simulated supermarketenvironment was also developed to evaluate the algorithms in simulatedshopping agents. In this environment, agents need to perform \"shoppingtasks\", fetching all items on their shopping list, putting them in abasket, and paying for them before leaving the store.&nbsp; Players arescored based on the number of norms they violate in the shopping task(e.g., forgetting to pay, blocking an isle, leaving a cart unattended,etc.).&nbsp; By allowing human players to interact with the artificialagents in the supermarket simulation, it was possible to collect dataabout human perceptions of norm-violating agents compared tonorm-abiding agents using the algorithms developed in this project.The simulation was also used as a novel tool for teaching in a courseon the \"Ethics in AI, Robotics and Human-Robot Interaction\" course.In this course students learned about different approaches towardsethical AI and had the opportunity to implement these differentmethods in their simulated shopping agents.&nbsp; Observing the agentsperform the task in a crowded store then allowed students to evaluatewhen their algorithms worked and failed at generating normativebehavior.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/06/2021<br>\n\t\t\t\t\tModified by: Matthias&nbsp;J&nbsp;Scheutz</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHuman societies are structured by norms. Members that violate normsare blamed and required to justify their actions.  When robots areembedded in human contexts, they need to be aware of human normativeexpectations, as otherwise they will inevitably violate norms withoutknowing.  The result can be a loss of trust in the artificial agentsand ultimately their complete rejection.  Hence, it is important todevelop algorithms that allow artificial agents like robots to learnnorms and apply them in human-like ways when making decisions andcarrying out actions.  The challenge, however, with learning and usinghuman norms is that they do not form a consistent set of rules, butare often mutually inconsistent, with one norm demanding a particularaction while a another prohibits it.\nThe central objective of this project was thus to develop algorithmsthat could adjudicate among different conflicting norms.  Thisinvolved weighing the \"cost\" of violating one set of norms against thecosts of violating others.  The algorithm then determined the smallestset of norms with the smallest \"violation cost\" that would have to beviolated when it is not possible to obey all norms simultaneously.  Itthen selected actions to obey all other norms and only \"suspended\" theviolating ones for the shorted time possible.\nIn conjunction with the development of a norm conflict resolutionalgorithm, it was imperative to develop methods for artificial agentsto communicate their choices to human interactants, supervisors, andinstruction givers. These explanations, to be acceptable to humans,had to include recourse to the involved normative principles.  Hencethe robot needed to have explicit knowledge of normative principles tobegin with (which is different from other approaches in AI that only\"implicitly\" follow norms, i.e., do the right thing without knowingwhy).\nIn addition to developing novel algorithms for norm conflictresolution and norm-based explanations, a simulated supermarketenvironment was also developed to evaluate the algorithms in simulatedshopping agents. In this environment, agents need to perform \"shoppingtasks\", fetching all items on their shopping list, putting them in abasket, and paying for them before leaving the store.  Players arescored based on the number of norms they violate in the shopping task(e.g., forgetting to pay, blocking an isle, leaving a cart unattended,etc.).  By allowing human players to interact with the artificialagents in the supermarket simulation, it was possible to collect dataabout human perceptions of norm-violating agents compared tonorm-abiding agents using the algorithms developed in this project.The simulation was also used as a novel tool for teaching in a courseon the \"Ethics in AI, Robotics and Human-Robot Interaction\" course.In this course students learned about different approaches towardsethical AI and had the opportunity to implement these differentmethods in their simulated shopping agents.  Observing the agentsperform the task in a crowded store then allowed students to evaluatewhen their algorithms worked and failed at generating normativebehavior.\n\n\t\t\t\t\tLast Modified: 11/06/2021\n\n\t\t\t\t\tSubmitted by: Matthias J Scheutz"
 }
}