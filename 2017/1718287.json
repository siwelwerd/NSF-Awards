{
 "awd_id": "1718287",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Collaborative Research: Flexible Resource Management and Coordination Schemes for Lightweight, Rapidly Deployable OS/Rs",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Erik Brunvand",
 "awd_eff_date": "2017-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 249964.0,
 "awd_amount": 249964.0,
 "awd_min_amd_letter_date": "2017-07-12",
 "awd_max_amd_letter_date": "2017-07-12",
 "awd_abstract_narration": "Current cloud systems leverage either heavy-weight virtualization (running applications inside full-fledged virtual machines (VMs) with their own operating systems) or containers (light-weight software environments that share a single underlying operating system). This collaborative project aims to increase the efficiency of cloud systems by enabling the rapid deployment of specialized operating system and runtime (SOS/R) environments optimized for particular applications or runtime systems. This will allow users of applications with special performance requirements to enjoy the convenience of cloud infrastructure without sacrificing performance or design constraints imposed by general-purpose operating systems.\r\n\r\nThe project will advance the state of the art in cloud system software by: (a) enabling the development of a general specialized hosting framework allowing the deployment of arbitrary system software stacks as both VMs and native co-kernel instances; (b) a set of interfaces and mechanisms to allow SOS/R environments to react to highly dynamic and variable hardware resource availability; (c) novel approaches that leverage hardware virtualization features to host SOS/Rs; and (d) a thorough evaluation of all of these approaches using realistic cloud workloads at scale.\r\n\r\nThis work will enable users of cloud systems to efficiently deploy diverse and specialized system architectures. This will not only enhance the capabilities of existing cloud and data center platforms, but will also enable the deployment of new types of operating systems and runtimes. Prototype systems produced in this effort will be made available to the general public as open-source software, and will also be leveraged as teaching and platforms for courses in operating systems, virtualization, runtime environments, and general cloud computing. \r\n\r\nData, code, and results generated from the project will be made freely available indefinitely on the team's webpages. All software packages will be accompanied by instructions for building and running them using freely available open-source tools. Data and results will be made available in compressed archive formats. Links to specific repositories (such as git repositories for code) will be made public either at http://hexsa.halek.co or http://prognosticlab.org.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Lange",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "John R Lange",
   "pi_email_addr": "jacklange@cs.pitt.edu",
   "nsf_id": "000576978",
   "pi_start_date": "2017-07-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pittsburgh",
  "inst_street_address": "4200 FIFTH AVENUE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4126247400",
  "inst_zip_code": "152600001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "MKAGLD59JRL1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pittsburgh",
  "perf_str_addr": "University of Pittsburgh",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152132303",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 249964.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project introduced a number of novel contributions related to the use of hardware virtualization features in the context of native multi-stack/co-kernel Specialized Operating System/Runtime (SOS/R) configurations. Existing approaches all required that co-kernel SOS/R instances run at the highest privilege level on a system and have de-facto access to all of the hardware resources present. With the Covirt Hypervisor developed as part of this project we were able to demonstrate that these co-kernel configurations could include resource protection without impacting either the performance or operation of the co-kernel environments. This approach has the ability to significantly improve the security and resiliency of co-kernel based environments as it ensures that bugs and faults in one SOS/R instance are not able to directly impact other SOS/Rs running on the system. This work addresses one of the key weaknesses with the co-kernel approach that has hitherto not been addressed by any of the existing work. In addition this work demonstrated the portability of the Pisces co-kernel environment by porting the Nautilus SOS/R environment to the Pisces infrastructure. This was able to demonstrate the potential of running mulitple heterogeneous SOS/R environments natively on the same underlying physical server. Finally, this project developed mechanisms to migrate SOS/R instances across virtualization infrastructure to allow dynamic resource partitioning and management. All told, this project introduced a number of significant new capabilties for co-kernel/multi-stack software environments that directly addressed a number of fundamental weaknesses and shortcomings.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/24/2022<br>\n\t\t\t\t\tModified by: John&nbsp;R&nbsp;Lange</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project introduced a number of novel contributions related to the use of hardware virtualization features in the context of native multi-stack/co-kernel Specialized Operating System/Runtime (SOS/R) configurations. Existing approaches all required that co-kernel SOS/R instances run at the highest privilege level on a system and have de-facto access to all of the hardware resources present. With the Covirt Hypervisor developed as part of this project we were able to demonstrate that these co-kernel configurations could include resource protection without impacting either the performance or operation of the co-kernel environments. This approach has the ability to significantly improve the security and resiliency of co-kernel based environments as it ensures that bugs and faults in one SOS/R instance are not able to directly impact other SOS/Rs running on the system. This work addresses one of the key weaknesses with the co-kernel approach that has hitherto not been addressed by any of the existing work. In addition this work demonstrated the portability of the Pisces co-kernel environment by porting the Nautilus SOS/R environment to the Pisces infrastructure. This was able to demonstrate the potential of running mulitple heterogeneous SOS/R environments natively on the same underlying physical server. Finally, this project developed mechanisms to migrate SOS/R instances across virtualization infrastructure to allow dynamic resource partitioning and management. All told, this project introduced a number of significant new capabilties for co-kernel/multi-stack software environments that directly addressed a number of fundamental weaknesses and shortcomings. \n\n\t\t\t\t\tLast Modified: 01/24/2022\n\n\t\t\t\t\tSubmitted by: John R Lange"
 }
}