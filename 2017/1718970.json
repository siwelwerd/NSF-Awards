{
 "awd_id": "1718970",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CCF-BSF:  AF:  Small:  Convex and Non-Convex Distributed Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2018-01-01",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 249978.0,
 "awd_amount": 257978.0,
 "awd_min_amd_letter_date": "2017-07-10",
 "awd_max_amd_letter_date": "2019-07-15",
 "awd_abstract_narration": "Machine learning is an increasingly important approach in tackling many difficult scientific, engineering and artificial intelligence tasks, ranging from machine translation and speech recognition, through control of self driving cars, to protein structure prediction and drug design.\u00a0 The core idea of machine learning is to use examples and data to automatically train a system to perform some task.\u00a0 Accordingly, the success of machine learning is tied to availability of large amounts of training data and our ability to process it.\u00a0 Much of the recent success of machine learning is fueled by the large amounts of data (text, images, videos, etc) that can now be collected.  But all this data also needs to be processed and learned from---indeed this data flood has shifted the bottleneck, to a large extent, from availability of data to our ability to process it.  In particular, the amounts of data involved can no longer be stored and handled on single computers.\u00a0 Consequently, distributed machine learning, where data is processed and learned from on many computers that communicate with each other, is a crucial element of modern large scale machine learning.\r\n\r\nThe goal of this project is to provide a rigorous framework for studying distributed machine learning, and through it develop efficient methods for distributed learning and a theoretical understanding of the benefits of these methods, as well as the inherent limitations of distributed learning.\u00a0 A central component in the PIs' approach is to model distributed learning as a stochastic optimization problem, where different machines receive samples drawn from the same source distribution, thus allowing methods and analysis that specifically leverage the relatedness between data on different machines.\u00a0 This is crucial for studying how availability of multiple computers can aid in reducing the computational cost of learning.  Furthermore, the project also encompasses the more challenging case where there are significant differences between the nature of the data on different machines (for instance, when different machines serve different geographical regions, or when each machine is a personal device, collecting data from a single user).\u00a0 In such a situation, the proposed approach to be studied is to integrate distributed learning with personalization or adaptation, which the PIs argue can not only improve learning performance, but also better leverage distributed computation.\r\n\r\nThis is an international collaboration, made possible through joint funding with the US-Israel Binational Science Foundation (BSF).  The project brings together two PIs that have worked together extensively on related topics in machine learning and optimization.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nathan",
   "pi_last_name": "Srebro",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nathan Srebro",
   "pi_email_addr": "nati@ttic.edu",
   "nsf_id": "000489181",
   "pi_start_date": "2017-07-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Toyota Technological Institute at Chicago",
  "inst_street_address": "6045 S KENWOOD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7738340409",
  "inst_zip_code": "606372803",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO",
  "org_prnt_uei_num": "ERBJF4DMW6G4",
  "org_uei_num": "ERBJF4DMW6G4"
 },
 "perf_inst": {
  "perf_inst_name": "Toyota Technological Institute at Chicago",
  "perf_str_addr": "6045 S Kenwood Ave",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606372902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "014Z",
   "pgm_ref_txt": "NSF and US-Israel Binational Science Fou"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7934",
   "pgm_ref_txt": "PARAL/DISTRIBUTED ALGORITHMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 249978.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-bf8aae00-7fff-3c2d-589b-61290f8687a5\"> </span></p>\n<p dir=\"ltr\"><span>The goal of the project was to study algorithmic and foundational aspects of training machine learning systems on distributed platforms, in particular using distributed stochastic optimization.&nbsp; Stochastic optimization refers to optimizing an objective, searching for a good data fit, or training a system, by considering only a few examples at each iteration, as opposed to repeatedly considering the entire data set.&nbsp; It traces its roots to a classic 1951 paper by Robbins and Monro, has been a staple of machine learning ever since the first learning systems in the 1950s and 1960s, and to this day stochastic optimization techniques, such as Stochastic Gradient Descent (SGD) form the basis of most machine learning training procedures.&nbsp; A firm theoretical foundation for convex stochastic approximation was provided by the seminal work of Nemirovskii and Yudin in the 1970s, and exact optimal complexities and methods were derived by Lan in 2012 based on Nemirovskii&rsquo;s work and Nesterov&rsquo;s 1984 follow-up.&nbsp; But modern machine learning frequently departs from the sequential settings and relies on parallel and distributed computation in order to handle massive data sets. &nbsp; This includes parallelization at many scales, ranging from parallelization across cores in a GPU or multiple processors in a single computer, through parallelization across servers in a data center, to distributed computation on edge devices (e.g. individual phones, cars, or sensors) as in Federated Learning.&nbsp; In this project we provide a firm theoretical framework for stochastic distributed optimization, develop methods with provable guarantees, and establish what is the best that can be done, and how.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The major achievement of the project, recognized by the Best Paper Award at the Conference on Learning Theory (the leading venue for such research) was establishing the &ldquo;optimal complexity&rdquo; of stochastic convex distributed optimization.&nbsp; That is, what is the best that could be achieved, in terms of the required computation and communication, and subject to established assumptions, and what methods achieve this performance.&nbsp; Previous work done as part of the project built towards this by providing the theoretical grounding, describing the relevant models, and studying the behavior of different methods, including showing that some commonly used methods can actually be problematic in some important regimes.&nbsp; Understanding what is the best that can be done using the standard assumptions and standard model, also allows us to go further, by allowing us to understand what additional assumptions or stronger types of operations we must use in order to break this barrier and obtain better methods, as we have done in work employing &ldquo;Hessian-vector-prodcuts&rdquo;.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Improvements in distributed stochastic optimization directly translate into faster training with less communication in many scientific and engineering applications of machine learning.&nbsp; E.g. through our interactions with Google, these advancements are quickly finding their way to reducing communication in Federated Learning.&nbsp; Beyond the reduction in resource consumption, Federated Learning also aims at improving user privacy by maintaining user data on their own devices, which now become nodes in a distributed training/optimization process.&nbsp; Reductions in communication may thus also lead to improved privacy.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The project resulted in 16 refereed conference and journal publications, including two awards (Best Student Paper and Best Paper) at the Conference on Learning Theory, which is the premier venue for research on the theory of machine learning.&nbsp; This project was an international collaboration between PI Nathan Srebro at TTI-Chicago and PI Ohad Shamir at the Weizmann Institute of Science in Israel.&nbsp; The project served as a basis for the PhD thesis of two graduate students, Blake Woodworth at TTI-Chicago and Yossi Arjevani at the Weizmann institute.&nbsp; In addition, Jialei Wang at the University of Chicago finished his PhD working on initial stages of the project, and Kshitij Patel at TTIC worked on the project during his first two years at the PhD program, and his PhD will be based on extensions to the project.&nbsp; All four students received mentorship from both PIs, and the project provided opportunities for exposure to different research ideas and environments, at Chicago and at the Weizmann.&nbsp; Woodworth is now a post-doctoral researcher with Francis Bach at INRIA Paris, further expanding his international exposure, and Arjevani recently started a faculty position at the Hebrew University, after post-doctoral training at NYU.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/01/2022<br>\n\t\t\t\t\tModified by: Nathan&nbsp;Srebro</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe goal of the project was to study algorithmic and foundational aspects of training machine learning systems on distributed platforms, in particular using distributed stochastic optimization.  Stochastic optimization refers to optimizing an objective, searching for a good data fit, or training a system, by considering only a few examples at each iteration, as opposed to repeatedly considering the entire data set.  It traces its roots to a classic 1951 paper by Robbins and Monro, has been a staple of machine learning ever since the first learning systems in the 1950s and 1960s, and to this day stochastic optimization techniques, such as Stochastic Gradient Descent (SGD) form the basis of most machine learning training procedures.  A firm theoretical foundation for convex stochastic approximation was provided by the seminal work of Nemirovskii and Yudin in the 1970s, and exact optimal complexities and methods were derived by Lan in 2012 based on Nemirovskii\u2019s work and Nesterov\u2019s 1984 follow-up.  But modern machine learning frequently departs from the sequential settings and relies on parallel and distributed computation in order to handle massive data sets.   This includes parallelization at many scales, ranging from parallelization across cores in a GPU or multiple processors in a single computer, through parallelization across servers in a data center, to distributed computation on edge devices (e.g. individual phones, cars, or sensors) as in Federated Learning.  In this project we provide a firm theoretical framework for stochastic distributed optimization, develop methods with provable guarantees, and establish what is the best that can be done, and how.\n\n \nThe major achievement of the project, recognized by the Best Paper Award at the Conference on Learning Theory (the leading venue for such research) was establishing the \"optimal complexity\" of stochastic convex distributed optimization.  That is, what is the best that could be achieved, in terms of the required computation and communication, and subject to established assumptions, and what methods achieve this performance.  Previous work done as part of the project built towards this by providing the theoretical grounding, describing the relevant models, and studying the behavior of different methods, including showing that some commonly used methods can actually be problematic in some important regimes.  Understanding what is the best that can be done using the standard assumptions and standard model, also allows us to go further, by allowing us to understand what additional assumptions or stronger types of operations we must use in order to break this barrier and obtain better methods, as we have done in work employing \"Hessian-vector-prodcuts\".\n\n \nImprovements in distributed stochastic optimization directly translate into faster training with less communication in many scientific and engineering applications of machine learning.  E.g. through our interactions with Google, these advancements are quickly finding their way to reducing communication in Federated Learning.  Beyond the reduction in resource consumption, Federated Learning also aims at improving user privacy by maintaining user data on their own devices, which now become nodes in a distributed training/optimization process.  Reductions in communication may thus also lead to improved privacy.\n\n \nThe project resulted in 16 refereed conference and journal publications, including two awards (Best Student Paper and Best Paper) at the Conference on Learning Theory, which is the premier venue for research on the theory of machine learning.  This project was an international collaboration between PI Nathan Srebro at TTI-Chicago and PI Ohad Shamir at the Weizmann Institute of Science in Israel.  The project served as a basis for the PhD thesis of two graduate students, Blake Woodworth at TTI-Chicago and Yossi Arjevani at the Weizmann institute.  In addition, Jialei Wang at the University of Chicago finished his PhD working on initial stages of the project, and Kshitij Patel at TTIC worked on the project during his first two years at the PhD program, and his PhD will be based on extensions to the project.  All four students received mentorship from both PIs, and the project provided opportunities for exposure to different research ideas and environments, at Chicago and at the Weizmann.  Woodworth is now a post-doctoral researcher with Francis Bach at INRIA Paris, further expanding his international exposure, and Arjevani recently started a faculty position at the Hebrew University, after post-doctoral training at NYU.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 06/01/2022\n\n\t\t\t\t\tSubmitted by: Nathan Srebro"
 }
}