{
 "awd_id": "1718313",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: 3D Audio Augmentation for Limited Field of View Augmented Reality Systems for Medical Training",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2017-08-29",
 "awd_max_amd_letter_date": "2017-08-29",
 "awd_abstract_narration": "Medical task simulators can provide a safe, affordable, and repeatable environment in which practitioners can rehearse procedures without impacting patient safety.  Augmented reality (AR) is therefore the ideal display technology in this field, allowing the user to directly interact and practice skills within a natural environment.  But current AR displays typically have a narrow field of view (FOV), which makes it difficult for users to immediately attend to an object outside of their periphery.  This research will employ 3D audio to overcome that challenge.  As a benefit to other AR and medical researchers, the system itself and other materials created for the present work (design, implementation, and tutorials) will be openly available on the project's website for all to use, modify, and contribute, and an open-source community will be created to link users and researchers who have similar interests as the present work.  Undergraduate, women, and minority students will be engaged in all project activities through the Distributed Research Experiences for Undergraduates (DREU) program of the NSF-funded iAAMCS (Institute for African-American Mentoring in Computing Sciences), as well as the PI's courses on 3D sound design.  Results will also be disseminated at relevant conference venues.  The present work will advance research relating to the use of 3D audio for cueing in narrow FOV contexts, thereby improving interaction in large AR environments, and will create generalizable best-practices that lead to successful experiences when using AR devices.  &#8232;\r\n\r\nThis research will develop the tools, methods, and infrastructure to evaluate how 3D sound can be used to enhance AR.  This area of research is increasingly important, because as virtual reality (VR) and AR systems become more commonplace, it is imperative to design tools to overcome device limitations.  The practical application of the proposed work will be realized through the evaluation of an AR-based prostate biopsy training procedure that will capture and reconstruct a full surgical procedure, with at least 3 dynamic participants in very close proximity.  The project will pursue three main themes: (1) Infrastructure Development - The PIs and team will develop an extensible open-source software system that allows users to test their desired sound mappings; (2) Sound Mapping Quantification - Although sound has been used to convey spatial information in numerous contexts, the appropriate strategy for mapping periphery information to spatial sound attributes has not yet been determined, so the PIs propose a series of user studies to determine the mappings that most successfully help a user to attend to a target outside of their FOV;  (3) Practical Application - To assess how well the proposed solution mitigates the challenge of a narrow FOV, a user study will be conducted to determine how the addition of 3D audio affects participants learning to perform the steps of a prostate biopsy procedure.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Henry",
   "pi_last_name": "Fuchs",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Henry Fuchs",
   "pi_email_addr": "fuchs@cs.unc.edu",
   "nsf_id": "000451367",
   "pi_start_date": "2017-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Chapel Hill",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275993175",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-2589360d-7fff-4d4b-3f11-f19b960679be\">\n<p dir=\"ltr\"><span>This project developed new technologies and techniques for recording activities such as surgical procedures, reconstructing them in 3D and time, to be enhanced with 3D audio, with the goal of making immersive augmented reality learning experiences for medical and nursing students.&nbsp; The recording of the activity was done with multiple video cameras and the reconstruction of the dynamically changing 3D situation was performed either in real time, for interactive applications, or offline for applications requiring higher-quality reconstruction. In early attempts recording a surgical procedure, a major problem was the occlusion from a camera&rsquo;s view of important areas, such as the surgical site, by the body of the surgeon or a nearby assistant. To overcome this serious problem, the project personnel developed methods to include in the 3D reconstruction, video imagery from miniature cameras worn on the heads of the surgeon or some other participant. The inclusion of such video data considerably complicates the 3D reconstruction process because the head-worn cameras are, of course, constantly being moved, and therefore the geometric relationship between the multiplicity of cameras around the room is no longer fixed.&nbsp; However, mounting cameras on the heads of the participants increases the likelihood that parts of the scene important to the 3D reconstruction and to the eventual educational use will be properly recorded, since participants, such as the surgeon and the assistants, need to look at the important places in the scene, such as the surgical site, and need to be able to see those places in appropriate detail.&nbsp; Also, if multiple miniature cameras can be worn by the participants, it may be possible to eliminate the need for the multiplicity of cameras mounted around the scene, and thereby be able to record situations and experiences anywhere that the participants are located -- other surgical suites, other places in the clinic or the hospital, in an ambulance, even outdoors.&nbsp; This goal, of completely eliminating the need for cameras in the environment, was a major focus of this project.&nbsp; The major accomplishment was a method to reconstruct the 3D position of a participant (a surgeon, a nurse, anyone) and to reconstruct that participant&rsquo;s body from only three cameras:&nbsp; one camera faces forward to record the parts of the scene the participant is looking at, and to determine the location of the participant in the environment (whether a surgical suite, an ordinary room, or outdoors), and two cameras that look down at the participant&rsquo;s body, to reconstruct the participants body pose -- the position and orientation of the hands, arms, and legs. The three cameras are mounted in an eyeglass frame so that with advancing technology, the size of the cameras could be miniaturized to such an extent that they fit within the eyeglass frame itself, and thus could be worn comfortably all day by the surgeon, the nurse, or any other user.&nbsp; The research paper summarizing many of the results of this project was presented at the 2021 IEEE Virtual Reality Conference, the major VR conference in the field, and was given one of the three Best Conference Paper Awards.</span></p>\n<br /></span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/19/2021<br>\n\t\t\t\t\tModified by: Henry&nbsp;Fuchs</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThis project developed new technologies and techniques for recording activities such as surgical procedures, reconstructing them in 3D and time, to be enhanced with 3D audio, with the goal of making immersive augmented reality learning experiences for medical and nursing students.  The recording of the activity was done with multiple video cameras and the reconstruction of the dynamically changing 3D situation was performed either in real time, for interactive applications, or offline for applications requiring higher-quality reconstruction. In early attempts recording a surgical procedure, a major problem was the occlusion from a camera\u2019s view of important areas, such as the surgical site, by the body of the surgeon or a nearby assistant. To overcome this serious problem, the project personnel developed methods to include in the 3D reconstruction, video imagery from miniature cameras worn on the heads of the surgeon or some other participant. The inclusion of such video data considerably complicates the 3D reconstruction process because the head-worn cameras are, of course, constantly being moved, and therefore the geometric relationship between the multiplicity of cameras around the room is no longer fixed.  However, mounting cameras on the heads of the participants increases the likelihood that parts of the scene important to the 3D reconstruction and to the eventual educational use will be properly recorded, since participants, such as the surgeon and the assistants, need to look at the important places in the scene, such as the surgical site, and need to be able to see those places in appropriate detail.  Also, if multiple miniature cameras can be worn by the participants, it may be possible to eliminate the need for the multiplicity of cameras mounted around the scene, and thereby be able to record situations and experiences anywhere that the participants are located -- other surgical suites, other places in the clinic or the hospital, in an ambulance, even outdoors.  This goal, of completely eliminating the need for cameras in the environment, was a major focus of this project.  The major accomplishment was a method to reconstruct the 3D position of a participant (a surgeon, a nurse, anyone) and to reconstruct that participant\u2019s body from only three cameras:  one camera faces forward to record the parts of the scene the participant is looking at, and to determine the location of the participant in the environment (whether a surgical suite, an ordinary room, or outdoors), and two cameras that look down at the participant\u2019s body, to reconstruct the participants body pose -- the position and orientation of the hands, arms, and legs. The three cameras are mounted in an eyeglass frame so that with advancing technology, the size of the cameras could be miniaturized to such an extent that they fit within the eyeglass frame itself, and thus could be worn comfortably all day by the surgeon, the nurse, or any other user.  The research paper summarizing many of the results of this project was presented at the 2021 IEEE Virtual Reality Conference, the major VR conference in the field, and was given one of the three Best Conference Paper Awards.\n\n\n\n \n\n\t\t\t\t\tLast Modified: 11/19/2021\n\n\t\t\t\t\tSubmitted by: Henry Fuchs"
 }
}