{
 "awd_id": "1750539",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Leveraging Sparsity in Massively Distributed Optimization",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2018-02-01",
 "awd_exp_date": "2025-01-31",
 "tot_intn_awd_amt": 458681.0,
 "awd_amount": 458681.0,
 "awd_min_amd_letter_date": "2017-12-15",
 "awd_max_amd_letter_date": "2021-09-11",
 "awd_abstract_narration": "This project develops novel parallel optimization techniques based on the Frank-Wolfe algorithm, enabling the massive parallelization, at an unprecedented scale, of several problems of key significance to computer science, engineering, and operations research. Massively parallelizing such problems can have a significant practical impact on both academia and industry. Using Apache Spark as a development platform, algorithms developed by the project can be implemented, deployed and evaluated over hundreds of machines and thousands of CPUs. The Massachusetts Green High Performance Computing Center (MGHPCC) as well as cloud services, such as Amazon Web Services and the Google Cloud Platform, are leveraged for this deployment, demonstrating both the scalability of developed algorithms as well as their applicability to commercial cluster environments. Educational activities are closely integrated with this research agenda, including a course developed by the principal investigator using MGHPCC as a computing platform, and outreach activities developed jointly with Northeastern University's Center for STEM Education.\r\n\r\nThis research advances our knowledge and understanding of the formal conditions under which problems can be massively parallelized via map-reduce implementations of the Frank-Wolfe algorithm. The project leverages sparsity properties that optimization problems exhibit under Frank-Wolfe, thereby enabling their parallelization via map-reduce operations. Beyond tailored, problem-specific implementations, the project identifies formal, structural properties of problems (or, classes of problems) under which such massive parallelization via map-reduce is possible. The use of Frank-Wolfe as a building block for parallelization, both in convex optimization but also in submodular maximization settings, is transformative. In the latter case, it amounts to a non-combinatorial approach for parallelization, attaining the same approximation guarantee as serial algorithms. \r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stratis",
   "pi_last_name": "Ioannidis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stratis Ioannidis",
   "pi_email_addr": "IOANNIDIS@ECE.NEU.EDU",
   "nsf_id": "000711788",
   "pi_start_date": "2017-12-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Avenue, 540-177",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 94246.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 96261.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 183116.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 85058.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-4743d4f9-7fff-edf9-89c9-f3033c9f2614\"> </span></p>\r\n<p dir=\"ltr\"><strong>Intellectual Merit</strong></p>\r\n<p dir=\"ltr\"><strong>Distributing the Frank-Wolfe Algorithm via Map-Reduce.&nbsp;</strong><span>We identified two structural properties of optimization problems under which Frank Wolfe can be parallelized through map-reduce operations. We showed that several important optimization problems, including experimental design, boosting, and projection to a convex hull satisfy these properties. We implemented our distributed Frank-Wolfe algorithm on Spark, an engine for large-scale distributed data processing. We extensively evaluated our Spark implementation over large synthetic and real-life datasets, illustrating the speedup and scalability properties of our algorithm. For example, using 350 compute cores, we can solve problems of 20 million variables in 79 minutes, an operation that would take 48 hours when executed serially.&nbsp;&nbsp;</span></p>\r\n<p dir=\"ltr\"><strong>Accelerating Submodular Maximization.</strong><span> Submodular functions are set functions that exhibit a diminishing returns property. They naturally arise in applications such as data summarization, facility location, recommendation systems, influence maximization, sensor placement, dictionary learning, cache networks, and active learning, to name a few. We studied how to apply and accelerate versions of the greedy and continuous greedy algorithm for submodular maximization under matroid constraints, leveraging the relationship of continuous greedy to the Frank-Wolfe algorithm. We introduced a class of submodular functions that can be expressed as weighted composition of analytic and multilinear functions. We proposed a polynomial estimator for approximating the multilinear relaxation for this class of problems and provided theoretical guarantees when using our estimator. We applied this explicitly to cache networks with queues as well as several related networking problems. We also studied submodular maximization in a stochastic setting, in which objectives are random and revealed sequentially, and an online setting, in which objectives are again revealed sequentially but are chosen by an adversary. In the stochastic setting, we showed that our polynomial estimator can again yield favorable approximation guarantees. In the online setting, we identified a class of problems for which bounded regret guarantees can be attained by reducing the problem to online convex optimization.</span></p>\r\n<p dir=\"ltr\"><strong>Distributed and Robust Learning via the Alternating Directions Method of Multipliers</strong><span><strong>.</strong> We proposed a distributed method for massively parallelizing graph distance computation using the alternating directions method of multipliers. Our solution uses a novel, distributed bisection algorithm for computing a p-norm proximal operator as a building block. We demonstrated its scalability by conducting experiments over multiple parallel environments, including Spark and OpenMP. We also extended our p-norm proximal operator to robust regression. We studied a general outlier-robust learning setting that replaces the usual mean-square error loss with a p-norm: this known to be robust and has applications such as training auto-encoders and multi-target regression. However, corresponding objectives cannot be optimized via, e.g., stochastic gradient descent, as they are non-differentiable. We show that such problems can be solved via so-called model-based optimization methods and propose a stochastic variant of the online alternating directions method of multipliers to solve the resulting nested optimization problem. We experimentally demonstrate the higher robustness of p-norms in comparison with mean-square error losses and establish the superior performance of our algorithm against stochastic gradient methods, both in terms of minimizing the objective and in terms of down-stream classification tasks. In some cases, we see that our algorithm attains losses that are 29.6 times smaller than the ones achieved by competitors.</span></p>\r\n<p dir=\"ltr\"><strong>Learning Neural Set Functions Under Optimal Subset Oracle Queries. </strong><span>Many interesting applications operate with set-valued outputs and/or inputs. Examples include product recommendations, compound selection, set matching, set prediction, and set anomaly detection, to name a few. Several works apply neural networks to learn set functions from input/function value pairs. However, in certain applications such as, e.g., recommender systems, function evaluations may not be readily available. Recent works have studied the question of learning set functions from optimal subset observations. As maximum likelihood estimation is intractable in this setting, prior work has approximated the underlying set function via an energy-based model, whose parameters are estimated via mean-field variational inference: this amounts to a fixed-point iterative computation involving the set function&rsquo;s multilinear relaxation. We provide guarantees under which the corresponding fixed-point iterations converge and show that parameter estimation can be accelerated via implicit differentiation in this setting. We empirically demonstrate the efficiency of our method on synthetic and real-world subset selection applications including product recommendation, set anomaly detection, and compound selection tasks.&nbsp;</span></p>\r\n<p dir=\"ltr\"><strong>Broader Impacts</strong></p>\r\n<p dir=\"ltr\">The project supported six Ph.D. students and one Post Doctoral researcher during its duration. Research findings were disseminated through 12 conference and 3 journal publications. Educational outcomes included the incorporation of distributed algorithms and their implementations in a graduate course taught at Northeastern University. Efforts were also integrated with activities of the Center for STEM Education at Northeastern University. Results were disseminated through conference presentations as well as through invited seminars. Our code was made publicly available and shared via github repositories.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/27/2025<br>\nModified by: Stratis&nbsp;Ioannidis</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nIntellectual Merit\r\n\n\nDistributing the Frank-Wolfe Algorithm via Map-Reduce.We identified two structural properties of optimization problems under which Frank Wolfe can be parallelized through map-reduce operations. We showed that several important optimization problems, including experimental design, boosting, and projection to a convex hull satisfy these properties. We implemented our distributed Frank-Wolfe algorithm on Spark, an engine for large-scale distributed data processing. We extensively evaluated our Spark implementation over large synthetic and real-life datasets, illustrating the speedup and scalability properties of our algorithm. For example, using 350 compute cores, we can solve problems of 20 million variables in 79 minutes, an operation that would take 48 hours when executed serially.\r\n\n\nAccelerating Submodular Maximization. Submodular functions are set functions that exhibit a diminishing returns property. They naturally arise in applications such as data summarization, facility location, recommendation systems, influence maximization, sensor placement, dictionary learning, cache networks, and active learning, to name a few. We studied how to apply and accelerate versions of the greedy and continuous greedy algorithm for submodular maximization under matroid constraints, leveraging the relationship of continuous greedy to the Frank-Wolfe algorithm. We introduced a class of submodular functions that can be expressed as weighted composition of analytic and multilinear functions. We proposed a polynomial estimator for approximating the multilinear relaxation for this class of problems and provided theoretical guarantees when using our estimator. We applied this explicitly to cache networks with queues as well as several related networking problems. We also studied submodular maximization in a stochastic setting, in which objectives are random and revealed sequentially, and an online setting, in which objectives are again revealed sequentially but are chosen by an adversary. In the stochastic setting, we showed that our polynomial estimator can again yield favorable approximation guarantees. In the online setting, we identified a class of problems for which bounded regret guarantees can be attained by reducing the problem to online convex optimization.\r\n\n\nDistributed and Robust Learning via the Alternating Directions Method of Multipliers. We proposed a distributed method for massively parallelizing graph distance computation using the alternating directions method of multipliers. Our solution uses a novel, distributed bisection algorithm for computing a p-norm proximal operator as a building block. We demonstrated its scalability by conducting experiments over multiple parallel environments, including Spark and OpenMP. We also extended our p-norm proximal operator to robust regression. We studied a general outlier-robust learning setting that replaces the usual mean-square error loss with a p-norm: this known to be robust and has applications such as training auto-encoders and multi-target regression. However, corresponding objectives cannot be optimized via, e.g., stochastic gradient descent, as they are non-differentiable. We show that such problems can be solved via so-called model-based optimization methods and propose a stochastic variant of the online alternating directions method of multipliers to solve the resulting nested optimization problem. We experimentally demonstrate the higher robustness of p-norms in comparison with mean-square error losses and establish the superior performance of our algorithm against stochastic gradient methods, both in terms of minimizing the objective and in terms of down-stream classification tasks. In some cases, we see that our algorithm attains losses that are 29.6 times smaller than the ones achieved by competitors.\r\n\n\nLearning Neural Set Functions Under Optimal Subset Oracle Queries. Many interesting applications operate with set-valued outputs and/or inputs. Examples include product recommendations, compound selection, set matching, set prediction, and set anomaly detection, to name a few. Several works apply neural networks to learn set functions from input/function value pairs. However, in certain applications such as, e.g., recommender systems, function evaluations may not be readily available. Recent works have studied the question of learning set functions from optimal subset observations. As maximum likelihood estimation is intractable in this setting, prior work has approximated the underlying set function via an energy-based model, whose parameters are estimated via mean-field variational inference: this amounts to a fixed-point iterative computation involving the set functions multilinear relaxation. We provide guarantees under which the corresponding fixed-point iterations converge and show that parameter estimation can be accelerated via implicit differentiation in this setting. We empirically demonstrate the efficiency of our method on synthetic and real-world subset selection applications including product recommendation, set anomaly detection, and compound selection tasks.\r\n\n\nBroader Impacts\r\n\n\nThe project supported six Ph.D. students and one Post Doctoral researcher during its duration. Research findings were disseminated through 12 conference and 3 journal publications. Educational outcomes included the incorporation of distributed algorithms and their implementations in a graduate course taught at Northeastern University. Efforts were also integrated with activities of the Center for STEM Education at Northeastern University. Results were disseminated through conference presentations as well as through invited seminars. Our code was made publicly available and shared via github repositories.\r\n\n\n\t\t\t\t\tLast Modified: 03/27/2025\n\n\t\t\t\t\tSubmitted by: StratisIoannidis\n"
 }
}