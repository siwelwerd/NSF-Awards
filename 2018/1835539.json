{
 "awd_id": "1835539",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CRII: RI: Multi-Source Domain Generalization Approaches to Visual Attribute Detection",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2017-12-23",
 "awd_exp_date": "2020-04-30",
 "tot_intn_awd_amt": 40778.0,
 "awd_amount": 40778.0,
 "awd_min_amd_letter_date": "2018-06-11",
 "awd_max_amd_letter_date": "2018-06-11",
 "awd_abstract_narration": "This project investigates how to accurately and robustly detect attributes from images (videos, and 3D data), with the goal of developing and publicly providing effective attribute detection tools. Visual attributes refer to human-namable and machine-detectable inherent characteristics of visual content from objects, scenes, and activities (e.g., four-legged, outdoor, and crowded). They possess versatile properties and application potentials by offering a natural human-computer interaction channel for involving humans in the loop of machine vision algorithms, serving as basic building blocks for one to compose categories and describe instances, and bringing rich prior knowledge and regularization to statistical learning models, to name a few. The project advances the long-standing pursuit of utilizing attributes for a wide variety of visual recognition and search tasks. The project also actively engages graduate and undergraduate students, and outreaches local high-school students. The research results from this project can impact several related communities such as NLP, speech, and robotics, etc.. \r\n \r\nThis research explicitly tackles the need that attribute detectors should generalize well across different categories, including those previously unseen ones. The research team approaches the problem based on multi-source domain generalization by taking each category as a domain. In particular, this project develops new feature extraction tools tailored to account for the middle-level attributes, as opposed to the traditional features primarily designed and tested for high-level visual recognition. The project consists of three major thrusts hinging on the key motivation of the analogy between attribute detection and domain generalization. It begins by learning a fine-grained \"shallow\" feature mapping (Thrust I) to distill attribute-discriminative signals that are category-invariant, and then investigates \"deeper\" into the feature extraction frameworks - Fisher vectors (Thrust II) and convolutional neural networks (Thrust III)-to revise them for the purpose of attribute detection.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Boqing",
   "pi_last_name": "Gong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Boqing Gong",
   "pi_email_addr": "bgong@icsi.berkeley.edu",
   "nsf_id": "000703661",
   "pi_start_date": "2018-06-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "International Computer Science Institute",
  "inst_street_address": "2150 SHATTUCK AVE",
  "inst_street_address_2": "SUITE 250",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106662900",
  "inst_zip_code": "947041345",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "INTERNATIONAL COMPUTER SCIENCE INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GSRMP1QCXU74"
 },
 "perf_inst": {
  "perf_inst_name": "International Computer Science Institute",
  "perf_str_addr": "1947 Center Street",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947041159",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "026Y00",
   "pgm_ele_name": "CRII CISE Research Initiation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 40778.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project investigates how to accurately and robustly detect attributes of objects, human activities, and scenes. Visual attributes refer to human-namable and machine-detectable properties of visual content, such as \"four-legged,\" \"crowded,\" and \"outdoor,\" to name a few. The visual attributes provide a natural human-computer interaction channel for involving humans in the loop of machine vision systems, serving as essential building blocks for composing categories and describing instances. Moreover, they also bring rich prior knowledge and regularization into statistical learning models. This project advances the long-standing pursuit of utilizing attributes for a wide variety of visual recognition and search tasks. The project also actively engages graduate and undergraduate students through state-of-the-art research activities. The research results from this project can impact a variety of related fields, such as robotics, speech, and natural language processing.</p>\n<p><br />The overarching research idea explicitly addresses the need that attribute detectors should generalize well across different categories, including those previously unseen ones. The research team approaches the problem from a multi-source domain generalization perspective by treating each class (e.g., an object like cats or an activity like dancing) as a domain. This perspective enables us to develop new class-invariant feature extraction tools to model the attributes, as opposed to the traditional features primarily designed and tested for high-level visual classes.&nbsp;</p>\n<p>More concretely, the project covers three main thrusts. It begins by learning a fine-grained \"shallow\" feature mapping (Thrust I) to distill attribute-discriminative signals that are category-invariant. This effort has led to two publications at CVPR 2016, a flagship conference in computer vision, and a book entitled \"Domain Adaptation in Computer Vision Applications,\" respectively. The computer vision community well receives the work. The organization committee of CVPR 2016 selected us to give a spotlight presentation about the approach, and the research team has presented it in numerous keynote talks at workshops.&nbsp;</p>\n<p>By the second and third thrusts, the team dives \"deeper\" into the feature extraction frameworks and uses deep neural networks to detect the attributes. Some representative works include but are not limited to the following. The team augments facial attribute detection by face component segmentation, which appeared at CVPR 2017. They use domain adaptation to mutually identify relevant Web images and Web video frames, which was accepted to ECCV 2016. They use the attributes to describe a class profile such that a computer vision model can recognize the class with zero-shot training examples, and published two papers at CVPR 2016 and one at ECCV 2016.&nbsp;</p>\n<p>The project hinges on domain adaptation and domain generalization in terms of methodologies. The team has also made significant contributions to the domain adaptation literature. Their work published at ICCV 2017 was the first to explore domain adaptation in semantic segmentation. In work published in CVPR 2018, they studied how to avoid forgetting the source domain knowledge in domain adaptation. In CVPR 2020, they further studied how to model latent domains in a test set.&nbsp;</p>\n<p>Last but not least, the team's research also spans several other subfields related to the visual attributes and domain adaptation. This award supported the research activities partially. The sequential determinantal point processes for supervised video summarization have given rise to a series of publications and keynote talks at ECCV 2016, CVPR 2017, and ECCV 2018. The team published their work on generative adversarial networks and adversarial attacks at ICLR 2018 and ICLR 2019, respectively.&nbsp;</p>\n<p>Through the research activities, the PI fostered great educational and research experiences for both graduate students and some undergraduate students. Most of the students successfully published their work as first authors at top-tier conferences. By attending the conferences and presenting their work, they became deeply involved in the computer vision community and built solid foundations for their future career development.&nbsp;</p>\n<p>Visual recognition is one of the most fundamental tasks in computer vision. Many applications in other fields and the real world depend on high-quality and robust visual recognition models. Hence, this project has a potentially far-reaching impact on various areas and beyond science and technology. The research results may improve the application scope of computer vision systems, expanding its benefits to a variety of environments where uncertainty prevails.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/01/2020<br>\n\t\t\t\t\tModified by: Boqing&nbsp;Gong</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project investigates how to accurately and robustly detect attributes of objects, human activities, and scenes. Visual attributes refer to human-namable and machine-detectable properties of visual content, such as \"four-legged,\" \"crowded,\" and \"outdoor,\" to name a few. The visual attributes provide a natural human-computer interaction channel for involving humans in the loop of machine vision systems, serving as essential building blocks for composing categories and describing instances. Moreover, they also bring rich prior knowledge and regularization into statistical learning models. This project advances the long-standing pursuit of utilizing attributes for a wide variety of visual recognition and search tasks. The project also actively engages graduate and undergraduate students through state-of-the-art research activities. The research results from this project can impact a variety of related fields, such as robotics, speech, and natural language processing.\n\n\nThe overarching research idea explicitly addresses the need that attribute detectors should generalize well across different categories, including those previously unseen ones. The research team approaches the problem from a multi-source domain generalization perspective by treating each class (e.g., an object like cats or an activity like dancing) as a domain. This perspective enables us to develop new class-invariant feature extraction tools to model the attributes, as opposed to the traditional features primarily designed and tested for high-level visual classes. \n\nMore concretely, the project covers three main thrusts. It begins by learning a fine-grained \"shallow\" feature mapping (Thrust I) to distill attribute-discriminative signals that are category-invariant. This effort has led to two publications at CVPR 2016, a flagship conference in computer vision, and a book entitled \"Domain Adaptation in Computer Vision Applications,\" respectively. The computer vision community well receives the work. The organization committee of CVPR 2016 selected us to give a spotlight presentation about the approach, and the research team has presented it in numerous keynote talks at workshops. \n\nBy the second and third thrusts, the team dives \"deeper\" into the feature extraction frameworks and uses deep neural networks to detect the attributes. Some representative works include but are not limited to the following. The team augments facial attribute detection by face component segmentation, which appeared at CVPR 2017. They use domain adaptation to mutually identify relevant Web images and Web video frames, which was accepted to ECCV 2016. They use the attributes to describe a class profile such that a computer vision model can recognize the class with zero-shot training examples, and published two papers at CVPR 2016 and one at ECCV 2016. \n\nThe project hinges on domain adaptation and domain generalization in terms of methodologies. The team has also made significant contributions to the domain adaptation literature. Their work published at ICCV 2017 was the first to explore domain adaptation in semantic segmentation. In work published in CVPR 2018, they studied how to avoid forgetting the source domain knowledge in domain adaptation. In CVPR 2020, they further studied how to model latent domains in a test set. \n\nLast but not least, the team's research also spans several other subfields related to the visual attributes and domain adaptation. This award supported the research activities partially. The sequential determinantal point processes for supervised video summarization have given rise to a series of publications and keynote talks at ECCV 2016, CVPR 2017, and ECCV 2018. The team published their work on generative adversarial networks and adversarial attacks at ICLR 2018 and ICLR 2019, respectively. \n\nThrough the research activities, the PI fostered great educational and research experiences for both graduate students and some undergraduate students. Most of the students successfully published their work as first authors at top-tier conferences. By attending the conferences and presenting their work, they became deeply involved in the computer vision community and built solid foundations for their future career development. \n\nVisual recognition is one of the most fundamental tasks in computer vision. Many applications in other fields and the real world depend on high-quality and robust visual recognition models. Hence, this project has a potentially far-reaching impact on various areas and beyond science and technology. The research results may improve the application scope of computer vision systems, expanding its benefits to a variety of environments where uncertainty prevails. \n\n\t\t\t\t\tLast Modified: 07/01/2020\n\n\t\t\t\t\tSubmitted by: Boqing Gong"
 }
}