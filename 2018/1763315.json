{
 "awd_id": "1763315",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: Medium: Collaborative Research: Beyond Sparsity: Refined Measures of Complexity for Linear Algebra",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2018-03-15",
 "awd_exp_date": "2022-02-28",
 "tot_intn_awd_amt": 552118.0,
 "awd_amount": 552118.0,
 "awd_min_amd_letter_date": "2018-03-14",
 "awd_max_amd_letter_date": "2019-09-08",
 "awd_abstract_narration": "Modern data science applications exploit structure in real life data using machine learning (including deep learning) algorithms. At the core of most of these systems are algorithms for a branch of mathematics called linear algebra. In particular, a large portion of these algorithms utilize the fact that real life data has properties that can be captured using certain parsimonious linear algebraic structures. This project studies new, more powerful linear algebraic structures and algorithms that exploit these new structures. Given the fundamental importance of these algorithms, ideas generated from this project are expected to be implemented in widely deployed machine learning systems. The outreach component of this project involves (1) a technical workshop for researchers from diverse areas and (2) outreach events for K-12 students.\r\n\r\nA variety of problems in modern data science have been successfully characterized using a width. For example, one of the most common widths, the rank of a matrix, has a near-ubiquitous use across many applications. This project significantly expands the understanding of several recently proposed widths and extracts their full potential for positive practical outcomes. Furthermore, it contributes to the recently growing work on beyond worst-case analysis in linear algebra, machine learning and coding theory.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Re",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher Re",
   "pi_email_addr": "chrismre@cs.stanford.edu",
   "nsf_id": "000555316",
   "pi_start_date": "2018-03-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "353 Serra Mall",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943055008",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 134482.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 417636.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>A branch of mathematics called linear algebra is at the core of most machine learning algorithms. The fundamental structure used in linear algebra, called a matrix, is an array of numbers used to represent mathematical concepts or objects. Because large matrices and operations over these matrices are ubiquitous in modern machine learning, designing more&nbsp;<em>efficient</em>&nbsp;representations directly impacts the speed, memory requirements, numerical stability, ease of use, and performance of machine learning systems. Our work studies new, more powerful, linear algebraic structures, inspired by properties observed in real world data, and proposes new algorithms based on these structures.</p>\n<p>&nbsp;</p>\n<p>First, there are many proposed methods for transforming large matrices to more efficient forms, however these different methods have different tradeoffs in terms of speed, memory requirements, and performance quality, and machine learning practitioners must hand-pick a method for each application. The goal is an efficient matrix structure that behaves as close as possible to the original large matrix, with less memory requirement and improved speed. Our work makes significant improvements towards a universal class of matrices that are automatically learned, rather than requiring hand engineering, and which are nearly optimal in speed and memory. We develop both the theory and systems implementations to support and demonstrate our methods. Our methods have been adopted in industry.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Next, a major challenge in machine learning is to learn from long sequences of data, such as large spans of text or time series data. Existing paradigms for learning over long sequences face prohibitive tradeoffs in terms of numerical stability and performance, versus efficiency. We develop theoretical connections between the different paradigms and design linear algebraic representations that can simultaneously retain the strengths and mitigate the fundamental limitations of the existing paradigms. Our work achieves state of the art results on applications requiring reasoning over long sequences.&nbsp;</p>\n<p>&nbsp;</p>\n<p>We release all our work through open-access publications and open-source code.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/29/2022<br>\n\t\t\t\t\tModified by: Christopher&nbsp;Re</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nA branch of mathematics called linear algebra is at the core of most machine learning algorithms. The fundamental structure used in linear algebra, called a matrix, is an array of numbers used to represent mathematical concepts or objects. Because large matrices and operations over these matrices are ubiquitous in modern machine learning, designing more efficient representations directly impacts the speed, memory requirements, numerical stability, ease of use, and performance of machine learning systems. Our work studies new, more powerful, linear algebraic structures, inspired by properties observed in real world data, and proposes new algorithms based on these structures.\n\n \n\nFirst, there are many proposed methods for transforming large matrices to more efficient forms, however these different methods have different tradeoffs in terms of speed, memory requirements, and performance quality, and machine learning practitioners must hand-pick a method for each application. The goal is an efficient matrix structure that behaves as close as possible to the original large matrix, with less memory requirement and improved speed. Our work makes significant improvements towards a universal class of matrices that are automatically learned, rather than requiring hand engineering, and which are nearly optimal in speed and memory. We develop both the theory and systems implementations to support and demonstrate our methods. Our methods have been adopted in industry. \n\n \n\nNext, a major challenge in machine learning is to learn from long sequences of data, such as large spans of text or time series data. Existing paradigms for learning over long sequences face prohibitive tradeoffs in terms of numerical stability and performance, versus efficiency. We develop theoretical connections between the different paradigms and design linear algebraic representations that can simultaneously retain the strengths and mitigate the fundamental limitations of the existing paradigms. Our work achieves state of the art results on applications requiring reasoning over long sequences. \n\n \n\nWe release all our work through open-access publications and open-source code.\n\n \n\n\t\t\t\t\tLast Modified: 04/29/2022\n\n\t\t\t\t\tSubmitted by: Christopher Re"
 }
}