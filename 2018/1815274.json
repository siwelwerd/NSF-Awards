{
 "awd_id": "1815274",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Collaborative Research: Overheard at Home - Mitigating Overhearing of Continuous Listening Devices",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 247989.0,
 "awd_amount": 247989.0,
 "awd_min_amd_letter_date": "2018-07-27",
 "awd_max_amd_letter_date": "2018-07-27",
 "awd_abstract_narration": "Smart home devices that converse with humans are becoming popular day by day. These hands-free, Internet-connected devices continuously listen to their surroundings, so that whenever someone speaks to them, they can respond right away. In the presence of such always-listening devices at home, people are at the greatest risk of privacy invasion. For instance, a hacker can hack into these devices and listen to everything one says or every sound one makes at home. The goal of this project is to thwart such attacks and to ensure that continuously listening devices do not overhear anything that they should not.   \r\n\r\nThis project takes a generalized approach to solve the overhearing problem of any acoustic sensing system. A programmable acoustic filter will be developed, which will inspect and filter out user-defined sounds before audio signals enter into an untrusted acoustic sensing system. To achieve this, the project is organized in four research tasks: (a) devising algorithms to separate a large number of sound sources on resource-constrained embedded platforms; (b) devising algorithms to count the number of active sources in real-time; (c) developing sound recognition algorithms that require very little training data; and (d) developing a custom, low-power, miniature embedded system.    \r\n\r\nPrivacy-preserving mobile health applications are important to bringing personalized and low-cost healthcare to the mass of people which they can trust. The system will be deployed in two audio-based mobile health applications: asthma and sleep monitoring, where privacy is of utmost importance. Hardware and software materials developed in this project will be used in graduate and undergraduate level courses at both University of North Carolina and Columbia University. This award will support a female graduate student. Research results will be demonstrated at outreach events, as well as through online course offerings, and conference publications. \r\n\r\nAnonymized datasets, algorithms, code, and the design of the embedded system will be shared with the research community through a website (https://overhearing.web.unc.edu/). The website is hosted at the University of North Carolina at Chapel Hill. The repository will be publicly accessible and will be kept up-to-date throughout the 3-year project. The website will remain alive beyond the project period for as long as possible.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiaofan",
   "pi_last_name": "Jiang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaofan Jiang",
   "pi_email_addr": "jiang@ee.columbia.edu",
   "nsf_id": "000721831",
   "pi_start_date": "2018-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "550 West 120th Street",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 247989.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-abbf44fa-7fff-2fe5-4817-3d9d5c7616dc\"> </span></p>\n<p dir=\"ltr\"><span>This project made significant advances in audio processing algorithms and systems under resource-constrained environments, with a particular focus on applications that enhance privacy and improve health. The project saw the creation of a low-power smartphone sleeve that employs machine learning to identify and localize sound sources using temporal and spatial features. This technology showed promising results in distinguishing sounds in various noisy environments, such as railroads and construction sites. The team explored adaptive filtering techniques for active noise cancellation, which performed well in simulations but faced challenges in more complex, real-world settings. Efforts to improve signal-to-noise ratio through audible frequency beam-forming and deep learning methods for noise cancellation yielded promising outcomes.&nbsp;</span></p>\n<p dir=\"ltr\"><span>A significant contribution was the development of the Probabilistic Template Matching (PTM) algorithm and its integration into a general acoustic filtering framework designed for real-time noise separation on resource-limited platforms. This framework was successfully applied to enhance audio privacy in mobile sleep monitoring systems, reducing the intelligibility of nearby speech signals while maintaining effective sleep activity detection. The project introduced an adaptive, privacy-preserving audio filtering architecture capable of filtering out specific secondary sound sources to protect user privacy. This architecture was implemented in various applications, including applications for baby monitoring and piano sound recording that demonstrated improved detection of primary sounds by filtering out background noises.</span></p>\n<p dir=\"ltr\"><span>In the final year, the project expanded to incorporate augmented reality (AR) with the acoustic filtering technology to create an AI-enhanced stethoscope platform. This platform significantly improves the analysis of heart sounds by guiding users to precise auscultation points and employing various techniques to filter ambient noises and amplify heart murmurs, making them more detectable by deep neural networks. The AR-guided system showcases the project's innovative approach to integrating audio processing with AR for health monitoring applications.</span></p>\n<p dir=\"ltr\"><span>Project outcomes were integrated into an undergraduate course on Internet-of-Things at Columbia University. This grant supported multiple Ph.D., master's, and undergraduate students, as well as high school summer interns.The project's technologies were showcased in a workshop aimed at middle school female students from Harlem and Bronx, promoting STEM education.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Research results from this work were published in top conferences, workshops, and journals on IoT, sensor networks, information processing, and acoustic systems. A privacy-aware, acoustic mobile system was presented at AIChallengeIoT &rsquo;20. A poster on two applications of our privacy-preserving filtering architecture was presented at IPSN &rsquo;21. Our IASA &rsquo;22 paper showcased a platform that combines privacy-enhancing audio filtering with edge-cloud computing for environmental sound adaptation, with applications in sleep monitoring. An AR-guided platform was introduced at IPSN &rsquo;23 for heart auscultation, leveraging audio cues and visual guidance to improve murmur diagnostic accuracy.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/13/2024<br>\nModified by: Xiaofan&nbsp;Jiang</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1815274/1815274_10562687_1710315318451_buma--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1815274/1815274_10562687_1710315318451_buma--rgov-800width.png\" title=\"BuMA: Non-Intrusive Breathing Detection using Microphone Array\"><img src=\"/por/images/Reports/POR/2024/1815274/1815274_10562687_1710315318451_buma--rgov-66x44.png\" alt=\"BuMA: Non-Intrusive Breathing Detection using Microphone Array\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">System architecture of BuMA</div>\n<div class=\"imageCredit\">Columbia ICSL</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Xiaofan&nbsp;Jiang\n<div class=\"imageTitle\">BuMA: Non-Intrusive Breathing Detection using Microphone Array</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1815274/1815274_10562687_1710315277767_pams--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1815274/1815274_10562687_1710315277767_pams--rgov-800width.png\" title=\"PAMS architecture and data pipeline\"><img src=\"/por/images/Reports/POR/2024/1815274/1815274_10562687_1710315277767_pams--rgov-66x44.png\" alt=\"PAMS architecture and data pipeline\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">PAMS architecture and data pipeline: the filter coefficients, &#120572;&#119894;, are updated based on the templates provided by the GMM noise model</div>\n<div class=\"imageCredit\">Columbia ICSL</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Xiaofan&nbsp;Jiang\n<div class=\"imageTitle\">PAMS architecture and data pipeline</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1815274/1815274_10562687_1710315209454_arsteth--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1815274/1815274_10562687_1710315209454_arsteth--rgov-800width.png\" title=\"ARSteth System Architecture\"><img src=\"/por/images/Reports/POR/2024/1815274/1815274_10562687_1710315209454_arsteth--rgov-66x44.png\" alt=\"ARSteth System Architecture\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">System architecture of ARSteth: three inputs (camera, user input and sound from the stethoscope) are used to guide the user and assess the potential heart ailments</div>\n<div class=\"imageCredit\">Columbia ICSL</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Xiaofan&nbsp;Jiang\n<div class=\"imageTitle\">ARSteth System Architecture</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThis project made significant advances in audio processing algorithms and systems under resource-constrained environments, with a particular focus on applications that enhance privacy and improve health. The project saw the creation of a low-power smartphone sleeve that employs machine learning to identify and localize sound sources using temporal and spatial features. This technology showed promising results in distinguishing sounds in various noisy environments, such as railroads and construction sites. The team explored adaptive filtering techniques for active noise cancellation, which performed well in simulations but faced challenges in more complex, real-world settings. Efforts to improve signal-to-noise ratio through audible frequency beam-forming and deep learning methods for noise cancellation yielded promising outcomes.\n\n\nA significant contribution was the development of the Probabilistic Template Matching (PTM) algorithm and its integration into a general acoustic filtering framework designed for real-time noise separation on resource-limited platforms. This framework was successfully applied to enhance audio privacy in mobile sleep monitoring systems, reducing the intelligibility of nearby speech signals while maintaining effective sleep activity detection. The project introduced an adaptive, privacy-preserving audio filtering architecture capable of filtering out specific secondary sound sources to protect user privacy. This architecture was implemented in various applications, including applications for baby monitoring and piano sound recording that demonstrated improved detection of primary sounds by filtering out background noises.\n\n\nIn the final year, the project expanded to incorporate augmented reality (AR) with the acoustic filtering technology to create an AI-enhanced stethoscope platform. This platform significantly improves the analysis of heart sounds by guiding users to precise auscultation points and employing various techniques to filter ambient noises and amplify heart murmurs, making them more detectable by deep neural networks. The AR-guided system showcases the project's innovative approach to integrating audio processing with AR for health monitoring applications.\n\n\nProject outcomes were integrated into an undergraduate course on Internet-of-Things at Columbia University. This grant supported multiple Ph.D., master's, and undergraduate students, as well as high school summer interns.The project's technologies were showcased in a workshop aimed at middle school female students from Harlem and Bronx, promoting STEM education.\n\n\nResearch results from this work were published in top conferences, workshops, and journals on IoT, sensor networks, information processing, and acoustic systems. A privacy-aware, acoustic mobile system was presented at AIChallengeIoT 20. A poster on two applications of our privacy-preserving filtering architecture was presented at IPSN 21. Our IASA 22 paper showcased a platform that combines privacy-enhancing audio filtering with edge-cloud computing for environmental sound adaptation, with applications in sleep monitoring. An AR-guided platform was introduced at IPSN 23 for heart auscultation, leveraging audio cues and visual guidance to improve murmur diagnostic accuracy.\n\n\n\t\t\t\t\tLast Modified: 03/13/2024\n\n\t\t\t\t\tSubmitted by: XiaofanJiang\n"
 }
}