{
 "awd_id": "1764000",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF : Medium : Collaborative Research: Decentralized On-Chip Infrastructure for Robustness and Portability in Heterogeneous Multicores",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2018-04-04",
 "awd_max_amd_letter_date": "2021-06-28",
 "awd_abstract_narration": "When new hardware options emerge in modern computer systems, software code must be rewritten or retailored to work on the new systems. The result is that application code-bases do not seamlessly port from one generation to another. Even worse, they often cannot fully or nimbly adjust to dynamic variations during execution even on a given system. This work will mitigate the complexity, portability, and robustness challenges of heterogeneous platforms, while also continuing to garner high performance. Leveraging the research team's experience in open-source software release and curriculum development, this project will distribute the tools developed in these research thrusts as well as teach these new tools via an existing course on design and programming of heterogeneous architectures.\r\n\r\nAddressing the research challenges of this project requires moving away from a processor-centric viewpoint, and towards a broader perspective aimed at managing communication issues.  The research team will introduce the concept of \"hardware shims\" that can be employed for a range of uses such as acting as prefetchers, translating between different communication protocols, or assisting with dynamic verification of properties specified by the designer. The project will automate the design and synthesis of static or dynamic shims that can verify the memory consistency of an arbitrary heterogeneous multicore for which the designer provides a set of ordering specifications. The proposed research activities will demonstrate gains in performance, functional robustness, and code portability as the application is compiled and executed on systems that differ significantly in terms of the number and type of processors and specialized hardware accelerators that they use.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Luca",
   "pi_last_name": "Carloni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Luca Carloni",
   "pi_email_addr": "luca@cs.columbia.edu",
   "nsf_id": "000490797",
   "pi_start_date": "2018-04-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "2960 Broadway",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7798",
   "pgm_ref_txt": "SOFTWARE & HARDWARE FOUNDATION"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 240649.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 101941.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 107410.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The last decade has seen the rise of heterogeneous computing. From high-performance servers at the core of the cloud to embedded processors at the edge of the cloud, computing systems are increasingly realized by combining many different sets of general-purpose processors (CPUs) and graphics processor units (GPUs) together with a growing number of fixed-function hardware accelerators. Uniquely optimized for a specific computationally intensive task, an accelerator executes it with better performance and energy efficiency than a corresponding software implementation running on a CPU or a GPU. &nbsp;While important progress has been made for the compilation of applications to combinations of CPUs and GPUs, deploying effectively software tasks on accelerators remains a cumbersome effort. Once deployed on a particular system, the same software applications cannot be easily ported across other systems featuring different combinations of processors and accelerators. For the very important domain of machine learning applications, most available programming frameworks target software deployment on CPUs or GPUs. To a large extent, benefiting from accelerators typically requires a time-consuming effort to understand the underlying structure of the computing system and to modify existing software applications so that they can invoke them via device drivers.</p>\n<p>In an important milestone towards the seamless deployment of machine learning applications on heterogeneous computing systems, the PI and his collaborators developed WOLT, an end-to-end efficient solution to run TensorFlow Lite application workloads on lightweight system-on-chip (SoC) computing systems that feature many fixed-function hardware accelerators. TensorFlow Lite is a specialized framework designed for running machine learning at the edge of the cloud on resource-constrained devices. WOLT enables the execution of TensorFlow Lite operations on pre-designed accelerators without requiring any modification of the application source code. As it establishes an interface between the high-level framework and the device drivers of the accelerators, WOLT includes a resource manager that allows multi-tenant and conflict-free hardware acceleration of many TensorFlow Lite applications running in parallel on the given SoC. The implementation of WOLT was evaluated with a comprehensive set of experiments by profiling and running 13 different workloads on a variety of complete SoC prototypes. These prototypes were designed by combining many RISC-V processors with multiple accelerators for vector-matrix multiplication and two-dimensional convolution. For these workloads, WOLT delivers major performance speedups and energy-efficient gains compared to a purely software execution. When running multiple TensorFlow Lite workloads in parallel, WOLT outperforms basic hardware acceleration, thanks to efficient resource management. WOLT advances edge computing by enabling efficient and portable deployment of TFLite workloads on many different heterogeneous SoC architectures.</p>\n<p>&nbsp;</p>\n<p>The rise of heterogeneous computing has also brought a rise in diversity of the instruction set architectures (ISA) of CPUs and GPUs. One of the most important levels of abstraction in the computing stack, an ISA is the set of instruction that can be used to program a processor by being the target of the compilation process of software application. As more diverse processors are brough together in a computing system, more diverse ISAs are used to program it. This phenomenon, together with the growing adoption of virtual machines, is driving a need for fast, scalable, full-system, cross-ISA emulation and instrumentation tools. Unfortunately, achieving high performance for these cross-ISA tools is challenging due to dynamic binary translation (DBT) overhead and the complexity of instrumenting full-system emulators.&nbsp;</p>\n<p>To address these challenges and improve cross-ISA emulation and instrumentation performance, the PI and his collaborator developed three novel techniques. First, they increased the emulation performance of floating-point (FP) operations by observing that most FP operations can be correctly emulated by surrounding the use of the host FP unit with a minimal amount of non-FP code. Second, they introduced the design of a translator with a shared code cache that scales for multi-core guests, even when they generate translated code in parallel at a high rate. Third, they developed an ISA-agnostic instrumentation layer that can instrument guest operations that occur outside of the DBT's intermediate representation, which are common in full-system emulators. They implemented these three techniques in Qelt, a novel cross-ISA machine emulator and tool for dynamic binary instrumentation based on QEMU, which is a free and open-source emulator of many popular commercial processors. With an extensive and thorough set of experimental results, they demonstrated the scalability of Qelt, how it outperforms QEMU as a full-system cross-ISA machine emulator for integer and FP workloads as well as state-of-the-art, cross-ISA, full-system instrumentation tools, and how it can match the performance of Pin, a state-of-the-art, same-ISA dynamic binary instrumentation tool, when used for complex instrumentation such as cache simulation.</p>\n<p>&nbsp;</p>\n<p>As part of the broader-impact activities for this project, the PI and his collaborators have (1) released the implementation of WOLT in the public domain and (2) made the innovative techniques that they developed in Qelt available to the open-source software community by merging them into the upstream QEMU distribution.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/03/2024<br>\nModified by: Luca&nbsp;Carloni</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe last decade has seen the rise of heterogeneous computing. From high-performance servers at the core of the cloud to embedded processors at the edge of the cloud, computing systems are increasingly realized by combining many different sets of general-purpose processors (CPUs) and graphics processor units (GPUs) together with a growing number of fixed-function hardware accelerators. Uniquely optimized for a specific computationally intensive task, an accelerator executes it with better performance and energy efficiency than a corresponding software implementation running on a CPU or a GPU. While important progress has been made for the compilation of applications to combinations of CPUs and GPUs, deploying effectively software tasks on accelerators remains a cumbersome effort. Once deployed on a particular system, the same software applications cannot be easily ported across other systems featuring different combinations of processors and accelerators. For the very important domain of machine learning applications, most available programming frameworks target software deployment on CPUs or GPUs. To a large extent, benefiting from accelerators typically requires a time-consuming effort to understand the underlying structure of the computing system and to modify existing software applications so that they can invoke them via device drivers.\n\n\nIn an important milestone towards the seamless deployment of machine learning applications on heterogeneous computing systems, the PI and his collaborators developed WOLT, an end-to-end efficient solution to run TensorFlow Lite application workloads on lightweight system-on-chip (SoC) computing systems that feature many fixed-function hardware accelerators. TensorFlow Lite is a specialized framework designed for running machine learning at the edge of the cloud on resource-constrained devices. WOLT enables the execution of TensorFlow Lite operations on pre-designed accelerators without requiring any modification of the application source code. As it establishes an interface between the high-level framework and the device drivers of the accelerators, WOLT includes a resource manager that allows multi-tenant and conflict-free hardware acceleration of many TensorFlow Lite applications running in parallel on the given SoC. The implementation of WOLT was evaluated with a comprehensive set of experiments by profiling and running 13 different workloads on a variety of complete SoC prototypes. These prototypes were designed by combining many RISC-V processors with multiple accelerators for vector-matrix multiplication and two-dimensional convolution. For these workloads, WOLT delivers major performance speedups and energy-efficient gains compared to a purely software execution. When running multiple TensorFlow Lite workloads in parallel, WOLT outperforms basic hardware acceleration, thanks to efficient resource management. WOLT advances edge computing by enabling efficient and portable deployment of TFLite workloads on many different heterogeneous SoC architectures.\n\n\n\n\n\nThe rise of heterogeneous computing has also brought a rise in diversity of the instruction set architectures (ISA) of CPUs and GPUs. One of the most important levels of abstraction in the computing stack, an ISA is the set of instruction that can be used to program a processor by being the target of the compilation process of software application. As more diverse processors are brough together in a computing system, more diverse ISAs are used to program it. This phenomenon, together with the growing adoption of virtual machines, is driving a need for fast, scalable, full-system, cross-ISA emulation and instrumentation tools. Unfortunately, achieving high performance for these cross-ISA tools is challenging due to dynamic binary translation (DBT) overhead and the complexity of instrumenting full-system emulators.\n\n\nTo address these challenges and improve cross-ISA emulation and instrumentation performance, the PI and his collaborator developed three novel techniques. First, they increased the emulation performance of floating-point (FP) operations by observing that most FP operations can be correctly emulated by surrounding the use of the host FP unit with a minimal amount of non-FP code. Second, they introduced the design of a translator with a shared code cache that scales for multi-core guests, even when they generate translated code in parallel at a high rate. Third, they developed an ISA-agnostic instrumentation layer that can instrument guest operations that occur outside of the DBT's intermediate representation, which are common in full-system emulators. They implemented these three techniques in Qelt, a novel cross-ISA machine emulator and tool for dynamic binary instrumentation based on QEMU, which is a free and open-source emulator of many popular commercial processors. With an extensive and thorough set of experimental results, they demonstrated the scalability of Qelt, how it outperforms QEMU as a full-system cross-ISA machine emulator for integer and FP workloads as well as state-of-the-art, cross-ISA, full-system instrumentation tools, and how it can match the performance of Pin, a state-of-the-art, same-ISA dynamic binary instrumentation tool, when used for complex instrumentation such as cache simulation.\n\n\n\n\n\nAs part of the broader-impact activities for this project, the PI and his collaborators have (1) released the implementation of WOLT in the public domain and (2) made the innovative techniques that they developed in Qelt available to the open-source software community by merging them into the upstream QEMU distribution.\n\n\n\t\t\t\t\tLast Modified: 11/03/2024\n\n\t\t\t\t\tSubmitted by: LucaCarloni\n"
 }
}