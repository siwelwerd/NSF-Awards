{
 "awd_id": "1841670",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Tagging and Browsing Videos According to the Preferences of Differing Affinity Groups",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 160000.0,
 "awd_amount": 160000.0,
 "awd_min_amd_letter_date": "2018-08-11",
 "awd_max_amd_letter_date": "2018-08-11",
 "awd_abstract_narration": "This exploratory project studies how different groups of people tend to create and describe (tag) videos of the same common event differently. Since each of these affinity groups share particular backgrounds, languages, interests, and locations, they also tag and browse these videos differently as well. For example, an international health crisis is portrayed rather differently by the news media of China compared to that of the U.S. Chinese coverage tends to include historical clips, talks about countries, and has less focus on individual participants. In contrast, U.S. coverage tends to be more contemporary, uses countries mainly to identify the background of individuals, and prefers to highlight the actions and reactions of people who are fully named. The aim of this project is to develop new ways of making the videos of other groups to be more accessible and more understandable to different groups by developing a browser that graphically illustrates the visual differences across such videos, and that translates the preferred tags of one group into the preferred tags of the other. The resulting new browser would make a broad range of videos, especially those in a different language, easier to find, scan, and compare.\r\n\r\nThis exploratory project has three major components: (1) Development of a novel, shareable catalog of statistically significant cross-group differences. To do this, it will first map visual and textual features, from many videos about a single topic, into a joint latent space. Then, by using reliable shared visual cues, it will determine tag relationships using variants of canonical correlation analysis, sort them using measures such as G2, and validate them against users who are members of both groups. (2) Exploration of how well these non-linear methods can be extended to the videos of multiple pairs of affinity groups with more subtle differences, such as comparing the U.S. to Canada. In particular, it will use a novel variant of the PageRank algorithm to track the influence and persistence of visual memes across groups, validating this by ground truth. (3) Implementing and evaluating a prototype browser that visualizes on parallel timelines the perspectives of different groups, both statically and dynamically as they evolve over time. The project will measure the usual browser attributes (time, accuracy, satisfaction, ease of use), but also some unusual and exploratory ones (appropriateness of retrieval, accuracy of tag translations, increases in user engagement, impact on journalists).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Kender",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "John R Kender",
   "pi_email_addr": "kender@cs.columbia.edu",
   "nsf_id": "000285996",
   "pi_start_date": "2018-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "2960 Broadway",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 160000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We investigated the new task of detecting and presenting those differences in news videos of a common event that are due to preferences of affinity groups.&nbsp; We mapped visual and multilingual textual features into a joint latent space using reliable visual cues, and determined word and tag relationships through various canonical correlation analysis variants.&nbsp; Using non-negative matrix factorizations, we detected the principal clusters of images and words most representative of a group's concerns.</p>\n<p>For human-interest international events, we detected culturally preferred emphases and tags from U.S., Chinese, and other cultures' news sources.&nbsp; We visualized these results with group-specific information on separate timelines in a custom video browser, with connections and visualizations showing commonalities and differences.&nbsp; Additionally, we investigated and visualized through customized word clouds the group-specific emotional reactions of their viewers to these events, using group-specific media such as Twitter and Weibo.</p>\n<p>The broader impacts of this project include the development of a set of algorithms and user interfaces that allow the exploration and visualization of affinity group differences, whether by country, language, news agency, political leanings, or other group definitions.&nbsp; These tools have demonstrated that there are no \"pure\" cultural data sources, and that even general purpose video providers like YouTube and Baidu have implicit biases that can be recovered and illustrated.&nbsp; This project hopes to broaden our knowledge and our acceptance of differing viewpoints to shared events.</p>\n<p>The primary intellectual merit of this project was the fusion and adaptation of the techniques of four separate computer science disciplines to this challenging domain.</p>\n<p>1) System integration, including data gathering</p>\n<p>We produced a prototype system for gathering videos of an international topic, reducing them to features, and then detecting concept clusters (image-word pairs) for each affinity group. On examination, its speed and accuracy was shown to be limited then by a need to distinguish white-list (named entities) and black-list (advertisement) words, so pipelines to determine those words were established.&nbsp; A more efficient clustering technique based on non-negative matrix factorization was implemented and analyzed.</p>\n<p>We established a back-end repository of videos for about eight international events, preprocessed to isolate keyframes and keywords based on written or spoken descriptions. Much of the subsequent research was focused on Chinese versus U.S. video coverage and viewer responses, principally to the AlphaGo and COVID events.&nbsp; Independently, we were able to show, using Google Trends, that an \"interesting\" international event was one characterized by the concern in one group about a named entity from the other group.&nbsp; This was particularly true if a death was involved.</p>\n<p>2) Computer vision</p>\n<p>Several different methods of determining a representative set of key frames under the constraints of news video creation were explored: preprocessing to handle blur; detection and extracting in-frame captions; encoding frames using a deep learning computer vision system, then hashing the results into buckets; clustering frames by their object content using YOLO; and designing a custom autoencoder for this data in order to obtain a short crisp encoding.&nbsp; The last proved most effective.</p>\n<p>Independently, we showed that even given the same text word (or its translation), the video imagery that was chosen by a particular group to illustrate that word very often reflected characteristic group preoccupations</p>\n<p>3) Natural language processing</p>\n<p>The detection of named entities were shown to be powerful cue to affinity group preferences; in fact, cultural groups tended to have characteristic statistical outlier entities.&nbsp; Given their impact on the image-word embeddings, we refined several methods for finding and cataloging such white-list and black-list words, and visualizing their relationships via t-SNE plots. The most successful methods were based on the BERT transformer, tuned to our custom black-list dictionary.&nbsp; White-list words were refined from standard corpora, but were again influenced by our custom news-based white-list dictionary.</p>\n<p>We noted that many of the words used in news reportage and viewer response were emotionally laden.&nbsp; In particular, we found that Chinese sources tended to be nationalistic and positive, whereas U.S sources tended to be individualist and somewhat negative.&nbsp; These emotional words were detected and visualized by color coding them and displaying them in a word cloud, as an add-on to the cross-culture browser.&nbsp; Likewise, we found that emoji usage was similarly group-sensitive.</p>\n<p>4) User interface design</p>\n<p>A prototype browser that displayed news videos as \"drops\" on two parallel timelines.was developed, which gave an overview of the coverage of an event. Particular time intervals could be zoomed in on.&nbsp; Hovering over a video's drop caused connecting lines to appear to show its counterpart in the other group.&nbsp; Clicking on a drop caused more detailed information to pop up, including the key visual frame, the top five keywords that uniquely defined this video against others, and the associated descriptive text.</p>\n<p>Informal user studies over several semesters led to several iterative improvements of the design, with generally positive feedback from bilingual users.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/03/2022<br>\n\t\t\t\t\tModified by: John&nbsp;R&nbsp;Kender</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1841670/1841670_10569274_1648992630291_21x_Luvena_Figure10_cropped--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1841670/1841670_10569274_1648992630291_21x_Luvena_Figure10_cropped--rgov-800width.jpg\" title=\"Chinese vs. U.S. responses to a COVID video\"><img src=\"/por/images/Reports/POR/2022/1841670/1841670_10569274_1648992630291_21x_Luvena_Figure10_cropped--rgov-66x44.jpg\" alt=\"Chinese vs. U.S. responses to a COVID video\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A novel word cloud simultaneously showing English and Chinese viewers' emotional responses to videos on YouTube about COVID, posted by ABC News and iQiyi, respectively.  Positive emotions are encoded in red, negative in blue.  The visualization shows dramatic differences between the two groups.</div>\n<div class=\"imageCredit\">Luvena Huo</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">John&nbsp;R&nbsp;Kender</div>\n<div class=\"imageTitle\">Chinese vs. U.S. responses to a COVID video</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1841670/1841670_10569274_1648993297885_19x_Jiaqi_Figure25_cropped--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1841670/1841670_10569274_1648993297885_19x_Jiaqi_Figure25_cropped--rgov-800width.jpg\" title=\"Cross-cultural video browser\"><img src=\"/por/images/Reports/POR/2022/1841670/1841670_10569274_1648993297885_19x_Jiaqi_Figure25_cropped--rgov-66x44.jpg\" alt=\"Cross-cultural video browser\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Prototype of cross-cultural browser, showing China and U.S. coverage of the AlphaGo tournament, with similar events matched via joint embeddings of images and text. The given pairing shows the Chinese emphasis is on the quality of game play, whereas the U.S. emphasis is on AI technology itself.</div>\n<div class=\"imageCredit\">Jiaqi Liu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">John&nbsp;R&nbsp;Kender</div>\n<div class=\"imageTitle\">Cross-cultural video browser</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nWe investigated the new task of detecting and presenting those differences in news videos of a common event that are due to preferences of affinity groups.  We mapped visual and multilingual textual features into a joint latent space using reliable visual cues, and determined word and tag relationships through various canonical correlation analysis variants.  Using non-negative matrix factorizations, we detected the principal clusters of images and words most representative of a group's concerns.\n\nFor human-interest international events, we detected culturally preferred emphases and tags from U.S., Chinese, and other cultures' news sources.  We visualized these results with group-specific information on separate timelines in a custom video browser, with connections and visualizations showing commonalities and differences.  Additionally, we investigated and visualized through customized word clouds the group-specific emotional reactions of their viewers to these events, using group-specific media such as Twitter and Weibo.\n\nThe broader impacts of this project include the development of a set of algorithms and user interfaces that allow the exploration and visualization of affinity group differences, whether by country, language, news agency, political leanings, or other group definitions.  These tools have demonstrated that there are no \"pure\" cultural data sources, and that even general purpose video providers like YouTube and Baidu have implicit biases that can be recovered and illustrated.  This project hopes to broaden our knowledge and our acceptance of differing viewpoints to shared events.\n\nThe primary intellectual merit of this project was the fusion and adaptation of the techniques of four separate computer science disciplines to this challenging domain.\n\n1) System integration, including data gathering\n\nWe produced a prototype system for gathering videos of an international topic, reducing them to features, and then detecting concept clusters (image-word pairs) for each affinity group. On examination, its speed and accuracy was shown to be limited then by a need to distinguish white-list (named entities) and black-list (advertisement) words, so pipelines to determine those words were established.  A more efficient clustering technique based on non-negative matrix factorization was implemented and analyzed.\n\nWe established a back-end repository of videos for about eight international events, preprocessed to isolate keyframes and keywords based on written or spoken descriptions. Much of the subsequent research was focused on Chinese versus U.S. video coverage and viewer responses, principally to the AlphaGo and COVID events.  Independently, we were able to show, using Google Trends, that an \"interesting\" international event was one characterized by the concern in one group about a named entity from the other group.  This was particularly true if a death was involved.\n\n2) Computer vision\n\nSeveral different methods of determining a representative set of key frames under the constraints of news video creation were explored: preprocessing to handle blur; detection and extracting in-frame captions; encoding frames using a deep learning computer vision system, then hashing the results into buckets; clustering frames by their object content using YOLO; and designing a custom autoencoder for this data in order to obtain a short crisp encoding.  The last proved most effective.\n\nIndependently, we showed that even given the same text word (or its translation), the video imagery that was chosen by a particular group to illustrate that word very often reflected characteristic group preoccupations\n\n3) Natural language processing\n\nThe detection of named entities were shown to be powerful cue to affinity group preferences; in fact, cultural groups tended to have characteristic statistical outlier entities.  Given their impact on the image-word embeddings, we refined several methods for finding and cataloging such white-list and black-list words, and visualizing their relationships via t-SNE plots. The most successful methods were based on the BERT transformer, tuned to our custom black-list dictionary.  White-list words were refined from standard corpora, but were again influenced by our custom news-based white-list dictionary.\n\nWe noted that many of the words used in news reportage and viewer response were emotionally laden.  In particular, we found that Chinese sources tended to be nationalistic and positive, whereas U.S sources tended to be individualist and somewhat negative.  These emotional words were detected and visualized by color coding them and displaying them in a word cloud, as an add-on to the cross-culture browser.  Likewise, we found that emoji usage was similarly group-sensitive.\n\n4) User interface design\n\nA prototype browser that displayed news videos as \"drops\" on two parallel timelines.was developed, which gave an overview of the coverage of an event. Particular time intervals could be zoomed in on.  Hovering over a video's drop caused connecting lines to appear to show its counterpart in the other group.  Clicking on a drop caused more detailed information to pop up, including the key visual frame, the top five keywords that uniquely defined this video against others, and the associated descriptive text.\n\nInformal user studies over several semesters led to several iterative improvements of the design, with generally positive feedback from bilingual users.\n\n \n\n\t\t\t\t\tLast Modified: 04/03/2022\n\n\t\t\t\t\tSubmitted by: John R Kender"
 }
}