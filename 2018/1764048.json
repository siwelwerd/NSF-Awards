{
 "awd_id": "1764048",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI:Medium:Collaborative Research: Developing a Uniform Meaning Representation for Natural Language Processing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 399894.0,
 "awd_amount": 399894.0,
 "awd_min_amd_letter_date": "2018-07-23",
 "awd_max_amd_letter_date": "2018-07-23",
 "awd_abstract_narration": "The use of intelligent agents that can communicate with us in human language has become an essential part of our daily lives.  Today's intelligent agents can respond appropriately to many things we say or text to them, but they cannot yet communicate fully like humans. They lack our general ability to arrive quickly at accurate and relevant interpretations of what others communicate to us and to form appropriate responses, particularly in sustained interactions.  The typical way we teach a machine to acquire such ability is to provide it with approximations of the meanings of utterances in the contexts in which they have occurred in the past.  Over the years these approximations have become increasingly rich and detailed, enabling ever more sophisticated systems for interacting with computers using natural language, such as searching for information, getting up-to-date recommendations for products and services, and translating foreign languages.  The goal of this project is to bring together linguists and computer scientists to jointly develop a practical meaning representation formalism based on these rich approximations that can be applied to a much more diverse set of languages.   This will allow us to use machine learning to develop techniques to automatically translate human utterances into our meaning formalism. In turn, this will enable intelligent agents to acquire more advanced communication capabilities, and for a wider range of languages.  The languages considered for the project include those spoken by large populations such as English, Chinese and Arabic, as well as native tongues of smaller groups such as Norwegian, and Arapaho and Kukama-Kukamira, two indigenous languages of the Americas.  As such, this project will help bring modern technology to smaller groups so that all people can benefit equally from technological advancement.  The project will also contribute to the development of the US workforce by training a new generation of researchers on cutting-edge technologies in artificial intelligence.  \r\n\r\nThis project brings together an interdisciplinary team of linguists and computer scientists from three institutions to jointly develop a Uniform Meaning Representation (UMR). UMR is a practical, formal, computationally tractable, and cross-linguistically valid meaning representation of natural language that can impact a wide range of downstream applications requiring deep natural language understanding (NLU).  UMR will extend existing meaning representations to include quantifier types and relations, modality, negation, tense and aspect, and be tested on a typologically diverse set of languages.  Methods and techniques for UMR annotation, parsing and generation, and evaluation will be uniform across languages.  The project will also develop novel algorithms and models for UMR-based broad-coverage and general-purpose multilingual semantic parsers.  Students participating in the project will receive training in the full cycle of conceptualizing, producing, processing, and consuming meaning representations at the sites of participating institutions.  This project will help to build a community of NLP researchers that will contribute to the development of UMR-based data and tools and advance the state of the art in Natural Language Processing (NLP) in particular, and Artificial Intelligence (AI) in general.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Martha",
   "pi_last_name": "Palmer",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Martha S Palmer",
   "pi_email_addr": "mpalmer@colorado.edu",
   "nsf_id": "000377893",
   "pi_start_date": "2018-07-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Martin",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "James H Martin",
   "pi_email_addr": "martin@cs.colorado.edu",
   "nsf_id": "000142893",
   "pi_start_date": "2018-07-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "J. Andrew",
   "pi_last_name": "Cowell",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "J. Andrew Cowell",
   "pi_email_addr": "James.Cowell@colorado.edu",
   "nsf_id": "000416270",
   "pi_start_date": "2018-07-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Colorado at Boulder",
  "inst_street_address": "3100 MARINE ST",
  "inst_street_address_2": "STE 481 572 UCB",
  "inst_city_name": "Boulder",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3034926221",
  "inst_zip_code": "803090001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "THE REGENTS OF THE UNIVERSITY OF COLORADO",
  "org_prnt_uei_num": "",
  "org_uei_num": "SPVKK1RC2MZ3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Colorado at Boulder",
  "perf_str_addr": "3100 Marine Street, Room 481",
  "perf_city_name": "Boulder",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "803090572",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 399894.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-da586174-7fff-fc6f-ff0a-eaa9070da0c4\"> </span></p>\n<p>UMR Phase 1 Outcome statement (200-800 words)</p>\n<p>Intelligent agents have increasingly become part of our everyday lives and can now respond appropriately to many verbal or textual communications. They often make use of a meaning formalism that can be understood or executed by the computer. As this meaning formalism gets richer and more detailed, intelligent agents can do more things that are valuable to humans, from answering our questions about the weather to playing the music that we want, from providing information about flights to translating one language into another.</p>\n<p>The goal of the Uniform Meaning Representation (UMR) project was to develop a meaning formalism to mediate communication between humans and computers in a wide range of languages of the world. This is a complicated undertaking that has required expertise from both computer scientists and linguists who understand the similarities and differences between the world&rsquo;s languages. Towards this goal, researchers from Brandeis University, the University of Colorado Boulder, and the University of New Mexico jointly developed the Uniform Meaning Representation in partnership with an international group of scientists.</p>\n<p>The researchers started from a popular existing meaning formalism called Abstraction Meaning Representation (AMR).&nbsp; They enriched AMR to cover a wider range of meanings and generalized it so that it can work for a comprehensive set of the world&rsquo;s languages. In this process, they tested UMR on diverse languages that include those spoken by large populations such as Arabic, Chinese, and English, as well as native languages of smaller groups such as Arapaho, Kukama-Kukamiria, Navajo, and Sanapan&aacute;. As part of this effort, they organized workshops to invite feedback from fellow researchers on the representation and held tutorials at computational linguistics conferences to disseminate the research. They also developed tools that fellow researchers can use to produce UMRs for their own languages.</p>\n<p>Project award statement:</p>\n<p>&ldquo;&hellip;The goal of this project is to bring together linguists and computer scientists to jointly develop a practical meaning representation formalism based on current rich approximations that can be applied to a much more diverse set of languages. This will allow us to use machine learning to develop techniques to automatically translate human utterances into our meaning formalism. In turn, this will enable intelligent agents to acquire more advanced communication capabilities, and for a wider range of languages. The languages considered for the project include those spoken by large populations such as English, Chinese and Arabic, as well as native tongues of smaller groups such as Norwegian, and Arapaho and Kukama-Kukamira, two indigenous languages of the Americas&hellip;. The project will also contribute to the development of the US workforce by training a new generation of researchers on cutting-edge technologies in artificial intelligence.&rdquo;</p>\n<p>Partial summary from project report:</p>\n<p>[We started] with Abstract Meaning Representation (AMR) and adapted it to a multilingual setting. This involves identifying English-specific categories that need to be modified to apply more generally across languages and simplifying appropriate aspects of AMR to facilitate annotation for languages where there are fewer resources. Another major goal is identifying possible enhancements of AMR that can better support downstream applications such as temporal relations, modality, aspect, quantification and negation. <strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></p>\n<p>Our general strategy has been to first develop annotation schemes for individual meaning components, and then integrate them after they are thoroughly tested. We decided to treat tense and modality as dependency relations that abstract away from their grammatical realizations, which tend to vary a great deal from language to language. We have also proposed annotating quantification and negation as scopal relations that can be represented structurally, simplifying addition to AMR. For purposes of readability, one of the most attractive properties of AMR, it is best to conceive of UMR as two companion structures, one at the sentence level and one at the discourse level. The sentence-level representation focuses on the argument structure of predicates; the sense of the predicates; aspect; named entities, and relations. The discourse-level structure focuses on coreference; temporal relations; modal dependencies, and discourse relations, all of which are relations that go beyond sentence boundaries.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>\n<p>In collaboration with our international partners, we have organized a series of international workshops on designing meaning representations.&nbsp; This has enabled gathering input from the broader research community, as well as providing a forum for hearing from fellow researchers who have attempted to annotate AMRs for other languages about which aspects of AMR readily apply to their language and which don&rsquo;t. We have also received feedback from AMR parser developers about which aspects of AMR are particularly difficult to parse, and from AMR users about which aspects of AMR are particularly useful (or not) for their applications.</p>\n<p dir=\"ltr\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/05/2023<br>\n\t\t\t\t\tModified by: Martha&nbsp;S&nbsp;Palmer</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \n\nUMR Phase 1 Outcome statement (200-800 words)\n\nIntelligent agents have increasingly become part of our everyday lives and can now respond appropriately to many verbal or textual communications. They often make use of a meaning formalism that can be understood or executed by the computer. As this meaning formalism gets richer and more detailed, intelligent agents can do more things that are valuable to humans, from answering our questions about the weather to playing the music that we want, from providing information about flights to translating one language into another.\n\nThe goal of the Uniform Meaning Representation (UMR) project was to develop a meaning formalism to mediate communication between humans and computers in a wide range of languages of the world. This is a complicated undertaking that has required expertise from both computer scientists and linguists who understand the similarities and differences between the world\u2019s languages. Towards this goal, researchers from Brandeis University, the University of Colorado Boulder, and the University of New Mexico jointly developed the Uniform Meaning Representation in partnership with an international group of scientists.\n\nThe researchers started from a popular existing meaning formalism called Abstraction Meaning Representation (AMR).  They enriched AMR to cover a wider range of meanings and generalized it so that it can work for a comprehensive set of the world\u2019s languages. In this process, they tested UMR on diverse languages that include those spoken by large populations such as Arabic, Chinese, and English, as well as native languages of smaller groups such as Arapaho, Kukama-Kukamiria, Navajo, and Sanapan&aacute;. As part of this effort, they organized workshops to invite feedback from fellow researchers on the representation and held tutorials at computational linguistics conferences to disseminate the research. They also developed tools that fellow researchers can use to produce UMRs for their own languages.\n\nProject award statement:\n\n\"&hellip;The goal of this project is to bring together linguists and computer scientists to jointly develop a practical meaning representation formalism based on current rich approximations that can be applied to a much more diverse set of languages. This will allow us to use machine learning to develop techniques to automatically translate human utterances into our meaning formalism. In turn, this will enable intelligent agents to acquire more advanced communication capabilities, and for a wider range of languages. The languages considered for the project include those spoken by large populations such as English, Chinese and Arabic, as well as native tongues of smaller groups such as Norwegian, and Arapaho and Kukama-Kukamira, two indigenous languages of the Americas&hellip;. The project will also contribute to the development of the US workforce by training a new generation of researchers on cutting-edge technologies in artificial intelligence.\"\n\nPartial summary from project report:\n\n[We started] with Abstract Meaning Representation (AMR) and adapted it to a multilingual setting. This involves identifying English-specific categories that need to be modified to apply more generally across languages and simplifying appropriate aspects of AMR to facilitate annotation for languages where there are fewer resources. Another major goal is identifying possible enhancements of AMR that can better support downstream applications such as temporal relations, modality, aspect, quantification and negation.                                \n\nOur general strategy has been to first develop annotation schemes for individual meaning components, and then integrate them after they are thoroughly tested. We decided to treat tense and modality as dependency relations that abstract away from their grammatical realizations, which tend to vary a great deal from language to language. We have also proposed annotating quantification and negation as scopal relations that can be represented structurally, simplifying addition to AMR. For purposes of readability, one of the most attractive properties of AMR, it is best to conceive of UMR as two companion structures, one at the sentence level and one at the discourse level. The sentence-level representation focuses on the argument structure of predicates; the sense of the predicates; aspect; named entities, and relations. The discourse-level structure focuses on coreference; temporal relations; modal dependencies, and discourse relations, all of which are relations that go beyond sentence boundaries.                       \n\nIn collaboration with our international partners, we have organized a series of international workshops on designing meaning representations.  This has enabled gathering input from the broader research community, as well as providing a forum for hearing from fellow researchers who have attempted to annotate AMRs for other languages about which aspects of AMR readily apply to their language and which don\u2019t. We have also received feedback from AMR parser developers about which aspects of AMR are particularly difficult to parse, and from AMR users about which aspects of AMR are particularly useful (or not) for their applications.\n \n\n\t\t\t\t\tLast Modified: 05/05/2023\n\n\t\t\t\t\tSubmitted by: Martha S Palmer"
 }
}