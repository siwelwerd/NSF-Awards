{
 "awd_id": "1822191",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium: Collaborative Research: Next-Generation Message Passing for Parallel Programming: Resiliency, Time-to-Solution, Performance-Portability, Scalability, and QoS",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 523740.0,
 "awd_amount": 659277.0,
 "awd_min_amd_letter_date": "2018-01-10",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Parallel programming based on MPI is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics.  Emerging supercomputer systems will have more faults and MPI needs to be able to workaround such faults to be appropriate to these emerging situations, rather than causing an entire application to fail.  Collaborative, transformative message passing research for High Performance Computing (HPC) critical to performance-portable parallel programming in new and forthcoming scalable systems (with a strategy of \"best practice-first, standardization-later\") is being reduced to practice. A substantial subset of the Message Passing Interface (MPI-3/4) application programmer interface is being made fault tolerant through extensions with weak collective transactions that synchronize between parallel tasks. \r\n\r\nThis research studies  the novel model that localizes faults, provides tunable fault-free overhead, allows for multiple kinds of faults, enables hierarchical recovery, and is data-parallel relevant.  Fault modeling of underlying networks is being studied. Application developers control the granularity and fault-free overhead in this effort. Performance and scalability results of the middleware prototype are being demonstrated principally through compact applications that relate to real use cases of practical and academic interest. The impact of this work ranges from users of the largest supercomputers in government labs to practical clusters that have long-running, time-critical applications, and to space-based and other parallel processing in \"hostile\" environments where faults occur more frequently than in past years.  The project is producing usable free software that will be widely shared in the community as well as guidance on how better parallel programs can be written in academia, industry, and government.  The project also provides guidelines for how to update existing or legacy programs to use the new capabilities that are being reduced to practice.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anthony",
   "pi_last_name": "Skjellum",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anthony Skjellum",
   "pi_email_addr": "askjellum@tntech.edu",
   "nsf_id": "000446874",
   "pi_start_date": "2018-01-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Chattanooga",
  "inst_street_address": "615 MCCALLIE AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHATTANOOGA",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "4234254431",
  "inst_zip_code": "374032504",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "TN03",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "RZ1YV5AUBN39",
  "org_uei_num": "JNZFHMGJN7M3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Chattanooga",
  "perf_str_addr": "615 McCallie Ave",
  "perf_city_name": "Chattanooga",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "374032504",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "TN03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 40370.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 156490.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 167938.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 174942.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 119537.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This SHF Project considered fault tolerance models for the Message Passing Interface (MPI). We designed and prototyped two new models of fault tolerance, FA-MPI and Stages, both of which have resulted in scholarly publications and student theses/dissertations. These models are distinct from the widely discussed ULFM model advanced by the University of Tennessee at Knoxville (UTK). The Stages model is a superset of the &ldquo;Reinit models&rdquo; defined by colleagues at LLNL, with whom we also collaborate extensively.&nbsp;&nbsp;Stages offers significant enhancements over Reinit, such as C++ compatibility and interfaces well with the checkpoint-restart best practices in HPC applications. We plan to propose these models for the MPI-5 standard; broader adoption of these models is a key, on-going task as well.&nbsp;&nbsp;Both Stages and FA-MPI are open-source software, which are open source, available upon request.</p>\n<p>&nbsp;</p>\n<p>We advanced the ExaMPI strong-progress implementation of MPI, which is a subset of MPI-4. This implementation enables unique explorations of new features and supports overlap of communication and computation in ways that OpenMPI, MPICH, and derivatives cannot.&nbsp;&nbsp;It is currently being used as part of the UNM, UA, and UTC PSAAP III Center as a key experimental vehicle for understanding the achievable performance of MPI+Accelerator primitives in heterogeneous architectures.&nbsp;&nbsp;This software is open source, and available to third parties upon request.&nbsp;&nbsp;It has generated several theses at UTC, Auburn University, and elsewhere.</p>\n<p>&nbsp;</p>\n<p>In addition to working on the fault-tolerant models, we designed nonblocking, persistent operations for collective communication together with collaborators, which were included in the MPI-4 standard.&nbsp;&nbsp;This is a key step to providing a complete set of nonblocking operations in MPI in order to enable the FA-MPI model, and other models, to support fully nonblocking parallel programs.</p>\n<p>&nbsp;</p>\n<p>Further, we designed, prototyped, and standardized in MPI-4, together with colleagues, the partitioned point-to-point communication primitives that describe multi-producer, multi-consumer models of buffer manipulation, consistent with on-node parallelism enabled by OpenMP and/or accelerator kernels. These APIs are important to support hybrid parallel programming in current architectures, which typically include GPUs. An open-source library, MPIPCL, was prototyped and disseminated (BSD 3-clause license).&nbsp;&nbsp;This software is enabling experimentation by third parties at present with partitioned communication, which has not been widely supported in production MPI implementations as of now. We also prototyped and studied the use of partitioned point-to-point communication in several well-known mini-apps, in order to determine how well these operations match with the idioms and properties of key application patterns.</p>\n<p>&nbsp;</p>\n<p>In addition to the point-to-point support for partitioned operations, we have designed a subset of the collective partitioned algorithms, published concerning these new operations, and worked with a graduate student and colleague at Tennessee Technological University on extending the MPIPCL library to support these operations.&nbsp;&nbsp;Early access to these collective operations will help enable their prospects for standardization in MPI-5.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>We played a significant role, overall, in the development of the MPI-4 standard.&nbsp;&nbsp;Besides those topics described above, we worked on the clarification of the semantic terms, helped create a partially executable specification via pythonization of language bindings, and we began work on a new C++-based language interface for MPI-5.</p>\n<p>&nbsp;</p>\n<p>Several REU students (funded through supplements) and one PhD at both UA and UTC have gone on to internships with US DOE laboratories (LLNL, Sandia, and PNNL), continuing work on aspects of MPI including Stages, FA-MPI, C++ language interfaces, and heterogeneous communication. One postdoc who worked at UTC, went on to working as professional staff at UA under non-NSF funding, and, subsequently, has gone on to an internship at LANL. A postdoc funded at UAB (prior to Dr. Bangalore&rsquo;s move to UA) has gone on to career employment at UAB. A PhD student who completed at UAB has gone on to professional employment at Bodo.AI, working on translation of python programs into MPI programs for AI applications.&nbsp;&nbsp;A PhD student who completed at Auburn (under subscontract from UTC), completed her PhD and now works at Intel Corporation in their Accelerator R&amp;D group.</p>\n<p>&nbsp;</p>\n<p>Our outreach includes the successful ExaMPI Workshop at the Supercomputing Conference over the past seven years, with the workshop continuing at SC22 in November, 2022.&nbsp;&nbsp;We also have supported the Compiler-assisted Correctness Checking and Performance Optimization for HPC</p>\n<p>(C3PO) workshop (source-to-source translation R&amp;D for HPC) at ISC in Germany over the past three years.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/23/2022<br>\n\t\t\t\t\tModified by: Anthony&nbsp;Skjellum</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis SHF Project considered fault tolerance models for the Message Passing Interface (MPI). We designed and prototyped two new models of fault tolerance, FA-MPI and Stages, both of which have resulted in scholarly publications and student theses/dissertations. These models are distinct from the widely discussed ULFM model advanced by the University of Tennessee at Knoxville (UTK). The Stages model is a superset of the \"Reinit models\" defined by colleagues at LLNL, with whom we also collaborate extensively.  Stages offers significant enhancements over Reinit, such as C++ compatibility and interfaces well with the checkpoint-restart best practices in HPC applications. We plan to propose these models for the MPI-5 standard; broader adoption of these models is a key, on-going task as well.  Both Stages and FA-MPI are open-source software, which are open source, available upon request.\n\n \n\nWe advanced the ExaMPI strong-progress implementation of MPI, which is a subset of MPI-4. This implementation enables unique explorations of new features and supports overlap of communication and computation in ways that OpenMPI, MPICH, and derivatives cannot.  It is currently being used as part of the UNM, UA, and UTC PSAAP III Center as a key experimental vehicle for understanding the achievable performance of MPI+Accelerator primitives in heterogeneous architectures.  This software is open source, and available to third parties upon request.  It has generated several theses at UTC, Auburn University, and elsewhere.\n\n \n\nIn addition to working on the fault-tolerant models, we designed nonblocking, persistent operations for collective communication together with collaborators, which were included in the MPI-4 standard.  This is a key step to providing a complete set of nonblocking operations in MPI in order to enable the FA-MPI model, and other models, to support fully nonblocking parallel programs.\n\n \n\nFurther, we designed, prototyped, and standardized in MPI-4, together with colleagues, the partitioned point-to-point communication primitives that describe multi-producer, multi-consumer models of buffer manipulation, consistent with on-node parallelism enabled by OpenMP and/or accelerator kernels. These APIs are important to support hybrid parallel programming in current architectures, which typically include GPUs. An open-source library, MPIPCL, was prototyped and disseminated (BSD 3-clause license).  This software is enabling experimentation by third parties at present with partitioned communication, which has not been widely supported in production MPI implementations as of now. We also prototyped and studied the use of partitioned point-to-point communication in several well-known mini-apps, in order to determine how well these operations match with the idioms and properties of key application patterns.\n\n \n\nIn addition to the point-to-point support for partitioned operations, we have designed a subset of the collective partitioned algorithms, published concerning these new operations, and worked with a graduate student and colleague at Tennessee Technological University on extending the MPIPCL library to support these operations.  Early access to these collective operations will help enable their prospects for standardization in MPI-5.\n\n \n\n \n\nWe played a significant role, overall, in the development of the MPI-4 standard.  Besides those topics described above, we worked on the clarification of the semantic terms, helped create a partially executable specification via pythonization of language bindings, and we began work on a new C++-based language interface for MPI-5.\n\n \n\nSeveral REU students (funded through supplements) and one PhD at both UA and UTC have gone on to internships with US DOE laboratories (LLNL, Sandia, and PNNL), continuing work on aspects of MPI including Stages, FA-MPI, C++ language interfaces, and heterogeneous communication. One postdoc who worked at UTC, went on to working as professional staff at UA under non-NSF funding, and, subsequently, has gone on to an internship at LANL. A postdoc funded at UAB (prior to Dr. Bangalore\u2019s move to UA) has gone on to career employment at UAB. A PhD student who completed at UAB has gone on to professional employment at Bodo.AI, working on translation of python programs into MPI programs for AI applications.  A PhD student who completed at Auburn (under subscontract from UTC), completed her PhD and now works at Intel Corporation in their Accelerator R&amp;D group.\n\n \n\nOur outreach includes the successful ExaMPI Workshop at the Supercomputing Conference over the past seven years, with the workshop continuing at SC22 in November, 2022.  We also have supported the Compiler-assisted Correctness Checking and Performance Optimization for HPC\n\n(C3PO) workshop (source-to-source translation R&amp;D for HPC) at ISC in Germany over the past three years.\n\n \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/23/2022\n\n\t\t\t\t\tSubmitted by: Anthony Skjellum"
 }
}