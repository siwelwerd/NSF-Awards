{
 "awd_id": "1813709",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Learning Dynamics and Evolution towards Cognitive Understanding of Videos",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 449990.0,
 "awd_amount": 465990.0,
 "awd_min_amd_letter_date": "2018-08-11",
 "awd_max_amd_letter_date": "2019-04-22",
 "awd_abstract_narration": "A fundamental capability of human intelligence is being able to learn to act by watching instructional videos. Such capability is reflected in abstraction and summarization of the instructional procedures as well as in answering questions such as \"why\" and \"how\" something happened in the video. This project aims to build computational models that are able to perform well in above tasks, which require, beyond the conventional recognition of objects, actions and attributes in the scene, the higher-order inference of any relations therein. Here, the higher-order inference refers to inference that cannot be answered immediately by direct observations and thus requires stronger semantics. The developed technology will enable many applications in other fields, e.g., multimedia (video indexing and retrieval), robotics (reasoning capability of why and how questions), and healthcare (assistive devices for visually impaired people). In addition, the project will contribute to education and diversity by involving underrepresented groups in research activities, integrating research results into teaching curriculum, and conducting outreach activities to local K-12 communities. \r\n\r\nThe research will develop a framework to perform higher-order inference in understanding web instructional videos, such that models devised in this framework are capable of not only discovering and captioning procedures that constitute the instructional event but also answering questions such as why and how something happened. The framework is built on a video story graph that models the dynamics (the composition of actions at different scales) and evolution (the change in object states and attributes), and it supports higher-order inference upon deep learning units and incorporation of external knowledge graph in a unified framework. Methodologies to extract such video story graphs and use them to discover, caption procedures and perform question-answering will be explored. Expected outcomes of this project include: a software package for constructing and performing inference on video story graphs and incorporating external knowledge; a web-deployed system to process user-uploaded instructional videos; and a large video dataset with procedure and question-answering annotations.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chenliang",
   "pi_last_name": "Xu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chenliang Xu",
   "pi_email_addr": "chenliang.xu@rochester.edu",
   "nsf_id": "000728261",
   "pi_start_date": "2018-08-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jiebo",
   "pi_last_name": "Luo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jiebo Luo",
   "pi_email_addr": "jluo@cs.rochester.edu",
   "nsf_id": "000601671",
   "pi_start_date": "2018-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146270140",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 449990.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>A fundamental capability of human intelligence is learning to act by watching instructional videos. Such power is reflected in abstraction and summarization of the instructional procedures and in answering questions such as \"why\" and \"how\" something happened in the video. This project aims to build computational models that can perform well in the above tasks, which require the higher-order inference of any relations beyond the conventional recognition of objects, actions, and attributes in the scene. Here, the higher-order inference refers to an inference that cannot be answered immediately by direct observations and thus requires stronger semantics.&nbsp;</p>\n<p>Throughout the project, we have made substantial progress on instructional video understanding. We created YouCookQA, an annotated question answering (QA) dataset for instructional videos based on our famous YouCook2 dataset. The YouCookQA dataset contains over 15K question-answer pairs, and answering many questions requires modeling complex human-object relations over multiple frames in the video. Observing the lack of effective representations for modeling long videos, we proposed a set of carefully designed neural network models that capture both temporal order and relational information. Furthermore, we incorporated different modalities, including video descriptions and transcripts, into the models to boost the QA performance. The new dataset and our extensive experiments set an important benchmark and provided insights for future work to study this problem.&nbsp;</p>\n<p>A key thrust of our research is to leverage instructional videos to study humans' decision-making processes so that AI can learn new skills just like humans. Therefore, we focused on learning a model to plan goal-directed actions in real-life videos. In contrast to conventional action recognition, goal-directed actions are based on expectations of their outcomes requiring causal knowledge of potential consequences of actions. Thus, integrating the environment structure with goals is critical for solving this task. Previous works learn a single world model will fail to distinguish various tasks, resulting in an ambiguous latent space; planning through it will gradually neglect the desired outcomes since the global information of the future goal degrades quickly as the procedure evolves. We addressed these limitations with a new formulation of procedure planning and proposed novel algorithms to model human behaviors through Bayesian Inference and model-based Imitation Learning. Our method achieved state-of-the-art performance in reaching the indicated goals.&nbsp;</p>\n<p>Furthermore, we advanced numerous fundamental technologies underpinning joint modeling of vision, language, and action. These include weakly-supervised video grounding, fast one-stage visual grounding, video moment localization with language, image/video captioning, scene graph generation, and language-driven image editing.&nbsp;</p>\n<p>For broader impacts, results obtained from this project are of keen interest to many other research communities. For example, assistive robots and security surveillance need robust reasoning to run in real-world scenarios. Researchers in multimedia are interested in indexing and retrieving information from large-scale Internet videos with natural languages. Researchers in data mining and social media analysis are interested in analyzing user behaviors from user-uploaded images and videos. In all of these cases, techniques developed in this project are instrumental.&nbsp;</p>\n<p>This project has provided opportunities for research, teaching, and mentoring in computer science by allowing the PI/Co-PI to work closely with both graduate and undergraduate students involved in the project and teach them research methodologies, scientific writing, and presentation skills. We have trained female and URM students in this project. The results of this project have been featured as course modules in the PI/Co-PI's Machine Vision courses and as interactive demos for a workshop that the PI offered to pre-college students. We have released many of our curated datasets and developed algorithms freely and publicly for use by the research community.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/23/2021<br>\n\t\t\t\t\tModified by: Chenliang&nbsp;Xu</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639363827476_YouCookQA-Example--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639363827476_YouCookQA-Example--rgov-800width.jpg\" title=\"YouCookQA -- A Question-Answering Dataset for Instructional Videos\"><img src=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639363827476_YouCookQA-Example--rgov-66x44.jpg\" alt=\"YouCookQA -- A Question-Answering Dataset for Instructional Videos\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We show examples in YouCookQA. Colored boxes and arrows represent different steps required to answer the given questions. Red boxes denote the first step, blue boxes denote the second, and green arrows are for the final step.</div>\n<div class=\"imageCredit\">Chenliang Xu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Chenliang&nbsp;Xu</div>\n<div class=\"imageTitle\">YouCookQA -- A Question-Answering Dataset for Instructional Videos</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639364585803_YouCookQA-Models--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639364585803_YouCookQA-Models--rgov-800width.jpg\" title=\"Various Models Developed for YouCookQA\"><img src=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639364585803_YouCookQA-Models--rgov-66x44.jpg\" alt=\"Various Models Developed for YouCookQA\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The QA pair for the video in the figure -- Q: \"How many actions involving physical changes to potatoes are done before adding salt?\" A: \"2.\" We show the preprocessing step in (a) and various neural network models in (b)-(d).</div>\n<div class=\"imageCredit\">Chenliang Xu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Chenliang&nbsp;Xu</div>\n<div class=\"imageTitle\">Various Models Developed for YouCookQA</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639364827298_Planning-Problem--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639364827298_Planning-Problem--rgov-800width.jpg\" title=\"Procedure Planning in Instructional Videos\"><img src=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639364827298_Planning-Problem--rgov-66x44.jpg\" alt=\"Procedure Planning in Instructional Videos\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Given a starting observation (picture of food ingredients) and a visual goal (picture of a made cake), the model needs to learn how to complete real-world tasks such as making a cake by planning a sequence of actions (blue circles) and retrieving the intermediate observations (yellow circles).</div>\n<div class=\"imageCredit\">Chenliang Xu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Chenliang&nbsp;Xu</div>\n<div class=\"imageTitle\">Procedure Planning in Instructional Videos</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639365198433_VideoGrounding--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639365198433_VideoGrounding--rgov-800width.jpg\" title=\"Text Grounding in Instructional Videos\"><img src=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1639365198433_VideoGrounding--rgov-66x44.jpg\" alt=\"Text Grounding in Instructional Videos\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We aim to ground textual signals to visual-spatial regions in instructional videos. The figure illustrates two approaches for learning weakly-supervised video grounding. Models need to overcome ambiguities when learning from partial, weak annotations.</div>\n<div class=\"imageCredit\">Chenliang Xu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Chenliang&nbsp;Xu</div>\n<div class=\"imageTitle\">Text Grounding in Instructional Videos</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1640228045223_aaai-20--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1640228045223_aaai-20--rgov-800width.jpg\" title=\"Moment Localization with Natural Language\"><img src=\"/por/images/Reports/POR/2021/1813709/1813709_10569239_1640228045223_aaai-20--rgov-66x44.jpg\" alt=\"Moment Localization with Natural Language\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Multi-Scale 2D Temporal Adjacency Networks for Moment Localization with Natural Language</div>\n<div class=\"imageCredit\">Jiebo Luo</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Jiebo&nbsp;Luo</div>\n<div class=\"imageTitle\">Moment Localization with Natural Language</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nA fundamental capability of human intelligence is learning to act by watching instructional videos. Such power is reflected in abstraction and summarization of the instructional procedures and in answering questions such as \"why\" and \"how\" something happened in the video. This project aims to build computational models that can perform well in the above tasks, which require the higher-order inference of any relations beyond the conventional recognition of objects, actions, and attributes in the scene. Here, the higher-order inference refers to an inference that cannot be answered immediately by direct observations and thus requires stronger semantics. \n\nThroughout the project, we have made substantial progress on instructional video understanding. We created YouCookQA, an annotated question answering (QA) dataset for instructional videos based on our famous YouCook2 dataset. The YouCookQA dataset contains over 15K question-answer pairs, and answering many questions requires modeling complex human-object relations over multiple frames in the video. Observing the lack of effective representations for modeling long videos, we proposed a set of carefully designed neural network models that capture both temporal order and relational information. Furthermore, we incorporated different modalities, including video descriptions and transcripts, into the models to boost the QA performance. The new dataset and our extensive experiments set an important benchmark and provided insights for future work to study this problem. \n\nA key thrust of our research is to leverage instructional videos to study humans' decision-making processes so that AI can learn new skills just like humans. Therefore, we focused on learning a model to plan goal-directed actions in real-life videos. In contrast to conventional action recognition, goal-directed actions are based on expectations of their outcomes requiring causal knowledge of potential consequences of actions. Thus, integrating the environment structure with goals is critical for solving this task. Previous works learn a single world model will fail to distinguish various tasks, resulting in an ambiguous latent space; planning through it will gradually neglect the desired outcomes since the global information of the future goal degrades quickly as the procedure evolves. We addressed these limitations with a new formulation of procedure planning and proposed novel algorithms to model human behaviors through Bayesian Inference and model-based Imitation Learning. Our method achieved state-of-the-art performance in reaching the indicated goals. \n\nFurthermore, we advanced numerous fundamental technologies underpinning joint modeling of vision, language, and action. These include weakly-supervised video grounding, fast one-stage visual grounding, video moment localization with language, image/video captioning, scene graph generation, and language-driven image editing. \n\nFor broader impacts, results obtained from this project are of keen interest to many other research communities. For example, assistive robots and security surveillance need robust reasoning to run in real-world scenarios. Researchers in multimedia are interested in indexing and retrieving information from large-scale Internet videos with natural languages. Researchers in data mining and social media analysis are interested in analyzing user behaviors from user-uploaded images and videos. In all of these cases, techniques developed in this project are instrumental. \n\nThis project has provided opportunities for research, teaching, and mentoring in computer science by allowing the PI/Co-PI to work closely with both graduate and undergraduate students involved in the project and teach them research methodologies, scientific writing, and presentation skills. We have trained female and URM students in this project. The results of this project have been featured as course modules in the PI/Co-PI's Machine Vision courses and as interactive demos for a workshop that the PI offered to pre-college students. We have released many of our curated datasets and developed algorithms freely and publicly for use by the research community. \n\n\t\t\t\t\tLast Modified: 12/23/2021\n\n\t\t\t\t\tSubmitted by: Chenliang Xu"
 }
}