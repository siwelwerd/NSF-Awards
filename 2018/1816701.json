{
 "awd_id": "1816701",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Towards Speech-Driven Multimodal Querying",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-07-31",
 "awd_max_amd_letter_date": "2018-07-31",
 "awd_abstract_narration": "Modern automatic speech recognition (ASR) tools offer near-human accuracy in many scenarios. This has increased the popularity of speech-driven input in many applications on modern device environments such as tablets and smartphones, while also enabling personal conversational assistants. In this context, this project will study a seemingly simple but important fundamental question: how should one design a speech-driven system to query structured data? Structured data querying is ubiquitous in the enterprise, healthcare, and other domains. Typing queries in the Structured Query Language (SQL) is the gold standard for such querying. But typing SQL is painful or impossible in the above environments, which restricts when and how users can consume their data. SQL also has a learning curve. Existing alternatives such as typed natural language interfaces help improve usability but sacrifice query sophistication substantially. For instance, conversational assistants today support queries mainly over curated vendor-specific datasets, not arbitrary database schemas, and they often fail to understand query intent. This has widened the gap with SQL's high query sophistication and unambiguity. This project will bridge this gap by enabling users to interact with structured data using spoken queries over arbitrary database schemas. It will lead to prototype systems on popular tablet, smartphone, and conversational assistant environments. This could help many data professionals such as data analysts, business reporters, and database administrators, as well as non-technical data enthusiasts. For instance, nurse informaticists can retrieve patient details more easily and unambiguously to assist doctors, while analysts can slice and dice their data even on the move. The research will be disseminated as publications in database and natural language processing conferences. The research and artifacts produced will be integrated into graduate and undergraduate courses on database systems. The PIs will continue supporting students from under-represented groups as part of this project.\r\n\r\nThis project will create three new systems for spoken querying at three levels of \"naturalness.\" The first level targets a tractable and meaningful subset of SQL. This research will exploit three powerful properties of SQL that regular English speech lacks--unambiguous context-free grammar, knowledge of the database schema queried, and knowledge of tokens from the database instance queried--to support arbitrary database schemas and tokens not present in the ASR vocabulary. The PIs will synthesize and innovate upon ideas from information retrieval, natural language processing, and database indexing and combine them with human-in-the-loop query correction to improve accuracy and efficiency. The second version will make SQL querying even more natural and stateful by changing its grammar. This will lead to the first speech-oriented dialect of SQL. The third version will apply the lessons from the previous versions to two state-of-the-art typed natural language interfaces for databases. This will lead to a redesign of such interfaces that exploits both the properties of speech and the database instance queried.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Arun",
   "pi_last_name": "Kumar",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Arun K Kumar",
   "pi_email_addr": "arunkk@eng.ucsd.edu",
   "nsf_id": "000732398",
   "pi_start_date": "2018-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Lawrence",
   "pi_last_name": "Saul",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lawrence Saul",
   "pi_email_addr": "saul@cs.ucsd.edu",
   "nsf_id": "000456841",
   "pi_start_date": "2018-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ndapandula",
   "pi_last_name": "Nakashole",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ndapandula Nakashole",
   "pi_email_addr": "nnakashole@eng.ucsd.edu",
   "nsf_id": "000763789",
   "pi_start_date": "2018-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "",
  "perf_city_name": "La Joll",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project studied the principles, methods, and tools of bringing relational database querying into the speech-first era of computing. Relational databases are the most common form of businesss-critical data in practice. Our project findings and artifacts could enable easier anytime-anywhere data retrieval and analytics for both data professionals and lay users on new devices such tablets, smartphones, and conversational assistants. We broke it down into three regimes of ease of use and query sophistication.&nbsp;</p>\n<p>The first regime focused on regular SQL, the most popular query language for relational databases. It was targeted at data professionals such as business analysts, nurse informatics practitioners, and system admins familiar with SQL. We devised the first speech+first multimodal querying interface for a non-trivial subset of SQL. With user studies we found that our system makes it significantly faster for people to specify their queries on tablets compared to typing SQL.</p>\n<p>For the second regime, we created the first speech-oriented dialect of SQL. This too is targeted at data professionals. It builds on our lessons of the rigidity of SQL for dictation from the first project to infuse a small set of focused relaxations to SQL syntax that are easy to pick up for SQL users. With user studies we found that our dialect is significantly easier for querying than speaking regular SQL, while still preserving the exactness and correctness guarantees of SQL.</p>\n<p>For the third regime, we studied the intersection between speech-first querying and natural language interfaces to databases, which convert regular English queries to SQL. This can be useful even for lay users of relational data, e.g., for querying about restaurants or general facts on conversational assistants. We showed the being aware of audio features when translating natural language to SQL can make it more accurate.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/08/2022<br>\n\t\t\t\t\tModified by: Arun&nbsp;K&nbsp;Kumar</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1816701/1816701_10563699_1667930547146_SpeakkQLThreeRegimesSpectrum--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1816701/1816701_10563699_1667930547146_SpeakkQLThreeRegimesSpectrum--rgov-800width.jpg\" title=\"SpeakQL Three Regimes\"><img src=\"/por/images/Reports/POR/2022/1816701/1816701_10563699_1667930547146_SpeakkQLThreeRegimesSpectrum--rgov-66x44.jpg\" alt=\"SpeakQL Three Regimes\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Spectrum of complexity of languages and where the three regimes of SpeakQL fall on it. CFG stands for Context-Free Grammar.</div>\n<div class=\"imageCredit\">From our ACM SIGMOD 2019 Demonstration Paper</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Arun&nbsp;K&nbsp;Kumar</div>\n<div class=\"imageTitle\">SpeakQL Three Regimes</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1816701/1816701_10563699_1667931351947_SpeakQLMultimodalInterface--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1816701/1816701_10563699_1667931351947_SpeakQLMultimodalInterface--rgov-800width.jpg\" title=\"SpeakQL Multimodal Interface\"><img src=\"/por/images/Reports/POR/2022/1816701/1816701_10563699_1667931351947_SpeakQLMultimodalInterface--rgov-66x44.jpg\" alt=\"SpeakQL Multimodal Interface\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Speech+touch oriented multimodal interface for SpeakQL 1.0, including an SQL-specific touch keyboard</div>\n<div class=\"imageCredit\">From our ACM SIGMOD 2020 paper</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Arun&nbsp;K&nbsp;Kumar</div>\n<div class=\"imageTitle\">SpeakQL Multimodal Interface</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project studied the principles, methods, and tools of bringing relational database querying into the speech-first era of computing. Relational databases are the most common form of businesss-critical data in practice. Our project findings and artifacts could enable easier anytime-anywhere data retrieval and analytics for both data professionals and lay users on new devices such tablets, smartphones, and conversational assistants. We broke it down into three regimes of ease of use and query sophistication. \n\nThe first regime focused on regular SQL, the most popular query language for relational databases. It was targeted at data professionals such as business analysts, nurse informatics practitioners, and system admins familiar with SQL. We devised the first speech+first multimodal querying interface for a non-trivial subset of SQL. With user studies we found that our system makes it significantly faster for people to specify their queries on tablets compared to typing SQL.\n\nFor the second regime, we created the first speech-oriented dialect of SQL. This too is targeted at data professionals. It builds on our lessons of the rigidity of SQL for dictation from the first project to infuse a small set of focused relaxations to SQL syntax that are easy to pick up for SQL users. With user studies we found that our dialect is significantly easier for querying than speaking regular SQL, while still preserving the exactness and correctness guarantees of SQL.\n\nFor the third regime, we studied the intersection between speech-first querying and natural language interfaces to databases, which convert regular English queries to SQL. This can be useful even for lay users of relational data, e.g., for querying about restaurants or general facts on conversational assistants. We showed the being aware of audio features when translating natural language to SQL can make it more accurate.\n\n\t\t\t\t\tLast Modified: 11/08/2022\n\n\t\t\t\t\tSubmitted by: Arun K Kumar"
 }
}