{
 "awd_id": "1811308",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Highly Principled Data Science for Multi-Domain Astronomical Measurements and Analysis",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2018-07-15",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 179972.0,
 "awd_amount": 179972.0,
 "awd_min_amd_letter_date": "2018-07-11",
 "awd_max_amd_letter_date": "2018-07-11",
 "awd_abstract_narration": "Massive data resources are coming online in every conceivable area of human exploration, and particularly in fields that are heavily observation-based such as astronomy and astrophysics. To extract the most information from these data, scientists and statisticians need to conduct highly principled data science, by using methods that are scientifically justified, statistically principled, and computationally efficient. This project outlines plans to achieve this goal while addressing four specific challenges in astronomical data involving space, time and energy.  The proposed research has the dual impact of more reliable statistical methods in astronomy and of new general statistical inference and computational methods. In addition to providing methods and free software, the investigators also plan to communicate to the astronomical community the benefit of principled statistical methods through workshops and sessions at conferences. A fundamental impact of the proposed research is the more general acceptance and use of principled methods among astronomers. The general methods for efficient modeling of scientific phenomena, science-driven classification and clustering, and for statistical computing, can also help to solve complex data challenges throughout the natural, social, medical, and engineering sciences.\r\n\r\nStriking advances in both space-based and terrestrial instrumentation continuously increase the quality and quantity of data available to astronomers. Observations are made across the electromagnetic spectrum and compiled into enormous catalogs of high-resolution, but heterogeneous spectrograph, imaging, and time series data. The proposed research aims to use such multi-domain astronomical measurements to better understand the physical environment, structure, and evolution of astronomical individual sources, clusters, and ultimately of the entire universe. There are four major projects.  (1) The PIs will develop methodology to solve the instrument calibration problem, which is a fundamental challenge in astrophysics, by fitting scientifically motivated statistical models to data from multiple astronomical objects observed by multiple instruments. (2) The PIs propose a statistically and computationally efficient algorithm to detect the boundaries of a power law distribution prevalent in various areas of astronomy and of far-reaching importance. (3) The PIs will extend image-processing algorithms designed for detecting point sources to complex extended multi-scale structures via a post-hoc analysis, which makes the computation efficient. (4) With astronomical images exhibiting complex structure, the PIs propose to explore image segmentation methods to distinguish overlapping point sources; the algorithm achieves the flux-conserving property, which is crucial for giving physically meaningful estimates that existing methods lack. These projects all involve significant challenges in developing efficient statistical methods, designing fast computational algorithms, and balancing subtle trade-offs between complexity and practicality. With their extensive and successful track record, the PIs will address these challenges by developing inferential and efficient computational methods under highly-structured models that involve multi-scale structure and/or multiple levels of latent variables. The central theme of the proposed research is the integration and pursuit of three desiderata in each of its four projects: scientific justification, statistical principles, and computational efficiency. This triple-goal advances the development of specifically designed methods that leverage computationally efficient and statistically principled data-driven techniques which explicitly incorporate scientific understanding of the astronomical sources.  This ensures that the statistical analyses enhance the scientists' ability to answer specific questions about the underlying astronomical and physical processes. This strategy requires state-of-the-art statistical inference, sophisticated scientific computing, and careful model-checking procedures, all of which have been the hallmark of the work by this team of investigators.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiao-Li",
   "pi_last_name": "Meng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiao-Li Meng",
   "pi_email_addr": "meng@stat.harvard.edu",
   "nsf_id": "000107265",
   "pi_start_date": "2018-07-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "One Oxford Street",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382901",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 179972.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-48d9f30c-7fff-69af-5023-2bc31ecd4587\">\n<p dir=\"ltr\"><span>With advances in both astrophysics and data science, there is an ever-increased desire to extract as much information from astronomical data and fit scientific models as reliably as possible. In doing so, however, there is sometimes a mix-up between improving computational/numerical accuracy and enhancing statistical power and efficiency, leading to counter-productive or even misleading results. Principled and rigorous statistical approaches therefore are fundamental for producing results that will advance our understanding of the universe in real terms.&nbsp; This is especially important with the steady increase in quality and quantity of observational data, which provide astronomers with an exceptional opportunity to view the Universe. But to most effectively leverage such opportunities require advanced data analysis methods, because the standard approaches are often insufficient for robust scientific inferences.&nbsp;</span></p>\n<p dir=\"ltr\"><span>For example, many phenomena in the high-energy universe are time-variable, from coronal flares on the smallest stars to accretion events in the most massive black holes. Sometimes this variability can just be seen \"by-eye,\" but at other times, we need to use robust methods found in statistics to distinguish random noise from intrinsic variability. Realizing where the change has occurred is critical for subsequent scientific analyses, e.g., spectral fitting and light curve modeling. Such analyses must focus on those intervals in data space that are properly tied to the changes in the physical processes that generate the observed photons. Therefore, it is of crucial importance to identify sources as well as to locate their spatial boundaries and time points where changes occur (e.g., the start and end times of a flare)&nbsp; A major contribution of this project is the development of several&nbsp; automatic methods that detect those sudden spatial and/or temporal changes that happened during the underlying astrophysical process. A similar contribution was to the identification of the regions (and of the exponent parameter) where the power law takes place with respect to the energy release during solar flares.</span></p>\n</span></p>\n<p><span id=\"docs-internal-guid-5501d7c0-7fff-b080-9ddd-a6791543cb6b\">\n<p dir=\"ltr\"><span>Another challenge often encountered in analyzing high-energy astronomical data is that the images are photon starved and sparse and contain many \"empty\" pixels.&nbsp; Unlike photon-rich images encountered at longer wavelengths, complex features in X-ray and gamma-ray data are difficult to recognize, characterize, and analyze. Working directly with photon counts, while simultaneously separating the contribution of the background, is a difficult process, especially when trying to detect faint non-uniform diffuse emission or separating faint point sources from larger scale emission. Finding the boundaries of such extended structures is thus a challenging problem. Another significant contribution of this project is the proposal of two&nbsp; novel approaches designed to tackle this problem. One approach is non-parametric in nature and computationally highly efficient. The other is Bayesian and offers a principled quantification of the uncertainty in the estimated boundaries of the large scale diffuse emission.&nbsp;&nbsp;</span></p>\n</span></p>\n<p><span id=\"docs-internal-guid-37642a51-7fff-873c-b019-426b10778699\"><span>Yet another example is to deal with the discrepancies among many observations about the same astronomical source caused by different instruments, observational perspectives, data processing, etc, and to reach a scientifically sound consensus. Traditionally this is done mostly by ad hoc adjustments that may work reasonably well in isolation, but they do not provide sufficient confidence in their statistical and scientific validity.&nbsp; The principled and unified </span><span>concordance</span><span> model developed in this proposal is the first of its kind for calibration concordance in the astrophysics community and has received much attention, both because it is developed from well-understood and tested Bayesian methods, and the resulting unified answer is scientifically and statistically easily interpretable.&nbsp; </span></span></p>\n<p><span id=\"docs-internal-guid-48d9f30c-7fff-69af-5023-2bc31ecd4587\">\n<div><span><br /></span></div>\n</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/13/2023<br>\n\t\t\t\t\tModified by: Xiao-Li&nbsp;Meng</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nWith advances in both astrophysics and data science, there is an ever-increased desire to extract as much information from astronomical data and fit scientific models as reliably as possible. In doing so, however, there is sometimes a mix-up between improving computational/numerical accuracy and enhancing statistical power and efficiency, leading to counter-productive or even misleading results. Principled and rigorous statistical approaches therefore are fundamental for producing results that will advance our understanding of the universe in real terms.  This is especially important with the steady increase in quality and quantity of observational data, which provide astronomers with an exceptional opportunity to view the Universe. But to most effectively leverage such opportunities require advanced data analysis methods, because the standard approaches are often insufficient for robust scientific inferences. \nFor example, many phenomena in the high-energy universe are time-variable, from coronal flares on the smallest stars to accretion events in the most massive black holes. Sometimes this variability can just be seen \"by-eye,\" but at other times, we need to use robust methods found in statistics to distinguish random noise from intrinsic variability. Realizing where the change has occurred is critical for subsequent scientific analyses, e.g., spectral fitting and light curve modeling. Such analyses must focus on those intervals in data space that are properly tied to the changes in the physical processes that generate the observed photons. Therefore, it is of crucial importance to identify sources as well as to locate their spatial boundaries and time points where changes occur (e.g., the start and end times of a flare)  A major contribution of this project is the development of several  automatic methods that detect those sudden spatial and/or temporal changes that happened during the underlying astrophysical process. A similar contribution was to the identification of the regions (and of the exponent parameter) where the power law takes place with respect to the energy release during solar flares.\n\n\n\nAnother challenge often encountered in analyzing high-energy astronomical data is that the images are photon starved and sparse and contain many \"empty\" pixels.  Unlike photon-rich images encountered at longer wavelengths, complex features in X-ray and gamma-ray data are difficult to recognize, characterize, and analyze. Working directly with photon counts, while simultaneously separating the contribution of the background, is a difficult process, especially when trying to detect faint non-uniform diffuse emission or separating faint point sources from larger scale emission. Finding the boundaries of such extended structures is thus a challenging problem. Another significant contribution of this project is the proposal of two  novel approaches designed to tackle this problem. One approach is non-parametric in nature and computationally highly efficient. The other is Bayesian and offers a principled quantification of the uncertainty in the estimated boundaries of the large scale diffuse emission.  \n\n\nYet another example is to deal with the discrepancies among many observations about the same astronomical source caused by different instruments, observational perspectives, data processing, etc, and to reach a scientifically sound consensus. Traditionally this is done mostly by ad hoc adjustments that may work reasonably well in isolation, but they do not provide sufficient confidence in their statistical and scientific validity.  The principled and unified concordance model developed in this proposal is the first of its kind for calibration concordance in the astrophysics community and has received much attention, both because it is developed from well-understood and tested Bayesian methods, and the resulting unified answer is scientifically and statistically easily interpretable.  \n\n\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 01/13/2023\n\n\t\t\t\t\tSubmitted by: Xiao-Li Meng"
 }
}