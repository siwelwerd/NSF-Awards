{
 "awd_id": "1763486",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Scalable Integration of Data-Driven and Model-Based Methods for Large Vocabulary Sign Recognition and Search",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 300023.0,
 "awd_amount": 300023.0,
 "awd_min_amd_letter_date": "2018-07-21",
 "awd_max_amd_letter_date": "2018-07-21",
 "awd_abstract_narration": "It is surprisingly difficult to look up an unfamiliar sign in American Sign Language (ASL). Most ASL dictionaries list signs in alphabetical order based on approximate English translations, so a user who does not understand a sign or know its English translation would not know how to find it. ASL lacks a written form or intuitive \"alphabetical sorting\" based on such a writing system. Although some dictionaries make available alternative ways to search for a sign, based on explicit specification of various properties, a user must often still look through hundreds of pictures of signs to find a match to the unfamiliar sign (if it is present at all in that dictionary). This research will create a framework that will enable the development of a user-friendly, video-based sign-lookup interface, for use with online ASL video dictionaries and resources, and for facilitation of ASL annotation.  Input will consist of either a webcam recording of a sign by the user, or user identification of the start and end frames of a sign from a digital video. To test the efficacy of the new tools in real-world applications, the team will partner with the leading producer of pedagogical materials for ASL instruction in high schools and colleges, which is developing the first multimedia ASL dictionary with video-based ASL definitions for signs. The lookup interface will be used experimentally to search the ASL dictionary in ASL classes at Boston University and RIT. Project outcomes will revolutionize how deaf children, students learning ASL, or families with deaf children search ASL dictionaries. They will accelerate research on ASL linguistics and technology, by increasing efficiency, accuracy, and consistency of annotations of ASL videos through video-based sign lookup. And they will lay the groundwork for future technologies to benefit deaf users, such as search by video example through ASL video collections, or ASL-to-English translation, for which sign-recognition is a precursor. The new linguistically annotated video data and software tools will be shared publicly, for use by others in linguistic and computer science research, as well as in education. \r\n\r\nSign recognition from video is still an open and difficult problem because of the nonlinearities involved in recognizing 3D structures from 2D video, and the complex linguistic organization of sign languages. The linguistic parameters relevant to sign production and discrimination include hand configuration and orientation, location relative to the body or in signing space, movement trajectory, and in some cases, facial expressions/head movements. An additional complication is that signs belonging to different classes have distinct internal structures, and are thus subject to different linguistic constraints and require distinct recognition strategies; yet prior research has generally failed to address these distinctions. The challenges are compounded by inter- and intra- signer variations, and, in continuous signing, by co-articulation effects (i.e., influence from adjacent signs) with respect to several of the above parameters. Purely data-driven approaches are ill-suited to sign recognition given the limited quantities of available, consistently annotated data and the complexity of the linguistic structures involved, which are hard to infer. Prior research has, for this reason, generally focused on selected aspects of the problem, often restricting the work to a limited vocabulary, and therefore resulting in methods that are not scalable. More importantly, few if any methods involve 4D (spatio-temporal) modeling and attention to the linguistic properties of specific types of signs. A new approach to computer-based recognition of ASL from video is needed. In this research, the approach will be to build a new hybrid, scalable, computational framework for sign identification from a large vocabulary, which has never before been achieved. This research will strategically combine state-of-the-art computer vision, machine-learning methods, and linguistic modeling. It will leverage the team's existing publicly shared ASL corpora and Sign Bank - linguistically annotated and categorized video recordings produced by native signers - which will be augmented to meet the requirements of this project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Carol",
   "pi_last_name": "Neidle",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Carol J Neidle",
   "pi_email_addr": "carol@bu.edu",
   "nsf_id": "000197237",
   "pi_start_date": "2018-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Boston University",
  "perf_str_addr": "621 Commonwealth Avenue",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151605",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 300023.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>It is surprisingly difficult to look up an unfamiliar sign in American Sign Language (ASL). Most ASL dictionaries list signs in alphabetical order based on approximate English translations, so a user who does not understand a sign or know its English translation would have no way to find it. ASL lacks a written form or intuitive \"alphabetical sorting\". &nbsp;Although some dictionaries offer alternative ways to search for a sign, based on explicit specification of various properties, a user must often still look through hundreds of pictures of signs to find the sign of interest, if it's present at all in that dictionary.&nbsp;</p>\n<p>We have developed a prototype for&nbsp;user-friendly search by video example for access to online ASL video dictionaries and other digital resources, and for facilitation of ASL annotation. A key component is a new machine-learning framework for sign recognition that we have developed. Input consists of either a webcam recording of a sign or a clip from a digital video.&nbsp;The system then uses machine learning to determine the top 5 most probable signs; the user makes the final selection.&nbsp;User studies to guide the development of the lookup interface have been conducted at RIT.&nbsp;</p>\n<p>Our new hybrid, scalable, computational framework for sign identification from a large vocabulary is based on Graph Convolutional Networks (GCNs). This research strategically combines state-of-the-art computer vision, machine-learning methods, and linguistic modeling.&nbsp;The linguistic parameters relevant to sign production and discrimination include hand configuration and orientation, location relative to the body or in signing space, movement trajectory, and in some cases, facial expressions/head movements. An additional complication is that signs of different classes have distinct internal structures, and are thus subject to different linguistic constraints and require distinct recognition strategies; yet prior research has generally failed to address these distinctions. The challenges are compounded by inter- and intra- signer variations, and, in continuous signing, by coarticulation effects (i.e., influence from adjacent signs) with respect to the above parameters. Purely data-driven approaches are ill-suited to sign recognition given the limited quantities of available, consistently annotated data and the complexity of the linguistic structures involved, which are hard to infer. Prior research has, for this reason, generally focused on selected aspects of the problem, often restricting the work to a limited vocabulary, therefore resulting in methods that are not scalable.&nbsp;</p>\n<p>We use a skeleton model to extract parameters known to be important linguistically, including the location of the arms, hands, and wrists in relation to locations on the body. This is important because movement patterns, as well as the relative location and orientation of the hands with respect to the body, are significant for sign identification. We also developed a new approach to estimate the 2D handshape from video, and to recognize the initial and final hand configurations, as well as the hand orientation from wrist movements; this is important for recognition of the linguistically significant aspects of sign production.</p>\n<p>This research leveraged the team's publicly shared ASL corpora and Sign Bank -- linguistically annotated and categorized video recordings of Deaf signers -- which were augmented through a substantial amount of new video data collected at RIT and through videos contributed by DawnSignPress. These new video files were annotated at Boston University using SignStream(R), our software for linguistic annotation of visual language data.&nbsp;</p>\n<p>Our new linguistically annotated video data and our enhanced software tools are shared publicly, for use by others in linguistic and computer science research, as well as in education.&nbsp;</p>\n<p>In&nbsp;our most recent experiments, we trained our model on video clips for 2,268 signs from a large combined video dataset (<span>40,255 sign videos in all)</span>&nbsp;for ASL. In addition to our own datasets, we also used the Word-Level American Sign Language (WLASL) dataset, but with modified gloss labeling -- to ensure both a 1-1 correspondence between gloss labels and distinct sign productions (to remedy a serious problem with inconsistencies in the WLASL annotations); and consistency in gloss labeling across the WLASL and our American Sign Language Linguistic Research Project (ASLLRP) datasets.&nbsp;</p>\n<p>The recognition accuracy for&nbsp;top-1&nbsp;was 76.01%&nbsp;and&nbsp;top-5,&nbsp;93.38%, which demonstrates the effectiveness and scalability of our method. This also makes it practical to exploit sign recognition for various valuable applications, including search by video example.</p>\n<p>This research paves the way for applications that can revolutionize how deaf children, students learning ASL, or families with deaf children can search ASL dictionaries and other digital resources. This will also enable development of tools that will accelerate research on ASL linguistics and technology, by increasing efficiency, accuracy, and consistency of annotations of ASL videos through video-based sign lookup. This research also lays the groundwork for future technologies to benefit deaf users, such as search by video example through ASL video collections, or ASL-to-English translation, for which sign-recognition is a precursor. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/21/2022<br>\n\t\t\t\t\tModified by: Carol&nbsp;J&nbsp;Neidle</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668879990256_LREC-part1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668879990256_LREC-part1--rgov-800width.jpg\" title=\"Sign recognition method\"><img src=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668879990256_LREC-part1--rgov-66x44.jpg\" alt=\"Sign recognition method\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Overview of our method for sign recognition, taken from poster for LREC 2022</div>\n<div class=\"imageCredit\">American Sign Language Linguistic Research Project</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Carol&nbsp;J&nbsp;Neidle</div>\n<div class=\"imageTitle\">Sign recognition method</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668880155332_LREC-part2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668880155332_LREC-part2--rgov-800width.jpg\" title=\"Sign recognition accuracy achieved\"><img src=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668880155332_LREC-part2--rgov-66x44.jpg\" alt=\"Sign recognition accuracy achieved\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Results reported by Dafnis et al. (LREC, 2022) for sign recognition</div>\n<div class=\"imageCredit\">American Sign Language Linguistic Research Project</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Carol&nbsp;J&nbsp;Neidle</div>\n<div class=\"imageTitle\">Sign recognition accuracy achieved</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668880497722_stats-new--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668880497722_stats-new--rgov-800width.jpg\" title=\"Some statistics for shared data\"><img src=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668880497722_stats-new--rgov-66x44.jpg\" alt=\"Some statistics for shared data\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Some statistics for the data shared online by the American Sign Language Linguistic Research Project</div>\n<div class=\"imageCredit\">American Sign Language Linguistic Research Project</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Carol&nbsp;J&nbsp;Neidle</div>\n<div class=\"imageTitle\">Some statistics for shared data</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668954467752_Prototype-1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668954467752_Prototype-1--rgov-800width.jpg\" title=\"Screen shot 1 from Prototype\"><img src=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668954467752_Prototype-1--rgov-66x44.jpg\" alt=\"Screen shot 1 from Prototype\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Screen shot 1 from Search by Video Example Prototype</div>\n<div class=\"imageCredit\">American Sign Language Linguistic Research Project</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Carol&nbsp;J&nbsp;Neidle</div>\n<div class=\"imageTitle\">Screen shot 1 from Prototype</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668954563717_Prototype-2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668954563717_Prototype-2--rgov-800width.jpg\" title=\"Screen shot 2 from Prototype\"><img src=\"/por/images/Reports/POR/2022/1763486/1763486_10560043_1668954563717_Prototype-2--rgov-66x44.jpg\" alt=\"Screen shot 2 from Prototype\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Screen shot 2 from Search by Video Example Prototype: Option to view sign variants, if there are variants, prior to making a selection</div>\n<div class=\"imageCredit\">American Sign Language Linguistic Research Project</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Carol&nbsp;J&nbsp;Neidle</div>\n<div class=\"imageTitle\">Screen shot 2 from Prototype</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIt is surprisingly difficult to look up an unfamiliar sign in American Sign Language (ASL). Most ASL dictionaries list signs in alphabetical order based on approximate English translations, so a user who does not understand a sign or know its English translation would have no way to find it. ASL lacks a written form or intuitive \"alphabetical sorting\".  Although some dictionaries offer alternative ways to search for a sign, based on explicit specification of various properties, a user must often still look through hundreds of pictures of signs to find the sign of interest, if it's present at all in that dictionary. \n\nWe have developed a prototype for user-friendly search by video example for access to online ASL video dictionaries and other digital resources, and for facilitation of ASL annotation. A key component is a new machine-learning framework for sign recognition that we have developed. Input consists of either a webcam recording of a sign or a clip from a digital video. The system then uses machine learning to determine the top 5 most probable signs; the user makes the final selection. User studies to guide the development of the lookup interface have been conducted at RIT. \n\nOur new hybrid, scalable, computational framework for sign identification from a large vocabulary is based on Graph Convolutional Networks (GCNs). This research strategically combines state-of-the-art computer vision, machine-learning methods, and linguistic modeling. The linguistic parameters relevant to sign production and discrimination include hand configuration and orientation, location relative to the body or in signing space, movement trajectory, and in some cases, facial expressions/head movements. An additional complication is that signs of different classes have distinct internal structures, and are thus subject to different linguistic constraints and require distinct recognition strategies; yet prior research has generally failed to address these distinctions. The challenges are compounded by inter- and intra- signer variations, and, in continuous signing, by coarticulation effects (i.e., influence from adjacent signs) with respect to the above parameters. Purely data-driven approaches are ill-suited to sign recognition given the limited quantities of available, consistently annotated data and the complexity of the linguistic structures involved, which are hard to infer. Prior research has, for this reason, generally focused on selected aspects of the problem, often restricting the work to a limited vocabulary, therefore resulting in methods that are not scalable. \n\nWe use a skeleton model to extract parameters known to be important linguistically, including the location of the arms, hands, and wrists in relation to locations on the body. This is important because movement patterns, as well as the relative location and orientation of the hands with respect to the body, are significant for sign identification. We also developed a new approach to estimate the 2D handshape from video, and to recognize the initial and final hand configurations, as well as the hand orientation from wrist movements; this is important for recognition of the linguistically significant aspects of sign production.\n\nThis research leveraged the team's publicly shared ASL corpora and Sign Bank -- linguistically annotated and categorized video recordings of Deaf signers -- which were augmented through a substantial amount of new video data collected at RIT and through videos contributed by DawnSignPress. These new video files were annotated at Boston University using SignStream(R), our software for linguistic annotation of visual language data. \n\nOur new linguistically annotated video data and our enhanced software tools are shared publicly, for use by others in linguistic and computer science research, as well as in education. \n\nIn our most recent experiments, we trained our model on video clips for 2,268 signs from a large combined video dataset (40,255 sign videos in all) for ASL. In addition to our own datasets, we also used the Word-Level American Sign Language (WLASL) dataset, but with modified gloss labeling -- to ensure both a 1-1 correspondence between gloss labels and distinct sign productions (to remedy a serious problem with inconsistencies in the WLASL annotations); and consistency in gloss labeling across the WLASL and our American Sign Language Linguistic Research Project (ASLLRP) datasets. \n\nThe recognition accuracy for top-1 was 76.01% and top-5, 93.38%, which demonstrates the effectiveness and scalability of our method. This also makes it practical to exploit sign recognition for various valuable applications, including search by video example.\n\nThis research paves the way for applications that can revolutionize how deaf children, students learning ASL, or families with deaf children can search ASL dictionaries and other digital resources. This will also enable development of tools that will accelerate research on ASL linguistics and technology, by increasing efficiency, accuracy, and consistency of annotations of ASL videos through video-based sign lookup. This research also lays the groundwork for future technologies to benefit deaf users, such as search by video example through ASL video collections, or ASL-to-English translation, for which sign-recognition is a precursor.  \n\n \n\n\t\t\t\t\tLast Modified: 11/21/2022\n\n\t\t\t\t\tSubmitted by: Carol J Neidle"
 }
}