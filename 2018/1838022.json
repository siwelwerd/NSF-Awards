{
 "awd_id": "1838022",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: IA: Collaborative Research: Protecting Yourself from Wildfire Smoke: Big Data-Driven Adaptive Air Quality Prediction Methodologies",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2019-01-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 298323.0,
 "awd_amount": 357987.0,
 "awd_min_amd_letter_date": "2018-09-07",
 "awd_max_amd_letter_date": "2019-08-27",
 "awd_abstract_narration": "The objective of this project is to develop a framework to achieve real-time smoke transport prediction and air quality forecasting.  Wildfire smoke can transport very fast and pose significant health hazards to communities.  State-of-the-art smoke forecasting models typically have infrequent updates and provide predictions with a coarse spatial resolution due to spatiotemporal resolution limitations of input data and the tremendous computational power required to simulate atmospheric conditions.  This project will develop real-time smoke transport and air quality prediction methodologies with better spatial resolution for improving the scalability and efficiency of the underlying data processing system to enable timely air quality alerts.  While this project is applied towards smoke transport and air quality prediction, this work can be generalized to solve many other big data problems that require such design. The principal investigators will use the materials and topics from this project to enhance education by creating new big data analytics related courses and developing a Big Data Minor program at the University of Nevada, Reno.  The project will also provide opportunities to engage more students from underrepresented groups and impact the education of several students, via K-12 outreach and mentoring undergraduate and graduate students.\r\n\r\nThe intellectual merit of this research is in establishing a novel big data driven air quality prediction for wildfire smoke to provide timely and effective health alerts. The planned new prediction methodology will integrate the novel Gaussian Markov Random Field based real-time spatiotemporal prediction with statistical-based long-term spatiotemporal prediction. To tackle the challenge of missing high-resolution data, a data fusion methodology is planned to integrate fine-grained image data collected from camera networks with air pollution monitoring data to increase data resolution. A Deep Neural Network based smoke density detection process will extract air quality information from camera image data. The planned novel signature time-series based prediction methodology will open opportunities to process larger amounts of spatiotemporal data using limited resources. By identifying critical data based on spatiotemporal properties, the project will develop a communication framework that enables efficient camera data transfer. Efficient parallel and distributed data processing is utterly important to support processing large scale data in real time. The planned decomposition-based parallelization methodology and a performance model driven scheduling framework will enable efficient dynamic computing resource management, which is key to the success of this project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Evgenia",
   "pi_last_name": "Smirni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Evgenia Smirni",
   "pi_email_addr": "esmirni@cs.wm.edu",
   "nsf_id": "000346140",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "College of William and Mary",
  "inst_street_address": "1314 S MOUNT VERNON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "WILLIAMSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "7572213965",
  "inst_zip_code": "23185",
  "inst_country_name": "United States",
  "cong_dist_code": "08",
  "st_cong_dist_code": "VA08",
  "org_lgl_bus_name": "COLLEGE OF WILLIAM AND MARY",
  "org_prnt_uei_num": "EVWJPCY6AD97",
  "org_uei_num": "EVWJPCY6AD97"
 },
 "perf_inst": {
  "perf_inst_name": "College of William and Mary",
  "perf_str_addr": "",
  "perf_city_name": "Williamsburg",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "231878795",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "VA01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 298323.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 59664.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Serverless computing is a new cloud paradigm that is widely used in big data analysis, Internet of Things, and machine learning (ML) tasks including model training and ML inference serving. Serverless simplifies application management. Its autoscaling properties coupled with its pay-as-you-go cost model makes it an effective solution, especially for bursty workloads.&nbsp;Efficient usage of serverless requires proper parameterization. For example on Amazon Web Services Lambda, the user needs to explicitly request the memory size as the application performance and associated monetary cost are determined as a function of allocated memory size. In this project we devised automatic methods to optimize how to serve&nbsp;inference requests (for example, voice recognition requests or image recognition) on serverless that can optimize the cost without comprimising on serving performance. The optimization that we proposed takes advantage of the inherent parallelism of machine learning serving and reduces the monetary cost of serverless by bundling several inference requests to serve them together as a batch.&nbsp;For batching to be effective, especially if inference requests arrive in bursts into the system, &nbsp;three parameters need to be optimized: the memory size, the size of the batch, and the timeout (i.e., when to stop accumulating requests to bundle together to avoid unacceptable delays). We have developed new, agile analytical models that facilitate this optimization process. Our implementation on Amazon Lambda illustrates clear advantages versus the native Amazon state-of-the-practice methods.</p>\n<p>In addition, because ML serving occurs in data centers, we analyzed the \"aging\" of their hardware components and especially of their storage devices. We have developed and analyzed prediction models that can predict failures of storage components in the Google file systems. This work offers a holistic reliability and performance view of such systems.</p><br>\n<p>\n Last Modified: 02/19/2024<br>\nModified by: Evgenia&nbsp;Smirni</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nServerless computing is a new cloud paradigm that is widely used in big data analysis, Internet of Things, and machine learning (ML) tasks including model training and ML inference serving. Serverless simplifies application management. Its autoscaling properties coupled with its pay-as-you-go cost model makes it an effective solution, especially for bursty workloads.Efficient usage of serverless requires proper parameterization. For example on Amazon Web Services Lambda, the user needs to explicitly request the memory size as the application performance and associated monetary cost are determined as a function of allocated memory size. In this project we devised automatic methods to optimize how to serveinference requests (for example, voice recognition requests or image recognition) on serverless that can optimize the cost without comprimising on serving performance. The optimization that we proposed takes advantage of the inherent parallelism of machine learning serving and reduces the monetary cost of serverless by bundling several inference requests to serve them together as a batch.For batching to be effective, especially if inference requests arrive in bursts into the system, three parameters need to be optimized: the memory size, the size of the batch, and the timeout (i.e., when to stop accumulating requests to bundle together to avoid unacceptable delays). We have developed new, agile analytical models that facilitate this optimization process. Our implementation on Amazon Lambda illustrates clear advantages versus the native Amazon state-of-the-practice methods.\n\n\nIn addition, because ML serving occurs in data centers, we analyzed the \"aging\" of their hardware components and especially of their storage devices. We have developed and analyzed prediction models that can predict failures of storage components in the Google file systems. This work offers a holistic reliability and performance view of such systems.\t\t\t\t\tLast Modified: 02/19/2024\n\n\t\t\t\t\tSubmitted by: EvgeniaSmirni\n"
 }
}