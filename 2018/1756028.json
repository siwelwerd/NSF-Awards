{
 "awd_id": "1756028",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CHS: Humanizing Algorithms: Empirical and Design Investigations of Sensitive Algorithmic Encounters",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2021-04-30",
 "tot_intn_awd_amt": 174954.0,
 "awd_amount": 183654.0,
 "awd_min_amd_letter_date": "2018-03-09",
 "awd_max_amd_letter_date": "2020-04-29",
 "awd_abstract_narration": "This project will study algorithmic interactions and develop strategies for the human-centered design of systems that incorporate algorithms and their underlying data.  Software developers of platforms of all kinds are creating features that make use of algorithmically curated content that leverage data about people's relationships, behavior, and identities. However, algorithms usually make decisions based on system metrics that are readily calculable, such as the number of likes, plays, and clicks. Even more sophisticated algorithms are limited by the social information explicitly given or inferred from provided data. As a result, algorithms can fail to capture the social context and human meaning that is important to the acceptability and success of the interactions these algorithms are meant to support.  The research will investigate both algorithmic and human understandings of social data, especially when they diverge. By attending to divergence, the research can examine human expectations of algorithms, how misunderstandings might be reframed, and how subsequent action is informed by those divergences. \r\n\r\nSpecifically, this project will identify (1) how people navigate sensitive algorithmic encounters; (2) how these encounters impact people; (3) what social concepts algorithms are failing to understand; and (4) what design strategies are needed to address sensitive content in algorithmic curation. To focus this work, the specific context of inquiry will be algorithmic encounters with content related to loss of life, given its prevalence and sensitivity at both communal and individual levels.  The broader impacts of the work include: (1) developing guidelines around the curation of and interactions with social data related to loss of life, which can also be applied to other groups and experiences where algorithms should be sensitive; (2) demonstrating how designs that incorporate social data can adopt human-centered approaches to sensitize encounters with algorithmically curated content; (3) contributing to the development of design practices that encompass the design of interactions, systems, algorithms, and data; and (4) engaging students in multiple fields, including Information Science, Computer Science, Media Studies, and Communication through research and curricular activities focused on human-centered approaches to studying and designing social algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jed",
   "pi_last_name": "Brubaker",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Jed R Brubaker",
   "pi_email_addr": "jed.brubaker@colorado.edu",
   "nsf_id": "000707530",
   "pi_start_date": "2018-03-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Colorado at Boulder",
  "inst_street_address": "3100 MARINE ST",
  "inst_street_address_2": "STE 481 572 UCB",
  "inst_city_name": "Boulder",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3034926221",
  "inst_zip_code": "803090001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "THE REGENTS OF THE UNIVERSITY OF COLORADO",
  "org_prnt_uei_num": "",
  "org_uei_num": "SPVKK1RC2MZ3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Colorado at Boulder",
  "perf_str_addr": "",
  "perf_city_name": "Boulder",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "803031058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 174954.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 8700.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-d6c08028-7fff-bd14-e5e1-6493bb432088\"> </span></p>\n<p class=\"p2\">This research focused on emotionally-laden experiences with algorithmically curated content. Across six studies, we investigated the interplay of human and computational understandings of data with a focus on when these understandings breakdown. The major goals were (1) to understand how people navigate sensitive algorithmic encounters; (2) how these encounters impact people; and (3) what social concepts algorithms are failing to understand.</p>\n<p class=\"p2\">Our studies fall into two different focus areas:</p>\n<p class=\"p4\"><strong>Sensitive Algorithmic Encounters</strong></p>\n<p class=\"p2\"><em>Encounters with Ex Romantic Partners:&nbsp;</em>We conducted a study of upsetting algorithmic encounters on social computing systems focused on encounters with ex-romantic partners. We found that upsetting encounters were exacerbated by the complex social networks and data that enable inferred connections around otherwise explicit (and often explicitly terminated) relationships -- a concept we termed as ?data peripheries.? We show how designing for peripheries can allow technologists to conceptualize differences between explicit and inferred connections, and how to design agency into systems based on each. Next, we conducted a study focused on encounters with digital objects and relationships following a break-up. We found that people took different actions towards their possessions and connections in service of creating a post-break-up identity. We found that existing tools are ill-designed to support competing desires to present authentic past and future online identities and produced design suggestions to address this tension.<em>&nbsp;</em></p>\n<p class=\"p3\"><em>Encounters through Technologically Mediated Reminiscence (TMR) Tools:&nbsp;</em>Recommender systems sometimes inadvertently curate content that users may find emotionally intense, such as a picture of an ex-partner or of a now-deceased family member. To explore this issue, we examined Facebook's Memories feature, a Technology-Mediated Reflection (TMR) system that shows users content about their pasts in order to prompt reflection. We interviewed 20 people who had recently seen sensitive curated content through this suite of features. We found that they wanted to see ?bittersweet? content, but they preferred to see it when it was expected and viewed in a context they felt was appropriate, and when they were able to make sense of why the recommender system curated the content. We recommend that designers engage in three practices to meet users? needs: (1) draw inspiration from no/low-technology artifacts, (2) use empirical research to identify which contextual features have negative impacts on users, and (3) conduct user studies to determine how users are doing sense-making around the perceived affect of recommender systems.</p>\n<p class=\"p3\"><em>Everyday Evaluations of Social Media Metrics:&nbsp;</em>The Like button is simultaneously a means of social interaction and a tool to evaluate social media content. Based on in-depth interviews with 25 artists who use Instagram, we identified three overlapping orientations to the Like button: affective, relational, and infrastructural. We found that the flexibility of the button creates ambiguity around the meaning of a Like that incentivizes an economic approach to evaluation that crowds out other value schemas, shaping how artists use the platform, make art, and even understand themselves.</p>\n<p class=\"p3\"><strong>Algorithmic Infrastructure and Bias</strong></p>\n<p class=\"p3\">AI systems have been critiqued for designs that cannot capture the nuance of human identity. However, there was scant empirical work at the time. To address this gap, we conducted a study of how gender is operationalized into commercial facial analysis services. We conducted a two-phrase study: (1) a system analysis of ten commercial facial analysis and image labeling services and (2) an evaluation of five services using a custom dataset of diverse genders using social media images with gender labels provided by their creators. We found that services performed consistently worse on transgender individuals and were universally unable to classify non-binary genders. We found that bias is the result of more than just engineering practices and training data. We identified bias in how gender is codified into the data standards that surround (and typically pre-date) classifiers. We demonstrate how the cloud based services we studied provide black-boxed infrastructures to many third-party developers that can include subtle and invisible forms of bias.&nbsp;</p>\n<p class=\"p3\">Next, we investigated race and gender bias in computer vision by examining the databases with which models are built. Race and gender have long sociopolitical histories of classification in technical infrastructures?from the passport to social media. In this study, we specifically focused on how race and gender are defined and annotated in image databases used for facial analysis. We found that the majority of databases rarely contain underlying source material for how those identities are defined. Further, when they are annotated with race and gender information, database authors rarely describe the process of annotation. In our publication we discuss the limitations of these approaches and argue that the lack of critical engagement renders databases opaque and less trustworthy. Our work encourages and provides guidance for database authors to address both the histories of classification inherently embedded into race and gender.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/29/2021<br>\n\t\t\t\t\tModified by: Jed&nbsp;R&nbsp;Brubaker</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis research focused on emotionally-laden experiences with algorithmically curated content. Across six studies, we investigated the interplay of human and computational understandings of data with a focus on when these understandings breakdown. The major goals were (1) to understand how people navigate sensitive algorithmic encounters; (2) how these encounters impact people; and (3) what social concepts algorithms are failing to understand.\nOur studies fall into two different focus areas:\nSensitive Algorithmic Encounters\nEncounters with Ex Romantic Partners: We conducted a study of upsetting algorithmic encounters on social computing systems focused on encounters with ex-romantic partners. We found that upsetting encounters were exacerbated by the complex social networks and data that enable inferred connections around otherwise explicit (and often explicitly terminated) relationships -- a concept we termed as ?data peripheries.? We show how designing for peripheries can allow technologists to conceptualize differences between explicit and inferred connections, and how to design agency into systems based on each. Next, we conducted a study focused on encounters with digital objects and relationships following a break-up. We found that people took different actions towards their possessions and connections in service of creating a post-break-up identity. We found that existing tools are ill-designed to support competing desires to present authentic past and future online identities and produced design suggestions to address this tension. \nEncounters through Technologically Mediated Reminiscence (TMR) Tools: Recommender systems sometimes inadvertently curate content that users may find emotionally intense, such as a picture of an ex-partner or of a now-deceased family member. To explore this issue, we examined Facebook's Memories feature, a Technology-Mediated Reflection (TMR) system that shows users content about their pasts in order to prompt reflection. We interviewed 20 people who had recently seen sensitive curated content through this suite of features. We found that they wanted to see ?bittersweet? content, but they preferred to see it when it was expected and viewed in a context they felt was appropriate, and when they were able to make sense of why the recommender system curated the content. We recommend that designers engage in three practices to meet users? needs: (1) draw inspiration from no/low-technology artifacts, (2) use empirical research to identify which contextual features have negative impacts on users, and (3) conduct user studies to determine how users are doing sense-making around the perceived affect of recommender systems.\nEveryday Evaluations of Social Media Metrics: The Like button is simultaneously a means of social interaction and a tool to evaluate social media content. Based on in-depth interviews with 25 artists who use Instagram, we identified three overlapping orientations to the Like button: affective, relational, and infrastructural. We found that the flexibility of the button creates ambiguity around the meaning of a Like that incentivizes an economic approach to evaluation that crowds out other value schemas, shaping how artists use the platform, make art, and even understand themselves.\nAlgorithmic Infrastructure and Bias\nAI systems have been critiqued for designs that cannot capture the nuance of human identity. However, there was scant empirical work at the time. To address this gap, we conducted a study of how gender is operationalized into commercial facial analysis services. We conducted a two-phrase study: (1) a system analysis of ten commercial facial analysis and image labeling services and (2) an evaluation of five services using a custom dataset of diverse genders using social media images with gender labels provided by their creators. We found that services performed consistently worse on transgender individuals and were universally unable to classify non-binary genders. We found that bias is the result of more than just engineering practices and training data. We identified bias in how gender is codified into the data standards that surround (and typically pre-date) classifiers. We demonstrate how the cloud based services we studied provide black-boxed infrastructures to many third-party developers that can include subtle and invisible forms of bias. \nNext, we investigated race and gender bias in computer vision by examining the databases with which models are built. Race and gender have long sociopolitical histories of classification in technical infrastructures?from the passport to social media. In this study, we specifically focused on how race and gender are defined and annotated in image databases used for facial analysis. We found that the majority of databases rarely contain underlying source material for how those identities are defined. Further, when they are annotated with race and gender information, database authors rarely describe the process of annotation. In our publication we discuss the limitations of these approaches and argue that the lack of critical engagement renders databases opaque and less trustworthy. Our work encourages and provides guidance for database authors to address both the histories of classification inherently embedded into race and gender.\n\n\t\t\t\t\tLast Modified: 08/29/2021\n\n\t\t\t\t\tSubmitted by: Jed R Brubaker"
 }
}