{
 "awd_id": "1763235",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Wearable Sound Sensing and Feedback Techniques for Persons who are Deaf or Hard of Hearing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2018-07-21",
 "awd_max_amd_letter_date": "2018-07-21",
 "awd_abstract_narration": "The goal of this research is to advance wearable sound sensing and feedback technology for users who are deaf or hard of hearing, along with appropriate techniques for human-computer interaction.  A wearable device with multiple microphones and audio processing algorithms to automatically sense, localize, and identify sounds will be developed.  Means to discreetly provide this information to the user via emerging wearable technologies such as head-mounted displays and smartwatches will also be implemented.  Evaluations will include lab and field studies, everyday tasks such as noticing sounds and participating in oral conversations, and objective and subjective measures. Project outcomes will have broad impact by enabling new sound awareness options for users who are deaf or hard of hearing, thereby augmenting the wearer's existing strategies with additional, unobtrusive information. The new assistive technologies will have the potential to improve the lives of a large portion of the population, in particular the growing number of older adults with hearing loss in the United States.\r\n\r\nTo these ends, major subgoals will include: understanding user needs for wearable sound sensing and feedback, including prioritizing the importance of different sounds across a variety of contexts and based on an individual user's level of hearing loss; developing and evaluating a lightweight wearable sound sensing platform and accompanying algorithms, including both new sound scene analysis algorithms for a microphone array conformal on, or in proximity to, a complex-shaped baffle (the wearer's head) and that take into account how sound scatters off the wearer's body, along with adaptive state-of-the-art sound classification and speech recognition approaches to work with this processed audio; and developing and evaluating visual and haptic or vibrational feedback of the sensed sound via wearable prototypes.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ramani",
   "pi_last_name": "Duraiswami",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ramani Duraiswami",
   "pi_email_addr": "ramani@umiacs.umd.edu",
   "nsf_id": "000233989",
   "pi_start_date": "2018-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "office of research administratio",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207420001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The multi-university project sought to develop an approach to create methods for the deaf and hard of hearing to experience the sound in their environment, using sensing, computing and appropriately designed information presentation.&nbsp; Our collaborators at University of Washington (lead) and Gallaudet were focused on user studies, evaluation and design, while the University of Maryland team was funded initially to help with the hardware design. The UMD funding was restricted to the first two years of the project.</p>\n<p>UMD focus was on designing the hardware: a wearable microphone array, integrated with wearable computing, and to provide software that could be integrated with the UI/UX layer and provided&nbsp; the locations of sound sources in the environment. This would then be presented to subjects by our collaborators, and a design iteration was to have taken place.</p>\n<p>The algorithms for source localization were based on a novel approach called sequential source detection. They accounted for body scattering using the boundary element method to compute array transfer functions.</p>\n<p>The initial microphone array design was based on a chainable architecture. Subsequently it became apparent that MEMS microphones would be better, but this design could not be completed during the project.</p>\n<p>Unfortunately, due to the pandemic, experimental work could not be completed in time, and funding ran out. Future work will build on these ideas.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/08/2023<br>\n\t\t\t\t\tModified by: Ramani&nbsp;Duraiswami</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe multi-university project sought to develop an approach to create methods for the deaf and hard of hearing to experience the sound in their environment, using sensing, computing and appropriately designed information presentation.  Our collaborators at University of Washington (lead) and Gallaudet were focused on user studies, evaluation and design, while the University of Maryland team was funded initially to help with the hardware design. The UMD funding was restricted to the first two years of the project.\n\nUMD focus was on designing the hardware: a wearable microphone array, integrated with wearable computing, and to provide software that could be integrated with the UI/UX layer and provided  the locations of sound sources in the environment. This would then be presented to subjects by our collaborators, and a design iteration was to have taken place.\n\nThe algorithms for source localization were based on a novel approach called sequential source detection. They accounted for body scattering using the boundary element method to compute array transfer functions.\n\nThe initial microphone array design was based on a chainable architecture. Subsequently it became apparent that MEMS microphones would be better, but this design could not be completed during the project.\n\nUnfortunately, due to the pandemic, experimental work could not be completed in time, and funding ran out. Future work will build on these ideas.\n\n\t\t\t\t\tLast Modified: 08/08/2023\n\n\t\t\t\t\tSubmitted by: Ramani Duraiswami"
 }
}