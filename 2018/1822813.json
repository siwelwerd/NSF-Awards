{
 "awd_id": "1822813",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Cyberlearning: Sensei: High-Fidelity, Non-Invasive Classroom Sensing for Professional Development",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925126",
 "po_email": "abaylor@nsf.gov",
 "po_sign_block_name": "Amy Baylor",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 766000.0,
 "awd_min_amd_letter_date": "2018-08-18",
 "awd_max_amd_letter_date": "2021-06-09",
 "awd_abstract_narration": "For years, research has shown that moving away from large lectures and increasing student engagement and participation in classrooms significantly improves learning. Unfortunately, professors lack quality professional development opportunities to improve their instruction, and typically receive no training on how to teach. This project is addressing the issue of college professors' professional development through a cyberlearning innovation called Sensei. Sensei has novel capabilities using sensors to capture, isolate, and analyze voice and video that will provide near real time data on classroom interactions such as the percent time students talk vs professors, the percent time students talk to students, student engagement through facial expression analysis, turn taking between students and professors, etc all of which involve multimodal analysis of voice and video. The second component of this research is the development of suggested actions to improve the professor's performance as a teacher.\r\n\r\nMore precisely, Sensei draws on technical and socio-technical advances in sensing arrays, computer vision, intelligent environments, and personal informatics, as well as frameworks of professional development in higher education. In this project the researchers will 1) develop the technologies needed to automatically sense and display feedback to instructors, 2) deploy this system in-vivo to college instructors over semesters of use in a series of design-based research studies, and interpret the results to 3) iterate on our framework for the routine incorporation of classroom data into professional development. This research is enabled by a cyber innovation in which computing is expanded by the capabilities of state of the art multimodal sensing approaches to achieve non-invasive sensing at classroom-scale. This cyber innovation drives a learning innovation of delivering near-real-time data on teaching practices in a combined reflection and training system by delivering rapid and frequent feedback and instruction on good strategies in manageable instructional units, that support a focus on student-centered beliefs. In turn, the learning innovation advances understanding of how instructors learn in technology-rich learning environments by exploring mechanisms in a framework of professional development that would not be possible without this new cyberlearning genre. In particular, through a series of design-based research studies with instructors teaching STEM college courses, the researchers explore ways in which Sensei  a) can trigger critical self reflection, b) how this self-reflection changes based on the features of the data viewed, c) how datadriven goal-setting can foster self-efficacy in teaching, and d) how these effects vary over time. All of the code will be developed as open source and, if successful, Sensei could be generalized to include K-12 teachers.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Amy",
   "pi_last_name": "Ogan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Amy Ogan",
   "pi_email_addr": "aeo@andrew.cmu.edu",
   "nsf_id": "000611527",
   "pi_start_date": "2018-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yuvraj",
   "pi_last_name": "Agarwal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuvraj Agarwal",
   "pi_email_addr": "Yuvraj.Agarwal@cs.cmu.edu",
   "nsf_id": "000552330",
   "pi_start_date": "2018-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Harrison",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher Harrison",
   "pi_email_addr": "chris.harrison@cs.cmu.edu",
   "nsf_id": "000664095",
   "pi_start_date": "2018-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "153600",
   "pgm_ele_name": "S-STEM-Schlr Sci Tech Eng&Math"
  },
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "13XX",
   "app_name": "H-1B FUND, EHR, NSF",
   "app_symb_id": "045176",
   "fund_code": "1300XXXXDB",
   "fund_name": "H-1B FUND, EDU, NSF",
   "fund_symb_id": "045176"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 750000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-79c8c37d-7fff-ad0b-7123-e26c289f20ec\">\n<p dir=\"ltr\"><span>For years, research has shown that moving away from traditional lecture format to methods like active learning that increase student engagement and participation in classrooms significantly improves learning and broader learning outcomes, such as increasing the likelihood of choosing a technical major. However, most colleges still rely on lectures where students passively receive information from a professor. Students thus gain access to critical domain knowledge, but are often disengaged from the process of learning, leading to lower rates of success and lower long-term retention in STEM fields. On the other side, professors lack quality professional development opportunities to improve their instruction, which contributes to this major societal problem. Professors are hired and promoted for their domain expertise, and typically view themselves as domain experts and not teaching experts. Unlike K-12 teachers, college faculty typically receive no training on how to teach and instead &ldquo;learn&rdquo; how to teach while pursuing an advanced degree, often developing inaccurate views of teaching and learning and continuing the lecture model practiced by their own professors.</span></p>\n<p dir=\"ltr\"><span>In this project we developed a new cyberlearning innovation, EduSense (Ahuja and Kim et al, 2019), supported by novel technical capabilities. Our system addresses the above challenges by automatically collecting, processing and displaying classroom data to provide feedback to college instructors that helps shift behaviors and related beliefs in a distributed and time-efficient way. It consists of a connected set of socio-technical systems that support instructors in acquiring scientifically-supported, student-centered teaching practices through reflection on feedback using near-real-time classroom sensed data.</span></p>\n<p dir=\"ltr\"><span>Almost all prior classroom sensing systems have limited practical real-world utility since they are either are single-classroom scale (e.g., requiring a server per classroom), are not comprehensive in terms of the sensing facets they can provide, or do not present a scalable system architecture that could approach campus-scale deployments &ndash; a key goal of this grant. The value in our system is in putting together disparate advances from several fields into a novel, comprehensive and scalable classroom-focused system, paired with a holistic evaluation combining both controlled studies and months-long, real-world deployments.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Our work was cited in a recent review paper on classroom sensing (Foster et al, 2024) as a prominent classroom sensing system in the discipline, noting that &ldquo;To our knowledge, Edusense is the first sensing tool used in real-life classroom environments to unify the detection of all these features into a single system.&rdquo; To do so, </span><span>we equipped multiple classrooms (38) at our institution with an infrastructure-level set of sensors (cameras for video and audio recording) connected to EduSense. This provides our university an unprecedented testbed to study classroom sensing technologies and their impact. </span><span>&nbsp;</span></p>\n<p dir=\"ltr\"><span>EduSense collects behavioral data using this dual camera setup, and utilizes computer vision and machine learning based methods to inform classroom behaviors at a fine granularity, consisting of information such as instructor and student location, gaze, and ambient audio. Our second system, ClassroomDigitalTwins (Ahuja et al, 2021), introduces 3D gaze estimation for better estimation of attention from instructors and students. It further utilizes this to build a digital twin of processed classroom sessions, which can be used by instructors to revisit moments from their teaching, and use it for further improvements.&nbsp;</span></p>\n<p dir=\"ltr\"><span>While Edusense and ClassroomDigitalTwins provides a way to capture data on teacher-student behaviors and interactions at a scale, the output is very high fidelity, and is too complex to be used directly by instructors or pedagogy researchers. We observed that other classroom sensing systems also face similar challenges, and translating sensing data into meaningful insights often requires custom analyses, thus making them non-scalable. To address these issues, we built Edulyze, an analytics engine that processes complex, multi-modal sensor data and translates it into a set of analytics that can provide meaningful insights to different stakeholders. Edulyze can combine sensing data from a variety of classroom sensing systems, and translates it into a unified schema regardless of the underlying sensing system or classroom configuration. Analytics from Edulyze can train machine learning models to predict activities happening in the classroom, based on standardized classroom observation protocols that can support the professional development of instructors to help them improve their teaching.&nbsp;</span></p>\n<p dir=\"ltr\"><span>As classroom sensing systems impact both instructors and students, the student perspective is also critical for the development and implementation of these systems. We conducted interviews with undergraduate students and presented them with possible scenarios of classroom sensing systems to understand their perceptions and views. We found that students had nuanced views of classroom sensing technologies related to how they impacted student autonomy, connections with instructors, and their thoughts towards the role of technology in the classroom. Our publication of these findings won a paper award at ACM Designing Interactive Systems Conference.</span></p>\n</span></p><br>\n<p>\n Last Modified: 02/16/2024<br>\nModified by: Amy&nbsp;Ogan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nFor years, research has shown that moving away from traditional lecture format to methods like active learning that increase student engagement and participation in classrooms significantly improves learning and broader learning outcomes, such as increasing the likelihood of choosing a technical major. However, most colleges still rely on lectures where students passively receive information from a professor. Students thus gain access to critical domain knowledge, but are often disengaged from the process of learning, leading to lower rates of success and lower long-term retention in STEM fields. On the other side, professors lack quality professional development opportunities to improve their instruction, which contributes to this major societal problem. Professors are hired and promoted for their domain expertise, and typically view themselves as domain experts and not teaching experts. Unlike K-12 teachers, college faculty typically receive no training on how to teach and instead learn how to teach while pursuing an advanced degree, often developing inaccurate views of teaching and learning and continuing the lecture model practiced by their own professors.\n\n\nIn this project we developed a new cyberlearning innovation, EduSense (Ahuja and Kim et al, 2019), supported by novel technical capabilities. Our system addresses the above challenges by automatically collecting, processing and displaying classroom data to provide feedback to college instructors that helps shift behaviors and related beliefs in a distributed and time-efficient way. It consists of a connected set of socio-technical systems that support instructors in acquiring scientifically-supported, student-centered teaching practices through reflection on feedback using near-real-time classroom sensed data.\n\n\nAlmost all prior classroom sensing systems have limited practical real-world utility since they are either are single-classroom scale (e.g., requiring a server per classroom), are not comprehensive in terms of the sensing facets they can provide, or do not present a scalable system architecture that could approach campus-scale deployments  a key goal of this grant. The value in our system is in putting together disparate advances from several fields into a novel, comprehensive and scalable classroom-focused system, paired with a holistic evaluation combining both controlled studies and months-long, real-world deployments.\n\n\nOur work was cited in a recent review paper on classroom sensing (Foster et al, 2024) as a prominent classroom sensing system in the discipline, noting that To our knowledge, Edusense is the first sensing tool used in real-life classroom environments to unify the detection of all these features into a single system. To do so, we equipped multiple classrooms (38) at our institution with an infrastructure-level set of sensors (cameras for video and audio recording) connected to EduSense. This provides our university an unprecedented testbed to study classroom sensing technologies and their impact. \n\n\nEduSense collects behavioral data using this dual camera setup, and utilizes computer vision and machine learning based methods to inform classroom behaviors at a fine granularity, consisting of information such as instructor and student location, gaze, and ambient audio. Our second system, ClassroomDigitalTwins (Ahuja et al, 2021), introduces 3D gaze estimation for better estimation of attention from instructors and students. It further utilizes this to build a digital twin of processed classroom sessions, which can be used by instructors to revisit moments from their teaching, and use it for further improvements.\n\n\nWhile Edusense and ClassroomDigitalTwins provides a way to capture data on teacher-student behaviors and interactions at a scale, the output is very high fidelity, and is too complex to be used directly by instructors or pedagogy researchers. We observed that other classroom sensing systems also face similar challenges, and translating sensing data into meaningful insights often requires custom analyses, thus making them non-scalable. To address these issues, we built Edulyze, an analytics engine that processes complex, multi-modal sensor data and translates it into a set of analytics that can provide meaningful insights to different stakeholders. Edulyze can combine sensing data from a variety of classroom sensing systems, and translates it into a unified schema regardless of the underlying sensing system or classroom configuration. Analytics from Edulyze can train machine learning models to predict activities happening in the classroom, based on standardized classroom observation protocols that can support the professional development of instructors to help them improve their teaching.\n\n\nAs classroom sensing systems impact both instructors and students, the student perspective is also critical for the development and implementation of these systems. We conducted interviews with undergraduate students and presented them with possible scenarios of classroom sensing systems to understand their perceptions and views. We found that students had nuanced views of classroom sensing technologies related to how they impacted student autonomy, connections with instructors, and their thoughts towards the role of technology in the classroom. Our publication of these findings won a paper award at ACM Designing Interactive Systems Conference.\n\t\t\t\t\tLast Modified: 02/16/2024\n\n\t\t\t\t\tSubmitted by: AmyOgan\n"
 }
}