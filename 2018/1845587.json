{
 "awd_id": "1845587",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Attention-Aware Mixed Reality Interfaces",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 244997.0,
 "awd_amount": 244997.0,
 "awd_min_amd_letter_date": "2018-08-15",
 "awd_max_amd_letter_date": "2018-08-15",
 "awd_abstract_narration": "This project develops new technologies that measure and model people's state of attention and applies these methodologies to virtual reality (VR) and augmented reality (AR) language learning applications. For determining the degree of people's attention on central learning tasks presented in VR and AR, this project uses two sensor modalities that can index attention: electrical activity of the brain, as measured by electroencephalography (EEG) signals, and eye gaze behavior, as measured by eye trackers. Context recognition plays a key element in future VR and AR application scenarios, and users, as well as content providers, can benefit substantially from information about user attention states during information consumption. The project can inform the development of optimized VR and AR content, as well as individualized learning strategies. The project's motivating application is the optimization of language learning for users across the complete spectrum of ability. In the longer run, additional benefits include the creation of special tools for students with known attention deficits, as well as for increasing productivity and safety in various commercial and industrial applications. \r\n\r\nThis research explores necessary novel technologies for attention-aware mixed reality (MR) interfaces. The project integrates signals from consumer-grade EEG and eye tracking devices, to determine if and how much the participant's attention is divided (i.e. distracted or multi-tasking) or focused, and to assist appropriately. In this attention-assisted paradigm, users are monitored by EEG and eye tracking devices while interacting with mixed reality user interfaces. Attention states are classified over time using both sensor modalities and can be spatially referenced in the user interface with eye tracking. Attention activity feedback can be reported in real time while the user is interacting with the interface or may be stored and later visualized for a more thorough analysis of attentional patterns. The project delivers technology demonstrations of attention-aware interfaces for foreign language vocabulary learning in VR (with a lookout on AR possibilities). Exploratory experiments will uncover the possibilities for characterizing human attention states through EEG and eye tracking data. The project opens new opportunities for advances in multi-modal interaction to contribute to MR interfaces and learning technologies.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tobias",
   "pi_last_name": "Hollerer",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Tobias H Hollerer",
   "pi_email_addr": "holl@cs.ucsb.edu",
   "nsf_id": "000166428",
   "pi_start_date": "2018-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Turk",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew Turk",
   "pi_email_addr": "mturk@cs.ucsb.edu",
   "nsf_id": "000245931",
   "pi_start_date": "2018-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Barry",
   "pi_last_name": "Giesbrecht",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Barry L Giesbrecht",
   "pi_email_addr": "giesbrecht@psych.ucsb.edu",
   "nsf_id": "000251476",
   "pi_start_date": "2018-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California, Santa Barbara",
  "perf_str_addr": "Computer Science Department",
  "perf_city_name": "Santa Barbara",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931065110",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 244997.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span> </span></p>\n<div>This research project developed novel technologies for measuring and modeling people&rsquo;s state of attention in mixed reality (MR) applications. Mixed reality, which subsumes the fields of augmented reality and virtual reality, presents computer-generated graphics and sound to people, making it appear as if the computer-generated information lives in - or even entirely forms - the user's surrounding environment. Such environments present great opportunities for controlled behavioral and human-computer interaction studies, but are also heavily invested in by virtually all big technology companies today as a potential technology to complement or replace smart phones and tablets. A big unresolved question about the use of mixed reality technology is how users will direct and split their attention among different user interface elements and between the virtual and physical environment.&nbsp; &nbsp; A central question is whether users get distracted from any given task, and, if so, to what extent? Divided attention can arise internally, when people attempt to multitask, or it can come from outside influences and events that draw attention away from the task at hand.&nbsp;For determining the degree of people&rsquo;s attention on information presented in VR and AR, this project uses two sensor modalities that can index attention: electrical activity of the brain, as measured by EEG signals, and eye gaze behavior, as measured by eye trackers. The project established a reference platform to collect sensor data from people engaged in controlled VR and AR decision-making activities.&nbsp;Identifying and tackling a core challenge in attention-aware mixed reality, namely incomplete and noisy sensor data, the project built and evaluated multi-modal machine-learning models for attention that are robust to noise. It applied&nbsp;these solutions to the challenging problem of meaningful EEG signal collection while walking, presenting an initial controlled user study on that front. And finally, it demonstrated potential use-cases of attention-aware mixed reality via several example applications, including situated product recommendations and foreign language vocabulary learning.&nbsp;</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/13/2022<br>\n\t\t\t\t\tModified by: Tobias&nbsp;H&nbsp;Hollerer</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1845587/1845587_10571486_1655148176499_AiwenEEGMRStudy--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1845587/1845587_10571486_1655148176499_AiwenEEGMRStudy--rgov-800width.jpg\" title=\"Attention-Aware Mixed Reality User Study\"><img src=\"/por/images/Reports/POR/2022/1845587/1845587_10571486_1655148176499_AiwenEEGMRStudy--rgov-66x44.jpg\" alt=\"Attention-Aware Mixed Reality User Study\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">EEG and eye tracking data collection in an attention-aware mixed reality user study. The participant is walking around a small table, reacting to word-image pairs (N400 task).</div>\n<div class=\"imageCredit\">\ufffd University of California, Santa Barbara, Four Eyes Laboratory and Attention Laboratory</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Tobias&nbsp;H&nbsp;Hollerer</div>\n<div class=\"imageTitle\">Attention-Aware Mixed Reality User Study</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nThis research project developed novel technologies for measuring and modeling people\u2019s state of attention in mixed reality (MR) applications. Mixed reality, which subsumes the fields of augmented reality and virtual reality, presents computer-generated graphics and sound to people, making it appear as if the computer-generated information lives in - or even entirely forms - the user's surrounding environment. Such environments present great opportunities for controlled behavioral and human-computer interaction studies, but are also heavily invested in by virtually all big technology companies today as a potential technology to complement or replace smart phones and tablets. A big unresolved question about the use of mixed reality technology is how users will direct and split their attention among different user interface elements and between the virtual and physical environment.    A central question is whether users get distracted from any given task, and, if so, to what extent? Divided attention can arise internally, when people attempt to multitask, or it can come from outside influences and events that draw attention away from the task at hand. For determining the degree of people\u2019s attention on information presented in VR and AR, this project uses two sensor modalities that can index attention: electrical activity of the brain, as measured by EEG signals, and eye gaze behavior, as measured by eye trackers. The project established a reference platform to collect sensor data from people engaged in controlled VR and AR decision-making activities. Identifying and tackling a core challenge in attention-aware mixed reality, namely incomplete and noisy sensor data, the project built and evaluated multi-modal machine-learning models for attention that are robust to noise. It applied these solutions to the challenging problem of meaningful EEG signal collection while walking, presenting an initial controlled user study on that front. And finally, it demonstrated potential use-cases of attention-aware mixed reality via several example applications, including situated product recommendations and foreign language vocabulary learning. \n\n \n\n\t\t\t\t\tLast Modified: 06/13/2022\n\n\t\t\t\t\tSubmitted by: Tobias H Hollerer"
 }
}