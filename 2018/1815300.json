{
 "awd_id": "1815300",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Feature Encoding for Reinforcement Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 499968.0,
 "awd_amount": 499968.0,
 "awd_min_amd_letter_date": "2018-07-26",
 "awd_max_amd_letter_date": "2018-09-14",
 "awd_abstract_narration": "This project focuses on the subfield of machine learning referred to as Reinforcement Learning (RL), in which algorithms or robots learn by trial and error. As with many areas of machine learning, there has been a surge of interest in \"deep learning\" approaches to reinforcement learning, i.e, \"Deep RL.\"  Deep learning uses computational models motivated by structures found in the brains of animals. Deep RL has enjoyed some stunning successes, including a recent advance by which a program learned to play the Asian game of Go better than the best human player. Notably, this level of performance was achieved without any human guidance. Given only the rules of the game, the program learned by playing against itself. Although games are intriguing and attention-grabbing, this feat was merely a technology demonstration. Firms are seeking to deploy Deep RL methods to increase the efficiency of their operations across a range of applications such as data center management and robotics. To realize fully the potential of Deep RL, further research is required to make the training process more predictable, reliable, and efficient. Current techniques require massive amounts of training data and computation, and subtle changes in the configuration of the system can cause huge differences in the quality of the results obtained. Thus, even though RL systems can learn autonomously by trial and error, a large amount of human intuition, experience and experimentation may be required to lay the groundwork for these systems to succeed. This proposal seeks to develop new techniques and theory to make high quality deep RL results more widely and easily obtainable. In addition, this proposal will provide opportunities for undergraduates to be involved in research through Duke's Data+ initiative.\r\n\r\nThe proposed research is partly inspired by past work on feature selection and discovery for reinforcement learning.  Much of that work focused primarily on linear value function approximation.  Its relevance to deep reinforcement learning is that methods such as Deep Q-learning have a linear final layer.  The preceding, nonlinear layers can, therefore, be interpreted as performing feature discovery for what is ultimately a linear value function approximation process.  Sufficient conditions on the features that were specified for successful linear value function approximation in earlier work can now be re-interpreted as an intermediate objective function for the penultimate layer of a deep network. The proposed research aims to achieve the following objectives: 1) Develop a theory of feature construction that explains and informs deep reinforcement learning methods, 2) develop improved approaches to value function approximation that are applicable to deep reinforcement learning, 3) develop improved approaches to policy search that are applicable to deep reinforcement learning, and 4) develop new algorithms for exploration in reinforcement learning that take advantage of learned feature representations, and 5) perform computational experiments demonstrating the efficacy of the new algorithms developed on benchmark problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ronald",
   "pi_last_name": "Parr",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ronald Parr",
   "pi_email_addr": "parr@cs.duke.edu",
   "nsf_id": "000188767",
   "pi_start_date": "2018-07-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Lawrence",
   "pi_last_name": "Carin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lawrence Carin",
   "pi_email_addr": "lcarin@ee.duke.edu",
   "nsf_id": "000522425",
   "pi_start_date": "2018-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054677",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499968.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">This project focused on reinforcement learning, by which an intelligent agent interacts with an environment and learns, through experience, to optimize the long term benefit for acting in the environment. Unlike supervised learning, which treats every decision as an independent choice disconnected from future decisions, reinforcement learning must take into account that decisions have consequences that impact the choices available in the future. A solution to a reinforcement learning problem is, therefore, a policy that potentially describes a long term strategy for action in a complex environment.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">Reinforcement learning has gained notoriety in recent years for is ability, when combined with deep learning (deep reinforcement learning), to solve problems such as the board game go, or video games. In addition to these attention-grabbing examples, people have become increasingly interested in using reinforcement for practical problems such as industrial automation, autonomous vehicles, and dynamic medical treatment regimes.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">Despite the promising initial successes of deep reinforcement learning, there is a long way to go in understanding when an why reinforcement learning methods succeed, and how solutions to reinforcement learning problems can generalize or transfer to new problems. It is generally agreed that understanding the representation of the problem - either as presented to the learner, or as learned in the early layers of a neural network - is central to addressing these questions.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">This project came to focus on a technique called successor features, which assumes that the reward or cost for operating in an environment can be expressed as a weighted combination functions called reward features. Leveraging this assumption, prior work has shown how a policy that is learned for one set of feature weights (a task) can be evaluated and used for a different set of feature weights (a new task). However, previous work provided only weak guidance on how close to optimal a policy learned for one task would be when used in a new task.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">A key result of this project was a new technique for understanding how policies for old tasks could be applied to new tasks. This technique provided much clearer guidance on how close to optimal old solutions might be when they are applied to new tasks. This allows an intelligent agent to make an informed choice about whether to try to learn a new policy, or whether it is good enough to stick with an old policy.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">The project also considered several extensions to this basic idea. The first extended the basic idea to continuous action spaces, building on a technique call deep radial basis value functions. A second extension combined successor features with hierarchical reinforcement learning. This combination is particularly interesting because it gives insight into how solutions can be shared not only across tasks, within subtasks of a larger, overall task.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/29/2023<br>\nModified by: Ronald&nbsp;Parr</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project focused on reinforcement learning, by which an intelligent agent interacts with an environment and learns, through experience, to optimize the long term benefit for acting in the environment. Unlike supervised learning, which treats every decision as an independent choice disconnected from future decisions, reinforcement learning must take into account that decisions have consequences that impact the choices available in the future. A solution to a reinforcement learning problem is, therefore, a policy that potentially describes a long term strategy for action in a complex environment.\n\n\n\n\n\nReinforcement learning has gained notoriety in recent years for is ability, when combined with deep learning (deep reinforcement learning), to solve problems such as the board game go, or video games. In addition to these attention-grabbing examples, people have become increasingly interested in using reinforcement for practical problems such as industrial automation, autonomous vehicles, and dynamic medical treatment regimes.\n\n\n\n\n\nDespite the promising initial successes of deep reinforcement learning, there is a long way to go in understanding when an why reinforcement learning methods succeed, and how solutions to reinforcement learning problems can generalize or transfer to new problems. It is generally agreed that understanding the representation of the problem - either as presented to the learner, or as learned in the early layers of a neural network - is central to addressing these questions.\n\n\n\n\n\nThis project came to focus on a technique called successor features, which assumes that the reward or cost for operating in an environment can be expressed as a weighted combination functions called reward features. Leveraging this assumption, prior work has shown how a policy that is learned for one set of feature weights (a task) can be evaluated and used for a different set of feature weights (a new task). However, previous work provided only weak guidance on how close to optimal a policy learned for one task would be when used in a new task.\n\n\n\n\n\nA key result of this project was a new technique for understanding how policies for old tasks could be applied to new tasks. This technique provided much clearer guidance on how close to optimal old solutions might be when they are applied to new tasks. This allows an intelligent agent to make an informed choice about whether to try to learn a new policy, or whether it is good enough to stick with an old policy.\n\n\n\n\n\nThe project also considered several extensions to this basic idea. The first extended the basic idea to continuous action spaces, building on a technique call deep radial basis value functions. A second extension combined successor features with hierarchical reinforcement learning. This combination is particularly interesting because it gives insight into how solutions can be shared not only across tasks, within subtasks of a larger, overall task.\n\n\n\t\t\t\t\tLast Modified: 11/29/2023\n\n\t\t\t\t\tSubmitted by: RonaldParr\n"
 }
}