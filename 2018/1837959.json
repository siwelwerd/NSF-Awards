{
 "awd_id": "1837959",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: IA: A Multi-phase Survey Strategy for Generalizing Inferences from Big Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2019-01-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 991127.0,
 "awd_amount": 991127.0,
 "awd_min_amd_letter_date": "2018-09-06",
 "awd_max_amd_letter_date": "2018-09-06",
 "awd_abstract_narration": "Many researchers argue that commercial services that yield a high volume of data have little scientific utility because their users are not representative of the general population. This project develops methods for generalizing inferences drawn from non-representative big data sources. The researchers are implementing a multi-phase survey to compare survey results with online social networking data. They will collect a nationwide probability sample and a separate sample of online users, and use novel statistical procedures to combine them in a manner will enable statistically valid estimators to be produced from the universe of online users.  The results of this project will be broadly applicable by enabling more accurate statistical models of to be created for applications in medicine, economics, and many other fields. \r\n \r\nStatistical weighting will be used to match the online media sample with the probability survey sample across a set of auxiliary variables observed for both samples. The researchers will then match the data in the universe of online users to the weighted sample across a second set of variables that is measured for all users. Doing so will enable extracting results from media users that are more representative of the general population. To produce real-time forecasts, Bayesian models for mixed frequency time series will be used to combine the weighted online-based analyses with traditional polls.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Robbins",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Robbins",
   "pi_email_addr": "mrobbins@rand.org",
   "nsf_id": "000696504",
   "pi_start_date": "2018-09-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rand Corporation",
  "inst_street_address": "1776 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA MONICA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3103930411",
  "inst_zip_code": "904013208",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "THE RAND CORPORATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "YY46Q97AEZA8"
 },
 "perf_inst": {
  "perf_inst_name": "RAND Corporation",
  "perf_str_addr": "1776 Main Street",
  "perf_city_name": "Santa Monica",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "904013208",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 991127.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Big data sources such as Twitter (now known as X) provide a vast wealth of information that is collected with tremendous volume and velocity. However, the utility of such data is hampered by the fact that groups that provide the data are not representative of a broad population, seemingly preventing the data from being used for generalizable inferences. In particular, basic demographic characteristics are often unknown for users of a platform such as Twitter, which prevents standard adjustments for non-representativeness (e.g., inverse probability weighting) from being applied. This project proposed a procedure for adjustment which effectively uses proxy characteristics that are calculated with information observed for users of the big database (e.g., a user profile, tweets). To determine these characteristics and then calculate the resulting weights that could be applied to a universe of users from a big data source, we proposed a dual survey strategy.</p>\r\n<p>First, we administer a survey to a probability sample (the probability survey) which collects auxiliary information such as demographics, and then we administer the same questionnaire to a set of users from the big database (the database-specific survey). Combination of data from these two surveys allows us to determine a nationally representative distribution of our proxy variables, which in turn enables us to adjust a larger universe of users of the big database.</p>\r\n<p>We developed and illustrated the theory and assumptions that underpin these methods. Two key assumptions are required: the mechanism that generates the big database must be ignorable with respect to the set of auxiliary variables, and the final outcome of interest must be conditionally independent of the auxiliary variables given the proxy variables. We first performed a simulation study that shows the sensitivity of the methods to the sample size of the various sources of data and to departures from the two assumptions.</p>\r\n<p>Our goal was to apply these methods to data from Twitter in order to develop a daily trendline for national sentiment towards Donald Trump and Joe Biden during the 2020 presidential election cycle (i.e., 9/15/2020 to 1/20/2021). Therefore, we collected data from three sources:</p>\r\n<ul>\r\n<li>A probability survey administered to the NORC AmeriSpeak panel (n = 2000)</li>\r\n<li>A survey administered to a group of Twitter users recruited using a targeted ad campaign (n = 900)</li>\r\n<li>A universe of Twitter users (n = 40,000)</li>\r\n</ul>\r\n<p>Using the Twitter API, we collected profiles and tweets posted during the aforementioned time period for all users from the second and third sources above.</p>\r\n<p>We used previously established procedures to illustrate that it is possible to align the survey of Twitter users to the probability survey (when validated across a variety of relevant outcomes), implying the feasibility of using Twitter data to produce nationally representative findings.</p>\r\n<p>The calculation of a trendline for sentiment towards Trump or Biden requires quantification of stance expressed towards those candidates within tweets posted by Twitter users. We explored a wide range of methods for doing so and established that a fine-tuned GPT procedure had superior performance. This procedure demonstrated utility for not only quantifying the overall sentiment of a piece of textm, but also (and more importantly) the stance expressed towards a specific subject.</p>\r\n<p>We developed proxies for demographic characteristics and political affiliation for each Twitter user in our Twitter survey sample and universe. These proxies were then used to calculate weights that could be applied to each user in our Twitter universe in order to make the universe representative of the general US population. We applied the aforementioned technique for scoring sentiment of all tweets posted by users in our Twitter universe during the 2020 presidential election cycle, which yielded the desired trendlines by calculating an average (or a weighted average) of sentiment expressed by day. We found that the unadjusted trendlines show a strong bias towards Biden and against Trump, but following adjustment of our Twitter universe with the weights, the trendlines align with the favorability indicated by national polls but show a more distinct and immediate reaction in response to noteworthy events such as presidential debates.</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/13/2025<br>\nModified by: Michael&nbsp;Robbins</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nBig data sources such as Twitter (now known as X) provide a vast wealth of information that is collected with tremendous volume and velocity. However, the utility of such data is hampered by the fact that groups that provide the data are not representative of a broad population, seemingly preventing the data from being used for generalizable inferences. In particular, basic demographic characteristics are often unknown for users of a platform such as Twitter, which prevents standard adjustments for non-representativeness (e.g., inverse probability weighting) from being applied. This project proposed a procedure for adjustment which effectively uses proxy characteristics that are calculated with information observed for users of the big database (e.g., a user profile, tweets). To determine these characteristics and then calculate the resulting weights that could be applied to a universe of users from a big data source, we proposed a dual survey strategy.\r\n\n\nFirst, we administer a survey to a probability sample (the probability survey) which collects auxiliary information such as demographics, and then we administer the same questionnaire to a set of users from the big database (the database-specific survey). Combination of data from these two surveys allows us to determine a nationally representative distribution of our proxy variables, which in turn enables us to adjust a larger universe of users of the big database.\r\n\n\nWe developed and illustrated the theory and assumptions that underpin these methods. Two key assumptions are required: the mechanism that generates the big database must be ignorable with respect to the set of auxiliary variables, and the final outcome of interest must be conditionally independent of the auxiliary variables given the proxy variables. We first performed a simulation study that shows the sensitivity of the methods to the sample size of the various sources of data and to departures from the two assumptions.\r\n\n\nOur goal was to apply these methods to data from Twitter in order to develop a daily trendline for national sentiment towards Donald Trump and Joe Biden during the 2020 presidential election cycle (i.e., 9/15/2020 to 1/20/2021). Therefore, we collected data from three sources:\r\n\r\nA probability survey administered to the NORC AmeriSpeak panel (n = 2000)\r\nA survey administered to a group of Twitter users recruited using a targeted ad campaign (n = 900)\r\nA universe of Twitter users (n = 40,000)\r\n\r\n\n\nUsing the Twitter API, we collected profiles and tweets posted during the aforementioned time period for all users from the second and third sources above.\r\n\n\nWe used previously established procedures to illustrate that it is possible to align the survey of Twitter users to the probability survey (when validated across a variety of relevant outcomes), implying the feasibility of using Twitter data to produce nationally representative findings.\r\n\n\nThe calculation of a trendline for sentiment towards Trump or Biden requires quantification of stance expressed towards those candidates within tweets posted by Twitter users. We explored a wide range of methods for doing so and established that a fine-tuned GPT procedure had superior performance. This procedure demonstrated utility for not only quantifying the overall sentiment of a piece of textm, but also (and more importantly) the stance expressed towards a specific subject.\r\n\n\nWe developed proxies for demographic characteristics and political affiliation for each Twitter user in our Twitter survey sample and universe. These proxies were then used to calculate weights that could be applied to each user in our Twitter universe in order to make the universe representative of the general US population. We applied the aforementioned technique for scoring sentiment of all tweets posted by users in our Twitter universe during the 2020 presidential election cycle, which yielded the desired trendlines by calculating an average (or a weighted average) of sentiment expressed by day. We found that the unadjusted trendlines show a strong bias towards Biden and against Trump, but following adjustment of our Twitter universe with the weights, the trendlines align with the favorability indicated by national polls but show a more distinct and immediate reaction in response to noteworthy events such as presidential debates.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 03/13/2025\n\n\t\t\t\t\tSubmitted by: MichaelRobbins\n"
 }
}