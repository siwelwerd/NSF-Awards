{
 "awd_id": "1709423",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Exploring Differences Between Instructors' Exams and How These Differences Produce Scores that Could Inaccurately and Inequitably Represent Student Understanding",
 "cfda_num": "47.076",
 "org_code": "11040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ellen Carpenter",
 "awd_eff_date": "2017-12-01",
 "awd_exp_date": "2021-11-30",
 "tot_intn_awd_amt": 50024.0,
 "awd_amount": 50024.0,
 "awd_min_amd_letter_date": "2017-11-22",
 "awd_max_amd_letter_date": "2017-11-22",
 "awd_abstract_narration": "In many STEM courses, students' exam scores determine their course grades.  In turn, when averaged together, course grades determine each student's grade point average, which can affect their persistence in a STEM major and their competitiveness for admission to professional or graduate schools. Thus, it is important that these exams accurately and equitably measure students' understanding of the subject matter that they are supposed to test. Little research has been done to determine whether specific exam questions accurately measure student understanding or whether they are fair to all students.  This collaborative project between Arizona State University and University of Washington will try to fill this gap in knowledge by analyzing questions on introductory biology course exams taught by different instructors.  They will examine the relationships between different types of questions, student scores on the questions, and student understanding of the concept that the question is supposed to test.  This information has the potential to help biology instructors more fairly and accurately test student understanding of biology.   It may also provide guidelines for building fair and accurate exam questions that are relevant to other STEM disciplines.\r\n\r\nThis project has four key goals: (1) characterizing the composition of instructor-generated biology exams across sections of the same introductory biology course; (2) characterizing elements of questions that result in students performing differently on questions; (3) determining if modifying questions can diminish differences in student performance; and (4) correlating exam scores to students' conceptual understanding of biology. This project will focus on the exams of different instructors teaching the same introductory biology course offered across multiple institutions within the same regional network. No published studies have explored differences in question performance on instructor-generated exams in introductory biology courses. Further, this project would be the largest, most comprehensive analysis of instructor-generated exams and questions in any STEM discipline done so far, providing insights into what factors may affect performance differences on individual questions.  This project can help instructors and researchers become more aware of elements of questions that result in unfair evaluation of certain groups of students, leading to more accurate and equitable measurement of student understanding.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Min",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Min Li",
   "pi_email_addr": "minli@u.washington.edu",
   "nsf_id": "000486022",
   "pi_start_date": "2017-11-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn Ave. NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981051016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "199800",
   "pgm_ele_name": "IUSE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8209",
   "pgm_ref_txt": "Improv Undergrad STEM Ed(IUSE)"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0418",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001819DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 50024.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>High-stakes course assessments (e.g., exams) in college are typically written by instructors and are often the primary means of evaluating student achievement in undergraduate STEM courses. Numerous studies have shown that questions on STEM assessments may involve serious validity threats and can be biased, leading to inaccurate indicators of student learning and can adversely impact grades, student identity, and retention. However, little work has been done exploring psychometric quality of instructor-generated college biology exams, particularly at the introductory level.</p>\n<p>&nbsp;</p>\n<p>As such, this collaborative research project focus on two objectives: (1) characterize the composition of instructor-generated biology exams and explore the incidences of differential performance on questions across many sections of the same introductory biology course, and (2) characterize elements of questions that result in differential performance. Below presents three sets of findings from this project.</p>\n<p>&nbsp;</p>\n<p><strong>Types of Instructor-generated Exam Questions</strong>. We compiled data on 9 major item features from all exams and examined the variation in those item features. We found statistically significant differences among instructors in the distribution of Bloom&rsquo;s level. Specifically, one of the instructors in our dataset used higher level Bloom&rsquo;s questions at the level of application and analysis more often than other instructors. We also found statistically significant differences in length of questions and use of figures. The same instructor that had a greater proportion of higher level Bloom&rsquo;s questions also had longer questions and used figures in questions more often. There was no significant difference in the use of negative words in stem among instructors. Finally, we assessed the relationship between item features and item difficulty. We found no correlation between these item features and item difficulty, except that questions that were at higher Bloom&rsquo;s level had higher item difficulty values.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>DIF Analysis of Instructor-generated Exam Questions</strong>. We conducted differential item functioning (DIF) analysis on data from six semesters of the introductory biology courses taught by three instructors/instructional teams and looked for cohort level differences by gender, race/ethnicity and socioeconomic status (as indicated by eligibility to receive federal financial assistance in the form of Pell grants and/or college generation status). Using logistic regression with a B-H correction for multiple testing, out of 987 items, only ten items show significant DIF by gender, eight items by first-generation status, four items by Pell-eligibility, and only one item by race/ethnicity. Thus, DIF analysis flagged only 23 out of 987 questions (2.3%). We initially suspected that low bias in analyzed exams might be due to the overall low cognitive level of the questions used in these exams. However, even in exams with more questions at a higher cognitive level, we observed a low amount of DIF.&nbsp;</p>\n<p>&nbsp;</p>\n<p>We used general linear modeling with Z-scores for exam and non-exam scores as outcomes and high school GPA, gender, race/ethnicity, Pell eligibility and first-generation status as the predictors. Women, Black/African American students, Hispanic/Latinx students, first-generation college students and Pell eligible students received lower scores on exams than men, white students, continuing generation students and non-Pell eligible students. However, there were no disparities by gender and college generation status on non-exam scores. Hispanic/Latinx students and Pell-eligible students still scored lower than reference groups, but the magnitude of the difference was much less than it was for exam scores. Thus, our results suggest that reducing the use of high stake exams and increasing the use of non-exam assessments can reduce disparities in student grades by social identities.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Differences in Exam Retake</strong>. We analyzed data on student scores from three semesters where all students had the opportunity to retake exams to investigate which variables influence the likelihood of a student retaking an exam. Black/African American students and those that worked more than 20 hours a week during the semester were less likely to retake exams even after controlling for student score on original attempts and other demographic variables. The proportion of students reporting that retake opportunities reduced their anxiety on the initial exam attempt was very high (85-90%) but did not show any consistent demographic differences.&nbsp;&nbsp;Broadly speaking, exam retakes significantly improved student scores; however, these retakes did not reduce the disparities in exam scores by race/ethnicity, number of hours worked, or financial need. On the contrary, optional exam retakes slightly increased disparities in scores received by Black students compared to white students and students that worked more than 20 hours a week compared to students that did not work, partly because of the difference in participation in retakes.</p>\n<p>&nbsp;</p>\n<p>Overall, our results suggest that optional exam retakes might be a useful tool in class to improve student performance and learning and reduce pressure associated with high stakes exams. However, differences in which students retake exams reduces their ability to reduce disparities in scores and could even increase disparities. This raises the question of whether exam retakes should be optional.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/14/2022<br>\n\t\t\t\t\tModified by: Min&nbsp;Li</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHigh-stakes course assessments (e.g., exams) in college are typically written by instructors and are often the primary means of evaluating student achievement in undergraduate STEM courses. Numerous studies have shown that questions on STEM assessments may involve serious validity threats and can be biased, leading to inaccurate indicators of student learning and can adversely impact grades, student identity, and retention. However, little work has been done exploring psychometric quality of instructor-generated college biology exams, particularly at the introductory level.\n\n \n\nAs such, this collaborative research project focus on two objectives: (1) characterize the composition of instructor-generated biology exams and explore the incidences of differential performance on questions across many sections of the same introductory biology course, and (2) characterize elements of questions that result in differential performance. Below presents three sets of findings from this project.\n\n \n\nTypes of Instructor-generated Exam Questions. We compiled data on 9 major item features from all exams and examined the variation in those item features. We found statistically significant differences among instructors in the distribution of Bloom\u2019s level. Specifically, one of the instructors in our dataset used higher level Bloom\u2019s questions at the level of application and analysis more often than other instructors. We also found statistically significant differences in length of questions and use of figures. The same instructor that had a greater proportion of higher level Bloom\u2019s questions also had longer questions and used figures in questions more often. There was no significant difference in the use of negative words in stem among instructors. Finally, we assessed the relationship between item features and item difficulty. We found no correlation between these item features and item difficulty, except that questions that were at higher Bloom\u2019s level had higher item difficulty values. \n\n \n\nDIF Analysis of Instructor-generated Exam Questions. We conducted differential item functioning (DIF) analysis on data from six semesters of the introductory biology courses taught by three instructors/instructional teams and looked for cohort level differences by gender, race/ethnicity and socioeconomic status (as indicated by eligibility to receive federal financial assistance in the form of Pell grants and/or college generation status). Using logistic regression with a B-H correction for multiple testing, out of 987 items, only ten items show significant DIF by gender, eight items by first-generation status, four items by Pell-eligibility, and only one item by race/ethnicity. Thus, DIF analysis flagged only 23 out of 987 questions (2.3%). We initially suspected that low bias in analyzed exams might be due to the overall low cognitive level of the questions used in these exams. However, even in exams with more questions at a higher cognitive level, we observed a low amount of DIF. \n\n \n\nWe used general linear modeling with Z-scores for exam and non-exam scores as outcomes and high school GPA, gender, race/ethnicity, Pell eligibility and first-generation status as the predictors. Women, Black/African American students, Hispanic/Latinx students, first-generation college students and Pell eligible students received lower scores on exams than men, white students, continuing generation students and non-Pell eligible students. However, there were no disparities by gender and college generation status on non-exam scores. Hispanic/Latinx students and Pell-eligible students still scored lower than reference groups, but the magnitude of the difference was much less than it was for exam scores. Thus, our results suggest that reducing the use of high stake exams and increasing the use of non-exam assessments can reduce disparities in student grades by social identities. \n\n \n\nDifferences in Exam Retake. We analyzed data on student scores from three semesters where all students had the opportunity to retake exams to investigate which variables influence the likelihood of a student retaking an exam. Black/African American students and those that worked more than 20 hours a week during the semester were less likely to retake exams even after controlling for student score on original attempts and other demographic variables. The proportion of students reporting that retake opportunities reduced their anxiety on the initial exam attempt was very high (85-90%) but did not show any consistent demographic differences.  Broadly speaking, exam retakes significantly improved student scores; however, these retakes did not reduce the disparities in exam scores by race/ethnicity, number of hours worked, or financial need. On the contrary, optional exam retakes slightly increased disparities in scores received by Black students compared to white students and students that worked more than 20 hours a week compared to students that did not work, partly because of the difference in participation in retakes.\n\n \n\nOverall, our results suggest that optional exam retakes might be a useful tool in class to improve student performance and learning and reduce pressure associated with high stakes exams. However, differences in which students retake exams reduces their ability to reduce disparities in scores and could even increase disparities. This raises the question of whether exam retakes should be optional. \n\n\t\t\t\t\tLast Modified: 06/14/2022\n\n\t\t\t\t\tSubmitted by: Min Li"
 }
}