{
 "awd_id": "1814056",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Designing Preferences, Beliefs, and Identities for Artificial Intelligence",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Roger Mailler",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2018-07-23",
 "awd_max_amd_letter_date": "2018-07-23",
 "awd_abstract_narration": "Historically, AI researchers have primarily focused on developing techniques that work well for pre-specified objectives that provide a useful measure of how well the techniques are working.  This approach is perfectly sensible in a situation where the techniques are not yet ready to make their way out of the lab and into the world.  However, as AI is now being broadly deployed in the world, more thought needs to be put into the methodologies for designing the objectives of AI systems.  This is because our aim is no longer just to evaluate whether our other techniques are able to pursue a given objective well, but rather to actually have them do good in the world.  Besides the AI system's objectives, we must also specify where one part of the system ends and another begins, as well as how it models the world.  Generally, it is not possible or desirable to simply hand off the system to a customer (in the broad sense of the word) who then must somehow fill in these blanks.  AI researchers need to be involved in this process because they understand how the system works and are able to provide algorithmic support for these decisions.  But rigorous computational frameworks for these processes are lacking, and they are what this research aims to provide.\r\n\r\nSpecifically, existing research in artificial intelligence, mirroring frameworks in economics and other related fields, is built on a conception of AI systems as agents.  It generally proceeds from the premise that each such agent has a well-defined identity over time, well-defined preferences over the different ways in which things may proceed, and well-defined beliefs about the world as it is and how it will develop over time.  Typical research then concerns the design of algorithms under the assumption that all these aspects have already been specified (with the common exception of still needing to do some learning about the environment).   However, as we design real AI systems, we in fact need to specify where the boundaries between one agent and another in the system lie, what objective functions these agents aim to maximize, and to some extent even what belief formation processes they use.  The premise of this research is that as AI is being broadly deployed in the world, we need well-founded theories of, and methodologies and algorithms for, how to design preferences, identities, and beliefs.  Doing so in a responsible fashion will require the development and rigorous evaluation of new techniques.  The project will address these questions from a rigorous foundation in decision theory, game theory, social choice theory, mechanism design theory, and the algorithmic and computational aspects of these fields.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vincent",
   "pi_last_name": "Conitzer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vincent Conitzer",
   "pi_email_addr": "conitzer@cs.duke.edu",
   "nsf_id": "000487289",
   "pi_start_date": "2018-07-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Traditionally in AI research, when considering building an AI agent, it is assumed that the agent's objective is exogenously provided, that the boundaries of the agent (where the agent begins and ends) are clear, and that it is conceptually clear how the agent should form beliefs.&nbsp; However, now that AI is being broadly deployed in the world, it is becoming increasingly clear that these aspects are often not exogenously determined.&nbsp; Rather, we need theory and methodologies for specifying them.&nbsp; (Is a network of self-driving cars a single agent or a collection of multiple agents?&nbsp; Should they always share all their information to arrive at a common belief?&nbsp; How should they trade off risks between occupants and others on the road?)&nbsp; The goal of this project has been to provide and analyze such theory and methodologies, drawing on techniques from not only computer science but also economic theory and philosophy.<br /><br />The project has produced several major research outcomes.<br /><br />First, we made significant contributions to the topic of learning an objective from the individual judgments of multiple people.&nbsp; These included new theoretical models for how to do so and accompanying results for how much information in fact needs to be elicited from people, new algorithms for aggregating their judgments in this context, and a study of how to handle cases where people do not want to indicate a strict preference in a specific choice scenario.<br /><br />Second, we made fundamental progress on techniques for a variety of machine learning settings in which the party providing information to the algorithm has a strategic interest in the outcome, and consequently may strategically alter the information it reports.&nbsp; We made theoretical and experimental progress in the setting of an individual agent being classified based on information that it itself provides, and we also found new algorithms for designing incentive mechanisms for dynamic settings.<br /><br />Third, we made a number of new contributions for game-theoretic settings with multiple agents.&nbsp; This included showing how we can in some cases make everyone better off by committing agents to use utilities that are different from our own, when those agents are game-theoretically strategic; showing the strategic stability of optimal symmetric strategies in team games; and a study of computational problems in repeated games in which there is a persistent state that only one of the players learns about.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/07/2023<br>\n\t\t\t\t\tModified by: Vincent&nbsp;Conitzer</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nTraditionally in AI research, when considering building an AI agent, it is assumed that the agent's objective is exogenously provided, that the boundaries of the agent (where the agent begins and ends) are clear, and that it is conceptually clear how the agent should form beliefs.  However, now that AI is being broadly deployed in the world, it is becoming increasingly clear that these aspects are often not exogenously determined.  Rather, we need theory and methodologies for specifying them.  (Is a network of self-driving cars a single agent or a collection of multiple agents?  Should they always share all their information to arrive at a common belief?  How should they trade off risks between occupants and others on the road?)  The goal of this project has been to provide and analyze such theory and methodologies, drawing on techniques from not only computer science but also economic theory and philosophy.\n\nThe project has produced several major research outcomes.\n\nFirst, we made significant contributions to the topic of learning an objective from the individual judgments of multiple people.  These included new theoretical models for how to do so and accompanying results for how much information in fact needs to be elicited from people, new algorithms for aggregating their judgments in this context, and a study of how to handle cases where people do not want to indicate a strict preference in a specific choice scenario.\n\nSecond, we made fundamental progress on techniques for a variety of machine learning settings in which the party providing information to the algorithm has a strategic interest in the outcome, and consequently may strategically alter the information it reports.  We made theoretical and experimental progress in the setting of an individual agent being classified based on information that it itself provides, and we also found new algorithms for designing incentive mechanisms for dynamic settings.\n\nThird, we made a number of new contributions for game-theoretic settings with multiple agents.  This included showing how we can in some cases make everyone better off by committing agents to use utilities that are different from our own, when those agents are game-theoretically strategic; showing the strategic stability of optimal symmetric strategies in team games; and a study of computational problems in repeated games in which there is a persistent state that only one of the players learns about.\n\n\t\t\t\t\tLast Modified: 02/07/2023\n\n\t\t\t\t\tSubmitted by: Vincent Conitzer"
 }
}