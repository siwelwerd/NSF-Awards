{
 "awd_id": "1818586",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative: A Research Agenda to Explore Privacy in Small Data Applications",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 191552.0,
 "awd_amount": 191552.0,
 "awd_min_amd_letter_date": "2018-02-08",
 "awd_max_amd_letter_date": "2018-02-08",
 "awd_abstract_narration": "One of the crucial ideas behind Privacy by Design (PbD) is that privacy should be taken into consideration in the process of design, not merely after-the-fact, as so often happens. Yet, this ideal has failed to gain widespread practical traction, challenged, in part, by the lack of developed methodologies and also because of privacy's conceptual complexity, which hampers its operationalization. This project addresses both challenges simultaneously, seeking (i) to demonstrate how a robust operationalization of privacy can lead to meaningful PbD and (ii) to contribute methodological insight by engaging with ongoing research and development in the area of small data applications, namely, systems that advance wellness and personal productivity by utilizing digital traces from individuals' day-to-day activities, such as e-mail, grocery shopping, TV watching, transportation, mobile devices, and so forth. Adopting the definition of privacy as contextual integrity, the project will focus on selected small-data applications currently \"on the drawing board\" in PI Deborah Estrin's Small Data lab. With these design cases, the project rises to one of the PbD challenges, namely consideration of privacy early on in development and, as a research enterprise, its primary aim is to uncover more and less productive methodological approaches for doing so, resulting in system characteristics well correlated with privacy requirements. \r\n\r\nAt the same time, the project will provide invaluable insight into how to operationalize contextual integrity, which conceives of privacy as appropriate flow of personal information, modeling appropriate flow as conformance with context-specific informational norms, which, prescribe (and prohibit) information flows according to three parameters: actors (subject, sender, recipient), information types, and transmission principles (functional constraints on flow). Adopting contextual integrity as an operational definition means that researchers will assess privacy properties by carefully mapping data flows, and evaluating these flows in terms of the context of application and use. The project also extends past work on formal representations of informational norms by demonstrating how they may be integrated into design practices.  In addition to its substantive contributions this project embodies an innovative collaborative model -- a novel pairing of a computer scientist, Deborah Estrin (Cornell), with a philosopher, Helen Nissenbaum (NYU), in an equal partnership to forge technologies that embody meaningful privacy.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Helen",
   "pi_last_name": "Nissenbaum",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Helen Nissenbaum",
   "pi_email_addr": "hn288@cornell.edu",
   "nsf_id": "000471690",
   "pi_start_date": "2018-02-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148502820",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8225",
   "pgm_ref_txt": "SaTC Special Projects"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 191551.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-c0244bb2-7fff-f40e-3975-4c4a94789db4\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Privacy-by-design (PbD) is a commitment that designers make to take privacy seriously from a project&rsquo;s inception to completion. An urgent goal set for the developers of digital systems that collect and utilize personal data, PbD is touted by technologists and regulators alike, recently earning pride of place in the EU&rsquo;s General Data Privacy Regulation (GDPR). Yet, a persistent challenge for those who attach to this idea is how to operationalize it in practice. In this first-time cross-disciplinary collaboration, Deborah Estrin and Helen Nissenbaum take up the challenge of putting flesh on the idea of PbD by articulating steps, methods, and principles that resonate with designers and computer scientists while remaining true to a meaningful concept privacy that supports its value to society..&nbsp; </span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Dr. Estrin, a network engineer, has been developing lightweight systems for the purpose of enhancing physical well being and quality of life by exploiting data about a particular individual for that individual&rsquo;s benefit. She labels this a &ldquo;small data&rdquo; approach, contrasting it with a big data approach that aggregates and centralizes data from vast populations of users. Helen Nissenbaum, a philosopher, had developed the theory of Contextual Integrity (CI) according to which privacy is not secrecy or control; instead, it is preserved when&nbsp; information flows in a way that is appropriate for a given context, benefiting individuals and societal ends. She has also worked for many years values-in-design, a more general, related concept than PbD.</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">After noticing common purposes in their respective work, Estrin and Nissenbaum joined forces to develop a blueprint of an approach to PbD. Choosing an area within Estrin&rsquo;s zone of expertise, namely, personal health/fitness apps, they further focused on an application, Limbr, to serve as a sandbox for their ideas. Because Limbr was under development at Cornell Tech as part of a clinical research trial on long-term monitoring and management of back pain, it offered a platform whose details were deeply familiar and one that could be studied for privacy properties before the full functionality and architecture were decided. The team created a heuristic to chart the ways that user data flows within the Limbr app: data was broken down by type and then tracked through the app according to how it was collected, aggregated, or shared. </span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">As a generalizable outcome, the project found a simple visualization of data flows labelled with CI parameter values to be surprisingly helpful. This heuristic was then applied to other apps under development at Cornell Tech, demonstrating its effectiveness in guiding app developers thinking through privacy in their respective systems. The team intends to further develop this heuristic into a more general and more usable method.</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Next, the team applied these understandings of data flow, generally, to smartphone apps, which collect and process large amounts of user data. Although some of this collection requires explicit permission, users often do not understand the implications of app permissions, thus allowing inappropriate uses of data by app companies or third parties. In an analysis of app metadata, permissions, and ad network use by Android apps, the team found that while surface level cues like permissions, app category, and whether the app shows ads might help users judge whether information is shared appropriately, these cues aren&rsquo;t conclusive. Additional analysis of &ldquo;deep&rdquo; cues, such as the presence of trackers that send user data to ad networks, can help identify misbehaving applications but are mostly invisible to users.</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In addition to healthcare and Android apps, the research team applied these heuristics to other applications where better understanding of information flow can result in greater privacy. Mapping how information flows within an app - whether in healthcare, smartphone apps, organizations, or other contexts - can help software engineers and app designers better identify potential threats to user privacy, in areas where those flows are unexpected, obscured, or inappropriate. </span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Effective application of these heuristics can result apps in that respect users&rsquo; privacy while allowing information to flow in ways that are necessary for the app to function. In the case of smartphone apps, this means ensuring that the permissions that apps request paint an accurate picture of what the app will do with a user&rsquo;s data. In addition, this research points to the need to make &ldquo;deep&rdquo; cues, like the presence of ad network trackers, visible to users. </span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Finally, this project advances the field of privacy and technology ethics by applying the theory of privacy as contextual integrity to real-world applications. This theory allows us to analyze data types and flows in a way that helps to identify which aspects of privacy should be emphasized when designing a software system or app, and which are less realistic. The frameworks and methodologies published as part of this project will help developers and researchers draw on contextual integrity for PbD and privacy research.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/19/2019<br>\n\t\t\t\t\tModified by: Helen&nbsp;Nissenbaum</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Privacy-by-design (PbD) is a commitment that designers make to take privacy seriously from a project\u2019s inception to completion. An urgent goal set for the developers of digital systems that collect and utilize personal data, PbD is touted by technologists and regulators alike, recently earning pride of place in the EU\u2019s General Data Privacy Regulation (GDPR). Yet, a persistent challenge for those who attach to this idea is how to operationalize it in practice. In this first-time cross-disciplinary collaboration, Deborah Estrin and Helen Nissenbaum take up the challenge of putting flesh on the idea of PbD by articulating steps, methods, and principles that resonate with designers and computer scientists while remaining true to a meaningful concept privacy that supports its value to society..  \n\n\nDr. Estrin, a network engineer, has been developing lightweight systems for the purpose of enhancing physical well being and quality of life by exploiting data about a particular individual for that individual\u2019s benefit. She labels this a \"small data\" approach, contrasting it with a big data approach that aggregates and centralizes data from vast populations of users. Helen Nissenbaum, a philosopher, had developed the theory of Contextual Integrity (CI) according to which privacy is not secrecy or control; instead, it is preserved when  information flows in a way that is appropriate for a given context, benefiting individuals and societal ends. She has also worked for many years values-in-design, a more general, related concept than PbD.\n\n\nAfter noticing common purposes in their respective work, Estrin and Nissenbaum joined forces to develop a blueprint of an approach to PbD. Choosing an area within Estrin\u2019s zone of expertise, namely, personal health/fitness apps, they further focused on an application, Limbr, to serve as a sandbox for their ideas. Because Limbr was under development at Cornell Tech as part of a clinical research trial on long-term monitoring and management of back pain, it offered a platform whose details were deeply familiar and one that could be studied for privacy properties before the full functionality and architecture were decided. The team created a heuristic to chart the ways that user data flows within the Limbr app: data was broken down by type and then tracked through the app according to how it was collected, aggregated, or shared. \n\n\nAs a generalizable outcome, the project found a simple visualization of data flows labelled with CI parameter values to be surprisingly helpful. This heuristic was then applied to other apps under development at Cornell Tech, demonstrating its effectiveness in guiding app developers thinking through privacy in their respective systems. The team intends to further develop this heuristic into a more general and more usable method.\n\n\nNext, the team applied these understandings of data flow, generally, to smartphone apps, which collect and process large amounts of user data. Although some of this collection requires explicit permission, users often do not understand the implications of app permissions, thus allowing inappropriate uses of data by app companies or third parties. In an analysis of app metadata, permissions, and ad network use by Android apps, the team found that while surface level cues like permissions, app category, and whether the app shows ads might help users judge whether information is shared appropriately, these cues aren\u2019t conclusive. Additional analysis of \"deep\" cues, such as the presence of trackers that send user data to ad networks, can help identify misbehaving applications but are mostly invisible to users.\n\n\nIn addition to healthcare and Android apps, the research team applied these heuristics to other applications where better understanding of information flow can result in greater privacy. Mapping how information flows within an app - whether in healthcare, smartphone apps, organizations, or other contexts - can help software engineers and app designers better identify potential threats to user privacy, in areas where those flows are unexpected, obscured, or inappropriate. \n\n\nEffective application of these heuristics can result apps in that respect users\u2019 privacy while allowing information to flow in ways that are necessary for the app to function. In the case of smartphone apps, this means ensuring that the permissions that apps request paint an accurate picture of what the app will do with a user\u2019s data. In addition, this research points to the need to make \"deep\" cues, like the presence of ad network trackers, visible to users. \n\n\nFinally, this project advances the field of privacy and technology ethics by applying the theory of privacy as contextual integrity to real-world applications. This theory allows us to analyze data types and flows in a way that helps to identify which aspects of privacy should be emphasized when designing a software system or app, and which are less realistic. The frameworks and methodologies published as part of this project will help developers and researchers draw on contextual integrity for PbD and privacy research.\n\n\t\t\t\t\tLast Modified: 12/19/2019\n\n\t\t\t\t\tSubmitted by: Helen Nissenbaum"
 }
}