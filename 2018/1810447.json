{
 "awd_id": "1810447",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Online Learning-based Real-time Control of Unknown Autonomous Systems",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Anthony Kuh",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 329999.0,
 "awd_amount": 329999.0,
 "awd_min_amd_letter_date": "2018-08-15",
 "awd_max_amd_letter_date": "2018-08-15",
 "awd_abstract_narration": "Many emerging autonomous systems, e.g., robots in unstructured environments, are too complex to be accurately modeled. There are unknown model parameters, partial state observations, or a drift in system characteristics. This makes the problem of system identification and control quite challenging. Real-time adaptation is needed for optimal and resilient operation. It is well-known that the classical adaptive con-trol approach of system identification and `certainty equivalent' control in the feedback-loop doesn't work. \r\n\r\nIn this project, we introduce a new paradigm of 'Learning-to-Control' unknown Autonomous Systems based on the newly developing approach of Thompson/Posterior sampling-based online learning. We will focus on discrete state space models of Markov decision processes (MDPs). We will first develop a posterior sampling-inspired algorithms for online learning-based control with real-time adaptation for MDP models with partial observation of the system state. We note that such approaches may be inter-preted to provide just the right amount of randomization for optimally trading off exploration and exploi-tation that is needed for online learning of the optimal policy at the fastest rate. We will then extend this to the setting where the system parameter may be varying or drifting with time. We will then develop such algorithms for more relevant but also more complicated system models - stochastic hybrid systems, that have both discrete and continuous states. The developed algorithms will be extensively validated in sim-ulation experiments in the classical control and robotics environments in OpenAI Gym. \r\n\r\nThe intellectual merit of the research lies in its contribution to the 'Science of Autonomous Systems' by development of foundations of online learning-based real-time control and adaptation for autonomous systems by addressing fundamental questions about separation of parameter estimation, state estima-tion and control for various stochastic system models, particularly when model parameters must be learnt from data. The broader impacts will include impact on the smart grid, autonomous robotics, and medical CPS devices via dissemination of research results, training of a female PhD student and a K-12 STEM outreach effort.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rahul",
   "pi_last_name": "Jain",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Rahul A Jain",
   "pi_email_addr": "rahul.jain@usc.edu",
   "nsf_id": "000515860",
   "pi_start_date": "2018-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S. Flower St",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900890001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "CA37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760700",
   "pgm_ele_name": "EPCN-Energy-Power-Ctrl-Netwrks"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1653",
   "pgm_ref_txt": "Adaptive & intelligent systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 329999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we investigated design of online learning-based control algorithms for unknown autonomous systems. Our investigation revealed that posterior sampling as a general principle for online learning algorithm design is very powerful, though in terms of theoretically-supported algorithm, as well as its empirical performance. We designed such algorithms for a number of settings: average-criterion infinite-horizon Markov decision processes (MDPs), random-horizon MDP models, Partially-Observed MDPs, Constrained MDPs, Zero-Sum Stochastic MDPs, etc. essentially covering the entire gamut of models relevant for autonomous systems with autonomy to learn and adapt upon deployment. The results are also significant from the point of view of learning efficiency which is an issue with data-driven control and offline reinforcement learning methods. The work lays the foundation for better optimal adaptive control algorithms (that are almost data-efficient) for use on real systems than what are currently available. &nbsp;The project also touched upon other related aspects such as optimal control with temporal logic contraints, including when the system is partially observed. This is important for application such as autonomous driving where hard safety constraints are needed to be satisfied by the controllers but given the stochastic nature of the model, this can be quite difficult. We also explored practical control design by first use of deep learning methods for control synthesis, and then distilling such controllers into computationally simpler and interpretable forms.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/02/2023<br>\n\t\t\t\t\tModified by: Rahul&nbsp;Jain</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this project, we investigated design of online learning-based control algorithms for unknown autonomous systems. Our investigation revealed that posterior sampling as a general principle for online learning algorithm design is very powerful, though in terms of theoretically-supported algorithm, as well as its empirical performance. We designed such algorithms for a number of settings: average-criterion infinite-horizon Markov decision processes (MDPs), random-horizon MDP models, Partially-Observed MDPs, Constrained MDPs, Zero-Sum Stochastic MDPs, etc. essentially covering the entire gamut of models relevant for autonomous systems with autonomy to learn and adapt upon deployment. The results are also significant from the point of view of learning efficiency which is an issue with data-driven control and offline reinforcement learning methods. The work lays the foundation for better optimal adaptive control algorithms (that are almost data-efficient) for use on real systems than what are currently available.  The project also touched upon other related aspects such as optimal control with temporal logic contraints, including when the system is partially observed. This is important for application such as autonomous driving where hard safety constraints are needed to be satisfied by the controllers but given the stochastic nature of the model, this can be quite difficult. We also explored practical control design by first use of deep learning methods for control synthesis, and then distilling such controllers into computationally simpler and interpretable forms. \n\n\t\t\t\t\tLast Modified: 02/02/2023\n\n\t\t\t\t\tSubmitted by: Rahul Jain"
 }
}