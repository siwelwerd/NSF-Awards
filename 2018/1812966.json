{
 "awd_id": "1812966",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: Wearable Fingertip Haptic Devices for Virtual and Augmented Reality: Design, Control, and Predictive Tracking",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 326662.0,
 "awd_amount": 334662.0,
 "awd_min_amd_letter_date": "2018-07-30",
 "awd_max_amd_letter_date": "2019-04-18",
 "awd_abstract_narration": "In the real world, people rely heavily on haptic (force and tactile) feedback to manipulate and explore objects.  However, in virtual and augmented reality environments, haptic sensations must be artificially generated.  Advances in both sensing and feedback are needed to create more realistic virtual scenarios for human interaction, education, and training.  This research will implement wearable fingertip haptic feedback systems that enhance human perception and task performance in virtual and augmented reality, and will provide the field of computer-mediated haptics with important new design and rendering insights for a promising form of wearable haptic device.  Successful development of fingertip haptic devices and improved tracking methods will lower the cost of integrating haptic technology into virtual reality, and yield more realistic and compelling virtual experiences.  Effective haptic feedback systems for virtual reality utilizing the developed technologies will have broad impact by improving human health and well-being through a myriad of applications such as training for critical tasks like surgery and defusing of explosive ordnance, tactile communication to enable design and e-commerce, and immersion in virtual worlds for enhanced education and training.  Project outcomes will be disseminated through demonstrations of educational applications, expansion of an online course on haptics, and publicly available software and data.  Outreach programs, public lab tours, and mentoring of female and minority graduate students, undergraduates, and high school students will broaden participation of underrepresented groups in engineering.\r\n\r\nThis project has three main technical components.  First, instrumented objects will be developed that measure force interactions and integrate that data with external measurements of finger pose.  Second, new wearable haptic devices will be developed that use a combination of local kinesthetic feedback and skin deformation to provide realistic haptic feedback with minimal encumbrance.  Modular haptic device designs will facilitate rapid testing of various design choices.  Then the basic design will be optimized in terms of control strategy, degrees of freedom, skin contact conditions, and finger grounding to best match the real-world grasping and manipulation data collected earlier.  Third, predictive tracking algorithms will be developed for grasping based on characteristic behaviors observed in real-world grasping and manipulation data.  All of the haptic devices and tracking methods will be evaluated both in virtual and in augmented reality environments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Allison",
   "pi_last_name": "Okamura",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Allison M Okamura",
   "pi_email_addr": "aokamura@stanford.edu",
   "nsf_id": "000443791",
   "pi_start_date": "2018-07-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "416 Escondido Mall, Stanford University, Bldg. 550-107",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052203",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 326662.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In the real world, people rely heavily on haptic (force and tactile) feedback to manipulate and explore objects. However, in virtual reality (VR) augmented reality (AR) environments, haptic sensations must be artificially created. Advancements in both sensing and feedback are needed to create more realistic virtual scenarios for human interaction, learning and training, and entertainment. We developed wearable fingertip haptic feedback systems that enhance human perception and task performance in VR and AR, with the long-term goal of improving the effectiveness of these environments in a wide variety of applications that improve human health and well-being.</p>\n<p>This research resulted in two main innovations: (1) measurement-driven design of fingertip haptic devices that are appropriate for the large workspace and free movements enabled and expected by immersive VR and AR environments, and (2) predictive tracking to provide accurate and precise measurements of human movements based on relevant data on human grasping and manipulation of real-world objects. We developed new wearable haptic devices that use a combination of local kinesthetic feedback and skin deformation to provide realistic haptic feedback with minimal encumbrance. Modular haptic device designs facilitated rapid testing of various design choices. Then the basic designs were optimized in terms of control strategy, degrees of freedom, skin contact conditions, and finger grounding to best match the real-world grasping and manipulation. A particularly exciting outcome is the development of novel origami and fully 3D-printed fingertip skin deformation haptic devices that are lightweight and provide 4 degrees of freedom of haptic feedback for increased realism and immersion. We also developed predictive tracking algorithms for grasping based on characteristic behaviors observed in the real-world grasping and manipulation data. We integrated finger sensing with world tracking and showed that the enhanced measurements can be used to drive haptic rendering.&nbsp;</p>\n<p>This work is the result of a collaboration between faculty and students at Stanford University and the University of South Florida.</p>\n<p><strong>Intellectual Merit:</strong>&nbsp;The outcomes of this project provide the field of computer-mediated haptics with important new design and rendering insights for a promising form of wearable haptic devices. Successful development of fingertip haptic devices and improved tracking methods lower the cost of integrating haptic technology into virtual reality and yield more realistic and compelling virtual experiences. In addition, improved virtual and augmented reality environments will enhance human collaboration, training, and learning.&nbsp;</p>\n<p><strong>Broader Impacts:</strong>&nbsp;Effective haptic feedback systems for virtual reality will improve human health and well-being through applications such as training for critical tasks like surgery and explosive ordnance defusing, tactile communication to enable design and e-commerce, and immersion in virtual worlds for enhanced education and entertainment. Our results have been disseminated through demonstrations of educational applications, expansion of an online course on haptics, and software and data made publicly available. Outreach programs, public lab tours, and mentoring of female and minority graduate students, undergraduates, and high school students have broadened participation of underrepresented groups in engineering.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2022<br>\n\t\t\t\t\tModified by: Allison&nbsp;M&nbsp;Okamura</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859816776_2.3D-PrintedFingertipHapticDevice--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859816776_2.3D-PrintedFingertipHapticDevice--rgov-800width.jpg\" title=\"3D-Printed Fingertip Haptic Device\"><img src=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859816776_2.3D-PrintedFingertipHapticDevice--rgov-66x44.jpg\" alt=\"3D-Printed Fingertip Haptic Device\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">FingerPrint is a novel soft wearable 4-Degree-of-Freedom haptic device for the fingertip that is 3-D printed and uses pneumatic actuation to generate pressure, linear and rotational shear, and vibration stimuli on the skin.</div>\n<div class=\"imageCredit\">Zhenishbek Zhakypov</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Allison&nbsp;M&nbsp;Okamura</div>\n<div class=\"imageTitle\">3D-Printed Fingertip Haptic Device</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859758652_1.OrigamiFingertipHapticDevice--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859758652_1.OrigamiFingertipHapticDevice--rgov-800width.jpg\" title=\"Origami Fingertip Haptic Device\"><img src=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859758652_1.OrigamiFingertipHapticDevice--rgov-66x44.jpg\" alt=\"Origami Fingertip Haptic Device\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Different views of a 4-DoF origami cutaneous haptic device, which can apply normal, x- and y-shear, and torsion haptic feedback. (a) Device with the tactor visible. (b) The motor base. (c) The complete 4-DoF device.</div>\n<div class=\"imageCredit\">Sophia Williams</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Allison&nbsp;M&nbsp;Okamura</div>\n<div class=\"imageTitle\">Origami Fingertip Haptic Device</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859896813_3.HapticVoxel(Hoxel)Device--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859896813_3.HapticVoxel(Hoxel)Device--rgov-800width.jpg\" title=\"Haptic Voxel (Hoxel) Device\"><img src=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859896813_3.HapticVoxel(Hoxel)Device--rgov-66x44.jpg\" alt=\"Haptic Voxel (Hoxel) Device\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A novel, fully 3D-printed soft wearable haptic voxel (hoxel) display for distributed and rich skin tactile stimulation on the wrist. Each hoxel has four actuators that move a tactor in 3-DoF for skin lateral shear and pressure. Multiple hoxels stimulate twist, stretch, and squeeze.</div>\n<div class=\"imageCredit\">Zhenishbek Zhakypov</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Allison&nbsp;M&nbsp;Okamura</div>\n<div class=\"imageTitle\">Haptic Voxel (Hoxel) Device</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859965850_4.Interaction-ExpectationModel--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859965850_4.Interaction-ExpectationModel--rgov-800width.jpg\" title=\"Interaction-Expectation Model\"><img src=\"/por/images/Reports/POR/2022/1812966/1812966_10563398_1669859965850_4.Interaction-ExpectationModel--rgov-66x44.jpg\" alt=\"Interaction-Expectation Model\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An Interaction-Expectation model uses current hand pose to predict future contact between the fingers and objects in the environment. Leveraging this information reduces or eliminates latency in providing haptic feedback during virtual object interaction.</div>\n<div class=\"imageCredit\">Millie Salvato</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Allison&nbsp;M&nbsp;Okamura</div>\n<div class=\"imageTitle\">Interaction-Expectation Model</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIn the real world, people rely heavily on haptic (force and tactile) feedback to manipulate and explore objects. However, in virtual reality (VR) augmented reality (AR) environments, haptic sensations must be artificially created. Advancements in both sensing and feedback are needed to create more realistic virtual scenarios for human interaction, learning and training, and entertainment. We developed wearable fingertip haptic feedback systems that enhance human perception and task performance in VR and AR, with the long-term goal of improving the effectiveness of these environments in a wide variety of applications that improve human health and well-being.\n\nThis research resulted in two main innovations: (1) measurement-driven design of fingertip haptic devices that are appropriate for the large workspace and free movements enabled and expected by immersive VR and AR environments, and (2) predictive tracking to provide accurate and precise measurements of human movements based on relevant data on human grasping and manipulation of real-world objects. We developed new wearable haptic devices that use a combination of local kinesthetic feedback and skin deformation to provide realistic haptic feedback with minimal encumbrance. Modular haptic device designs facilitated rapid testing of various design choices. Then the basic designs were optimized in terms of control strategy, degrees of freedom, skin contact conditions, and finger grounding to best match the real-world grasping and manipulation. A particularly exciting outcome is the development of novel origami and fully 3D-printed fingertip skin deformation haptic devices that are lightweight and provide 4 degrees of freedom of haptic feedback for increased realism and immersion. We also developed predictive tracking algorithms for grasping based on characteristic behaviors observed in the real-world grasping and manipulation data. We integrated finger sensing with world tracking and showed that the enhanced measurements can be used to drive haptic rendering. \n\nThis work is the result of a collaboration between faculty and students at Stanford University and the University of South Florida.\n\nIntellectual Merit: The outcomes of this project provide the field of computer-mediated haptics with important new design and rendering insights for a promising form of wearable haptic devices. Successful development of fingertip haptic devices and improved tracking methods lower the cost of integrating haptic technology into virtual reality and yield more realistic and compelling virtual experiences. In addition, improved virtual and augmented reality environments will enhance human collaboration, training, and learning. \n\nBroader Impacts: Effective haptic feedback systems for virtual reality will improve human health and well-being through applications such as training for critical tasks like surgery and explosive ordnance defusing, tactile communication to enable design and e-commerce, and immersion in virtual worlds for enhanced education and entertainment. Our results have been disseminated through demonstrations of educational applications, expansion of an online course on haptics, and software and data made publicly available. Outreach programs, public lab tours, and mentoring of female and minority graduate students, undergraduates, and high school students have broadened participation of underrepresented groups in engineering.\n\n \n\n\t\t\t\t\tLast Modified: 11/30/2022\n\n\t\t\t\t\tSubmitted by: Allison M Okamura"
 }
}