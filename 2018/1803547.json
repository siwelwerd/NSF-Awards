{
 "awd_id": "1803547",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Fellowship Award",
 "awd_titl_txt": "PostDoctoral Research Fellowship",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924878",
 "po_email": "adpollin@nsf.gov",
 "po_sign_block_name": "Andrew Pollington",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2018-03-28",
 "awd_max_amd_letter_date": "2018-03-28",
 "awd_abstract_narration": "This award is made as part of the FY 2018 Mathematical Sciences Postdoctoral Research Fellowships Program. Each of the fellowships supports a research and training project at a host institution in the mathematical sciences, including applications to other disciplines, under the mentorship of a sponsoring scientist. The title of the project for this fellowship to David Rolnick is \"Theory of artificial and biological neural networks.\" The host institution for the fellowship is\u00a0University of Pennsylvania, and the sponsoring scientist is Konrad Kording.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Rolnick",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Rolnick",
   "pi_email_addr": "",
   "nsf_id": "000759954",
   "pi_start_date": "2018-03-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rolnick                 David",
  "inst_street_address": "",
  "inst_street_address_2": "",
  "inst_city_name": "Cambridge",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "",
  "inst_zip_code": "021394744",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "",
  "org_prnt_uei_num": "",
  "org_uei_num": ""
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": null,
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046270",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "060Y00",
   "pgm_ele_name": "Workforce (MSPRF) MathSciPDFel"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9219",
   "pgm_ref_txt": "POSTDOCTORAL FELLOWSHIPS IN MATH SCIENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern artificial intelligence (AI) has come to rely on algorithms using neural networks. Artificial neural networks, also known as deep learning, are loosely inspired by biological neural networks in the brain, but the reasons for the success of such algorithms in solving tough problems remain poorly understood. In the project, we consider the behavior of neural networks from a mathematical, computer scientific, and neuroscientific perspective.<strong>&nbsp;</strong></p>\n<p>&nbsp;</p>\n<p class=\"Default\">We first worked mathematically to quantify how the structure of an artificial neural network affects its ability to fit data. We showed that a significant gap exists between the functions that a deep neural network can express and the functions that it is likely to express in practice (either at initialization or after training). That is, the theoretical upper bound on the complexity of the expressible functions is considerably above (in some cases, exponentially above) the complexity of the functions expressed in practice. We then leveraged these theoretical results to show that it is possible to reverse engineer a deep neural network by observing its output, a longstanding question in both machine learning and neuroscience.</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\">So-called \"modules\" have been observed at many levels throughout biological neural networks - e.g. the visual or auditory centers of the brain - representing structural patterns of organization that confer the ability to solve subproblems hierarchically. We considered the widely proposed idea that modules exist in artificial neural networks also, analyzing the natural modularity of such systems and designing methods for integrating neural network approaches with a more modular problem-solving structure.</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\">The structure of biological neural networks remains very poorly understood. We introduced a scalable deep learning-based pipeline for reconstructing the fine-scale structure of the brain from electron microscopy imagery. We also considered how innovations in generative models, such as variational autoencoders, may be useful in understanding and modeling biological neural networks.</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\">Finally, we used the computational capabilities of biological neural networks to inspire the design of artificial neural network algorithms. We showed that a simple approach of \"replaying\" past experiences, long proposed as a mechanism of memory consolidation in the biological brain, can significantly improve retention of information in deep reinforcement learning algorithms. Our results suggest takeaways both for neuroscience (by affirming the effectiveness of this approach) and machine learning (by proposing an effective algorithm for continual learning).</p>\n<p>&nbsp;</p>\n<p>Overall, we hope our work will help guide the development of more effective and innovative artificial intelligence, as well as rigorous guarantees for when these algorithms will and will not work. Algorithms used by the brain can potentially be implemented in artificial neural networks to solve problems hitherto unapproachable by artificial intelligence. Conversely, insights into artificial neural networks can inform our understanding of the constraints and principles underlying biological algorithms.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/27/2023<br>\n\t\t\t\t\tModified by: David&nbsp;Rolnick</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModern artificial intelligence (AI) has come to rely on algorithms using neural networks. Artificial neural networks, also known as deep learning, are loosely inspired by biological neural networks in the brain, but the reasons for the success of such algorithms in solving tough problems remain poorly understood. In the project, we consider the behavior of neural networks from a mathematical, computer scientific, and neuroscientific perspective. \n\n \nWe first worked mathematically to quantify how the structure of an artificial neural network affects its ability to fit data. We showed that a significant gap exists between the functions that a deep neural network can express and the functions that it is likely to express in practice (either at initialization or after training). That is, the theoretical upper bound on the complexity of the expressible functions is considerably above (in some cases, exponentially above) the complexity of the functions expressed in practice. We then leveraged these theoretical results to show that it is possible to reverse engineer a deep neural network by observing its output, a longstanding question in both machine learning and neuroscience.\n \nSo-called \"modules\" have been observed at many levels throughout biological neural networks - e.g. the visual or auditory centers of the brain - representing structural patterns of organization that confer the ability to solve subproblems hierarchically. We considered the widely proposed idea that modules exist in artificial neural networks also, analyzing the natural modularity of such systems and designing methods for integrating neural network approaches with a more modular problem-solving structure.\n \nThe structure of biological neural networks remains very poorly understood. We introduced a scalable deep learning-based pipeline for reconstructing the fine-scale structure of the brain from electron microscopy imagery. We also considered how innovations in generative models, such as variational autoencoders, may be useful in understanding and modeling biological neural networks.\n \nFinally, we used the computational capabilities of biological neural networks to inspire the design of artificial neural network algorithms. We showed that a simple approach of \"replaying\" past experiences, long proposed as a mechanism of memory consolidation in the biological brain, can significantly improve retention of information in deep reinforcement learning algorithms. Our results suggest takeaways both for neuroscience (by affirming the effectiveness of this approach) and machine learning (by proposing an effective algorithm for continual learning).\n\n \n\nOverall, we hope our work will help guide the development of more effective and innovative artificial intelligence, as well as rigorous guarantees for when these algorithms will and will not work. Algorithms used by the brain can potentially be implemented in artificial neural networks to solve problems hitherto unapproachable by artificial intelligence. Conversely, insights into artificial neural networks can inform our understanding of the constraints and principles underlying biological algorithms.\n\n\t\t\t\t\tLast Modified: 10/27/2023\n\n\t\t\t\t\tSubmitted by: David Rolnick"
 }
}