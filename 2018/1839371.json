{
 "awd_id": "1839371",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TRIPODS+X:RES: Safe Imitation Learning for Robotics",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Christopher Stark",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2018-09-10",
 "awd_max_amd_letter_date": "2018-09-10",
 "awd_abstract_narration": "Learning algorithms are now pervasively deployed in robotic systems. However, safe learning procedures with high-probability theoretical guarantees on the acceptability of the predictions have been far less studied, especially for robotic systems trained with data collected from experts and making decisions sequentially. The PIs shall bring together ideas and techniques from statistics, machine learning, and mathematical optimization, to design the next generation of imitation learning approaches with provable safety guarantees for several classes of modern robots that interact with humans.   \r\n\r\nThe project aims to: (1) develop new formulations of safe imitation learning; (2) design fast learning algorithms with theoretical guarantees on safety; (3) explore trust-building processes for beneficial human-machine interaction. The research approaches will be evaluated in several robotic problem domains, including robotic manipulation and wheeled mobile robot navigation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zaid",
   "pi_last_name": "Harchaoui",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zaid Harchaoui",
   "pi_email_addr": "zaid@uw.edu",
   "nsf_id": "000730146",
   "pi_start_date": "2018-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Maryam",
   "pi_last_name": "Fazel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Maryam Fazel",
   "pi_email_addr": "mfazel@uw.edu",
   "nsf_id": "000488519",
   "pi_start_date": "2018-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sham",
   "pi_last_name": "Kakade",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sham Kakade",
   "pi_email_addr": "sham@seas.harvard.edu",
   "nsf_id": "000553028",
   "pi_start_date": "2018-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Siddhartha",
   "pi_last_name": "Srinivasa",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Siddhartha Srinivasa",
   "pi_email_addr": "siddh@cs.washington.edu",
   "nsf_id": "000747916",
   "pi_start_date": "2018-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn AVE NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981950001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "041Y00",
   "pgm_ele_name": "TRIPODS Transdisciplinary Rese"
  },
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "047Z",
   "pgm_ref_txt": "TRIPODS Phase 1"
  },
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 600000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-45e28964-7fff-0332-7cf6-886b6208d287\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The project made advances on the foundations of risk-sensitive, safe, supervised learning and interactive learning. Risk-sensitive objectives allow learning systems to interpolate between optimizing average-case performance, as in empirical risk minimization, and worst-case performance on a task. We developed stochastic algorithms to optimize these quantities by characterizing their subdifferential and addressing challenges such as biasedness of subgradient estimates and non-smoothness of the objective. We showed theoretically and experimentally that out-of-the-box approaches such as stochastic subgradient and dual averaging can be outperformed by carefully thought stochastic algorithms.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The project made advances on the foundations of nonlinear control algorithms using iterative quadratic linearization. While popular in many domains, these approaches are still lacking solid modern theoretical guarantees .We showed that global convergence guarantees can be ensured provided that the linearized discrete time dynamics are surjective and costs on the state variables are strongly convex. We presented how the surjectivity of the linearized dynamics can be ensured by appropriate discretization schemes given a feedback linearization scheme. We obtained complexity bounds of algorithms based on linear quadratic approximations through the lens of generalized Gauss-Newton methods. Our analysis uncovered several convergence phases for regularized generalized Gauss-Newton algorithms.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The project made advances on imitation learning with multi-modal demonstrations. Instead of attempting to learn all modes, we argued that in many tasks it is sufficient to imitate any one of them. We showed that the state-of-the-art methods such behavior cloning, due to their choice of loss function, often incorrectly interpolate between such modes. Our key insight was to minimize the right divergence between the learner and the expert state-action distributions, namely the reverse KL divergence or I-projection. We proposed a general imitation learning framework for estimating and minimizing any f-Divergence. By plugging in different divergences, we were able to recover existing algorithms such as Behavior Cloning (Kullback-Leibler), GAIL (Jensen Shannon) and Dagger (Total Variation). Empirical results showed that our approximate I-projection technique is able to imitate multi-modal behaviors more reliably than GAIL and behavior cloning.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The project addressed representational and distributional conditions that permit provable sample-efficient offline reinforcement learning. The main results showed that even if we have realizability and our off-policy data has good coverage over all features , then any algorithm still requires a number of offline samples that is exponential in the problem horizon in order to non-trivially estimate the value of any given policy. The results highlighted that sample-efficient, offline policy evaluation is simply not possible unless significantly stronger conditions hold; such conditions include either having low distribution shift or significantly stronger representational conditions beyond realizability.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The project made advances on the estimation of policy gradients for continuous-time systems with known dynamics. By reframing policy learning in continuous-time, we show that it is possible to construct a more efficient and accurate gradient estimator. The standard back-propagation through time estimator (BPTT) computes exact gradients for a crude discretization of the continuous-time system. In contrast, we do continuous-time gradients in the original system. With the explicit goal of estimating continuous-time gradients, we were able to discretize adaptively and construct a more efficient policy gradient estimator which we call the Continuous-Time Policy Gradient (CTPG). We showed that replacing BPTT policy gradients with more efficient CTPG estimates results in faster and more robust learning in a variety of control tasks and simulators.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/01/2023<br>\n\t\t\t\t\tModified by: Zaid&nbsp;Harchaoui</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The project made advances on the foundations of risk-sensitive, safe, supervised learning and interactive learning. Risk-sensitive objectives allow learning systems to interpolate between optimizing average-case performance, as in empirical risk minimization, and worst-case performance on a task. We developed stochastic algorithms to optimize these quantities by characterizing their subdifferential and addressing challenges such as biasedness of subgradient estimates and non-smoothness of the objective. We showed theoretically and experimentally that out-of-the-box approaches such as stochastic subgradient and dual averaging can be outperformed by carefully thought stochastic algorithms. \n\n \nThe project made advances on the foundations of nonlinear control algorithms using iterative quadratic linearization. While popular in many domains, these approaches are still lacking solid modern theoretical guarantees .We showed that global convergence guarantees can be ensured provided that the linearized discrete time dynamics are surjective and costs on the state variables are strongly convex. We presented how the surjectivity of the linearized dynamics can be ensured by appropriate discretization schemes given a feedback linearization scheme. We obtained complexity bounds of algorithms based on linear quadratic approximations through the lens of generalized Gauss-Newton methods. Our analysis uncovered several convergence phases for regularized generalized Gauss-Newton algorithms.\n\n \nThe project made advances on imitation learning with multi-modal demonstrations. Instead of attempting to learn all modes, we argued that in many tasks it is sufficient to imitate any one of them. We showed that the state-of-the-art methods such behavior cloning, due to their choice of loss function, often incorrectly interpolate between such modes. Our key insight was to minimize the right divergence between the learner and the expert state-action distributions, namely the reverse KL divergence or I-projection. We proposed a general imitation learning framework for estimating and minimizing any f-Divergence. By plugging in different divergences, we were able to recover existing algorithms such as Behavior Cloning (Kullback-Leibler), GAIL (Jensen Shannon) and Dagger (Total Variation). Empirical results showed that our approximate I-projection technique is able to imitate multi-modal behaviors more reliably than GAIL and behavior cloning.\n\n \nThe project addressed representational and distributional conditions that permit provable sample-efficient offline reinforcement learning. The main results showed that even if we have realizability and our off-policy data has good coverage over all features , then any algorithm still requires a number of offline samples that is exponential in the problem horizon in order to non-trivially estimate the value of any given policy. The results highlighted that sample-efficient, offline policy evaluation is simply not possible unless significantly stronger conditions hold; such conditions include either having low distribution shift or significantly stronger representational conditions beyond realizability.\n\n \nThe project made advances on the estimation of policy gradients for continuous-time systems with known dynamics. By reframing policy learning in continuous-time, we show that it is possible to construct a more efficient and accurate gradient estimator. The standard back-propagation through time estimator (BPTT) computes exact gradients for a crude discretization of the continuous-time system. In contrast, we do continuous-time gradients in the original system. With the explicit goal of estimating continuous-time gradients, we were able to discretize adaptively and construct a more efficient policy gradient estimator which we call the Continuous-Time Policy Gradient (CTPG). We showed that replacing BPTT policy gradients with more efficient CTPG estimates results in faster and more robust learning in a variety of control tasks and simulators.\n\n\t\t\t\t\tLast Modified: 03/01/2023\n\n\t\t\t\t\tSubmitted by: Zaid Harchaoui"
 }
}