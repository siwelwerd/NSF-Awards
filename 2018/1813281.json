{
 "awd_id": "1813281",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Enhancing Data Analysis Strategies with Mixed-Initiative Visual Analytics",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 491371.0,
 "awd_amount": 491371.0,
 "awd_min_amd_letter_date": "2018-08-09",
 "awd_max_amd_letter_date": "2018-08-09",
 "awd_abstract_narration": "People make important decisions based on data every day, from simple choices such as which restaurant to dine at when visiting a city, to important and complex decisions in healthcare about which course of treatment to pursue, and even decisions that impact national security and policy. Visual analytic systems play a critical role in these decision-making processes. They allow people to interact with their data and analytic models to view different perspectives of data and gain insights. This interactive data analysis process consists of people incrementally guiding analytic models to produce alternate views of the data in support of their tasks. In most cases, such human-in-the-loop processes have successful, insightful outcomes. However, the cognitive sciences tell us that people can exhibit innate biased behavior. As a result, their data analysis behaviors and strategies may suffer. Ultimately, this could lead to decisions made from incomplete information and limited perspectives on how the data can be interpreted. Biased analysis processes can lead to biased results and misinformation. This project will perform fundamental research to discover how to detect such potential bias and develop visual analytic systems that mitigate it.  It will also produce educational impacts for graduate and undergraduate students from groups underrepresented in STEM fields, in part through outreach workshops with instructors from minority-serving institutions and historically black colleges and universities to help them integrate visual analytics and general data literacy learning objectives into course curricula. \r\n\r\nThe proposed multi-disciplinary research will develop techniques that enhance mixed-initiative visual analytic analysis processes by intervening and providing guidance when necessary. To accomplish this goal, three primary lines of research are proposed. First, the team will develop and evaluate computational metrics to detect poor and potentially biased analysis strategies from user interaction patterns and system parameters. These metrics consist of probabilistic computational models that take into consideration metrics such as data coverage over the duration of the data exploration. Second, the team will develop and study different visual analytic system designs to guide and improve people's analysis processes. Each prototype will give people guidance using the metrics, but display information to users via different interface designs (e.g., dialog boxes, visual overlays of coverage, etc.) They will be developed, evaluated, and made available via publications and open-source code. Third, the studies proposed will generate empirical results and design guidelines for future mixed-initiative visual analytic systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Endert",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander Endert",
   "pi_email_addr": "endert@gatech.edu",
   "nsf_id": "000678221",
   "pi_start_date": "2018-08-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 491371.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-81c8c0e6-7fff-62c8-6cbb-016e7eda3485\"> </span></p>\n<p dir=\"ltr\"><span>People are biased. This bias ranges from perceptual biases that allow for rapid detection of movement which helps us detect danger, to cognitive biases that lead to flawed decision-making hindered by stereotypes, implicit biases, and other harmful behavior people are often unaware of. When people analyze data, there is a danger that these harmful biases influence the way data is analyzed, and the decisions made as a result.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Visual analytic systems play a critical role in these decision-making processes. They allow people to interact with their data and analytic models to view different perspectives of data and gain insights. This interactive data analysis process consists of people incrementally guiding analytic models to produce alternate views of the data in support of their tasks. In most cases, such human-in-the-loop processes have successful, insightful outcomes. However, the cognitive sciences tell us that people can exhibit innate biased behavior. As a result, their data analysis behaviors and strategies may suffer. Ultimately, this could lead to decisions made from incomplete information and limited perspectives on how the data can be interpreted. Biased analysis processes can lead to biased results and misinformation. The goal of this research project is to visual develop data analysis tools that detect and warn people that they may be exhibiting such behaviors.&nbsp;</span></p>\n<p dir=\"ltr\"><span>To accomplish this goal, three complementary lines of research were performed. First, our research team developed computational metrics to detect poor and potentially biased analysis strategies from user interaction patterns and system parameters. These metrics listen to the user interactions performed during data analysis and detect when they resemble specific biases. Second, our team created a variety of different visual analytic system designs to guide and improve people's analysis processes. Each prototype guided people, but displayed information to users via different interface designs (e.g., dialog boxes, visual overlays of coverage, etc.) Third, these tools were applied and tested in specific domains where biases can have detrimental effects on the outcomes of the data analysis performed.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We have created these metrics that detect exploration bias, and validated that they do in fact detect when someone is exhibiting bias. We implemented those detection metrics into two different visual analytic prototypes. Our studies of these prototypes compared to baseline tools show that they make people more aware of implicit biases during data analysis, and often result in people considering more alternatives and more fairly considering multiple aspects of the data. We have tested this in domains including political policy decision-making, university admissions, and report dashboard creation. In summary, we are excited about the impact these findings can have on future visual data analysis tools as the community works to promote fairness when analyzing data.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/09/2023<br>\n\t\t\t\t\tModified by: Alexander&nbsp;Endert</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1813281/1813281_10568285_1691605189272_Screenshot2023-08-09at2.13.06PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1813281/1813281_10568285_1691605189272_Screenshot2023-08-09at2.13.06PM--rgov-800width.jpg\" title=\"Bias Metrics\"><img src=\"/por/images/Reports/POR/2023/1813281/1813281_10568285_1691605189272_Screenshot2023-08-09at2.13.06PM--rgov-66x44.jpg\" alt=\"Bias Metrics\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Bias metrics showing detected user bias (shown in blue) compared to a target baseline (proportional, equal, or custom). In this case, we show how analyzing a movie dataset can result in biases on specific data attributes. These were detected from user interaction with a visualization.</div>\n<div class=\"imageCredit\">Arpit Narechania, Alex Endert</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Alexander&nbsp;Endert</div>\n<div class=\"imageTitle\">Bias Metrics</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1813281/1813281_10568285_1691605278066_Screenshot2023-08-09at2.12.25PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1813281/1813281_10568285_1691605278066_Screenshot2023-08-09at2.12.25PM--rgov-800width.jpg\" title=\"Bias mitigation process\"><img src=\"/por/images/Reports/POR/2023/1813281/1813281_10568285_1691605278066_Screenshot2023-08-09at2.12.25PM--rgov-66x44.jpg\" alt=\"Bias mitigation process\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Showing the process of how a system shows a bias, allows interaction to mitigate the bias, and ultimately results in an outcome where data was more fairly assessed.</div>\n<div class=\"imageCredit\">Arpit Narechania, Alex Endert</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Alexander&nbsp;Endert</div>\n<div class=\"imageTitle\">Bias mitigation process</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nPeople are biased. This bias ranges from perceptual biases that allow for rapid detection of movement which helps us detect danger, to cognitive biases that lead to flawed decision-making hindered by stereotypes, implicit biases, and other harmful behavior people are often unaware of. When people analyze data, there is a danger that these harmful biases influence the way data is analyzed, and the decisions made as a result. \nVisual analytic systems play a critical role in these decision-making processes. They allow people to interact with their data and analytic models to view different perspectives of data and gain insights. This interactive data analysis process consists of people incrementally guiding analytic models to produce alternate views of the data in support of their tasks. In most cases, such human-in-the-loop processes have successful, insightful outcomes. However, the cognitive sciences tell us that people can exhibit innate biased behavior. As a result, their data analysis behaviors and strategies may suffer. Ultimately, this could lead to decisions made from incomplete information and limited perspectives on how the data can be interpreted. Biased analysis processes can lead to biased results and misinformation. The goal of this research project is to visual develop data analysis tools that detect and warn people that they may be exhibiting such behaviors. \nTo accomplish this goal, three complementary lines of research were performed. First, our research team developed computational metrics to detect poor and potentially biased analysis strategies from user interaction patterns and system parameters. These metrics listen to the user interactions performed during data analysis and detect when they resemble specific biases. Second, our team created a variety of different visual analytic system designs to guide and improve people's analysis processes. Each prototype guided people, but displayed information to users via different interface designs (e.g., dialog boxes, visual overlays of coverage, etc.) Third, these tools were applied and tested in specific domains where biases can have detrimental effects on the outcomes of the data analysis performed. \nWe have created these metrics that detect exploration bias, and validated that they do in fact detect when someone is exhibiting bias. We implemented those detection metrics into two different visual analytic prototypes. Our studies of these prototypes compared to baseline tools show that they make people more aware of implicit biases during data analysis, and often result in people considering more alternatives and more fairly considering multiple aspects of the data. We have tested this in domains including political policy decision-making, university admissions, and report dashboard creation. In summary, we are excited about the impact these findings can have on future visual data analysis tools as the community works to promote fairness when analyzing data.\n\n \n\n\t\t\t\t\tLast Modified: 08/09/2023\n\n\t\t\t\t\tSubmitted by: Alexander Endert"
 }
}