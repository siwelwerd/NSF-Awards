{
 "awd_id": "1751143",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Reliable and Efficient Data Encoding for Extreme-Scale Simulation and Analysis",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922625",
 "po_email": "jjli@nsf.gov",
 "po_sign_block_name": "Juan Li",
 "awd_eff_date": "2018-04-15",
 "awd_exp_date": "2024-03-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-04-09",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Transformative research in science and engineering to address challenges of our time, such as designing new combustion systems, depends on progressively sophisticated computational models and simulations that operate on high performance computing systems.  These simulations and analyses are increasingly constrained by the massive volumes of data that they must use, generate, and analyze.  To manage this enormous amount of data, this project explores innovative mechanisms to optimize the performance of these simulations by reducing data movement and maximizing the use of computing power, while minimizing errors and information loss.  Such performance improvements support NSF's mission to advance emerging, data-intensive science discovery and contribute to solving the world's most pressing and complex contemporary science and engineering problems.  This project implements comprehensive outreach and education to train the next-generation of professional workers and researchers in the latest computing architectures and programming methodologies, and provides rich opportunities for student engagement, research, and employment.  It leverages multiple campus and national resources and implements proven, research-based interventions to attract, retain, and educate female and underrepresented minority populations in computer engineering, which furthers the US national goal of increased participation in engineering. \r\n\r\nThe research goal of this project is to adapt techniques and formats for compressing video data to the investigation of novel data encoding and decoding schemes to optimize data movement and computation in data-intensive simulation and analyses.  Innovative new mechanisms have the potential to efficiently reduce the volume of data generated and transferred while also enabling rapid execution of various analysis kernels using compressed data, and permitting seamless scaling of their performance on current and future extreme-scale platforms.  The research objectives are to investigate data encoding/decoding of scientific datasets and harness encoded data, employ and scale encoded datasets seamlessly within current extreme-scale scientific workflows, and optimize machine learning and data mining algorithms with the goal of maximizing the use of computing power while minimizing errors.  These new mechanisms are applied to an evaluation framework and validated on multiple extreme-scale data-driven scientific applications, including climate, multiphysics, and fluid dynamics.  This approach is expected to transform data representation and encoding while incurring minimal disturbance to existing applications, responding to the trends in hardware architecture and dataset characteristics.  It is anticipated to improve the overall performance of computational scientists' workloads by reducing defensive and productive I/O costs, respectively, up to 100x and 200x data reduction spatially and temporally, potentially resulting in up to an overall 50x I/O cost improvement.  The project leverages multiple collaborations in order to establish the governing principles for system co-design and scalable system software layers for better data encoding within world-class computational infrastructures.  This project strengthens the University of Massachusetts Lowell computer engineering curriculum, broadens participation in computer engineering, and creates a collaborative, interdisciplinary research program geared toward exploiting ever-evolving computing paradigms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Seung Woo",
   "pi_last_name": "Son",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Seung Woo Son",
   "pi_email_addr": "SeungWoo_Son@uml.edu",
   "nsf_id": "000688110",
   "pi_start_date": "2018-04-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Lowell",
  "inst_street_address": "220 PAWTUCKET ST STE 400",
  "inst_street_address_2": "",
  "inst_city_name": "LOWELL",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "9789344170",
  "inst_zip_code": "018543573",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MA03",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS LOWELL",
  "org_prnt_uei_num": "",
  "org_uei_num": "LTNVSTJ3R6D5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Lowell",
  "perf_str_addr": "One University Avenue",
  "perf_city_name": "Lowell",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "018542827",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "MA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "104500",
   "pgm_ele_name": "CAREER: FACULTY EARLY CAR DEV"
  },
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 334234.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 165766.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to&nbsp;adapt techniques and algorithms for compressing image/video data to investigate novel data encoding and decoding schemes for floating-point data. The goal is to optimize data movement and computation in data-intensive applications in scientific and other relevant domains, potentially advancing these fields.&nbsp;</p>\n<p>In this project, we investigated novel lossy data encoding mechanisms for numerical datasets. We also extensively evaluated the impact of lossy compressions on various data workflows, considering various metrics such as compression ratios, the quality of reconstructed data, and other measurements depending on applications or use cases. We summarize the project achievements as follows.</p>\n<p>(1) We developed a novel lossy compression framework called DCTZ. DCTZ is a lossy compression software for numeric datasets in double (or single) precision. It is based on a block decomposition mechanism combined with well-known discrete cosine transformation (DCT) on floating-point datasets. It also uses an adaptive quantization with two specific task-oriented quantizers: guaranteed user-defined error bounds and higher compression ratios. Using real-world datasets from several numerical solvers (in physics, climate, and fluid dynamics), DCTZ achieves 3x-38x compression ratios while guaranteeing user-specified error bound, showing comparable performance with state-of-the-art lossy compressors, SZ, and ZFP. The overarching idea of the DCTZ framework is published in the 2019 IEEE International Conference on Massive Storage Systems and Technology (MSST).</p>\n<p>(2) The DCTZ framework is extended in several ways. We implemented a knee-detection algorithm that improves the distortion in peak signal-to-noise ratio by 2.45 dB. We also extended the DCTZ by a bit-efficient quantizer with a unique ordering mechanism in the quantization table and variable encoding indexing mechanism, improving DCTZ by 1.38x compression ratios.&nbsp;&nbsp;The application of the knee detection algorithm and bit-efficient quantizer was published at the IEEE High Performance Extreme Computing Conference (HPEC) in 2019 and 2020, respectively. Additional improvement to the DCTZ framework was made through a preconditioning method based on level offsetting and scaling to control the magnitude of input of the DCTZ framework, thereby enforcing stricter error bounds. We also explored a novel quantization process to support a fixed-PSNR mode for a transform-based lossy compressor called DCTZ-F. Our experimental results show that DCTZ-F can generate up to three times the compression ratio than SZ with fixed-PSNR mode while meeting the user-defined PSNR.&nbsp;</p>\n<p>(2)&nbsp;We developed a novel lossy compression technique, DPZ, based on multi-stage feature extractions, a commonly employed step in IR (information retrieval). Unlike the prior works where the compression is either done by predicting or bit-plane encoding, this work focuses on preserving the key data content from each stage to the maximum extent, ultimately elevating the compression ratio. With the application of DCT, principal component analysis (PCA), and quantization, DPZ obtains the dominant features with the least amount of bits possible. Specifically, a knee-point detection and an explained variance variation method are designed for finding optimal tradeoffs. DPZ also employs a sampling strategy to reduce computational overhead and estimate compressibility and parameters before compression. We evaluate the performance of DPZ using real-world scientific datasets. Experiments demonstrate that DPZ achieves superior compression ratios through multi-stage retrievals and outperforms SZ and ZFP at medium to high accuracy on most of the evaluated datasets. This work is published at the 2021 IEEE International Conference on Cluster Computing. We developed another multi-stage compressor but with an emphasis on effective anomaly detection in image datasets where autoencoder (AE) and PCA are combined to reduce dimension size significantly while incorporating a knee detection algorithm to automatically select optimal dimension size that maximizes the anomaly detection accuracy.</p>\n<p>(3) We extensively evaluated the impact of lossy compression on various workflows, including checkpoint/restart in large-scale scientific simulations and several downstream applications in multiple domains. The initial DCTZ framework published at 2019 MSST demonstrated that error-bounded lossy compressors (including ours and other state-of-the-art compressors, SZ and ZFP) provide viable reconstructed data for various checkpoint/restart scenarios in the FLASH application. In our published work at the 2020 IEEE Conference on Big Data, we showed that the underpinning algorithm used in DCTZ (i.e., discrete transforms in conjunction with finding minimum or optimal coefficients containing maximum information and quantization/encoding) could approximate (compress) and successfully detect 64%-94% of anomalies in IoT (internet of things) datasets using only 1.9% of the entire data. In our later work, we evaluated the impact of the reconstructed data through lossy compressions or feature/dimension reduction on various other scenarios, including lossy predictive models for accurate classification algorithms, lossy posture classification models using flexible pressure sensors, and anomaly detection in scientific datasets using sparse representation.</p><br>\n<p>\n Last Modified: 07/30/2024<br>\nModified by: Seung Woo&nbsp;Son</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1751143/1751143_10538122_1722307533014_NSF_CAREER_project_outcome_figure1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/1751143/1751143_10538122_1722307533014_NSF_CAREER_project_outcome_figure1--rgov-800width.jpg\" title=\"DCTZ compression framework\"><img src=\"/por/images/Reports/POR/2024/1751143/1751143_10538122_1722307533014_NSF_CAREER_project_outcome_figure1--rgov-66x44.jpg\" alt=\"DCTZ compression framework\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">DCT-based lossy compression framework for numerical datasets</div>\n<div class=\"imageCredit\">Jialing Zhang</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Seung Woo&nbsp;Son\n<div class=\"imageTitle\">DCTZ compression framework</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project aims toadapt techniques and algorithms for compressing image/video data to investigate novel data encoding and decoding schemes for floating-point data. The goal is to optimize data movement and computation in data-intensive applications in scientific and other relevant domains, potentially advancing these fields.\n\n\nIn this project, we investigated novel lossy data encoding mechanisms for numerical datasets. We also extensively evaluated the impact of lossy compressions on various data workflows, considering various metrics such as compression ratios, the quality of reconstructed data, and other measurements depending on applications or use cases. We summarize the project achievements as follows.\n\n\n(1) We developed a novel lossy compression framework called DCTZ. DCTZ is a lossy compression software for numeric datasets in double (or single) precision. It is based on a block decomposition mechanism combined with well-known discrete cosine transformation (DCT) on floating-point datasets. It also uses an adaptive quantization with two specific task-oriented quantizers: guaranteed user-defined error bounds and higher compression ratios. Using real-world datasets from several numerical solvers (in physics, climate, and fluid dynamics), DCTZ achieves 3x-38x compression ratios while guaranteeing user-specified error bound, showing comparable performance with state-of-the-art lossy compressors, SZ, and ZFP. The overarching idea of the DCTZ framework is published in the 2019 IEEE International Conference on Massive Storage Systems and Technology (MSST).\n\n\n(2) The DCTZ framework is extended in several ways. We implemented a knee-detection algorithm that improves the distortion in peak signal-to-noise ratio by 2.45 dB. We also extended the DCTZ by a bit-efficient quantizer with a unique ordering mechanism in the quantization table and variable encoding indexing mechanism, improving DCTZ by 1.38x compression ratios.The application of the knee detection algorithm and bit-efficient quantizer was published at the IEEE High Performance Extreme Computing Conference (HPEC) in 2019 and 2020, respectively. Additional improvement to the DCTZ framework was made through a preconditioning method based on level offsetting and scaling to control the magnitude of input of the DCTZ framework, thereby enforcing stricter error bounds. We also explored a novel quantization process to support a fixed-PSNR mode for a transform-based lossy compressor called DCTZ-F. Our experimental results show that DCTZ-F can generate up to three times the compression ratio than SZ with fixed-PSNR mode while meeting the user-defined PSNR.\n\n\n(2)We developed a novel lossy compression technique, DPZ, based on multi-stage feature extractions, a commonly employed step in IR (information retrieval). Unlike the prior works where the compression is either done by predicting or bit-plane encoding, this work focuses on preserving the key data content from each stage to the maximum extent, ultimately elevating the compression ratio. With the application of DCT, principal component analysis (PCA), and quantization, DPZ obtains the dominant features with the least amount of bits possible. Specifically, a knee-point detection and an explained variance variation method are designed for finding optimal tradeoffs. DPZ also employs a sampling strategy to reduce computational overhead and estimate compressibility and parameters before compression. We evaluate the performance of DPZ using real-world scientific datasets. Experiments demonstrate that DPZ achieves superior compression ratios through multi-stage retrievals and outperforms SZ and ZFP at medium to high accuracy on most of the evaluated datasets. This work is published at the 2021 IEEE International Conference on Cluster Computing. We developed another multi-stage compressor but with an emphasis on effective anomaly detection in image datasets where autoencoder (AE) and PCA are combined to reduce dimension size significantly while incorporating a knee detection algorithm to automatically select optimal dimension size that maximizes the anomaly detection accuracy.\n\n\n(3) We extensively evaluated the impact of lossy compression on various workflows, including checkpoint/restart in large-scale scientific simulations and several downstream applications in multiple domains. The initial DCTZ framework published at 2019 MSST demonstrated that error-bounded lossy compressors (including ours and other state-of-the-art compressors, SZ and ZFP) provide viable reconstructed data for various checkpoint/restart scenarios in the FLASH application. In our published work at the 2020 IEEE Conference on Big Data, we showed that the underpinning algorithm used in DCTZ (i.e., discrete transforms in conjunction with finding minimum or optimal coefficients containing maximum information and quantization/encoding) could approximate (compress) and successfully detect 64%-94% of anomalies in IoT (internet of things) datasets using only 1.9% of the entire data. In our later work, we evaluated the impact of the reconstructed data through lossy compressions or feature/dimension reduction on various other scenarios, including lossy predictive models for accurate classification algorithms, lossy posture classification models using flexible pressure sensors, and anomaly detection in scientific datasets using sparse representation.\t\t\t\t\tLast Modified: 07/30/2024\n\n\t\t\t\t\tSubmitted by: Seung WooSon\n"
 }
}