{
 "awd_id": "1836936",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FMitF: Collaborative Research: Synergies between Program Synthesis and Neural Learning of Graph Structures",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922585",
 "po_email": "pprabhak@nsf.gov",
 "po_sign_block_name": "Pavithra Prabhakar",
 "awd_eff_date": "2019-01-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 466000.0,
 "awd_min_amd_letter_date": "2018-09-04",
 "awd_max_amd_letter_date": "2019-06-11",
 "awd_abstract_narration": "A challenging problem in a diverse and growing body of applications concerns automatically generating computer programs that satisfy desired functional requirements. Two promising and complementary approaches that have recently emerged to address this problem are program synthesis and neural learning. This project aims to synergistically combine the two approaches to improve the productivity of programmers and the quality of software. The project also aims to train graduate students at the intersection of formal methods and machine learning, engage undergraduate students in research through internships, and disseminate results in the form of publicly available course materials and open-source software artifacts.\r\n\r\nProgram synthesis ensures that the generated program is correct with respect to a logical specification. Moreover, users can easily guide the synthesizer away from an undesired program and towards a desired one, by changing the specification. On the other hand, neural learning can handle user requirements that are impossible to provide via a logical specification -- a fact highlighted by the success of neural networks in domains such as natural language processing, computer vision, and robotics. Moreover, neural networks scale extremely well, by virtue of their ability to learn latent patterns that repeat across different programs. This project builds upon recent progress in program synthesis by developing novel learning-based mechanisms that enable flexible specifications, richer verifiers, and scalable solvers. In the realm of machine learning, it enables deep neural networks to provide correctness guarantees that are typically required when reasoning about rich structured data. In doing so, it develops novel architectures and methodologies for representation learning, reinforcement learning, and learning with limited data.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mayur",
   "pi_last_name": "Naik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mayur Naik",
   "pi_email_addr": "mhnaik@cis.upenn.edu",
   "nsf_id": "000601297",
   "pi_start_date": "2018-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of the University of Pennsylvania",
  "perf_str_addr": "3451 Walnut Street; 5th fl",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046205",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "094Y00",
   "pgm_ele_name": "FMitF: Formal Methods in the F"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "8206",
   "pgm_ref_txt": "Formal Methods and Verification"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 450000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span class=\"s1\">This project set out to investigate a fundamental and challenging problem: how can we automate reasoning over graph-structured data accurately and efficiently?<span>&nbsp;&nbsp;</span>This problem has wide-ranging practical implications, from enabling the masses of end-users to automate repetitive tasks without learning how to program computers, to enhancing the productivity of the masses of programmers by automating complex tasks such as debugging, program repair, and verification.</span></p>\n<p class=\"p2\">Two pre-dominant approaches have been proposed to address this problem by different communities: <em>program synthesis</em> from the formal methods community and <em>neural learning</em> from the machine learning community. Each of these approaches has significant strengths and limitations.&nbsp; Program synthesis ensures that the generated program is correct with respect to a logical specification. Moreover, users can easily guide the synthesizer away from an undesired program and towards a desired one, by changing the specification. On the other hand, neural learning can handle user requirements that are impossible to provide via a logical specification -- a fact highlighted by the success of neural networks in domains such as natural language processing, computer vision, and robotics. Moreover, neural networks scale extremely well, by virtue of their ability to learn latent patterns that repeat across different programs.&nbsp; This project developed new frameworks to combine these approaches in a synergistic manner and demonstrated state-of-the-art performance on a wide range of practical and challenging applications.</p>\n<p class=\"p2\">In the first half of the project, we focussed on applications in improving programmer productivity and software reliability, and developed the first frameworks that sytematically and effectively combined program synthesis and neural learning for three important domains: 1) bug localization and program repair (depicted in adjoining figure); 2) loop invariant generation for program verification; and 3) type inference for binary security analysis. All of these works employed <em>Graph Neural Networks (GNNs)</em> -- a neural network architecture that is capable of learning rich syntactic and sematic patterns in programs. We demonstrated state-of-the-art performance in all three domains by leveraging rich data- and control-flow information extracted statically from programs under test.&nbsp; These works addressed unique challenges compared to previous learning tasks in domains with graph-structured data, most notably learning in data-, compute-, and sample-efficient settings.&nbsp; For instance, in the domain of loop invariant generation, our approach Code2Inv demonstrated for the first time that it was possible to learn complex loop invariants without <em>any</em> ground truth solutions; instead, it was sufficient to use a binary objective function and an extremely sparse reward that is given by an automated theorem prover only after the complete loop invariant is proposed.</p>\n<p class=\"p2\">In the second half of the project, we built upon the lessons learnt from the aforementioned software engineering applications to encompass arbitrary of <em>neurosymbolic</em>&nbsp;applications.&nbsp; For instance, Code2Inv could be viewed as a neurosymbolic application wherein the neural network is a GNN that predicts a candidate loop invariant, and the classical program is an automated theorem prover.&nbsp; The result was Scallop, a general-purpose neurosymbolic programming language and compiler toolchain.&nbsp; We evaluated Scallop on challenging applications in software engineering, computer vision, natural language processing, planning, and knowledge graph querying.&nbsp; Our evaluation demonstrated that Scallop is expressive and yields solutions of comparable, and often times superior, accuracy than state-of-the-art end-to-end neural approaches.&nbsp; Additionally, Scallop's solutions had better runtime and data efficiency, interpretability, and generalizability.</p>\n<p class=\"p2\">The results of the project were disseminated via a series of publications at top-tier conferences in a variety of specializations including programming languages, machine learning, and security.&nbsp; Hands-on tutorials were delivered at different conferences and summer schools.&nbsp; All software artifacts and data curated or generated in the project was open-sourced and made freely available.&nbsp; Eight PhD students, including four women, were trained in conducting research.&nbsp; Six of these students graduated during the course of the project and went on to pursue tenure-track faculty positions in academia or researcher positions in industry.&nbsp; Finally, key techniques and tools developed in the project were incorporated into a graduate course on software analysis that is taught in both on-campus and online degree programs and is taken by over 300 students each year.</p><br>\n<p>\n Last Modified: 06/05/2024<br>\nModified by: Mayur&nbsp;Naik</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1836936/1836936_10579503_1717640470339_hoppity--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1836936/1836936_10579503_1717640470339_hoppity--rgov-800width.png\" title=\"Architecture of Hoppity\"><img src=\"/por/images/Reports/POR/2024/1836936/1836936_10579503_1717640470339_hoppity--rgov-66x44.png\" alt=\"Architecture of Hoppity\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Automated and end-to-end bug localization and program repair using our approach, called Hoppity, which was published in ICLR 2020 as a Spotlight Paper.</div>\n<div class=\"imageCredit\">Authors of ICLR 2020 paper on Hoppity</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Mayur&nbsp;Naik\n<div class=\"imageTitle\">Architecture of Hoppity</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project set out to investigate a fundamental and challenging problem: how can we automate reasoning over graph-structured data accurately and efficiently?This problem has wide-ranging practical implications, from enabling the masses of end-users to automate repetitive tasks without learning how to program computers, to enhancing the productivity of the masses of programmers by automating complex tasks such as debugging, program repair, and verification.\n\n\nTwo pre-dominant approaches have been proposed to address this problem by different communities: program synthesis from the formal methods community and neural learning from the machine learning community. Each of these approaches has significant strengths and limitations. Program synthesis ensures that the generated program is correct with respect to a logical specification. Moreover, users can easily guide the synthesizer away from an undesired program and towards a desired one, by changing the specification. On the other hand, neural learning can handle user requirements that are impossible to provide via a logical specification -- a fact highlighted by the success of neural networks in domains such as natural language processing, computer vision, and robotics. Moreover, neural networks scale extremely well, by virtue of their ability to learn latent patterns that repeat across different programs. This project developed new frameworks to combine these approaches in a synergistic manner and demonstrated state-of-the-art performance on a wide range of practical and challenging applications.\n\n\nIn the first half of the project, we focussed on applications in improving programmer productivity and software reliability, and developed the first frameworks that sytematically and effectively combined program synthesis and neural learning for three important domains: 1) bug localization and program repair (depicted in adjoining figure); 2) loop invariant generation for program verification; and 3) type inference for binary security analysis. All of these works employed Graph Neural Networks (GNNs) -- a neural network architecture that is capable of learning rich syntactic and sematic patterns in programs. We demonstrated state-of-the-art performance in all three domains by leveraging rich data- and control-flow information extracted statically from programs under test. These works addressed unique challenges compared to previous learning tasks in domains with graph-structured data, most notably learning in data-, compute-, and sample-efficient settings. For instance, in the domain of loop invariant generation, our approach Code2Inv demonstrated for the first time that it was possible to learn complex loop invariants without any ground truth solutions; instead, it was sufficient to use a binary objective function and an extremely sparse reward that is given by an automated theorem prover only after the complete loop invariant is proposed.\n\n\nIn the second half of the project, we built upon the lessons learnt from the aforementioned software engineering applications to encompass arbitrary of neurosymbolicapplications. For instance, Code2Inv could be viewed as a neurosymbolic application wherein the neural network is a GNN that predicts a candidate loop invariant, and the classical program is an automated theorem prover. The result was Scallop, a general-purpose neurosymbolic programming language and compiler toolchain. We evaluated Scallop on challenging applications in software engineering, computer vision, natural language processing, planning, and knowledge graph querying. Our evaluation demonstrated that Scallop is expressive and yields solutions of comparable, and often times superior, accuracy than state-of-the-art end-to-end neural approaches. Additionally, Scallop's solutions had better runtime and data efficiency, interpretability, and generalizability.\n\n\nThe results of the project were disseminated via a series of publications at top-tier conferences in a variety of specializations including programming languages, machine learning, and security. Hands-on tutorials were delivered at different conferences and summer schools. All software artifacts and data curated or generated in the project was open-sourced and made freely available. Eight PhD students, including four women, were trained in conducting research. Six of these students graduated during the course of the project and went on to pursue tenure-track faculty positions in academia or researcher positions in industry. Finally, key techniques and tools developed in the project were incorporated into a graduate course on software analysis that is taught in both on-campus and online degree programs and is taken by over 300 students each year.\t\t\t\t\tLast Modified: 06/05/2024\n\n\t\t\t\t\tSubmitted by: MayurNaik\n"
 }
}