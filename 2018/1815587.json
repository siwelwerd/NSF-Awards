{
 "awd_id": "1815587",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: Validating and Communicating Model-Based Approaches for Data Visualization Ability Assessment",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Balakrishnan Prabhakaran",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 248337.0,
 "awd_amount": 248337.0,
 "awd_min_amd_letter_date": "2018-08-04",
 "awd_max_amd_letter_date": "2018-09-13",
 "awd_abstract_narration": "People are encountering graphs, charts, and other visual representations of data now more than ever before. Yet creators of these visualizations currently must reason with sparse and conflicting evidence on how well people can read the visualizations they publish.  Current guidelines do not take into account the possibility that different people have different strengths and weaknesses when interpreting visual data. This project will use studies of visualization effectiveness to inform our understanding of the abilities and biases of viewers, both individually and collectively.  To do this, the project team will use a combination of experiments, statistical modeling, and interview studies to both challenge long-standing assumptions about visualization effectiveness, and to lay a foundation for future experiments that account for differences in visualization reading ability. The work will also support a broader educational goal of using robust statistical modeling techniques in experimentation, through course modules that can be integrated into existing data visualization courses, and through outreach activities that allow individuals to see how well they perform visualization tasks compared to others who have taken the experiments.\r\n\r\nThis work seeks to answer three primary research questions. The first is to determine the extent to which individuals differ in their ability to perform basic tasks with data visualizations, through large-scale crowdsourced experiments that use transparent statistical methodologies to establish individual differences in data visualization performance. The second question evaluates the relationship between low-level visualization performance and higher-level assessments such as visualization literacy and cognitive abilities, recruiting both expert and novice populations to evaluate the extent to which these hypothesized measures of visualization literacy correlate with each other. The third question determines how alternative ways of presenting visualization experiment results shape the design recommendations researchers and designers draw from them, through a comparative evaluation of longstanding ways of presenting visualization experiment results, and by designing new ways of presenting results that may lead to more mature interpretation of experiment results by broader visualization community. The work will provide new perspectives on visualization literacy by augmenting chart reading experiments with novel measures of visualization ability, and by studying how creators currently make use of existing visualization design guidelines in their design process.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lane",
   "pi_last_name": "Harrison",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lane Harrison",
   "pi_email_addr": "ltharrison@wpi.edu",
   "nsf_id": "000710896",
   "pi_start_date": "2018-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Worcester Polytechnic Institute",
  "inst_street_address": "100 INSTITUTE RD",
  "inst_street_address_2": "",
  "inst_city_name": "WORCESTER",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "5088315000",
  "inst_zip_code": "016092280",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "WORCESTER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJNQME41NBU4"
 },
 "perf_inst": {
  "perf_inst_name": "Worcester Polytechnic Institute",
  "perf_str_addr": "100 Institute Road",
  "perf_city_name": "Worcester",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "016092247",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 248337.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-b845636f-7fff-802a-e498-f1b7e5e91098\"> </span></p>\n<p dir=\"ltr\"><span>As graphs, charts, and other visual representations of data continue to appear in everyday life, it is important that visualization creators understand how well people can read the visualizations they publish. Visualization creators often rely on design guidelines when crafting visualizations, many of which are based on the results of research studies. These research studies, however, do not necessarily take into account the fact that people can vary in their ability to interpret visual data. To bridge this gap, this project explored peoples? biases and abilities in performing basic visualization reading tasks. We used a combination of experiments and statistical modeling to assess how people might differ. Our results show that published design guidelines can overstate the differences in performance between visualization types, since people were found to have different optimal orderings of which charts performed best for them. As part of these efforts, the developed statistical models were shown to more faithfully represent how well individual people perform across a selection of common visualization reading tasks. Such models might be used in future training and education efforts. We also investigated assessment scales used in prior research that correlate with visualization performance. In doing so, we hope to provide the community with more nuanced ways of discussing and showing how people can vary in their abilities to interpret visualizations, how differences between visualization choices might impact people, and a better foundation for assessing visualization literacy moving forward.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/28/2022<br>\n\t\t\t\t\tModified by: Lane&nbsp;Harrison</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nAs graphs, charts, and other visual representations of data continue to appear in everyday life, it is important that visualization creators understand how well people can read the visualizations they publish. Visualization creators often rely on design guidelines when crafting visualizations, many of which are based on the results of research studies. These research studies, however, do not necessarily take into account the fact that people can vary in their ability to interpret visual data. To bridge this gap, this project explored peoples? biases and abilities in performing basic visualization reading tasks. We used a combination of experiments and statistical modeling to assess how people might differ. Our results show that published design guidelines can overstate the differences in performance between visualization types, since people were found to have different optimal orderings of which charts performed best for them. As part of these efforts, the developed statistical models were shown to more faithfully represent how well individual people perform across a selection of common visualization reading tasks. Such models might be used in future training and education efforts. We also investigated assessment scales used in prior research that correlate with visualization performance. In doing so, we hope to provide the community with more nuanced ways of discussing and showing how people can vary in their abilities to interpret visualizations, how differences between visualization choices might impact people, and a better foundation for assessing visualization literacy moving forward.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/28/2022\n\n\t\t\t\t\tSubmitted by: Lane Harrison"
 }
}