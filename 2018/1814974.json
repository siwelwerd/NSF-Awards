{
 "awd_id": "1814974",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Data Structure Designs and Implementations for Modern Multicore Architectures and Applications",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922594",
 "po_email": "kkaravan@nsf.gov",
 "po_sign_block_name": "Karen Karavanic",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-06-29",
 "awd_max_amd_letter_date": "2018-06-29",
 "awd_abstract_narration": "Data Structures are the conventional way to store data managed by applications and system software. Developing data structures that function correctly even in the presence of multiple simultaneous operations has been one of the most important topics in the last decade. This project moves research on data structures ahead by targeting the way these data structures interact with the underlying hardware infrastructure. The project proposes innovative software designs and features that allow data structure implementations to improve performance when deployed on modern servers equipped with hundreds of processors, and to meet new application necessities.\r\n\r\nThis project investigates data structure designs and implementations that exploit the hardware organization of modern multicore architectures, where memory is partitioned in multiple zones and each zone is subject to an access latency that is influenced by the source and destination zone (also known as Non-Uniform Memory Access, or NUMA, architectures), to improve application performance. The core intuition is to divide the data structure into independent layers, avoid synchronization across them, and distributed them among NUMA zones. The proposal identifies four research tasks, ranging from hardware optimizations to scalable bulk operations over the data within the data structures.\r\n\r\nThe outcomes of this project include delivering to programmers, industry, and academia a new open-source data structure library that scales its performance in modern multicore architectures and meets emerging application needs. The results of this research will provide a basis for a new module focused on data structures in a course that includes concurrency in its curriculum, and will influence the materials that will be included in a reference textbook on principles and best practices of concurrent programming. Findings of this project will enable new engagement with local communities and public schools to inspire next generation of practitioners.\r\n\r\nAny data produced in the context of this project will be made available to the public and rigorously maintained throughout the duration of the project and beyond. Developed source code will initially be maintained in a private repository (Bitbucket), with periodic public deliverables. After the software becomes stable, it will transition to a public repository (Github) so that a larger open-source community can use and maintain it. Technical reports will be registered into arXiv.org. Published results will be made available at: http://www.cse.lehigh.edu/~palmieri/. The Github account for public code release is the following: \"https://github.com/robertopalmieri/dev\".\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Roberto",
   "pi_last_name": "Palmieri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Roberto Palmieri",
   "pi_email_addr": "palmieri@lehigh.edu",
   "nsf_id": "000762198",
   "pi_start_date": "2018-06-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Spear",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Michael F Spear",
   "pi_email_addr": "spear@cse.lehigh.edu",
   "nsf_id": "000538657",
   "pi_start_date": "2018-06-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Lehigh University",
  "inst_street_address": "526 BRODHEAD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BETHLEHEM",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "6107583021",
  "inst_zip_code": "180153008",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "PA07",
  "org_lgl_bus_name": "LEHIGH UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "E13MDBKHLDB5"
 },
 "perf_inst": {
  "perf_inst_name": "Lehigh University",
  "perf_str_addr": "19 Memorial Drive West",
  "perf_city_name": "Bethlehem",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "180153005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "PA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-ad05a8cd-7fff-bcf7-3c7d-7041e1fb9c2e\"> </span></p>\n<p dir=\"ltr\"><span>This project delivered a number of data structure designs and implementations suitable for large multicore architectures, where computing resources and memory are grouped into so-called non-uniform memory access (NUMA) zones. In these architectures, the latency incurred when a computing thread retrieves information from other NUMA zones is significantly higher than when accessing data from the NUMA zone to which it belongs. In addition, the project delivers a number of data structure designs and implementations that support bulk operations, i.e., range queries and range updates.</span></p>\n<p dir=\"ltr\"><span>Among our new NUMA-aware data structures, a main contribution is NUMASK, a skip list data structure specifically designed for NUMA machines. NUMASK deploys an architecture around a concurrent skip list so that all metadata accesses (e.g., traversals of the skip list index levels) read and write memory blocks allocated in the NUMA zone where the thread executes. We tested NUMASK on a large multicore server with four NUMA zones. Its performance scales for both read-intensive and write-intensive workloads.</span></p>\n<p dir=\"ltr\"><span>Among the data structures developed to support bulk operations, the project delivered three high-performance ordered sets with support for linearizable range operations.</span>&nbsp;</p>\n<ul>\n<li>Bundling is a general technique for designing concurrent data structures that support range query operations. Bundling allows range queries to traverse a path through the data structure that is consistent with an atomic snapshot and that is not influenced by concurrent modifications. We applied Bundling to three data structures, a linked list, a skip list, and a binary tree.&nbsp;</li>\n<li>The skip vector is a high-performance concurrent skip list in which the key innovation is to flatten the index and data layers of the skip list into vectors. The skip vector increases spatial locality, reduces synchronization overhead, avoids pointer chasing, and works well with conservative memory reclamation techniques like Hazard Pointers. The performance benefits of the skip vector over a skip list increase as the amount of data increases.</li>\n<li>We invented a technique for making the linearization of bulk operations, including updates, visible to concurrent elemental operations. With that, operations scale well, keep overhead low, and operate within tight memory bounds. This technique was applied to three different data structures, where we showed that it did not hinder the performance of elemental operations in elemental-only workloads while allowing scalability among concurrent mutating bulk operations.</li>\n</ul>\n<p>The implementations of the above data structures have been released as open-source projects and are available to the academic and industrial communities. Results have been integrated into undergraduate and graduate courses, and material is publicly available for other instructors.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/27/2022<br>\n\t\t\t\t\tModified by: Roberto&nbsp;Palmieri</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis project delivered a number of data structure designs and implementations suitable for large multicore architectures, where computing resources and memory are grouped into so-called non-uniform memory access (NUMA) zones. In these architectures, the latency incurred when a computing thread retrieves information from other NUMA zones is significantly higher than when accessing data from the NUMA zone to which it belongs. In addition, the project delivers a number of data structure designs and implementations that support bulk operations, i.e., range queries and range updates.\nAmong our new NUMA-aware data structures, a main contribution is NUMASK, a skip list data structure specifically designed for NUMA machines. NUMASK deploys an architecture around a concurrent skip list so that all metadata accesses (e.g., traversals of the skip list index levels) read and write memory blocks allocated in the NUMA zone where the thread executes. We tested NUMASK on a large multicore server with four NUMA zones. Its performance scales for both read-intensive and write-intensive workloads.\nAmong the data structures developed to support bulk operations, the project delivered three high-performance ordered sets with support for linearizable range operations. \n\nBundling is a general technique for designing concurrent data structures that support range query operations. Bundling allows range queries to traverse a path through the data structure that is consistent with an atomic snapshot and that is not influenced by concurrent modifications. We applied Bundling to three data structures, a linked list, a skip list, and a binary tree. \nThe skip vector is a high-performance concurrent skip list in which the key innovation is to flatten the index and data layers of the skip list into vectors. The skip vector increases spatial locality, reduces synchronization overhead, avoids pointer chasing, and works well with conservative memory reclamation techniques like Hazard Pointers. The performance benefits of the skip vector over a skip list increase as the amount of data increases.\nWe invented a technique for making the linearization of bulk operations, including updates, visible to concurrent elemental operations. With that, operations scale well, keep overhead low, and operate within tight memory bounds. This technique was applied to three different data structures, where we showed that it did not hinder the performance of elemental operations in elemental-only workloads while allowing scalability among concurrent mutating bulk operations.\n\n\nThe implementations of the above data structures have been released as open-source projects and are available to the academic and industrial communities. Results have been integrated into undergraduate and graduate courses, and material is publicly available for other instructors.\n\n\t\t\t\t\tLast Modified: 10/27/2022\n\n\t\t\t\t\tSubmitted by: Roberto Palmieri"
 }
}