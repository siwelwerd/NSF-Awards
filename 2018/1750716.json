{
 "awd_id": "1750716",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Faster and Smaller Sketches for Bigger Data",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2018-02-01",
 "awd_exp_date": "2024-01-31",
 "tot_intn_awd_amt": 499882.0,
 "awd_amount": 499882.0,
 "awd_min_amd_letter_date": "2018-01-08",
 "awd_max_amd_letter_date": "2021-09-11",
 "awd_abstract_narration": "The advent of new sensing and tracking technologies and expansive use of social networks detailing every walk of life have generated enormous new datasets. The difficulty in dealing with new datasets arises from not only the sheer volume but also the speed required for the analysis and the complex and heterogeneous nature of the data. Underlying these challenges is the need for suitable representations of the data that facilitate efficient computation and are sufficiently compact for storage and communication. This project aims to address fundamental gaps in our understanding of these representations (so-called sketches) and develops both new data representations and new algorithms for massive datasets in a holistic fashion. The project builds on techniques from a wide variety of areas including mathematical analysis, information theory, coding theory, combinatorics, and optimization, and enriches the deep connections among them. Undergraduate and graduate students will be trained and equipped with technical tools to work in these areas. The PI and the students involved in the project will also distill new findings into general audience surveys and give talks at workshops in different technical areas for broadest possible dissemination of information.\r\n\r\nThis project aims to study sketching algorithms by focusing on three main thrusts:(a) Study time complexity of sketches in streaming algorithms in both upper and lower bounds.(b) Develop new forms of sketches for distributed environments. The project focuses on sketching for submodular functions, a popular model for machine learning, computer vision, economics, etc. Problems in these applications are modeled as submodular maximization subject to various types of constraints. (c) Study space complexity of linear sketches in sparse recovery with respect to different recovery guarantees.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Huy",
   "pi_last_name": "Nguyen",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Huy L Nguyen",
   "pi_email_addr": "hu.nguyen@neu.edu",
   "nsf_id": "000711661",
   "pi_start_date": "2018-01-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 99067.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 102039.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 105101.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 193675.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project developed foundational results for streaming algorithms, parallel and distributed algorithms, and optimization algorithms for both convex functions and submodular functions.&nbsp;<br />For streaming algorithms, new algorithms for the classical task of finding the frequent items were developed that can take advantage of machine learning to improve the accuracy and memory usage. To model applications ranging from influence maximization with multiple products to sensor placement with multiple sensor types and online ad allocation, the project introduces new streaming algorithms for maximizing a monotone k-submodular function, which is a common abstraction for all these applications. New streaming algorithms for maximizing a submodular function, another common abstraction for sensor placement and influence maximization, were developed that simultaneously guarantee good accuracy and the privacy of the participants.<br />For parallel and distributed algorithms, new algorithm were developed that can be used for multiple entities to collaboratively develop a common machine learning model that work simultaneously for their different data distributions. Such a situation arises for example when multiple hospitals collaborate to develop a common model that is accurate simultaneously for the different patient distributions of different hospitals. A new parallel algorithm for maximizing a submodular function is designed with a very small number of rounds of evaluation. In situations where the objective function can only be evaluated via costly experiments or surveys, the new algorithm uses only a small number of rounds of experiments and thus significantly speed up the total time.<br />As algorithms are increasingly applied to personal data, privacy and fairness have become important concerns. Clustering algorithms that take data and partition them into groups of similar items or find a small set of representative items is a focus of the project. New clustering algorithms were developed that guarantee both high quality solutions and the privacy of the participants. New algorithms with privacy guarantees for submodular maximization subject to various types of constraints were also developed. In the context of fairness, the project introduces new clustering algorithms that can handle many demographic groups and produce centers that are good representatives and in the right proportion of the underlying population. New fair algorithms for online decision making that simultaneously affect many participants were also developed.<br />In addition to the results above, other efficient optimization algorithms for submodular functions and convex functions were developed. Highlights include fast algorithms for maximizing a submodular functions subject to different types of constraints in the standard single machine computation, and gradient descent methods for a wide variety of scenarios that can adapt to the problem parameters and do not require much tuning of the hyperparameters. A new fast algorithm for computing the maximum weight base of a linear matroid were developed.</p><br>\n<p>\n Last Modified: 04/30/2024<br>\nModified by: Huy&nbsp;L&nbsp;Nguyen</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project developed foundational results for streaming algorithms, parallel and distributed algorithms, and optimization algorithms for both convex functions and submodular functions.\nFor streaming algorithms, new algorithms for the classical task of finding the frequent items were developed that can take advantage of machine learning to improve the accuracy and memory usage. To model applications ranging from influence maximization with multiple products to sensor placement with multiple sensor types and online ad allocation, the project introduces new streaming algorithms for maximizing a monotone k-submodular function, which is a common abstraction for all these applications. New streaming algorithms for maximizing a submodular function, another common abstraction for sensor placement and influence maximization, were developed that simultaneously guarantee good accuracy and the privacy of the participants.\nFor parallel and distributed algorithms, new algorithm were developed that can be used for multiple entities to collaboratively develop a common machine learning model that work simultaneously for their different data distributions. Such a situation arises for example when multiple hospitals collaborate to develop a common model that is accurate simultaneously for the different patient distributions of different hospitals. A new parallel algorithm for maximizing a submodular function is designed with a very small number of rounds of evaluation. In situations where the objective function can only be evaluated via costly experiments or surveys, the new algorithm uses only a small number of rounds of experiments and thus significantly speed up the total time.\nAs algorithms are increasingly applied to personal data, privacy and fairness have become important concerns. Clustering algorithms that take data and partition them into groups of similar items or find a small set of representative items is a focus of the project. New clustering algorithms were developed that guarantee both high quality solutions and the privacy of the participants. New algorithms with privacy guarantees for submodular maximization subject to various types of constraints were also developed. In the context of fairness, the project introduces new clustering algorithms that can handle many demographic groups and produce centers that are good representatives and in the right proportion of the underlying population. New fair algorithms for online decision making that simultaneously affect many participants were also developed.\nIn addition to the results above, other efficient optimization algorithms for submodular functions and convex functions were developed. Highlights include fast algorithms for maximizing a submodular functions subject to different types of constraints in the standard single machine computation, and gradient descent methods for a wide variety of scenarios that can adapt to the problem parameters and do not require much tuning of the hyperparameters. A new fast algorithm for computing the maximum weight base of a linear matroid were developed.\t\t\t\t\tLast Modified: 04/30/2024\n\n\t\t\t\t\tSubmitted by: HuyLNguyen\n"
 }
}