{
 "awd_id": "1754284",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Computational approaches to human spoken word recognition",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2018-03-15",
 "awd_exp_date": "2023-02-28",
 "tot_intn_awd_amt": 602267.0,
 "awd_amount": 654529.0,
 "awd_min_amd_letter_date": "2018-02-28",
 "awd_max_amd_letter_date": "2019-09-18",
 "awd_abstract_narration": "This project addresses one of the grand challenges facing cognitive science -- how humans understand speech. People recognize words far more easily than even the best computer speech recognition systems, even though the actual sounds we hear as consonants and vowels vary greatly depending on context (what sounds come before or after), who is talking, and the setting (a quiet room versus a crowded airport). Most current models of speech recognition cannot handle the huge variability in real speech because they do not operate on the actual speech signal. Also, they do not learn, so they cannot model how people acquire language. This project addresses these challenges by comparing current models of speech recognition to each other and to human capabilities, with the goal of understanding how human speech processing is so robust and flexible.  In addition, simplified \"deep learning\" networks will be developed and evaluated as models of human speech recognition. Deep learning networks are similar to cognitive models in that they learn abstract representations of the data, not task-specific rules or algorithms. These networks have been used to create accurate commercial speech recognition systems. By comparing them to human performance, the investigators may provide new insights into why human speech recognition is so robust. The results of this project will have technical implications (better understanding of human flexibility may aid in improving computer speech recognition) and health implications (better understanding of human speech recognition will aid in developing better interventions for language disorders). The project will also support the training of a postdoctoral researcher and a PhD student, both of whom will develop skills that can be used to contribute to research and development in academia or industry. \r\n\r\nThis project focuses on the development of a \"shallow deep network\" model called \"DeepListener\" that will be compared with the behavior of human listeners. A close match in the millisecond-level behavior of the network (for example, in which words are temporarily confusable with each other) and human performance suggests that human speech processing may emerge from similar principles as those in the model. In preliminary work, DeepListener learned to recognize 93% of 2000 real words (200 words produced by 10 talkers). DeepListener will be evaluated by detailed comparison to standard neural network models of cognitive theories and to human performance. The ways in which DeepListener is similar and dissimilar to human performance and competing models will help to advance scientific theories of human speech recognition. This project will follow emerging standards for open science: experiments will be pre-registered and data and computer code will be made freely and publicly available.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Magnuson",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "James S Magnuson",
   "pi_email_addr": "james.magnuson@uconn.edu",
   "nsf_id": "000065934",
   "pi_start_date": "2018-02-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jay",
   "pi_last_name": "Rueckl",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jay Rueckl",
   "pi_email_addr": "jay.rueckl@uconn.edu",
   "nsf_id": "000688589",
   "pi_start_date": "2018-02-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Connecticut",
  "inst_street_address": "438 WHITNEY RD EXTENSION UNIT 1133",
  "inst_street_address_2": "",
  "inst_city_name": "STORRS",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "8604863622",
  "inst_zip_code": "062699018",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CT02",
  "org_lgl_bus_name": "UNIVERSITY OF CONNECTICUT",
  "org_prnt_uei_num": "",
  "org_uei_num": "WNTPS995QBM7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Connecticut",
  "perf_str_addr": "406 Babbidge Road Unit 1020",
  "perf_city_name": "Storrs",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "062691020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CT02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 396499.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 258030.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The aim of this project was to develop computational models that would be powerful enough to operate on real speech, but simple enough for us to analyze and understand. When we try to explain human cognition and language, we develop theories that make sense of experimental data. The data to be included in our explanations includes behavior, such as how quickly different words can be recognized. It also includes neural responses to speech, such as fast changes in electrical potential associated with how surprising a word is, that can be measured from the scalp using external electrodes (electroencephalography, or EEG) or via more complex neurimaging techniques, such as magentoencephalograpy (MEG) or functional magnetic resonance imaging (fMRI). The most detailed neural data comes from electrocorticography (ECoG), which is recorded from the cortical surface in awake brain surgery patients. Models must first account for behavior, and then the ability of competiting models to simulate neural-level can be compared.&nbsp;</p>\n<p>Since the 1980s, cognitive scientists have been using neural network models of human spoken language processing that can simulate the fine-grained timecourse of human word recognition surprisingly well. These models correctly predict competition between words that have similar beginnings and words that rhyme. However, they are highly simplified and use approximations of natural speech encoded as numbers representing different acoustic features of real speech. While they are neurally inspired, in that they use many simple nodes that function like simplified neurons, they do not provide a strong basis for attempting to simulate neural responses to spoken language. All the same, these models are relatively simple, and have guided detailed explanations of how speech might be processsed at a cogntiive level -- that is, they have helped identify core principles that most theorists now assume are essential parts of the system, no matter how it is implemented neurally. This includes sensitivity to prior probability (how frequently specific words or word combinations occur) and the need for some kind of competitive process to allow the best-matching word in memory to 'rise to the top' among a set of candidate words with partial match to patterns in speech.&nbsp;</p>\n<p>Cognitive scientists have proposed at some future date, when available computing resources become powerful enough, we would return to the challenge of working with actual speech inputs. In light of incredible advances in 'deep neural network' technology for speech (which allows literally billions of people to interact daily with smart phones using their voices), we said that the time had arrived.</p>\n<p>The deep neural network models used to power automatic speech recognition on smart phones does not provide much insight for theories of human speech processing. Those networks are so complex that to understand them, we would have to build simpler models of candidate computations that they might carry out. Rather than studying the models themselves, we asked whether we might be able to add aspects of current deep learning models to simpler \"explainable\" models used previously in cognitive science. Our hope was that by adding only a small amount of complexity, we could create models that would take real speech as input but still be simple enough for us to analyze and understand.&nbsp;</p>\n<p>In our EARSHOT model (Emulation of Auditory Recognition of Speech by Humans Over Time), we borrowed one key element from deep learning: nodes that are called \"long short-term memory\" (LSTM) nodes. These neuron-like nodes have multiple parameters that allow them to learn to flexibly direct greater attention to current or past information. We created EARSHOT, which is what we call a \"minimal network model\" -- one that takes speech inputs and maps them to semantic outputs via just one LSTM layer. This is minimally more complex than previous 'cognitive' models, but many times simpler than the deep learning models used for automatic speech recognition.&nbsp;</p>\n<p>We train EARSHOT to recognize 1000 words produced by 9 talkers, and asses its abilty to generalize to a novel talker. We start with 10 talkers, so we create 10 models; each excludes a different talker. EARSHOT achieves high accuracy on training talkers, and variable accuracy on excluded talkers. Crucially, EARSHOT simulates competition between onset competitors and rhymes approximately as well as simpler models. In addition, even though the model is never trained explicitly to label consonants and vowels in speech, it develops internal representations that are highly similar to human cortex responses to speech measured with ECoG, and also provides new hypotheses for responses that might be found in such data.&nbsp;</p>\n<p>In the era of ChatGPT, we have taken a contrarian approach. Rather than using highly complex models, we are slowly increasing complexity in order to simultaneously simulate behavior and neural data without creating models that are too complex to understand. This approach has the potential to link decades of theoretical progress with cutting edge neural network models.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/26/2023<br>\n\t\t\t\t\tModified by: James&nbsp;S&nbsp;Magnuson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe aim of this project was to develop computational models that would be powerful enough to operate on real speech, but simple enough for us to analyze and understand. When we try to explain human cognition and language, we develop theories that make sense of experimental data. The data to be included in our explanations includes behavior, such as how quickly different words can be recognized. It also includes neural responses to speech, such as fast changes in electrical potential associated with how surprising a word is, that can be measured from the scalp using external electrodes (electroencephalography, or EEG) or via more complex neurimaging techniques, such as magentoencephalograpy (MEG) or functional magnetic resonance imaging (fMRI). The most detailed neural data comes from electrocorticography (ECoG), which is recorded from the cortical surface in awake brain surgery patients. Models must first account for behavior, and then the ability of competiting models to simulate neural-level can be compared. \n\nSince the 1980s, cognitive scientists have been using neural network models of human spoken language processing that can simulate the fine-grained timecourse of human word recognition surprisingly well. These models correctly predict competition between words that have similar beginnings and words that rhyme. However, they are highly simplified and use approximations of natural speech encoded as numbers representing different acoustic features of real speech. While they are neurally inspired, in that they use many simple nodes that function like simplified neurons, they do not provide a strong basis for attempting to simulate neural responses to spoken language. All the same, these models are relatively simple, and have guided detailed explanations of how speech might be processsed at a cogntiive level -- that is, they have helped identify core principles that most theorists now assume are essential parts of the system, no matter how it is implemented neurally. This includes sensitivity to prior probability (how frequently specific words or word combinations occur) and the need for some kind of competitive process to allow the best-matching word in memory to 'rise to the top' among a set of candidate words with partial match to patterns in speech. \n\nCognitive scientists have proposed at some future date, when available computing resources become powerful enough, we would return to the challenge of working with actual speech inputs. In light of incredible advances in 'deep neural network' technology for speech (which allows literally billions of people to interact daily with smart phones using their voices), we said that the time had arrived.\n\nThe deep neural network models used to power automatic speech recognition on smart phones does not provide much insight for theories of human speech processing. Those networks are so complex that to understand them, we would have to build simpler models of candidate computations that they might carry out. Rather than studying the models themselves, we asked whether we might be able to add aspects of current deep learning models to simpler \"explainable\" models used previously in cognitive science. Our hope was that by adding only a small amount of complexity, we could create models that would take real speech as input but still be simple enough for us to analyze and understand. \n\nIn our EARSHOT model (Emulation of Auditory Recognition of Speech by Humans Over Time), we borrowed one key element from deep learning: nodes that are called \"long short-term memory\" (LSTM) nodes. These neuron-like nodes have multiple parameters that allow them to learn to flexibly direct greater attention to current or past information. We created EARSHOT, which is what we call a \"minimal network model\" -- one that takes speech inputs and maps them to semantic outputs via just one LSTM layer. This is minimally more complex than previous 'cognitive' models, but many times simpler than the deep learning models used for automatic speech recognition. \n\nWe train EARSHOT to recognize 1000 words produced by 9 talkers, and asses its abilty to generalize to a novel talker. We start with 10 talkers, so we create 10 models; each excludes a different talker. EARSHOT achieves high accuracy on training talkers, and variable accuracy on excluded talkers. Crucially, EARSHOT simulates competition between onset competitors and rhymes approximately as well as simpler models. In addition, even though the model is never trained explicitly to label consonants and vowels in speech, it develops internal representations that are highly similar to human cortex responses to speech measured with ECoG, and also provides new hypotheses for responses that might be found in such data. \n\nIn the era of ChatGPT, we have taken a contrarian approach. Rather than using highly complex models, we are slowly increasing complexity in order to simultaneously simulate behavior and neural data without creating models that are too complex to understand. This approach has the potential to link decades of theoretical progress with cutting edge neural network models. \n\n\t\t\t\t\tLast Modified: 06/26/2023\n\n\t\t\t\t\tSubmitted by: James S Magnuson"
 }
}