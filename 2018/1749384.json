{
 "awd_id": "1749384",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Multimethod Investigation of Articulatory and Perceptual Constraints on Natural Language Evolution",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tyler Kendall",
 "awd_eff_date": "2018-05-15",
 "awd_exp_date": "2021-10-31",
 "tot_intn_awd_amt": 67554.0,
 "awd_amount": 67554.0,
 "awd_min_amd_letter_date": "2018-05-17",
 "awd_max_amd_letter_date": "2018-05-17",
 "awd_abstract_narration": "Languages change over time, such that the way we speak English now is very different than the speech patterns of elder generations and our distant ancestors. This project will exploit the visual nature of sign languages--where the body parts producing language are highly visible--to determine whether languages change so that they are easier to produce or so that they are easier to understand. In doing so, the project will address fundamental theoretical questions about language change that cannot be addressed by analyzing historical samples of spoken languages. To this end, the researchers will develop computational tools that allow 3D human body poses to be automatically extracted from 2D video. Such tools will be useful for the development of automated sign language recognition, promoting accessibility for deaf and hard-of-hearing people, and for developing automated systems for recognizing and classifying human gestures. The research will involve deaf and hard-of-hearing students, helping to increase diversity in the nation's scientific workforce.\r\n\r\nIt is well documented that sign languages change over time, and it is a commonly held belief that those changes have resulted from successive generations making language easier to perceive. However, most of this evidence has been anecdotal and descriptive and has not quantified changes in the ease of perception and production of ASL over time. The research team will take advantage of the fully visible articulators of sign languages to develop novel pose estimation algorithms that are able to automatically extract information contained in 2D video to create accurate 3D models of articulator movement during language production. The recent birth and rapid evolution of Nicaraguan Sign Language (NSL) has allowed researchers to study language change, from the beginning, on a compressed time-scale. By leveraging an existing NSL database - comprised of 2D videos from four generations of Nicaraguan signers - and utilizing these novel pose estimation algorithms, the researchers will be able to empirically assess the extent to which linguistic changes are driven by perceptual constraints imposed by the human visual system and/or articulatory constraints imposed by the musculoskeletal system. The researchers will also query lexical databases of American Sign Language to test predictions about the perceptual form of modern day ASL, and conduct behavioral studies with deaf and hearing users of ASL to test hypotheses regarding the allocation of visual attention as a result of both deafness and acquisition of a sign language. In doing so, the research will provide valuable information about how the human brain changes the tools we use (in this case, language) and the way that those tools in turn shape the function of the human brain. This will provide a more complex understanding of language change that illuminates the complex interaction between languages and the human beings that use them.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Naomi",
   "pi_last_name": "Caselli",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Naomi Caselli",
   "pi_email_addr": "nkc@bu.edu",
   "nsf_id": "000715234",
   "pi_start_date": "2018-05-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "881 Commonwealth Avenue",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151300",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 67554.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Intellectual Merit</strong>. After generations of being used by people, do languages evolve to make communication as efficient as possible? What is efficient for communication in the visual-gestural modality (i.e., communication in sign languages) is different from the auditory-oral modality (spoken languages). In this study, we asked whether American Sign Language adapted to the to make communication in the signed modality efficient.</p>\n<p>During sign perception, perceivers look almost exclusively at the lower face, rarely looking down at the hands. This means that signs articulated far from the lower face must be perceived through peripheral vision, which has less acuity than central vision. We tested the hypothesis that signs that are more predictable (those that are used frequently or have common handshapes) tend to be produced further from the face where precise visual resolution is not necessary for recognition.</p>\n<p>This study made use of ASL-LEX lexical database for American Sign Language (<a href=\"http://www.asl-lex.org\">www.asl-lex.org</a>), which includes information about how frequently signs are used and how common each handshape is. All of the videos in ASL-LEX were processed using a human pose estimation software (see Figure 1), to identify the coordinates of the hand and face in each sign in ASL-LEX.</p>\n<p>We found that frequent signs with rare handshapes tended to occur closer to the signer's face than frequent signs with common handshapes, and that frequent signs are generally more likely to be articulated further from the face than infrequent signs (see Figure 2). Together these results suggest that the structure of sign language is shaped by the properties of the human visual and motor systems.&nbsp;</p>\n<p><strong>Broader Impacts. </strong>The results of this study suggest that languages may evolve to conform to the needs of the user. In this case, sign languages evolve to match the visual and motor systems of sign language users.&nbsp;</p>\n<p>Recent advances in computer vision and human pose estimation have generated enthusiasm for sign language computing. However, the use cases commonly described in the literature (e.g., sign language recognition and translation) are currently far from feasible. This study is among the first to use human pose estimation in a linguistic study of a sign language. It serves as an example use case in which currently available sign language computing technology can be usefully applied now.</p>\n<p>Deaf people are underrepresented in scientific careers, in part because scientific training is often inaccessible. During this project, the researchers have provided training to several emerging deaf scientists in a research environment that is uniquely accessible in American Sign Language. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/01/2022<br>\n\t\t\t\t\tModified by: Naomi&nbsp;Caselli</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1749384/1749384_10544817_1646146438703_ScreenShot2022-03-01at9.52.34AM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1749384/1749384_10544817_1646146438703_ScreenShot2022-03-01at9.52.34AM--rgov-800width.jpg\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2022/1749384/1749384_10544817_1646146438703_ScreenShot2022-03-01at9.52.34AM--rgov-66x44.jpg\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Human Pose Estimate for the sign HUSBAND from ASL-LEX.</div>\n<div class=\"imageCredit\">ASL-LEX</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Naomi&nbsp;Caselli</div>\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1749384/1749384_10544817_1646146600989_FigureS1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1749384/1749384_10544817_1646146600989_FigureS1--rgov-800width.jpg\" title=\"Figure 2\"><img src=\"/por/images/Reports/POR/2022/1749384/1749384_10544817_1646146600989_FigureS1--rgov-66x44.jpg\" alt=\"Figure 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The distribution of hand positions in the ASL-LEX lexicon according to handshape probability and sign frequency.</div>\n<div class=\"imageCredit\">https://doi.org/10.1016/j.cognition.2022.105040</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Naomi&nbsp;Caselli</div>\n<div class=\"imageTitle\">Figure 2</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIntellectual Merit. After generations of being used by people, do languages evolve to make communication as efficient as possible? What is efficient for communication in the visual-gestural modality (i.e., communication in sign languages) is different from the auditory-oral modality (spoken languages). In this study, we asked whether American Sign Language adapted to the to make communication in the signed modality efficient.\n\nDuring sign perception, perceivers look almost exclusively at the lower face, rarely looking down at the hands. This means that signs articulated far from the lower face must be perceived through peripheral vision, which has less acuity than central vision. We tested the hypothesis that signs that are more predictable (those that are used frequently or have common handshapes) tend to be produced further from the face where precise visual resolution is not necessary for recognition.\n\nThis study made use of ASL-LEX lexical database for American Sign Language (www.asl-lex.org), which includes information about how frequently signs are used and how common each handshape is. All of the videos in ASL-LEX were processed using a human pose estimation software (see Figure 1), to identify the coordinates of the hand and face in each sign in ASL-LEX.\n\nWe found that frequent signs with rare handshapes tended to occur closer to the signer's face than frequent signs with common handshapes, and that frequent signs are generally more likely to be articulated further from the face than infrequent signs (see Figure 2). Together these results suggest that the structure of sign language is shaped by the properties of the human visual and motor systems. \n\nBroader Impacts. The results of this study suggest that languages may evolve to conform to the needs of the user. In this case, sign languages evolve to match the visual and motor systems of sign language users. \n\nRecent advances in computer vision and human pose estimation have generated enthusiasm for sign language computing. However, the use cases commonly described in the literature (e.g., sign language recognition and translation) are currently far from feasible. This study is among the first to use human pose estimation in a linguistic study of a sign language. It serves as an example use case in which currently available sign language computing technology can be usefully applied now.\n\nDeaf people are underrepresented in scientific careers, in part because scientific training is often inaccessible. During this project, the researchers have provided training to several emerging deaf scientists in a research environment that is uniquely accessible in American Sign Language.  \n\n \n\n\t\t\t\t\tLast Modified: 03/01/2022\n\n\t\t\t\t\tSubmitted by: Naomi Caselli"
 }
}