{
 "awd_id": "1819161",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Inexact Optimization Methods for Structured Nonlinear Optimization",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2018-07-15",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2018-07-12",
 "awd_max_amd_letter_date": "2018-07-12",
 "awd_abstract_narration": "New efficient computational algorithms will be developed for solving large-scale optimization problems with particular structure. Structured nonlinear optimization has played a central role in various modern applications ranging from image processing, optimal control to stochastic learning in big data area. The algorithms developed in the project will provide solutions in a more robust and faster way, and will be made publicly available to benefit both optimization and computational data science community. The student supported in this project will have excellent opportunities for interdisciplinary research.\r\n\r\nThe current methods for solving structured optimization problems often need to solve a sequence of subproblems according to the problem structure. This project aims to develop efficient methods and software that allow to solve their subproblems inexactly while still theoretically guarantee the global convergence and maintain the same or almost the same computational complexity of the corresponding methods that require exact solve of the subproblems. In particular, the investigator will develop (I) a framework of inexact alternating direction methods of multipliers for separable convex optimization, where the subproblem is solved to the accuracy relative to the whole problem KKT error; (II) inexact stochastic gradient methods for the composite optimization, which combines the(accelerated) proximal gradient methods and stochastic variance reduction techniques; (III) inexact active-set algorithms for polyhedral constrained nonlinear optimization.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hongchao",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hongchao Zhang",
   "pi_email_addr": "hozhang@math.lsu.edu",
   "nsf_id": "000518125",
   "pi_start_date": "2018-07-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Louisiana State University",
  "inst_street_address": "202 HIMES HALL",
  "inst_street_address_2": "",
  "inst_city_name": "BATON ROUGE",
  "inst_state_code": "LA",
  "inst_state_name": "Louisiana",
  "inst_phone_num": "2255782760",
  "inst_zip_code": "708030001",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "LA06",
  "org_lgl_bus_name": "LOUISIANA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "ECQEYCHRNKJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Louisiana State University & Agricultural and Mechanical College",
  "perf_str_addr": "",
  "perf_city_name": "Baton Rouge",
  "perf_st_code": "LA",
  "perf_st_name": "Louisiana",
  "perf_zip_code": "708032701",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "LA06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>New efficient computational algorithms have been developed for solving large-scale nonlinear optimization problems with particular structure. Structured nonlinear optimization has played a central role in various modern applications ranging from image processing, optimal control to stochastic learning in the big data area. The algorithms developed in the project will provide solutions of these structured optimization problems in a more robust and faster way. The methods that are developed will be implemented in high quality software that will be made publicly available and will be beneficial to both the optimization and computational data science community as well as engineers in the industry.&nbsp; &nbsp;The students supported in this project have excellent opportunities for interdisciplinary research.</p>\n<p>Modern optimization methods for solving structured optimization problems often need to solve a sequence of subproblems according to the problem structure. This project aims to develop efficient methods and software that allow to solve their subproblems inexactly while still theoretically guarantee the global convergence and maintain the same or almost the same computational complexity of the corresponding methods that require exact solution of the subproblems. In this project, the investigator has developed: (I) A framework of inexact alternating direction methods of multipliers for separable convex optimization, in which each subproblem is solved inexactly to an adaptive accuracy based on the summation of the stepsizes for solving the subproblem and an error relative to the whole problem KKT error; (II) Inexact stochastic proximal gradient (ISPG) methods for the composite optimization, where the objective functionis a summation of an average of very large number of smooth convex component functions and a convex, but possibly nonsmooth, function. These stochastic gradient methods combine the (accelerated) proximal gradient methods and stochastic variance reduction techniques to accelerate the convergence; (III) Inexact optimization techniques for solving nonlinear optimizationwith polyhedral constraints. These techniques are based on both the active set strategies in optimization for solving polyhedral constrained optimization and the sparse linear algebratechnologies for exploring the the sparsity in the polyhedral constraints.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/26/2022<br>\n\t\t\t\t\tModified by: Hongchao&nbsp;Zhang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nNew efficient computational algorithms have been developed for solving large-scale nonlinear optimization problems with particular structure. Structured nonlinear optimization has played a central role in various modern applications ranging from image processing, optimal control to stochastic learning in the big data area. The algorithms developed in the project will provide solutions of these structured optimization problems in a more robust and faster way. The methods that are developed will be implemented in high quality software that will be made publicly available and will be beneficial to both the optimization and computational data science community as well as engineers in the industry.   The students supported in this project have excellent opportunities for interdisciplinary research.\n\nModern optimization methods for solving structured optimization problems often need to solve a sequence of subproblems according to the problem structure. This project aims to develop efficient methods and software that allow to solve their subproblems inexactly while still theoretically guarantee the global convergence and maintain the same or almost the same computational complexity of the corresponding methods that require exact solution of the subproblems. In this project, the investigator has developed: (I) A framework of inexact alternating direction methods of multipliers for separable convex optimization, in which each subproblem is solved inexactly to an adaptive accuracy based on the summation of the stepsizes for solving the subproblem and an error relative to the whole problem KKT error; (II) Inexact stochastic proximal gradient (ISPG) methods for the composite optimization, where the objective functionis a summation of an average of very large number of smooth convex component functions and a convex, but possibly nonsmooth, function. These stochastic gradient methods combine the (accelerated) proximal gradient methods and stochastic variance reduction techniques to accelerate the convergence; (III) Inexact optimization techniques for solving nonlinear optimizationwith polyhedral constraints. These techniques are based on both the active set strategies in optimization for solving polyhedral constrained optimization and the sparse linear algebratechnologies for exploring the the sparsity in the polyhedral constraints.\n\n\t\t\t\t\tLast Modified: 11/26/2022\n\n\t\t\t\t\tSubmitted by: Hongchao Zhang"
 }
}