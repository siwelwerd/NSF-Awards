{
 "awd_id": "1763673",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 299999.0,
 "awd_amount": 299999.0,
 "awd_min_amd_letter_date": "2018-06-28",
 "awd_max_amd_letter_date": "2020-08-10",
 "awd_abstract_narration": "Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. \r\n\r\nThe specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes \"coded computing\", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Amir",
   "pi_last_name": "Avestimehr",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Amir S Avestimehr",
   "pi_email_addr": "avestime@usc.edu",
   "nsf_id": "000532667",
   "pi_start_date": "2018-06-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3740 McClintock Ave EEB 526",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900890001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "CA37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 71244.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 73049.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 155706.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>This project focused on mitigating two foundational bottlenecks of large-scale distributed machine learning: communication bandwidth and</span><span>&nbsp;</span><span>stragglers effects. To address these challenges, we developed new advances in Coded Computing, a transformative</span><span>&nbsp;</span><span>framework that combines coding theory with distributed computing to&nbsp;inject computational redundancy in novel coded forms.</span></p>\n<p>One key invention was the development of Lagrange Coded Computing (LCC), which&nbsp;significantly expands the set of computations for which coded computing can be applied. In particular, LCC can be applied to any computation scenario in which the function of interest is an arbitrary&nbsp;multivariate polynomial of the input dataset, hence covering many computations of interest in machine learning. Furthermore, LCC simultaneously provides (1) resiliency against stragglers that may&nbsp;prolong computations;(2) security against Byzantine (or malicious) workers that deliberately modify the computation for their benefit; and (3)(information-theoretic) privacy of the dataset amidst possible&nbsp;collusion of workers.&nbsp;</p>\n<p>&nbsp;</p>\n<div>The project also resulted in several other inventions, such as speed-adaptive coded computing that makes&nbsp;workload distribution strategy for coded computation adaptive; and Coded Private ML that leverages LCC to enable scalable privacy-preserving machine learning. The broader impact of the project included developing of a new course, arranging a workshop on coded computing, and writing survey papers.</div>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/13/2024<br>\nModified by: Amir&nbsp;S&nbsp;Avestimehr</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project focused on mitigating two foundational bottlenecks of large-scale distributed machine learning: communication bandwidth andstragglers effects. To address these challenges, we developed new advances in Coded Computing, a transformativeframework that combines coding theory with distributed computing toinject computational redundancy in novel coded forms.\n\n\nOne key invention was the development of Lagrange Coded Computing (LCC), whichsignificantly expands the set of computations for which coded computing can be applied. In particular, LCC can be applied to any computation scenario in which the function of interest is an arbitrarymultivariate polynomial of the input dataset, hence covering many computations of interest in machine learning. Furthermore, LCC simultaneously provides (1) resiliency against stragglers that mayprolong computations;(2) security against Byzantine (or malicious) workers that deliberately modify the computation for their benefit; and (3)(information-theoretic) privacy of the dataset amidst possiblecollusion of workers.\n\n\n\nThe project also resulted in several other inventions, such as speed-adaptive coded computing that makesworkload distribution strategy for coded computation adaptive; and Coded Private ML that leverages LCC to enable scalable privacy-preserving machine learning. The broader impact of the project included developing of a new course, arranging a workshop on coded computing, and writing survey papers.\n\n\n\t\t\t\t\tLast Modified: 01/13/2024\n\n\t\t\t\t\tSubmitted by: AmirSAvestimehr\n"
 }
}