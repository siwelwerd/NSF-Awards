{
 "awd_id": "1822819",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Teaching Human Motion Tasks at Population Scale",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Robert Russell",
 "awd_eff_date": "2018-09-15",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 636087.0,
 "awd_amount": 636087.0,
 "awd_min_amd_letter_date": "2018-09-06",
 "awd_max_amd_letter_date": "2018-09-06",
 "awd_abstract_narration": "The project will develop technology and study methods for teaching motion tasks, with the teaching of sign language as a first application. Simultaneous placement or quick movement of parts of the body is hard to observe, explain, and execute. While tactile-sensing and augmented-reality systems have been developed to enable machine-human communication of physical processes, the focus has largely been on execution, rather than on teaching and learning. The initial focus of the project will be on teaching sign language, but principles and techniques discovered will be generalized to research the learning of increasingly complex physical motions, ranging from simple posing tasks to high-speed fine manipulation tasks. The proposed work is transformative in that it will directly address the scientific question of how to use technology to understand correct or incorrect human motion and provide constructive guidance, leading to a better understanding of human motion learning. The project will disseminate findings and resources through traditional scientific publications. In addition, models, algorithms, and designs for rapidly-prototyped tools for manipulation will be made available on the online. Results will also be communicated broadly through collaborations with local high schools and museums, and through participation in events such as the USA Science and Engineering Festival.\r\n\r\nThe task of teaching motion motivates the research of three fundamental challenges. First, closed-loop control is a core feature of cyber-physical systems. With a human participant in the system, how can the loop be closed around slow and low-bandwidth human attention? Actuation that guides the human must be easily communicated and sufficient to stabilize the human-suit system. Second, due to limitations in how much information may be communicated, complex human motions must be broken down, and components taught in isolation. How can these component motions be discovered, taught, and re-integrated? Third, algorithms and systems must be developed to measure the accuracy and retention of the learner during the teaching process, guiding repetition and selection of practice material. The project will design and build a lightweight sensing and guidance system that allows interactive communication about motion between human and computer. This technology will allow the investigators to address fundamental research questions in cyber-learning about how to better teach and learn human motion tasks. Research questions include how to measure and evaluate human motion with respect to the task, how to select sensory input to use as guidance, and how to selectively apply or remove training aids, until the learner can complete the motion task with no assistance.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Devin",
   "pi_last_name": "Balkcom",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Devin J Balkcom",
   "pi_email_addr": "devin.balkcom@dartmouth.edu",
   "nsf_id": "000237261",
   "pi_start_date": "2018-09-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Xia",
   "pi_last_name": "Zhou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xia Zhou",
   "pi_email_addr": "xia@cs.columbia.edu",
   "nsf_id": "000659637",
   "pi_start_date": "2018-09-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Kraemer",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "David J Kraemer",
   "pi_email_addr": "David.J.M.Kraemer@Dartmouth.edu",
   "nsf_id": "000662294",
   "pi_start_date": "2018-09-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Dartmouth College",
  "inst_street_address": "7 LEBANON ST",
  "inst_street_address_2": "",
  "inst_city_name": "HANOVER",
  "inst_state_code": "NH",
  "inst_state_name": "New Hampshire",
  "inst_phone_num": "6036463007",
  "inst_zip_code": "037552170",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NH02",
  "org_lgl_bus_name": "TRUSTEES OF DARTMOUTH COLLEGE",
  "org_prnt_uei_num": "T4MWFG59C6R3",
  "org_uei_num": "EB8ASJBCFER9"
 },
 "perf_inst": {
  "perf_inst_name": "Dartmouth College",
  "perf_str_addr": "6211 Sudikoff",
  "perf_city_name": "Hanover",
  "perf_st_code": "NH",
  "perf_st_name": "New Hampshire",
  "perf_zip_code": "037553510",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NH02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "005Y00",
   "pgm_ele_name": "STEM + Computing (STEM+C) Part"
  },
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0418",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001819DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 636087.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-bab8fdbf-7fff-b210-0b11-2eb29e5a94fd\" style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project studied, designed, and implemented methods to augment the teaching of human motion such as sign language, dance, and athletics by technology, increasing the speed and accuracy of learning, scaling teaching to larger populations, and allowing archival of physical teaching practices. The contributions of the work include new sensors that capture human motion so as to provide feedback, new complete hardware and software systems for teaching motion, and experimental studies of how humans learn while interacting with automated teaching systems.&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">New sensors include a glove that measures hand and finger motions during the signing of American Sign Language (ASL). Automated teaching systems developed include an augmented-reality ASL teaching system, which, when combined with the glove, provides a learner with stereo 3D virtual reality feedback overlaid on their own body. User studies found this approach to be effective, when compared to learning from sign-language teaching videos that do not provide feedback.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The project also developed a dance teaching system that is less reliant on costly hardware, and uses a simple webcam to capture learner motions as they learn to dance. The focus of this system was primarily in the development and software implementation of algorithms that deconstruct raw video of a dance into a segmented lesson plan, so that the learner can master one section of a complex motion at a time. User studies found this work to be effective compared to learning from unsegmented video.&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A key scientific question is how to measure a learner&rsquo;s progress and emotional state (satisfaction at progress vs frustration level) during the learning process. One approach, explored by the user studies, measures progress on observable tasks, such as the correspondence between the learner&rsquo;s sign language motion and that of the teacher. Ongoing work takes a different approach: fMRI scans, and other biometric approaches, can be compared.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/06/2023<br>\n\t\t\t\t\tModified by: Devin&nbsp;J&nbsp;Balkcom</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "This project studied, designed, and implemented methods to augment the teaching of human motion such as sign language, dance, and athletics by technology, increasing the speed and accuracy of learning, scaling teaching to larger populations, and allowing archival of physical teaching practices. The contributions of the work include new sensors that capture human motion so as to provide feedback, new complete hardware and software systems for teaching motion, and experimental studies of how humans learn while interacting with automated teaching systems. \nNew sensors include a glove that measures hand and finger motions during the signing of American Sign Language (ASL). Automated teaching systems developed include an augmented-reality ASL teaching system, which, when combined with the glove, provides a learner with stereo 3D virtual reality feedback overlaid on their own body. User studies found this approach to be effective, when compared to learning from sign-language teaching videos that do not provide feedback.\nThe project also developed a dance teaching system that is less reliant on costly hardware, and uses a simple webcam to capture learner motions as they learn to dance. The focus of this system was primarily in the development and software implementation of algorithms that deconstruct raw video of a dance into a segmented lesson plan, so that the learner can master one section of a complex motion at a time. User studies found this work to be effective compared to learning from unsegmented video. \nA key scientific question is how to measure a learner\u2019s progress and emotional state (satisfaction at progress vs frustration level) during the learning process. One approach, explored by the user studies, measures progress on observable tasks, such as the correspondence between the learner\u2019s sign language motion and that of the teacher. Ongoing work takes a different approach: fMRI scans, and other biometric approaches, can be compared.\n\n\t\t\t\t\tLast Modified: 01/06/2023\n\n\t\t\t\t\tSubmitted by: Devin J Balkcom"
 }
}