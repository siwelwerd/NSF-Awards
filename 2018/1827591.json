{
 "awd_id": "1827591",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative research: An integrated model of phonetic analysis and lexical analysis based on individual acoustic cues to features",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 202981.0,
 "awd_amount": 202981.0,
 "awd_min_amd_letter_date": "2018-08-28",
 "awd_max_amd_letter_date": "2020-04-07",
 "awd_abstract_narration": "One of the greatest mysteries in the cognitive and neural sciences is how humans achieve robust speech perception given extreme variation in the precise acoustics produced for any given speech sound or word. For example, people can produce different acoustics for the same vowel sound, while in other cases the acoustics for two different vowels may be nearly identical. The acoustic patterns also change depending on the rate at which the sounds are spoken.  Listeners may also perceive a sound that was not actually produced due to massive reductions in speech pronunciation (e.g., the \"t\" and \"y\" sounds in \"don't you\" are often reduced to \"doncha\"). Most theories assume that listeners recognize words in continuous speech by extracting consonants and vowels in a strictly sequential order. However, previous research has failed to find evidence for invariant cues in the acoustic signal that would allow listeners to extract the important information. This project uses a new tool for the study of language processing, LEXI (for Linguistic-Event EXtraction and Interpretation), to test the hypothesis that individual acoustic cues for consonants and vowels can in fact be extracted from the signal and can be used to determine the speaker's intended words. When some acoustic cues for speech sounds are modified or missing, LEXI can detect the remaining cues and evaluate them as evidence for the intended sounds and words. This research has potentially broad societal benefits, including optimization of human-machine interactions to accommodate atypical speech patterns seen in speech disorders or accented speech. This project supports training of 1-2 doctoral students and 8-10 undergraduate students through hands-on experience in experimental and computational research. All data, including code for computational models, the LEXI system, and speech databases labeled for acoustic cues will be publicly available through the Open Science Framework; preprints of all publications will be publicly available at PsyArxiv and NSF-PAR.\r\n\r\nThis interdisciplinary project unites signal analysis, psycholinguistic experimentation, and computational modeling to (1) survey the ways that acoustic cues vary in different contexts, (2) experimentally test how listeners use these cues through distributional learning for speech, and (3) use computational modeling to evaluate competing theories of how listeners recognize spoken words. The work will identify cue patterns in the signal that listeners use to recognize massive reductions in pronunciation and will experimentally test how listeners keep track of this systematic variation. This knowledge will be used to model how listeners \"tune in\" to the different ways speakers produce speech sounds. By using cues detected by LEXI as input to competing models of word recognition, the work provides an opportunity to examine the fine-grained time course of human speech recognition with large sets of spoken words; this is an important innovation because most cognitive models of speech do not work with speech input directly. Theoretical benefits include a strong test of the cue-based model of word recognition and the development of tools to allow virtually any model of speech recognition to work on real speech input, with practical implications for optimizing automatic speech recognition.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rachel",
   "pi_last_name": "Theodore",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Rachel M Theodore",
   "pi_email_addr": "rachel.theodore@uconn.edu",
   "nsf_id": "000691901",
   "pi_start_date": "2018-08-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Magnuson",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "James S Magnuson",
   "pi_email_addr": "james.magnuson@uconn.edu",
   "nsf_id": "000065934",
   "pi_start_date": "2018-08-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Paul",
   "pi_last_name": "Allopenna",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Paul Allopenna",
   "pi_email_addr": "paul.allopenna@uconn.edu",
   "nsf_id": "000727968",
   "pi_start_date": "2018-08-28",
   "pi_end_date": "2020-04-07"
  }
 ],
 "inst": {
  "inst_name": "University of Connecticut",
  "inst_street_address": "438 WHITNEY RD EXTENSION UNIT 1133",
  "inst_street_address_2": "",
  "inst_city_name": "STORRS",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "8604863622",
  "inst_zip_code": "062699018",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CT02",
  "org_lgl_bus_name": "UNIVERSITY OF CONNECTICUT",
  "org_prnt_uei_num": "",
  "org_uei_num": "WNTPS995QBM7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Connecticut",
  "perf_str_addr": "850 Bolton Road",
  "perf_city_name": "Storrs",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "062691085",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CT02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 202981.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>One of the greatest mysteries in the cognitive and neural sciences is how humans achieve robust speech perception given extreme variation in the precise acoustics produced for any given speech sound or word. For example, people can produce different acoustics for the same vowel sound, while in other cases the acoustics for two different vowels may be nearly identical. Most theories assume that listeners recognize words in continuous speech by extracting consonants and vowels in a strictly sequential order. However, previous research has failed to find evidence for invariant cues in the acoustic signal that would allow listeners to extract the important information. This interdisciplinary project united signal analysis, psycholinguistic experimentation, and computational modeling to (1) survey the ways that acoustic cues vary in different contexts, (2) experimentally test how listeners use these cues through distributional learning for speech, and (3) use computational modeling to evaluate competing theories of how listeners recognize spoken words. This was a collaborative project between teams at the University of Connecticut (UConn) and the Massachusetts Institute of Technology (MIT); final outcomes are reported separately for each award.</p>\n<p>In terms of Intellectual Merit, results include (1)&nbsp;demonstrating that listeners use talker-specific phonetic cues to facilitate talker identification (2) showing that listeners modify the mapping to speech sounds to reflect a cumulative registration of a talker's phonetic input and that this process can be captured by an ideal adapter model of speech adaptation, (3) discovering that processing costs associated with talker variability are graded to reflect acoustic variation among talkers, (4) providing a critical test of auditory attention as the locus of mixed talker processing costs, (5) discovering that listeners dynamically down-weight lexical cues when lexical information competes with temporally-local phonetic cues, (6) demonstrating that drawing minimally from deep learning architectures is sufficient to create a computational model of word recognition that operates on real speech achieves high word recognition, (7) revealing the role of within-talker phonetic variation on talker adaptation, (8) examining&nbsp;how listeners dynamically weight phonetic and lexical information for speech perception when acoustic and lexical input change over time, (9)&nbsp;revealing&nbsp;listeners' ability to simultaneously update talker-specific generative models to reflect structured phonetic variation, including constraints on talker-specific learning, and (10) establishing a pipeline that prepares output from the Linguistic Event Extraction and Interpretation system (from the MIT team) as appropriate input for a single recurrent network model of spoken word recognition.</p>\n<p>Five primary Broader Impacts were achieved through this project. First, this project developed substantial stimuli, analysis code, and trial-level data for speech perception experiments that have been publicly distributed via the Open Science Framework and GitHub, including an R package to facilitate researchers' ability to perform simulations with a Bayesian belief-updating model of speech adaptation. Second, this project provided innovative educational opportunities for five Ph.D. students, one undergraduate Honors student, and numerous undergraduate research assistants in computational models of speech perception, analysis of behavioral data, stimulus creation, experimental design, scientific writing including manuscript preparation, best practices for reproducibility of research, and theories associated with perceptual learning for speech including adaptation to talker-specific phonetic variability, integration between phonetic and lexical/semantic information for speech perception, and individual differences in speech processing. Third, this project served as a catalyst for awareness and adoption of new best practices for promoting reproducibility of research by leading training sessions for students and trainees at UConn and other institutions, and by providing a model of these practices, including preregistration, transparency in data dissemination, and reproducible analysis pipelines. Fourth, this project advanced innovation in experimental methods through its use of web-based testing. Web-based (or \"crowd-sourced\") data collection increases involvement in science by removing physical barriers to participation, thus promoting a more inclusive science, in addition to providing tools to meet best practices for promoting reproducibility of research including frequent replications, testing more diverse samples, and testing larger sample sizes. In addition to the experimental findings generated with web-based testing, this project yielded publicly-available tutorials and professional developmental opportunities to support researchers' use of web-based testing in their research. Fifth, the knowledge gained from the project has begun to impact research in other domains, including speech prosody, first and second language acquisition, and voice recognition, serving as evidence of an impact beyond the specific domain of the project.</p>\n<p>Collectively, this project yielded knowledge and tools for understanding how listeners access meaning from the acoustic speech signal, with a particular focus on how listeners \"tune in\" to the different ways that speakers produce speech sounds. The foundational knowledge generated in this project has potentially broad societal benefits, including optimization of human-machine interactions to accommodate atypical speech patterns that are present in speech disorders or accented speech.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/28/2022<br>\n\t\t\t\t\tModified by: Rachel&nbsp;M&nbsp;Theodore</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOne of the greatest mysteries in the cognitive and neural sciences is how humans achieve robust speech perception given extreme variation in the precise acoustics produced for any given speech sound or word. For example, people can produce different acoustics for the same vowel sound, while in other cases the acoustics for two different vowels may be nearly identical. Most theories assume that listeners recognize words in continuous speech by extracting consonants and vowels in a strictly sequential order. However, previous research has failed to find evidence for invariant cues in the acoustic signal that would allow listeners to extract the important information. This interdisciplinary project united signal analysis, psycholinguistic experimentation, and computational modeling to (1) survey the ways that acoustic cues vary in different contexts, (2) experimentally test how listeners use these cues through distributional learning for speech, and (3) use computational modeling to evaluate competing theories of how listeners recognize spoken words. This was a collaborative project between teams at the University of Connecticut (UConn) and the Massachusetts Institute of Technology (MIT); final outcomes are reported separately for each award.\n\nIn terms of Intellectual Merit, results include (1) demonstrating that listeners use talker-specific phonetic cues to facilitate talker identification (2) showing that listeners modify the mapping to speech sounds to reflect a cumulative registration of a talker's phonetic input and that this process can be captured by an ideal adapter model of speech adaptation, (3) discovering that processing costs associated with talker variability are graded to reflect acoustic variation among talkers, (4) providing a critical test of auditory attention as the locus of mixed talker processing costs, (5) discovering that listeners dynamically down-weight lexical cues when lexical information competes with temporally-local phonetic cues, (6) demonstrating that drawing minimally from deep learning architectures is sufficient to create a computational model of word recognition that operates on real speech achieves high word recognition, (7) revealing the role of within-talker phonetic variation on talker adaptation, (8) examining how listeners dynamically weight phonetic and lexical information for speech perception when acoustic and lexical input change over time, (9) revealing listeners' ability to simultaneously update talker-specific generative models to reflect structured phonetic variation, including constraints on talker-specific learning, and (10) establishing a pipeline that prepares output from the Linguistic Event Extraction and Interpretation system (from the MIT team) as appropriate input for a single recurrent network model of spoken word recognition.\n\nFive primary Broader Impacts were achieved through this project. First, this project developed substantial stimuli, analysis code, and trial-level data for speech perception experiments that have been publicly distributed via the Open Science Framework and GitHub, including an R package to facilitate researchers' ability to perform simulations with a Bayesian belief-updating model of speech adaptation. Second, this project provided innovative educational opportunities for five Ph.D. students, one undergraduate Honors student, and numerous undergraduate research assistants in computational models of speech perception, analysis of behavioral data, stimulus creation, experimental design, scientific writing including manuscript preparation, best practices for reproducibility of research, and theories associated with perceptual learning for speech including adaptation to talker-specific phonetic variability, integration between phonetic and lexical/semantic information for speech perception, and individual differences in speech processing. Third, this project served as a catalyst for awareness and adoption of new best practices for promoting reproducibility of research by leading training sessions for students and trainees at UConn and other institutions, and by providing a model of these practices, including preregistration, transparency in data dissemination, and reproducible analysis pipelines. Fourth, this project advanced innovation in experimental methods through its use of web-based testing. Web-based (or \"crowd-sourced\") data collection increases involvement in science by removing physical barriers to participation, thus promoting a more inclusive science, in addition to providing tools to meet best practices for promoting reproducibility of research including frequent replications, testing more diverse samples, and testing larger sample sizes. In addition to the experimental findings generated with web-based testing, this project yielded publicly-available tutorials and professional developmental opportunities to support researchers' use of web-based testing in their research. Fifth, the knowledge gained from the project has begun to impact research in other domains, including speech prosody, first and second language acquisition, and voice recognition, serving as evidence of an impact beyond the specific domain of the project.\n\nCollectively, this project yielded knowledge and tools for understanding how listeners access meaning from the acoustic speech signal, with a particular focus on how listeners \"tune in\" to the different ways that speakers produce speech sounds. The foundational knowledge generated in this project has potentially broad societal benefits, including optimization of human-machine interactions to accommodate atypical speech patterns that are present in speech disorders or accented speech.\n\n\t\t\t\t\tLast Modified: 12/28/2022\n\n\t\t\t\t\tSubmitted by: Rachel M Theodore"
 }
}