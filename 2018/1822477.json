{
 "awd_id": "1822477",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Leverage Augmented Reality for Safety Education in the Logistics Industry",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Robert Russell",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 749542.0,
 "awd_amount": 749542.0,
 "awd_min_amd_letter_date": "2018-08-24",
 "awd_max_amd_letter_date": "2018-08-24",
 "awd_abstract_narration": "The project will design and research an immersive and personalized education approach by leveraging augmented reality (AR) to support workers' understanding of risk factors associated with their work, and encourage workers to perform tasks using appropriate body motion. The project will develop an AR-assisted posture training procedure for minimizing biomechanical exposures on the low back and the shoulder, as injuries of those two body regions are the most common in logistics industry. The research will expand knowledge in AR-assisted education and training in terms of work safety promotion, motor adaptation, and transfer of learning. The AR infrastructure can also used to provide just-in-time work procedure training and has potential applications in high school and college courses in physics and biomechanics. The project will advance research in to motor learning, such as sports training and tele-rehabilitation.  The overall outcome will advance human-technology system in body motion education and learning science.\r\n\r\nThe project team will reconstruct a virtual instructor demonstrating the recommended and non-recommended body motions that are commonly seen in material handling. AR, depth sensing, and biomechanical modeling will be integrated to deliver knowledge on work-related biomechanical exposures on joints and muscles. Workers will be able to observe the virtual instructor overlaid on the actual physical workplace from various view angles through AR goggles. The system will be designed to enhance workers' understanding of work-related risks via visualization of abstract biomechanical exposures. Biomechanical exposures, such as forces exerted on the joints and the muscles at work, will be visually integrated with the virtual instructor. The innovative geometric representation of biomechanical exposures will assist learners in visualizing abstract mechanics quantities. The AR user interface and the biomechanical visualization scheme will be iteratively refined through a series of usability evaluations and operational tests to ensure worker's understanding of the risk in musculo-skeletal disorder. The research will also examine worker learning and transfer of learning in AR-assisted safety education approach.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jing",
   "pi_last_name": "Feng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jing Feng",
   "pi_email_addr": "jfeng2@ncsu.edu",
   "nsf_id": "000745128",
   "pi_start_date": "2018-08-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xu",
   "pi_last_name": "Xu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xu Xu",
   "pi_email_addr": "xxu@ncsu.edu",
   "nsf_id": "000733651",
   "pi_start_date": "2018-08-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Karen",
   "pi_last_name": "Chen",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Karen B Chen",
   "pi_email_addr": "kbchen2@ncsu.edu",
   "nsf_id": "000733650",
   "pi_start_date": "2018-08-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tianfu",
   "pi_last_name": "Wu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tianfu Wu",
   "pi_email_addr": "tianfu_wu@ncsu.edu",
   "nsf_id": "000738028",
   "pi_start_date": "2018-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276957906",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 749542.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"clear\">Augmented reality combines physical and virtual objects in the same environment and allows physical and virtual objects to run interactively in real-time. With the ability to superimpose virtual objects onto physical objects through spatial and temporal registration, AR has been widely investigated for its potential applications in education and training. In particular, AR is gaining popularity in the industry for the purpose of job training. In this project, we established an immersive education approach by leveraging AR to support workers' understanding of musculoskeletal disorders (MSDs) risk factors associated with their work, and to encourage workers to perform tasks using appropriate body motion.</p>\n<p class=\"clear\">The outcomes of this project have intellectually contributed to the following aspects. First, we developed an algorithm to reconstruct a virtual instructor in an AR environment by using low-cost depth sensors. With this algorithm, we can record the body motions of a real safety educator and transfer them into an AR environment so that users can holistically observe the correct body postures and movements to reduce the exposure to MSD risk factors. Second, we refined a computer vision algorithm that can recognize human posture from an image. The refined algorithm can then directly predict the MSD risk level in real-time from a recorded video. This way, workers can immediately compare different working postures in AR in terms of the MSD risk level. &nbsp;Third, a comprehensive AR user interface was developed and evaluated by target users (e.g., individuals who perform manual material handling in warehouses and grocery stores). Because the target user may not have any AR experiences, the user interface needs to walk users through the training process and promote the adequate knowledge transfer. By integrating the outcomes in these three aspects, we delivered an AR application that can be implemented in industry for facilitating workers to learn MSD-related knowledge for injury prevention.</p>\n<p class=\"clear\">The outcomes of this project have broader impacts on the research community and the society. First, subsequent augmented reality experiments conducted in our lab have utilized the findings and recommendations from this project. Moreover, we have presented the findings at a number of journals in the areas of AR, human factors, and biomechanics. The findings have been also presented in Human Factors and Ergonomics Society annual conferences. In addition, the AR scenarios produced from this projects were used for summer camp activities dedicating to K-12 students from underrepresented minority group, lab tours, open houses, and demonstrations for undergraduate and graduate classes.</p>\n<p class=\"clear\">In sum, the outcomes of this project advanced the understanding of human pose reconstruction and visualization, the utility of computer-vision in safety monitoring, as well as the design recommendations for AR scenarios. The products of this project were disseminated to relevant research communities, and also exposed students at different age to AR and human-computer interaction through a series of outreach activities.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/07/2023<br>\n\t\t\t\t\tModified by: Xu&nbsp;Xu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Augmented reality combines physical and virtual objects in the same environment and allows physical and virtual objects to run interactively in real-time. With the ability to superimpose virtual objects onto physical objects through spatial and temporal registration, AR has been widely investigated for its potential applications in education and training. In particular, AR is gaining popularity in the industry for the purpose of job training. In this project, we established an immersive education approach by leveraging AR to support workers' understanding of musculoskeletal disorders (MSDs) risk factors associated with their work, and to encourage workers to perform tasks using appropriate body motion.\nThe outcomes of this project have intellectually contributed to the following aspects. First, we developed an algorithm to reconstruct a virtual instructor in an AR environment by using low-cost depth sensors. With this algorithm, we can record the body motions of a real safety educator and transfer them into an AR environment so that users can holistically observe the correct body postures and movements to reduce the exposure to MSD risk factors. Second, we refined a computer vision algorithm that can recognize human posture from an image. The refined algorithm can then directly predict the MSD risk level in real-time from a recorded video. This way, workers can immediately compare different working postures in AR in terms of the MSD risk level.  Third, a comprehensive AR user interface was developed and evaluated by target users (e.g., individuals who perform manual material handling in warehouses and grocery stores). Because the target user may not have any AR experiences, the user interface needs to walk users through the training process and promote the adequate knowledge transfer. By integrating the outcomes in these three aspects, we delivered an AR application that can be implemented in industry for facilitating workers to learn MSD-related knowledge for injury prevention.\nThe outcomes of this project have broader impacts on the research community and the society. First, subsequent augmented reality experiments conducted in our lab have utilized the findings and recommendations from this project. Moreover, we have presented the findings at a number of journals in the areas of AR, human factors, and biomechanics. The findings have been also presented in Human Factors and Ergonomics Society annual conferences. In addition, the AR scenarios produced from this projects were used for summer camp activities dedicating to K-12 students from underrepresented minority group, lab tours, open houses, and demonstrations for undergraduate and graduate classes.\nIn sum, the outcomes of this project advanced the understanding of human pose reconstruction and visualization, the utility of computer-vision in safety monitoring, as well as the design recommendations for AR scenarios. The products of this project were disseminated to relevant research communities, and also exposed students at different age to AR and human-computer interaction through a series of outreach activities.\n\n \n\n\t\t\t\t\tLast Modified: 01/07/2023\n\n\t\t\t\t\tSubmitted by: Xu Xu"
 }
}