{
 "awd_id": "1821894",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Multimodal Affective Pedagogical Agents for Different Types of Learners",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925126",
 "po_email": "abaylor@nsf.gov",
 "po_sign_block_name": "Amy Baylor",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 498823.0,
 "awd_amount": 498823.0,
 "awd_min_amd_letter_date": "2018-07-23",
 "awd_max_amd_letter_date": "2018-07-23",
 "awd_abstract_narration": "While most research on embodied pedagogical agents (adaptive virtual agents that guide and mentor learners) explores cognitive features, this project investigates the role of agent affect/emotion. This project examines how the agent's affective state (e.g., seeming interested or concerned) impacts different types of students (e.g., differing by knowledge level, gender, underrepresented group status, interest in STEM fields, and personality profile) when learning from online statistics lessons. The project integrates several areas of research: a) computer graphics research on life-like and believable representation of emotion in embodied agents, b) advanced methods and techniques from artificial intelligence and computer vision for real-time recognition of emotions, c) cognitive psychology research on learning from affective agents, and d) education research on the efficacy of affective agents for improving student learning of STEM concepts. Through experimental research the project will advance the state of the art in agent design and implementation by integrating findings on effective emotion regulation with algorithms that support life-like expression of emotions in embodied agents. \r\n\r\nTo investigate the multimodal design features of affective pedagogical agents, the project has two main objectives: (1) research and develop novel algorithms for emotion recognition and for life-like emotion representation in embodied animated agents, and (2) develop an empirically grounded research base to guide the design of affective pedagogical agents for different types of learners. In one series of experiments the project will determine evidence-based design principles to guide the development of agents that demonstrate emotion/affect, including which kinds of affective states are most effective for which kinds of learners. In a second series of experiments, the project will implement a web-camera system to detect the emotional state of the learner (e.g., confused, interested, content, or bored), adapting the emotional state displayed by the agent in response. Of interest is whether students learn the statistics lesson better when the pedagogical agent is sensitive to the learner's emotional state than when it is not. In addition to its scientific merit, the project will develop and make available a toolkit of affective animated pedagogical agents that adapt to learner characteristics to be used by learners of all ages, for education and training in a variety of subject matters and settings.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nicoletta",
   "pi_last_name": "Adamo-Villani",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nicoletta Adamo-Villani",
   "pi_email_addr": "nadamovi@purdue.edu",
   "nsf_id": "000490363",
   "pi_start_date": "2018-07-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Bedrich",
   "pi_last_name": "Benes",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bedrich Benes",
   "pi_email_addr": "bbenes@purdue.edu",
   "nsf_id": "000291472",
   "pi_start_date": "2018-07-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "401 N Grant Street",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 498823.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to improve student learning from online science and mathematics lessons delivered by animated pedagogical agents that display appropriate affective features through their gestures, facial expressions, and voice.&nbsp; This involves designing and developing animated pedagogical agents--i.e., onscreen animated instructors--who display affective features appropriate for different kinds of learners--and testing their impact on learners.&nbsp; In improving student learning from online lessons with pedagogical agents, it is important to consider not only what information the agent presents but also how the agent primes affective and social responses in the learners.&nbsp;&nbsp; Specifically, this project involves (1) developing novel algorithms for representing affective cues in onscreen agents and for recognizing emotional reactions of learners, (2) integrating the algorithms in a new system for creating pedagogical capable of displaying various emotions to be embedded in digital lessons, (3) developing an empirically-grounded research base to guide the design of pedagogical agents who display affective cues that help various kinds of students learn from online lessons in introductory statistics, and (4) disseminating results and principles for incorporating affective cues through gestures, facial expressions, and voice in pedagogical agents.&nbsp;</p>\n<p>On the technical level, this project's accomplishments include the development and validation of&nbsp; novel animation algorithms and a new computer animation system for creating affective agents that show different emotional designs,&nbsp; a new facial expression detector, and new methods for producing facial micro-expressions, facial macro-expressions, body gestures, and speech that convey specific emotions in onscreen animated agents. &nbsp;&nbsp;On the psychological level, this project's accomplishments include finding that people can treat animated instructors like real humans, by recognizing and responding to whether animated instructors display positive or negative cues in their voice, facial expressions, and gestures.&nbsp; On the educational level, this project's accomplishments include showing that students are able to recognize the emotion displayed by an onscreen instructor for a statistics lesson, tend to feel the same emotion that is displayed by the instructor, tend to form a stronger social bond with animated instructors who display positive emotions, and in some cases learn instructional material better with animated instructors who display positive emotions.&nbsp; There was also preliminary evidence that learners may respond differently to the emotional tone of male and female animated instructors.&nbsp;</p>\n<p>The project contributes to our understanding of how to create onscreen agents that display affective cues involving voice, facial expression, and gesture and to determine how learners respond to them. &nbsp;The broader impact is to improve online science and mathematics lessons by understanding how to design onscreen animated instructors that display emotions that help students learn and feel positive about the material.&nbsp;&nbsp;</p><br>\n<p>\n Last Modified: 11/21/2023<br>\nModified by: Nicoletta&nbsp;Adamo-Villani</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project aims to improve student learning from online science and mathematics lessons delivered by animated pedagogical agents that display appropriate affective features through their gestures, facial expressions, and voice. This involves designing and developing animated pedagogical agents--i.e., onscreen animated instructors--who display affective features appropriate for different kinds of learners--and testing their impact on learners. In improving student learning from online lessons with pedagogical agents, it is important to consider not only what information the agent presents but also how the agent primes affective and social responses in the learners. Specifically, this project involves (1) developing novel algorithms for representing affective cues in onscreen agents and for recognizing emotional reactions of learners, (2) integrating the algorithms in a new system for creating pedagogical capable of displaying various emotions to be embedded in digital lessons, (3) developing an empirically-grounded research base to guide the design of pedagogical agents who display affective cues that help various kinds of students learn from online lessons in introductory statistics, and (4) disseminating results and principles for incorporating affective cues through gestures, facial expressions, and voice in pedagogical agents.\n\n\nOn the technical level, this project's accomplishments include the development and validation of novel animation algorithms and a new computer animation system for creating affective agents that show different emotional designs, a new facial expression detector, and new methods for producing facial micro-expressions, facial macro-expressions, body gestures, and speech that convey specific emotions in onscreen animated agents. On the psychological level, this project's accomplishments include finding that people can treat animated instructors like real humans, by recognizing and responding to whether animated instructors display positive or negative cues in their voice, facial expressions, and gestures. On the educational level, this project's accomplishments include showing that students are able to recognize the emotion displayed by an onscreen instructor for a statistics lesson, tend to feel the same emotion that is displayed by the instructor, tend to form a stronger social bond with animated instructors who display positive emotions, and in some cases learn instructional material better with animated instructors who display positive emotions. There was also preliminary evidence that learners may respond differently to the emotional tone of male and female animated instructors.\n\n\nThe project contributes to our understanding of how to create onscreen agents that display affective cues involving voice, facial expression, and gesture and to determine how learners respond to them. The broader impact is to improve online science and mathematics lessons by understanding how to design onscreen animated instructors that display emotions that help students learn and feel positive about the material.\t\t\t\t\tLast Modified: 11/21/2023\n\n\t\t\t\t\tSubmitted by: NicolettaAdamo-Villani\n"
 }
}