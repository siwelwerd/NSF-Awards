{
 "awd_id": "1751400",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER:Enabling Scalable, Modular, and Efficient Architecture Specialization Fabrics",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Danella Zhao",
 "awd_eff_date": "2018-05-01",
 "awd_exp_date": "2023-04-30",
 "tot_intn_awd_amt": 483606.0,
 "awd_amount": 483606.0,
 "awd_min_amd_letter_date": "2018-04-26",
 "awd_max_amd_letter_date": "2021-08-02",
 "awd_abstract_narration": "Past exponential improvements to computer processor capabilities are now being threatened by a long-term slowdown of the progress in physical device technologies.  An alternate approach is to specialize processor design for a small set of tasks, sacrificing generality for improved performance and energy-efficiency.  However, if hardware specialization is relied on too much, the pace of innovation for new and fast-moving applications will be stifled.  Hence, new techniques are required to balance the fundamental trade-offs between the efficiency of specialized processors with the usefulness of general processors. A promising way forward is the development of specialization fabrics, where the software interface and hardware implementation are co-designed to enable efficient execution of programs with broad application characteristics.  This CAREER award develops the principles behind building such fabrics, addressing their two main challenges.  The first is scalability: how to support vast amounts of computational units without mitigating the benefits of specialization.  The second is modularity: how to enable simple tailoring of the fabric to a particular design setting (e.g. for a datacenter machine, phone, or wearable) through composable hardware. The broad potential of this work is to enable principled specialized hardware which can continue to bring exponential performance and energy improvements, both through dissemination of discovered principles and through open source releases of hardware and software.  For industry, the developed frameworks can enable hardware companies to leverage zero-design-effort hardware tailored for their use cases.  For academia, these can lower the cost of entry of hardware/software co-design research, and enable researchers to make cross-domain innovations more easily.  This framework is being integrated into courses to teach the fundamental interactions between hardware and software.\r\n\r\nOverall, the broad goal of this work is to create a programmable accelerator fabric, scalable to high throughput, and whose features can be modularly composed - ultimately enabling accelerator-like performance and energy efficiency across many domains. Intellectually, it furthers the unification of two disparate fields of computer architecture, the study of on-chip memory systems and the study of architectural specialization. The project develops the principles of scalable and modular specialization fabrics through two thrusts.  The first explores how to leverage high-level ISA constructs to rethink the design of the cache hierarchy and on-chip communication network for specialization fabrics, which are typically constrained by communication bandwidth or access/storage energy.  The key innovation is to expose to the memory system a set of higher level abstractions describing coarse-grain patterns of memory access.  Leveraging this information can reduce the inefficiencies of communication and tag/redundant cache access, but requires a significant overhaul to existing protocols to maintain simple memory semantics. The second thrust develops a framework for composing modular architecture features to enable trivial hardware customization for vastly different application domains. The key innovation is the development of high-level ISA features which have a direct correspondence to composable hardware structures. This thrust develops a template design and instruction-set description for composing ISA-exposed microarchitecture features, explores a set of ISA features for regular and irregular workloads, and develops a compiler to hide ISA complexity.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anthony",
   "pi_last_name": "Nowatzki",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anthony Nowatzki",
   "pi_email_addr": "tjn@cs.ucla.edu",
   "nsf_id": "000752659",
   "pi_start_date": "2018-04-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "UCLA Computer Science Department",
  "perf_str_addr": "420 Westwood Plaza, 3731G BH",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951596",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 95769.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 96850.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 97805.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 193182.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-aeeb4f35-7fff-c431-6395-0c75dea99654\">\n<p dir=\"ltr\"><span>The context for this work is the slowing in improvements to general purpose computing due to the waning of Moore&rsquo;s law, combined with limited gains from restrictive traditional architectures (CPUs/GPUs).&nbsp; Over the last decade, both industry and academia have shifted focus towards domain-specific accelerators that are highly optimized for a few workloads (e.g. NVIDIA&rsquo;s Tensor Cores, Intel&rsquo;s ISA extensions, mobile SoC accelerators, AI accelerators, etc.). While effective, the current accelerator design paradigm is inefficient, requiring exorbitant manual effort that is only economically efficient in a few domains with billion+ dollar funding (e.g. AI).&nbsp; Moreover, restrictive accelerators and a slow design process are limiting software and algorithm innovation, as they favor only simpler, less-efficient algorithms.</span></p>\n<p dir=\"ltr\"><span>Our work took a step towards addressing the root problem: the from-scratch accelerator development cycle, including compiler, ISA, and microarchitecture design.&nbsp; The broad vision of this work was to democratize accelerator design, analogously to most software development today.&nbsp; In this paradigm, automated tools could analyze applications and produce not only a customized accelerator design, but also a supporting stack comprising a compiler, runtime, and drivers that make such hardware immediately useful. Generated designs could be manufactured in custom chips, integrated tightly with other components as chiplets/dielets, or be mapped to FPGAs (i.e. as FPGA overlays). When genuinely new forms of hardware acceleration are needed, such a development platform would reduce the design burden and let researchers focus on new ideas.&nbsp; We specifically made the following contributions:</span></p>\n<p dir=\"ltr\"><span>First, we developed a unifying design space encompassing many accelerators: e.g. application/domain-specific, systolic arrays, reconfigurable, and vector architectures.&nbsp; This design-space was codified as a graph representation, called the architecture description graph (ADG).&nbsp; The ADG represents accelerators as a composition of composable primitives like memories, switches, controllers, synchronization, and computation units.&nbsp; We developed hardware generation tools that could produce the RTL for the ADG (i.e. outputting a multicore-accelerator system), which optimizes the ISA for the required functionality.&nbsp; We also developed the algorithms and implementation of a compiler that could understand how to compile C programs onto any graph, dealing with the variability of features at the ISA level in different accelerator instances.&nbsp; Our open-source framework, DSAGEN, can produce designs that significantly out-perform manual designs, with no human design effort.</span></p>\n<p dir=\"ltr\"><span>One of the key challenges is irregularity -- how to support algorithms with data dependences.&nbsp; Irregularity is challenging because it interferes with vectorization and prefetching techniques, which are two of the most important optimizations for data-processing workloads.&nbsp; What we found is that not all types of irregularity need to be supported -- primitives for data-dependence idioms are sufficient to support many important workloads, and don&rsquo;t require general purpose support.&nbsp; Our work discovered key hardware/software primitives for joining, indirection, and task scheduling.&nbsp; These primitives enabled flexible hardware accelerators to be competitive with, and sometimes outperform, their more specialized or domain-specific counterparts.</span></p>\n<p dir=\"ltr\"><span>Our work also uniquely focused on performing specialization for memory systems.&nbsp; While many processors of today have a thin interface to memory, consisting of only single addresses, we found that supplying richer semantics for memory operations can open up a variety of new optimizations.&nbsp; Specifically, we explored exposing the concepts of &ldquo;streams&rdquo;, or long term memory access patterns, to the memory system through simple ISA extensions.&nbsp; Streams can be used for implementing &ldquo;perfect&rdquo; prefetching that is highly flexible and much more accurate than conventional learning-based prefetchers.&nbsp; Streams provide information that can inform cache bypassing and replacement policies.&nbsp; They are also useful for enabling near-data computing, both by offloading long-term memory access patterns into the memory system to eliminate request traffic, and also as an abstraction to co-locate computation.&nbsp; These optimizations together enable scaling to larger multicores by greatly reducing network traffic on chip.</span></p>\n<p dir=\"ltr\"><span>Finally, we developed our ideas into a framework that can simplify FPGA software development. The issue with the mainstream approach of high-level synthesis (HLS), is that it leaves a significant programmability gap in terms of reconfigurability, customization and versatility: 1. FPGA physical design can take hours, 2. FPGA reconfiguration time limits HLS from targeting complex workloads, and 3. HLS tools do not reason about cross-workload flexibility. We developed a new FPGA programming framework called OverGen, where an overlay architecture is automatically specialized to a set of representative applications.&nbsp; This work overcomes the limitations of prior overlay architectures through automated specialization.&nbsp; While it requires more resources to support more general architectures, we found that OverGen achieves nearly equivalent performance with state-of-the-art HLS, while requiring 10,000x less compile and reconfiguration time.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Overall, this body of work took significant steps towards making it both easier to design accelerators, while also making them more general purpose and effective at challenging problems.&nbsp; Finally, this grant was the primary funding source for three PhDs, and was instrumental in enabling their academic success.</span></p>\n</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/28/2023<br>\n\t\t\t\t\tModified by: Anthony&nbsp;Nowatzki</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThe context for this work is the slowing in improvements to general purpose computing due to the waning of Moore\u2019s law, combined with limited gains from restrictive traditional architectures (CPUs/GPUs).  Over the last decade, both industry and academia have shifted focus towards domain-specific accelerators that are highly optimized for a few workloads (e.g. NVIDIA\u2019s Tensor Cores, Intel\u2019s ISA extensions, mobile SoC accelerators, AI accelerators, etc.). While effective, the current accelerator design paradigm is inefficient, requiring exorbitant manual effort that is only economically efficient in a few domains with billion+ dollar funding (e.g. AI).  Moreover, restrictive accelerators and a slow design process are limiting software and algorithm innovation, as they favor only simpler, less-efficient algorithms.\nOur work took a step towards addressing the root problem: the from-scratch accelerator development cycle, including compiler, ISA, and microarchitecture design.  The broad vision of this work was to democratize accelerator design, analogously to most software development today.  In this paradigm, automated tools could analyze applications and produce not only a customized accelerator design, but also a supporting stack comprising a compiler, runtime, and drivers that make such hardware immediately useful. Generated designs could be manufactured in custom chips, integrated tightly with other components as chiplets/dielets, or be mapped to FPGAs (i.e. as FPGA overlays). When genuinely new forms of hardware acceleration are needed, such a development platform would reduce the design burden and let researchers focus on new ideas.  We specifically made the following contributions:\nFirst, we developed a unifying design space encompassing many accelerators: e.g. application/domain-specific, systolic arrays, reconfigurable, and vector architectures.  This design-space was codified as a graph representation, called the architecture description graph (ADG).  The ADG represents accelerators as a composition of composable primitives like memories, switches, controllers, synchronization, and computation units.  We developed hardware generation tools that could produce the RTL for the ADG (i.e. outputting a multicore-accelerator system), which optimizes the ISA for the required functionality.  We also developed the algorithms and implementation of a compiler that could understand how to compile C programs onto any graph, dealing with the variability of features at the ISA level in different accelerator instances.  Our open-source framework, DSAGEN, can produce designs that significantly out-perform manual designs, with no human design effort.\nOne of the key challenges is irregularity -- how to support algorithms with data dependences.  Irregularity is challenging because it interferes with vectorization and prefetching techniques, which are two of the most important optimizations for data-processing workloads.  What we found is that not all types of irregularity need to be supported -- primitives for data-dependence idioms are sufficient to support many important workloads, and don\u2019t require general purpose support.  Our work discovered key hardware/software primitives for joining, indirection, and task scheduling.  These primitives enabled flexible hardware accelerators to be competitive with, and sometimes outperform, their more specialized or domain-specific counterparts.\nOur work also uniquely focused on performing specialization for memory systems.  While many processors of today have a thin interface to memory, consisting of only single addresses, we found that supplying richer semantics for memory operations can open up a variety of new optimizations.  Specifically, we explored exposing the concepts of \"streams\", or long term memory access patterns, to the memory system through simple ISA extensions.  Streams can be used for implementing \"perfect\" prefetching that is highly flexible and much more accurate than conventional learning-based prefetchers.  Streams provide information that can inform cache bypassing and replacement policies.  They are also useful for enabling near-data computing, both by offloading long-term memory access patterns into the memory system to eliminate request traffic, and also as an abstraction to co-locate computation.  These optimizations together enable scaling to larger multicores by greatly reducing network traffic on chip.\nFinally, we developed our ideas into a framework that can simplify FPGA software development. The issue with the mainstream approach of high-level synthesis (HLS), is that it leaves a significant programmability gap in terms of reconfigurability, customization and versatility: 1. FPGA physical design can take hours, 2. FPGA reconfiguration time limits HLS from targeting complex workloads, and 3. HLS tools do not reason about cross-workload flexibility. We developed a new FPGA programming framework called OverGen, where an overlay architecture is automatically specialized to a set of representative applications.  This work overcomes the limitations of prior overlay architectures through automated specialization.  While it requires more resources to support more general architectures, we found that OverGen achieves nearly equivalent performance with state-of-the-art HLS, while requiring 10,000x less compile and reconfiguration time. \nOverall, this body of work took significant steps towards making it both easier to design accelerators, while also making them more general purpose and effective at challenging problems.  Finally, this grant was the primary funding source for three PhDs, and was instrumental in enabling their academic success.\n\n\n\t\t\t\t\tLast Modified: 08/28/2023\n\n\t\t\t\t\tSubmitted by: Anthony Nowatzki"
 }
}