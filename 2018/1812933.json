{
 "awd_id": "1812933",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: Wearable Fingertip Haptic Devices for Virtual and Augmented Reality: Design, Control, and Predictive Tracking",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 173338.0,
 "awd_amount": 173338.0,
 "awd_min_amd_letter_date": "2018-07-30",
 "awd_max_amd_letter_date": "2018-07-30",
 "awd_abstract_narration": "In the real world, people rely heavily on haptic (force and tactile) feedback to manipulate and explore objects.  However, in virtual and augmented reality environments, haptic sensations must be artificially generated.  Advances in both sensing and feedback are needed to create more realistic virtual scenarios for human interaction, education, and training.  This research will implement wearable fingertip haptic feedback systems that enhance human perception and task performance in virtual and augmented reality, and will provide the field of computer-mediated haptics with important new design and rendering insights for a promising form of wearable haptic device.  Successful development of fingertip haptic devices and improved tracking methods will lower the cost of integrating haptic technology into virtual reality, and yield more realistic and compelling virtual experiences.  Effective haptic feedback systems for virtual reality utilizing the developed technologies will have broad impact by improving human health and well-being through a myriad of applications such as training for critical tasks like surgery and defusing of explosive ordnance, tactile communication to enable design and e-commerce, and immersion in virtual worlds for enhanced education and training.  Project outcomes will be disseminated through demonstrations of educational applications, expansion of an online course on haptics, and publicly available software and data.  Outreach programs, public lab tours, and mentoring of female and minority graduate students, undergraduates, and high school students will broaden participation of underrepresented groups in engineering.\r\n\r\nThis project has three main technical components.  First, instrumented objects will be developed that measure force interactions and integrate that data with external measurements of finger pose.  Second, new wearable haptic devices will be developed that use a combination of local kinesthetic feedback and skin deformation to provide realistic haptic feedback with minimal encumbrance.  Modular haptic device designs will facilitate rapid testing of various design choices.  Then the basic design will be optimized in terms of control strategy, degrees of freedom, skin contact conditions, and finger grounding to best match the real-world grasping and manipulation data collected earlier.  Third, predictive tracking algorithms will be developed for grasping based on characteristic behaviors observed in real-world grasping and manipulation data.  All of the haptic devices and tracking methods will be evaluated both in virtual and in augmented reality environments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yu",
   "pi_last_name": "Sun",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yu Sun",
   "pi_email_addr": "yusun@usf.edu",
   "nsf_id": "000552980",
   "pi_start_date": "2018-07-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of South Florida",
  "inst_street_address": "4202 E FOWLER AVE",
  "inst_street_address_2": "",
  "inst_city_name": "TAMPA",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "8139742897",
  "inst_zip_code": "336205800",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "FL15",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTH FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NKAZLXLL7Z91"
 },
 "perf_inst": {
  "perf_inst_name": "University of South Florida",
  "perf_str_addr": "3702 Spectrum Blvd.",
  "perf_city_name": "Tampa",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "336129446",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "FL15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 173338.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>At USF, we have created a finger-object-interaction tracking system that is composed of a VICON Vero 2.2 motion tracking system for finger motion tracking, an Intel RealSense RGBD camera for scene capturing, and a six-axis ATI force sensor for contact force recording. It is used to capture the finger-object interactions in human grasping and manipulations and study the finger-object interaction in terms of finger motion and the contact force between the fingertip and the object.</p>\n<p>Using the finger-object-interaction tracking system, we have collected numerous grasping and manipulation sequences, including lifting mass, pouring, and twisting and opening a bottle cap. We continue to collect new data and are in preparation for releasing the collected data set. Using the finger-object-interaction tracking system and the collected data, we have studied different types of manipulations and developed a motion taxonomy that groups similar motions based on trajectories and contact types. We have also recorded human grasping of multiple objects and discovered six multi-object grasping types, and developed a multi-object grasping taxonomy.</p>\n<p>The research has supported and trained two Ph.D. students. They are the main contributors in designing the measurement setup and performing a pilot data collection. While performing the research work, they are taught robotics and haptics knowledge and trained rigorously on research methodologies and experiment designs.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/06/2022<br>\n\t\t\t\t\tModified by: Yu&nbsp;Sun</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1812933/1812933_10563394_1670381289158_system--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1812933/1812933_10563394_1670381289158_system--rgov-800width.jpg\" title=\"Data Collection System\"><img src=\"/por/images/Reports/POR/2022/1812933/1812933_10563394_1670381289158_system--rgov-66x44.jpg\" alt=\"Data Collection System\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Finger-object-interaction tracking system that is composed of a VICON Vero 2.2 motion tracking system for finger motion tracking, an Intel RealSense RGBD camera for scene capturing, and a six-axis ATI force sensor for contact force recording</div>\n<div class=\"imageCredit\">Yu Sun</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Yu&nbsp;Sun</div>\n<div class=\"imageTitle\">Data Collection System</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1812933/1812933_10563394_1670381436786_collection--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1812933/1812933_10563394_1670381436786_collection--rgov-800width.jpg\" title=\"Example\"><img src=\"/por/images/Reports/POR/2022/1812933/1812933_10563394_1670381436786_collection--rgov-66x44.jpg\" alt=\"Example\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The recorded RGBD camera readings; finger and hand tracking visualization; force and torque sensor reading.</div>\n<div class=\"imageCredit\">Yu Sun</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Yu&nbsp;Sun</div>\n<div class=\"imageTitle\">Example</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nAt USF, we have created a finger-object-interaction tracking system that is composed of a VICON Vero 2.2 motion tracking system for finger motion tracking, an Intel RealSense RGBD camera for scene capturing, and a six-axis ATI force sensor for contact force recording. It is used to capture the finger-object interactions in human grasping and manipulations and study the finger-object interaction in terms of finger motion and the contact force between the fingertip and the object.\n\nUsing the finger-object-interaction tracking system, we have collected numerous grasping and manipulation sequences, including lifting mass, pouring, and twisting and opening a bottle cap. We continue to collect new data and are in preparation for releasing the collected data set. Using the finger-object-interaction tracking system and the collected data, we have studied different types of manipulations and developed a motion taxonomy that groups similar motions based on trajectories and contact types. We have also recorded human grasping of multiple objects and discovered six multi-object grasping types, and developed a multi-object grasping taxonomy.\n\nThe research has supported and trained two Ph.D. students. They are the main contributors in designing the measurement setup and performing a pilot data collection. While performing the research work, they are taught robotics and haptics knowledge and trained rigorously on research methodologies and experiment designs.\n\n \n\n\t\t\t\t\tLast Modified: 12/06/2022\n\n\t\t\t\t\tSubmitted by: Yu Sun"
 }
}