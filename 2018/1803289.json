{
 "awd_id": "1803289",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Fellowship Award",
 "awd_titl_txt": "PostDoctoral Research Fellowship",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924878",
 "po_email": "adpollin@nsf.gov",
 "po_sign_block_name": "Andrew Pollington",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2018-03-28",
 "awd_max_amd_letter_date": "2018-03-28",
 "awd_abstract_narration": "This award is made as part of the FY 2018 Mathematical Sciences Postdoctoral Research Fellowships Program. Each of the fellowships supports a research and training project at a host institution in the mathematical sciences, including applications to other disciplines, under the mentorship of a sponsoring scientist. The title of the project for this fellowship to Courtney Paquette is \"Structure and complexity in non-convex and nonsmooth optimization.\" The host institution for the fellowship is\u00a0University of Waterloo, Waterloo, Ontario, Canada, and the sponsoring scientist is\u00a0Stephen Vavasis.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Courtney",
   "pi_last_name": "Paquette",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "Courtney Y Paquette",
   "pi_email_addr": "",
   "nsf_id": "000758746",
   "pi_start_date": "2018-03-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Paquette                Courtney       Y",
  "inst_street_address": "",
  "inst_street_address_2": "",
  "inst_city_name": "Columbus",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "",
  "inst_zip_code": "432101101",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "",
  "org_prnt_uei_num": "",
  "org_uei_num": ""
 },
 "perf_inst": {
  "perf_inst_name": "University of Waterloo",
  "perf_str_addr": null,
  "perf_city_name": "Waterloo",
  "perf_st_code": "",
  "perf_st_name": "RI REQUIRED",
  "perf_zip_code": "",
  "perf_ctry_code": "CA",
  "perf_cong_dist": "",
  "perf_st_cong_dist": "",
  "perf_ctry_name": "Canada",
  "perf_ctry_flag": "0"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "060Y00",
   "pgm_ele_name": "Workforce (MSPRF) MathSciPDFel"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9219",
   "pgm_ref_txt": "POSTDOCTORAL FELLOWSHIPS IN MATH SCIENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">Modern applications of continuous optimization seek to extract conclusions from large data sets. Nonconvex formulations, in particular, have gained much interest driven by applications in high dimensional statistics and machine learning. Consequently, first order methods, algorithms which rely only on function value and gradient information, have dominated large scale computing. Classical complexity theory based on oracles targets problems that are both smooth and convex. Without smoothness and convexity, successful first order methods exploit underlying structure: for example low dimensionality and separation of smooth and nonsmooth functional components. This project addressed problems related to classical complexity theory and stochastic optimization with applications in machine learning.&nbsp;</p>\n<p class=\"p1\">A central task in machine learning is minimizing objective functions where the function values and derivatives cannot be computed directly but are accessible only through noisy approximations. A typical algorithm to solve these tasks is the stochastic gradient method. Despite the prevalent use of this method, it is known to be sensitive to the step-size (learning rate). In deterministic settings, line-search strategies were developed to provide stability, improved efficiency, and adaptivity to unknown parameters, such as stepsizes. While the practical benefits for deterministic algorithms are well-documented, line-search strategies for stochastic algorithms are in their infancy. Scheinberg and the PI presented one of the first practical <em>adaptive</em> line-search method for stochastic optimization, which has rigorous convergence guarantees and requires only knowable quantities for implementation.&nbsp;</p>\n<p class=\"p1\">Another challenge for implementing stochastic algorithms is determining a suitable criterion for when to stop the algorithm that is both cost efficient and close enough to the true solution. Vavasis, Baghal, and the PI designed a practical stopping criterion for logistic regression that satisfied these two desirable properties.</p>\n<p class=\"p1\">Lastly the project explored acceleration -- a widely used term in contemporary optimization to describe methods with efficiency guarantees that match the best possible complexity for a given problem class. With Vavasis, the PI made progress by finding a unifying theory, based on a single potential function, as a way of explaining all optimal first-order methods for composite minimization problems.&nbsp;</p>\n<p class=\"p1\">These works have implications for both industry and the academic communities. The proposed approaches could be applied to many problems in physics, machine learning, image processing, and medicine, to name a few, and used in association with heuristics to develop practical and implementable versions.</p>\n<p class=\"p1\">The project involved collaborations with graduate students and other academic researchers. It supported seminars aimed to advance the knowledge of undergraduates and graduate students in the field of optimization.&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/28/2022<br>\n\t\t\t\t\tModified by: Courtney&nbsp;Y&nbsp;Paquette</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Modern applications of continuous optimization seek to extract conclusions from large data sets. Nonconvex formulations, in particular, have gained much interest driven by applications in high dimensional statistics and machine learning. Consequently, first order methods, algorithms which rely only on function value and gradient information, have dominated large scale computing. Classical complexity theory based on oracles targets problems that are both smooth and convex. Without smoothness and convexity, successful first order methods exploit underlying structure: for example low dimensionality and separation of smooth and nonsmooth functional components. This project addressed problems related to classical complexity theory and stochastic optimization with applications in machine learning. \nA central task in machine learning is minimizing objective functions where the function values and derivatives cannot be computed directly but are accessible only through noisy approximations. A typical algorithm to solve these tasks is the stochastic gradient method. Despite the prevalent use of this method, it is known to be sensitive to the step-size (learning rate). In deterministic settings, line-search strategies were developed to provide stability, improved efficiency, and adaptivity to unknown parameters, such as stepsizes. While the practical benefits for deterministic algorithms are well-documented, line-search strategies for stochastic algorithms are in their infancy. Scheinberg and the PI presented one of the first practical adaptive line-search method for stochastic optimization, which has rigorous convergence guarantees and requires only knowable quantities for implementation. \nAnother challenge for implementing stochastic algorithms is determining a suitable criterion for when to stop the algorithm that is both cost efficient and close enough to the true solution. Vavasis, Baghal, and the PI designed a practical stopping criterion for logistic regression that satisfied these two desirable properties.\nLastly the project explored acceleration -- a widely used term in contemporary optimization to describe methods with efficiency guarantees that match the best possible complexity for a given problem class. With Vavasis, the PI made progress by finding a unifying theory, based on a single potential function, as a way of explaining all optimal first-order methods for composite minimization problems. \nThese works have implications for both industry and the academic communities. The proposed approaches could be applied to many problems in physics, machine learning, image processing, and medicine, to name a few, and used in association with heuristics to develop practical and implementable versions.\nThe project involved collaborations with graduate students and other academic researchers. It supported seminars aimed to advance the knowledge of undergraduates and graduate students in the field of optimization. \n \n \n \n\n \n\n\t\t\t\t\tLast Modified: 10/28/2022\n\n\t\t\t\t\tSubmitted by: Courtney Y Paquette"
 }
}