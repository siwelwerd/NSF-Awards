{
 "awd_id": "1763786",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: Medium: Collaborative Research: Foundations of Adaptive Data Analysis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2018-03-01",
 "awd_exp_date": "2022-02-28",
 "tot_intn_awd_amt": 260000.0,
 "awd_amount": 260000.0,
 "awd_min_amd_letter_date": "2018-02-14",
 "awd_max_amd_letter_date": "2019-09-17",
 "awd_abstract_narration": "Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. \r\n\r\nThe technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of \"over-fitting\" and \"false discovery.\" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Adam",
   "pi_last_name": "Smith",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Adam Smith",
   "pi_email_addr": "ads22@bu.edu",
   "nsf_id": "000105737",
   "pi_start_date": "2018-02-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "111 Cummington Mall",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022152413",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 86913.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 173087.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-6ea56003-7fff-03ab-592d-ef941b9ca800\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project has advanced the science of adaptive, also known as exploratory, data analysis, which was the primary research goal.&nbsp; Prior to this project, Co-PIs Dwork and Roth, together with other collaborators, in 2014, established that differential privacy provides a universal protection against the risks to validity incurred by adaptivity in data analysis.&nbsp; They did this through the establishment of transfer theorems which convert the privacy and accuracy guarantees of an arbitrary differentially private algorithm to distributional accuracy guarantees for queries posed to the dataset.&nbsp; Intuitively, the result says that differentially private access to a dataset prevents an &ldquo;accuracy adversary&rdquo; from formulating a query whose value on the dataset differs substantially from its value on the population as a whole.&nbsp; They used these ideas to construct the first reusable holdout set.&nbsp; Later, co-PI Smith, with other collaborators, introduced new techniques leading to improved transfer theorems.&nbsp; Because of the tight connection to differential privacy, the development of differentially private algorithms for new tasks and improved differentially private algorithms for previously studied tasks, contribute to the literature on adaptive data analysis.</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">During the course of this grant, co-PI Roth and collaborators obtained the current state-of-the-art transfer theorem for linear queries.&nbsp; Co-PIs Roth and Smith and collaborators developed a software package for answering linear queries on a holdout set with concrete confidence intervals, allowing for safe but &ldquo;optimistic&rdquo; answering of linear queries, where the system either provides a tight confidence interval or certifies that a highly adaptive query was posed.&nbsp; The project also gave rise to several advances in privacy-protective and adaptive multiple-hypothesis testing, as well as the development of new relaxations of differential privacy, specifically, t-CDP (truncated Differential Privacy), and f-Differential Privacy and Gaussian Differential Privacy, which use the entire trade-off between type I and type II errors of the hypothesis testing problem as a measure of the privacy guarantees.&nbsp;&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Differential privacy is a stability guarantee&mdash;it says that the distribution on outcomes of any analysis will remain essentially unchanged under small perturbations of the dataset.&nbsp; The project also developed new notions of stability leading to improved results in generalization theory over traditional stability notions and a characterization of optimal simple hypothesis tests that are algorithmically stable.</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Because adaptivity is such a natural source of honest mistakes that completely destroy validity, formulating a benign adversary that is less dangerous than a malicious adversary has proved challenging: there are some very simple counterexamples.&nbsp; Instead, Co-PI Dwork formulated data models that are realistic in some computational settings, such as streaming analysis of DNA, under which it is possible to &ldquo;re-use&rdquo; the privacy budget to obtain more accurate results than when the data generation and access models are unrestricted.&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A connection, established elsewhere, between differential privacy and robustness in machine learning led the project to the study of robustness and adversarial training of neural nets.&nbsp; Here, developments pioneered by PIs Dwork, Zou, Roth and Smith include an emerging theory addressing questions of why and when gradient method based adversarial training works, how to quantify the trade-off between adversarial robustness and prediction accuracy, and how to further use out-of domain data to enhance adversarial robustness, as well progress in analyzing why popular modern data augmentation techniques help in adversarial robustness and generalization in learning. In addition, research in representation learning has explored how adversarial training and data augmentation techniques such as contrastive learning encourages more transferable and expressive representations.&nbsp;&nbsp;&nbsp;&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Another significant development, led by PI Smith and collaborators, was the first cohesive framework for reasoning about example memorization in machine learning&mdash;the oft-noted but poorly understood phenomenon by which current, large neural network models encode in their parameters detailed versions of many of their training examples. The results included natural learning problems where this type of memorization is unavoidable for any method that obtains a high-accuracy predictor&mdash;no matter the type of model and no matter how the training algorithm works.</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Finally and importantly, the project supported the development of courses on adaptive data analysis at the University of Pennsylvania, Boston University, and Harvard University, and course materials are now available at </span><a style=\"text-decoration: none;\" href=\"https://adaptivedataanalysis.com/about/\"><span style=\"font-size: 11pt; font-family: Arial; color: #2693d6; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;\">The Algorithmic Foundations of Adaptive Data Analysis</span></a><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/24/2022<br>\n\t\t\t\t\tModified by: Adam&nbsp;Smith</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "This project has advanced the science of adaptive, also known as exploratory, data analysis, which was the primary research goal.  Prior to this project, Co-PIs Dwork and Roth, together with other collaborators, in 2014, established that differential privacy provides a universal protection against the risks to validity incurred by adaptivity in data analysis.  They did this through the establishment of transfer theorems which convert the privacy and accuracy guarantees of an arbitrary differentially private algorithm to distributional accuracy guarantees for queries posed to the dataset.  Intuitively, the result says that differentially private access to a dataset prevents an \"accuracy adversary\" from formulating a query whose value on the dataset differs substantially from its value on the population as a whole.  They used these ideas to construct the first reusable holdout set.  Later, co-PI Smith, with other collaborators, introduced new techniques leading to improved transfer theorems.  Because of the tight connection to differential privacy, the development of differentially private algorithms for new tasks and improved differentially private algorithms for previously studied tasks, contribute to the literature on adaptive data analysis.\nDuring the course of this grant, co-PI Roth and collaborators obtained the current state-of-the-art transfer theorem for linear queries.  Co-PIs Roth and Smith and collaborators developed a software package for answering linear queries on a holdout set with concrete confidence intervals, allowing for safe but \"optimistic\" answering of linear queries, where the system either provides a tight confidence interval or certifies that a highly adaptive query was posed.  The project also gave rise to several advances in privacy-protective and adaptive multiple-hypothesis testing, as well as the development of new relaxations of differential privacy, specifically, t-CDP (truncated Differential Privacy), and f-Differential Privacy and Gaussian Differential Privacy, which use the entire trade-off between type I and type II errors of the hypothesis testing problem as a measure of the privacy guarantees.  \nDifferential privacy is a stability guarantee&mdash;it says that the distribution on outcomes of any analysis will remain essentially unchanged under small perturbations of the dataset.  The project also developed new notions of stability leading to improved results in generalization theory over traditional stability notions and a characterization of optimal simple hypothesis tests that are algorithmically stable.\nBecause adaptivity is such a natural source of honest mistakes that completely destroy validity, formulating a benign adversary that is less dangerous than a malicious adversary has proved challenging: there are some very simple counterexamples.  Instead, Co-PI Dwork formulated data models that are realistic in some computational settings, such as streaming analysis of DNA, under which it is possible to \"re-use\" the privacy budget to obtain more accurate results than when the data generation and access models are unrestricted. \nA connection, established elsewhere, between differential privacy and robustness in machine learning led the project to the study of robustness and adversarial training of neural nets.  Here, developments pioneered by PIs Dwork, Zou, Roth and Smith include an emerging theory addressing questions of why and when gradient method based adversarial training works, how to quantify the trade-off between adversarial robustness and prediction accuracy, and how to further use out-of domain data to enhance adversarial robustness, as well progress in analyzing why popular modern data augmentation techniques help in adversarial robustness and generalization in learning. In addition, research in representation learning has explored how adversarial training and data augmentation techniques such as contrastive learning encourages more transferable and expressive representations.    \nAnother significant development, led by PI Smith and collaborators, was the first cohesive framework for reasoning about example memorization in machine learning&mdash;the oft-noted but poorly understood phenomenon by which current, large neural network models encode in their parameters detailed versions of many of their training examples. The results included natural learning problems where this type of memorization is unavoidable for any method that obtains a high-accuracy predictor&mdash;no matter the type of model and no matter how the training algorithm works.\nFinally and importantly, the project supported the development of courses on adaptive data analysis at the University of Pennsylvania, Boston University, and Harvard University, and course materials are now available at The Algorithmic Foundations of Adaptive Data Analysis.\n\n\t\t\t\t\tLast Modified: 06/24/2022\n\n\t\t\t\t\tSubmitted by: Adam Smith"
 }
}