{
 "awd_id": "1711272",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Exploring Differences Between Instructors' Exams and How These Differences Produce Scores that Could Inaccurately and Inequitably Represent Student Understanding",
 "cfda_num": "47.076",
 "org_code": "11040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ellen Carpenter",
 "awd_eff_date": "2017-12-01",
 "awd_exp_date": "2022-11-30",
 "tot_intn_awd_amt": 249922.0,
 "awd_amount": 249922.0,
 "awd_min_amd_letter_date": "2017-11-22",
 "awd_max_amd_letter_date": "2021-12-22",
 "awd_abstract_narration": "In many STEM courses, students' exam scores determine their course grades.  In turn, when averaged together, course grades determine each student's grade point average, which can affect their persistence in a STEM major and their competitiveness for admission to professional or graduate schools. Thus, it is important that these exams accurately and equitably measure students' understanding of the subject matter that they are supposed to test. Little research has been done to determine whether specific exam questions accurately measure student understanding or whether they are fair to all students.  This collaborative project between Arizona State University and University of Washington will try to fill this gap in knowledge by analyzing questions on introductory biology course exams taught by different instructors.  They will examine the relationships between different types of questions, student scores on the questions, and student understanding of the concept that the question is supposed to test.  This information has the potential to help biology instructors more fairly and accurately test student understanding of biology.   It may also provide guidelines for building fair and accurate exam questions that are relevant to other STEM disciplines.\r\n\r\nThis project has four key goals: (1) characterizing the composition of instructor-generated biology exams across sections of the same introductory biology course; (2) characterizing elements of questions that result in students performing differently on questions; (3) determining if modifying questions can diminish differences in student performance; and (4) correlating exam scores to students' conceptual understanding of biology. This project will focus on the exams of different instructors teaching the same introductory biology course offered across multiple institutions within the same regional network. No published studies have explored differences in question performance on instructor-generated exams in introductory biology courses. Further, this project would be the largest, most comprehensive analysis of instructor-generated exams and questions in any STEM discipline done so far, providing insights into what factors may affect performance differences on individual questions.  This project can help instructors and researchers become more aware of elements of questions that result in unfair evaluation of certain groups of students, leading to more accurate and equitable measurement of student understanding.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sara",
   "pi_last_name": "Brownell",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Sara E Brownell",
   "pi_email_addr": "sara.brownell@asu.edu",
   "nsf_id": "000636591",
   "pi_start_date": "2020-01-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Christian",
   "pi_last_name": "Wright",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christian Wright",
   "pi_email_addr": "cdwrigh2@asu.edu",
   "nsf_id": "000681973",
   "pi_start_date": "2017-11-22",
   "pi_end_date": "2020-01-21"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Sara",
   "pi_last_name": "Brownell",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Sara E Brownell",
   "pi_email_addr": "sara.brownell@asu.edu",
   "nsf_id": "000636591",
   "pi_start_date": "2017-11-22",
   "pi_end_date": "2020-01-21"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Christian",
   "pi_last_name": "Wright",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christian Wright",
   "pi_email_addr": "cdwrigh2@asu.edu",
   "nsf_id": "000681973",
   "pi_start_date": "2020-01-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "P.O. Box 876011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852876011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "199800",
   "pgm_ele_name": "IUSE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8209",
   "pgm_ref_txt": "Improv Undergrad STEM Ed(IUSE)"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0418",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001819DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 249922.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Instructors write exams that are the basis for student grades and are meant to gauge student understanding, yet there are well-established inequities in student exam scores.&nbsp; Specically, inequities have been observed with men and students from higher socioeconomic statuses performing better than women and students from lower socioeconomic statuses.&nbsp; However, we did not know where these inequities were originating from: the individual exam questions, the format of the exam, or other factors that were influencing students when they were taking the exam.&nbsp; This project aimed to identify if there is bias in individual exam questions and surprisingly, we found that there were few questions that instructors wrote that had bias.&nbsp; This implies that other factors influence the inequities in exam scores, such as test format or test anxiety.&nbsp; We found that giving students the option to retake their exams could decrease their test anxiety and increase their exam scores overall, but actually increased inequities in student scores.&nbsp; Finally, we identified that during the COVID-19 pandemic, student exam scores were actually higher than semesters before the pandemic, but student perceived that they learned less, which indicates that instructors likely changed the rigor of exams.&nbsp; All of this work indicates that how instructors both write exam questions and administer exams can impact students in ways that can increase inequities.&nbsp;&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/23/2023<br>\n\t\t\t\t\tModified by: Sara&nbsp;E&nbsp;Brownell</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nInstructors write exams that are the basis for student grades and are meant to gauge student understanding, yet there are well-established inequities in student exam scores.  Specically, inequities have been observed with men and students from higher socioeconomic statuses performing better than women and students from lower socioeconomic statuses.  However, we did not know where these inequities were originating from: the individual exam questions, the format of the exam, or other factors that were influencing students when they were taking the exam.  This project aimed to identify if there is bias in individual exam questions and surprisingly, we found that there were few questions that instructors wrote that had bias.  This implies that other factors influence the inequities in exam scores, such as test format or test anxiety.  We found that giving students the option to retake their exams could decrease their test anxiety and increase their exam scores overall, but actually increased inequities in student scores.  Finally, we identified that during the COVID-19 pandemic, student exam scores were actually higher than semesters before the pandemic, but student perceived that they learned less, which indicates that instructors likely changed the rigor of exams.  All of this work indicates that how instructors both write exam questions and administer exams can impact students in ways that can increase inequities.  \n\n\t\t\t\t\tLast Modified: 06/23/2023\n\n\t\t\t\t\tSubmitted by: Sara E Brownell"
 }
}