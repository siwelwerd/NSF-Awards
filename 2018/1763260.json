{
 "awd_id": "1763260",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CSR: Medium: Collaborative Research: GP&#955;: General-Purpose Lambda Computing",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Erik Brunvand",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2018-07-27",
 "awd_max_amd_letter_date": "2019-09-16",
 "awd_abstract_narration": "The project will design ways for computer programs to do things in a few seconds that would ordinarily take hours, by borrowing thousands of computers at once. Until recently, this was not possible or economical: the companies that rent computers required minutes of advance notice, with a minimum bill of at least ten minutes. Recently, new technology called \"function-as-a-service computing\" allows people to rent thousands of unreliable computers at the same time with no advance notice, for fractions of a second. This project will design software, storage systems, and programming languages to use this technology robustly for large calculations.\r\n\r\nThis research aims to deliver a new model of \"burst parallel\" applications and systems. This model takes advantage of new function-as-a-service offerings by cloud-computing providers such as Amazon, Microsoft, and Google. In this model, fine-grained \"ephemeral lambdas\" enable large-scale parallel computations at low cost.  With careful attention to algorithm parallelization, we argue that many both old and new data processing tasks can be adapted into a model of functions that operate on small pieces of data, executed in a burst-parallel manner to achieve interactive performance demands, in a way that can be debugged, traced, and understood.\r\n\r\nThis project intends to provide the application blueprint and systems foundation to enable end-users and developers to access massive amounts of parallelism on demand, while only paying for those resources actually consumed by an application. The researchers will engage with the developer community to ensure that others are able to use and build upon the work. In addition to integrating this research into course curricula, the researchers have a formalized program for embedding under-represented minority undergraduate students into their research groups.\r\n\r\nSoftware will be released under open-source software licenses pursuant to the policies of the University of California and Stanford University, and distributed through the project website and through traditional means of distribution of open-source software (e.g., GNU/Linux distributions, ports collections, app stores, etc.). Data, code, and evaluation results will continue to be hosted for at least three years after project completion at https://gplambda.github.io.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "George",
   "pi_last_name": "Porter",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "George Porter",
   "pi_email_addr": "gmporter@cs.ucsd.edu",
   "nsf_id": "000553493",
   "pi_start_date": "2018-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Geoffrey",
   "pi_last_name": "Voelker",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Geoffrey M Voelker",
   "pi_email_addr": "voelker@cs.ucsd.edu",
   "nsf_id": "000441341",
   "pi_start_date": "2018-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Drive #0934",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 133333.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 266667.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Advancements in increasingly lightweight virtualized containers has enabled major cloud providers to offer very short-lived execution contexts, paired with new fine-grained pricing models. This so-called serverless execution model was primarily aimed at microservice deployment, however we envision adopting it to implement &ldquo;burst parallel&rdquo; computing, which allows applications to rapidly deploy hundreds or thousands of concurrent ephemeral \"lambdas\" on demand. This capability enables applications to quickly scale up their level of parallelism at sub-second timespans, reducing the run time of data processing tasks by multiple orders of magnitude and transforming batch tasks into interactive ones.&nbsp; The goal of this project has been to develop the systems and networking support necessary to realize this vision. Furthermore, we have built applications on top of this infrastructure to demonstrate the potential of burst parallel computing.</p>\n<p>We developed a burst-parallel video processing application called Sprocket, which we deployed onto Amazon&rsquo;s &ldquo;Lambda&rdquo; serverless platform.&nbsp; Sprocket implements video pipelines, which are composed operations that manipulate, edit, and transcode video content for later consumption. Sprocket divides this task, which is traditionally done in a serial fashion, into many thousand smaller units of work, which run in a burst-parallel manner. In addition to taking advantage of the parallelism offered by lightweight containers, Sprocket enables video pipeline stages to perform advanced analysis of individual frames by interfacing with AWS&rsquo;s Rekognition API that implements a number of computer vision algorithms.&nbsp; The end result is that Sprocket can process a full-length HD movie as a 1000-way burst-parallel job in less than one minute at a cost of less than $3.</p>\n<p>While the systems and networking research community has made great strides in reducing the startup time of virtualized containers, attaching those containers to the network has remained a major performance bottleneck.&nbsp; For burst-parallel applications, many hundreds of containers need access to networking resources, and alleviating the scaling bottlenecks has been a major focus of our work.&nbsp; To this end, we developed Particle, which is an optimized Linux network stack designed for burst-parallel environments.&nbsp; Particle increases scalability by removing intra-tenant isolation from the network stack.&nbsp; For applications in which many containers are designed to work together to complete a task, this isolation is not necessary, and eschewing it enables a dramatic reduction in network startup time, allowing serverless burst-parallel applications to increase their scale with lower resource requirements and with lower latency.</p>\n<p>The work in this grant has been made available to the public through the release of open-source code. The mechanisms and systems have been documented in published papers at the ACM Symposium on Cloud Computing conference. &nbsp;The project has provided research mentorship and training to doctoral students as well as undergraduates through UC San Diego&rsquo;s Early Research Scholars Program (ERSP).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/06/2022<br>\n\t\t\t\t\tModified by: George&nbsp;Porter</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAdvancements in increasingly lightweight virtualized containers has enabled major cloud providers to offer very short-lived execution contexts, paired with new fine-grained pricing models. This so-called serverless execution model was primarily aimed at microservice deployment, however we envision adopting it to implement \"burst parallel\" computing, which allows applications to rapidly deploy hundreds or thousands of concurrent ephemeral \"lambdas\" on demand. This capability enables applications to quickly scale up their level of parallelism at sub-second timespans, reducing the run time of data processing tasks by multiple orders of magnitude and transforming batch tasks into interactive ones.  The goal of this project has been to develop the systems and networking support necessary to realize this vision. Furthermore, we have built applications on top of this infrastructure to demonstrate the potential of burst parallel computing.\n\nWe developed a burst-parallel video processing application called Sprocket, which we deployed onto Amazon\u2019s \"Lambda\" serverless platform.  Sprocket implements video pipelines, which are composed operations that manipulate, edit, and transcode video content for later consumption. Sprocket divides this task, which is traditionally done in a serial fashion, into many thousand smaller units of work, which run in a burst-parallel manner. In addition to taking advantage of the parallelism offered by lightweight containers, Sprocket enables video pipeline stages to perform advanced analysis of individual frames by interfacing with AWS\u2019s Rekognition API that implements a number of computer vision algorithms.  The end result is that Sprocket can process a full-length HD movie as a 1000-way burst-parallel job in less than one minute at a cost of less than $3.\n\nWhile the systems and networking research community has made great strides in reducing the startup time of virtualized containers, attaching those containers to the network has remained a major performance bottleneck.  For burst-parallel applications, many hundreds of containers need access to networking resources, and alleviating the scaling bottlenecks has been a major focus of our work.  To this end, we developed Particle, which is an optimized Linux network stack designed for burst-parallel environments.  Particle increases scalability by removing intra-tenant isolation from the network stack.  For applications in which many containers are designed to work together to complete a task, this isolation is not necessary, and eschewing it enables a dramatic reduction in network startup time, allowing serverless burst-parallel applications to increase their scale with lower resource requirements and with lower latency.\n\nThe work in this grant has been made available to the public through the release of open-source code. The mechanisms and systems have been documented in published papers at the ACM Symposium on Cloud Computing conference.  The project has provided research mentorship and training to doctoral students as well as undergraduates through UC San Diego\u2019s Early Research Scholars Program (ERSP).\n\n\t\t\t\t\tLast Modified: 01/06/2022\n\n\t\t\t\t\tSubmitted by: George Porter"
 }
}