{
 "awd_id": "1813974",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: Understanding, Measuring, and Defending against Malicious Web Crawlers",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jeremy Epstein",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 499567.0,
 "awd_amount": 499567.0,
 "awd_min_amd_letter_date": "2018-08-30",
 "awd_max_amd_letter_date": "2018-08-30",
 "awd_abstract_narration": "Given the constant expansion of the web, search engines rely on automated web crawlers to automatically discover new web pages and index them. Next to search engines, many different industries rely on web crawlers, ranging from security-related crawlers that find abusive pages, to crawlers that take snapshots of content in order to show previews of pages on social networks. At the same time, attackers are utilizing malicious crawlers to automatically find and exploit vulnerabilities on websites, to scrape content and email addresses, and to brute-force login forms. This project focuses on better understanding malicious web crawlers, gathering data about their activity online, and developing defensive systems that can differentiate between benign and malicious web crawlers.\r\n\r\nThe project seeks to understand, measure, and defend against malicious web crawlers through a multi-pronged approach. First, the project proposes the development of honeypot-like infrastructure for collecting information on existing benign and malicious crawlers. This information is used to track the most abusive crawlers and offer statistics about crawler activity on the web. Second, the project includes the design of tools and techniques for differentiating between real browsing users and malicious crawlers that pretend to be real users. Third, the project proposes the design, development, and evaluation of technologies for real-time detection of web crawlers and for defending against them. Last, the project includes the design of new crawling protocols that allow legitimate crawlers to work unhindered while severely restricting the crawling abilities of malicious crawlers. The outcomes of this research effort are expected to improve the understanding of malicious crawler activity on the web and to achieve substantial practical impact in protecting benign websites against malicious crawlers. Moreover, by improving the detection of malicious crawlers that compromise websites and exfiltrate user data, the project improves the security of all web users.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nick",
   "pi_last_name": "Nikiforakis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nick Nikiforakis",
   "pi_email_addr": "nick@cs.stonybrook.edu",
   "nsf_id": "000678059",
   "pi_start_date": "2018-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "West 5510 Frk Mel Lib",
  "perf_city_name": "Stony Brook",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117940001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499567.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Given the constant expansion of the web, users increasingly rely on search engines to identify content that is relevant to their interests. In order to compile these vast databases of content, search engines employ automated web crawlers which constantly scour the web, finding new pages, indexing them, and following links to other websites. Next to search engines, many different industries rely on web crawlers, ranging from security-related crawlers that find abusive pages, to crawlers that take snapshots of content in order to show previews of pages on social networks.<br /><br />The same features that make crawlers powerful are also the ones that allow them to become abusive. Attackers are using malicious crawlers to automatically find and exploit vulnerabilities on websites, to scrape content and email addresses, to buy large numbers of tickets for ticket scalping, and to brute-force login forms. Prior to this project, there was little research that aimed to understand how malicious crawlers work, how many families of malicious crawlers operate in the wild, how they evolve over time, and how malicious crawlers could be detected and neutralized.<br /><br />The research outcomes of this project enabled us to better understand malicious bots, their operators, how they attempt to exploit web applications, and how we can defend from them. Among others, we built honeypot-based systems that appear as potential victims to malicious crawlers and deployed hundreds of such honeypots in the wild. We observed crawler-originating traffic and highlighted the tell-tale signs that differentiate that traffic from traffic of users and benign web crawlers. We recorded the geographical location of malicious web crawlers, how these crawlers attempt to fingerprint vulnerable web applications, and how their operators can weaponize zero-day vulnerabilities in mere hours, as opposed to days. We proposed multiple defenses against malicious web crawlers, both by detecting and blocking them in incoming traffic as well as by hardening deployed web applications so that break-ins from crawlers do not necessarily result in a complete takeover of the web application.<br /><br />Next to observing, documenting, and detecting malicious web crawlers, we proposed multiple benign web crawlers, tailored towards the detection of different types of attackers and cybercrime. We showed how our crawlers could identify cryptocurrency scams, advanced phishing sites, and fake websites used for boosting the search-engine ranking of malicious websites. <br /><br />The research work in the context of this project provided the opportunity to graduate and undergraduate students to conduct research in the broader area of malicious web crawlers and defenses against them. Students that worked on this project developed research skills in network and systems programming, malware analysis and detection, digital forensics, data mining and machine learning, as well as experience with technologies such as virtualization, cloud computing, and crowdsourcing.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2023<br>\n\t\t\t\t\tModified by: Nick&nbsp;Nikiforakis</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698650549_Screenshot2023-10-30at16-39-34goodbotbadbot_oakland2021.pdf--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698650549_Screenshot2023-10-30at16-39-34goodbotbadbot_oakland2021.pdf--rgov-800width.jpg\" title=\"ARISTAEUS architecture\"><img src=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698650549_Screenshot2023-10-30at16-39-34goodbotbadbot_oakland2021.pdf--rgov-66x44.jpg\" alt=\"ARISTAEUS architecture\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Architecture of our web-application honeypot system for fingerprinting malicious web crawlers</div>\n<div class=\"imageCredit\">IEEE S&P 2021</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nick&nbsp;Nikiforakis</div>\n<div class=\"imageTitle\">ARISTAEUS architecture</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698698829_Screenshot2023-10-30at16-40-51goodbotbadbot_oakland2021.pdf--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698698829_Screenshot2023-10-30at16-40-51goodbotbadbot_oakland2021.pdf--rgov-800width.jpg\" title=\"Geolocation of bot activity\"><img src=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698698829_Screenshot2023-10-30at16-40-51goodbotbadbot_oakland2021.pdf--rgov-66x44.jpg\" alt=\"Geolocation of bot activity\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Geolocation of bot activity in our S&P 2021 paper</div>\n<div class=\"imageCredit\">IEEE S&P 2021</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nick&nbsp;Nikiforakis</div>\n<div class=\"imageTitle\">Geolocation of bot activity</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698806305_Screenshot2023-10-30at16-41-10goodbotbadbot_oakland2021.pdf--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698806305_Screenshot2023-10-30at16-41-10goodbotbadbot_oakland2021.pdf--rgov-800width.jpg\" title=\"True identities of malicious bots\"><img src=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698806305_Screenshot2023-10-30at16-41-10goodbotbadbot_oakland2021.pdf--rgov-66x44.jpg\" alt=\"True identities of malicious bots\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The results of using TLS fingerprinting on malicious web bots (revealing their true identity, as opposed to their stated identity)</div>\n<div class=\"imageCredit\">IEEE S&P 2021</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nick&nbsp;Nikiforakis</div>\n<div class=\"imageTitle\">True identities of malicious bots</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698888341_Screenshot2023-10-30at16-42-04ctpot_usec2022.pdf--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698888341_Screenshot2023-10-30at16-42-04ctpot_usec2022.pdf--rgov-800width.jpg\" title=\"Traffic that different honeypots attracted\"><img src=\"/por/images/Reports/POR/2023/1813974/1813974_10578235_1698698888341_Screenshot2023-10-30at16-42-04ctpot_usec2022.pdf--rgov-66x44.jpg\" alt=\"Traffic that different honeypots attracted\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Comparison of the hosts that each class of our honeypot hosts attracted</div>\n<div class=\"imageCredit\">USENIX Security 2022</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nick&nbsp;Nikiforakis</div>\n<div class=\"imageTitle\">Traffic that different honeypots attracted</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nGiven the constant expansion of the web, users increasingly rely on search engines to identify content that is relevant to their interests. In order to compile these vast databases of content, search engines employ automated web crawlers which constantly scour the web, finding new pages, indexing them, and following links to other websites. Next to search engines, many different industries rely on web crawlers, ranging from security-related crawlers that find abusive pages, to crawlers that take snapshots of content in order to show previews of pages on social networks.\n\nThe same features that make crawlers powerful are also the ones that allow them to become abusive. Attackers are using malicious crawlers to automatically find and exploit vulnerabilities on websites, to scrape content and email addresses, to buy large numbers of tickets for ticket scalping, and to brute-force login forms. Prior to this project, there was little research that aimed to understand how malicious crawlers work, how many families of malicious crawlers operate in the wild, how they evolve over time, and how malicious crawlers could be detected and neutralized.\n\nThe research outcomes of this project enabled us to better understand malicious bots, their operators, how they attempt to exploit web applications, and how we can defend from them. Among others, we built honeypot-based systems that appear as potential victims to malicious crawlers and deployed hundreds of such honeypots in the wild. We observed crawler-originating traffic and highlighted the tell-tale signs that differentiate that traffic from traffic of users and benign web crawlers. We recorded the geographical location of malicious web crawlers, how these crawlers attempt to fingerprint vulnerable web applications, and how their operators can weaponize zero-day vulnerabilities in mere hours, as opposed to days. We proposed multiple defenses against malicious web crawlers, both by detecting and blocking them in incoming traffic as well as by hardening deployed web applications so that break-ins from crawlers do not necessarily result in a complete takeover of the web application.\n\nNext to observing, documenting, and detecting malicious web crawlers, we proposed multiple benign web crawlers, tailored towards the detection of different types of attackers and cybercrime. We showed how our crawlers could identify cryptocurrency scams, advanced phishing sites, and fake websites used for boosting the search-engine ranking of malicious websites. \n\nThe research work in the context of this project provided the opportunity to graduate and undergraduate students to conduct research in the broader area of malicious web crawlers and defenses against them. Students that worked on this project developed research skills in network and systems programming, malware analysis and detection, digital forensics, data mining and machine learning, as well as experience with technologies such as virtualization, cloud computing, and crowdsourcing.\n\n\t\t\t\t\tLast Modified: 10/30/2023\n\n\t\t\t\t\tSubmitted by: Nick Nikiforakis"
 }
}