{
 "awd_id": "1801426",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Medium: Collaborative: Towards Trustworthy Deep Neural Network Based AI: A Systems Approach",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032920000",
 "po_email": "doliveir@nsf.gov",
 "po_sign_block_name": "Daniela Oliveira",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 789965.0,
 "awd_min_amd_letter_date": "2018-07-20",
 "awd_max_amd_letter_date": "2021-04-22",
 "awd_abstract_narration": "Artificial intelligence (AI) is poised to revolutionize the world in fields ranging from technology to medicine, physics and the social sciences. Yet as AI is deployed in these domains, recent work has shown that systems may be vulnerable to different types of attacks that cause them to misbehave; for instance, attacks that cause an AI system to recognize a stop sign as a speed-limit sign. The project seeks to develop methodologies for testing, verifying and debugging AI systems, with a specific focus on deep neural network (DNN)-based AI systems, to ensure their safety and security. \r\n\r\nThe intellectual merits of the proposed research are encompassed in four new software tools that will be developed: (1) DeepXplore, a tool for automated and systematic testing of DNNs that discovers erroneous behavior that might be either inadvertently or maliciously introduced; (2) BadNets, a framework that automatically generated DNNs with known and stealthy misbehaviours in order to stress-test DeepXplore; (3) SafetyNets; a low-overhead scheme for safe and verifiable execution of DNNs in the cloud; and (4) VisualBackProp; a visual debugging tool for DNNs. The synergistic use of these tools for secure deployment of an AI system for autonomous driving will be demonstrated.\r\n\r\nThe project outcomes will significantly improve the security and safety of AI systems and increase their deployment in safety- and security-critical settings, resulting in broad societal impact. The results of the project will be widely disseminated via publications, talks, open access code, and competitions hosted on sites such as Kaggle and NYU's annual Cyber-Security Awareness Week (CSAW). Furthermore, students from under-represented minority groups in science, technology, engineering and mathematics (STEM) will be actively recruited and mentored to be leaders in this critical area.  \r\n\r\nThe code for this project will be made publicly available via github.com. Preliminary code for the tools that will be developed is already hosted on this website, including DeepXplore (https://github.com/peikexin9/deepxplore) and BadNets (https://github.com/Kooscii/BadNets/). These repositories will be linked to from a homepage that describes the entire project. The project homepage will be hosted on wp.nyu.edu/mlsecproject.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Suman",
   "pi_last_name": "Jana",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Suman Jana",
   "pi_email_addr": "suman@cs.columbia.edu",
   "nsf_id": "000701617",
   "pi_start_date": "2018-07-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "2960 Broadway",
  "perf_city_name": "New York City",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "8237",
   "pgm_ref_txt": "CISE Interagency Agreements"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 300000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 163310.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 326655.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">The security and robustness failures in machine learning-augmented security software can have serious consequences. For example, a malware detector might be easily fooled by an attacker that strategically perturbs a malware to evade detection while keeping the malicious functionality unchanged. Recent works have shown that even the state-of-the-art malware classifiers used by major vendors such as Gmail can be trivially evaded through such perturbation. Such cases demonstrate the urgent need for new techniques to systematically test different desired security, and privacy properties of real-world ML-augmented software.&nbsp;</p>\n<p class=\"p1\"><span class=\"s1\">As part of this project, we explored how to enforce different types of robustness properties on machine learning-augmented software (including deep neural networks) against different classes of adversaries. We demonstarted new training methods and bound propagation techniques that can provably eliminate classes of evasion attacks for a group inputs (i.e., local robustness). We further designed new model architectures to enforce certain robustness properties that can hold for all inputs (i.e., global robustness). We used our framework and tools for training and verifying real-world machine learning models that satisfy such strong robustness properties. We have diseeminiated the details of all our algorithms, model architectures, etc. through peer-reviewed publications as well as open-source technical reports. We have also&nbsp;</span>made all of our prototype implementations open source and publicly available for use by the community.&nbsp;</p>\n<p class=\"p1\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/01/2023<br>\n\t\t\t\t\tModified by: Suman&nbsp;Jana</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The security and robustness failures in machine learning-augmented security software can have serious consequences. For example, a malware detector might be easily fooled by an attacker that strategically perturbs a malware to evade detection while keeping the malicious functionality unchanged. Recent works have shown that even the state-of-the-art malware classifiers used by major vendors such as Gmail can be trivially evaded through such perturbation. Such cases demonstrate the urgent need for new techniques to systematically test different desired security, and privacy properties of real-world ML-augmented software. \nAs part of this project, we explored how to enforce different types of robustness properties on machine learning-augmented software (including deep neural networks) against different classes of adversaries. We demonstarted new training methods and bound propagation techniques that can provably eliminate classes of evasion attacks for a group inputs (i.e., local robustness). We further designed new model architectures to enforce certain robustness properties that can hold for all inputs (i.e., global robustness). We used our framework and tools for training and verifying real-world machine learning models that satisfy such strong robustness properties. We have diseeminiated the details of all our algorithms, model architectures, etc. through peer-reviewed publications as well as open-source technical reports. We have also made all of our prototype implementations open source and publicly available for use by the community. \n \n\n\t\t\t\t\tLast Modified: 02/01/2023\n\n\t\t\t\t\tSubmitted by: Suman Jana"
 }
}