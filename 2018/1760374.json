{
 "awd_id": "1760374",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FRG: Collaborative Research: Randomization as a Resource for Rapid Prototyping",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 366109.0,
 "awd_amount": 366109.0,
 "awd_min_amd_letter_date": "2018-08-02",
 "awd_max_amd_letter_date": "2023-04-26",
 "awd_abstract_narration": "A principled foundation for fast prototyping data analysis methods will be developed.  The main approach will be to use fast randomized matrix algorithms, as developed within the research area known as Randomized Numerical Linear Algebra (RandNLA).  Prior work has shown that these RandNLA algorithms come with strong theory and that they perform well for many practical data science and machine learning problems.  The foundation will develop novel uses of randomization to combine complementary algorithmic and statistical perspectives. The statistical viewpoint attributes randomness to an inherent and desirable property of the data, while the algorithmic viewpoint claims randomness as a computational resource to be exploited.  The coupling of these complementary approaches poses challenging mathematical problems to be investigated in the proposed work.\r\n\r\nThe proposed work will establish the foundations for fast prototyping in two directions: A Multi-Pronged Direction to bring RandNLA to the next level and explore what is technically feasible; and an overarching Synergy Direction that fuses the results for prototyping. The Multi-Pronged Direction includes the following topics: (i) Matrix perturbation theory, to bridge the gap between traditional worst-case bounds for asymptotically small perturbations on the one hand; and perturbations caused by stochastic noise, and missing or highly corrupted matrix entries on the other hand. (ii) Implicit versus explicit regularization, where randomness as a computational resource for speeding up algorithms additionally contributes to implicit statistical regularization, thereby improving statistical and numerical robustness. (iii) Krylov space methods for fast computation of good warm-starts and computation of surrogate models in the form of low-rank approximations, and specifically a better understanding of these methods in an algorithm-independent setting. (iv) Randomized basis construction methods that use matrix factorizations to compute low-rank approximations at low to moderate levels of accuracy. The Synergy Direction will explore topics like ultra-low accuracy matrix computations in machine learning applications, where merely a correct sign or exponent is sufficient. As a group, the PIs possess unrivaled and complementary expertise in applying fundamental mathematical tools to numerical applications in machine learning, data mining and scientific computing.  Importantly, the proposed methods will have significant impact in big data analysis, scientific computing, data mining and machine learning, where matrix computations are of paramount importance. The proposed work is fundamentally interdisciplinary and will enable fast, yet user-friendly extraction of insight from large-scale data these societally-important scientific domains.  Specifically, the proposed work will (i) create a numerically reliable and robust footing for fast prototyping; (ii) advance mathematics at the interface of computer science and statistics, one of the objectives being a synergy of numerical and statistical robustness; and (iii) advance the development of an interdisciplinary community with RandNLA as a pillar for the mathematics of data. The award will allow the investigators to increase their active engagement in reaching out to undergraduate and graduate students, and research communities in numerical linear algebra, theoretical computer science, machine learning, and scientific domains such as astronomy, materials science, and genetics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ilse",
   "pi_last_name": "Ipsen",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Ilse C Ipsen",
   "pi_email_addr": "ipsen@ncsu.edu",
   "nsf_id": "000220534",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "2701 Sullivan Dr, Admin Services",
  "perf_city_name": "Raleigh",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276958205",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  },
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1616",
   "pgm_ref_txt": "FOCUSED RESEARCH GROUPS IN MATH SCIENCES"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 366109.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project concerns the use of randomization and probability to analyze, model,&nbsp; and develop algorithms for matrix computations. The broader impacts of matrix computations are wide-ranging: Matrix computations are at the heart of many or even most large-scale computations in science, engineering, data science, machine learning, and AI.<br /><br />One can think of a randomized or probabilistic algorithm as a method that plays dice (randomly samples) which columns or rows of a matrix to use. Although this may sound dangerous, one can \"pre-treat\" the matrix to smooth out its mass evenly (premultiply by a Gaussian or random orthogonal matrix) so that, more or less, it does not matter which rows or columns one picks, since they all do a good job of capturing the important information inherent in the matrix (preserve its rank). Randomization can be especially safe and effective when used to build an accelerator (preconditioner) to speed up a time-consuming computation, or when probabilistic tools are used to achieve more realistic analyses of a computational process.<br /><br />The intellectual merits of the project include the following:</p>\n<ol>\n<li>The development of probabilistic numeric algorithms for the ubiquitous problem of solving systems of linear equations. Probabilistic numeric algorithms facilitate a deeper insight into the progress (convergence) of a computation than do traditional analyses, and they are also able to track the effect of errors in the input matrix, say, due to noisy measurement or discretization.</li>\n<li>&nbsp;The development of probabilistic roundoff error analyses for floating point computations that quantify the effects of finite precision much more realistically than traditional worst-case bounds.</li>\n<li>The development of an analysis for the ubquitous least squares/regression problem that quantifies the effect of a randomized solver on the uncertainty from the underlying Gaussian linear model. </li>\n<li>The analysis of how new developments in computer architecture (lower arithmetic precision and stochastic rounding) can benefit matrix computations: Their effect is to make a matrix better behaved&nbsp; (increase its small singular value), thereby enhancing the accuracy and reliability of many matrix computations.</li>\n<li>The development of a randomized orthogonalization method (transforming a linearly independent basis to an orthonormal basis) that is attractive for high performance computations due to its prevalence of matrix-matrix (BLAS level-3) operations.</li>\n</ol><br>\n<p>\n Last Modified: 08/03/2024<br>\nModified by: Ilse&nbsp;C&nbsp;Ipsen</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project concerns the use of randomization and probability to analyze, model, and develop algorithms for matrix computations. The broader impacts of matrix computations are wide-ranging: Matrix computations are at the heart of many or even most large-scale computations in science, engineering, data science, machine learning, and AI.\n\nOne can think of a randomized or probabilistic algorithm as a method that plays dice (randomly samples) which columns or rows of a matrix to use. Although this may sound dangerous, one can \"pre-treat\" the matrix to smooth out its mass evenly (premultiply by a Gaussian or random orthogonal matrix) so that, more or less, it does not matter which rows or columns one picks, since they all do a good job of capturing the important information inherent in the matrix (preserve its rank). Randomization can be especially safe and effective when used to build an accelerator (preconditioner) to speed up a time-consuming computation, or when probabilistic tools are used to achieve more realistic analyses of a computational process.\n\nThe intellectual merits of the project include the following:\n\nThe development of probabilistic numeric algorithms for the ubiquitous problem of solving systems of linear equations. Probabilistic numeric algorithms facilitate a deeper insight into the progress (convergence) of a computation than do traditional analyses, and they are also able to track the effect of errors in the input matrix, say, due to noisy measurement or discretization.\nThe development of probabilistic roundoff error analyses for floating point computations that quantify the effects of finite precision much more realistically than traditional worst-case bounds.\nThe development of an analysis for the ubquitous least squares/regression problem that quantifies the effect of a randomized solver on the uncertainty from the underlying Gaussian linear model. \nThe analysis of how new developments in computer architecture (lower arithmetic precision and stochastic rounding) can benefit matrix computations: Their effect is to make a matrix better behaved (increase its small singular value), thereby enhancing the accuracy and reliability of many matrix computations.\nThe development of a randomized orthogonalization method (transforming a linearly independent basis to an orthonormal basis) that is attractive for high performance computations due to its prevalence of matrix-matrix (BLAS level-3) operations.\n\t\t\t\t\tLast Modified: 08/03/2024\n\n\t\t\t\t\tSubmitted by: IlseCIpsen\n"
 }
}