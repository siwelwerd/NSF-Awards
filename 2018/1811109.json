{
 "awd_id": "1811109",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small:Extremely Energy-Efficient Monolithic 3D System Architectures",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2018-05-18",
 "awd_max_amd_letter_date": "2018-05-18",
 "awd_abstract_narration": "With the slowing down of Moore's Law, traditional 2D scaling is not expected to deliver the area, power, and performance benefits the semiconductor industry has been counting on.  Thus, the time has come to go vertical, accommodating processor cores, accelerators, cache, and main memory in the same package. There are two ways of doing this: use through-silicon vias (TSVs) or monolithic 3D integration. TSVs do not have memory-on-logic stacking success stories.  Monolithic 3D integration uses monolithic inter-tier vias that have a much smaller diameter than TSVs, and enable many different design styles: transistor-level monolithic, gate-level monolithic, and block-level monolithic.  While fabrication and test techniques for monolithic 3D integration are maturing, monolithic 3D system architectures have not been investigated in depth.  The work on this project fills this gap. The methodologies and tools developed under this grant will be made available on the web. They will also be made available to the industry.  The material will be included in course materials, and under-represented graduate students will be attracted to this research through Princeton Fellowships.  Results will be disseminated through research articles and seminars.\r\n\r\nThere are various \"walls\" confronting computer system architects these days.  The Power Wall constrains the portion of the chip that can be powered on.  This is also known as the dark silicon problem.  The Memory Wall prevents efficient access to off-chip memory. Monolithic 3D integration has the potential to significantly alleviate the problems associated with these walls, especially for abundant-data problems, such as machine learning and inference, which are becoming commonplace. This project seeks to improve the energy efficiency of monolithic 3D system architectures by a factor of 500 relative to traditional system architectures.  It will do so by exploiting synergies across the device, logic, memory, accelerator, micro-architecture, chip multiprocessor, and monolithic 3D IC levels of the design hierarchy, providing a common computation platform from high-performance mobile devices to data centers.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Niraj",
   "pi_last_name": "Jha",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Niraj K Jha",
   "pi_email_addr": "jha@princeton.edu",
   "nsf_id": "000123477",
   "pi_start_date": "2018-05-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "P.O. Box 36",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With Moore&rsquo;s Law nearing its end, the semiconductor industry is likely to shift its attention to 3D integration.&nbsp; Monolithic 3D integration is an important contender in this arena. Its advantage lies in its ability to provide serial implementation of logic and memory layers.&nbsp; This leads to significant benefits in energy efficiency, area footprint, latency, etc.&nbsp; To take full advantage of this technology, novel architectures, design methodologies, and tools are needed.&nbsp; This was the focus of this project.</p>\n<p>Monolithic 3-D integration offers a very high connectivity between vertical transistor layers owing to its nanoscale monolithic inter-tier vias. Monolithic integration can be realized at block-, gate-, and transistor-level granularity. A hybrid monolithic (HM) design aims to further optimize area, power, and performance of the chip by combining different monolithic styles. We introduced McPAT-monolithic, a framework for modeling HM multi-core architectures. We made it available for download by other researchers to spur further research in this promising area.</p>\n<p>Deep neural networks (DNNs) and convolutional neural networks (CNNs) outperform conventional machine learning (ML) algorithms across a wide range of applications, e.g., image recognition, object detection, robotics, and natural language processing. However, the high computational complexity of DNNs often necessitates extremely fast and efficient hardware. The problem gets worse as the size of neural networks grows exponentially. As a result, customized hardware accelerators are needed to accelerate DNN processing without sacrificing model accuracy. We developed an application-driven framework for architectural design space exploration of DNN/CNN accelerators. This framework is based on a hardware analytical model of individual DNN/CNN operations. It models the accelerator design task as a multi-dimensional optimization problem. Furthermore, we explored the use of the framework for accelerator configuration optimization under simultaneous diverse DNN applications. The framework is also capable of improving neural network models to best fit the underlying hardware resources.</p>\n<p>Most CNN accelerators focus on exploring various dataflow styles and designs that exploit computational parallelism. However, potential performance improvement from sparsity (in activations and weights) was not previously adequately addressed. The computation and memory footprint of CNNs can be significantly reduced if sparsity is exploited in network evaluations. Therefore, different pruning methods have been proposed to increase sparsity. To take advantage of sparsity, some accelerator designs explore sparsity encoding and evaluation on CNN accelerators. However, sparsity encoding is just performed on activation data or CNN weights and only used in inference. It has been shown that activations and weights also have high sparsity levels during the CNN training phase. Hence, sparsity-aware computation should also be considered in the training phase. To further improve performance and energy efficiency, some accelerators evaluate CNNs with limited precision. However, this is limited to the inference phase since reduced precision sacrifices network accuracy if used in training. In addition, CNN evaluation is usually memory-intensive, especially during training. A 3D memory interface has been used on high-end GPUs to alleviate memory bandwidth shortage. We proposed SPRING, a SParsity-aware Reduced-precision Monolithic 3D CNN accelerator for trainING and inference. SPRING supports both CNN training and inference. It uses a binary mask scheme to encode sparsities in activations and weights. It uses the stochastic rounding algorithm to train CNNs with reduced precision without accuracy loss. To alleviate the memory bottleneck in CNN evaluation, especially during training, SPRING uses an efficient monolithic 3D nonvolatile memory interface to increase memory bandwidth. Compared to state-of-the-art GPUs, SPRING achieves an order of magnitude improvement in performance and two orders of magnitude improvement in energy efficiency, for both training and inference.</p>\n<p>Recently, automated co-design of ML models and accelerator architectures has attracted significant attention from both the industry and academia. However, most co-design frameworks either explore a limited search space or employ suboptimal exploration techniques for simultaneous design decision investigations of the ML model and the accelerator. Furthermore, training the ML model and simulating the accelerator performance is computationally expensive. To address these limitations, we proposed a novel neural architecture and hardware accelerator co-design framework, called CODEBench. It is composed of two new benchmarking sub-frameworks, CNNBench and AccelBench, which explore expanded design spaces of CNNs and CNN accelerators. CNNBench leverages an advanced search technique, BOSHNAS, to efficiently train a neural heteroscedastic surrogate model to converge to an optimal CNN architecture by employing second-order gradients. AccelBench performs cycle-accurate simulations for a diverse set of accelerator architectures in a vast design space. CODEBench provides higher accuracy and throughput while reducing energy consumption by two orders of magnitude and chip area by 2X over prior state-of-the-art.</p>\n<p>Five PhD students were trained in this exciting area of research.&nbsp; Their work resulted in several journal articles.&nbsp; The results were disseminated to the wider public through various invited talks. The students also did technology transfer through their summer internships at various companies.&nbsp; The frameworks were used in various course projects.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/03/2022<br>\n\t\t\t\t\tModified by: Niraj&nbsp;K&nbsp;Jha</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWith Moore\u2019s Law nearing its end, the semiconductor industry is likely to shift its attention to 3D integration.  Monolithic 3D integration is an important contender in this arena. Its advantage lies in its ability to provide serial implementation of logic and memory layers.  This leads to significant benefits in energy efficiency, area footprint, latency, etc.  To take full advantage of this technology, novel architectures, design methodologies, and tools are needed.  This was the focus of this project.\n\nMonolithic 3-D integration offers a very high connectivity between vertical transistor layers owing to its nanoscale monolithic inter-tier vias. Monolithic integration can be realized at block-, gate-, and transistor-level granularity. A hybrid monolithic (HM) design aims to further optimize area, power, and performance of the chip by combining different monolithic styles. We introduced McPAT-monolithic, a framework for modeling HM multi-core architectures. We made it available for download by other researchers to spur further research in this promising area.\n\nDeep neural networks (DNNs) and convolutional neural networks (CNNs) outperform conventional machine learning (ML) algorithms across a wide range of applications, e.g., image recognition, object detection, robotics, and natural language processing. However, the high computational complexity of DNNs often necessitates extremely fast and efficient hardware. The problem gets worse as the size of neural networks grows exponentially. As a result, customized hardware accelerators are needed to accelerate DNN processing without sacrificing model accuracy. We developed an application-driven framework for architectural design space exploration of DNN/CNN accelerators. This framework is based on a hardware analytical model of individual DNN/CNN operations. It models the accelerator design task as a multi-dimensional optimization problem. Furthermore, we explored the use of the framework for accelerator configuration optimization under simultaneous diverse DNN applications. The framework is also capable of improving neural network models to best fit the underlying hardware resources.\n\nMost CNN accelerators focus on exploring various dataflow styles and designs that exploit computational parallelism. However, potential performance improvement from sparsity (in activations and weights) was not previously adequately addressed. The computation and memory footprint of CNNs can be significantly reduced if sparsity is exploited in network evaluations. Therefore, different pruning methods have been proposed to increase sparsity. To take advantage of sparsity, some accelerator designs explore sparsity encoding and evaluation on CNN accelerators. However, sparsity encoding is just performed on activation data or CNN weights and only used in inference. It has been shown that activations and weights also have high sparsity levels during the CNN training phase. Hence, sparsity-aware computation should also be considered in the training phase. To further improve performance and energy efficiency, some accelerators evaluate CNNs with limited precision. However, this is limited to the inference phase since reduced precision sacrifices network accuracy if used in training. In addition, CNN evaluation is usually memory-intensive, especially during training. A 3D memory interface has been used on high-end GPUs to alleviate memory bandwidth shortage. We proposed SPRING, a SParsity-aware Reduced-precision Monolithic 3D CNN accelerator for trainING and inference. SPRING supports both CNN training and inference. It uses a binary mask scheme to encode sparsities in activations and weights. It uses the stochastic rounding algorithm to train CNNs with reduced precision without accuracy loss. To alleviate the memory bottleneck in CNN evaluation, especially during training, SPRING uses an efficient monolithic 3D nonvolatile memory interface to increase memory bandwidth. Compared to state-of-the-art GPUs, SPRING achieves an order of magnitude improvement in performance and two orders of magnitude improvement in energy efficiency, for both training and inference.\n\nRecently, automated co-design of ML models and accelerator architectures has attracted significant attention from both the industry and academia. However, most co-design frameworks either explore a limited search space or employ suboptimal exploration techniques for simultaneous design decision investigations of the ML model and the accelerator. Furthermore, training the ML model and simulating the accelerator performance is computationally expensive. To address these limitations, we proposed a novel neural architecture and hardware accelerator co-design framework, called CODEBench. It is composed of two new benchmarking sub-frameworks, CNNBench and AccelBench, which explore expanded design spaces of CNNs and CNN accelerators. CNNBench leverages an advanced search technique, BOSHNAS, to efficiently train a neural heteroscedastic surrogate model to converge to an optimal CNN architecture by employing second-order gradients. AccelBench performs cycle-accurate simulations for a diverse set of accelerator architectures in a vast design space. CODEBench provides higher accuracy and throughput while reducing energy consumption by two orders of magnitude and chip area by 2X over prior state-of-the-art.\n\nFive PhD students were trained in this exciting area of research.  Their work resulted in several journal articles.  The results were disseminated to the wider public through various invited talks. The students also did technology transfer through their summer internships at various companies.  The frameworks were used in various course projects.\n\n\t\t\t\t\tLast Modified: 07/03/2022\n\n\t\t\t\t\tSubmitted by: Niraj K Jha"
 }
}