{
 "awd_id": "1763314",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: MEDIUM: Collaborative Research: Foundations of Adaptive Data Analysis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2018-03-01",
 "awd_exp_date": "2022-02-28",
 "tot_intn_awd_amt": 378000.0,
 "awd_amount": 378000.0,
 "awd_min_amd_letter_date": "2018-02-14",
 "awd_max_amd_letter_date": "2020-04-25",
 "awd_abstract_narration": "Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. \r\n\r\nThe technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of \"over-fitting\" and \"false discovery.\" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "AARON",
   "pi_last_name": "ROTH",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "AARON ROTH",
   "pi_email_addr": "aaroth@cis.upenn.edu",
   "nsf_id": "000624673",
   "pi_start_date": "2018-02-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Weijie",
   "pi_last_name": "Su",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Weijie Su",
   "pi_email_addr": "suw@wharton.upenn.edu",
   "nsf_id": "000731531",
   "pi_start_date": "2018-02-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": "3451 Walnut",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046205",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 123490.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 125809.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 128701.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In order for data analysis to be trustworthy, it must correctly reflect the statistical properties of the source from which the data was drawn rather than over-fit to the noisy peculiarities of the data points that happened to be sampled. We lack much theory for how to do this when data analyses make repeated re-use of the same data, even though this is the common case in practice. Our project aimed to fill this gap.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Over the course of this award, we discovered new fundamental theory tying a privacy technology (\"differential privacy\") to guarantees of statistical validity when re-using data. We developed practical applications of these ideas, as well as applications of these ideas to new domains (like data deletion --- the ability to remove any trace of an individual person's data from a statistical model if they request that their data be deleted). These works were primarily published in computer science and machine learning venues. We also developed a variant of differential privacy based on a hypothesis testing characterization that allows for more tightly keeping track of its privacy guarantees over the lifetime of an analysis. We published this work in a top statistics venue, which also helped translate ideas developed in computer science to a new community.&nbsp;</p>\n<p>&nbsp;</p>\n<p>In addition to supporting basic research and the development of scientific personel through the PhD students we advised, we developed the first PhD level course on \"adaptive data analysis\" together with a set of lecture notes that we made publicly available. This will help disseminate the ideas developed as part of this project to a broad scientific audience.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/16/2022<br>\n\t\t\t\t\tModified by: Aaron&nbsp;Roth</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn order for data analysis to be trustworthy, it must correctly reflect the statistical properties of the source from which the data was drawn rather than over-fit to the noisy peculiarities of the data points that happened to be sampled. We lack much theory for how to do this when data analyses make repeated re-use of the same data, even though this is the common case in practice. Our project aimed to fill this gap. \n\n \n\nOver the course of this award, we discovered new fundamental theory tying a privacy technology (\"differential privacy\") to guarantees of statistical validity when re-using data. We developed practical applications of these ideas, as well as applications of these ideas to new domains (like data deletion --- the ability to remove any trace of an individual person's data from a statistical model if they request that their data be deleted). These works were primarily published in computer science and machine learning venues. We also developed a variant of differential privacy based on a hypothesis testing characterization that allows for more tightly keeping track of its privacy guarantees over the lifetime of an analysis. We published this work in a top statistics venue, which also helped translate ideas developed in computer science to a new community. \n\n \n\nIn addition to supporting basic research and the development of scientific personel through the PhD students we advised, we developed the first PhD level course on \"adaptive data analysis\" together with a set of lecture notes that we made publicly available. This will help disseminate the ideas developed as part of this project to a broad scientific audience. \n\n\t\t\t\t\tLast Modified: 06/16/2022\n\n\t\t\t\t\tSubmitted by: Aaron Roth"
 }
}