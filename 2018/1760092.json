{
 "awd_id": "1760092",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HARNESSING MACHINE LEARNING ALGORITHMS TO STUDY SCIENTIFIC GRANT PEER REVIEW",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": "7032925150",
 "po_email": "tswoodso@nsf.gov",
 "po_sign_block_name": "Thomas S. Woodson",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 499258.0,
 "awd_amount": 499258.0,
 "awd_min_amd_letter_date": "2018-04-02",
 "awd_max_amd_letter_date": "2024-02-28",
 "awd_abstract_narration": "Armed with a $30 billion annual budget, the U.S. National Institutes of Health (NIH) leads the world in funding research to advance human health and treatments for disease. Like most funding agencies, NIH uses peer review to evaluate the merit of grant applications. Approximately three reviewers from a given review group (called a \"study section\") assign preliminary impact scores and write critiques to evaluate each application before attending study section meetings where all members contribute to a final priority score. Although NIH's review process is considered one of the best in the world, reports and self-studies show that racial/ethnic minorities and women have lower award rates for first time, and renewal applications, respectively, for NIH's largest funding mechanism, the R01 grant. This is problematic because R01s are critical for career advancement, and research conducted by racial/ethnic minorities and women is linked to technological innovation and is known to address costly education, economic, and health disparities. As a leader in efforts to diversify the science and medical workforce, NIH has called for studies to test for the possibility that bias may operate in its peer review process. This call brings to light the broad need for research on the effectiveness of peer review, which is used across all science and technology fields, and for more scientists to engage in such research. If factors unrelated to the quality of the proposed science negatively impact the outcome of a grant review, it runs counter to funding agencies' goals to select the best science, blocks expensive downstream federal efforts to broaden participation in science, and undermines the competitiveness of the U.S. scientific enterprise.\r\n\r\nOur group was the first to show that, when combined with traditional analyses of scores and award rates, linguistic analysis of NIH peer reviewers' narrative critiques of R01 applications can show evidence of potential stereotype-based bias in reviewers' decision making. Although such bias is generally unintentional and impacts reviewers' judgment regardless of their own sex or race, it can lead reviewers to differentially enforce evaluation criteria. Controlled experiments show, for instance, that cultural stereotypes that racial/ethnic minorities and women lack intrinsic ability for fields like science, can lead reviewers to unconsciously require more proof to confirm their competence. Over the past decade machine learning technologies have made data-, text-, and video-mining into state-of-the-art analytic techniques, which, if applied to scientific peer review, could revolutionize the field. Long Short Term Memory (LSTMs) neural networks -- algorithms that function like the human brain to identify complex patterns in data -- in particular, have catapulted the application of computer science to the study of social and psychological phenomena. Using a large, demographically diverse set of NIH R01 application critiques, scores, and video of constructed study section discussions, this project is producing analytical tools that use LSTMs to capture evidence of stereotype-based bias in both written and oral discussion of grant applications. Resulting technologies are open-access, and available for applied use across scientific funding agencies.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "You-Geon",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "You-Geon Lee",
   "pi_email_addr": "yglee@wisc.edu",
   "nsf_id": "000691064",
   "pi_start_date": "2019-04-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Anna",
   "pi_last_name": "Kaatz",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Anna C Kaatz",
   "pi_email_addr": "akaatz@wisc.edu",
   "nsf_id": "000752168",
   "pi_start_date": "2018-04-02",
   "pi_end_date": "2019-04-04"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "You-Geon",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "You-Geon Lee",
   "pi_email_addr": "yglee@wisc.edu",
   "nsf_id": "000691064",
   "pi_start_date": "2018-04-02",
   "pi_end_date": "2019-04-04"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anna",
   "pi_last_name": "Kaatz",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Anna C Kaatz",
   "pi_email_addr": "akaatz@wisc.edu",
   "nsf_id": "000752168",
   "pi_start_date": "2019-04-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "21 N Park",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537151218",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "762600",
   "pgm_ele_name": "SciSIP-Sci of Sci Innov Policy"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7626",
   "pgm_ref_txt": "SCIENCE OF SCIENCE POLICY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499258.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aimed to develop analytic techniques to test peer review systems for reliability, validity, and fairness without jeopardizing applicant and reviewer privacy. This study hypothesized that bias might operate in NIH peer review processes, precluding the career persistence of scientists from historically underrepresented groups. This bias might ultimately thwart expensive, downstream federal efforts to broaden participation in science, catalyze innovation, and boost the nation's health and economy. This project leveraged state-of-the-art machine learning algorithms to detect potential bias in NIH's peer review process. This project used written critiques of R01 applications and audio/video recordings of R01 study sections related to score changes in proposal reviews. These materials were used to train machine learning algorithms to identify bias. Extensive analyses conducted over several years have yielded major findings and outcomes, which are described below.</p>\r\n<p>&nbsp;</p>\r\n<p>1.&nbsp;&nbsp;&nbsp;&nbsp; Identification of potential bias due to scientists&rsquo; gender, race, and funding outcome of application</p>\r\n<p>&nbsp;</p>\r\n<p>Machine learning models developed in this project revealed variations in reviewer sentiment based on applicant gender, race, and funding outcome, suggesting the presence of implicit bias in the review process. Notably, the models were particularly sensitive to &ldquo;standout adjective words,&rdquo; achieving a higher accuracy in predicting applicant sex when sentences with these words were present in reviewer critiques, which is supported by the prior study. These findings imply that NIH grant reviewers may draw on different implicit competence standards to evaluate women and/or minority scientists.</p>\r\n<p>&nbsp;</p>\r\n<p>2.&nbsp;&nbsp;&nbsp;&nbsp; Development and refinement of machine learning models</p>\r\n<p>&nbsp;</p>\r\n<p>The project trained and tested various machine learning models, including Long Short-Term Memory neural networks (LSTMs), Bidirectional, Encoder, Representations from Transformers (BERT), Logistic Regression Models, and Random Forest Models, to analyze text data from grant summary statements and transcripts from the mock study section meetings. The limited sample size at the critique level (~ 14,000) hindered the performance of complex models like LSTM and BERT, preventing them from surpassing simpler models such as random forest. However, with a larger dataset at the sentence level (~276,000 sentences), LSTMs demonstrated the potential for higher accuracy in predicting applicant sex, race, and funding outcome. In other words, LSTM models performed well at the sentence level but were less effective with smaller datasets at the critique level. Random forest models showed superior performance with smaller datasets at the critique level, exceeding LSTM performance in some cases.</p>\r\n<p>&nbsp;</p>\r\n<p>3.&nbsp;&nbsp;&nbsp;&nbsp; Important of specific criteria and language</p>\r\n<p>&nbsp;</p>\r\n<p>This project revealed the importance of specific criteria and language in detecting potential bias in grant reviews. The relatively strong association between the \"Overall Impact\" criterion, strengths section, \"standout\" adjectives, and applicant sex suggests that these areas may be particularly vulnerable to implicit bias. This highlights the need for interventions and training to raise reviewers' awareness of these potential biases and promote more equitable evaluation practices.</p>\r\n<p>4.&nbsp;&nbsp;&nbsp;&nbsp; Value of Linguistic Inquiry Word Count (LIWC) categories analysis</p>\r\n<p>&nbsp;</p>\r\n<p class=\"Pa4\">This project revealed that existing pre-trained word vectors, such as Google's word2vec and GloVe, did not significantly improve the performance of LSTM models for analyzing NIH grant reviews. This suggests that these general-purpose word vectors may not capture the specific nuances of language used in this domain. In contrast, focusing on word frequencies within LIWC categories proved highly effective. Even though random forest models disregarded word order, using LIWC category counts as features provided valuable analytical insights and improved accuracy.</p>\r\n<p class=\"Default\">&nbsp;</p>\r\n<p class=\"Default\">5.&nbsp;&nbsp;&nbsp;&nbsp; The limitation of analyzing transcript data from study section meetings</p>\r\n<p class=\"Default\">&nbsp;</p>\r\n<p class=\"Default\">Analysis of transcript data from mock study section meetings (25 grant applications) showed promise, with pre-trained LSTM models achieving reasonable accuracy in classifying applicant sex. However, the limited sample size hindered further model refinement and training.</p>\r\n<p class=\"Default\">&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/12/2025<br>\nModified by: You-Geon&nbsp;Lee</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project aimed to develop analytic techniques to test peer review systems for reliability, validity, and fairness without jeopardizing applicant and reviewer privacy. This study hypothesized that bias might operate in NIH peer review processes, precluding the career persistence of scientists from historically underrepresented groups. This bias might ultimately thwart expensive, downstream federal efforts to broaden participation in science, catalyze innovation, and boost the nation's health and economy. This project leveraged state-of-the-art machine learning algorithms to detect potential bias in NIH's peer review process. This project used written critiques of R01 applications and audio/video recordings of R01 study sections related to score changes in proposal reviews. These materials were used to train machine learning algorithms to identify bias. Extensive analyses conducted over several years have yielded major findings and outcomes, which are described below.\r\n\n\n\r\n\n\n1. Identification of potential bias due to scientists gender, race, and funding outcome of application\r\n\n\n\r\n\n\nMachine learning models developed in this project revealed variations in reviewer sentiment based on applicant gender, race, and funding outcome, suggesting the presence of implicit bias in the review process. Notably, the models were particularly sensitive to standout adjective words, achieving a higher accuracy in predicting applicant sex when sentences with these words were present in reviewer critiques, which is supported by the prior study. These findings imply that NIH grant reviewers may draw on different implicit competence standards to evaluate women and/or minority scientists.\r\n\n\n\r\n\n\n2. Development and refinement of machine learning models\r\n\n\n\r\n\n\nThe project trained and tested various machine learning models, including Long Short-Term Memory neural networks (LSTMs), Bidirectional, Encoder, Representations from Transformers (BERT), Logistic Regression Models, and Random Forest Models, to analyze text data from grant summary statements and transcripts from the mock study section meetings. The limited sample size at the critique level (~ 14,000) hindered the performance of complex models like LSTM and BERT, preventing them from surpassing simpler models such as random forest. However, with a larger dataset at the sentence level (~276,000 sentences), LSTMs demonstrated the potential for higher accuracy in predicting applicant sex, race, and funding outcome. In other words, LSTM models performed well at the sentence level but were less effective with smaller datasets at the critique level. Random forest models showed superior performance with smaller datasets at the critique level, exceeding LSTM performance in some cases.\r\n\n\n\r\n\n\n3. Important of specific criteria and language\r\n\n\n\r\n\n\nThis project revealed the importance of specific criteria and language in detecting potential bias in grant reviews. The relatively strong association between the \"Overall Impact\" criterion, strengths section, \"standout\" adjectives, and applicant sex suggests that these areas may be particularly vulnerable to implicit bias. This highlights the need for interventions and training to raise reviewers' awareness of these potential biases and promote more equitable evaluation practices.\r\n\n\n4. Value of Linguistic Inquiry Word Count (LIWC) categories analysis\r\n\n\n\r\n\n\nThis project revealed that existing pre-trained word vectors, such as Google's word2vec and GloVe, did not significantly improve the performance of LSTM models for analyzing NIH grant reviews. This suggests that these general-purpose word vectors may not capture the specific nuances of language used in this domain. In contrast, focusing on word frequencies within LIWC categories proved highly effective. Even though random forest models disregarded word order, using LIWC category counts as features provided valuable analytical insights and improved accuracy.\r\n\n\n\r\n\n\n5. The limitation of analyzing transcript data from study section meetings\r\n\n\n\r\n\n\nAnalysis of transcript data from mock study section meetings (25 grant applications) showed promise, with pre-trained LSTM models achieving reasonable accuracy in classifying applicant sex. However, the limited sample size hindered further model refinement and training.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 02/12/2025\n\n\t\t\t\t\tSubmitted by: You-GeonLee\n"
 }
}