{
 "awd_id": "1810758",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NSF-BSF: AF: Small: An Algorithmic Theory of Brain Networks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2018-06-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2018-05-18",
 "awd_max_amd_letter_date": "2018-05-18",
 "awd_abstract_narration": "Understanding how the brain works, as a computational device, is a central challenge of modern neuroscience and AI.  Different research communities approach this challenge in different ways, including examining neural network structure as a clue to computational function, using functional imaging to study neural activation patterns, developing theory based on simplified models of neural computation, and engineering of neural-inspired machine learning  architectures. This project will approach the problem using techniques from distributed computing theory and other branches of theoretical computer science. This project has the potential to improve our understanding of computation in the brain, by identifying key problems that are solved in the brain and key mechanisms that may be used to solve them.  This work can also have impact on theoretical computer science, by contributing a new and fruitful direction for theoretical study. This collaboration between MIT and the Weizmann Institute in Israel will increase the participation of women and minority participants in this field and will seek to bridge the gap between computer scientists and biology researchers.\r\n\r\nSpecifically, the project develops an algorithmic theory for brain networks, based on novel stochastic Spiking Neural Network models with general interconnection patterns. It defines a collection of abstract problems to be solved by these networks, inspired by problems that are solved in actual brains, such as problems of focus, recognition, learning, and memory.  The project designs algorithms (networks) that solve the problems, and analyze them in terms of static costs such as network size, and dynamic costs such as time to converge to a correct solution.  The investigators consider tradeoffs between the various costs, and will prove corresponding lower bound results. The models, problems, and solutions should be simple enough to enable theoretical analysis, yet realistic enough to provide insight into the behavior of real neural networks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nancy",
   "pi_last_name": "Lynch",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Nancy A Lynch",
   "pi_email_addr": "lynch@csail.mit.edu",
   "nsf_id": "000169435",
   "pi_start_date": "2018-05-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7934",
   "pgm_ref_txt": "PARAL/DISTRIBUTED ALGORITHMS"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Understanding how the brain works, as a computational device, is a  central challenge of modern neuroscience and AI. This NSF project, joint  between Profs. Nancy Lynch at MIT and Merav Parter at the Weizmann  Institute, approached the problem using techniques from distributed  computing theory and other branches of theoretical computer science.<br /><br />Specifically,  the project contributed to the deveopment of an \"algorithmic theory for  brain networks\", based on stochastic Spiking Neural Network (SNN)  models with general interconnection patterns.&nbsp; It defined abstract  problems to be solved by SNNs, inspired by problems that are solved in  actual brains, such as problems of focus, recognition, learning, and  memory.&nbsp; The project designed new algorithms (SNNs) to solve the  problems, and analyzed them in terms of static costs such as network  size and dynamic costs such as time to converge to a solution.&nbsp; It  considered tradeoffs between the various costs, and proved corresponding  lower bound results.&nbsp; The project also considered issues such as  robustness to noise and uncertainty, the power of randomness in neural  computation, and composing networks that solve simple problems into  networks that solve more complex problems.<br /><br />The investigators  began by studying many particular problems such as Winner-Take-All [1]  and the more general k-Winner-Take-All [2]. These key problems capture,  abstractly, the notions of focus and attention in the brain.&nbsp; Other  problems studied included counting, data representation using random  sketching and clustering, and short-term memorization [3].&nbsp; The  investigators also obtained new algorithms and analysis results for  representation and learning of hierarchically-structured concepts [4].<br /><br />The  study of many particular problems led to a study of more general  issues.&nbsp; The investigators developed a detailed, formal mathematical  framework, in the style of concurrency theory, for synchronous,  stochastic SNNs [5].&nbsp; Notably, this framework supported composition of  networks.&nbsp; In addition to being used for modeling particular SNN  algorithms, the model served as the basis for a simulator, developed by  Prof. Karl Berggren's research group, for new energy-efficient computing  devices based on nanowires.<br /><br />Other general results included new  methods for analyzing neural network learning rules [6], and theoretical  results comparing the computational power of the SNN model with related models such as the streaming model commonly studied in theoretical computer science.<br /><br />A  preliminary paper describes, in an algorithmic style, a likely  separation in the brain between mechanisms used for intuitive and  symbolic reasoning [7].<br /><br />Later work on the project focused on  studies directly motivated by experimental neuroscience.&nbsp; This work  included studies of mechanisms for pattern recognition in the retina  [8], of thalamic regulation of frontal interactions in human cognitive  flexibility [9], and of thalamocortical contribution to flexible  learning in neural systems [10]. This work involved devising plausible  models, simulating their behavior and comparing with experimental  results, and sometimes analyzing the models' behavior theoretically.&nbsp;  This type of work involved collaboration with MIT neuroscientists,  including Profs. Ann Graybiel and Michael Halassa.<br /><br />Overall, this  work has contributed to theoretical computer science by presenting a  new, fruitful direction for theoretical study.&nbsp; The investigators hope  that it has also helped to improve understanding of computation in the  brain, by identifying key problems that are solved in the brain and  proposing mechanisms that may be used to solve them.<br /><br />References:<br /><br />[1]&nbsp; Lynch, Musco, Parter.&nbsp; Winner-Take-All computation in Spiking Neural Networks.&nbsp; arXiv:1904,12591, April 2019.<br /><br />[2]&nbsp;  Su, Chang, Lynch.&nbsp; Spike-based Winner-Take-All Computation: fundamental  limits and order-optimal circuits.&nbsp; Neural Computation<br />31(12), December 2019.<br /><br />[3]&nbsp; Hitron, Lynch, Musco, Parter. Random sketching, clustering, and<br />short-term memory in Spiking Neural Networks. ITCS 2020, January 2020.<br /><br />[4]&nbsp; Lynch, Mallmann-Trenn. Learning hierarchically structured concepts. Neural Networks, 143:798-817, November 2021.<br /><br />[5]&nbsp;  Lynch, Musco. A basic compositional model for Spiking Neural Networks.  In Jansen, N., Stoelinga, M., van den Bos, P., editors, A Journey from  Process Algebra via Timed Automata to Model Learning. LNCS volume 13560,  2022.<br /><br />[6]&nbsp; Chou, Sandhu, Wang, Yu. A general framework for  analyzing stochastic dynamics in learning algorithms.  ArXiv:2006.06171v3, September 2022.<br /><br />[7]&nbsp; Lynch. Symbolic Knowledge Structures and Intuitive Knowledge Structures. arXiv:2206.02932, June 2022.<br /><br />[8]&nbsp;  Murray, Wang, Lynch. Emergence of direction selective retinal cell  types in task-optimized deep learning models. Journal of Computational  Biology 29(4):370-381, April 2022.<br /><br />[9]&nbsp; Wang, Halassa.&nbsp;  Thalamocortical contribution to flexible learning in neural systems.&nbsp;  Network Neuroscience 6(4), October, 2022.<br /><br />[10]&nbsp; Hummos, Wang,  Drammis, Halassa, Pleger. Evidence for thalamic regulation of frontal  interactions in human cognitive flexibility. PLoS Computational Biology,  18(9), September 2022.<br /><br /><br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/03/2023<br>\n\t\t\t\t\tModified by: Nancy&nbsp;A&nbsp;Lynch</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nUnderstanding how the brain works, as a computational device, is a  central challenge of modern neuroscience and AI. This NSF project, joint  between Profs. Nancy Lynch at MIT and Merav Parter at the Weizmann  Institute, approached the problem using techniques from distributed  computing theory and other branches of theoretical computer science.\n\nSpecifically,  the project contributed to the deveopment of an \"algorithmic theory for  brain networks\", based on stochastic Spiking Neural Network (SNN)  models with general interconnection patterns.  It defined abstract  problems to be solved by SNNs, inspired by problems that are solved in  actual brains, such as problems of focus, recognition, learning, and  memory.  The project designed new algorithms (SNNs) to solve the  problems, and analyzed them in terms of static costs such as network  size and dynamic costs such as time to converge to a solution.  It  considered tradeoffs between the various costs, and proved corresponding  lower bound results.  The project also considered issues such as  robustness to noise and uncertainty, the power of randomness in neural  computation, and composing networks that solve simple problems into  networks that solve more complex problems.\n\nThe investigators  began by studying many particular problems such as Winner-Take-All [1]  and the more general k-Winner-Take-All [2]. These key problems capture,  abstractly, the notions of focus and attention in the brain.  Other  problems studied included counting, data representation using random  sketching and clustering, and short-term memorization [3].  The  investigators also obtained new algorithms and analysis results for  representation and learning of hierarchically-structured concepts [4].\n\nThe  study of many particular problems led to a study of more general  issues.  The investigators developed a detailed, formal mathematical  framework, in the style of concurrency theory, for synchronous,  stochastic SNNs [5].  Notably, this framework supported composition of  networks.  In addition to being used for modeling particular SNN  algorithms, the model served as the basis for a simulator, developed by  Prof. Karl Berggren's research group, for new energy-efficient computing  devices based on nanowires.\n\nOther general results included new  methods for analyzing neural network learning rules [6], and theoretical  results comparing the computational power of the SNN model with related models such as the streaming model commonly studied in theoretical computer science.\n\nA  preliminary paper describes, in an algorithmic style, a likely  separation in the brain between mechanisms used for intuitive and  symbolic reasoning [7].\n\nLater work on the project focused on  studies directly motivated by experimental neuroscience.  This work  included studies of mechanisms for pattern recognition in the retina  [8], of thalamic regulation of frontal interactions in human cognitive  flexibility [9], and of thalamocortical contribution to flexible  learning in neural systems [10]. This work involved devising plausible  models, simulating their behavior and comparing with experimental  results, and sometimes analyzing the models' behavior theoretically.   This type of work involved collaboration with MIT neuroscientists,  including Profs. Ann Graybiel and Michael Halassa.\n\nOverall, this  work has contributed to theoretical computer science by presenting a  new, fruitful direction for theoretical study.  The investigators hope  that it has also helped to improve understanding of computation in the  brain, by identifying key problems that are solved in the brain and  proposing mechanisms that may be used to solve them.\n\nReferences:\n\n[1]  Lynch, Musco, Parter.  Winner-Take-All computation in Spiking Neural Networks.  arXiv:1904,12591, April 2019.\n\n[2]   Su, Chang, Lynch.  Spike-based Winner-Take-All Computation: fundamental  limits and order-optimal circuits.  Neural Computation\n31(12), December 2019.\n\n[3]  Hitron, Lynch, Musco, Parter. Random sketching, clustering, and\nshort-term memory in Spiking Neural Networks. ITCS 2020, January 2020.\n\n[4]  Lynch, Mallmann-Trenn. Learning hierarchically structured concepts. Neural Networks, 143:798-817, November 2021.\n\n[5]   Lynch, Musco. A basic compositional model for Spiking Neural Networks.  In Jansen, N., Stoelinga, M., van den Bos, P., editors, A Journey from  Process Algebra via Timed Automata to Model Learning. LNCS volume 13560,  2022.\n\n[6]  Chou, Sandhu, Wang, Yu. A general framework for  analyzing stochastic dynamics in learning algorithms.  ArXiv:2006.06171v3, September 2022.\n\n[7]  Lynch. Symbolic Knowledge Structures and Intuitive Knowledge Structures. arXiv:2206.02932, June 2022.\n\n[8]   Murray, Wang, Lynch. Emergence of direction selective retinal cell  types in task-optimized deep learning models. Journal of Computational  Biology 29(4):370-381, April 2022.\n\n[9]  Wang, Halassa.   Thalamocortical contribution to flexible learning in neural systems.   Network Neuroscience 6(4), October, 2022.\n\n[10]  Hummos, Wang,  Drammis, Halassa, Pleger. Evidence for thalamic regulation of frontal  interactions in human cognitive flexibility. PLoS Computational Biology,  18(9), September 2022.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 08/03/2023\n\n\t\t\t\t\tSubmitted by: Nancy A Lynch"
 }
}