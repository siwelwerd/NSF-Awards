{
 "awd_id": "1801501",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SaTC: CORE: Medium: Collaborative: Contextual Integrity: From Theory to Practice",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925177",
 "po_email": "asquicci@nsf.gov",
 "po_sign_block_name": "Anna Squicciarini",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 463687.0,
 "awd_min_amd_letter_date": "2018-07-26",
 "awd_max_amd_letter_date": "2021-05-19",
 "awd_abstract_narration": "Current user-facing computer systems apply a \"notice and consent\" approach to managing user privacy: the user is presented with a privacy notice and then must consent to its terms. Decades of prior research show that this approach is unmanageable: policies are vague, ambiguous, and often include legal terms that make them very difficult to understand, if they are even read at all. These problems are magnified across Internet of Things (IoT) devices, which may not include displays to present privacy information, and may become so ubiquitous in the environment that users cannot possibly determine when their data is actually being captured. This project aims to solve these problems by designing new privacy management systems that automatically infer users' context-specific privacy expectations and then use them to manage the data-capture and data-sharing behaviors of mobile and IoT devices in users' environments. The goals of this research are to better understand privacy expectations, design privacy controls that require minimal user intervention, and demonstrate how emergent technologies can be designed to empower users to best manage their privacy.\r\n\r\nThe theory of \"Privacy as Contextual Integrity\" (CI) postulates that privacy expectations are based on contextual norms, and that privacy violations occur when data flows in ways that defy these norms. The framework can be applied by modeling data flows in terms of the data type, sender, recipient, as well as the specific context (i.e., the purpose for which data is being shared). While this model makes intuitive sense, there are several open research questions that have prevented it from being applied in computer systems. Specifically, the project investigates how privacy expectations change across varying contexts through the use of surveys, interviews, and behavioral studies, and designs systems to automatically infer contextual information so that the process of determining whether or not a data flow is likely to defy user expectations can be automated. The investigators develop a prototype of the novel privacy controls and validate their usability and privacy-preserving properties through iterative laboratory and field experiments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Serge",
   "pi_last_name": "Egelman",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Serge M Egelman",
   "pi_email_addr": "egelman@icsi.berkeley.edu",
   "nsf_id": "000553035",
   "pi_start_date": "2018-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947045940",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "CL10",
   "pgm_ref_txt": "CLB-Career Life Balance"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 94458.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 114026.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 207516.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 47687.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Technology users are frequently asked to make privacy decisions, such as whether or not to share data with third-party applications. Current systems apply a &ldquo;notice and consent&rdquo; approach to privacy: users are given notice about privacy practices, and then must consent to them, if they wish to continue using the system. We know that this model is deeply flawed for a variety of reasons. In particular, one such reason is failure to account for context. For example, while users may agree to provide location data for location-based services, they may object to disclosing the same data for location-based ads. Because current privacy notices and controls fail to account for contextual privacy norms, they do not effectively represent users&rsquo; expectations, invariably leading to perceived privacy violations.</span></p>\n<p><span>The theory of privacy as contextual integrity (CI) asserts that people&rsquo;s privacy expectations are shaped by contextual informational norms; it predicts that practices breaching these norms will be experienced as privacy violations. The theory further asserts that contextual informational norms prescribe data flows according to social context, actors, information types, and transmission principles, and that all of these parameters must be specified in modeling privacy expectations. Failure to do so results in faulty practices and ambiguous notices. Thus, to improve end-user privacy, we have been performing research to: 1) map information flows in terms of CI parameters, 2) inform users about them, and 3) ascertain whether these flows meet contextual&nbsp;</span>norms and/or users&rsquo; expectations.</p>\n<p>During the course of this project, we performed large-scale vignette studies to sharpen our understanding of contextual privacy norms (i.e., how people's privacy expectations change based on changes to the various CI parameters). We also used data collected from in situ experiments to train classifiers to detect context shifts (e.g., to help design system to more intelligently ask people to make privacy decisions or to automatically apply controls on their behalf). Overall, this project has led to a greater understanding of how people make privacy decisions, particularly in the context of in-home devices that are increasingly able to capture highly-sensitive data, so that better controls can be designed.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 10/01/2024<br>\nModified by: Serge&nbsp;M&nbsp;Egelman</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nTechnology users are frequently asked to make privacy decisions, such as whether or not to share data with third-party applications. Current systems apply a notice and consent approach to privacy: users are given notice about privacy practices, and then must consent to them, if they wish to continue using the system. We know that this model is deeply flawed for a variety of reasons. In particular, one such reason is failure to account for context. For example, while users may agree to provide location data for location-based services, they may object to disclosing the same data for location-based ads. Because current privacy notices and controls fail to account for contextual privacy norms, they do not effectively represent users expectations, invariably leading to perceived privacy violations.\n\n\nThe theory of privacy as contextual integrity (CI) asserts that peoples privacy expectations are shaped by contextual informational norms; it predicts that practices breaching these norms will be experienced as privacy violations. The theory further asserts that contextual informational norms prescribe data flows according to social context, actors, information types, and transmission principles, and that all of these parameters must be specified in modeling privacy expectations. Failure to do so results in faulty practices and ambiguous notices. Thus, to improve end-user privacy, we have been performing research to: 1) map information flows in terms of CI parameters, 2) inform users about them, and 3) ascertain whether these flows meet contextualnorms and/or users expectations.\n\n\nDuring the course of this project, we performed large-scale vignette studies to sharpen our understanding of contextual privacy norms (i.e., how people's privacy expectations change based on changes to the various CI parameters). We also used data collected from in situ experiments to train classifiers to detect context shifts (e.g., to help design system to more intelligently ask people to make privacy decisions or to automatically apply controls on their behalf). Overall, this project has led to a greater understanding of how people make privacy decisions, particularly in the context of in-home devices that are increasingly able to capture highly-sensitive data, so that better controls can be designed.\n\n\n\t\t\t\t\tLast Modified: 10/01/2024\n\n\t\t\t\t\tSubmitted by: SergeMEgelman\n"
 }
}