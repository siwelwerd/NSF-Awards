{
 "awd_id": "1750439",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Learning Nonverbal Signatures",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Roger Mailler",
 "awd_eff_date": "2018-02-15",
 "awd_exp_date": "2023-01-31",
 "tot_intn_awd_amt": 550000.0,
 "awd_amount": 597560.0,
 "awd_min_amd_letter_date": "2018-02-05",
 "awd_max_amd_letter_date": "2022-04-04",
 "awd_abstract_narration": "Nonverbal communication is an essential component of our daily life. A quick eyebrow frown from a student will inform the teacher of possible confusion. A tilt of the head may show doubt during negotiation. A patient's smile dynamics can even predict their future recovery. This project will build innovative technologies to allow computers to understand subtle nonverbal behaviors of human users. The main novelty of this project will be in its capacity to learn the inherent variability between individuals on how nonverbal behaviors are expressed. The same emotion can be expressed very differently by different people. A nonverbal signature is how a specific person gestures visually when talking with other people. By recognizing these nonverbal communicative behaviors such as facial expressions, head gestures and eye contact, computers and mobile devices will be able to better understand the user's emotions and affective states. In mental health treatment, the developed technologies have the potential to change how doctors assess disorders with new behavioral objective measures and possibily to predict eventual relapse for the patient. For researchers and animators of virtual characters (e.g., movie studies), the nonverbal signatures will enable an \"a la carte\" selection of the virtual human nonverbal signature that best aligns with the character's profile and personality. In education and online learning, the student's nonverbal signature will help to assess engagement and perceive affective states related to success in learning.   \r\n\r\nThis project will advocate a novel paradigm for learning visual representations of human nonverbal behaviors: a major focus on intra-personal variability followed by analysis of group structures and eventual learning of visual representations generalizable across the population. The research methodology will be motivated by well-studied concepts of idiosyncrasy in nonverbal behavior expressions. The project will introduce the concept of nonverbal signatures which are low-dimensional computational representations of an individual's nonverbal behaviors contextualized by the verbal content and affective context. This project will address four fundamental research challenges: (1) personalized nonverbal embedding -- learning computational representations that summarize the individual variations in nonverbal appearance, (2) learning nonverbal signatures -- contextualizing the nonverbal behaviors with verbal and affective cues to learn more effective representations, (3) signature portfolio analysis -- discovering structure, prototypes and idiosyncrasy from a collection of nonverbal signatures, and (4) generalized nonverbal representations -- learning generalizable nonverbal representations able to adapt to new individuals. These four research aims will be complemented by a comprehensive evaluation plan to include four intermediate evaluations and a continuous overarching evaluation. This research effort will open the door to new sources of human-centric data where accurate interpretation of nonverbal behaviors is essential.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Louis-Philippe",
   "pi_last_name": "Morency",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Louis-Philippe Morency",
   "pi_email_addr": "morency@cs.cmu.edu",
   "nsf_id": "000519300",
   "pi_start_date": "2018-02-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Ave",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 128640.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 117400.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 120761.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 107497.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 123262.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This CAREER project was designed to better understand how to model and interpret nonverbal communicative behaviors during social interactions. The main contributions of this project are at three levels: (1) building large-scale dataset resources to enable study of multimodal and social interactions, (2) building computational algorithms and representations to models multimodal and social data, and (3) create educational recourses to help train researchers in multimodal and social AI.</p>\n<p>The first level of contributions for this CAREER project is related to building large-scale datasets. Most central to this project was the creation of two datasets (PATS v1 and PATS v2) to study the relationship between gestures and speech. These datasets contain more than 300 hours of videos with complete body pose information, time-aligned transcriptions of the spoken words and audio-video streams. 85 speakers are present in these datasets, allowing to study individual differences and also commonalities. The CAREER project also supported the creation of 5 more datasets. The first dataset, named Social-IQ, is designed to benchmark AI algorithms and their skills in understanding social interactions. The dataset contains 1,250 videos and 52,500 question-answer pairs. An extension of this dataset (Social-IQ 2.0) was also created to include short video clips from movies and interactions in car settings. The MOSEI and MOSEAS datasets are designed to study multimodal recognition of emotions and sentiments. The MOSEI dataset focuses on English-speaking videos, while MOSEAS include 4 languages: Spanish, Portuguese, German and French. Finally, the MultiBench dataset is a collection of 15 datasets to study multimodal fusion, beyond just language and vision. MultiBench includes 10 different modalities with 20 predictions tasks and 6 different research areas.</p>\n<p>The second level of contributions for this CAREER project is related to building computational algorithms and representations of multimodal and social interaction data. These contributions can be grouped into three technical challenges: multimodal fusion, multimodal representation learning and cross-modal translation. For multimodal fusion, we proposed many new algorithms that integrate language, vision and audio modalities. A key novelty was to explicitly model unimodal, bimodal and trimodal interactions. This was done efficiently in the Low-Rank Multimodal Fusion model and with improved interpretability in our Dynamic Fusion Graph approach. A second key novelty was to perform multimodal fusion for longer sequences, with the introduction of the Memory Fusion model and the Multi-Attention Recurrent Network. For multimodal representations learning, our most impactful contribution was the introduction of the multimodal transformer: to our knowledge, this is the first transformer model to learn from three different modalities (language, vision and audio). This work was extended to improve interpretability with Multimodal Factorized Models. We also proposed the <em>shifting </em>paradigm where unimodal representations are shifted by their nonverbal modalities (vision and audio). Finally, for cross-modal translation, we develop a family of generative models able to animate gestures based on inputs from language and audio. Our first two models, MixSTAGE and AISLe, emphasized diversity in gestures, beyond the more common gestures (e.g., beat gestures). This work was extended to enable style transfer: learning to adapt to a new speaker&rsquo;s gesture style. We showed how to perform this adaptation from limited data and without forgetting previous styles learned by the generative model.</p>\n<p>The third level of contributions for this CAREER project is related to creating educational resources for training in multimodal and social AI. A significant endeavor was achieved over two years, with the goal of summarizing recent research in multimodal machine learning and building a taxonomy to group these papers into core challenges and sub-challenges. The final survey paper identified 6 core challenges: representation, alignment, reasoning, generation, transference and quantification. This new survey paper enabled the creation of 3-hour tutorial presented at top conferences in computer vision, natural language processing and machine learning. Also, a new 14-week course on multimodal machine learning was recorded and made available online with video lectures, and assignments. Finally, a new course on artificial social intelligence was created to better understand the link between AI and human social intelligence, including social cognition, skills and competency.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/24/2023<br>\n\t\t\t\t\tModified by: Louis-Philippe&nbsp;Morency</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis CAREER project was designed to better understand how to model and interpret nonverbal communicative behaviors during social interactions. The main contributions of this project are at three levels: (1) building large-scale dataset resources to enable study of multimodal and social interactions, (2) building computational algorithms and representations to models multimodal and social data, and (3) create educational recourses to help train researchers in multimodal and social AI.\n\nThe first level of contributions for this CAREER project is related to building large-scale datasets. Most central to this project was the creation of two datasets (PATS v1 and PATS v2) to study the relationship between gestures and speech. These datasets contain more than 300 hours of videos with complete body pose information, time-aligned transcriptions of the spoken words and audio-video streams. 85 speakers are present in these datasets, allowing to study individual differences and also commonalities. The CAREER project also supported the creation of 5 more datasets. The first dataset, named Social-IQ, is designed to benchmark AI algorithms and their skills in understanding social interactions. The dataset contains 1,250 videos and 52,500 question-answer pairs. An extension of this dataset (Social-IQ 2.0) was also created to include short video clips from movies and interactions in car settings. The MOSEI and MOSEAS datasets are designed to study multimodal recognition of emotions and sentiments. The MOSEI dataset focuses on English-speaking videos, while MOSEAS include 4 languages: Spanish, Portuguese, German and French. Finally, the MultiBench dataset is a collection of 15 datasets to study multimodal fusion, beyond just language and vision. MultiBench includes 10 different modalities with 20 predictions tasks and 6 different research areas.\n\nThe second level of contributions for this CAREER project is related to building computational algorithms and representations of multimodal and social interaction data. These contributions can be grouped into three technical challenges: multimodal fusion, multimodal representation learning and cross-modal translation. For multimodal fusion, we proposed many new algorithms that integrate language, vision and audio modalities. A key novelty was to explicitly model unimodal, bimodal and trimodal interactions. This was done efficiently in the Low-Rank Multimodal Fusion model and with improved interpretability in our Dynamic Fusion Graph approach. A second key novelty was to perform multimodal fusion for longer sequences, with the introduction of the Memory Fusion model and the Multi-Attention Recurrent Network. For multimodal representations learning, our most impactful contribution was the introduction of the multimodal transformer: to our knowledge, this is the first transformer model to learn from three different modalities (language, vision and audio). This work was extended to improve interpretability with Multimodal Factorized Models. We also proposed the shifting paradigm where unimodal representations are shifted by their nonverbal modalities (vision and audio). Finally, for cross-modal translation, we develop a family of generative models able to animate gestures based on inputs from language and audio. Our first two models, MixSTAGE and AISLe, emphasized diversity in gestures, beyond the more common gestures (e.g., beat gestures). This work was extended to enable style transfer: learning to adapt to a new speaker\u2019s gesture style. We showed how to perform this adaptation from limited data and without forgetting previous styles learned by the generative model.\n\nThe third level of contributions for this CAREER project is related to creating educational resources for training in multimodal and social AI. A significant endeavor was achieved over two years, with the goal of summarizing recent research in multimodal machine learning and building a taxonomy to group these papers into core challenges and sub-challenges. The final survey paper identified 6 core challenges: representation, alignment, reasoning, generation, transference and quantification. This new survey paper enabled the creation of 3-hour tutorial presented at top conferences in computer vision, natural language processing and machine learning. Also, a new 14-week course on multimodal machine learning was recorded and made available online with video lectures, and assignments. Finally, a new course on artificial social intelligence was created to better understand the link between AI and human social intelligence, including social cognition, skills and competency.\n\n \n\n\t\t\t\t\tLast Modified: 05/24/2023\n\n\t\t\t\t\tSubmitted by: Louis-Philippe Morency"
 }
}