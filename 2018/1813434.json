{
 "awd_id": "1813434",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Bitstream Processing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Danella Zhao",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 449997.0,
 "awd_amount": 449997.0,
 "awd_min_amd_letter_date": "2018-08-23",
 "awd_max_amd_letter_date": "2018-08-23",
 "awd_abstract_narration": "Embedded computing systems are becoming very common in today's world, ranging from wearable devices, to in-home smart speakers, to autonomous appliances and vehicles. Many of the computational tasks that these systems implement require very high performance hardware for tasks like audio voice recognition, visual object recognition, path optimization, and autonomous control.  Historically, such high-performance hardware relied on binary fixed-point algorithms deployed on low-power microcontrollers or digital signal processors. However, the sensing and control interfaces themselves do not use binary number representations, but instead use bitstreams, which encode numeric input and output values using the density of ones over time. Conventional computing substrates requires conversion of both inputs and outputs to interface with physical systems that utilize bitstreams.  This project is developing novel, biologically-inspired approaches for directly operating on data represented in the native bitstream format. Compute hardware that directly operates on these bitstreams can be seamlessly integrated into systems that sense the real world, process sensory data, and issue control commands based on the processed data as well as learned actions based on rewards from the environment. The capability of these new approaches will be demonstrated through two experimental platforms: a very low power speech recognition system that operates on bitstream audio data, and an autonomous airborne vehicle that learns to navigate its environment. This research has broad industry- and economy-wide impact since it will lead to the discovery and realization of novel, powerful, and energy-efficient approaches for implementing power- and energy-constrained embedded computing systems.\r\n\r\nThis research advocates development of novel, biologically-inspired approaches for processing data represented as bitstreams. Bitstreams, which encode numeric values using density of ones (unary) or ones and zeroes (binary) are a natural representation for data sensed from the environment (input) as well as robotic control (output), and can be inexpensively generated using low-cost, yet accurate, sigma-delta modulators. The initial phase of this research project focuses on developing the theoretical and algorithmic underpinnings for visual, auditory, and inertial sensory processing, including feature extraction, bandpass filtering, perspective and coordinate transforms, linear optimization, and memory formation, which are grounded in principles from the speech processing, computer vision, spiking neural networks, reinforcement learning, and signal processing domains. The novel sensory processing capabilities are then deployed in two experimental platforms: first, an ultra-low power acoustic model for speech recognition that can demonstrate the suitability of bitstream processing for feature extraction and sequence learning via long short-term memory. Next, bitstream sensory processing technology is coupled directly to a control system that enables an unmanned aerial vehicle to navigate in a controlled indoor environment while learning, with increasing efficiency, to identify and target sources of rewards. Both demonstration platforms rely on concepts from biological spiking neural networks, stochastic computing for arithmetic operations, as well as oversampled sigma-delta modulation theory for data representation and signal processing tasks, and provide unprecedented levels of efficiency in terms of energy consumption, compute density, and autonomous operation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mikko",
   "pi_last_name": "Lipasti",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Mikko H Lipasti",
   "pi_email_addr": "mikko@engr.wisc.edu",
   "nsf_id": "000290802",
   "pi_start_date": "2018-08-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "21 North Park Street",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537151218",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "2878",
   "pgm_ref_txt": "SPECIAL PROJECTS - CCF"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 449997.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project investigated a range of algorithms and a suite of associated techniques for deploying those algorithms on ultra-low power computational substrates based on the bitstream paradigm.&nbsp; In this paradigm, inspired by stochastic computing, numerical computations proceed one bit at a time, using a simple unary representation for each variable.&nbsp; Due to the simple representation, arithmetic circuits in bitstream datapaths are trivially inexpensive (e.g. a single logical AND gate can be used to multiply two numbers), but the logic can be slow, and frequently presents numerous challenges for achieving performance that is sufficient for real deployment scenarios.&nbsp; The main goal of the project was to investigate many of these challenges and make the bitstream computing approach both practical and useful for applications ranging from navigation and signal processing to deep neural networks.</p>\n<p>Developing and deploying algorithmic kernels and end-to-end applications using bitstream computing has proven to be an extremely challenging, time-consuming, and error-prone process, which discourages designers from adopting this approach. Prior approaches to developing bitstream-based applications required that algorithms be developed in a high-level language using floating-point data types, then rewritten to emulate bitstream logic in a high-level simulator, and finally prototyped using a hardware description language synthesized to a suitable hardware substrate. Implementation and validation of three separate realizations was error-prone and required significant duplication of effort and repetitive validation to ensure correctness.</p>\n<p>To address this challenge, this project developed BitSAD, the first domain-specific language for bitstream computing. BitSAD allows users to write high-level algorithms in a custom high-level language, utilizing typical high-level programming language features like matrices along with custom data types for bitstream computing, and built-in libraries to perform convenient linear algebra matrix operations. Any algorithm developed in BitSAD can be simulated at the software level to debug bitstream-level issues quickly, and the BitSAD compiler automatically generates fully functional and synthesizable hardware descriptions using the Verilog hardware description language (HDL). The HDL can then be effortlessly deployed on a field-programmable gate array (FPGA) or even synthesized into a custom chip. In order to facilitate widespread adoption of this language, the BitSAD compiler has been released under an open source license, and can be downloaded from https://github.com/UW-PHARM/BitSAD.</p>\n<p>The project also resulted in developing a suite of benchmark kernels and applications that are suitable for deployment on bitstream substrates; these are similarly released under an open source license at&nbsp;<a href=\"https://github.com/UW-PHARM/BitBench\">https://github.com/UW-PHARM/BitBench</a></p>\n<p>The project has also implemented and evaluated several new applications from the machine learning and deep neural network domains to further demonstrate the suitability of bitstream computing for complex problems and has significantly advanced the state of the art for designing and implementing complex arithmetic datapaths built from bitstream logic.</p>\n<p>Without the kinds of innovations described here, that dramatically alter the design and architecture of future computing substrates, the continued device scaling of future nanometer semiconductor technologies will fail to provide substantial returns in terms of improvements in utility or performance. As a result, the semiconductor industry, and by extension, the computer industry as a whole. faces serious challenges in maintaining the growth-based business model that has sustained it for decades. This research has had broad industry- and economy-wide impact by helping to address or avert these impending challenges, while effectively training graduate students in the technical skills required to accomplish these goals and leading to technology transfer via internships and permanent employment.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/13/2023<br>\n\t\t\t\t\tModified by: Mikko&nbsp;H&nbsp;Lipasti</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project investigated a range of algorithms and a suite of associated techniques for deploying those algorithms on ultra-low power computational substrates based on the bitstream paradigm.  In this paradigm, inspired by stochastic computing, numerical computations proceed one bit at a time, using a simple unary representation for each variable.  Due to the simple representation, arithmetic circuits in bitstream datapaths are trivially inexpensive (e.g. a single logical AND gate can be used to multiply two numbers), but the logic can be slow, and frequently presents numerous challenges for achieving performance that is sufficient for real deployment scenarios.  The main goal of the project was to investigate many of these challenges and make the bitstream computing approach both practical and useful for applications ranging from navigation and signal processing to deep neural networks.\n\nDeveloping and deploying algorithmic kernels and end-to-end applications using bitstream computing has proven to be an extremely challenging, time-consuming, and error-prone process, which discourages designers from adopting this approach. Prior approaches to developing bitstream-based applications required that algorithms be developed in a high-level language using floating-point data types, then rewritten to emulate bitstream logic in a high-level simulator, and finally prototyped using a hardware description language synthesized to a suitable hardware substrate. Implementation and validation of three separate realizations was error-prone and required significant duplication of effort and repetitive validation to ensure correctness.\n\nTo address this challenge, this project developed BitSAD, the first domain-specific language for bitstream computing. BitSAD allows users to write high-level algorithms in a custom high-level language, utilizing typical high-level programming language features like matrices along with custom data types for bitstream computing, and built-in libraries to perform convenient linear algebra matrix operations. Any algorithm developed in BitSAD can be simulated at the software level to debug bitstream-level issues quickly, and the BitSAD compiler automatically generates fully functional and synthesizable hardware descriptions using the Verilog hardware description language (HDL). The HDL can then be effortlessly deployed on a field-programmable gate array (FPGA) or even synthesized into a custom chip. In order to facilitate widespread adoption of this language, the BitSAD compiler has been released under an open source license, and can be downloaded from https://github.com/UW-PHARM/BitSAD.\n\nThe project also resulted in developing a suite of benchmark kernels and applications that are suitable for deployment on bitstream substrates; these are similarly released under an open source license at https://github.com/UW-PHARM/BitBench\n\nThe project has also implemented and evaluated several new applications from the machine learning and deep neural network domains to further demonstrate the suitability of bitstream computing for complex problems and has significantly advanced the state of the art for designing and implementing complex arithmetic datapaths built from bitstream logic.\n\nWithout the kinds of innovations described here, that dramatically alter the design and architecture of future computing substrates, the continued device scaling of future nanometer semiconductor technologies will fail to provide substantial returns in terms of improvements in utility or performance. As a result, the semiconductor industry, and by extension, the computer industry as a whole. faces serious challenges in maintaining the growth-based business model that has sustained it for decades. This research has had broad industry- and economy-wide impact by helping to address or avert these impending challenges, while effectively training graduate students in the technical skills required to accomplish these goals and leading to technology transfer via internships and permanent employment.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/13/2023\n\n\t\t\t\t\tSubmitted by: Mikko H Lipasti"
 }
}