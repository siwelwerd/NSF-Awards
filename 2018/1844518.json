{
 "awd_id": "1844518",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research: Toward Informing Users About Algorithmic Fairness",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 52170.0,
 "awd_amount": 52170.0,
 "awd_min_amd_letter_date": "2018-08-28",
 "awd_max_amd_letter_date": "2018-08-28",
 "awd_abstract_narration": "Computers make important decisions about people, including about criminal justice issues such as sentencing and bail. These decisions can sometimes be considered discriminatory if the computer system does not treat people -- for example, people of different races -- fairly. However, deciding what it means for a computer system to be \"fair\" is complicated: there are many possible mathematical definitions of fairness, and a system cannot achieve them all at the same time. For society to make policy related to these definitions of fairness, non-technical people -- from legal and policy experts to the general public -- must be able to understand subtle distinctions between mathematical concepts. This research will develop and evaluate approaches to explaining these concepts to non-experts, so that future research can investigate people's opinions about them. \r\n\r\nThe proposed work will develop and evaluate text and graphical descriptions and/or vignettes illustrating different nondiscrimination properties and their tradeoffs. For concreteness, in this exploratory work the project will focus only on accuracy-like nondiscrimination properties, only in the context of criminal justice, such as algorithms used in bail and sentencing decisions. The project will use iterative, qualitative, person-centered design, including interviews and co-design studies with both non-computer-science subject-matter experts in law and social science and laypeople to develop and preliminarily evaluate the explanations. In parallel, the project will systematize the space of nondiscrimination properties.  This effort will inform qualitative design efforts; concurrently, interviews with legal and ethical experts will also shape the systematization, in a process of iterative refinement.  The end product will be a description of how various nondiscrimination definitions differ along the axes empirical studies find most important.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Tschantz",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Tschantz",
   "pi_email_addr": "mct@icsi.berkeley.edu",
   "nsf_id": "000690105",
   "pi_start_date": "2018-08-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "International Computer Science Institute",
  "inst_street_address": "2150 SHATTUCK AVE",
  "inst_street_address_2": "SUITE 250",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106662900",
  "inst_zip_code": "947041345",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "INTERNATIONAL COMPUTER SCIENCE INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GSRMP1QCXU74"
 },
 "perf_inst": {
  "perf_inst_name": "International Computer Science Institute",
  "perf_str_addr": "1947 Center Street, Suite 600",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947041159",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 52170.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Research supported by this award developed artifacts for explaining different mathematical approaches to algorithmic fairness or non-discrimination to non-expert users who may be affected by algorithmic systems. In particular, we developed and validated explanations for demographic parity, equalized odds, and equal opportunity, three commonly discussed fairness mechanisms. We also developed a set of comprehension questions that can be used to accurately measure whether or not the explanation was understandable for the target audience. Our explanations and comprehension questions were designed to highlight both how these approaches work generally and also specific cases where they provide counterintuitive results. While validating our artifacts, we found that the better people understand each of these definitions (all of which have significant limitations), the less they like them or want to use them in decision-making systems.</p>\n<p>Work done under this award was published in top conferences in computer science, including AAAI/ACM AIES and ICML. In addition, this work was disseminated at workshops focusing on the intersection of machine learning with society and at a workshop hosted by the American National Standards Institute (ANSI). Our artifacts for explaining fairness definitions were used in class by a professor from another institution. This award also contributed to graduate education by training three graduate students both in algorithmic fairness mechanisms and in methods for survey and interview studies.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2020<br>\n\t\t\t\t\tModified by: Michael&nbsp;Tschantz</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nResearch supported by this award developed artifacts for explaining different mathematical approaches to algorithmic fairness or non-discrimination to non-expert users who may be affected by algorithmic systems. In particular, we developed and validated explanations for demographic parity, equalized odds, and equal opportunity, three commonly discussed fairness mechanisms. We also developed a set of comprehension questions that can be used to accurately measure whether or not the explanation was understandable for the target audience. Our explanations and comprehension questions were designed to highlight both how these approaches work generally and also specific cases where they provide counterintuitive results. While validating our artifacts, we found that the better people understand each of these definitions (all of which have significant limitations), the less they like them or want to use them in decision-making systems.\n\nWork done under this award was published in top conferences in computer science, including AAAI/ACM AIES and ICML. In addition, this work was disseminated at workshops focusing on the intersection of machine learning with society and at a workshop hosted by the American National Standards Institute (ANSI). Our artifacts for explaining fairness definitions were used in class by a professor from another institution. This award also contributed to graduate education by training three graduate students both in algorithmic fairness mechanisms and in methods for survey and interview studies. \n\n \n\n\t\t\t\t\tLast Modified: 12/30/2020\n\n\t\t\t\t\tSubmitted by: Michael Tschantz"
 }
}