{
 "awd_id": "1815886",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Coordination in tightly coupled domains: Stepping stone rewards to induce the correct joint actions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2018-07-02",
 "awd_max_amd_letter_date": "2018-07-02",
 "awd_abstract_narration": "This project introduces a new multiagent learning approach that leads to coordinated behavior in tightly coupled domains, that is, in domains where all agents must do the right thing at the right time for the team to achieve its goals. For example, getting a team of agents to lift and move an object heavier than the payload capacity of any single agent requires a sufficient number of agents to perform the correct action at the correct time. Unfortunately, most current learning methods fail in such situations because they rely on reinforcing the correct agent behavior only after the agents stumble upon the right actions. But what if the agents never jointly find the right actions? This project addresses this issue by introducing \"stepping-stone rewards\" that incentivize agents to perform the right actions even if their teammates have not yet found the correct complementary actions. The impact of this project will be to create larger and more capable multiagent teams that can be deployed in industry (such as factory robots that are not limited to a single task), in the field (such as autonomous search and rescue systems), in education (such as interactive learning via online gameplay) and in the home (such as networks of smart appliances).\r\n\r\nThe main technical contribution of this project is to shift the learning problem faced by an agent from \"did I take the correct action?\" to \"would my action have been correct had other agents taken the complementary action?\" In tightly coupled multiagent domains, the first question results in very little positive feedback, creating a difficult to impossible learning problem. The new stepping stone rewards leverage hypothetical partners (partners that are surmised by an agent to explore the joint-action space) to overcome this difficulty by assessing the potential benefits of a particular action. Intuitively, stepping-stone rewards create a gradient for the agents to follow to enable fast and efficient learning in tightly coupled domains.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kagan",
   "pi_last_name": "Tumer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kagan Tumer",
   "pi_email_addr": "kagan.tumer@oregonstate.edu",
   "nsf_id": "000240751",
   "pi_start_date": "2018-07-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon State University",
  "inst_street_address": "1500 SW JEFFERSON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CORVALLIS",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5417374933",
  "inst_zip_code": "973318655",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OR04",
  "org_lgl_bus_name": "OREGON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MZ4DYXE1SL98"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon State University",
  "perf_str_addr": "",
  "perf_city_name": "Corvallis",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "973312140",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OR04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>This proposal introduces a new multiagent learning paradigm based on&nbsp;</span><span>providing agents with intermediate rewards so they can achieve complex tasks. This is a critical step in the broad use of AI agents in the real world because to date, most machine&nbsp;</span><span>learning (single or multiagent) algorithms are predicated on the simple premise that agents can randomly sample their actions and&nbsp;</span><span>eventually&nbsp;</span><span>stumble on the correct action.</span></p>\n<p><span>Unfortunately that assumption breaks down in complex multiagent problems that require many agents to simultaneously stumble upon the correct joint action at the correct time. In this work, we direct the agents to&nbsp;</span><span>discover correct partial actions. We introduce rewards that encourage agents to explore broadly, whikle also introducing rewards to&nbsp;</span><span>incentivize actions that are potentially useful. We also introuce a new paradigm dubbed the Asymetric Island Model, that creates islands to train different agents for different capabilities before putting them together in functional teams.</span></p>\n<p>The Impact&nbsp;of this work is to increase the applicability of multiagent learning to diverse and challenging tasks, leading to larger and more capable multiagent teams that can be deployed in industry (factory robots that are not limited to a single task), in the field (autonomous search and rescue systems), in education (interactive learning via online gameplay) and in the home (networks of smart appliances).</p>\n<p><span><br /></span></p>\n</div>\n</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/11/2023<br>\n\t\t\t\t\tModified by: Kagan&nbsp;Tumer</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\nThis proposal introduces a new multiagent learning paradigm based on providing agents with intermediate rewards so they can achieve complex tasks. This is a critical step in the broad use of AI agents in the real world because to date, most machine learning (single or multiagent) algorithms are predicated on the simple premise that agents can randomly sample their actions and eventually stumble on the correct action.\n\nUnfortunately that assumption breaks down in complex multiagent problems that require many agents to simultaneously stumble upon the correct joint action at the correct time. In this work, we direct the agents to discover correct partial actions. We introduce rewards that encourage agents to explore broadly, whikle also introducing rewards to incentivize actions that are potentially useful. We also introuce a new paradigm dubbed the Asymetric Island Model, that creates islands to train different agents for different capabilities before putting them together in functional teams.\n\nThe Impact of this work is to increase the applicability of multiagent learning to diverse and challenging tasks, leading to larger and more capable multiagent teams that can be deployed in industry (factory robots that are not limited to a single task), in the field (autonomous search and rescue systems), in education (interactive learning via online gameplay) and in the home (networks of smart appliances).\n\n\n\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 10/11/2023\n\n\t\t\t\t\tSubmitted by: Kagan Tumer"
 }
}