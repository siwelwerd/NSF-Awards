{
 "awd_id": "1841354",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: EAGER: Handling Online Risks and Creating Safe Spaces: Content Moderation in Live Streaming Micro Communities",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 214735.0,
 "awd_amount": 230055.0,
 "awd_min_amd_letter_date": "2018-07-26",
 "awd_max_amd_letter_date": "2019-05-06",
 "awd_abstract_narration": "This research will investigate how individuals and small groups handle content moderation real time in the context of live streaming, from both technical and social perspectives, distinguishing between professional content creators who create content for a living, and hobbyists.  Live streaming services such as Twitch are the latest form of social media that marries user-generated content with the traditional concept of live television broadcasting: as someone broadcasts, viewers can post comments in a chat interface that is displayed alongside the broadcast, creating an interactive synchronous media experience. This real-time interaction, however, makes the platform ripe for deviant behavior, as potential harassers can visually see the immediate impact of their harsh words on the person who is broadcasting. Most current forms of social media rely on crowdsourced methods of moderation, where users report bad content that is ultimately reviewed by a human moderator. This does not work well in the context of real-time moderation, posing greater social and technological challenges. This project will study approaches to improving understanding of the sociotechnical aspects of content moderation from the perspective of micro communities on live streaming platforms. By understanding how streamers currently moderate audiences through manual and automated labor, the research will identify opportunities for technology to assist and enhance the moderation process and provide guidelines for sustainable and scalable moderation. Exploration of different governance structures of moderation may also yield insights into alternative models of moderation for the future of social media and understanding of how different moderation practices may influence the evolution of positive and negative norms in micro communities. \r\n\r\nBecause live streaming is such a new phenomenon, presenting novel technical and social challenges, exploratory research is required before any serious attempt to solve its problems through technology design.  This research agenda will advance knowledge about how moderation influences the development of social norms in micro communities from a qualitative perspective, laying the groundwork for future large-scale empirical studies, experiments, and development of useful artificial intelligence tools. The research will be able to identify the breadth of methods that are employed in the practice of moderation that will yield a comprehensive framework of understanding the conceptual functions of moderation by building a taxonomy of moderation, and develop a common language for both academics and practitioners that enables mapping between problems and potential design solutions. Moreover, through ethnographic work, the research will provide descriptive knowledge of this new form of social media that results in novel research questions unique to this particular technology.  This research will inform design of moderation tools and practices that could impact millions of people who publish content online and yet even more people who view that content. By focusing on the individual producer, rather than the corporation running the system, as the center of their own system, the findings may be able to empower a new era of Internet activity in which individuals and small groups have more agency over what happens online.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Donghee Yvette",
   "pi_last_name": "Wohn",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Donghee Yvette Wohn",
   "pi_email_addr": "wohn@njit.edu",
   "nsf_id": "000679141",
   "pi_start_date": "2018-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New Jersey Institute of Technology",
  "inst_street_address": "323 DR MARTIN LUTHER KING JR BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "NEWARK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "9735965275",
  "inst_zip_code": "071021824",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NJ10",
  "org_lgl_bus_name": "NEW JERSEY INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "SGBMHQ7VXNH5"
 },
 "perf_inst": {
  "perf_inst_name": "New Jersey Institute of Technology",
  "perf_str_addr": "University Heights",
  "perf_city_name": "Newark",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "071021982",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NJ10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 214735.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 15320.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Livestreaming services are the latest form of social media that combines user-generated content with the traditional concept of live television broadcasting: as someone broadcasts, viewers can post comments in a chat interface that is displayed alongside the broadcast, creating an interactive synchronous media experience. This real-time interaction, however, makes the platform ripe for deviant behavior, as potential harassers can visually see the immediate impact of their harsh words on the person who is broadcasting. Most current forms of social media rely on crowdsourced methods of moderation, where users report bad content that is ultimately reviewed by a human moderator. This &ldquo;reporting&rdquo; function does not work well in the context of real-time moderation because of the significant time delays, posing greater social and technological challenges.</p>\n<p>Twitch is one of the leading livestreaming platforms that is interesting from a moderation perspective because of its unique governance structure. Unlike centrally managed social media such as Facebook or Twitter, Twitch users form micro communities centered around the broadcasters (called &ldquo;streamers&rdquo;). These micro communities each operate under different norms, have different audiences, and are responsible for the moderation of their own chat. There are over a million streamers on Twitch, yet none of the channels are exactly the same and what may be acceptable in one channel may not apply to another channel.</p>\n<p>One of the main reasons of the lack of literature on individual and small group content moderation is the lack of social systems that enable its content producers to take a more active role in moderation aside from reporting abusive people or negative content. For example, social media systems such as Facebook and YouTube handle harassment reports centrally, and rely on a combination of algorithms and human labor to detect and remove deviant content. With the increasing number of content producers and increased volume and bandwidth usage of content per person, the exponential increase of multimedia-heavy content will inevitably create limitations in current methods of content moderation. Thus exploration of different governance structures of moderation may yield insights into alternative models of moderation for the future of social media and understanding of how different moderation practices may influence the evolution of positive and negative norms in micro communities.</p>\n<p>This project set out to understand how norms form in live streaming communities and the role of moderation in that process. The project set out to shed light on challenges of utilizing automated and human moderation to identify opportunities for more technical intervention, and try to understand how individuals handle human moderation. The project also aimed to understand the emotional aspects of handling online risks, including relationships between content producers and moderators, identity and audience management, what kind of social support structures, if any, are in place to alleviate psychological distress, and the role technology plays in all of these processes.</p>\n<p>The research conducted included conducted extensive qualitative and quantitative work with both the volunteer content moderators and live streamers who are the leaders of the micro communities. Much of this work was foundational descriptive work that had not been done in the context of live streaming, leading to insight into how these communities work and how similar/different they are compared to other types of online communities. The activities conducted to achieve this objective involved digital ethnography (observations of live streams), interviews that were conducted both remotely and in-person,</p>\n<p>surveys, and video analysis of moderators' screens when they were working. Through the myriad of different research methods employed, our team was able to obtain a holistic understanding of the ecology and practices of these live streaming communities, leading to specific design guidelines for future online communities. We analyzed the use of algorithms, bots, and different tools to understand the role that technology explicitly takes in the process of community management, and also talked to moderators to understand their individual strategies. This led to the building of a taxonomy of moderation, mapping of the moderation workflow, and categorizing functions of technical interventions for future AI development. To understand how governance influences moderation practices and public perceptions of moderation, we also conducted comparative studies across multiple social media platforms. Research of live streaming communities also led to some derivative projects around specific cultures of these communities and practices of the creators and fans.</p>\n<p>The grant was able to provide research experience and training for one postdoctoral scholar, three graduate students, and 12 undergraduate students. Seven of the 15 students were women. The research from this grant resulted in at least 16 scholarly publications in peer-reviewed, reputable outlets in the areas of computing, media studies, and applied psychology.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/04/2022<br>\n\t\t\t\t\tModified by: Donghee Yvette&nbsp;Wohn</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nLivestreaming services are the latest form of social media that combines user-generated content with the traditional concept of live television broadcasting: as someone broadcasts, viewers can post comments in a chat interface that is displayed alongside the broadcast, creating an interactive synchronous media experience. This real-time interaction, however, makes the platform ripe for deviant behavior, as potential harassers can visually see the immediate impact of their harsh words on the person who is broadcasting. Most current forms of social media rely on crowdsourced methods of moderation, where users report bad content that is ultimately reviewed by a human moderator. This \"reporting\" function does not work well in the context of real-time moderation because of the significant time delays, posing greater social and technological challenges.\n\nTwitch is one of the leading livestreaming platforms that is interesting from a moderation perspective because of its unique governance structure. Unlike centrally managed social media such as Facebook or Twitter, Twitch users form micro communities centered around the broadcasters (called \"streamers\"). These micro communities each operate under different norms, have different audiences, and are responsible for the moderation of their own chat. There are over a million streamers on Twitch, yet none of the channels are exactly the same and what may be acceptable in one channel may not apply to another channel.\n\nOne of the main reasons of the lack of literature on individual and small group content moderation is the lack of social systems that enable its content producers to take a more active role in moderation aside from reporting abusive people or negative content. For example, social media systems such as Facebook and YouTube handle harassment reports centrally, and rely on a combination of algorithms and human labor to detect and remove deviant content. With the increasing number of content producers and increased volume and bandwidth usage of content per person, the exponential increase of multimedia-heavy content will inevitably create limitations in current methods of content moderation. Thus exploration of different governance structures of moderation may yield insights into alternative models of moderation for the future of social media and understanding of how different moderation practices may influence the evolution of positive and negative norms in micro communities.\n\nThis project set out to understand how norms form in live streaming communities and the role of moderation in that process. The project set out to shed light on challenges of utilizing automated and human moderation to identify opportunities for more technical intervention, and try to understand how individuals handle human moderation. The project also aimed to understand the emotional aspects of handling online risks, including relationships between content producers and moderators, identity and audience management, what kind of social support structures, if any, are in place to alleviate psychological distress, and the role technology plays in all of these processes.\n\nThe research conducted included conducted extensive qualitative and quantitative work with both the volunteer content moderators and live streamers who are the leaders of the micro communities. Much of this work was foundational descriptive work that had not been done in the context of live streaming, leading to insight into how these communities work and how similar/different they are compared to other types of online communities. The activities conducted to achieve this objective involved digital ethnography (observations of live streams), interviews that were conducted both remotely and in-person,\n\nsurveys, and video analysis of moderators' screens when they were working. Through the myriad of different research methods employed, our team was able to obtain a holistic understanding of the ecology and practices of these live streaming communities, leading to specific design guidelines for future online communities. We analyzed the use of algorithms, bots, and different tools to understand the role that technology explicitly takes in the process of community management, and also talked to moderators to understand their individual strategies. This led to the building of a taxonomy of moderation, mapping of the moderation workflow, and categorizing functions of technical interventions for future AI development. To understand how governance influences moderation practices and public perceptions of moderation, we also conducted comparative studies across multiple social media platforms. Research of live streaming communities also led to some derivative projects around specific cultures of these communities and practices of the creators and fans.\n\nThe grant was able to provide research experience and training for one postdoctoral scholar, three graduate students, and 12 undergraduate students. Seven of the 15 students were women. The research from this grant resulted in at least 16 scholarly publications in peer-reviewed, reputable outlets in the areas of computing, media studies, and applied psychology.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/04/2022\n\n\t\t\t\t\tSubmitted by: Donghee Yvette Wohn"
 }
}