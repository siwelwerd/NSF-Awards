{
 "awd_id": "1763481",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: Medium: Collaborative Research: Beyond Sparsity: Refined Measures of Complexity for Linear Algebra",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2018-03-15",
 "awd_exp_date": "2023-02-28",
 "tot_intn_awd_amt": 469428.0,
 "awd_amount": 501428.0,
 "awd_min_amd_letter_date": "2018-03-14",
 "awd_max_amd_letter_date": "2021-04-21",
 "awd_abstract_narration": "Modern data science applications exploit structure in real life data using machine learning (including deep learning) algorithms. At the core of most of these systems are algorithms for a branch of mathematics called linear algebra. In particular, a large portion of these algorithms utilize the fact that real life data has properties that can be captured using certain parsimonious linear algebraic structures. This project studies new, more powerful linear algebraic structures and algorithms that exploit these new structures. Given the fundamental importance of these algorithms, ideas generated from this project are expected to be implemented in widely deployed machine learning systems. The outreach component of this project involves (1) a technical workshop for researchers from diverse areas and (2) outreach events for K-12 students.\r\n\r\nA variety of problems in modern data science have been successfully characterized using a width. For example, one of the most common widths, the rank of a matrix, has a near-ubiquitous use across many applications. This project significantly expands the understanding of several recently proposed widths and extracts their full potential for positive practical outcomes. Furthermore, it contributes to the recently growing work on beyond worst-case analysis in linear algebra, machine learning and coding theory.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Atri",
   "pi_last_name": "Rudra",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Atri Rudra",
   "pi_email_addr": "atri@buffalo.edu",
   "nsf_id": "000505466",
   "pi_start_date": "2018-03-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Buffalo",
  "inst_street_address": "520 LEE ENTRANCE STE 211",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "7166452634",
  "inst_zip_code": "142282577",
  "inst_country_name": "United States",
  "cong_dist_code": "26",
  "st_cong_dist_code": "NY26",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "GMZUKXFDJMA9",
  "org_uei_num": "LMCJKRFW5R81"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Buffalo",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "142282567",
  "perf_ctry_code": "US",
  "perf_cong_dist": "26",
  "perf_st_cong_dist": "NY26",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 120648.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 138162.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 226618.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Deep learning pipelines at their core depend on efficient algorithms for a branch of mathematics called linear algebra. A lot of the recent advances in deep learning have come from more efficient algorithms and hardware for implementing some of these basic linear algebra operations. One big bottleneck in creating such efficient algorithms is that traditionally during the training phase one would try and learn basic linear algebra primitives (especially matrices) that had no structure encoded in them. This is at odds with the fact that real life data has properties that can be captured using certain parsimonious linear algebraic structures. A second bottleneck is that existing improvements in deep learning pipelines that use more structured matrices&nbsp; had not been targeted at the specific hardware that is used to train deep learning pipelines (which means that the theoretical promise of structured matrices has not been transferred to practice yet).<br /><br />One of the main contributions of this project is the design of deep learning pipelines that use specific kinds of structured matrices called Monarch matrices that are in sync with the modern hardware. As its second main contribution, the project has designed a new algorithm (called Flash Attention) that optimizes the ubiquitous \"Attention\" mechanism to make them better utilize the hardware that they are trained on. Our work combines known techniques from multiple fields in ways that had not been done before in machine learning. For example, the Monarch matrices are based on the ubiquitous Fast Fourier transform from signal processing and Flash Attention uses ideas from Input/Output complexity, which is a staple in database systems. Both of these new systems have shown substantial savings (in time and memory) over the state of the art systems.<br /><br />Systems developed as part of this project already have had substantial impact in the broader machine learning community. Specifically, Flash Attention is now part of standard machine learning frameworks and used in systems build by big tech companies. For example, Flash Attention has now been incorporated into the following standard machine learning frameworks: pytorch (which is arguably the de facto standard deep learning framework), Huggingface's transformers library, Microsoft's DeepSpeed and Nvidia's Megatron-LM. Further, Flash attention is (partially) responsible for speeding up the following systems: Meta's AITemplate, Nvidia's FasterTransformer and Huggingface's diffusers library for diffusion models. <br /><br />The project supported the training of two Ph.D. student (both female) and ten UG students (many of whom made fundamental contributions to the development of Monarch matrices). A survey outlining the main theoretical ideas behind our use of structured matrices in deep learning pipelines was also written as part of this project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/30/2023<br>\n\t\t\t\t\tModified by: Atri&nbsp;Rudra</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDeep learning pipelines at their core depend on efficient algorithms for a branch of mathematics called linear algebra. A lot of the recent advances in deep learning have come from more efficient algorithms and hardware for implementing some of these basic linear algebra operations. One big bottleneck in creating such efficient algorithms is that traditionally during the training phase one would try and learn basic linear algebra primitives (especially matrices) that had no structure encoded in them. This is at odds with the fact that real life data has properties that can be captured using certain parsimonious linear algebraic structures. A second bottleneck is that existing improvements in deep learning pipelines that use more structured matrices  had not been targeted at the specific hardware that is used to train deep learning pipelines (which means that the theoretical promise of structured matrices has not been transferred to practice yet).\n\nOne of the main contributions of this project is the design of deep learning pipelines that use specific kinds of structured matrices called Monarch matrices that are in sync with the modern hardware. As its second main contribution, the project has designed a new algorithm (called Flash Attention) that optimizes the ubiquitous \"Attention\" mechanism to make them better utilize the hardware that they are trained on. Our work combines known techniques from multiple fields in ways that had not been done before in machine learning. For example, the Monarch matrices are based on the ubiquitous Fast Fourier transform from signal processing and Flash Attention uses ideas from Input/Output complexity, which is a staple in database systems. Both of these new systems have shown substantial savings (in time and memory) over the state of the art systems.\n\nSystems developed as part of this project already have had substantial impact in the broader machine learning community. Specifically, Flash Attention is now part of standard machine learning frameworks and used in systems build by big tech companies. For example, Flash Attention has now been incorporated into the following standard machine learning frameworks: pytorch (which is arguably the de facto standard deep learning framework), Huggingface's transformers library, Microsoft's DeepSpeed and Nvidia's Megatron-LM. Further, Flash attention is (partially) responsible for speeding up the following systems: Meta's AITemplate, Nvidia's FasterTransformer and Huggingface's diffusers library for diffusion models. \n\nThe project supported the training of two Ph.D. student (both female) and ten UG students (many of whom made fundamental contributions to the development of Monarch matrices). A survey outlining the main theoretical ideas behind our use of structured matrices in deep learning pipelines was also written as part of this project.\n\n\t\t\t\t\tLast Modified: 06/30/2023\n\n\t\t\t\t\tSubmitted by: Atri Rudra"
 }
}