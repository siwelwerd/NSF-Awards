{
 "awd_id": "1815619",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Towards Efficient Deep Inference for Mobile Applications",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Alexander Jones",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 499723.0,
 "awd_amount": 515723.0,
 "awd_min_amd_letter_date": "2018-06-25",
 "awd_max_amd_letter_date": "2019-02-28",
 "awd_abstract_narration": "An ever-increasing number of mobile applications are using deep learning models to provide novel and useful features, such as language translation and object recognition. These features are supported by passing input data, for example a photo or an audio clip, to complex models in order to generate meaningful output. However, mobile applications that use deep learning models currently need to choose between prediction accuracy and speed at development time. This can lead to poor user experience due to reasons such as running state-of-the-art models on older mobile devices. \r\n\r\nThe proposed MODI (MObile Deep Inference) project outlines new research in designing and implementing a mobile-aware deep inference platform that combines innovations in both algorithm and system optimizations. The proposed work will address mobile deep inference performance problems by enabling flexible, fine-grained model partition and layer-based inference execution, as well as mobile-specific model designs. In addition, MODI enables a scalable mobile deep inference paradigm with efficient model management both on-device and in the cloud. \r\n\r\nThe project will empower deep learning to provide useful features for mobile applications with significantly improved performance. Consequently, this project will open doors to allow running optimized deep learning models on much more resource-constrained devices such as embedded devices. The MODI project can be used as a standalone cloud system or integrated with existing general inference serving platforms by incorporating its mobile-specific optimizations, thereby increasing adoption. \r\n\r\nThe broader impacts of the project will include graduate and undergraduate courses that incorporate research results, outreach to expose undergraduates and K-12 students to research in both computer systems and deep learning. In addition, project related source code and other resources will be released to the research community through the project website at http://tianguo.info/projects/modi.html\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tian",
   "pi_last_name": "Guo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tian Guo",
   "pi_email_addr": "tian@wpi.edu",
   "nsf_id": "000753495",
   "pi_start_date": "2018-06-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Xiangnan",
   "pi_last_name": "Kong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiangnan Kong",
   "pi_email_addr": "xkong@wpi.edu",
   "nsf_id": "000679449",
   "pi_start_date": "2018-06-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Worcester Polytechnic Institute",
  "inst_street_address": "100 INSTITUTE RD",
  "inst_street_address_2": "",
  "inst_city_name": "WORCESTER",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "5088315000",
  "inst_zip_code": "016092280",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "WORCESTER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJNQME41NBU4"
 },
 "perf_inst": {
  "perf_inst_name": "Worcester Polytechnic Institute",
  "perf_str_addr": "100 Institute Rd",
  "perf_city_name": "Worcester",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "016092247",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499723.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Today's mobile applications often rely on deep learning models to provide novel and useful features, such as object detection and language translation. However, earlier deep learning models are resource intensive, and the execution can take unreasonably long time on the mobile device. A common approach is to offload the deep learning execution, commonly referred to as inference, to a powerful server. However, depending on factors such as mobile device capacity, mobile network condition, and server workload, the end-to-end server-based inference time might execute the on-device inference time. In short, it is unclear what the execution option will deliver good user experience and how mobile developers can make such informed decisions.</p>\n<p>This project aims to address the key research questions in enabling good mobile deep inference performance from two orthogonal directions. In the first direction, the project designed frameworks, and policies to run unmodified deep learning models based on empirical performance characterization. For example, collective frameworks were built for efficiently preprocessing the input data, selecting the most accurate model within the time budget, and managing a set of functionally equivalent models given the memory constraint. Using one type of inference requests called image classification, we showed that these frameworks enable dynamic decision making to adapt to different execution environments thus improved accuracy and performance trade-offs. In the second direction, the project redesigned deep learning models that are more mobile-friendly and more suitable for mobile-cloud collaborative inference. For example, we designed a memory-adaptive model with multiple exits that can flexibly use available memory to generate inference results with different accuracies.</p>\n<p>The project provided research opportunities and training to both graduate and undergraduate students. The research results were disseminated in various formats: peer-reviewed publications, open-sourcing, and invited talks. Relevant projects have been transformed into course materials that were offered at the Principal Investigator's institute.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/27/2022<br>\n\t\t\t\t\tModified by: Tian&nbsp;Guo</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nToday's mobile applications often rely on deep learning models to provide novel and useful features, such as object detection and language translation. However, earlier deep learning models are resource intensive, and the execution can take unreasonably long time on the mobile device. A common approach is to offload the deep learning execution, commonly referred to as inference, to a powerful server. However, depending on factors such as mobile device capacity, mobile network condition, and server workload, the end-to-end server-based inference time might execute the on-device inference time. In short, it is unclear what the execution option will deliver good user experience and how mobile developers can make such informed decisions.\n\nThis project aims to address the key research questions in enabling good mobile deep inference performance from two orthogonal directions. In the first direction, the project designed frameworks, and policies to run unmodified deep learning models based on empirical performance characterization. For example, collective frameworks were built for efficiently preprocessing the input data, selecting the most accurate model within the time budget, and managing a set of functionally equivalent models given the memory constraint. Using one type of inference requests called image classification, we showed that these frameworks enable dynamic decision making to adapt to different execution environments thus improved accuracy and performance trade-offs. In the second direction, the project redesigned deep learning models that are more mobile-friendly and more suitable for mobile-cloud collaborative inference. For example, we designed a memory-adaptive model with multiple exits that can flexibly use available memory to generate inference results with different accuracies.\n\nThe project provided research opportunities and training to both graduate and undergraduate students. The research results were disseminated in various formats: peer-reviewed publications, open-sourcing, and invited talks. Relevant projects have been transformed into course materials that were offered at the Principal Investigator's institute. \n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/27/2022\n\n\t\t\t\t\tSubmitted by: Tian Guo"
 }
}