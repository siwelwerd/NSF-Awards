{
 "awd_id": "1837991",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Compositional Learning, Maps and Transfer: Statistical and Machine Learning on Collections of Data Sets",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2019-01-01",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 700000.0,
 "awd_amount": 700000.0,
 "awd_min_amd_letter_date": "2018-09-06",
 "awd_max_amd_letter_date": "2018-09-06",
 "awd_abstract_narration": "One of the landmarks of human intelligence is the ability to not only find solutions to hard problems, but to learn from past experiences and accumulate knowledge that may be (partially) transferred for quickly solving new problems. This project will develop novel foundational techniques for learning compositional rules, from collections of data sets and machine learning problems. The building blocks that the investigator will develop enable sharing of learning across multiple data sets and modalities. A first building block will enable machine learning algorithms to store solutions to past problems and use maps and abstractions to transfer knowledge to new problems. This requires efficient techniques for learning maps, how to compose them to enable knowledge transfer, all in a way that is compatible with the representation of the problems and their solutions, which also need to be automatically learned. These ideas will be tested on problems ranging from object and pattern recognition of images to behavior of interacting agent systems, from fusing data sets acquired with different sensors to controlling virtual and real agents.  This project will provide general, foundational results in machine learning, which can be applied to applications in virtually any domain of human endeavor. \r\n\r\nThe investigator will develop new techniques focused on representation and transfer learning, in particular: (i) Compositional Learning: the ability to learn and factorize through composition maps between data sets, and of functions (for classification and regression tasks) on data sets (e.g. the task f may be learned by using the map h to one data set on which learning already occurred and the already-learned function g on that data), in order to enhance both learning rates, knowledge extraction and transfer across data sets and data types; (ii) Map Learning: the ability to efficiently learn, represent, store, recall and apply maps between complex data sets, possibly of different modalities; but also learn maps that transform, at least approximately, one task into another, and transfer knowledge from one task to another; (iii) Representation Learning: the ability to learn how to efficiently represent, store and recall complex data sets, across multiple sensor modalities, and across different levels of abstractions -- for example, learning efficient representations of data from multiple types of sensors, learning of classifiers and regression functions, or learning interaction kernels in agent-based systems, as well as transfer those functions across sensor modalities, data sets, dynamical systems.  While advancing current state of art techniques in each of these learning abilities, the research will tackle applications in learning invariances and performing object recognition tasks in images, detecting whether objects in an image are new or known, learn interaction rules from observing trajectories of interacting agent systems, and implement the ideas of compositional learning in the context of learning systems both virtual (for examples, using the OpenAI challenges) and real (for example, using robots), on sequences of tasks of increasing difficulty.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mauro",
   "pi_last_name": "Maggioni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mauro Maggioni",
   "pi_email_addr": "mauro.maggioni@jhu.edu",
   "nsf_id": "000398937",
   "pi_start_date": "2018-09-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 700000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We developed novel techniques for a variety of problems in statistics and machine learning, that are challenging due to the high-dimensionality of the data involved. These challenges can be of at least two types: (a) statistical, for example implying that a very large amount of data would be needed to make any significant progress in accuracy of a statistical estimator or a machine-learned approximation, or (b) computational, implying that only with large computational power it would be possible to solve a statistical/machine learning problem to a sufficiently desirable degree of accuracy.</p>\n<p>The problems we consider come from a variety of disciplines (dynamical systems, imaging, econometric, etc...) but all share the property that in each problem there is some (problem-dependent) structure that may allow to avoid the challenges due to the high-dimensionality of the problem. For example, in the case of learning how to predict the values of a function defined on high-dimensional outputs - a problem were it is well-known that in general one suffers from the statistical challenges mentioned above, in particular that only with an amount of data exponential in the dimension of the input can one learn reasonably accurate predictions - we assume that the function factors as the compositions of functions, where the first function has the special property of lowering the dimension, linearly or nonlinearly. It is then possible to then avoid the \"curse of dimensionality\" mentioned above, and learn good approximations at rates that depend only on the smaller, reduced dimension. This however requires algorithms which are significantly more elaborate, nonlinear, and while many existing ones were computationally unfeasible for high-dimensional data, the ones we develop can be run very efficiently.</p>\n<p>In the context of dynamical systems, we consider both the situation of learning from very highly incomplete observations, in the case of unknown linear dynamics on an unknown&nbsp;network (the linearity of the dynamics is the key ingredients that helps us in this case), and the situation of highly nonlinear dynamics for systems of interacting agents, where what helps is the assumption that the laws of interactions between the agents are relatively simple (e.g. there are only pairwise interactions, and those depend on a few variables, e.g. pairwise distance).</p>\n<p>We also consider the situation of an agent that learns about the surrounding world while trying to solve tasks assigned by a teacher, in increasing order of difficulty, and develop ways in which the agent can develop abstractions that help solve harder and harder problems.</p>\n<p>In all these cases, the particular assumption on the problem we consider - compositional form in the case of learning functions, linearity or interaction laws in the case of learning dynamical systems, abstraction levels and the guide of a teacher in the case of reinforcement learning, are exploited to obtain algorithms that avoid the \"curses of dimensionality\", statistical and computational.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/30/2023<br>\n\t\t\t\t\tModified by: Mauro&nbsp;Maggioni</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWe developed novel techniques for a variety of problems in statistics and machine learning, that are challenging due to the high-dimensionality of the data involved. These challenges can be of at least two types: (a) statistical, for example implying that a very large amount of data would be needed to make any significant progress in accuracy of a statistical estimator or a machine-learned approximation, or (b) computational, implying that only with large computational power it would be possible to solve a statistical/machine learning problem to a sufficiently desirable degree of accuracy.\n\nThe problems we consider come from a variety of disciplines (dynamical systems, imaging, econometric, etc...) but all share the property that in each problem there is some (problem-dependent) structure that may allow to avoid the challenges due to the high-dimensionality of the problem. For example, in the case of learning how to predict the values of a function defined on high-dimensional outputs - a problem were it is well-known that in general one suffers from the statistical challenges mentioned above, in particular that only with an amount of data exponential in the dimension of the input can one learn reasonably accurate predictions - we assume that the function factors as the compositions of functions, where the first function has the special property of lowering the dimension, linearly or nonlinearly. It is then possible to then avoid the \"curse of dimensionality\" mentioned above, and learn good approximations at rates that depend only on the smaller, reduced dimension. This however requires algorithms which are significantly more elaborate, nonlinear, and while many existing ones were computationally unfeasible for high-dimensional data, the ones we develop can be run very efficiently.\n\nIn the context of dynamical systems, we consider both the situation of learning from very highly incomplete observations, in the case of unknown linear dynamics on an unknown network (the linearity of the dynamics is the key ingredients that helps us in this case), and the situation of highly nonlinear dynamics for systems of interacting agents, where what helps is the assumption that the laws of interactions between the agents are relatively simple (e.g. there are only pairwise interactions, and those depend on a few variables, e.g. pairwise distance).\n\nWe also consider the situation of an agent that learns about the surrounding world while trying to solve tasks assigned by a teacher, in increasing order of difficulty, and develop ways in which the agent can develop abstractions that help solve harder and harder problems.\n\nIn all these cases, the particular assumption on the problem we consider - compositional form in the case of learning functions, linearity or interaction laws in the case of learning dynamical systems, abstraction levels and the guide of a teacher in the case of reinforcement learning, are exploited to obtain algorithms that avoid the \"curses of dimensionality\", statistical and computational.\n\n\t\t\t\t\tLast Modified: 04/30/2023\n\n\t\t\t\t\tSubmitted by: Mauro Maggioni"
 }
}