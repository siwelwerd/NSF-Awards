{
 "awd_id": "1825709",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Understanding Pedestrian Dynamics for Seamless Human-Robot Interaction",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922633",
 "po_email": "aleoness@nsf.gov",
 "po_sign_block_name": "Alex Leonessa",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 358535.0,
 "awd_amount": 358535.0,
 "awd_min_amd_letter_date": "2018-08-22",
 "awd_max_amd_letter_date": "2018-08-22",
 "awd_abstract_narration": "In the near future, robots will navigate alongside people in busy, crowded and unconstrained environments such as shopping malls, airports and elder-care facilities. Current mobile robot motion planners are inadequate because current models of pedestrian dynamics do not fully capture the complexity of human motion behavior in crowds. As a result, robots can \"freeze\" in places where crowd density is high, which further impedes pedestrian traffic flow. This project will use novel machine learning techniques to develop a robot motion planner with human-like navigations features. The project team will use objective and subjective performance measures to evaluate the predictability and acceptability of human-robot interactions with robots and their novel control algorithms deployed in shopping malls and campus buildings. This project serves the national interest because the resulting pedestrian dynamics modeling methods and robot controllers may result in public safety applications such as emergency evacuations and crowd planning/management. The project will involve an educational component that provides engineering and research methods training to graduate and undergraduate students, as well as STEM outreach to high-school and middle-school students. Additional efforts will be made to attract and retain women into careers in science and engineering. \r\n\r\nThis research investigates new control methodologies that promise improved pedestrian/mobile-robot navigation through crowded and unconstrained environments. Methods include the extraction of features related to pedestrian behavior from existing datasets and their use in training a deep neural network (DNN) to model pedestrian dynamics; the use of inverse reinforcement learning to generate a cost map that humans follow to navigate; the implementation of the map within a robot motion controller; and the experimental testing and validation of the controller in real-world human environments using objective and subjective performance criteria.  The experimental and evaluation data will be made available for use by the research community.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yi",
   "pi_last_name": "Guo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yi Guo",
   "pi_email_addr": "yguo1@stevens.edu",
   "nsf_id": "000068724",
   "pi_start_date": "2018-08-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stevens Institute of Technology",
  "inst_street_address": "ONE CASTLE POINT ON HUDSON",
  "inst_street_address_2": "",
  "inst_city_name": "HOBOKEN",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "2012168762",
  "inst_zip_code": "070305906",
  "inst_country_name": "United States",
  "cong_dist_code": "08",
  "st_cong_dist_code": "NJ08",
  "org_lgl_bus_name": "THE TRUSTEES OF THE STEVENS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJ6CN5Y5A2R5"
 },
 "perf_inst": {
  "perf_inst_name": "Stevens Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "070305991",
  "perf_ctry_code": "US",
  "perf_cong_dist": "08",
  "perf_st_cong_dist": "NJ08",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "004Y00",
   "pgm_ele_name": "Science of Learning"
  },
  {
   "pgm_ele_code": "058Y00",
   "pgm_ele_name": "M3X - Mind, Machine, and Motor"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "059Z",
   "pgm_ref_txt": "Science of Learning"
  },
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "070E",
   "pgm_ref_txt": "INTEG OF HUMAN & COGNITIVE"
  },
  {
   "pgm_ref_code": "6856",
   "pgm_ref_txt": "ARTIFICIAL INTELL & COGNIT SCI"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 358535.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The past few years have witnessed rapid progresses on robotic technologies in real-world applications such as autonomous delivery robots and tele-presence robots. For mobile robots navigating alongside people, seamless human-robot interaction (HRI) is necessary. This project investigates non-communicating interaction between robots and pedestrians, and focuses on developing HRI grounded in human-human interaction for common ground and mutual understanding.&nbsp;&nbsp;Existing model-based methods on pedestrian dynamics cannot fully capture the complexity of human navigation in crowds, and when used in the HRI study, may freeze the robot once the environment surpasses a certain level of complexity.</p>\n<p>The objective of this project is to understand pedestrian dynamics through learning-based approaches, and design robot motion planner to navigate seamlessly in human-centered environments. Inspired by the remarkable success of deep learning in various areas, two main questions were studied in the project: 1) whether we can learn from a large amount of real-world pedestrian trajectories for a better understanding of human motion dynamics, and 2) whether a mobile robot can learn to navigate like a human in human environments.&nbsp;</p>\n<p>Towards the goals, a deep inverse reinforcement learning approach was first developed to learn how pedestrians navigate from an expert dataset with real-world pedestrian trajectories collected in indoor shopping-mall environments. The developed algorithm took feature inputs including social affinity map that were extracted from human motion trajectories. Evaluation results showed that the proposed method has acceptable prediction accuracy compared to other state-of-the-art methods, and it can generate pedestrian trajectories similar to real human trajectories with natural social navigation behaviors such as collision avoidance, leader-follower, and split-and-rejoin.</p>\n<p>Furthermore, to control the robot to navigate in crowed human environments, a generative adversarial imitation learning (GAIL) method was developed to learn from human navigation behaviors directly and extract control policy in a model free formulation. A large open human trajectory dataset was used to generate human demonstration data in a 3D robotic simulator. The GAIL-based robot control algorithm uses Lidar generated occupancy maps as the input, and outputs the control policy for robot navigation. Simulation experiments were conducted, and performance evaluation was documented on both quantitative and qualitative metrics. Compared with existing works using social force models to generate human demonstration trajectories, learning-based methods learn from intrinsic human trajectories, thus exhibit more human-like navigation behaviors. Casting the robot motion planning problem in the imitation learning framework and adopting the GAIL-based method have the advantage of end-to-end learning without explicitly specifying features and hand coding feature extraction.&nbsp;</p>\n<p>The project has provided system and algorithmic support for robot navigation in human environments. The learning-based pedestrian dynamics modeling can be used to simulate crowd dynamics for crowd control and management applications. The robot navigation algorithms can be integrated on any mobile robot platforms with onboard Lidar sensors. The project has provided education and training to undergraduate and graduate students at Stevens. The research results have also enriched robotics courses at Stevens. Research results have been published in referred journals and international conferences and disseminated to the community.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/20/2023<br>\n\t\t\t\t\tModified by: Yi&nbsp;Guo</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1825709/1825709_10574964_1697472435784_fig-overall--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1825709/1825709_10574964_1697472435784_fig-overall--rgov-800width.jpg\" title=\"Robot navigation in human-centered environments.\"><img src=\"/por/images/Reports/POR/2023/1825709/1825709_10574964_1697472435784_fig-overall--rgov-66x44.jpg\" alt=\"Robot navigation in human-centered environments.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Demonstration of robot navigation in human-centered environments created in V-Rep simulator. Imitation learning methods were developed to learn from human navigation behaviors using real-world human motion trajectories in crowed spaces.</div>\n<div class=\"imageCredit\">Muhammad Fahad and Yi Guo</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yi&nbsp;Guo</div>\n<div class=\"imageTitle\">Robot navigation in human-centered environments.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1825709/1825709_10574964_1697472640909_fig-robot-traj--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1825709/1825709_10574964_1697472640909_fig-robot-traj--rgov-800width.jpg\" title=\"Robot navigation trajectory in crowed space.\"><img src=\"/por/images/Reports/POR/2023/1825709/1825709_10574964_1697472640909_fig-robot-traj--rgov-66x44.jpg\" alt=\"Robot navigation trajectory in crowed space.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Robot trajectory (in red) and pedestrian trajectory (in blue). The circles on the trajectory were drawn every second.</div>\n<div class=\"imageCredit\">Guang Yan and Yi Guo</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yi&nbsp;Guo</div>\n<div class=\"imageTitle\">Robot navigation trajectory in crowed space.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe past few years have witnessed rapid progresses on robotic technologies in real-world applications such as autonomous delivery robots and tele-presence robots. For mobile robots navigating alongside people, seamless human-robot interaction (HRI) is necessary. This project investigates non-communicating interaction between robots and pedestrians, and focuses on developing HRI grounded in human-human interaction for common ground and mutual understanding.  Existing model-based methods on pedestrian dynamics cannot fully capture the complexity of human navigation in crowds, and when used in the HRI study, may freeze the robot once the environment surpasses a certain level of complexity.\n\nThe objective of this project is to understand pedestrian dynamics through learning-based approaches, and design robot motion planner to navigate seamlessly in human-centered environments. Inspired by the remarkable success of deep learning in various areas, two main questions were studied in the project: 1) whether we can learn from a large amount of real-world pedestrian trajectories for a better understanding of human motion dynamics, and 2) whether a mobile robot can learn to navigate like a human in human environments. \n\nTowards the goals, a deep inverse reinforcement learning approach was first developed to learn how pedestrians navigate from an expert dataset with real-world pedestrian trajectories collected in indoor shopping-mall environments. The developed algorithm took feature inputs including social affinity map that were extracted from human motion trajectories. Evaluation results showed that the proposed method has acceptable prediction accuracy compared to other state-of-the-art methods, and it can generate pedestrian trajectories similar to real human trajectories with natural social navigation behaviors such as collision avoidance, leader-follower, and split-and-rejoin.\n\nFurthermore, to control the robot to navigate in crowed human environments, a generative adversarial imitation learning (GAIL) method was developed to learn from human navigation behaviors directly and extract control policy in a model free formulation. A large open human trajectory dataset was used to generate human demonstration data in a 3D robotic simulator. The GAIL-based robot control algorithm uses Lidar generated occupancy maps as the input, and outputs the control policy for robot navigation. Simulation experiments were conducted, and performance evaluation was documented on both quantitative and qualitative metrics. Compared with existing works using social force models to generate human demonstration trajectories, learning-based methods learn from intrinsic human trajectories, thus exhibit more human-like navigation behaviors. Casting the robot motion planning problem in the imitation learning framework and adopting the GAIL-based method have the advantage of end-to-end learning without explicitly specifying features and hand coding feature extraction. \n\nThe project has provided system and algorithmic support for robot navigation in human environments. The learning-based pedestrian dynamics modeling can be used to simulate crowd dynamics for crowd control and management applications. The robot navigation algorithms can be integrated on any mobile robot platforms with onboard Lidar sensors. The project has provided education and training to undergraduate and graduate students at Stevens. The research results have also enriched robotics courses at Stevens. Research results have been published in referred journals and international conferences and disseminated to the community.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/20/2023\n\n\t\t\t\t\tSubmitted by: Yi Guo"
 }
}