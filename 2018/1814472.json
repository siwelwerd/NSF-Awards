{
 "awd_id": "1814472",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Multimodal Conversational Assistant that Learns from Demonstrations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 499019.0,
 "awd_amount": 531019.0,
 "awd_min_amd_letter_date": "2018-08-08",
 "awd_max_amd_letter_date": "2020-04-09",
 "awd_abstract_narration": "Intelligent assistants such as Apple's Siri, Amazon's Alexa and Microsoft's Cortana are rapidly gaining popularity by providing a conversational natural language interface for users to access various online services and digital content.  They allow computing tasks to be performed in contexts where users cannot touch their phones (such as while driving), and on wearable and Internet of Things (IoT) devices (such as Google Home).  However, such conversational interfaces are limited in their ability to handle the \"long-tail\" of tasks and suffer from lack of customizability.  This research will explore a new multi-modal, interactive, programming-by-demonstration (PBD) approach that enables end users to add new capabilities to an intelligent assistant by programming automation scripts for tasks in any existing third-party Android mobile app using a combination of demonstrations and verbal instructions.  The system will leverage state-of-the-art machine learning and natural language processing techniques to comprehend the user's verbal instructions that supply information missing in the demonstration, such as implicit conditions, user intent and personal preferences.  The user's demonstration on the graphical user interface will be used for grounding the conversation and reinforcing the natural language understanding model.  The system will point the way to allowing the general public to more effectively use their smartphones, IoT devices and intelligent assistants, increasing the adoption, efficiency and correctness of uses of these technologies.  The integration of intelligent assistants with PBD will have broad impact by exposing people to programming concepts in an easy-to-learn way, and thereby increasing computational thinking.  \r\n\r\nThis project will result in several innovations beyond the current state of the art through advances in programming by demonstration (PBD) and intelligent assistants, and especially in their integration.  The work will explore leveraging verbal instructions as an additional modality to address long-standing challenges in PBD research including generalizing the data descriptions and adding control structures.  How to coordinate the two modalities to help the intelligent assistant learn new tasks effectively and efficiently from users will be investigated, and how users utilize the two modalities in multi-modal PBD systems for programming tasks in different situations will also be studied.  New ways to leverage the displayed graphical user interfaces (GUI) of apps to enhance the speech recognition and language understanding by using the strings and other context of the GUI on the smartphone will be developed.  The ability of the conversational assistant to participate in this generalization process will be enhanced, with a focus on having the system ask appropriate and helpful questions so the task automation will fit the user's needs and intentions.  New approaches to representing scripts created by PBD systems that users can read, understand and edit will be explored, as will increasing trust and usefulness of the scripts and supporting error handling, debugging and maintenance.  The new technology will also be able to extract data from and enter data into apps, and to learn, through demonstration and verbal instruction, how to transform the data into appropriate formats.  Finally, how to support sharing of scripts created by PBD systems while ensuring the appropriate levels of privacy and security will also be investigated.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brad",
   "pi_last_name": "Myers",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Brad A Myers",
   "pi_email_addr": "bam@cs.cmu.edu",
   "nsf_id": "000360868",
   "pi_start_date": "2018-08-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tom",
   "pi_last_name": "Mitchell",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Tom M Mitchell",
   "pi_email_addr": "Tom.Mitchell@cs.cmu.edu",
   "nsf_id": "000167874",
   "pi_start_date": "2018-08-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499019.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project centered around the design, development, and study of SUGILITE, a new intelligent agent that allows end users to teach new tasks and concepts in a natural way. Using SUGILITE, end users of smartphones without programing expertise can ?teach? an intelligent agent the procedures of performing new tasks (e.g., ordering coffee) in any third-party mobile app, the relevant concepts (e.g., the size of coffee), and conditions that determine how tasks should be performed. This project introduces a new human-AI interaction paradigm for interactive task learning, where the existing third-party app GUIs are used as a medium for users to communicate their intents with an AI agent in addition to being the interface for interacting with the underlying computing services.</p>\n<p>&nbsp;</p>\n<p>Through the design and development of the integrated SUGILITE system, this projects presents seven main technical contributions including: (i) a new approach to allow the agent to generalize from learned task procedures by inferring task parameters and their associated possible values from verbal instructions and mobile app GUIs, (ii) a new method to address the data description problem in PBD by allowing users to verbally explain ambiguous or vague demonstrated actions, (iii) a new multi-modal interface to enable users to teach the conceptual knowledge used in conditionals to the agent, (iv) a new mechanism to extend mobile app based PBD to smart home and Internet of Things (IoT) automation, (v) a new multi-modal interface that helps users discover, identify the causes of, and recover from conversational breakdowns using existing mobile app GUIs for grounding, (vi) a new privacy-preserving approach that can identify and obfuscate the potential personal information in GUI-based PBD scripts based on the uniqueness of information entries with respect to the corresponding app GUI context, and (vii) a new technique for generating semantic representations of GUI screens and components without requiring manual annotation.</p>\n<p>&nbsp;</p>\n<p>The usability, usefulness, and effectiveness of SUGILITE was evaluated in a series of lab usability evaluations and a small-scale week-long field deployment. Findings from this project have been disseminated in more than 10 conference, journal, and book chapter publications (including 4 that won paper awards) and more than 10 invited seminars and keynote speeches at academic institutions (e.g., Stanford University, MIT), companies (e.g., Google, Apple, Microsoft Research, IBM), and conferences (e.g., IUI, ACL). Results from this project have inspired, motivated, and enabled many follow-up research on interactive task learning, interface semantics mining, and human-AI collaboration across multiple computer science disciplines including Human-Computer Interaction, Machine Learning, Natural Language Processing, and Robotics. This project provided training for one Ph.D. graduate, one Master?s graduate, and 12 undergraduate research assistants.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/19/2021<br>\n\t\t\t\t\tModified by: Brad&nbsp;A&nbsp;Myers</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637359293764_Sugilitemainscreens--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637359293764_Sugilitemainscreens--rgov-800width.jpg\" title=\"Screens showing how Sugilite records actions as users demonstrate them\"><img src=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637359293764_Sugilitemainscreens--rgov-66x44.jpg\" alt=\"Screens showing how Sugilite records actions as users demonstrate them\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Screenshots of Sugilite: (a) the conversational interface; (b) the recording confirmation popup; (c) the recording disambiguation/operation editing panel and (d) the viewing/editing script window.</div>\n<div class=\"imageCredit\">Toby Li, Amos Azaria, and Brad Myers. \"SUGILITE: Creating Multimodal Smartphone Automation by Demonstration\", Proceedings CHI'2017, Denver, CO, May 6-11, 2017. pp. 6038-6049.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Brad&nbsp;A&nbsp;Myers</div>\n<div class=\"imageTitle\">Screens showing how Sugilite records actions as users demonstrate them</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637359037745_Li2021_Chapter_DemonstrationNaturalLanguageMua--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637359037745_Li2021_Chapter_DemonstrationNaturalLanguageMua--rgov-800width.jpg\" title=\"Screens showing how Sugilite can interactively disambiguate the scripts\"><img src=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637359037745_Li2021_Chapter_DemonstrationNaturalLanguageMua--rgov-66x44.jpg\" alt=\"Screens showing how Sugilite can interactively disambiguate the scripts\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The screenshots of the demonstration mechanism and its multi-modal mixed-initiative intent clarification process for the demonstrated actions in the Appinite part of Sugilite.</div>\n<div class=\"imageCredit\">Toby Jia-Jun Li, Tom M. Mitchell and Brad A. Myers, \"Demonstration + Natural Language: Multimodal Interfaces for GUI-based Interactive Task Learning Agents\", Artificial Intelligence for Human Computer Interaction: A Modern Approach. Springer. 2021, pp. 495-537.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Brad&nbsp;A&nbsp;Myers</div>\n<div class=\"imageTitle\">Screens showing how Sugilite can interactively disambiguate the scripts</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637359222363_multi-modalerrorrepair--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637359222363_multi-modalerrorrepair--rgov-800width.jpg\" title=\"Screens showing how users can fix errors in Sugilite\ufffds understanding of their actions\"><img src=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637359222363_multi-modalerrorrepair--rgov-66x44.jpg\" alt=\"Screens showing how users can fix errors in Sugilite\ufffds understanding of their actions\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The interface of the Sovite part of Sugilite: (a) Sovite shows a GUI screenshot to communicate its state of understanding. (b) To fix intent detection errors, the user can refer to an app that represents their desired task. (c) If the intent is still ambiguous, the user can show a specific screen.</div>\n<div class=\"imageCredit\">Toby Jia-Jun Li, Jingya Chen, Haijun Xia, Tom Mitchell, Brad Myers. \"Multi-Modal Repairs of Conversational Breakdowns in Task-Oriented Dialogs\", ACM UIST'20. Pages 1094-1107</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Brad&nbsp;A&nbsp;Myers</div>\n<div class=\"imageTitle\">Screens showing how users can fix errors in Sugilite\ufffds understanding of their actions</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637358869345_Epidositedevicecontrol--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637358869345_Epidositedevicecontrol--rgov-800width.jpg\" title=\"Screens showing how Sugilite allows users to control IoT devices\"><img src=\"/por/images/Reports/POR/2021/1814472/1814472_10567891_1637358869345_Epidositedevicecontrol--rgov-66x44.jpg\" alt=\"Screens showing how Sugilite allows users to control IoT devices\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Screenshots of the script view and trigger creation interfaces for the EPIDOSITE part of Sugilite: (a) the script view showing the script from the example usage scenario; (b) the window for creating an app notification trigger; (c) the window for creating an app launch trigger.</div>\n<div class=\"imageCredit\">Toby Jia-Jun Li, Yuanchun Li, Fanglin Chen and Brad A. Myers. \"Programming IoT Devices by Demonstration on Mobile Apps\", IS-EUD 2017. Springer, Cham, LNCS 10303. pp. 3-17.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Brad&nbsp;A&nbsp;Myers</div>\n<div class=\"imageTitle\">Screens showing how Sugilite allows users to control IoT devices</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe project centered around the design, development, and study of SUGILITE, a new intelligent agent that allows end users to teach new tasks and concepts in a natural way. Using SUGILITE, end users of smartphones without programing expertise can ?teach? an intelligent agent the procedures of performing new tasks (e.g., ordering coffee) in any third-party mobile app, the relevant concepts (e.g., the size of coffee), and conditions that determine how tasks should be performed. This project introduces a new human-AI interaction paradigm for interactive task learning, where the existing third-party app GUIs are used as a medium for users to communicate their intents with an AI agent in addition to being the interface for interacting with the underlying computing services.\n\n \n\nThrough the design and development of the integrated SUGILITE system, this projects presents seven main technical contributions including: (i) a new approach to allow the agent to generalize from learned task procedures by inferring task parameters and their associated possible values from verbal instructions and mobile app GUIs, (ii) a new method to address the data description problem in PBD by allowing users to verbally explain ambiguous or vague demonstrated actions, (iii) a new multi-modal interface to enable users to teach the conceptual knowledge used in conditionals to the agent, (iv) a new mechanism to extend mobile app based PBD to smart home and Internet of Things (IoT) automation, (v) a new multi-modal interface that helps users discover, identify the causes of, and recover from conversational breakdowns using existing mobile app GUIs for grounding, (vi) a new privacy-preserving approach that can identify and obfuscate the potential personal information in GUI-based PBD scripts based on the uniqueness of information entries with respect to the corresponding app GUI context, and (vii) a new technique for generating semantic representations of GUI screens and components without requiring manual annotation.\n\n \n\nThe usability, usefulness, and effectiveness of SUGILITE was evaluated in a series of lab usability evaluations and a small-scale week-long field deployment. Findings from this project have been disseminated in more than 10 conference, journal, and book chapter publications (including 4 that won paper awards) and more than 10 invited seminars and keynote speeches at academic institutions (e.g., Stanford University, MIT), companies (e.g., Google, Apple, Microsoft Research, IBM), and conferences (e.g., IUI, ACL). Results from this project have inspired, motivated, and enabled many follow-up research on interactive task learning, interface semantics mining, and human-AI collaboration across multiple computer science disciplines including Human-Computer Interaction, Machine Learning, Natural Language Processing, and Robotics. This project provided training for one Ph.D. graduate, one Master?s graduate, and 12 undergraduate research assistants.\n\n \n\n\t\t\t\t\tLast Modified: 11/19/2021\n\n\t\t\t\t\tSubmitted by: Brad A Myers"
 }
}