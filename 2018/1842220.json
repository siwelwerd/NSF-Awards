{
 "awd_id": "1842220",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Computer-Assisted Video Analysis Methods for Understanding Underrepresented Student Participation and Learning in Collaborative Learning Environments",
 "cfda_num": "47.076",
 "org_code": "11090000",
 "po_phone": "7032927593",
 "po_email": "wuhe@nsf.gov",
 "po_sign_block_name": "Wu He",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 299999.0,
 "awd_amount": 299999.0,
 "awd_min_amd_letter_date": "2018-08-07",
 "awd_max_amd_letter_date": "2018-08-07",
 "awd_abstract_narration": "Research that seeks to understand classroom interactions often relies on video recordings of classrooms so that researchers can document and analyze what teachers and students are doing in the learning environment. When studies are large scale, this analysis is challenging in part because it is time-consuming to review and code large quantities of video. For example, hundreds of hours of videotaped interaction between students working in an after-school program for advancing computational thinking and engineering learning for Latino/a students. This project is exploring the use of computer-assisted methods for video analysis to support manual coding by researchers. The project is adapting procedures used for computer-aided diagnosis systems for medical systems. The computer-assisted process creates summaries that can then be used by researchers to identify critical events and to describe patterns of activities in the classroom such as students talking to each other or writing during a small group project. Creating the summaries requires analyzing video for facial recognition, motion, color and object identification. The project will investigate what parts of student participation and teaching can be analyzed using computer-assisted video analysis. This project is supported by NSF's EHR Core Research (ECR) program, the STEM+C program and the AISL program. The ECR program emphasizes fundamental STEM education research that generates foundational knowledge in the field.  The project is funded by the STEM+Computing program, which seeks to address emerging challenges in computational STEM areas through the applied integration of computational thinking and computing activities within disciplinary STEM teaching and learning in early childhood education through high school (preK-12). As part of its overall strategy to enhance learning in informal environments, the Advancing Informal STEM Learning (AISL) program seeks to advance new approaches to, and evidence-based understanding of, the design and development of STEM learning in informal environments. This includes providing multiple pathways for broadening access to and engagement in STEM learning experiences, advancing innovative research on and assessment of STEM learning in informal environments, and developing understandings of deeper learning by participants.\r\n\r\nThe video analysis systems will provide video summarizations for specific activities which will allow researchers to use these results to quantify student participation and document teaching practices that support student learning. This will support the analysis of large volumes of video data that are often time-consuming to analyze. The video analysis system will identify objects in the scene and then use measures of distances between objects and other tracking methods to code different activities (e.g., typing, talking, interaction between the student and a facilitator).  The two groups of research questions are as follows. (1) How can human review of digital videos benefit from computer-assisted video analysis methods? Which aspects of video summarization (e.g., detected activities) can help reduce the time it takes to review the videos? Beyond audio analytics, what types of future research in video summarization can help reduce the time that it takes to review videos? (2) How can we quantify student participation using computer-assisted video analysis methods? What aspects of student participation can be accurately measures by computer-assisted video analysis methods? The video to be used for this study is drawn from a project focused on engineering and computational thinking learning for Latino/a students in an after-school setting. Hundreds of hours of video are available to be reviewed and analyzed to design and refine the system. The resulting coding will also help document patterns of engagement in the learning environment.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DRL",
 "org_div_long_name": "Division of Research on Learning in Formal and Informal Settings (DRL)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marios",
   "pi_last_name": "Pattichis",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Marios S Pattichis",
   "pi_email_addr": "pattichi@unm.edu",
   "nsf_id": "000475187",
   "pi_start_date": "2018-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sylvia",
   "pi_last_name": "Celedon-Pattichis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sylvia Celedon-Pattichis",
   "pi_email_addr": "sylvia.celedon@austin.utexas.edu",
   "nsf_id": "000452073",
   "pi_start_date": "2018-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Carlos",
   "pi_last_name": "LopezLeiva",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Carlos A LopezLeiva",
   "pi_email_addr": "callopez@unm.edu",
   "nsf_id": "000607341",
   "pi_start_date": "2018-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of New Mexico",
  "inst_street_address": "1 UNIVERSITY OF NEW MEXICO",
  "inst_street_address_2": "",
  "inst_city_name": "ALBUQUERQUE",
  "inst_state_code": "NM",
  "inst_state_name": "New Mexico",
  "inst_phone_num": "5052774186",
  "inst_zip_code": "871310001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NM01",
  "org_lgl_bus_name": "UNIVERSITY OF NEW MEXICO",
  "org_prnt_uei_num": "",
  "org_uei_num": "F6XLTRUQJEN4"
 },
 "perf_inst": {
  "perf_inst_name": "University of New Mexico",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NM",
  "perf_st_name": "New Mexico",
  "perf_zip_code": "871310001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NM01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "005Y00",
   "pgm_ele_name": "STEM + Computing (STEM+C) Part"
  },
  {
   "pgm_ele_code": "725900",
   "pgm_ele_name": "AISL"
  },
  {
   "pgm_ele_code": "798000",
   "pgm_ele_name": "ECR-EDU Core Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7259",
   "pgm_ref_txt": "INFORMAL SCIENCE EDUCATION"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8244",
   "pgm_ref_txt": "EHR CL Opportunities (NSF 14-302)"
  },
  {
   "pgm_ref_code": "8817",
   "pgm_ref_txt": "STEM Learning & Learning Environments"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0418",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001819DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 299999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of the project was aimed at the development of computer-assisted video analysis methods for understanding student participation and learning in collaborative learning environments. The underrepresented populations consisted primarily of LatinX students who were interacting in both Spanish and English.</p>\n<p>An important innovation of the project was to recognize bilingualism as an asset that needed to be used in AI systems. This led to significant efforts to focus on detecting when the students were talking and what they were saying in both Spanish and English. Yet, modern speech recognition methods required training over large datasets, which was not possible with limited resources. Furthermore, the employment of state-of-the-art systems for face recognition proved challenging due to the well-documented limitations of face recognition systems in that they give the worst recognition rates when it comes to underrepresented students of color. Additionally, a greater challenge came from the need to train and test on large-scale video datasets that required the need to integrate information over sessions that lasted an hour to an hour and a half.</p>\n<p>Firstly, the project had to address the need to train over large video datasets. This was addressed through the development of fast convolution methods that allowed the use of large convolution kernels for arbitrary image sizes. The new approach significantly outperforms Tensorflow for kernels of sizes 5x5 and 11x11. As a result, the new convolution methods enabled the development of multiple representations for person detection.</p>\n<p>Secondly, there was a need to develop effective video face recognition methods for students from underrepresented groups. The problem was addressed through the use of a multi-objective optimization approach. Here, we simultaneously optimized for a reduced number of face prototypes and the best possible recognition accuracy. The approach allowed us to develop an effective method for recognizing the faces of students from underrepresented groups by selecting a sufficient number of student prototypes from the training videos. For recognizing students from underrepresented groups, the developed video face recognition system was shown to be significantly more accurate and faster than current state-of-the-art systems.</p>\n<p>Over each detected face, we developed methods for detecting talking activities. Furthermore, to quantify student participation in coding and written exercises, we developed an effective system for hand detection. Over each hand region, we applied a low-parameter convolutional neural network to recognize writing and typing activities. For recognizing typing and writing in collaborative learning groups, the proposed system outperformed the state-of-the-art system while using 1000 times fewer parameters.</p>\n<p>Overall, to facilitate quantification of student participation, we summarize video activities in participation maps, which are used for summarizing talking, writing, and typing activities. Participation maps consist of rows of activities for each student participant. Along each row, a bar signifies the start and end of the activity. Then, by clicking on each bar, the research is taken directly to the video of the activity.</p>\n<p>Building on our video analysis methods, we also constructed methods for speaker identification and bilingual speech recognition. Our approach uses video information to reconstruct an approximation of the 3D speaker geometry. Based on the 3D speaker geometry, we use the correlation patterns of a virtual microphone array to recognize the speakers. For detecting student speakers in collaborative learning environments, our method based on virtual microphones performed significantly better than Google Speech-to-text or Amazon AWS. Furthermore, to address the problem of lacking ground truth on large audio datasets, we simulated student sessions based on transcriptions and the 3D speaker geometry. After training on the simulated dataset, our phoneme-based bilingual audio recognizer performed slightly better on Google Speech-to-text on actual classroom audios.</p>\n<p>The broader impact of the computer-aided methods is to allow for quantifying the participation of students over hundreds of hours of educational videos. The developed methods will identify the specific moments where a student is talking, writing, or typing. Furthermore, the current system can also detect specific keywords in Spanish or English. The methods can also provide total participation time in each activity. Building on these methods, it is possible to construct more complex queries about student participation. For example, student interactions can be characterized by alternating talking activities. The proposed methods can support educational researchers to track student participation through multiple sessions, without the need to manually review each video segment.</p>\n<p>The developed methods also had a strong impact on the graduate program in Electrical and Computer Engineering at UNM. Four graduate students completed their Masters Theses on the development of audio and video analysis methods. Another three Ph.D. students are in the process of completing their dissertations on student participation maps for summarizing the activities across video sessions. The developed video analysis has been used in three graduate courses in Digital Image Processing, Computer Vision, and Numerical Optimization. More generally, the methods are also applicable for accurate video activity recognition in large-scale video datasets.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/22/2021<br>\n\t\t\t\t\tModified by: Marios&nbsp;S&nbsp;Pattichis</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1842220/1842220_10567087_1640161285366_Face-recognition--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1842220/1842220_10567087_1640161285366_Face-recognition--rgov-800width.jpg\" title=\"Face recognition example\"><img src=\"/por/images/Reports/POR/2021/1842220/1842220_10567087_1640161285366_Face-recognition--rgov-66x44.jpg\" alt=\"Face recognition example\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Video face recognition and student de-identification example.</div>\n<div class=\"imageCredit\">Phuong Tran</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Marios&nbsp;S&nbsp;Pattichis</div>\n<div class=\"imageTitle\">Face recognition example</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1842220/1842220_10567087_1640161398919_Talking-Participation--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1842220/1842220_10567087_1640161398919_Talking-Participation--rgov-800width.jpg\" title=\"Talking Participation\"><img src=\"/por/images/Reports/POR/2021/1842220/1842220_10567087_1640161398919_Talking-Participation--rgov-66x44.jpg\" alt=\"Talking Participation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Talking Participation Map. The participation map summarizes talking activity recognition results. The horizontal axis represents time. Each row represents a different student.</div>\n<div class=\"imageCredit\">Wenjing Shi</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Marios&nbsp;S&nbsp;Pattichis</div>\n<div class=\"imageTitle\">Talking Participation</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1842220/1842220_10567087_1640161505675_Phonemes-ex--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1842220/1842220_10567087_1640161505675_Phonemes-ex--rgov-800width.jpg\" title=\"Bilingual Speech Recognition\"><img src=\"/por/images/Reports/POR/2021/1842220/1842220_10567087_1640161505675_Phonemes-ex--rgov-66x44.jpg\" alt=\"Bilingual Speech Recognition\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Bilingual Speech Recognition example based on Spanish and English Phonemes.</div>\n<div class=\"imageCredit\">Mario Esparza and Luis Sanchez Tapia</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Marios&nbsp;S&nbsp;Pattichis</div>\n<div class=\"imageTitle\">Bilingual Speech Recognition</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe goal of the project was aimed at the development of computer-assisted video analysis methods for understanding student participation and learning in collaborative learning environments. The underrepresented populations consisted primarily of LatinX students who were interacting in both Spanish and English.\n\nAn important innovation of the project was to recognize bilingualism as an asset that needed to be used in AI systems. This led to significant efforts to focus on detecting when the students were talking and what they were saying in both Spanish and English. Yet, modern speech recognition methods required training over large datasets, which was not possible with limited resources. Furthermore, the employment of state-of-the-art systems for face recognition proved challenging due to the well-documented limitations of face recognition systems in that they give the worst recognition rates when it comes to underrepresented students of color. Additionally, a greater challenge came from the need to train and test on large-scale video datasets that required the need to integrate information over sessions that lasted an hour to an hour and a half.\n\nFirstly, the project had to address the need to train over large video datasets. This was addressed through the development of fast convolution methods that allowed the use of large convolution kernels for arbitrary image sizes. The new approach significantly outperforms Tensorflow for kernels of sizes 5x5 and 11x11. As a result, the new convolution methods enabled the development of multiple representations for person detection.\n\nSecondly, there was a need to develop effective video face recognition methods for students from underrepresented groups. The problem was addressed through the use of a multi-objective optimization approach. Here, we simultaneously optimized for a reduced number of face prototypes and the best possible recognition accuracy. The approach allowed us to develop an effective method for recognizing the faces of students from underrepresented groups by selecting a sufficient number of student prototypes from the training videos. For recognizing students from underrepresented groups, the developed video face recognition system was shown to be significantly more accurate and faster than current state-of-the-art systems.\n\nOver each detected face, we developed methods for detecting talking activities. Furthermore, to quantify student participation in coding and written exercises, we developed an effective system for hand detection. Over each hand region, we applied a low-parameter convolutional neural network to recognize writing and typing activities. For recognizing typing and writing in collaborative learning groups, the proposed system outperformed the state-of-the-art system while using 1000 times fewer parameters.\n\nOverall, to facilitate quantification of student participation, we summarize video activities in participation maps, which are used for summarizing talking, writing, and typing activities. Participation maps consist of rows of activities for each student participant. Along each row, a bar signifies the start and end of the activity. Then, by clicking on each bar, the research is taken directly to the video of the activity.\n\nBuilding on our video analysis methods, we also constructed methods for speaker identification and bilingual speech recognition. Our approach uses video information to reconstruct an approximation of the 3D speaker geometry. Based on the 3D speaker geometry, we use the correlation patterns of a virtual microphone array to recognize the speakers. For detecting student speakers in collaborative learning environments, our method based on virtual microphones performed significantly better than Google Speech-to-text or Amazon AWS. Furthermore, to address the problem of lacking ground truth on large audio datasets, we simulated student sessions based on transcriptions and the 3D speaker geometry. After training on the simulated dataset, our phoneme-based bilingual audio recognizer performed slightly better on Google Speech-to-text on actual classroom audios.\n\nThe broader impact of the computer-aided methods is to allow for quantifying the participation of students over hundreds of hours of educational videos. The developed methods will identify the specific moments where a student is talking, writing, or typing. Furthermore, the current system can also detect specific keywords in Spanish or English. The methods can also provide total participation time in each activity. Building on these methods, it is possible to construct more complex queries about student participation. For example, student interactions can be characterized by alternating talking activities. The proposed methods can support educational researchers to track student participation through multiple sessions, without the need to manually review each video segment.\n\nThe developed methods also had a strong impact on the graduate program in Electrical and Computer Engineering at UNM. Four graduate students completed their Masters Theses on the development of audio and video analysis methods. Another three Ph.D. students are in the process of completing their dissertations on student participation maps for summarizing the activities across video sessions. The developed video analysis has been used in three graduate courses in Digital Image Processing, Computer Vision, and Numerical Optimization. More generally, the methods are also applicable for accurate video activity recognition in large-scale video datasets.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/22/2021\n\n\t\t\t\t\tSubmitted by: Marios S Pattichis"
 }
}