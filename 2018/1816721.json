{
 "awd_id": "1816721",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: SMALL: Methods to Assess Automotive Augmented Reality Head-up Display Effects on Driver Performance",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 499996.0,
 "awd_amount": 499996.0,
 "awd_min_amd_letter_date": "2018-08-13",
 "awd_max_amd_letter_date": "2018-08-13",
 "awd_abstract_narration": "Augmented Reality (AR) windshield displays have the potential to provide fundamentally new driving experiences by displaying computer graphics directly on the roadway, and with the increasing commercialization of AR technologies are likely to become more common in cars. If designed properly, AR has the potential to enhance driving and driver safety.  However, we currently do not know how to evaluate the safety of emerging AR driving interfaces, as existing methods used for evaluating other in-vehicle information displays (such as navigation and entertainment systems) are not rich enough to capture effects of AR displays on driver performance.  This project will develop new methods for assessing drivers' visual distraction when using AR displays.  These methods will be specifically tuned for developing AR user interfaces for driving that reduce injuries and minimize loss of life and property. If successful, this project will help auto manufacturers more quickly create and release new and safe AR driving applications, and may have a positive impact on other areas where AR is used, such as in construction and manufacturing. The project will also support educational impacts, through the training of researchers better able to consider both human and technical factors in system design and evaluation, and through efforts to recruit students who are members of populations traditionally underrepresented in engineering.\r\n\r\nThe project will address open questions that arise from expected increases in the field of view provided by AR heads-up displays (HUDs). This will support a wider variety of possible interfaces and information locations than current displays; however, we currently do not know how to effectively quantify the effects of AR user interfaces on driving.  This project will evaluate glance allocation and visual attention capabilities of drivers with AR HUDs, and apply this knowledge to inform new methods of AR HUD assessment.  Specifically, the team will establish thresholds for AR HUD glance durations along with a set of ecologically valid visual AR HUD tasks that can be broadly adopted by both researchers and designers of automotive AR HUDs.  The team will further create a set of methods to assess the effects of AR HUD visual demand on drivers' ability to detect events occurring in both the drivers' central and peripheral fields of view. Lastly, the research will employ user studies to test the refined methods on an actual roadway and recommend best practices for applying the methods.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Joseph",
   "pi_last_name": "Gabbard",
   "pi_mid_init": "L",
   "pi_sufx_name": "Jr",
   "pi_full_name": "Joseph L Gabbard",
   "pi_email_addr": "jgabbard@vt.edu",
   "nsf_id": "000356087",
   "pi_start_date": "2018-08-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ronald",
   "pi_last_name": "Gibbons",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Ronald B Gibbons",
   "pi_email_addr": "rgibbons@vtti.vt.edu",
   "nsf_id": "000572092",
   "pi_start_date": "2018-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "240610001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "VA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499996.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This work aimed to better understand how we can assess emerging augmented reality (AR) based head-up display (HUD) user interfaces for use in transportation systems &ndash; specifically automobile driving.&nbsp; During the project we conducted a literature review and several human factors studies that leveraged a state-of-the-art driving simulator designed specifically to study AR in driving.&nbsp; These simulator studies, specifically allowed us to : (1) examine thresholds for AR HUD glance durations, where we found that drivers could sustain glances at AR HIDs for very long periods of time &ndash; much longer than for example glancing at an in-vehicle radio or information display.&nbsp; These results suggest that AR HUDs could capture drivers visual attention and provide a false sense of overconfidence in perceived hazards in the roadway, and (2) explore new stimuli for assessing AR HUDs that include central detection tasks (like detecting a vehicle breaking in front of a driver) and<strong> </strong>peripheral detection tasks (like detecting a pedestrian on the side of the road).&nbsp; Lastly, we conducted an on-road study where we combined findings from simulator studies to assess AR HUDs effect on central and peripheral detection of road hazard.&nbsp; Results of our on-road confirm not only that existing methods for assessing in-vehicle displays are not sufficient for AR HUDs, but that the addition of central and peripheral detection tasks may be part of an improved method for assessment.&nbsp; Finally, our work has led to a proposed workshop that will bring together researchers from academia and government sectors as well as industry practitioners to generate a deep discussion around how best to design and evaluate in-vehicle AR HUD interfaces to ensure public safety for drivers and other roadway users.</p>\n<p>&nbsp;</p>\n<p>The results from this work have many implications across society. For example:</p>\n<ul>\n<li>Our work has created several new AR HUD driving testbeds as well as for methods for analyzing AR use in transportation domains.&nbsp; These scientific outcomes, in turn, afford University researchers additional safety-critical research by students in transportation as well as other AR application domains.&nbsp; For example, students in PI Gabbard&rsquo;s lab are using similar analysis techniques and methods to assess the operator visual distraction in AR-enabled human-robot collaboration.</li>\n<li>The AR HUD driving simulations we developed were built and used by undergraduate students as well,&nbsp; allowing these students to get hands-on experience using cutting edge AR technology, while also affording an appreciation of science and engineering in the transportation domain.</li>\n<li>The project supported a graduate student who upon graduation with a PhD degree, will enter the workforce with specialization in augmented reality for transportation, and will contribute to future generations of student and public well-being (with respect to AR usage in genera).</li>\n<li>The AR HUD driving scenarios we investigated are now being used to frame classroom discussions, where students engage in semester-long projects can design AR interfaces to support different driving settings and take into account driver/public safety and health.</li>\n</ul>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/28/2023<br>\n\t\t\t\t\tModified by: Joseph&nbsp;L&nbsp;Gabbard</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis work aimed to better understand how we can assess emerging augmented reality (AR) based head-up display (HUD) user interfaces for use in transportation systems &ndash; specifically automobile driving.  During the project we conducted a literature review and several human factors studies that leveraged a state-of-the-art driving simulator designed specifically to study AR in driving.  These simulator studies, specifically allowed us to : (1) examine thresholds for AR HUD glance durations, where we found that drivers could sustain glances at AR HIDs for very long periods of time &ndash; much longer than for example glancing at an in-vehicle radio or information display.  These results suggest that AR HUDs could capture drivers visual attention and provide a false sense of overconfidence in perceived hazards in the roadway, and (2) explore new stimuli for assessing AR HUDs that include central detection tasks (like detecting a vehicle breaking in front of a driver) and peripheral detection tasks (like detecting a pedestrian on the side of the road).  Lastly, we conducted an on-road study where we combined findings from simulator studies to assess AR HUDs effect on central and peripheral detection of road hazard.  Results of our on-road confirm not only that existing methods for assessing in-vehicle displays are not sufficient for AR HUDs, but that the addition of central and peripheral detection tasks may be part of an improved method for assessment.  Finally, our work has led to a proposed workshop that will bring together researchers from academia and government sectors as well as industry practitioners to generate a deep discussion around how best to design and evaluate in-vehicle AR HUD interfaces to ensure public safety for drivers and other roadway users.\n\n \n\nThe results from this work have many implications across society. For example:\n\nOur work has created several new AR HUD driving testbeds as well as for methods for analyzing AR use in transportation domains.  These scientific outcomes, in turn, afford University researchers additional safety-critical research by students in transportation as well as other AR application domains.  For example, students in PI Gabbard\u2019s lab are using similar analysis techniques and methods to assess the operator visual distraction in AR-enabled human-robot collaboration.\nThe AR HUD driving simulations we developed were built and used by undergraduate students as well,  allowing these students to get hands-on experience using cutting edge AR technology, while also affording an appreciation of science and engineering in the transportation domain.\nThe project supported a graduate student who upon graduation with a PhD degree, will enter the workforce with specialization in augmented reality for transportation, and will contribute to future generations of student and public well-being (with respect to AR usage in genera).\nThe AR HUD driving scenarios we investigated are now being used to frame classroom discussions, where students engage in semester-long projects can design AR interfaces to support different driving settings and take into account driver/public safety and health.\n\n\n \n\n\t\t\t\t\tLast Modified: 06/28/2023\n\n\t\t\t\t\tSubmitted by: Joseph L Gabbard"
 }
}