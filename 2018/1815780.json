{
 "awd_id": "1815780",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: ARTEMIS: Algorithm-Hardware Co-Design for Efficient Machine Learning Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Alexander Jones",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-07-21",
 "awd_max_amd_letter_date": "2021-12-08",
 "awd_abstract_narration": "With the increased popularity of machine learning algorithms deployed on a variety of hardware systems, the problem of identifying the best model among numerous possible configurations has drawn significant attention. The problem is compounded by the need to select the right platform to run these applications, under given power or latency constraints. This \"hardware wall\" forces machine learning service providers to constantly redesign the underlying hardware fabric to satisfy certain constraints. This project develops tools for automatic and efficient co-design of machine learning algorithms and hardware platforms that will result in significant cost and time-to-market reduction for machine learning systems.\r\n\r\nThe project introduces efficient meta-learning for machine learning systems and algorithm-hardware platform co-design. Specifically, the project will develop meta-learning algorithms for the optimization of machine learning models under system hardware constraints and formulate the hardware design of efficient machine learning systems as a machine learning problem itself, that can be effectively solved by meta-learning optimization algorithms. Finally, the project will develop multi-objective algorithms for the co-design of machine learning applications and hardware platforms they need to run on, and exploit domain knowledge from hardware engineering and design schemes to substantially accelerate hardware-aware model optimization.\r\n\r\nThe results of the project seek to change the landscape of modeling, optimization, and design methodologies for efficient machine learning systems. Furthermore, the work aims to have an important educational and mentoring component by potentially changing how engineers are trained in a multidisciplinary fashion for dealing with next generation technological advances in general, and the problem of efficiently and intelligently co-designing machine learning algorithms and the hardware platforms they are running on, in particular. The project will involve a diverse graduate and undergraduate trainee population, while expanding the project's outreach to high-school and middle-school students.\r\n\r\nThe data, code, results, and simulators developed in this project will be made available publicly throughout the duration of the project and for at least four years after the end of the project. The location of the repository is on the website of Carnegie Mellon University's Energy Aware Computing group (www.ece.cmu.edu/~enyac).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Diana",
   "pi_last_name": "Marculescu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Diana Marculescu",
   "pi_email_addr": "dianam@utexas.edu",
   "nsf_id": "000155945",
   "pi_start_date": "2018-07-21",
   "pi_end_date": "2021-11-16"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gauri",
   "pi_last_name": "Joshi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gauri Joshi",
   "pi_email_addr": "gaurij@andrew.cmu.edu",
   "nsf_id": "000732900",
   "pi_start_date": "2021-11-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie Mellon University",
  "perf_str_addr": "5000 Forbes Ave.",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-0b0b4665-7fff-e500-3474-084fce657d93\">&nbsp;</span></p>\n<p dir=\"ltr\"><span>With the increased popularity of machine learning algorithms deployed on a variety of hardware systems, the problem of identifying the best model among numerous possible configurations has drawn significant attention. The problem is computationally difficult and it is compounded by the need to select the right platform to run these applications, under given power or latency constraints. This <em>hardware wall</em> forces machine learning service providers to constantly redesign the underlying hardware fabric to satisfy certain target constraints. This project has developed tools for automatic and efficient co-design of machine learning algorithms </span><span>and</span><span> hardware platforms that result in significant cost and time-to-market reduction for machine learning systems.</span></p>\n<p dir=\"ltr\"><span>To this end, the project has introduced efficient meta-learning for machine learning systems and algorithm-hardware platform co-design. In more detail, the project has developed meta-learning algorithms for the optimization of machine learning models under system </span><span>hardware constraints</span><span> and has formulated the </span><span>hardware design of efficient machine learning systems</span><span> as a machine learning problem itself, which can be effectively solved by generic optimization algorithms. Finally, the project has developed </span><span>multi-objective</span><span> algorithms for the co-design of machine learning applications and hardware platforms they need to run on, and has exploited domain knowledge from </span><span>hardware engineering</span><span> and design schemes to substantially accelerate hardware-aware model optimization.</span></p>\n<p dir=\"ltr\"><span>The results of the project are likely to change the landscape of modeling, optimization, and design methodologies for efficient machine learning systems. A selected set of our technical contributions and outreach activities completed during the duration of this grant is included below:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Single-Path NAS: An efficient hardware-aware neural architecture search framework that is able to find the best machine learning model (with highest accuracy) under given serving latency constraints in hours, rather than days.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Joslim and WidthXfer: A set of efficient neural architecture search frameworks that address the shortcomings that existing approaches still have. To alleviate these, we have improved on top of our previous work Single-Path NAS by proposing width optimization by proxy (WidthXfer) and joint weight/network width optimization (Joslim) to speed up the process of finding a hardware constrained machine learning model.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Deep-NVM and Deep-NVM++: An efficient&nbsp; framework for cross-layer modeling and optimization for GPU-based deep learning systems using nonvolatile storage. Non-volatile memory (NVM) technologies have significant advantages compared to conventional storage due to their non-volatility, higher cell density, and scalability features.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>QUIDAM: A framework for power, performance, and area results not just for a single hardware design point but for a range of different hardware designs. The framework can be used to jointly analyze trade-offs of various architectural design choices and machine learning workloads and achieve better trade-offs between accuracy and hardware-efficiency metrics such as performance per area and energy.</span></p>\n</li>\n</ul>\n<p dir=\"ltr\"><span>Furthermore, the work has had an important educational and mentoring component by impacting how engineers are trained in a multidisciplinary fashion for dealing with next generation technological advances in general, and the problem of efficiently and intelligently co-designing machine learning algorithms and the hardware platforms they are running on, in particular. We have summarized our educational contributions in a publication on the course developed by PI D. Marculescu on Hardware Architectures for Machine Learning. The project has included a diverse graduate and undergraduate trainee population, while expanding the project's outreach to high-school and middle-school students.</span></p>\n<p dir=\"ltr\"><span>The data, code, results, and simulators developed in this project have been made available publicly throughout the duration of the project and will be available at least for the next four years. The location of the repository is https://github.com/enyac-group.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/12/2023<br>\n\t\t\t\t\tModified by: Gauri&nbsp;Joshi</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219700970_fig1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219700970_fig1--rgov-800width.jpg\" title=\"Fig 1\"><img src=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219700970_fig1--rgov-66x44.jpg\" alt=\"Fig 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">SinglePath NAS framework</div>\n<div class=\"imageCredit\">Dimitrios Stamoulis</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Gauri&nbsp;Joshi</div>\n<div class=\"imageTitle\">Fig 1</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219744129_fig2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219744129_fig2--rgov-800width.jpg\" title=\"Fig 2\"><img src=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219744129_fig2--rgov-66x44.jpg\" alt=\"Fig 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">SinglePath NAS results</div>\n<div class=\"imageCredit\">Dimitrios Stamoulis</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Gauri&nbsp;Joshi</div>\n<div class=\"imageTitle\">Fig 2</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219815417_fig3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219815417_fig3--rgov-800width.jpg\" title=\"Fig 3\"><img src=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219815417_fig3--rgov-66x44.jpg\" alt=\"Fig 3\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">WidthXfer steps</div>\n<div class=\"imageCredit\">Ting-Wu (Rudy) Chin</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Gauri&nbsp;Joshi</div>\n<div class=\"imageTitle\">Fig 3</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219999203_fig4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219999203_fig4--rgov-800width.jpg\" title=\"Fig 4\"><img src=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676219999203_fig4--rgov-66x44.jpg\" alt=\"Fig 4\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">WidthXfer framework</div>\n<div class=\"imageCredit\">Ting-Wu (Rudy) Chin</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Gauri&nbsp;Joshi</div>\n<div class=\"imageTitle\">Fig 4</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676220053946_fig5--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676220053946_fig5--rgov-800width.jpg\" title=\"Fig 5\"><img src=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676220053946_fig5--rgov-66x44.jpg\" alt=\"Fig 5\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">DeepNVM++ framework</div>\n<div class=\"imageCredit\">Ahmet Inci</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Gauri&nbsp;Joshi</div>\n<div class=\"imageTitle\">Fig 5</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676220160995_fig6--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676220160995_fig6--rgov-800width.jpg\" title=\"Fig 6\"><img src=\"/por/images/Reports/POR/2023/1815780/1815780_10560078_1676220160995_fig6--rgov-66x44.jpg\" alt=\"Fig 6\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">QUIDAM framework</div>\n<div class=\"imageCredit\">Ahmet Inci</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Gauri&nbsp;Joshi</div>\n<div class=\"imageTitle\">Fig 6</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nWith the increased popularity of machine learning algorithms deployed on a variety of hardware systems, the problem of identifying the best model among numerous possible configurations has drawn significant attention. The problem is computationally difficult and it is compounded by the need to select the right platform to run these applications, under given power or latency constraints. This hardware wall forces machine learning service providers to constantly redesign the underlying hardware fabric to satisfy certain target constraints. This project has developed tools for automatic and efficient co-design of machine learning algorithms and hardware platforms that result in significant cost and time-to-market reduction for machine learning systems.\nTo this end, the project has introduced efficient meta-learning for machine learning systems and algorithm-hardware platform co-design. In more detail, the project has developed meta-learning algorithms for the optimization of machine learning models under system hardware constraints and has formulated the hardware design of efficient machine learning systems as a machine learning problem itself, which can be effectively solved by generic optimization algorithms. Finally, the project has developed multi-objective algorithms for the co-design of machine learning applications and hardware platforms they need to run on, and has exploited domain knowledge from hardware engineering and design schemes to substantially accelerate hardware-aware model optimization.\nThe results of the project are likely to change the landscape of modeling, optimization, and design methodologies for efficient machine learning systems. A selected set of our technical contributions and outreach activities completed during the duration of this grant is included below:\n\n\nSingle-Path NAS: An efficient hardware-aware neural architecture search framework that is able to find the best machine learning model (with highest accuracy) under given serving latency constraints in hours, rather than days.\n\n\nJoslim and WidthXfer: A set of efficient neural architecture search frameworks that address the shortcomings that existing approaches still have. To alleviate these, we have improved on top of our previous work Single-Path NAS by proposing width optimization by proxy (WidthXfer) and joint weight/network width optimization (Joslim) to speed up the process of finding a hardware constrained machine learning model.\n\n\nDeep-NVM and Deep-NVM++: An efficient  framework for cross-layer modeling and optimization for GPU-based deep learning systems using nonvolatile storage. Non-volatile memory (NVM) technologies have significant advantages compared to conventional storage due to their non-volatility, higher cell density, and scalability features.\n\n\nQUIDAM: A framework for power, performance, and area results not just for a single hardware design point but for a range of different hardware designs. The framework can be used to jointly analyze trade-offs of various architectural design choices and machine learning workloads and achieve better trade-offs between accuracy and hardware-efficiency metrics such as performance per area and energy.\n\n\nFurthermore, the work has had an important educational and mentoring component by impacting how engineers are trained in a multidisciplinary fashion for dealing with next generation technological advances in general, and the problem of efficiently and intelligently co-designing machine learning algorithms and the hardware platforms they are running on, in particular. We have summarized our educational contributions in a publication on the course developed by PI D. Marculescu on Hardware Architectures for Machine Learning. The project has included a diverse graduate and undergraduate trainee population, while expanding the project's outreach to high-school and middle-school students.\nThe data, code, results, and simulators developed in this project have been made available publicly throughout the duration of the project and will be available at least for the next four years. The location of the repository is https://github.com/enyac-group.\n\n \n\n\t\t\t\t\tLast Modified: 02/12/2023\n\n\t\t\t\t\tSubmitted by: Gauri Joshi"
 }
}