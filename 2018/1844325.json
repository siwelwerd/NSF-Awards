{
 "awd_id": "1844325",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Hierarchical Contrastive Explanations for Robot-Human Communication",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 274581.0,
 "awd_amount": 274581.0,
 "awd_min_amd_letter_date": "2018-08-11",
 "awd_max_amd_letter_date": "2018-08-11",
 "awd_abstract_narration": "Intelligent assistive robots have the potential to improve our society in many walks of life: they could help us take care of the elderly and the sick, act as first responders in emergencies, and kickstart extra-planetary exploration.  However, today's robots require highly trained experts for their customization, configuration, and repair. This not only makes it difficult to realize the potential benefits of assistive robots in society, but also creates large uncertainties in the future of employment for millions in the workforce.  To help address such issues, this project develops new ways for robots to explain their actions to humans, considering the proficiency of the users.  Thus, robots will be able to tailor their explanations to what someone already understands about the robots' capabilities and limitations.\r\n\r\nThe project focuses on automatically explaining unexpected robot behavior to users with potentially imprecise knowledge about the underlying task and/or the robot. This can be used to efficiently diagnose specification problems and customize robots towards desired behaviors. The proposed approach formalizes three general principles: 1) customizing explanations according to the audience; 2) treating explanation as an interactive process; and 3) using the questions asked of a robot to estimate the user's level of expertise. In this framework, a user may present a foil, or a counterfactual proposal of alternative robot behavior, that s/he finds more natural. The proposed approach estimates the user's proficiency using a lattice of abstract models and computes reasons why the proposed alternatives would not work using minimal additional detail. In this way, the system can produce explanations that are contrastive and aligned with the proficiency of the user.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Siddharth",
   "pi_last_name": "Srivastava",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Siddharth Srivastava",
   "pi_email_addr": "siddharths@asu.edu",
   "nsf_id": "000762398",
   "pi_start_date": "2018-08-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Subbarao",
   "pi_last_name": "Kambhampati",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Subbarao Kambhampati",
   "pi_email_addr": "rao@asu.edu",
   "nsf_id": "000233965",
   "pi_start_date": "2018-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "ORSPA",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852816011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 274581.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The objective of this EAGER project was to develop technical<br />foundations for the intuitive notion that explanations<br />need to be more abstract to be understandable to users with lesser<br />expertise about an AI system. In particular, it addresses the problem<br />of making the behavior of taskable AI systems easy to understand for<br />non-experts. Taskable AI systems allow users to provide high-level<br />instructions about desired objectives. Such systems autonomously<br />compute plans for achieving user-desired objectives. However, these<br />plans can differ widely from those that a user would compute on their<br />own---both because of differences in the robot's and the user's<br />physical capabilities as well as because of differences in their<br />computational processes and resources.<br /><br />This project used the mathematical framework of abstraction lattices<br />for expressing AI systems and their environments at multiple levels of<br />abstraction. Given a user's question (typically a counterfactual<br />``foil'', or alternative plan for achieving the same objective),<br />algorithms developed as a part of this project identify abstract<br />models that are close to the user's understanding of the problem. They<br />also compute explanations in the form of details that would help<br />refine the abstract model to a version where the foils can be<br />clarified. These details serve as effective explanations for<br />users. This core idea was developed and extend to multiple settings,<br />including situations where the robot had to explain why it couldn't<br />find any plan for achieving a desired objective, as well as those<br />where the robot had to summarize for its user complex, contingent<br />policies for achieving goals in stochastic environments.<br /><br />Intellectual Merit<br /><br />In operationalizing the intuitive notion of tuning explanations to<br />users by adjusting the level of detail, advances made in this<br />project's helped develop practical computational paradigms that can be<br />used to autonomously compute explanations that are ``easy to<br />understand'' for a user. In addition, the mathematical framework<br />developed as a part of this project helped create foundations for<br />designing AI systems that can assess the limits and capabilities of<br />other, black-box AI systems.<br /><br />Broader Impact<br /><br />The promise of taskable AI systems as productivity aids and assistants<br />rests on the ability of non AI-experts to effectively understand what<br />an AI system can and can't do. The mathematical frameworks and<br />algorithms developed as a part of this EAGER project can be used to<br />develop user-training modules for AI systems that provide autonomous<br />on-the-fly training even as the AI systems adapt or change in response<br />to changes in their tasks and environments. This would help enable<br />future workplaces with a workforce that is continually upskilled to<br />utilize state-of-the-art AI systems.<br /><br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/27/2021<br>\n\t\t\t\t\tModified by: Siddharth&nbsp;Srivastava</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe objective of this EAGER project was to develop technical\nfoundations for the intuitive notion that explanations\nneed to be more abstract to be understandable to users with lesser\nexpertise about an AI system. In particular, it addresses the problem\nof making the behavior of taskable AI systems easy to understand for\nnon-experts. Taskable AI systems allow users to provide high-level\ninstructions about desired objectives. Such systems autonomously\ncompute plans for achieving user-desired objectives. However, these\nplans can differ widely from those that a user would compute on their\nown---both because of differences in the robot's and the user's\nphysical capabilities as well as because of differences in their\ncomputational processes and resources.\n\nThis project used the mathematical framework of abstraction lattices\nfor expressing AI systems and their environments at multiple levels of\nabstraction. Given a user's question (typically a counterfactual\n``foil'', or alternative plan for achieving the same objective),\nalgorithms developed as a part of this project identify abstract\nmodels that are close to the user's understanding of the problem. They\nalso compute explanations in the form of details that would help\nrefine the abstract model to a version where the foils can be\nclarified. These details serve as effective explanations for\nusers. This core idea was developed and extend to multiple settings,\nincluding situations where the robot had to explain why it couldn't\nfind any plan for achieving a desired objective, as well as those\nwhere the robot had to summarize for its user complex, contingent\npolicies for achieving goals in stochastic environments.\n\nIntellectual Merit\n\nIn operationalizing the intuitive notion of tuning explanations to\nusers by adjusting the level of detail, advances made in this\nproject's helped develop practical computational paradigms that can be\nused to autonomously compute explanations that are ``easy to\nunderstand'' for a user. In addition, the mathematical framework\ndeveloped as a part of this project helped create foundations for\ndesigning AI systems that can assess the limits and capabilities of\nother, black-box AI systems.\n\nBroader Impact\n\nThe promise of taskable AI systems as productivity aids and assistants\nrests on the ability of non AI-experts to effectively understand what\nan AI system can and can't do. The mathematical frameworks and\nalgorithms developed as a part of this EAGER project can be used to\ndevelop user-training modules for AI systems that provide autonomous\non-the-fly training even as the AI systems adapt or change in response\nto changes in their tasks and environments. This would help enable\nfuture workplaces with a workforce that is continually upskilled to\nutilize state-of-the-art AI systems.\n\n\n\n\n\t\t\t\t\tLast Modified: 12/27/2021\n\n\t\t\t\t\tSubmitted by: Siddharth Srivastava"
 }
}