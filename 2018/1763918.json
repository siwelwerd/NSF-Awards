{
 "awd_id": "1763918",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Compute Caches: Opportunistic Parallelism in General Purpose Processors at Extreme Scale",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2018-09-04",
 "awd_max_amd_letter_date": "2018-09-04",
 "awd_abstract_narration": "Computer designers have traditionally separated the role of storage and compute units. Memories and caches stored data. Processors' logic units computed them. It is not obvious that this separation is needed. A human brain does not separate the two so distinctly. This project addresses this fundamental question regarding the role of caches. Caches are used in almost all modern processors. They occupy a large fraction (over 70%) of the computer chip area. Latest Intel's server class Xeon processor, for instance, devotes several tens of megabytes just for its last-level cache. By avoiding movement of data in and out of memory arrays, this project will demonstrate the efficiency of compute caches, across a broad range of data-intensive applications that span several domains: cognitive computing, data analytics, security and graphs, and save vast amounts of energy spent in shuffling data between compute and memory units in modern computing systems. \r\n\r\nThis project will develop novel SRAM array designs for supporting a rich set of operation types and address various architectural challenges that arise in integrating highly parallel compute caches with a general-purpose host processor. Until today, caches have  served only as an intermediate low-latency storage unit. This project directly challenges this conventional design, and imposes a dual responsibility on caches: store and compute data. The key advantage of this approach is that it allows data stored across hundreds of memory arrays in caches to be operated on concurrently. The end result is that memory arrays morph into massive vector compute units that are potentially one to two orders of magnitude wider than a modern graphics processors (GPUs) vector units. This project considers vertically integrated solutions that cut across the computing stack: circuits, architecture, compilers, to applications.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Reetuparna",
   "pi_last_name": "Das",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Reetuparna Das",
   "pi_email_addr": "reetudas@umich.edu",
   "nsf_id": "000750892",
   "pi_start_date": "2018-09-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Dennis",
   "pi_last_name": "Sylvester",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dennis Sylvester",
   "pi_email_addr": "dennis@eecs.umich.edu",
   "nsf_id": "000322633",
   "pi_start_date": "2018-09-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Blaauw",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Blaauw",
   "pi_email_addr": "blaauw@umich.edu",
   "nsf_id": "000252405",
   "pi_start_date": "2018-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "2260 Hayward, 3624 Beyster Bldg",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>SRAM caches have been an important component of contemporary computer architecture for its frequency and process friendliness, and existent in every computer as temporary storage, silently sitting next to cores. This work transforms such backseat player into a powerful lead of parallel processing.</p>\n<p>For decades, processing-in-memory (PIM) has been an attractive idea. PIM moves compute logic near the memory, and thereby reduces data movement. In contrast, SRAM can morph themselves into compute units by exploiting multi-row activation and augmented sense amps, making them intrinsically more efficient than PIM. Recent works have leveraged compute capability of SRAM to perform bit-line computing and to accelerate specific neural network workloads. Our work not only extends the compute capability of memory arrays, but also make them applicable to wide range of data-parallel applications. Introducing versatile compute primitives and programming stack makes it the first general-purpose in-memory system that can execute arbitrary data-parallel programs directly from cache.</p>\n<p>We envision large SRAM persists in modern processors as a crucial enabler of high-performance computing. With its large capacity and frequency, re-purposing it to a general data-parallel processing unit will provide ample opportunities for a variety of applications to reap the benefits of in-situ parallel computing. To that end, the holistic approach of in-memory computation stack is essential, not only hardware support and low-level API. We adopted powerful SIMT ISA, execution model that can expose multiple levels of parallelism, and a compiler that takes advantage of the in-memory parallelism and transforms high-level arithmetic operations in CUDA/OpenACC programs into a set of SIMD VLIW instructions with low-level memory operations.</p>\n<p>We have demonstrated our holistic approach to general in-memory computing provides large efficiency gains for parallel applications. By unlocking in-cache computation for general parallel programs, we can expose large thread resources (150X PUs, 10X regs over GPU) to the kernels. The combined effect of reduced data movement and massive parallelism provides a&nbsp;<span>72.6X speedup over CPUs, with only 3.5% of area cost.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/13/2022<br>\n\t\t\t\t\tModified by: Reetuparna&nbsp;Das</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSRAM caches have been an important component of contemporary computer architecture for its frequency and process friendliness, and existent in every computer as temporary storage, silently sitting next to cores. This work transforms such backseat player into a powerful lead of parallel processing.\n\nFor decades, processing-in-memory (PIM) has been an attractive idea. PIM moves compute logic near the memory, and thereby reduces data movement. In contrast, SRAM can morph themselves into compute units by exploiting multi-row activation and augmented sense amps, making them intrinsically more efficient than PIM. Recent works have leveraged compute capability of SRAM to perform bit-line computing and to accelerate specific neural network workloads. Our work not only extends the compute capability of memory arrays, but also make them applicable to wide range of data-parallel applications. Introducing versatile compute primitives and programming stack makes it the first general-purpose in-memory system that can execute arbitrary data-parallel programs directly from cache.\n\nWe envision large SRAM persists in modern processors as a crucial enabler of high-performance computing. With its large capacity and frequency, re-purposing it to a general data-parallel processing unit will provide ample opportunities for a variety of applications to reap the benefits of in-situ parallel computing. To that end, the holistic approach of in-memory computation stack is essential, not only hardware support and low-level API. We adopted powerful SIMT ISA, execution model that can expose multiple levels of parallelism, and a compiler that takes advantage of the in-memory parallelism and transforms high-level arithmetic operations in CUDA/OpenACC programs into a set of SIMD VLIW instructions with low-level memory operations.\n\nWe have demonstrated our holistic approach to general in-memory computing provides large efficiency gains for parallel applications. By unlocking in-cache computation for general parallel programs, we can expose large thread resources (150X PUs, 10X regs over GPU) to the kernels. The combined effect of reduced data movement and massive parallelism provides a 72.6X speedup over CPUs, with only 3.5% of area cost.\n\n\t\t\t\t\tLast Modified: 02/13/2022\n\n\t\t\t\t\tSubmitted by: Reetuparna Das"
 }
}