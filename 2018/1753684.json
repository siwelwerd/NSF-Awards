{
 "awd_id": "1753684",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CHS: Learning Procedural Modeling Programs for Computer Graphics from Examples",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-05-01",
 "awd_exp_date": "2021-04-30",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2018-04-24",
 "awd_max_amd_letter_date": "2018-04-24",
 "awd_abstract_narration": "Procedural modeling is used to programmatically generate visual content for instruction, simulation, animation, visual effects, architecture, graphic design, and other applications.  An effective procedural model can produce a variety of detailed, visually interesting, and even pleasantly surprising results.  Unfortunately, such models are difficult to author, requiring both visual creativity and programming expertise.  More people could be empowered to create and use procedural models were it possible to deduce them from examples.  The current project will tackle this long-standing open problem in computer graphics by building on the PI's prior work to develop a research program investigating new approaches to learning procedural models from examples by combining probabilistic programs with neural nets; programs are expressive enough to represent a variety of visual content, while neural networks provide flexible learning from data.  Project outcomes will help democratize procedural modeling by allowing users to create procedural models with examples rather than by writing code, so that a wider demographic of creative professionals and enthusiasts can participate.  All code and data produced will be released as open-source, to allow other researchers and developers to apply and extend the new techniques.\r\n\r\nBecause graphical content is often hierarchical, (probabilistic) grammars are typically used to procedurally model it.  However, such content is also characterized by continuous attributes: colors, affine transformations, and so on.  While grammars can be extended to support some of these attributes, there are no general-purpose methods for learning such models from examples.  Existing approaches either ignore continuous attributes or are specialized to one type of content (e.g., building facades).  This research presents a new general-purpose approach for example-based learning of procedural models which generate discrete hierarchical structures with continuous attributes.  The key insight is representing a procedural model as a probabilistic program whose control flow and data flow can be governed by neural networks.  Like a grammar, such a program can naturally represent (possibly recursive) hierarchical structure.  The neural network logic of the program can represent complex functions which generate continuous attributes such as transformations.  The model is efficiently learnable with stochastic-gradient-based methods and has the potential to scale from small numbers of examples to large datasets.  The initial focus will be on learning procedural models of 3D scene graphs, which are 3D objects composed of a hierarchy of parts.  The research will then expand into learning procedural models from large datasets of examples, applying the techniques to domains beyond 3D scene graphs, and leveraging unstructured inputs such as images as examples.  Project outcomes will include new mathematical frameworks for learning procedural models from examples, algorithms for efficiently solving the learning problem, and evaluations of the quality of content generated by learned models.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Ritchie",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Ritchie",
   "pi_email_addr": "daniel_ritchie@brown.edu",
   "nsf_id": "000737205",
   "pi_start_date": "2018-04-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "115 Waterman St.",
  "perf_city_name": "Providence",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This award funded the creation of a new method for learning procedural models for part-based 3D objects from examples. The method extends prior work on learning probabilistic grammars from examples to learn continuous design patterns as well as discrete structural ones (e.g. the range of angles for an airplane?s wings).</p>\n<p>Funds from this award also supported the development of program-like generative models for 3D indoor scenes. The most sophisticated of these models, PlanIT, operates by first generating a type of declarative program (a graph of objects and their relationships) and then ?executes? that program by efficiently searching for object arrangements which satisfy its declarative specification while also respecting spatial arrangement patterns observed in a large scene dataset.&nbsp;</p>\n<p>This award also funded work on ShapeAssembly, a domain-specific programming language for describing 3D shape part structures (the sizes of parts and how they are connected). We also developed a neural network capable of writing ShapeAssembly programs; this can be used to generate novel shape structures (e.g. chairs, tables) whose parameters can then be manipulated by editing the output program text (e.g. changing the table width or the number of chair back slats).</p>\n<p>In addition, this award partially funded work on the inverse problem of how to infer a program which generates a given input shape. In our work on ShapeAssembly, we demonstrated how to train neural networks which take raw point clouds as input and produce ShapeAssembly programs as output?effectively ?reverse-engineering? the underlying part structure of the object depicted in the point cloud. More recently, we have applied similar ideas to inferring computer-aided design (CAD) programs which produce the geometry of individual object parts.</p>\n<p>In total, this project has significantly advanced the state-of-the-art on AI-assisted modeling of 3D shapes and scenes by combining procedural representations with data-driven methods (such as neural networks). All results described above have been accompanied by open-source code releases.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/29/2021<br>\n\t\t\t\t\tModified by: Daniel&nbsp;Ritchie</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258031254_ScreenShot2021-08-29at1.21.33PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258031254_ScreenShot2021-08-29at1.21.33PM--rgov-800width.jpg\" title=\"Learning procedural models with discrete and continuous variabilities\"><img src=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258031254_ScreenShot2021-08-29at1.21.33PM--rgov-66x44.jpg\" alt=\"Learning procedural models with discrete and continuous variabilities\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Given a small set of examples of shapes made from a collection of parts, our method infers a probabilistic program that generates more shapes in the style of the examples</div>\n<div class=\"imageCredit\">Daniel Ritchie, Sarah Jobalia, Anna Thomas</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Daniel&nbsp;Ritchie</div>\n<div class=\"imageTitle\">Learning procedural models with discrete and continuous variabilities</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258110385_ScreenShot2021-08-29at1.22.09PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258110385_ScreenShot2021-08-29at1.22.09PM--rgov-800width.jpg\" title=\"PlanIT\"><img src=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258110385_ScreenShot2021-08-29at1.22.09PM--rgov-66x44.jpg\" alt=\"PlanIT\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Indoor scenes generated by PlanIT, our data-driven model. Each scene is paired with a visual representation of its underlying object relationship graph.</div>\n<div class=\"imageCredit\">Kai Wang, Yu-an Lin, Ben Weissmann, Manolis Savva, Angel X. Chang, and Daniel Ritchie</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Daniel&nbsp;Ritchie</div>\n<div class=\"imageTitle\">PlanIT</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258237332_teaser--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258237332_teaser--rgov-800width.jpg\" title=\"ShapeAssembly\"><img src=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258237332_teaser--rgov-66x44.jpg\" alt=\"ShapeAssembly\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The ShapeAssembly language allows concise specification of object parts and how they are attached to one another. We also train a generative model which allows generating new ShapeAssembly programs or interpolating between existing ones.</div>\n<div class=\"imageCredit\">R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy Mitra, and Daniel Ritchie</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Daniel&nbsp;Ritchie</div>\n<div class=\"imageTitle\">ShapeAssembly</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258310864_sm7--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258310864_sm7--rgov-800width.jpg\" title=\"Inferring ShapeAssembly programs from point clouds\"><img src=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258310864_sm7--rgov-66x44.jpg\" alt=\"Inferring ShapeAssembly programs from point clouds\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We demonstrated how to train neural networks to infer ShapeAssembly programs from point clouds, effectively 'reverse-engineering' the part structure of the input object.</div>\n<div class=\"imageCredit\">R. Kenny Jones, David Charatan, Paul Guerrero, Niloy Mitra, and Daniel Ritchie</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Daniel&nbsp;Ritchie</div>\n<div class=\"imageTitle\">Inferring ShapeAssembly programs from point clouds</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258383653_teaser-image--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258383653_teaser-image--rgov-800width.jpg\" title=\"Inferring CAD programs\"><img src=\"/por/images/Reports/POR/2021/1753684/1753684_10540639_1630258383653_teaser-image--rgov-66x44.jpg\" alt=\"Inferring CAD programs\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We also built a system for inferring a computer-aided design (CAD) program from an input 3D shape.</div>\n<div class=\"imageCredit\">Xianghao Xu, Wenzhe Peng, Chin-Yi Cheng, Karl D. D. Willis, and Daniel Ritchie</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Daniel&nbsp;Ritchie</div>\n<div class=\"imageTitle\">Inferring CAD programs</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis award funded the creation of a new method for learning procedural models for part-based 3D objects from examples. The method extends prior work on learning probabilistic grammars from examples to learn continuous design patterns as well as discrete structural ones (e.g. the range of angles for an airplane?s wings).\n\nFunds from this award also supported the development of program-like generative models for 3D indoor scenes. The most sophisticated of these models, PlanIT, operates by first generating a type of declarative program (a graph of objects and their relationships) and then ?executes? that program by efficiently searching for object arrangements which satisfy its declarative specification while also respecting spatial arrangement patterns observed in a large scene dataset. \n\nThis award also funded work on ShapeAssembly, a domain-specific programming language for describing 3D shape part structures (the sizes of parts and how they are connected). We also developed a neural network capable of writing ShapeAssembly programs; this can be used to generate novel shape structures (e.g. chairs, tables) whose parameters can then be manipulated by editing the output program text (e.g. changing the table width or the number of chair back slats).\n\nIn addition, this award partially funded work on the inverse problem of how to infer a program which generates a given input shape. In our work on ShapeAssembly, we demonstrated how to train neural networks which take raw point clouds as input and produce ShapeAssembly programs as output?effectively ?reverse-engineering? the underlying part structure of the object depicted in the point cloud. More recently, we have applied similar ideas to inferring computer-aided design (CAD) programs which produce the geometry of individual object parts.\n\nIn total, this project has significantly advanced the state-of-the-art on AI-assisted modeling of 3D shapes and scenes by combining procedural representations with data-driven methods (such as neural networks). All results described above have been accompanied by open-source code releases.\n\n \n\n\t\t\t\t\tLast Modified: 08/29/2021\n\n\t\t\t\t\tSubmitted by: Daniel Ritchie"
 }
}