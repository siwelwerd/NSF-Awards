{
 "awd_id": "1754049",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Understanding the Cognitive Factors and Legal Processes of Expert Opinions",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "reginald sheehan",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 334787.0,
 "awd_amount": 334787.0,
 "awd_min_amd_letter_date": "2018-07-31",
 "awd_max_amd_letter_date": "2018-07-31",
 "awd_abstract_narration": "Title: Understanding Cognitive Factors and Legal Processes of Expert Opinions\r\n\r\nAbstract\r\n\r\nDuring trials, experts are often called to provide their opinion in order to help courts understand complicated issues. These experts are expected to remain objective when providing their evaluations, in large part because judges and juries can be influenced by expert opinions. This project will examine how expert decision making and legal procedures may lead to bias during forensic evaluations by experts. Further, this project will study and compare whether experts are influenced by the party that retains their services to provide expert opinions. \r\n\r\nThe project includes the psychological theories of cognitive dissonance, framing, and confirmation bias as potential contributing factors that may bias expert opinions. Using actual forensic clinicians tasked with conducting mock forensic evaluations, this project will use an experimental approach to examine expert opinions. Manipulations in the experimental research design will examine experts \"information review strategies to assess how cognitive dissonance, framing, and confirmation bias may affect experts\" decision making during an evaluation and their ultimate impressions. The project also will examine the impact of these cognitive factors when experts are kept blind to the referral source retaining their services. By identifying how, when, and why expert bias develops during a forensic evaluation, this research can identify ways to increase objectivity among experts and bring about fairer case outcomes.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jennifer",
   "pi_last_name": "Perillo",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Jennifer T Perillo",
   "pi_email_addr": "jperillo@iup.edu",
   "nsf_id": "000557726",
   "pi_start_date": "2018-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anthony",
   "pi_last_name": "Perillo",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Anthony D Perillo",
   "pi_email_addr": "aperillo@iup.edu",
   "nsf_id": "000084494",
   "pi_start_date": "2018-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University of Pennsylvania Research Institute",
  "inst_street_address": "1179 GRANT ST",
  "inst_street_address_2": "STE 1",
  "inst_city_name": "INDIANA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "7243572223",
  "inst_zip_code": "157012898",
  "inst_country_name": "United States",
  "cong_dist_code": "14",
  "st_cong_dist_code": "PA14",
  "org_lgl_bus_name": "IUP RESEARCH INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "UGHXHKJB8LL6"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University of Pennsylvania",
  "perf_str_addr": "1020 Oakland Avenue",
  "perf_city_name": "Indiana",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "157051064",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "137200",
   "pgm_ele_name": "LSS-Law And Social Sciences"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 334787.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, practicing forensic clinicians completed a simulated forensic evaluation to address several research goals. One, we aimed to identify factors that explain why evaluators tend to arrive at biased conclusions for the side hiring them (prosecution or defense), despite their attempts to be objective and unbiased. Two, we tracked what evaluators did during the simulated evaluation to examine how approaches to the evaluation might differ when biased for the prosecution or the defense. Three, we examined whether this bias could be reduced if evaluators were not told which side was asking them to do the evaluation.</p>\n<p>To meet these goals, we engaged in a four-year investigation led by two scholars at Indiana University of Pennsylvania with consultation from a leading forensic bias scholar from University of Virginia. In this project, we had practicing forensic clinicians complete a simulated insanity evaluation on their computers. Clinicians were presented a screen with different files of case information they could review (or choose not to review). After opening a file, they rated the evidence in the file before returning to the original menu screen. Clinicians could review as many files as they wanted until they felt they had an opinion on whether the defendant was insane. They then entered their expert opinion on the defendant?s insanity. The files either favored the defense, favored the prosecution, or were ambiguous (determined at random).</p>\n<p>We recorded the clinicians? responses to our questions about the evidence and their opinions on insanity and also recorded their file search strategies: the files they opened, the order in which they were opened, and time spent reviewing each opened file.</p>\n<p>The project included two studies. In Study One, we asked clinicians to do the evaluation in a way that was deliberately biased for a side chosen at random (prosecution or defense). In Study Two, we had clinicians speak to the prosecutor or defense attorney (actually an actor) hiring them to do the evaluation before clinicians actually proceeded to the case. The prosecutor/defense attorney presented the case either neutrally or in a biased way. For other clinicians, they were informed we would not be telling them which side was hiring them. This neutral party presented the case either neutrally or biased toward the prosecution or defense.</p>\n<p>In Study One, the strength of the evidence appeared to impact expert opinions more than anything. When the evidence was skewed toward the defense, experts tended to find the defendant insane (regardless of the side they were asked to be biased for). Defense experts rated the files to be higher quality when the files favored the defense than when they favored the prosecution or were ambiguous, but we did not see this trend with prosecution experts. We found similar trends in Study Two: again evidence strength seemed to impact opinions of insanity, regardless of who clinicians were hired by. We saw the same trends for clinicians who were not told what side was hiring them.</p>\n<p>We did not see expert opinions biased for the side hiring them like seen in prior studies, but we found that how the case was presented to evaluators could affect their judgment. In Study Two, we saw some critical case information (e.g., police report, hospital records) was interpreted differently when the case was presented by the attorney/neutral party in a particularly biased way, especially when suggesting the defendant was insane. These findings suggest it may be best to have attorneys provide less, not more, background to evaluators at the start, which directly contradicts recommendations in some trainings (e.g., the co-PI recently took a competency evaluation training where the trainers supporting receiving substantial background from the referring attorney to streamline the assessment process).</p>\n<p>This project contributed to the education, training, and professional development of a diverse group of students and scholars. It supported formal research experiences for two clinical psychology doctoral students, both of whom has since graduated and acquired highly regarded fellowship positions. Four additional doctoral students received research training through this project. We presented a sneak peak of the project?s results in the University of New Mexico?s Law and Mental Health series (available on YouTube). We have submitted to present the results of Study One to the 2023 American Psychology-Law Society conference in Philadelphia and plan to do the same for Study Two for the 2024 APLS conference. We are also collaboratively writing up two papers for publication now.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/14/2022<br>\n\t\t\t\t\tModified by: Jennifer&nbsp;T&nbsp;Perillo</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this project, practicing forensic clinicians completed a simulated forensic evaluation to address several research goals. One, we aimed to identify factors that explain why evaluators tend to arrive at biased conclusions for the side hiring them (prosecution or defense), despite their attempts to be objective and unbiased. Two, we tracked what evaluators did during the simulated evaluation to examine how approaches to the evaluation might differ when biased for the prosecution or the defense. Three, we examined whether this bias could be reduced if evaluators were not told which side was asking them to do the evaluation.\n\nTo meet these goals, we engaged in a four-year investigation led by two scholars at Indiana University of Pennsylvania with consultation from a leading forensic bias scholar from University of Virginia. In this project, we had practicing forensic clinicians complete a simulated insanity evaluation on their computers. Clinicians were presented a screen with different files of case information they could review (or choose not to review). After opening a file, they rated the evidence in the file before returning to the original menu screen. Clinicians could review as many files as they wanted until they felt they had an opinion on whether the defendant was insane. They then entered their expert opinion on the defendant?s insanity. The files either favored the defense, favored the prosecution, or were ambiguous (determined at random).\n\nWe recorded the clinicians? responses to our questions about the evidence and their opinions on insanity and also recorded their file search strategies: the files they opened, the order in which they were opened, and time spent reviewing each opened file.\n\nThe project included two studies. In Study One, we asked clinicians to do the evaluation in a way that was deliberately biased for a side chosen at random (prosecution or defense). In Study Two, we had clinicians speak to the prosecutor or defense attorney (actually an actor) hiring them to do the evaluation before clinicians actually proceeded to the case. The prosecutor/defense attorney presented the case either neutrally or in a biased way. For other clinicians, they were informed we would not be telling them which side was hiring them. This neutral party presented the case either neutrally or biased toward the prosecution or defense.\n\nIn Study One, the strength of the evidence appeared to impact expert opinions more than anything. When the evidence was skewed toward the defense, experts tended to find the defendant insane (regardless of the side they were asked to be biased for). Defense experts rated the files to be higher quality when the files favored the defense than when they favored the prosecution or were ambiguous, but we did not see this trend with prosecution experts. We found similar trends in Study Two: again evidence strength seemed to impact opinions of insanity, regardless of who clinicians were hired by. We saw the same trends for clinicians who were not told what side was hiring them.\n\nWe did not see expert opinions biased for the side hiring them like seen in prior studies, but we found that how the case was presented to evaluators could affect their judgment. In Study Two, we saw some critical case information (e.g., police report, hospital records) was interpreted differently when the case was presented by the attorney/neutral party in a particularly biased way, especially when suggesting the defendant was insane. These findings suggest it may be best to have attorneys provide less, not more, background to evaluators at the start, which directly contradicts recommendations in some trainings (e.g., the co-PI recently took a competency evaluation training where the trainers supporting receiving substantial background from the referring attorney to streamline the assessment process).\n\nThis project contributed to the education, training, and professional development of a diverse group of students and scholars. It supported formal research experiences for two clinical psychology doctoral students, both of whom has since graduated and acquired highly regarded fellowship positions. Four additional doctoral students received research training through this project. We presented a sneak peak of the project?s results in the University of New Mexico?s Law and Mental Health series (available on YouTube). We have submitted to present the results of Study One to the 2023 American Psychology-Law Society conference in Philadelphia and plan to do the same for Study Two for the 2024 APLS conference. We are also collaboratively writing up two papers for publication now.\n\n \n\n\t\t\t\t\tLast Modified: 11/14/2022\n\n\t\t\t\t\tSubmitted by: Jennifer T Perillo"
 }
}