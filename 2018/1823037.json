{
 "awd_id": "1823037",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: Collaborative Research: Global Address Programming with Accelerators",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Seung-Jong Park",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 386025.0,
 "awd_amount": 386025.0,
 "awd_min_amd_letter_date": "2018-08-30",
 "awd_max_amd_letter_date": "2018-08-30",
 "awd_abstract_narration": "Large-scale computing today is dominated by parallel computing, where a large task is divided into many smaller tasks and those smaller tasks run at the same time. Traditionally each of those tasks run independently up to a common stopping point, then they halt, exchange information, and continue. This global stop-and-communicate step is quite expensive. This project instead pursues a different approach, where individual tasks directly communicate with other tasks asynchronously, without having to wait for a global stopping point. This approach is likely to yield better performance on large-scale computing tasks, specifically on what is becoming the dominant large-scale machine, a heterogeneous machine with many CPUs and other\r\nmany-core processors. The project will deliver a set of high-performance, open-source data structures and algorithm implementations to support irregular patterns of communication, notably those that arise in biology, graph analytics, and sparse linear algebra for machine learning. These will not only be directly useful for end users but also demonstrate how to design and engineer primitives for accelerator-equipped distributed-memory machines. The project also engages application developers (both in our groups and externally) to make the outcomes broadly useful.\r\n\r\nThe project will develop a programming environment for accelerator-based HPC systems that integrates accelerators into a Partitioned Global Address Space (PGAS) model, which will allow direct communication between GPUs in a manner that is well suited to both applications and the underlying hardware. Specifically, GPU programming will be integrated with the UPC++ PGAS programming model (\"GPUPC++\"). The project will thus advance the state of the art in algorithms, programming models, and low-level support for the heterogeneous large-scale computers.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Owens",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "John D Owens",
   "pi_email_addr": "jowens@ece.ucdavis.edu",
   "nsf_id": "000377403",
   "pi_start_date": "2018-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "One Shields Avenue",
  "perf_city_name": "Davis",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956186134",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 386025.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focused on exploring novel approaches to scalable application development on graphics processing units (GPUs), specifically in the area of graph analytics. Our work was motivated by the need for high performance in large-scale applications.</p>\n<p>We focused specifically on graphs because they are ubiquitous data structures that can be used to represent objects and the relationships between them, for instance friendships in a social network, interactions among genes, or roads between cities. Graphs are at the core of modern applications that make recommendations (\"you might be friends with ...\" or \"you might like this product\"), detect anomalies (\"this looks like credit card fraud!\"), and solve interesting computational problems (\"what's the fastest route between here and ...\"). Both engineers and scientists want to solve these problems quickly, and thus our project targets powerful parallel processors (GPUs) that can deliver the fastest possible results. However, programming GPUs remains a significant challenge. Our open-source \"Gunrock\" graph-analytics framework represents the state of the art in traditional approaches to GPU graph analytics. In this project, we pursued a different strategy for expressing graph analytics, centered on two advances:</p>\n<ul>\n<li>Traditional approaches to parallel graph analytics are data-parallel, processing vertices or edges in bulk and synchronously. We instead expressed our workload as parallel tasks, an approach that is atypical and not an obvious good fit for GPUs. We showed in our work that the task-parallel approach, however, can potentially achieve excellent efficiency on the GPU, particularly when we enable flexibility in the configuration of parallel tasks. The task-parallel framework that we developed represents the state of the art in GPU computing.</li>\n<li>Expressing our workloads as tasks, in turn, allows us to relax some of the ordering constraints inherent in the bulk-synchronous-parallel approach. We showed in our work that relaxing these constraints allows us to make better use of the GPU's computational resources, and additionally showed multiple ways that we could ably address issues that arise from relaxed constraints.</li>\n</ul>\n<p>We implemented our resulting \"Atos\" system on both single-GPU and multi-GPU machines, demonstrating the viability of this approach, and showed performance results that surpass the previous state of the art in GPU computing. We also released Atos as open-source software to allow others to use and build on our approach. Because both graphs and GPUs are widely used in the engineering and scientific communities, we expect that our work will directly impact scientists and engineers who create, process, and analyze graphs. NSF support also allowed us to train multiple graduate students whose talents will positively impact the development of future technologies in high-performance computing.</p>\n<p>The development of our framework spanned the entire lifetime of this project and we appreciate the vision shown by the National Science Foundation, its reviewers, and US taxpayers in funding long-term projects that may require multiple years of work to show results.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/20/2023<br>\n\t\t\t\t\tModified by: John&nbsp;D&nbsp;Owens</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project focused on exploring novel approaches to scalable application development on graphics processing units (GPUs), specifically in the area of graph analytics. Our work was motivated by the need for high performance in large-scale applications.\n\nWe focused specifically on graphs because they are ubiquitous data structures that can be used to represent objects and the relationships between them, for instance friendships in a social network, interactions among genes, or roads between cities. Graphs are at the core of modern applications that make recommendations (\"you might be friends with ...\" or \"you might like this product\"), detect anomalies (\"this looks like credit card fraud!\"), and solve interesting computational problems (\"what's the fastest route between here and ...\"). Both engineers and scientists want to solve these problems quickly, and thus our project targets powerful parallel processors (GPUs) that can deliver the fastest possible results. However, programming GPUs remains a significant challenge. Our open-source \"Gunrock\" graph-analytics framework represents the state of the art in traditional approaches to GPU graph analytics. In this project, we pursued a different strategy for expressing graph analytics, centered on two advances:\n\nTraditional approaches to parallel graph analytics are data-parallel, processing vertices or edges in bulk and synchronously. We instead expressed our workload as parallel tasks, an approach that is atypical and not an obvious good fit for GPUs. We showed in our work that the task-parallel approach, however, can potentially achieve excellent efficiency on the GPU, particularly when we enable flexibility in the configuration of parallel tasks. The task-parallel framework that we developed represents the state of the art in GPU computing.\nExpressing our workloads as tasks, in turn, allows us to relax some of the ordering constraints inherent in the bulk-synchronous-parallel approach. We showed in our work that relaxing these constraints allows us to make better use of the GPU's computational resources, and additionally showed multiple ways that we could ably address issues that arise from relaxed constraints.\n\n\nWe implemented our resulting \"Atos\" system on both single-GPU and multi-GPU machines, demonstrating the viability of this approach, and showed performance results that surpass the previous state of the art in GPU computing. We also released Atos as open-source software to allow others to use and build on our approach. Because both graphs and GPUs are widely used in the engineering and scientific communities, we expect that our work will directly impact scientists and engineers who create, process, and analyze graphs. NSF support also allowed us to train multiple graduate students whose talents will positively impact the development of future technologies in high-performance computing.\n\nThe development of our framework spanned the entire lifetime of this project and we appreciate the vision shown by the National Science Foundation, its reviewers, and US taxpayers in funding long-term projects that may require multiple years of work to show results.\n\n \n\n\t\t\t\t\tLast Modified: 02/20/2023\n\n\t\t\t\t\tSubmitted by: John D Owens"
 }
}