{
 "awd_id": "1844740",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Exploring Cognitively Plausible Computational Models for Processing Human Language",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 109926.0,
 "awd_amount": 109926.0,
 "awd_min_amd_letter_date": "2018-08-17",
 "awd_max_amd_letter_date": "2018-08-17",
 "awd_abstract_narration": "Despite the recent successes of artificial intelligence techniques designed to process human language, most contemporary solutions are designed to handle very specific language processing tasks.  As a result, human-level language understanding is still out of reach for most current computational approaches, especially when retaining new information and reasoning over the accumulated knowledge is involved.  This exploratory project advances the goal of developing more cognitively realistic computational models that can mimic some of the known properties of human language processing, and as a result, be more robust and better suited as general systems for language understanding, with human-like learning which involves obtaining and updating knowledge over time.\r\n\r\nWhile most contemporary deep learning approaches in natural language processing focus on task-specific end-to-end models, this project prioritizes generalist architectures that would be consistent with the current data on semantic priming, grouping and chunking effects in the formation and use of conceptual systems, and the effects of long- and short-term memory on the storage and retrieval of knowledge. In this project, novel neural network architectures are planned that model a subset of these properties.  The processes that enable learning and memory via strengthening of synaptic connections in the brain will be emulated by a set of representational units (r-units) with bidirectional connections, modeling the interaction between small regions of neocortex during information processing. Memory Store Activation State Model represents the connections between r-units in terms of convolutional filters applied to the memory store. The priming effects will be modeled by a pre-activation pattern produced via a sequence of deconvolutional operation.  Rate-Based Connectivity Network model combines reinforcement learning on per-node basis with a form of Hebbian learning applied to a time-varying system where each r-unit calculates rate of change of its output, allowing node activations to linger through time; it is trained with a discrete global reward signal. The goal of this project is to establish the feasibility of the proposed architectures by developing the initial proof-of-concept prototypes, demonstrating that they are able to converge on simple learning tasks, and applying them to the task of language modeling to ensure that a practically useful representation can be learned.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anna",
   "pi_last_name": "Rumshisky",
   "pi_mid_init": "",
   "pi_sufx_name": "Dr.",
   "pi_full_name": "Anna Rumshisky",
   "pi_email_addr": "arumshisky@gmail.com",
   "nsf_id": "000611069",
   "pi_start_date": "2018-08-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Lowell",
  "inst_street_address": "220 PAWTUCKET ST STE 400",
  "inst_street_address_2": "",
  "inst_city_name": "LOWELL",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "9789344170",
  "inst_zip_code": "018543573",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MA03",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS LOWELL",
  "org_prnt_uei_num": "",
  "org_uei_num": "LTNVSTJ3R6D5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Lowell",
  "perf_str_addr": "",
  "perf_city_name": "Lowell",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "018543643",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "MA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 109926.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-a8b6d8bb-7fff-5e45-9737-3532989604bc\"> </span></p>\n<p dir=\"ltr\"><span>Language models trained on large amounts of human-generated language present both an opportunity and a change in the way society interacts with information. With their influence becoming ever more pervasive in everyday life, understanding how they learn, store and use information has become extremely important.&nbsp; Through this award, we developed a number of methods for analyzing how information is represented in large language models.&nbsp;</span>We analyzed the way large language models store linguistically meaningful information by 1) examining attention patterns learned during training, 2) pruning the models to retain only meaningful segments of the neural network, 3) large-scale probing of model performance with fine-grained human language understanding tasks.</p>\n<p dir=\"ltr\">Contemporary language models perform several types of computation in order to process language. One of the key components of their architecture is the self-attention computation, which is performed in parallel by multiple sets of model parameters. Through this award, we developed several methods to analyze and interpret self-attention, and other information encoded by model parameters. First, we developed a classification of different attention patterns learned by the model, showing that different parts of the model learn similar patterns, which can be disabled without loss of performance. We then demonstrated that pre-trained language models are surprisingly fragile to the removal of a very small number of specific high-magnitude model parameters that emerge early in training, demonstrating this effect across different classes of language models.</p>\n<p dir=\"ltr\"><span>In a follow-up study, we explored whether pruning some of the parameters of pre-trained neural language models can reveal meaningful substructures in these neural networks, corresponding to specific linguistic features and language understanding and reasoning tasks. We showed that it is possible to trim the model down to a much smaller size, and then retrain it to perform similarly to the original larger model. Importantly, we demonstrated that during this process, the sets of model parameters that encode the information necessary to understand language is not stable across different tasks. Also, exactly how the model was trimmed did not have a large effect on performance. This inidicated that the standard pre-training regime for such models, in which they are trained to predict human-generated text, initializes model parameters in a way favorable to many different downstream language understanding tasks across the board.</span></p>\n<p dir=\"ltr\">Finally, we probed a large variety of different model architectures using specialized language understanding tasks, each of which targeted the understanding of specific linguistic and psycholinguistic information. We found that such language tasks as answering complex multi-part questions remained out of reach of the studied models. We then extended some of the psycholinguistic probing benchmarks with additional examples, resulting in up to 40-fold increase in dataset size.&nbsp;The resulting datasets will enable a more thorough testing of different model architectures in the future.</p>\n<p dir=\"ltr\"><span>This award also enabled the team to write the first survey which synthesized over 150 studies of the popular pre-trained language models, reviewing the current state of knowledge about how these models works, what kind of information they learn, and how this information is represented, as well as different variants of such models and the methods for compressing them. This survey continues to be widely used by the research community in our field, where the pace of research makes it challenging to keep up with the current literature.</span></p>\n<p dir=\"ltr\">Overall, as a result of this award, the team published 5 papers, with the 6th paper currently under review, and an estimated total of 1400 citations to date. This award also facilitated the professional development of several students, including 4 PhD students, a Master's student, a postdoctoral fellow, and a high-school student who collaborated on this work. Two PhD students defended their PhD dissertations based in part on this work. Published work also allowed two PhD students and a postdoctoral fellow from our team to give multiple invited talks and secure job interviews.&nbsp;</p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/30/2023<br>\n\t\t\t\t\tModified by: Anna&nbsp;Rumshisky</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1844740/1844740_10572755_1675090717861_eager-outcomes-figure-1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1844740/1844740_10572755_1675090717861_eager-outcomes-figure-1--rgov-800width.jpg\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2023/1844740/1844740_10572755_1675090717861_eager-outcomes-figure-1--rgov-66x44.jpg\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Self-attention patterns in a pre-trained language model.</div>\n<div class=\"imageCredit\">Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Anna&nbsp;Rumshisky</div>\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nLanguage models trained on large amounts of human-generated language present both an opportunity and a change in the way society interacts with information. With their influence becoming ever more pervasive in everyday life, understanding how they learn, store and use information has become extremely important.  Through this award, we developed a number of methods for analyzing how information is represented in large language models. We analyzed the way large language models store linguistically meaningful information by 1) examining attention patterns learned during training, 2) pruning the models to retain only meaningful segments of the neural network, 3) large-scale probing of model performance with fine-grained human language understanding tasks.\nContemporary language models perform several types of computation in order to process language. One of the key components of their architecture is the self-attention computation, which is performed in parallel by multiple sets of model parameters. Through this award, we developed several methods to analyze and interpret self-attention, and other information encoded by model parameters. First, we developed a classification of different attention patterns learned by the model, showing that different parts of the model learn similar patterns, which can be disabled without loss of performance. We then demonstrated that pre-trained language models are surprisingly fragile to the removal of a very small number of specific high-magnitude model parameters that emerge early in training, demonstrating this effect across different classes of language models.\nIn a follow-up study, we explored whether pruning some of the parameters of pre-trained neural language models can reveal meaningful substructures in these neural networks, corresponding to specific linguistic features and language understanding and reasoning tasks. We showed that it is possible to trim the model down to a much smaller size, and then retrain it to perform similarly to the original larger model. Importantly, we demonstrated that during this process, the sets of model parameters that encode the information necessary to understand language is not stable across different tasks. Also, exactly how the model was trimmed did not have a large effect on performance. This inidicated that the standard pre-training regime for such models, in which they are trained to predict human-generated text, initializes model parameters in a way favorable to many different downstream language understanding tasks across the board.\nFinally, we probed a large variety of different model architectures using specialized language understanding tasks, each of which targeted the understanding of specific linguistic and psycholinguistic information. We found that such language tasks as answering complex multi-part questions remained out of reach of the studied models. We then extended some of the psycholinguistic probing benchmarks with additional examples, resulting in up to 40-fold increase in dataset size. The resulting datasets will enable a more thorough testing of different model architectures in the future.\nThis award also enabled the team to write the first survey which synthesized over 150 studies of the popular pre-trained language models, reviewing the current state of knowledge about how these models works, what kind of information they learn, and how this information is represented, as well as different variants of such models and the methods for compressing them. This survey continues to be widely used by the research community in our field, where the pace of research makes it challenging to keep up with the current literature.\nOverall, as a result of this award, the team published 5 papers, with the 6th paper currently under review, and an estimated total of 1400 citations to date. This award also facilitated the professional development of several students, including 4 PhD students, a Master's student, a postdoctoral fellow, and a high-school student who collaborated on this work. Two PhD students defended their PhD dissertations based in part on this work. Published work also allowed two PhD students and a postdoctoral fellow from our team to give multiple invited talks and secure job interviews. \n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/30/2023\n\n\t\t\t\t\tSubmitted by: Anna Rumshisky"
 }
}