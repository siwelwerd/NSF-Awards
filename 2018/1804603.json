{
 "awd_id": "1804603",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SaTC: CORE: Frontier: Collaborative: End-to-End Trustworthiness of Machine-Learning Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 926074.0,
 "awd_amount": 958074.0,
 "awd_min_amd_letter_date": "2018-08-07",
 "awd_max_amd_letter_date": "2023-05-25",
 "awd_abstract_narration": "This frontier project establishes the Center for Trustworthy Machine Learning (CTML), a large-scale, multi-institution, multi-disciplinary effort whose goal is to develop scientific understanding of the risks inherent to machine learning, and to develop the tools, metrics, and methods to manage and mitigate them.  The center is led by a cross-disciplinary team developing unified theory, algorithms and empirical methods within complex and ever-evolving ML approaches, application domains, and environments. The science and arsenal of defensive techniques emerging within the center will provide the basis for building future systems in a more trustworthy and secure manner, as well as fostering a long term community of research within this essential domain of technology. The center has a number of outreach efforts, including a massive open online course (MOOC) on this topic, an annual conference, and broad-based educational initiatives. The investigators continue their ongoing efforts at broadening participation in computing via a joint summer school on trustworthy ML aimed at underrepresented groups, and by engaging in activities for high school students across the country via a sequence of webinars advertised through the She++ network and other organizations.\r\n\r\nThe center focuses on three interconnected and parallel investigative directions that represent the different classes of attacks attacking ML systems: inference attacks, training attacks, and abuses of ML.   The first direction explores inference time security, namely methods to defend a trained model from adversarial inputs. This effort emphasizes developing formally grounded measurements of robustness against adversarial examples (defenses), as well as understanding the limits and costs of attacks. The second research direction aims to develop rigorously grounded measures of robustness to attacks that corrupt the training data and new training methods that are robust to adversarial manipulation.   The final direction tackles the general security implications of sophisticated ML algorithms including the potential abuses of generative ML models, such as models that generate (fake) content, as well as data mechanisms to prevent the theft of a machine learning model by an adversary who interacts with the model.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Evans",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "David E Evans",
   "pi_email_addr": "evans@virginia.edu",
   "nsf_id": "000319935",
   "pi_start_date": "2018-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia",
  "perf_str_addr": "85 Engineer's Way",
  "perf_city_name": "Charlottesville",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229044740",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "8087",
   "pgm_ref_txt": "Frontiers in SaTC"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 182666.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 178995.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 199466.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 188086.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 192861.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Recent advances in artificial intelligence and machine learning have vastly improved computational reasoning over complex domains. When machine learning models are exposed to adversarial behavior, however, the systems built upon them can be fooled, evaded, and misled in ways that can have profound security implications. As more critical systems employ machine learning&mdash;from financial systems to self-driving cars to network monitoring tools&mdash;it is vitally important that we develop the rigorous scientific techniques needed to make machine learning more robust to attack. This nascent field, which we call trustworthy machine learning, is currently fragmented across several research communities including machine learning, security, statistics, and theoretical computer science. The focus of the NSF Center for Trustworthy Machine Learning was to develop a rigorous understanding of the vulnerabilities inherent to machine learning, and to develop the tools, metrics, and methods to mitigate them. <br /><br />The main outcomes of the UVA team's contributions to the project include developing new methods for evaluating the robustness of machine learning systems and advancing the theoretical foundations of robust machine learning. Our work resulted in theoretical results showing the limits of intrinsic robustness, and practical attacks and auditing methods for measuring the robustness of a machine learning system to both training (poisoning attacks) and inference time attacks. We also developed a new approach to understanding privacy risks in a machine learning system, and developed the first formal notion of distribution inference privacy. We introduced state-of-the-art attacks to audit distribution inference and showed that traditional methods for improving privacy are not effective when the training distribution itself is sensitive. Our work resulted in theoretical advances to understand the intrinsic robustness of data distributions, publications in research conferences in security, privacy, and machine learning, and in open source tools for the research community and industrial practitioners to use to audit machine learning systems.</p><br>\n<p>\n Last Modified: 02/02/2025<br>\nModified by: David&nbsp;E&nbsp;Evans</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nRecent advances in artificial intelligence and machine learning have vastly improved computational reasoning over complex domains. When machine learning models are exposed to adversarial behavior, however, the systems built upon them can be fooled, evaded, and misled in ways that can have profound security implications. As more critical systems employ machine learningfrom financial systems to self-driving cars to network monitoring toolsit is vitally important that we develop the rigorous scientific techniques needed to make machine learning more robust to attack. This nascent field, which we call trustworthy machine learning, is currently fragmented across several research communities including machine learning, security, statistics, and theoretical computer science. The focus of the NSF Center for Trustworthy Machine Learning was to develop a rigorous understanding of the vulnerabilities inherent to machine learning, and to develop the tools, metrics, and methods to mitigate them. \n\nThe main outcomes of the UVA team's contributions to the project include developing new methods for evaluating the robustness of machine learning systems and advancing the theoretical foundations of robust machine learning. Our work resulted in theoretical results showing the limits of intrinsic robustness, and practical attacks and auditing methods for measuring the robustness of a machine learning system to both training (poisoning attacks) and inference time attacks. We also developed a new approach to understanding privacy risks in a machine learning system, and developed the first formal notion of distribution inference privacy. We introduced state-of-the-art attacks to audit distribution inference and showed that traditional methods for improving privacy are not effective when the training distribution itself is sensitive. Our work resulted in theoretical advances to understand the intrinsic robustness of data distributions, publications in research conferences in security, privacy, and machine learning, and in open source tools for the research community and industrial practitioners to use to audit machine learning systems.\t\t\t\t\tLast Modified: 02/02/2025\n\n\t\t\t\t\tSubmitted by: DavidEEvans\n"
 }
}