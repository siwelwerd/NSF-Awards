{
 "awd_id": "1763349",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: Medium: Collaborative Research: Foundations of Fair Data Analysis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2018-06-12",
 "awd_max_amd_letter_date": "2021-08-04",
 "awd_abstract_narration": "Machine learning algorithms increasingly make or inform critical decisions that affect peoples' every day lives. For instance, algorithms make decisions pertaining to hiring, college admissions, credit card and mortgage approvals, sentencing and parole of the incarcerated, first-responder deployment, and what advertisements and search results a user sees on the internet. An attractive feature is that these algorithms can efficiently process large amounts of data in making these decisions, thus hopefully improving economic and social efficiency. Because such decisions are so consequential, their fairness has become a matter of increasing concern. It has been argued that automation, by removing the human element, guarantees fairness, but this is not so -- several empirical studies have demonstrated that automation is no panacea. Further, the reasons for unfairness and discrimination can be complex and non-obvious. This project will study the frictions that may cause unfairness in algorithmic decision making, and the costs of mitigating unfairness -- that is, quantitative trade-offs between fairness and other desiderata, including accuracy, computational efficiency, and economic efficiency. \r\n\r\nSpecifically, this project will study frictions to fairness arising from several factors. There may not be sufficient data about minority populations. There can be feedback loops arising from the fact that observations can only be made on an individual if a risky action is taken, e.g., the person is granted a loan, or hired. Decision makers can be myopic, choosing to maximize short-term gains rather than exploring riskier options that may pay off in the long run. Economic frictions include self-confirming equilibria---differing subjective perceptions of opportunities leading to choices by individuals and communities which sustain those perceptions, and competition among classifiers (for example, credit agencies) leading to less accurate qualifiers in equilibrium. Finally, the problem of finding fair and accurate classifiers can be computationally intractable. This project will seek ways to mitigate the unfairness arising from these frictions. It will study the cost of incentivizing myopic agents to explore and examine the short-term costs of such incentives, and their long-term impact on fairness. It will also seek to design computationally tractable classifiers that achieve provably good approximations for fairness and accuracy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mallesh",
   "pi_last_name": "Pai",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mallesh Pai",
   "pi_email_addr": "Mallesh.Pai@rice.edu",
   "nsf_id": "000570733",
   "pi_start_date": "2018-06-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "William Marsh Rice University",
  "inst_street_address": "6100 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "Houston",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7133484820",
  "inst_zip_code": "770051827",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TX09",
  "org_lgl_bus_name": "WILLIAM MARSH RICE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "K51LECU1G8N3"
 },
 "perf_inst": {
  "perf_inst_name": "William Marsh Rice University",
  "perf_str_addr": "6100 MAIN ST",
  "perf_city_name": "Houston",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "770051827",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "TX09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7932",
   "pgm_ref_txt": "COMPUT GAME THEORY & ECON"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 63168.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 124177.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 62655.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project ended up producing 3 major strands of research.</p>\n<p>The first was a robust Machine Learning/ AI tool for prediction and uncertainty quantification with associated accuracy/ fairness guarantees (i.e., that the predictions are accurate for possibly large sets of subgroups). As an example consider making predictions about medical risk for a population. This tool can make accurate predictions as a function of available data, with guarantees that it is accurate not just for the population as a whole, but also, e.g., for all men, women, different races, different ages and other subpopulations of interest. We showed that this tool has both robust theoretical guarantees and is usable and easy to deploy in practice. We apply this to accurate predictions, uncertainty quantification, variance quantification and also newer applications such as \"calibeating\", i.e. taking a set of given forecasts and algorithmically improving on it.&nbsp;</p>\n<p>The second is algorithimic pricing and risks of algorithmic collusion. Many firms now use algorithms to price their products. A question that then arises is: if these firms' algorithms are interacting/ competing in the market, what is the outcome and how is that different from when humans price/ compete. We showed the possibility of algorithimic collusion, i.e., the possibility of neutrally coded independent algorithms among competing firms learning collusive-seeming prices. Our work in this area won the best paper from the Marketing Science Retail and Pricing Special Interest Group. It is also the source of much regulatory interest.&nbsp;</p>\n<p>&nbsp;</p>\n<p>The final, ongoing thread concerns evaluating the fairness of current prediction/ classficiation algorithms. Currently works view the data generating process as fixed/ exogenous and evaluate whether the algorithm is fair based on this assumption: for example, the gold standard is what is known as an outcome test--- compare the (true, realized) outcomes for two groups with respect to whom you want the outcome to be fair. For example, if you want to evaluate the fairness of a recidivism prediction algorithm across the race of subjects, compare the prediction to the actual outcome, and see if there are differences in the algorithm's correctness for different races.&nbsp; An issue with this is that the data generating process in some settings might not be exogenous, i.e., agents may themselves be strategic trying to get a certain classification from the algorithm. We study how to evaluate fairness in such settings and show how such evaluation differs. In particular, if the two groups are different in their ability to be strategic, then this needs to be taken into account.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/03/2023<br>\n\t\t\t\t\tModified by: Mallesh&nbsp;Pai</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project ended up producing 3 major strands of research.\n\nThe first was a robust Machine Learning/ AI tool for prediction and uncertainty quantification with associated accuracy/ fairness guarantees (i.e., that the predictions are accurate for possibly large sets of subgroups). As an example consider making predictions about medical risk for a population. This tool can make accurate predictions as a function of available data, with guarantees that it is accurate not just for the population as a whole, but also, e.g., for all men, women, different races, different ages and other subpopulations of interest. We showed that this tool has both robust theoretical guarantees and is usable and easy to deploy in practice. We apply this to accurate predictions, uncertainty quantification, variance quantification and also newer applications such as \"calibeating\", i.e. taking a set of given forecasts and algorithmically improving on it. \n\nThe second is algorithimic pricing and risks of algorithmic collusion. Many firms now use algorithms to price their products. A question that then arises is: if these firms' algorithms are interacting/ competing in the market, what is the outcome and how is that different from when humans price/ compete. We showed the possibility of algorithimic collusion, i.e., the possibility of neutrally coded independent algorithms among competing firms learning collusive-seeming prices. Our work in this area won the best paper from the Marketing Science Retail and Pricing Special Interest Group. It is also the source of much regulatory interest. \n\n \n\nThe final, ongoing thread concerns evaluating the fairness of current prediction/ classficiation algorithms. Currently works view the data generating process as fixed/ exogenous and evaluate whether the algorithm is fair based on this assumption: for example, the gold standard is what is known as an outcome test--- compare the (true, realized) outcomes for two groups with respect to whom you want the outcome to be fair. For example, if you want to evaluate the fairness of a recidivism prediction algorithm across the race of subjects, compare the prediction to the actual outcome, and see if there are differences in the algorithm's correctness for different races.  An issue with this is that the data generating process in some settings might not be exogenous, i.e., agents may themselves be strategic trying to get a certain classification from the algorithm. We study how to evaluate fairness in such settings and show how such evaluation differs. In particular, if the two groups are different in their ability to be strategic, then this needs to be taken into account. \n\n\t\t\t\t\tLast Modified: 11/03/2023\n\n\t\t\t\t\tSubmitted by: Mallesh Pai"
 }
}