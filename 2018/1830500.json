{
 "awd_id": "1830500",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: INT: COLLAB: Shared Autonomy for Unstructured Underwater Environments through Vision and Language",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Juan Wachs",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 511549.0,
 "awd_amount": 511549.0,
 "awd_min_amd_letter_date": "2018-08-31",
 "awd_max_amd_letter_date": "2018-08-31",
 "awd_abstract_narration": "Existing underwater robotic systems typically provide one of two operating modes---full teleoperation or full autonomy. Teleoperation is by far the most common, particularly for tasks involving interaction with the environment, such as grasping and manipulation. Autonomy is restricted to non-contact survey missions and to controlled laboratory settings. The ability to operate between teleoperation and autonomy will improve the efficiency and effectiveness of tasks performed in underwater environments. This research will develop and evaluate a novel shared autonomy framework. The research leverages the different nature of humans and robots. This work will reduce the need for multiple, highly trained operators. It has the potential to drastically reduce the cost of underwater missions. The contributions of this research will impact the way in which humans work together with robots within a wide variety of applications, including space exploration, disaster relief, and assistive robotics.\r\n\r\nAs robotic systems play an ever-larger role as our surrogates for marine science and exploration, the ability to leverage the complementary nature of humans and robots becomes critical for scientific discovery. This research will develop new models and algorithms that exploit multiple non-commensurate sensing and control modalities to realize intelligent shared autonomy in complex unstructured environments. Novel to this research is the use of natural language and vision as complementary forms of weak supervision to enable robots to learn human-collaborative sensorimotor manipulation policies opportunistically from narrated human demonstrations. Fundamental to these methods is their ability to then refine these policies in situ based upon interaction with a human operator. Together, these models and algorithms will enhance the efficiency and effectiveness of underwater scientific exploration.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Camilli",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Richard Camilli",
   "pi_email_addr": "rcamilli@whoi.edu",
   "nsf_id": "000324462",
   "pi_start_date": "2018-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Woods Hole Oceanographic Institution",
  "inst_street_address": "266 WOODS HOLE RD",
  "inst_street_address_2": "",
  "inst_city_name": "WOODS HOLE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "5082893542",
  "inst_zip_code": "025431535",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "MA09",
  "org_lgl_bus_name": "WOODS HOLE OCEANOGRAPHIC INSTITUTION",
  "org_prnt_uei_num": "",
  "org_uei_num": "GFKFBWG2TV98"
 },
 "perf_inst": {
  "perf_inst_name": "Woods Hole Oceanographic Institution",
  "perf_str_addr": "266 Woods Hole Road",
  "perf_city_name": "Woods Hole",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "025431535",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "MA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 511549.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project sought to produce a set of novel algorithms that enable intelligent shared autonomy by learning shared representations for sensor, natural language, and manipulator commands for grasping and intervention in complex unstructured underwater environments. These capabilities provide a mechanism for automating tasks that currently require multiple highly trained operators and have the potential to drastically reduce the cost and increase the accessibility of underwater remotely operated and human-occupied vehicle missions, and the efficiency and effectiveness of manipulation within unstructured environments. Furthermore, learning policies that improve the autonomy of human-controlled underwater manipulators can bridge the divide between teleoperated robotic platforms and fully autonomous underwater vehicles. This enables autonomous systems to work in coordination with human-controlled vehicles, which currently do not operate in the same workspace simultaneously.</p>\n<p>This research resulted This project produced novel algorithms that enable intelligent shared autonomy by learning shared representations for sensor, natural language, and manipulator commands for grasping and intervention in complex unstructured underwater environments. Through this work the researchers also developed new methods for shared autonomy, 3D scene reconstruction, object detection, natural language understanding, and manipulation. These efforts culminated in the development and demonstration of the SHared Autonomy for Remote Collaboration (SHARC) framework which enables remote operators to plan and control robotic underwater manipulation tasks from anywhere in the world by using natural language speech and hand gestures through a virtual reality headset or desktop interface. SHARC was demonstrated during an oceanographic dive mission with the Nereus Under Ice (NUI) vehicle. During this mission in the San Pedro Basin of the Eastern Pacific Ocean, remote scientists operated the NUI vehicle from thousands of kilometers away using SHARC&rsquo;s VR and desktop interfaces. The team collaboratively collected a physical push core sample and recorded in situ x-ray fluorescence measurements of seafloor microbial mats and sediments at water depths exceeding 1km, despite being physically located across the United States.</p>\n<p>SHARC improves user performance benchmarks over conventional control methods by distributing tasks between the robot and the human operator based on their respective strengths. While the robot&rsquo;s autonomy system can complete tasks such as trajectory planning and control, SHARC relies on human input for data analysis, selecting scientific samples, and high-level goal planning, which are particularly difficult to automate in the natural unstructured environments typically encountered during deep ocean science missions. As our experimental results with SHARC indicate, this shared-autonomy approach can improve operational efficiency and robustness.</p>\n<p>SHARC renders the robot arm&rsquo;s planned trajectory and intended actions prior to execution and within context of its surrounding environment (e.g., a 3D workspace reconstruction along with the location and label of detected tools), thereby making its behavior more predictable than conventional interfaces such as the topside controller. Distributing tasks between the operator and the robot also enables users to focus on high-level scientific objectives while offloading low-level control tasks to the robot, which should reduce operators&rsquo; cognitive load during operation.</p>\n<p>SHARC enables use of natural language speech and gestures to convey high-level objectives to the robot. Complex commands that would be time-consuming and difficult to execute with conventional controllers can be succinctly communicated with language. Within a matter of seconds, users can specify a task that then takes the robot several minutes to execute. In addition to reducing the cognitive load required of the operator, the intuitive nature of natural language speech and gestures minimizes the training required for operation and makes SHARC accessible to a diverse population of users. These natural input modalities also have the benefit of remaining functional during intermittent, low-bandwidth, and high-latency communication, thereby reducing telemetry requirements and enabling participation from remote users with only rudimentary Internet access.</p>\n<p>Expanding shore-based access for remote scientists to observe and control robotic sampling processes can increase the number of scientists engaged in the expedition, while reducing barriers to participation (e.g., physical ability, experience, and geographic location). This is well suited for K&ndash;12 and undergraduate education and outreach for natural science, robotics and computer science domains, particularly among historically underrepresented groups in STEM disciplines. Experimental results highlight SHARC&rsquo;s utility in enabling delicate operations in unstructured environments under bandwidth-limited conditions, which may be extensible to other sensitive domains where dexterity is required (e.g., nuclear decommissioning, deep space operations, unexploded ordnance/disposed military munition remediation). SHARC&rsquo;s reduced bandwidth requirement also facilitates real-time remote collaboration among shore-side users and enables it to scale up to support multiple simultaneous operations with additional operators and instrumentation. This can increase operations tempo by parallelizing sampling tasks and data analysis.</p>\n<p>Readers are encouraged to learn more about this research program. The following reference and URL provide a peer-reviewed journal publication and video overview, respectively, describing this research.</p>\n<p>&nbsp;</p>\n<p>Phung, A., Billings, G., Daniele, A. F., Walter, M. R., &amp; Camilli, R. (2023). Enhancing scientific exploration of the deep sea through shared autonomy in remote manipulation. <em>Science Robotics</em>, <em>8</em>(81), eadi5227.</p>\n<p>&nbsp;</p>\n<p>https://vimeo.com/813523071/93fcec8d4e</p><br>\n<p>\n Last Modified: 12/29/2023<br>\nModified by: Richard&nbsp;Camilli</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project sought to produce a set of novel algorithms that enable intelligent shared autonomy by learning shared representations for sensor, natural language, and manipulator commands for grasping and intervention in complex unstructured underwater environments. These capabilities provide a mechanism for automating tasks that currently require multiple highly trained operators and have the potential to drastically reduce the cost and increase the accessibility of underwater remotely operated and human-occupied vehicle missions, and the efficiency and effectiveness of manipulation within unstructured environments. Furthermore, learning policies that improve the autonomy of human-controlled underwater manipulators can bridge the divide between teleoperated robotic platforms and fully autonomous underwater vehicles. This enables autonomous systems to work in coordination with human-controlled vehicles, which currently do not operate in the same workspace simultaneously.\n\n\nThis research resulted This project produced novel algorithms that enable intelligent shared autonomy by learning shared representations for sensor, natural language, and manipulator commands for grasping and intervention in complex unstructured underwater environments. Through this work the researchers also developed new methods for shared autonomy, 3D scene reconstruction, object detection, natural language understanding, and manipulation. These efforts culminated in the development and demonstration of the SHared Autonomy for Remote Collaboration (SHARC) framework which enables remote operators to plan and control robotic underwater manipulation tasks from anywhere in the world by using natural language speech and hand gestures through a virtual reality headset or desktop interface. SHARC was demonstrated during an oceanographic dive mission with the Nereus Under Ice (NUI) vehicle. During this mission in the San Pedro Basin of the Eastern Pacific Ocean, remote scientists operated the NUI vehicle from thousands of kilometers away using SHARCs VR and desktop interfaces. The team collaboratively collected a physical push core sample and recorded in situ x-ray fluorescence measurements of seafloor microbial mats and sediments at water depths exceeding 1km, despite being physically located across the United States.\n\n\nSHARC improves user performance benchmarks over conventional control methods by distributing tasks between the robot and the human operator based on their respective strengths. While the robots autonomy system can complete tasks such as trajectory planning and control, SHARC relies on human input for data analysis, selecting scientific samples, and high-level goal planning, which are particularly difficult to automate in the natural unstructured environments typically encountered during deep ocean science missions. As our experimental results with SHARC indicate, this shared-autonomy approach can improve operational efficiency and robustness.\n\n\nSHARC renders the robot arms planned trajectory and intended actions prior to execution and within context of its surrounding environment (e.g., a 3D workspace reconstruction along with the location and label of detected tools), thereby making its behavior more predictable than conventional interfaces such as the topside controller. Distributing tasks between the operator and the robot also enables users to focus on high-level scientific objectives while offloading low-level control tasks to the robot, which should reduce operators cognitive load during operation.\n\n\nSHARC enables use of natural language speech and gestures to convey high-level objectives to the robot. Complex commands that would be time-consuming and difficult to execute with conventional controllers can be succinctly communicated with language. Within a matter of seconds, users can specify a task that then takes the robot several minutes to execute. In addition to reducing the cognitive load required of the operator, the intuitive nature of natural language speech and gestures minimizes the training required for operation and makes SHARC accessible to a diverse population of users. These natural input modalities also have the benefit of remaining functional during intermittent, low-bandwidth, and high-latency communication, thereby reducing telemetry requirements and enabling participation from remote users with only rudimentary Internet access.\n\n\nExpanding shore-based access for remote scientists to observe and control robotic sampling processes can increase the number of scientists engaged in the expedition, while reducing barriers to participation (e.g., physical ability, experience, and geographic location). This is well suited for K12 and undergraduate education and outreach for natural science, robotics and computer science domains, particularly among historically underrepresented groups in STEM disciplines. Experimental results highlight SHARCs utility in enabling delicate operations in unstructured environments under bandwidth-limited conditions, which may be extensible to other sensitive domains where dexterity is required (e.g., nuclear decommissioning, deep space operations, unexploded ordnance/disposed military munition remediation). SHARCs reduced bandwidth requirement also facilitates real-time remote collaboration among shore-side users and enables it to scale up to support multiple simultaneous operations with additional operators and instrumentation. This can increase operations tempo by parallelizing sampling tasks and data analysis.\n\n\nReaders are encouraged to learn more about this research program. The following reference and URL provide a peer-reviewed journal publication and video overview, respectively, describing this research.\n\n\n\n\n\nPhung, A., Billings, G., Daniele, A. F., Walter, M. R., & Camilli, R. (2023). Enhancing scientific exploration of the deep sea through shared autonomy in remote manipulation. Science Robotics, 8(81), eadi5227.\n\n\n\n\n\nhttps://vimeo.com/813523071/93fcec8d4e\t\t\t\t\tLast Modified: 12/29/2023\n\n\t\t\t\t\tSubmitted by: RichardCamilli\n"
 }
}