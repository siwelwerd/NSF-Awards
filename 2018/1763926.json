{
 "awd_id": "1763926",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI:Medium:Collaborative Research:Developing a uniform meaning representation for natural language processing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 399237.0,
 "awd_amount": 399237.0,
 "awd_min_amd_letter_date": "2018-07-23",
 "awd_max_amd_letter_date": "2018-07-23",
 "awd_abstract_narration": "The use of intelligent agents that can communicate with us in human language has become an essential part of our daily lives.  Today's intelligent agents can respond appropriately to many things we say or text to them, but they cannot yet communicate fully like humans. They lack our general ability to arrive quickly at accurate and relevant interpretations of what others communicate to us and to form appropriate responses, particularly in sustained interactions.  The typical way we teach a machine to acquire such ability is to provide it with approximations of the meanings of utterances in the contexts in which they have occurred in the past.  Over the years these approximations have become increasingly rich and detailed, enabling ever more sophisticated systems for interacting with computers using natural language, such as searching for information, getting up-to-date recommendations for products and services, and translating foreign languages.  The goal of this project is to bring together linguists and computer scientists to jointly develop a practical meaning representation formalism based on these rich approximations that can be applied to a much more diverse set of languages.   This will allow us to use machine learning to develop techniques to automatically translate human utterances into our meaning formalism. In turn, this will enable intelligent agents to acquire more advanced communication capabilities, and for a wider range of languages.  The languages considered for the project include those spoken by large populations such as English, Chinese and Arabic, as well as native tongues of smaller groups such as Norwegian, and Arapaho and Kukama-Kukamira, two indigenous languages of the Americas.  As such, this project will help bring modern technology to smaller groups so that all people can benefit equally from technological advancement.  The project will also contribute to the development of the US workforce by training a new generation of researchers on cutting-edge technologies in artificial intelligence.  \r\n\r\nThis project brings together an interdisciplinary team of linguists and computer scientists from three institutions to jointly develop a Uniform Meaning Representation (UMR). UMR is a practical, formal, computationally tractable, and cross-linguistically valid meaning representation of natural language that can impact a wide range of downstream applications requiring deep natural language understanding (NLU).  UMR will extend existing meaning representations to include quantifier types and relations, modality, negation, tense and aspect, and be tested on a typologically diverse set of languages.  Methods and techniques for UMR annotation, parsing and generation, and evaluation will be uniform across languages.  The project will also develop novel algorithms and models for UMR-based broad-coverage and general-purpose multilingual semantic parsers.  Students participating in the project will receive training in the full cycle of conceptualizing, producing, processing, and consuming meaning representations at the sites of participating institutions.  This project will help to build a community of NLP researchers that will contribute to the development of UMR-based data and tools and advance the state of the art in Natural Language Processing (NLP) in particular, and Artificial Intelligence (AI) in general.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nianwen",
   "pi_last_name": "Xue",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nianwen Xue",
   "pi_email_addr": "xuen@cs.brandeis.edu",
   "nsf_id": "000509758",
   "pi_start_date": "2018-07-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Pustejovsky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "James Pustejovsky",
   "pi_email_addr": "pustejovsky@gmail.com",
   "nsf_id": "000369969",
   "pi_start_date": "2018-07-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brandeis University",
  "inst_street_address": "415 SOUTH ST",
  "inst_street_address_2": "",
  "inst_city_name": "WALTHAM",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "7817362121",
  "inst_zip_code": "024532728",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "BRANDEIS UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MXLZGAMFEKN5"
 },
 "perf_inst": {
  "perf_inst_name": "Brandeis University",
  "perf_str_addr": "415 South Street, MS-116",
  "perf_city_name": "Waltham",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "024532728",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 399237.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-c5e6dfd1-7fff-53c7-8f67-a2a20489e125\"> </span></p>\n<p><span id=\"docs-internal-guid-8f004750-7fff-4b26-2fc2-0f2b8be02d08\"> </span></p>\n<p dir=\"ltr\"><span>Intelligent agents have increasingly become part of our everyday lives, and they can now respond appropriately to many things that we say or text to them. Many of them do this by first translating human language into a meaning formalism that can be understood or executed by the computer. As this meaning formalism gets richer and more detailed, intelligent agents are capable of doing more things that are valuable to humans, from answering our questions about the weather to playing the music that we want, from providing information about flights to translating one language into another.&nbsp;</span></p>\n<p>The goal of the Uniform Meaning Representation (UMR) project was to develop a meaning formalism that can mediate communication between humans and computers in a wide range of languages of the world. Developing a meaning formalism that all languages can translate into is a complicated undertaking, however. It has required expertise from both computer scientists and linguists who understand the similarities and differences between the world's languages. Towards this goal, researchers from Brandeis University, the University of Colorado at Boulder, and the University of New Mexico jointly developed the Uniform Meaning Representation in partnership with an international group of scientists.&nbsp;</p>\n<p dir=\"ltr\"><span>The researchers started from a popular existing meaning formalism called Abstraction Meaning Representation (AMR).&nbsp; They enriched AMR to cover a wider range of meanings, and generalized it so that it can work for a comprehensive set&nbsp; of the world's languages. In this process, they tested UMR on diverse languages that include those spoken by large populations such as Arabic, Chinese, and English, as well as native languages of smaller groups such as </span><span>Arapaho, Kukama-Kukamiria, Navajo, and Sanapana. As part of this effort, they organized workshops to invite feedback from fellow researchers on the representation, and held tutorials at computational linguistics conferences to disseminate the research. They also developed tools that fellow researchers can use to produce UMRs for their own languages.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/28/2023<br>\n\t\t\t\t\tModified by: Nianwen&nbsp;Xue</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \n\n \nIntelligent agents have increasingly become part of our everyday lives, and they can now respond appropriately to many things that we say or text to them. Many of them do this by first translating human language into a meaning formalism that can be understood or executed by the computer. As this meaning formalism gets richer and more detailed, intelligent agents are capable of doing more things that are valuable to humans, from answering our questions about the weather to playing the music that we want, from providing information about flights to translating one language into another. \n\nThe goal of the Uniform Meaning Representation (UMR) project was to develop a meaning formalism that can mediate communication between humans and computers in a wide range of languages of the world. Developing a meaning formalism that all languages can translate into is a complicated undertaking, however. It has required expertise from both computer scientists and linguists who understand the similarities and differences between the world's languages. Towards this goal, researchers from Brandeis University, the University of Colorado at Boulder, and the University of New Mexico jointly developed the Uniform Meaning Representation in partnership with an international group of scientists. \nThe researchers started from a popular existing meaning formalism called Abstraction Meaning Representation (AMR).  They enriched AMR to cover a wider range of meanings, and generalized it so that it can work for a comprehensive set  of the world's languages. In this process, they tested UMR on diverse languages that include those spoken by large populations such as Arabic, Chinese, and English, as well as native languages of smaller groups such as Arapaho, Kukama-Kukamiria, Navajo, and Sanapana. As part of this effort, they organized workshops to invite feedback from fellow researchers on the representation, and held tutorials at computational linguistics conferences to disseminate the research. They also developed tools that fellow researchers can use to produce UMRs for their own languages.\n\n\n\n \n \n\n\t\t\t\t\tLast Modified: 04/28/2023\n\n\t\t\t\t\tSubmitted by: Nianwen Xue"
 }
}