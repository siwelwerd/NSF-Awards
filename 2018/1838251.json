{
 "awd_id": "1838251",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Collaborative Research: Mining for Patterns in Graphs and High-Dimensional Data: Achieving the Limits",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 737645.0,
 "awd_amount": 737645.0,
 "awd_min_amd_letter_date": "2018-09-10",
 "awd_max_amd_letter_date": "2018-09-10",
 "awd_abstract_narration": "While modern datasets are very large, the amount of information per variable is often relatively small. This includes datasets from genomics, social networks, and many applications in machine learning and artificial intelligence. For instance, in genomics we often track hundreds of thousands of genes, but only have a few hundred independent samples for each one. Similarly, online social networks are massive, but the structure of friendships only gives us a relatively small amount of data per individual. This kind of data is called \"high-dimensional\", and poses new challenges for mathematics, statistics, and computer science, especially when (as with all real data) they are noisy or incomplete. This project will identify exactly when and how it is mathematically possible to find patterns in these massive but noisy datasets, giving scientists across many fields a useful guide to how much data they need to draw reliable conclusions, and to develop new algorithms that will solve modern data science problems efficiently and optimally.\r\n\r\nThrough the study of community detection, noisy graph isomorphism, and matrix/tensor factorization, this project will develop a general framework to 1) locate the information-theoretic limit below which the observation is too noisy to detect the underlying pattern, or even to tell if a pattern exists; 2) devise efficient algorithms that succeed all the way down to the lowest possible signal-to-noise ratio; 3) prove that important classes of algorithms need super-polynomial time in certain hard regimes.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Cristopher",
   "pi_last_name": "Moore",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Cristopher D Moore",
   "pi_email_addr": "moore@santafe.edu",
   "nsf_id": "000193005",
   "pi_start_date": "2018-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Santa Fe Institute",
  "inst_street_address": "1399 HYDE PARK RD",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA FE",
  "inst_state_code": "NM",
  "inst_state_name": "New Mexico",
  "inst_phone_num": "5059462727",
  "inst_zip_code": "875018943",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "NM03",
  "org_lgl_bus_name": "SANTA FE INSTITUTE OF SCIENCE",
  "org_prnt_uei_num": "",
  "org_uei_num": "M8SBQ7NVNAH4"
 },
 "perf_inst": {
  "perf_inst_name": "Santa Fe Institute",
  "perf_str_addr": "1399 Hyde Park Road",
  "perf_city_name": "Santa Fe",
  "perf_st_code": "NM",
  "perf_st_name": "New Mexico",
  "perf_zip_code": "875018943",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "NM03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 737645.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The world is full of data, but this data is noisy and imperfect. Computer scientists, physicists, and statisticians all try to find patterns in noisy data &mdash; to extract the signal, or &ldquo;ground truth&rdquo;, from the noise. This could mean finding communities in a social network, or trends and patterns in large tables of numbers, or predicting the spread of an epidemic.</p>\n<p>These problems get harder when the data gets noisier. There is a mathematical analogy between noise and heat: in physics, hotter systems are noisier and more random. In many physical systems, their behavior jumps suddenly when the temperature crosses a precise threshold: ice crystals suddenly melt, magnets suddenly lose their magnetic fields, and so on. In the same way, it can suddenly become impossible to find signals in data when the amount of noise crosses a threshold.</p>\n<p>In computer science, there are two different kinds of thresholds that can occur. At one level of noise, it might become very difficult to find the signal: rather than an efficient algorithm that can quickly zero in on it, it might become necessary to search through all possible signals to see which one fits. The problem is that the number of possible signals is enormous &mdash; exponentially large &mdash; so doing this takes an astronomical amount of time. At an even higher level of noise, the signal can become completely lost, so that even this exhaustive search would fail. This creates three zones in the difficulty of these problems as the noise increases, where finding signals is easy, and then computationally hard, and then impossible.</p>\n<p>Our project uses ideas from physics to find these thresholds, and to design algorithms that find signals in data, as accurately and efficiently as possible, all the way up to the point where no algorithm can succeed. Our methods work well both in theory and in practice. They can be used to find patterns in &ldquo;matrices&rdquo; or &ldquo;tensors&rdquo; (higher-dimensional tables of numbers), to calculate the threshold at which a disease will jump from small outbreaks to a large epidemic, to figure out which links in a network are the most important to the spread of a disease, and to detect important changes in a network&rsquo;s structure. Our interdisciplinary approach, building bridges between physics and computer science, promises to solve many problems in future &mdash; and to figure out when problems become unsolvable.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/26/2024<br>\nModified by: Cristopher&nbsp;D&nbsp;Moore</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe world is full of data, but this data is noisy and imperfect. Computer scientists, physicists, and statisticians all try to find patterns in noisy data  to extract the signal, or ground truth, from the noise. This could mean finding communities in a social network, or trends and patterns in large tables of numbers, or predicting the spread of an epidemic.\n\n\nThese problems get harder when the data gets noisier. There is a mathematical analogy between noise and heat: in physics, hotter systems are noisier and more random. In many physical systems, their behavior jumps suddenly when the temperature crosses a precise threshold: ice crystals suddenly melt, magnets suddenly lose their magnetic fields, and so on. In the same way, it can suddenly become impossible to find signals in data when the amount of noise crosses a threshold.\n\n\nIn computer science, there are two different kinds of thresholds that can occur. At one level of noise, it might become very difficult to find the signal: rather than an efficient algorithm that can quickly zero in on it, it might become necessary to search through all possible signals to see which one fits. The problem is that the number of possible signals is enormous  exponentially large  so doing this takes an astronomical amount of time. At an even higher level of noise, the signal can become completely lost, so that even this exhaustive search would fail. This creates three zones in the difficulty of these problems as the noise increases, where finding signals is easy, and then computationally hard, and then impossible.\n\n\nOur project uses ideas from physics to find these thresholds, and to design algorithms that find signals in data, as accurately and efficiently as possible, all the way up to the point where no algorithm can succeed. Our methods work well both in theory and in practice. They can be used to find patterns in matrices or tensors (higher-dimensional tables of numbers), to calculate the threshold at which a disease will jump from small outbreaks to a large epidemic, to figure out which links in a network are the most important to the spread of a disease, and to detect important changes in a networks structure. Our interdisciplinary approach, building bridges between physics and computer science, promises to solve many problems in future  and to figure out when problems become unsolvable.\n\n\n\t\t\t\t\tLast Modified: 01/26/2024\n\n\t\t\t\t\tSubmitted by: CristopherDMoore\n"
 }
}