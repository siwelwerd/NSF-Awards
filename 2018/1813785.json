{
 "awd_id": "1813785",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Sparse Predictive Coding for Energy Efficient Visual Navigation in Dynamic Environments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2018-08-15",
 "awd_max_amd_letter_date": "2018-08-15",
 "awd_abstract_narration": "This project develops efficient machine vision algorithms inspired by the architecture and energetic efficiency of the primate visual system for motion processing. Navigating through a rich cluttered natural environment, while both the observer and the objects in the scene are moving, is a difficult problem in machine vision, particularly for real-time processing under power constraints. However, humans and other animals perform these tasks with ease. The nervous system is under tight metabolic constraints and this leads to incredibly efficient representations of important environmental features, such as the observer's heading, the depth of objects, and the motion of objects.  In addition, these efficient machine vision algorithms can be applied to robotics, the IoT, and edge processing.  The algorithms can be applied to a wide range of applications, including augmented reality, assistive robotics, autonomous vehicles, and the Internet of Things (IoT) Thus, they could have a transformative economic and societal impact by creating applications that can operate autonomously over long periods in remote locations.\r\n\r\nInspired by ability of the nervous system to efficiently encode and appropriately respond to the visual features that make up a dynamic scene, the algorithm uses sparse predictive coding techniques to process data streams from cameras. Because the algorithms can be realized in spiking neural networks, where the artificial neurons only send signals when an event occurs, they can run efficiently on low powered neuromorphic systems; computers that support such representations.  By employing an architecture inspired by the brain, where op-down signals from the frontal cortex and parietal cortex predict where objects will be in the future, the system will have better object tracking and overcome difficulties when objects become hidden from view.  These representations are sparse and reduced, leading to energy efficient processing, less computation, and thus low power consumption.  In summary, the machine vision algorithms: (1) increase our understanding of how the brain encodes behaviorally relevant signals in the world, (2) lead to computationally efficient handling of large data streams, and (3) realize power efficient processing for a wide range of embedded applications.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Krichmar",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey L Krichmar",
   "pi_email_addr": "jkrichma@uci.edu",
   "nsf_id": "000099941",
   "pi_start_date": "2018-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Charless",
   "pi_last_name": "Fowlkes",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Charless Fowlkes",
   "pi_email_addr": "charless.fowlkes@gmail.com",
   "nsf_id": "000505333",
   "pi_start_date": "2018-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "SBSG 2220",
  "perf_city_name": "Irvine",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926975100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project developed efficient machine vision algorithms inspired by the architecture and energetic efficiency of the primate visual system for motion processing. The algorithms were applied to a wide range of applications, including artificial intelligence, autonomous robots, and computer vision. Through our work, we demonstrated how the brain represents complex information with minimal neural processing. The nervous system is under tight metabolic constraints, and this leads to incredibly efficient representations of important environmental features, such as the observer&rsquo;s heading, the depth of objects, and the motion of objects. These representations are sparse and reduced, leading to energy efficient processing, less computation, and thus low power consumption. It may explain why the nervous system uses so much less energy that artificial systems. This could have an impact on practical applications, such as LLMs, self-driving vehicles, and devices that do not rely on the Internet cloud.</p>\n<p>Navigating through a rich cluttered natural environment, while both the observer and the objects in the scene are moving, is a difficult problem in machine vision, particularly for real-time processing under power constraints. However, humans and other animals perform these tasks with ease. In addition, top-down signals from higher-order brain areas, such as the frontal cortex and parietal cortex, can predict where objects will be in the future. This can lead to better object tracking and overcoming difficulties when objects become hidden from view. We developed algorithms that could accurately separate objects from the background and predict where that object should be when they move behind barriers (e.g., predicting where a car will be when it exits a tunnel). &nbsp;</p>\n<p>During our daily moving around the world, we are using our eyes to take note of objects arounds us place ourselves on a global map of our location.&nbsp; This becomes more apparent when we are in unfamiliar environments where we might rely on a map to find our way around but use our vision to link what we are seeing to locations on the map. Interestingly, there is little experimental data on this important aspect of cognition.&nbsp; Initial neural network modeling suggests a method on how the brain might achieve these functions. In the future, we will test these ideas in people and animal models.</p>\n<p>In general, the work from this project has been inspired by ability of the nervous system to efficiently encode and appropriately respond to the visual features that make up a dynamic scene. The project outcomes may lead to widely available products and applications. One direct outcome is the development of a spiking neural network simulation framework that is available for AI developers and scientists who are not proficient in computer science but want to develop models. These efficient machine vision algorithms developed on this project can be applied to robots, and devices at the edge, where they may not be an Internet connection or a power source. Thus, they could have a transformative economic and societal impact by creating applications that can operate autonomously over long periods in remote locations.</p>\n<p>In summary, the vision algorithms developed here: (1) have increased our understanding of how the brain encodes behaviorally relevant signals in the world, (2) have led to computationally efficient handling of large datasets, and (3) realized power efficient processing for a wide range of embedded applications.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/14/2023<br>\nModified by: Jeffrey&nbsp;L&nbsp;Krichmar</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702579082236_Fig_Navigation--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702579082236_Fig_Navigation--rgov-800width.png\" title=\"Neurorobotic navigation.\"><img src=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702579082236_Fig_Navigation--rgov-66x44.png\" alt=\"Neurorobotic navigation.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Using visual perspectives to navigate dynamic environments. The robot's camera provides an on the ground view and satellite imagery provides an overhead view.  Robot can add data to map. Overhead can help robot navigate. Project inspired by work on this grant.</div>\n<div class=\"imageCredit\">J. Krichmar</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jeffrey&nbsp;L&nbsp;Krichmar\n<div class=\"imageTitle\">Neurorobotic navigation.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702578558596_Fig_PerspectiveTaking--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702578558596_Fig_PerspectiveTaking--rgov-800width.png\" title=\"Linking top-down and first-person perspectives.\"><img src=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702578558596_Fig_PerspectiveTaking--rgov-66x44.png\" alt=\"Linking top-down and first-person perspectives.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A. Variational autoencoder (VAE) to reconstruct top-down view from first-person view. B. VAE to reconstruct first-person view from top-down view. C. Robot simulation with an overhead camera and robot view (inset in upper left). The robot is in the upper right corner. D1 and D2. Head direction respon</div>\n<div class=\"imageCredit\">J. Krichmar</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jeffrey&nbsp;L&nbsp;Krichmar\n<div class=\"imageTitle\">Linking top-down and first-person perspectives.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702579290359_chen_mstd--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702579290359_chen_mstd--rgov-800width.png\" title=\"Cortical Motion Perception Emerges from Dimensionality Reduction with Evolved Spike-Timing-Dependent Plasticity Rules\"><img src=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702579290359_chen_mstd--rgov-66x44.png\" alt=\"Cortical Motion Perception Emerges from Dimensionality Reduction with Evolved Spike-Timing-Dependent Plasticity Rules\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Using the parameter tuning feature of our CARLsim simulator, we showed how the visual system learns to represent motion patterns efficiently by neurobiologically inspired local plasticity rules.</div>\n<div class=\"imageCredit\">J. Krichmar and K. Chen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jeffrey&nbsp;L&nbsp;Krichmar\n<div class=\"imageTitle\">Cortical Motion Perception Emerges from Dimensionality Reduction with Evolved Spike-Timing-Dependent Plasticity Rules</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702578750820_Fig_CRCNS--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702578750820_Fig_CRCNS--rgov-800width.png\" title=\"Collaborative project inspired by work in this grant.\"><img src=\"/por/images/Reports/POR/2023/1813785/1813785_10571226_1702578750820_Fig_CRCNS--rgov-66x44.png\" alt=\"Collaborative project inspired by work in this grant.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Project includes modeling led by Jeff Krichmar\ufffds group, human experiments led by Liz Chrasti\ufffds group, and rodent experiments led by Doug Nitz\ufffds group. Understanding how animals link different spatial perspectives.</div>\n<div class=\"imageCredit\">J. Krichmar</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jeffrey&nbsp;L&nbsp;Krichmar\n<div class=\"imageTitle\">Collaborative project inspired by work in this grant.</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project developed efficient machine vision algorithms inspired by the architecture and energetic efficiency of the primate visual system for motion processing. The algorithms were applied to a wide range of applications, including artificial intelligence, autonomous robots, and computer vision. Through our work, we demonstrated how the brain represents complex information with minimal neural processing. The nervous system is under tight metabolic constraints, and this leads to incredibly efficient representations of important environmental features, such as the observers heading, the depth of objects, and the motion of objects. These representations are sparse and reduced, leading to energy efficient processing, less computation, and thus low power consumption. It may explain why the nervous system uses so much less energy that artificial systems. This could have an impact on practical applications, such as LLMs, self-driving vehicles, and devices that do not rely on the Internet cloud.\n\n\nNavigating through a rich cluttered natural environment, while both the observer and the objects in the scene are moving, is a difficult problem in machine vision, particularly for real-time processing under power constraints. However, humans and other animals perform these tasks with ease. In addition, top-down signals from higher-order brain areas, such as the frontal cortex and parietal cortex, can predict where objects will be in the future. This can lead to better object tracking and overcoming difficulties when objects become hidden from view. We developed algorithms that could accurately separate objects from the background and predict where that object should be when they move behind barriers (e.g., predicting where a car will be when it exits a tunnel). \n\n\nDuring our daily moving around the world, we are using our eyes to take note of objects arounds us place ourselves on a global map of our location. This becomes more apparent when we are in unfamiliar environments where we might rely on a map to find our way around but use our vision to link what we are seeing to locations on the map. Interestingly, there is little experimental data on this important aspect of cognition. Initial neural network modeling suggests a method on how the brain might achieve these functions. In the future, we will test these ideas in people and animal models.\n\n\nIn general, the work from this project has been inspired by ability of the nervous system to efficiently encode and appropriately respond to the visual features that make up a dynamic scene. The project outcomes may lead to widely available products and applications. One direct outcome is the development of a spiking neural network simulation framework that is available for AI developers and scientists who are not proficient in computer science but want to develop models. These efficient machine vision algorithms developed on this project can be applied to robots, and devices at the edge, where they may not be an Internet connection or a power source. Thus, they could have a transformative economic and societal impact by creating applications that can operate autonomously over long periods in remote locations.\n\n\nIn summary, the vision algorithms developed here: (1) have increased our understanding of how the brain encodes behaviorally relevant signals in the world, (2) have led to computationally efficient handling of large datasets, and (3) realized power efficient processing for a wide range of embedded applications.\n\n\n\t\t\t\t\tLast Modified: 12/14/2023\n\n\t\t\t\t\tSubmitted by: JeffreyLKrichmar\n"
 }
}