{
 "awd_id": "1763665",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: Medium: Collaborative Proposal: Foundations of Adaptive Data Analysis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2018-03-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 286000.0,
 "awd_amount": 286000.0,
 "awd_min_amd_letter_date": "2018-02-14",
 "awd_max_amd_letter_date": "2020-04-25",
 "awd_abstract_narration": "Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. \r\n\r\nThe technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of \"over-fitting\" and \"false discovery.\" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Cynthia",
   "pi_last_name": "Dwork",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Cynthia Dwork",
   "pi_email_addr": "dwork@seas.harvard.edu",
   "nsf_id": "000732659",
   "pi_start_date": "2018-02-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "33 Oxford Street",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382933",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 109142.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 87720.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 89138.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-96a2a0c1-7fff-9cca-a411-eb6e584035d8\"> </span></p>\n<p dir=\"ltr\">This project has advanced the science of adaptive, also known as exploratory, data analysis, which was the primary research goal.&nbsp; Prior to this project, Co-PIs Dwork and Roth, together with other collaborators, in 2014, established that differential privacy provides a universal protection against the risks to validity incurred by adaptivity in data analysis.&nbsp; They did this through the establishment of transfer theorems which convert the privacy and accuracy guarantees of an arbitrary differentially private algorithm to distributional accuracy guarantees for queries posed to the dataset.&nbsp; Intuitively, the result says that differentially private access to a dataset prevents an &ldquo;accuracy adversary&rdquo; from formulating a query whose value on the dataset differs substantially from its value on the population as a whole.&nbsp; They used these ideas to construct the first reusable holdout set.&nbsp; Later, co-PI Smith, with other collaborators, introduced new techniques leading to improved transfer theorems.&nbsp; Because of the tight connection to differential privacy, the development of differentially private algorithms for new tasks and improved differentially private algorithms for previously studied tasks, contribute to the literature on adaptive data analysis.</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>During the course of this grant, co-PI Roth and collaborators obtained the current state-of-the-art transfer theorem for linear queries.&nbsp; Co-PI Smith, Thakkar, and collaborators developed a software package for answering linear queries on a holdout set with concrete confidence intervals, allowing for safe but &ldquo;optimistic&rdquo; answering of linear queries, where the system either provides a tight confidence interval or certifies that a highly adaptive query was posed.&nbsp; </span><span>The project also gave rise to several advances in privacy-protective and adaptive multiple-hypothesis testing, as well as the development of new relaxations of differential privacy, specifically, t-CDP (truncated Differential Privacy), and f-Differential Privacy and Gaussian Differential Privacy, which use the entire trade-off between type I and type II errors of the hypothesis testing problem as a measure of the privacy guarantees.&nbsp;&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Differential privacy is a stability guarantee &ndash; it says that the distribution on outcomes of any analysis will remain essentially unchanged under small perturbations of the dataset.&nbsp; The project also developed new notions of stability leading to improved results in generalization theory over traditional stability notions and a characterization of optimal simple hypothesis tests that are algorithmically stable.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Because adaptivity is such a natural source of honest mistakes that completely destroy validity, formulating a benign adversary that is less dangerous than a malicious adversary has proved challenging: there are some very simple counterexamples.&nbsp; Instead, Co-PI Dwork </span><span>formulated data models </span><span>that are realistic in some computational settings, such as streaming analysis of DNA, under which it is possible to &ldquo;re-use&rdquo; the privacy budget to obtain more accurate results than when the data generation and access models are unrestricted.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>A connection, established elsewhere, between differential privacy and robustness in machine learning led the project to the study of robustness and adversarial training of neural nets.&nbsp; Here, developments include an emerging theory addressing questions of why and when gradient method based adversarial training works, how to quantify the trade-off between adversarial&nbsp;</span>robustness and prediction accuracy, and how to further use out-of domain data to enhance adversarial robustness, as well progress in analyzing why popular modern data augmentation&nbsp;techniques help in adversarial robustness and generalization in learning. In addition, research in representation learning has explored how adversarial training and data augmentation techniques such as contrastive learning encourages more transferable and expressive representations.&nbsp; &nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p><span id=\"docs-internal-guid-96a2a0c1-7fff-9cca-a411-eb6e584035d8\">A different direction yielded several results in the theory of algorithmic fairness.</span></p>\n<p>Finally and importantly, the project supported the development of courses on adaptive data analysis at the University of Pennsylvania, Boston University, and Harvard University, and course materials are now available at <a href=\"https://adaptivedataanalysis.com/about/\">About &ndash; The Algorithmic Foundations of Adaptive Data Analysis</a>.</p>\n<p><span id=\"docs-internal-guid-96a2a0c1-7fff-9cca-a411-eb6e584035d8\"> </span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/19/2021<br>\n\t\t\t\t\tModified by: Cynthia&nbsp;Dwork</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis project has advanced the science of adaptive, also known as exploratory, data analysis, which was the primary research goal.  Prior to this project, Co-PIs Dwork and Roth, together with other collaborators, in 2014, established that differential privacy provides a universal protection against the risks to validity incurred by adaptivity in data analysis.  They did this through the establishment of transfer theorems which convert the privacy and accuracy guarantees of an arbitrary differentially private algorithm to distributional accuracy guarantees for queries posed to the dataset.  Intuitively, the result says that differentially private access to a dataset prevents an \"accuracy adversary\" from formulating a query whose value on the dataset differs substantially from its value on the population as a whole.  They used these ideas to construct the first reusable holdout set.  Later, co-PI Smith, with other collaborators, introduced new techniques leading to improved transfer theorems.  Because of the tight connection to differential privacy, the development of differentially private algorithms for new tasks and improved differentially private algorithms for previously studied tasks, contribute to the literature on adaptive data analysis.\n\n \nDuring the course of this grant, co-PI Roth and collaborators obtained the current state-of-the-art transfer theorem for linear queries.  Co-PI Smith, Thakkar, and collaborators developed a software package for answering linear queries on a holdout set with concrete confidence intervals, allowing for safe but \"optimistic\" answering of linear queries, where the system either provides a tight confidence interval or certifies that a highly adaptive query was posed.  The project also gave rise to several advances in privacy-protective and adaptive multiple-hypothesis testing, as well as the development of new relaxations of differential privacy, specifically, t-CDP (truncated Differential Privacy), and f-Differential Privacy and Gaussian Differential Privacy, which use the entire trade-off between type I and type II errors of the hypothesis testing problem as a measure of the privacy guarantees.  \n\n \nDifferential privacy is a stability guarantee &ndash; it says that the distribution on outcomes of any analysis will remain essentially unchanged under small perturbations of the dataset.  The project also developed new notions of stability leading to improved results in generalization theory over traditional stability notions and a characterization of optimal simple hypothesis tests that are algorithmically stable.\n\n \nBecause adaptivity is such a natural source of honest mistakes that completely destroy validity, formulating a benign adversary that is less dangerous than a malicious adversary has proved challenging: there are some very simple counterexamples.  Instead, Co-PI Dwork formulated data models that are realistic in some computational settings, such as streaming analysis of DNA, under which it is possible to \"re-use\" the privacy budget to obtain more accurate results than when the data generation and access models are unrestricted. \n\n \nA connection, established elsewhere, between differential privacy and robustness in machine learning led the project to the study of robustness and adversarial training of neural nets.  Here, developments include an emerging theory addressing questions of why and when gradient method based adversarial training works, how to quantify the trade-off between adversarial robustness and prediction accuracy, and how to further use out-of domain data to enhance adversarial robustness, as well progress in analyzing why popular modern data augmentation techniques help in adversarial robustness and generalization in learning. In addition, research in representation learning has explored how adversarial training and data augmentation techniques such as contrastive learning encourages more transferable and expressive representations.    \n\n \n\nA different direction yielded several results in the theory of algorithmic fairness.\n\nFinally and importantly, the project supported the development of courses on adaptive data analysis at the University of Pennsylvania, Boston University, and Harvard University, and course materials are now available at About &ndash; The Algorithmic Foundations of Adaptive Data Analysis.\n\n \n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/19/2021\n\n\t\t\t\t\tSubmitted by: Cynthia Dwork"
 }
}