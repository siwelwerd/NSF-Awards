{
 "awd_id": "1763702",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 299510.0,
 "awd_amount": 299510.0,
 "awd_min_amd_letter_date": "2018-06-28",
 "awd_max_amd_letter_date": "2021-08-17",
 "awd_abstract_narration": "Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. \r\n\r\nThe specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes \"coded computing\", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Georgios-Alex",
   "pi_last_name": "Dimakis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Georgios-Alex Dimakis",
   "pi_email_addr": "dimakis@austin.utexas.edu",
   "nsf_id": "000515168",
   "pi_start_date": "2018-06-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "2501 Speedway",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121077",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 71994.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 73877.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 75816.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 77823.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Deep learning models are transforming numerous data science applications like image recognition, automatic translation and autonomous driving. This project studied the use of distributed computation, and coding theoretic techniques for accelerating machine learning training and inference.&nbsp;This project focused on mitigating bottlenecks of distributed machine learning and finding low-dimensional representations and algorithms for training and inference. The first part of the work proposed \"coded computing\", a transformative framework that leverages coding theory and applied it to a distributed computing framework to inject computational redundancy. The second part of this work focused on compressed sensing methods using information theory and coding theoretic techniques.&nbsp;</p>\n<p><br />Another significant outcome of this project was the development of gradient coding techniques. Gradient coding is a method for straggler mitigation in distributed learning. This project designed novel gradient codes using tools from classical coding theory, namely, cyclic MDS codes, which compare favorably with prior work, both in the applicable range of parameters and in the complexity of the involved algorithms. Second, an approximate variant of the gradient coding problem was introduced: In this case we settle for approximate gradient computation instead of the exact one. This approach enables graceful degradation, i.e., the mean squared error of the approximate gradient is a decreasing function of the number of stragglers. Our main result is that normalized adjacency matrices of expander graphs yield excellent approximate gradient codes, which enable significantly less computation compared to exact gradient coding, and guarantee faster convergence than trivial solutions under standard assumptions. Our experimental results on real cloud computing platforms showed that the generalization error of approximate gradient coding is very close to the full gradient while requiring significantly less computation from the cloud worker machines.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/06/2023<br>\n\t\t\t\t\tModified by: Georgios-Alex&nbsp;Dimakis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDeep learning models are transforming numerous data science applications like image recognition, automatic translation and autonomous driving. This project studied the use of distributed computation, and coding theoretic techniques for accelerating machine learning training and inference. This project focused on mitigating bottlenecks of distributed machine learning and finding low-dimensional representations and algorithms for training and inference. The first part of the work proposed \"coded computing\", a transformative framework that leverages coding theory and applied it to a distributed computing framework to inject computational redundancy. The second part of this work focused on compressed sensing methods using information theory and coding theoretic techniques. \n\n\nAnother significant outcome of this project was the development of gradient coding techniques. Gradient coding is a method for straggler mitigation in distributed learning. This project designed novel gradient codes using tools from classical coding theory, namely, cyclic MDS codes, which compare favorably with prior work, both in the applicable range of parameters and in the complexity of the involved algorithms. Second, an approximate variant of the gradient coding problem was introduced: In this case we settle for approximate gradient computation instead of the exact one. This approach enables graceful degradation, i.e., the mean squared error of the approximate gradient is a decreasing function of the number of stragglers. Our main result is that normalized adjacency matrices of expander graphs yield excellent approximate gradient codes, which enable significantly less computation compared to exact gradient coding, and guarantee faster convergence than trivial solutions under standard assumptions. Our experimental results on real cloud computing platforms showed that the generalization error of approximate gradient coding is very close to the full gradient while requiring significantly less computation from the cloud worker machines. \n\n\t\t\t\t\tLast Modified: 04/06/2023\n\n\t\t\t\t\tSubmitted by: Georgios-Alex Dimakis"
 }
}