{
 "awd_id": "1750760",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Compiler and Runtime Support for Multi-Tasking on Commodity GPUs",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032920000",
 "po_email": "doliveir@nsf.gov",
 "po_sign_block_name": "Daniela Oliveira",
 "awd_eff_date": "2018-05-01",
 "awd_exp_date": "2024-04-30",
 "tot_intn_awd_amt": 501545.0,
 "awd_amount": 517545.0,
 "awd_min_amd_letter_date": "2018-01-09",
 "awd_max_amd_letter_date": "2022-06-30",
 "awd_abstract_narration": "General-purpose Graphics Processing Units (GPU) computing has become mainstream, as witnessed in various domains such as machine learning, graph analytics, and scientific simulation. One notable trend is employing GPUs in data centers and cloud computing infrastructures to satisfy users' increasing demand to accelerate their applications. In such multi-tasking environments, applications from different users contend to use the shared GPU, leading to unpredictable and unacceptable performance degradation. This CAREER project aims at developing a set of compiler and runtime techniques to support multi-tasking on commodity GPUs in a transparent and efficient manner. The compiler techniques circumvent the hardware limitations to enable a set of features, such as preemption, and the runtime system schedules applications to utilize the potential of the GPU and guarantees quality of service. In addition, the investigator advances GPU education in the University to target both Computer Science (CS) and non-CS students based on a GPU education center.\r\n\r\nSpecifically, the project investigates how to integrate compiler and runtime techniques to support multi-tasking on GPUs by building a system that achieves three goals. First, the system addresses GPU core contention by enabling flexible GPU kernel preemption. The compiler transforms the GPU program to be a preemptable form by circumventing the limitation imposed by the hardware thread scheduler. The runtime intercepts all GPU kernel launch requests and makes global preemption and scheduling decisions to maximize performance. Second, the system supports fine-grained sharing for threads from different applications to fully utilize hardware resources within GPU streaming multi-processors. The runtime guarantees the QoS of user-facing applications while optimizing overall throughput aided by performance prediction. Third, the system addresses GPU memory contention by coordinating GPU memory transfers, which considers memory access patterns and array reuse patterns.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bo",
   "pi_last_name": "Wu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bo Wu",
   "pi_email_addr": "bwu@mines.edu",
   "nsf_id": "000676262",
   "pi_start_date": "2018-01-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Colorado School of Mines",
  "inst_street_address": "1500 ILLINOIS ST",
  "inst_street_address_2": "",
  "inst_city_name": "GOLDEN",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3032733000",
  "inst_zip_code": "804011887",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "CO07",
  "org_lgl_bus_name": "TRUSTEES OF THE COLORADO SCHOOL OF MINES",
  "org_prnt_uei_num": "JW2NGMP4NMA3",
  "org_uei_num": "JW2NGMP4NMA3"
 },
 "perf_inst": {
  "perf_inst_name": "Colorado School of Mines",
  "perf_str_addr": "1610 Illinois St.",
  "perf_city_name": "Golden",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "804011833",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "CO07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 92331.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 96748.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 219705.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 108761.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-3720ac18-7fff-701e-750a-bf4b52d92d20\">\n<p dir=\"ltr\"><span>This project aimed to develop compiler and runtime support for efficiently running multiple tasks on commodity GPUs. As GPUs are increasingly used for domain-specific applications like graph processing and deep learning, the ability to effectively share GPU resources among concurrent workloads has become critical. The research focused on two key application domains: graph workloads and deep learning model serving.</span></p>\n<p dir=\"ltr\"><span>For graph workloads, the project developed novel compiler and runtime techniques to optimize the co-execution of multiple graph queries on a single GPU. A prototype GPU-based graph mining system was implemented to study performance bottlenecks. The research found that naively co-running queries could lead to over 2x performance degradation due to factors like lack of data sharing awareness and query serialization. The project designed a compiler to merge query execution plans and maximize data reuse, along with a runtime scheduler to allocate GPU cores across queries. These optimizations improved performance by up to 30% for mergeable queries and increased overall GPU utilization by 13%. Additionally, the project explored efficient training of large-scale Graph Neural Networks (GNNs) on a single GPU. By implementing a two-stage framework that uses out-of-core feature streaming and subgraph sampling, the project enabled training of GNNs on graphs with over 1 billion edges using just 24GB of GPU memory, albeit with some performance trade-offs compared to distributed training.</span></p>\n<p dir=\"ltr\"><span>In the deep learning domain, the project collaborated with Microsoft to tackle challenges in serving multiple large natural language processing models on GPUs with limited memory. The researchers developed new model pruning techniques tailored for modern GPU architectures, particularly leveraging online pruning capabilities in NVIDIA GPUs. This enabled fitting and efficiently serving multiple real-world NLP models on a single GPU while minimizing accuracy loss. Overall, the project advanced understanding of GPU multi-tasking for domain-specific workloads and developed practical compiler and runtime solutions to improve resource utilization and application performance. The research findings were integrated into graduate and undergraduate courses, training the next generation of high-performance computing experts.</span></p>\n</span></p><br>\n<p>\n Last Modified: 08/30/2024<br>\nModified by: Bo&nbsp;Wu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nThis project aimed to develop compiler and runtime support for efficiently running multiple tasks on commodity GPUs. As GPUs are increasingly used for domain-specific applications like graph processing and deep learning, the ability to effectively share GPU resources among concurrent workloads has become critical. The research focused on two key application domains: graph workloads and deep learning model serving.\n\n\nFor graph workloads, the project developed novel compiler and runtime techniques to optimize the co-execution of multiple graph queries on a single GPU. A prototype GPU-based graph mining system was implemented to study performance bottlenecks. The research found that naively co-running queries could lead to over 2x performance degradation due to factors like lack of data sharing awareness and query serialization. The project designed a compiler to merge query execution plans and maximize data reuse, along with a runtime scheduler to allocate GPU cores across queries. These optimizations improved performance by up to 30% for mergeable queries and increased overall GPU utilization by 13%. Additionally, the project explored efficient training of large-scale Graph Neural Networks (GNNs) on a single GPU. By implementing a two-stage framework that uses out-of-core feature streaming and subgraph sampling, the project enabled training of GNNs on graphs with over 1 billion edges using just 24GB of GPU memory, albeit with some performance trade-offs compared to distributed training.\n\n\nIn the deep learning domain, the project collaborated with Microsoft to tackle challenges in serving multiple large natural language processing models on GPUs with limited memory. The researchers developed new model pruning techniques tailored for modern GPU architectures, particularly leveraging online pruning capabilities in NVIDIA GPUs. This enabled fitting and efficiently serving multiple real-world NLP models on a single GPU while minimizing accuracy loss. Overall, the project advanced understanding of GPU multi-tasking for domain-specific workloads and developed practical compiler and runtime solutions to improve resource utilization and application performance. The research findings were integrated into graduate and undergraduate courses, training the next generation of high-performance computing experts.\n\t\t\t\t\tLast Modified: 08/30/2024\n\n\t\t\t\t\tSubmitted by: BoWu\n"
 }
}