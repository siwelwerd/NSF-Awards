{
 "awd_id": "1816732",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS:Small: Improved Cross-Subject Cognitive and Emotional State Classification  Using Functional Near-Infrared Spectroscopy Data for Deep Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 494374.0,
 "awd_amount": 494374.0,
 "awd_min_amd_letter_date": "2018-08-14",
 "awd_max_amd_letter_date": "2019-10-31",
 "awd_abstract_narration": "New advances in bio-technology suggest devices to wear and measure the brain will be available to support many different activities. Future technologies may use brain activity data to adapt or customize educational software in real-time. Activity support based on interpreted brain activity could be used to reduce mental workload, modify emotional states, or help someone with post-traumatic stress disorder. However, brain activity data is complex and difficult to interpret. This project will use deep machine learning methods to overcome the challenge of classifying and interpreting brain activity data using real-time data from participants. The objective is to harness the tremendous potential of cognitive sensors and computational methods to help individuals function more effectively.  \r\n\r\nAlthough many early successes were achieved using machine learning on brain data, several notable challenges have arisen, which significantly limit the impacts of these early successes. The technical approach in this project has three research thrusts. The investigators will develop models specifically for use on high density functional-near infrared spectroscopy (fNIRS) data. Thrust 1 involves the development of advanced deep learning techniques that are particularly well-suited for fNIRS data, and address spatial and temporal inter-relations. Thrust 2 involves development and adaptation of algorithm transparency (AT) techniques that are well-suited to shed light on brain dynamics embedded within the deep learning model structures. This will help the research team interpret the underlying structure of the models, with respect to brain spatial and temporal dynamics at the individual and group level. Thrust 3 collates the model and AT techniques developed in the prior thrusts and evaluates them using an extensive cross-subject and cross-participant fNIRS dataset. Using this data for evaluation purposes, the research team will work together to interpret results to improve upon classifier performance and model generalizability.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Senem",
   "pi_last_name": "Velipasalar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Senem Velipasalar",
   "pi_email_addr": "svelipas@syr.edu",
   "nsf_id": "000111075",
   "pi_start_date": "2019-09-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Leanne",
   "pi_last_name": "Hirshfield",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Leanne Hirshfield",
   "pi_email_addr": "leanne.hirshfield@colorado.edu",
   "nsf_id": "000689086",
   "pi_start_date": "2018-08-14",
   "pi_end_date": "2019-09-19"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Kalish",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Kalish",
   "pi_email_addr": "mlkalish@syr.edu",
   "nsf_id": "000122156",
   "pi_start_date": "2018-08-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Senem",
   "pi_last_name": "Velipasalar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Senem Velipasalar",
   "pi_email_addr": "svelipas@syr.edu",
   "nsf_id": "000111075",
   "pi_start_date": "2018-08-14",
   "pi_end_date": "2019-09-19"
  }
 ],
 "inst": {
  "inst_name": "Syracuse University",
  "inst_street_address": "900 S CROUSE AVE",
  "inst_street_address_2": "",
  "inst_city_name": "SYRACUSE",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "3154432807",
  "inst_zip_code": "13244",
  "inst_country_name": "United States",
  "cong_dist_code": "22",
  "st_cong_dist_code": "NY22",
  "org_lgl_bus_name": "SYRACUSE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "C4BXLBC11LC6"
 },
 "perf_inst": {
  "perf_inst_name": "Syracuse University",
  "perf_str_addr": "215 University Place",
  "perf_city_name": "Syracuse",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "132102100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "22",
  "perf_st_cong_dist": "NY22",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 494374.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>In Human Computer Interaction (HCI), optimum human performance can be achieved with systems that help users maintain an ideal level of workload. Too little workload can result in low productivity, boredom and complacency, while too much workload can cause human error, shedding of tasks, and frustration. Thus,&nbsp;real-time predictions of workload can be used to build adaptive systems that can regulate users' workload.&nbsp;</span>These systems would benefit not only from information about overall workload, but by information gleaned from taking a more fine-grained view of workload by differentiating between the load on one's perceptual and working memory resources. This way, an adaptive system could change the modality by which support is presented, based on information about the auditory or visual perceptual load (VPL) of the person.&nbsp;</p>\n<p><span><span><span>Predicting workload using physiological sensors has taken on a diffuse set of methods. However, the majority of these methods train models on small datasets, with small numbers of channel locations on the brain, limiting their ability to transfer across participants, tasks, or experimental sessions.&nbsp;<span>We developed a new method of modeling a large, cross-participant and cross-session set of high density functional near infrared spectroscopy (fNIRS) data by using an approach grounded in cognitive load theory and employing a Bi-Directional Gated Recurrent Unit incorporating attention mechanism and self-supervised label augmentation. We evaluated our model using leave-one-participant-out and 10-fold cross validation, across 22 participants, and showed that our model can classify different levels of working memory load (WML) and VPL across participants. Importantly, we leverage a multi-label classification scheme, where our models are trained to predict simultaneously occurring levels of WML and VPL.&nbsp;</span></span></span></span></p>\n<p>fNIRS data introduces additional challenges for cognitive workload classification. High variations in inter-subject fNIRS data as well as intra-subject data captured during different sessions need to be addressed. Intra-subject variances can occur due to differences in sensor placement across different sessions. Even within the same session, head motion, body motion or noise due to the light source can cause difficulties.&nbsp;Block-wise experiments are common for collecting fNIRS data, where participants perform a series of trails that are repeated in blocks. In each trail, participants are asked to do one specific task. To address high variance issues, we developed a new method, which views different blocks from same/different subjects as different domains and employs contrastive learning (CL).&nbsp;<span>We have performed experiments and compared our approach with three state-of-the-art baselines on three publicly available datasets. The results show that our proposed CL method improves the performance of all the baselines.</span></p>\n<p>Studying the relationship between the brain and finger tapping motions can contribute towards an improved understanding of neuromuscular impairment. We have developed a promising approach for spatially descriptive multi-labeling of spatiotemporal fNIRS data to autonomously detect different finger tapping levels in different regions of the brain simultaneously. Our multi-class multi-labeling technique assigns labels to the left and right index fingers, and a given label describes one of three different finger tapping frequencies (rest, 80bpm, and 120bpm) to be monitored in the corresponding contralateral spatial location in the brain's motor cortex. We train a&nbsp;Convolutional Neural Network and Long Short Term Memory-based network to classify the finger tapping levels spatially and simultaneously. Promising testing results have been obtained with an average Hamming Loss of 0.185, average F-Score of 0.81, and average Accuracy of 0.81. Moreover, we explain our model and multi-labeling approach by generating Shapley Additive Explanation values and plotting them on an image-like background, which represents the fNIRS channel layout used as data input. Shapley values help to add interpretability to our deep learning model and by confirming expected results, offer a pathway to the future development of complex deep learning models that attempt to predict social-cognitive-affective states.</p>\n<p>Broader Impacts: Performing multi-label classification and detecting simultaneous activity in different regions of multidimensional data with spatiotemporal characteristics can be beneficial for a wide range of applications, including neuroscience and video surveillance.</p>\n<p>In general, integrating an algorithm, which can autonomously and simultaneously detect anomalies in different regions of interest captured through video can enhance security for various surveillance applications. Moreover, the ability to not only detect presence, but also describe the level of activity in a region offers an additional facet to the alert. Furthermore, detecting simultaneous and various activity levels in different parts of video frames can lead to an improvement in event annotation.&nbsp;</p>\n<p>As for the neuroscience domain, we can gain better understanding of the concurrent use of different functional cognitive brain regions. For example, a&nbsp;person who has high levels of WML while staring at a radar monitor could be assisted through his or her auditory channel, while a person with high WML with very low visual channel demands may benefit best by the assistance provided on his/her monitor.</p>\n<p>This project has resulted in 13 peer-reviewed publications in journals and conference proceedings.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/21/2023<br>\nModified by: Senem&nbsp;Velipasalar</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn Human Computer Interaction (HCI), optimum human performance can be achieved with systems that help users maintain an ideal level of workload. Too little workload can result in low productivity, boredom and complacency, while too much workload can cause human error, shedding of tasks, and frustration. Thus,real-time predictions of workload can be used to build adaptive systems that can regulate users' workload.These systems would benefit not only from information about overall workload, but by information gleaned from taking a more fine-grained view of workload by differentiating between the load on one's perceptual and working memory resources. This way, an adaptive system could change the modality by which support is presented, based on information about the auditory or visual perceptual load (VPL) of the person.\n\n\nPredicting workload using physiological sensors has taken on a diffuse set of methods. However, the majority of these methods train models on small datasets, with small numbers of channel locations on the brain, limiting their ability to transfer across participants, tasks, or experimental sessions.We developed a new method of modeling a large, cross-participant and cross-session set of high density functional near infrared spectroscopy (fNIRS) data by using an approach grounded in cognitive load theory and employing a Bi-Directional Gated Recurrent Unit incorporating attention mechanism and self-supervised label augmentation. We evaluated our model using leave-one-participant-out and 10-fold cross validation, across 22 participants, and showed that our model can classify different levels of working memory load (WML) and VPL across participants. Importantly, we leverage a multi-label classification scheme, where our models are trained to predict simultaneously occurring levels of WML and VPL.\n\n\nfNIRS data introduces additional challenges for cognitive workload classification. High variations in inter-subject fNIRS data as well as intra-subject data captured during different sessions need to be addressed. Intra-subject variances can occur due to differences in sensor placement across different sessions. Even within the same session, head motion, body motion or noise due to the light source can cause difficulties.Block-wise experiments are common for collecting fNIRS data, where participants perform a series of trails that are repeated in blocks. In each trail, participants are asked to do one specific task. To address high variance issues, we developed a new method, which views different blocks from same/different subjects as different domains and employs contrastive learning (CL).We have performed experiments and compared our approach with three state-of-the-art baselines on three publicly available datasets. The results show that our proposed CL method improves the performance of all the baselines.\n\n\nStudying the relationship between the brain and finger tapping motions can contribute towards an improved understanding of neuromuscular impairment. We have developed a promising approach for spatially descriptive multi-labeling of spatiotemporal fNIRS data to autonomously detect different finger tapping levels in different regions of the brain simultaneously. Our multi-class multi-labeling technique assigns labels to the left and right index fingers, and a given label describes one of three different finger tapping frequencies (rest, 80bpm, and 120bpm) to be monitored in the corresponding contralateral spatial location in the brain's motor cortex. We train aConvolutional Neural Network and Long Short Term Memory-based network to classify the finger tapping levels spatially and simultaneously. Promising testing results have been obtained with an average Hamming Loss of 0.185, average F-Score of 0.81, and average Accuracy of 0.81. Moreover, we explain our model and multi-labeling approach by generating Shapley Additive Explanation values and plotting them on an image-like background, which represents the fNIRS channel layout used as data input. Shapley values help to add interpretability to our deep learning model and by confirming expected results, offer a pathway to the future development of complex deep learning models that attempt to predict social-cognitive-affective states.\n\n\nBroader Impacts: Performing multi-label classification and detecting simultaneous activity in different regions of multidimensional data with spatiotemporal characteristics can be beneficial for a wide range of applications, including neuroscience and video surveillance.\n\n\nIn general, integrating an algorithm, which can autonomously and simultaneously detect anomalies in different regions of interest captured through video can enhance security for various surveillance applications. Moreover, the ability to not only detect presence, but also describe the level of activity in a region offers an additional facet to the alert. Furthermore, detecting simultaneous and various activity levels in different parts of video frames can lead to an improvement in event annotation.\n\n\nAs for the neuroscience domain, we can gain better understanding of the concurrent use of different functional cognitive brain regions. For example, aperson who has high levels of WML while staring at a radar monitor could be assisted through his or her auditory channel, while a person with high WML with very low visual channel demands may benefit best by the assistance provided on his/her monitor.\n\n\nThis project has resulted in 13 peer-reviewed publications in journals and conference proceedings.\n\n\n\t\t\t\t\tLast Modified: 12/21/2023\n\n\t\t\t\t\tSubmitted by: SenemVelipasalar\n"
 }
}