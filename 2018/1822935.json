{
 "awd_id": "1822935",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRI: CI-EN: Collaborative Research: mResearch: A platform for Reproducible and Extensible Mobile Sensor Big Data Research",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 299993.0,
 "awd_amount": 299993.0,
 "awd_min_amd_letter_date": "2018-09-17",
 "awd_max_amd_letter_date": "2018-09-17",
 "awd_abstract_narration": "The Center of Excellence for Mobile Sensor Data-to-Knowledge (MD2K) has developed open-source software for smart phones and cloud. Scientists use MD2K software to develop and test algorithms to monitor health, wellness, and work productivity via wearable sensors. The mResearch project is aimed at assisting Computer and Information Science and Engineering (CISE) researchers. The mResearch project will significantly enhance MD2K software and integrate Internet-of-Things (IoT) devices. The enhanced MD2K software will accelerate research in sensors design, mobile computing, privacy, analytics (especially machine learning and deep learning), and visualization. mResearch will enable CISE researchers to easily deploy their contributed software in scientific studies for health, smart homes, and workplace. The resulting discoveries and tools will help individuals improve their health, wellness, and work productivity.\r\n\r\nMD2K has developed open-source mobile sensor big data software platforms mCerebrum for smartphones and Cerebral Cortex for the cloud. This scalable and generalizable infrastructure is used for collecting, analyzing, and sharing high-frequency, mobile sensor data and associated labels in the context of scientific field studies. In particular, it supports the development and validation of models and algorithms for inferring markers of health, wellness, and productivity, and their associated risk factors. It has already been used at eleven sites across the country to collect over 300 terabytes of mobile sensor data in the field setting from over 2,000 participants. It has resulted in new computational models for the detection of conversation, smoking, eating, craving, stress, and cocaine use. The mResearch project is making five significant infrastructure enhancements to the MD2K infrastructure to assist CISE researchers in mobile sensor development, mobile computing, privacy, analytics, visualization, and participant engagement. First, it will enable data analytic workflow management across multiple layers of the system to enable reproducible and extensible experimentation. Second, it will allow encapsulation of data sources to provide convenient and responsible access to them in data analytic workflows. Third, it will facilitate cloud-assisted complex, real-time analytics for personalizing mobile interventions and improving engagement. Fourth, simulators will be developed with the ability to feed stored data into the platform at various points to enable research on system components and properties such as data compression, transfer and storage, as well as the scalability of data analytics. Finally, Internet-of-Things (IoT) devices and services will be integrated. With these five enhancements, the MD2K software will provide a complete, open, and modularized architecture. It will include all aspects of sensor data collection, data processing algorithms, cloud-based machine learning, and IoT integration. The enhanced MD2K software will facilitate reproducible and extensible CISE research with high-frequency mobile sensor data.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mani",
   "pi_last_name": "Srivastava",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Mani B Srivastava",
   "pi_email_addr": "mbs@ucla.edu",
   "nsf_id": "000468770",
   "pi_start_date": "2018-09-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vivek",
   "pi_last_name": "Shetty",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vivek Shetty",
   "pi_email_addr": "vshetty@ucla.edu",
   "nsf_id": "000645429",
   "pi_start_date": "2018-09-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "ECE Dept, 1762 Boelter Hall",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951594",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 299993.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-e5b541c7-7fff-8d20-183f-9e78a0c26b19\"> </span></p>\n<p dir=\"ltr\"><span>Activities in the multi-institutional collaborative mResearch project focused on software infrastructure for collecting, storing, analyzing, and visualizing high-frequency streaming sensor data from mobile and wearable devices. The project team built upon the mobile sensor big data cyberinfrastructure developed previously by the NIH MD2K Center to make the infrastructure usable and extensible for the computing research community while enhancing it to accommodate new mobile devices. Additionally, the project organized activities to broaden the research community&rsquo;s adoption of MD2K tools, methods, and resources. The UCLA team made the following contributions:</span></p>\n<p dir=\"ltr\"><strong>1. Broadening adoption of MD2K tools, methods, and resources</strong></p>\n<p dir=\"ltr\"><span>UCLA research team helped in preparing, nurturing, and supporting the national scientific research workforce of CISE researchers focused on mHealth. In collaboration with the NIH MD2K Center at U. Memphis, the UCLA team hosted (virtually during the pandemic) the annual NIH mHealth Training Institute (mHTI) targeting CS and mHealth researchers interested in practical healthcare solutions with societal impact, with over one hundred scholars being trained on relevant core computational and health sciences topics, as well as on MD2K tools. The researchers also established the </span><span>mHealthHUB (</span><a href=\"https://mhealth.md2k.org/\"><span>https://mhealth.md2k.org/</span></a><span>) dissemination platform, which curates all MD2K tools, source code with documentation, user manuals, educational materials, publications, webinars, etc., for access by a global audience of CISE/mHealth researchers.</span><span>&nbsp;</span></p>\n<p dir=\"ltr\"><strong>2. Auritus: An open-source optimization toolkit and dataset for training and development of human motion-based biomarkers on earables</strong></p>\n<p dir=\"ltr\"><span>In recent years, smart ear-worn devices, often referred to as earables, have emerged. They are equipped with various onboard sensors and algorithms, transforming earphones from simple audio transducers to multi-modal interfaces, making rich inferences about human motion and vital signals. UCLA researchers developed Auritus, an extendable and open-source optimization toolkit designed to (a) handle data collection, pre-processing, and labeling tasks for creating customized earable datasets for training machine learning models and (b) provide a tightly-integrated hardware-in-the-loop (HIL) optimizer and software suite for developing lightweight and real-time machine-learning (ML) models for activity detection, head-pose tracking, and other sensor-derived biomarkers. Auritus, whose architecture is shown in Figure 1, thus helps overcome significant barriers to developing sensory applications using earables, a cumbersome task requiring expertise in machine learning and embedded computing. The researchers released the Auritus system as open-source software (https://github.com/nesl/auritus</span><span>)</span><span>.</span></p>\n<p dir=\"ltr\"><strong>3. TinyOdom: Tools and dataset for human trajectory tracking on resource-constrained wearables</strong></p>\n<p dir=\"ltr\"><span>A valuable marker of human health is large-scale motion. Traditional step counting, double integration, and other model-based approaches lack enough fidelity. In recent years, deep inertial sequence learning has shown promise but is unsuitable for real-time deployment on ultra-resource-constrained (URC) devices. This is due to substantial memory, power, and compute bounds. Current deep inertial odometry techniques also suffer from gravity pollution, high-frequency inertial disturbances, varying sensor orientation, heading rate singularity, and failure in altitude estimation. UCLA researchers developed and released TinyOdom, an open-source software framework (</span><a href=\"https://github.com/nesl/tinyodom\"><span>https://github.com/nesl/tinyodom</span></a><span>) for training and deploying neural inertial models on extremely resource-constrained platforms.&nbsp; Figure 2 overviews the TinyOdom robust 3D inertial sequence learning approach.</span></p>\n<p dir=\"ltr\"><strong>4. Privacy-aware integration of sensory data sources</strong></p>\n<p dir=\"ltr\"><span>Integration of a wide array of data sources, both online connected devices, and previously collected and archived datasets, is foundational to the goals of the mResearch project. The UCLA researchers implemented several computational mechanisms that can responsibly do such integration so that the data source is provided with privacy assurance while maintaining the data&rsquo;s utility and quantifying the impact of any privacy measures on its quality. One such mechanism developed by the research team is PhysioGAN (Figure 3), which uses generative adversarial networks (GANs) and variational autoencoders (VAEs) to produce high-fidelity synthetic physiological sensor data readings.&nbsp; A second mechanism developed by the UCLA researchers uses an alternative approach of adversarial perturbation of the privacy-sensitive clinical time series sensor data with the twin objective of protecting the privacy of membership inference while meeting the quality of prediction on a specific downstream task.</span></p>\n<p dir=\"ltr\"><strong>5. X-CHAR: Concept-based&nbsp; explainable complex human activity recognition (HAR) models</strong></p>\n<p dir=\"ltr\"><span>Digital biomarkers involve the detection and classification of human activities from sensory data streams, often complex activities that unfold over a period of time and involve sequences of simpler events. With blackbox deep learning models often being employed in HAR, it becomes challenging to explain the predictions to the downstream users and thus affects user trust in the biomarker.&nbsp; UCLA researchers developed X-CHAR (eXplainable Complex Human Activity Recognition) model that, unlike prior work, doesn&rsquo;t require precise annotation of low-level activities, offers explanations in the form of human-understandable, high-level concepts while maintaining the robust performance of end-to-end deep learning models for time series data. X-CHAR learns to model complex activity recognition in the form of a sequence of concepts, and for each classification outputs a sequence of concepts and a counterfactual example as the explanation.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/06/2024<br>\nModified by: Mani&nbsp;B&nbsp;Srivastava</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709686021609_figure2--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709686021609_figure2--rgov-800width.png\" title=\"Figure 2\"><img src=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709686021609_figure2--rgov-66x44.png\" alt=\"Figure 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">TinyOdom System for large-scale motion trajectory on resource-constrained wearables</div>\n<div class=\"imageCredit\">Swapnil Saha (UCLA)</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mani&nbsp;B&nbsp;Srivastava\n<div class=\"imageTitle\">Figure 2</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709686075347_figure3--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709686075347_figure3--rgov-800width.png\" title=\"Figure 3\"><img src=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709686075347_figure3--rgov-66x44.png\" alt=\"Figure 3\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">PhysioGAN architecture for high fidelity conditional &#8232;generation of synthetic time-series physiological sensor data</div>\n<div class=\"imageCredit\">Mustafa Alzantot</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mani&nbsp;B&nbsp;Srivastava\n<div class=\"imageTitle\">Figure 3</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709685958560_figure1--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709685958560_figure1--rgov-800width.png\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709685958560_figure1--rgov-66x44.png\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Architecture of Auritus System for Earables</div>\n<div class=\"imageCredit\">Swapnil Saha (UCLA)</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mani&nbsp;B&nbsp;Srivastava\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709686169165_figure4--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709686169165_figure4--rgov-800width.png\" title=\"Figure 4\"><img src=\"/por/images/Reports/POR/2024/1822935/1822935_10583301_1709686169165_figure4--rgov-66x44.png\" alt=\"Figure 4\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">X-CHAR model for eXplainable Complex Human Activity Recognition</div>\n<div class=\"imageCredit\">J.Vikranth Jeyakumar</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mani&nbsp;B&nbsp;Srivastava\n<div class=\"imageTitle\">Figure 4</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nActivities in the multi-institutional collaborative mResearch project focused on software infrastructure for collecting, storing, analyzing, and visualizing high-frequency streaming sensor data from mobile and wearable devices. The project team built upon the mobile sensor big data cyberinfrastructure developed previously by the NIH MD2K Center to make the infrastructure usable and extensible for the computing research community while enhancing it to accommodate new mobile devices. Additionally, the project organized activities to broaden the research communitys adoption of MD2K tools, methods, and resources. The UCLA team made the following contributions:\n\n\n1. Broadening adoption of MD2K tools, methods, and resources\n\n\nUCLA research team helped in preparing, nurturing, and supporting the national scientific research workforce of CISE researchers focused on mHealth. In collaboration with the NIH MD2K Center at U. Memphis, the UCLA team hosted (virtually during the pandemic) the annual NIH mHealth Training Institute (mHTI) targeting CS and mHealth researchers interested in practical healthcare solutions with societal impact, with over one hundred scholars being trained on relevant core computational and health sciences topics, as well as on MD2K tools. The researchers also established the mHealthHUB (https://mhealth.md2k.org/) dissemination platform, which curates all MD2K tools, source code with documentation, user manuals, educational materials, publications, webinars, etc., for access by a global audience of CISE/mHealth researchers.\n\n\n2. Auritus: An open-source optimization toolkit and dataset for training and development of human motion-based biomarkers on earables\n\n\nIn recent years, smart ear-worn devices, often referred to as earables, have emerged. They are equipped with various onboard sensors and algorithms, transforming earphones from simple audio transducers to multi-modal interfaces, making rich inferences about human motion and vital signals. UCLA researchers developed Auritus, an extendable and open-source optimization toolkit designed to (a) handle data collection, pre-processing, and labeling tasks for creating customized earable datasets for training machine learning models and (b) provide a tightly-integrated hardware-in-the-loop (HIL) optimizer and software suite for developing lightweight and real-time machine-learning (ML) models for activity detection, head-pose tracking, and other sensor-derived biomarkers. Auritus, whose architecture is shown in Figure 1, thus helps overcome significant barriers to developing sensory applications using earables, a cumbersome task requiring expertise in machine learning and embedded computing. The researchers released the Auritus system as open-source software (https://github.com/nesl/auritus).\n\n\n3. TinyOdom: Tools and dataset for human trajectory tracking on resource-constrained wearables\n\n\nA valuable marker of human health is large-scale motion. Traditional step counting, double integration, and other model-based approaches lack enough fidelity. In recent years, deep inertial sequence learning has shown promise but is unsuitable for real-time deployment on ultra-resource-constrained (URC) devices. This is due to substantial memory, power, and compute bounds. Current deep inertial odometry techniques also suffer from gravity pollution, high-frequency inertial disturbances, varying sensor orientation, heading rate singularity, and failure in altitude estimation. UCLA researchers developed and released TinyOdom, an open-source software framework (https://github.com/nesl/tinyodom) for training and deploying neural inertial models on extremely resource-constrained platforms. Figure 2 overviews the TinyOdom robust 3D inertial sequence learning approach.\n\n\n4. Privacy-aware integration of sensory data sources\n\n\nIntegration of a wide array of data sources, both online connected devices, and previously collected and archived datasets, is foundational to the goals of the mResearch project. The UCLA researchers implemented several computational mechanisms that can responsibly do such integration so that the data source is provided with privacy assurance while maintaining the datas utility and quantifying the impact of any privacy measures on its quality. One such mechanism developed by the research team is PhysioGAN (Figure 3), which uses generative adversarial networks (GANs) and variational autoencoders (VAEs) to produce high-fidelity synthetic physiological sensor data readings. A second mechanism developed by the UCLA researchers uses an alternative approach of adversarial perturbation of the privacy-sensitive clinical time series sensor data with the twin objective of protecting the privacy of membership inference while meeting the quality of prediction on a specific downstream task.\n\n\n5. X-CHAR: Concept-based explainable complex human activity recognition (HAR) models\n\n\nDigital biomarkers involve the detection and classification of human activities from sensory data streams, often complex activities that unfold over a period of time and involve sequences of simpler events. With blackbox deep learning models often being employed in HAR, it becomes challenging to explain the predictions to the downstream users and thus affects user trust in the biomarker. UCLA researchers developed X-CHAR (eXplainable Complex Human Activity Recognition) model that, unlike prior work, doesnt require precise annotation of low-level activities, offers explanations in the form of human-understandable, high-level concepts while maintaining the robust performance of end-to-end deep learning models for time series data. X-CHAR learns to model complex activity recognition in the form of a sequence of concepts, and for each classification outputs a sequence of concepts and a counterfactual example as the explanation.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 03/06/2024\n\n\t\t\t\t\tSubmitted by: ManiBSrivastava\n"
 }
}