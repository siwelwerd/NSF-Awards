{
 "awd_id": "1763642",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Recognizing, Mitigating and Governing Bias in AI",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 800000.0,
 "awd_amount": 811016.0,
 "awd_min_amd_letter_date": "2018-05-31",
 "awd_max_amd_letter_date": "2020-07-23",
 "awd_abstract_narration": "Artificial Intelligence (AI) technologies mediate our interactions with the world and our daily decision making, ranging from shopping to hiring to surveillance. The development of rich AI algorithms able to process and learn from unparalleled amounts of data holds promise for making impartial, well-informed decisions. However, such systems also absorb human biases, such as gender stereotyping of activities and occupations. Left unchecked, they will perpetuate these biases on an unparalleled scale. A steady stream of press confirms that this is a widespread problem in real-world applications. This research brings together an interdisciplinary team to develop the science of AI bias. The findings will impact AI researchers and developers (through novel methodologies), computational social scientists (through a deeper study of human biases at web scale), educators and policy makers (through the comprehensive analysis of bias), and downstream users of AI technology. \r\n\r\nCompared to applications such as criminal risk scoring where fairness has traditionally been studied, modern AI systems are characterized by massive datasets, complex deep models and an unprecedented breadth of applications. This results in a wider spectrum of biases with complex propagation pathways, requiring an in-depth scientific investigation. The project develops the tools and techniques for recognizing, mitigating and governing bias in AI by combining expertise in deep learning, crowdsourcing and dataset curation, AI ethics, analyzing inference risk, web measurement, and science and technology studies. The component on recognizing bias includes an application of the Implicit Association Test combined with zero-shot learning to understand the societal bias of web corpora. Mitigating bias includes bridging active learning with research on adversarial examples for AI models. Governing bias includes a qualitative and quantitative study of downstream bias effects. The research is designed to be tightly connected, as for example when the recognition of curation bias in datasets leads to techniques in mitigating bias through enforcing group fairness in deep learning to governing bias in deployed system through developing bias observatories. The study will include advancements in machine learning (decomposing deep architectures, adapting reinforcement learning, exploring domain adaptation), human-computer interaction (developing novel active learning techniques, studying model interpretability), and digital ethnography (studying the effect of AI bias on culture, establishing an AI bias taxonomy). It will serve as a bridge between these fields, establishing tighter connections between them.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Arvind",
   "pi_last_name": "Narayanan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Arvind Narayanan",
   "pi_email_addr": "arvindn@princeton.edu",
   "nsf_id": "000663233",
   "pi_start_date": "2018-05-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Olga",
   "pi_last_name": "Russakovsky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Olga Russakovsky",
   "pi_email_addr": "olgarus@princeton.edu",
   "nsf_id": "000745440",
   "pi_start_date": "2018-05-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "35 Olden Street",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 800000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 11016.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-087753eb-7fff-c3d1-4d3a-a1ce5404226e\"> </span></p>\n<p dir=\"ltr\"><span>Artificial Intelligence (AI) is pervasive. It mediates our interactions with the world and our daily decision making. By default, AI reflects and even amplifies biases in society. The project sought to develop the science of AI bias and use it to find ways to recognize, mitigate, and govern bias in AI.</span></p>\n<p dir=\"ltr\"><span>On </span><strong>recognizing bias</strong><span>, the project produced many notable findings. Gender artifacts are ubiquitous in standard computer vision datasets, occurring everywhere from low-level information (e.g., the mean value of the color channels) to the higher-level composition of the image (e.g., pose and location of people).&nbsp; On the COCO image captioning dataset, there are differences in caption performance, sentiment, and word choice between images of lighter versus darker-skinned people. Commercial image recognition platforms have gender biases, which affect the visibility of women, reinforce harmful gender stereotypes, and limit the validity of the insights that can be gathered from online image data. Finally the project&rsquo;s REVISE tool can assists in the investigation of visual datasets, surfacing potential biases. Overall, the project revealed the pervasiveness of bias in visual datasets and the ways in which those biases manifest in AI models trained using those datasets.</span></p>\n<p dir=\"ltr\"><span>Building on this body of work, the project showed the potential of </span><span>mitigating biases</span><span> in many kinds of AI, often using AI itself as a tool. Selected projects include:</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span>1. Three key factors in the ImageNet dataset may lead to problematic behavior in downstream computer vision technology: the stagnant concept vocabulary, the exhaustive illustration of all concepts with images, and the inequality of representation in the images within concepts. The project took the first steps to mitigate them constructively.</p>\n<p dir=\"ltr\">2. Training a visual classifier for an attribute (e.g., wearing hat) can be complicated by correlations in the training data. For example, the presence of hats can be correlated with the presence of glasses. The project developed a dataset augmentation strategy using Generative Adversarial Networks (GANs) to create balanced datasets.</p>\n<p dir=\"ltr\">3. Image obfuscation (blurring, mosaicing, etc.) is widely used for protecting private information in images. The project showed how face blurring---a typical obfuscation technique---impacts visual recognition accuracy and how the impact can be minimized.</p>\n<p dir=\"ltr\">4. In online recommendation systems like Netflix, personalization creates a rich user experience but can also leave users feeling stereotyped. The project developed a new online learning algorithm that preserves benefits of personalization while increasing the commonality in treatments across users.&nbsp;</p>\n<p dir=\"ltr\"><span>The project took a multi-pronged approach to develop ways for </span><strong>governing bias</strong><span> in AI. It developed an ethical framework to determine when automated decision making is legitimate, the reason why it may not be legitimate, and a rubric for developers and civil rights advocates to interrogate a given system to analyze its legitimacy. It also produced scholarship that connected technical notions of fairness to moral ones. Finally, the project challenged the folk wisdom that mitigating the harms associated with ML datasets should occur at dataset creation time. Instead it proposed a different governance model: &ldquo;dataset stewarding&rdquo; throughout the lifecycle of datasets.&nbsp;</span></p>\n<p><span><strong>Other activities</strong>. </span><span>In addition to peer-reviewed publications, project findings were disseminated through talks, public lectures, media engagement, open-source software, and other channels. Notable activities in pursuit of broader impacts included (1) Princeton AI4ALL, an annual summer outreach camp for high-schoolers, (2) a textbook on fairness and machine learning that has been used in many courses around the country, and (3) engagement with policy makers and regulators including congressional staffers, federal agencies, and state attorney generals&rsquo; offices.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 06/05/2024<br>\nModified by: Arvind&nbsp;Narayanan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nArtificial Intelligence (AI) is pervasive. It mediates our interactions with the world and our daily decision making. By default, AI reflects and even amplifies biases in society. The project sought to develop the science of AI bias and use it to find ways to recognize, mitigate, and govern bias in AI.\n\n\nOn recognizing bias, the project produced many notable findings. Gender artifacts are ubiquitous in standard computer vision datasets, occurring everywhere from low-level information (e.g., the mean value of the color channels) to the higher-level composition of the image (e.g., pose and location of people). On the COCO image captioning dataset, there are differences in caption performance, sentiment, and word choice between images of lighter versus darker-skinned people. Commercial image recognition platforms have gender biases, which affect the visibility of women, reinforce harmful gender stereotypes, and limit the validity of the insights that can be gathered from online image data. Finally the projects REVISE tool can assists in the investigation of visual datasets, surfacing potential biases. Overall, the project revealed the pervasiveness of bias in visual datasets and the ways in which those biases manifest in AI models trained using those datasets.\n\n\nBuilding on this body of work, the project showed the potential of mitigating biases in many kinds of AI, often using AI itself as a tool. Selected projects include:\n\n\n1. Three key factors in the ImageNet dataset may lead to problematic behavior in downstream computer vision technology: the stagnant concept vocabulary, the exhaustive illustration of all concepts with images, and the inequality of representation in the images within concepts. The project took the first steps to mitigate them constructively.\n\n\n2. Training a visual classifier for an attribute (e.g., wearing hat) can be complicated by correlations in the training data. For example, the presence of hats can be correlated with the presence of glasses. The project developed a dataset augmentation strategy using Generative Adversarial Networks (GANs) to create balanced datasets.\n\n\n3. Image obfuscation (blurring, mosaicing, etc.) is widely used for protecting private information in images. The project showed how face blurring---a typical obfuscation technique---impacts visual recognition accuracy and how the impact can be minimized.\n\n\n4. In online recommendation systems like Netflix, personalization creates a rich user experience but can also leave users feeling stereotyped. The project developed a new online learning algorithm that preserves benefits of personalization while increasing the commonality in treatments across users.\n\n\nThe project took a multi-pronged approach to develop ways for governing bias in AI. It developed an ethical framework to determine when automated decision making is legitimate, the reason why it may not be legitimate, and a rubric for developers and civil rights advocates to interrogate a given system to analyze its legitimacy. It also produced scholarship that connected technical notions of fairness to moral ones. Finally, the project challenged the folk wisdom that mitigating the harms associated with ML datasets should occur at dataset creation time. Instead it proposed a different governance model: dataset stewarding throughout the lifecycle of datasets.\n\n\nOther activities. In addition to peer-reviewed publications, project findings were disseminated through talks, public lectures, media engagement, open-source software, and other channels. Notable activities in pursuit of broader impacts included (1) Princeton AI4ALL, an annual summer outreach camp for high-schoolers, (2) a textbook on fairness and machine learning that has been used in many courses around the country, and (3) engagement with policy makers and regulators including congressional staffers, federal agencies, and state attorney generals offices.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 06/05/2024\n\n\t\t\t\t\tSubmitted by: ArvindNarayanan\n"
 }
}