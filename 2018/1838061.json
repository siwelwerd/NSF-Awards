{
 "awd_id": "1838061",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Multiaffine Constrained Optimization for High-Dimensional Big Data Models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 699952.0,
 "awd_amount": 699952.0,
 "awd_min_amd_letter_date": "2018-09-11",
 "awd_max_amd_letter_date": "2018-09-11",
 "awd_abstract_narration": "This project addresses fundamental questions about properties of optimization models involving multiaffine functions and algorithms for solving them. Multiaffine functions are functions of variables, or blocks of variables, that are linear in them when all other variables, or blocks of variables, are held fixed. Optimization problems of this type arise in a wide variety of big data applications in science, engineering, medicine, statistics and social media, including machine learning, computer vision, medical and hyperspectral imaging, and tensor models to name just a few. Because these problems often involve massive amounts of data and huge numbers of variables, this project will attempt to develop efficient distributed and probabilistic approaches, enabling solutions to be obtained more rapidly than is currently possible. It is expected that the methodology that is developed will have a pervasive influence on practice in the interdisciplinary field of data science. The project will demonstrate this methodology on data sets from specific applications from a wide range of fields and disseminate the results through websites, code release and conference talks and tutorials, as well as through interactions with faculty and students from various applied disciplines in Columbia University's Data Science Institute, female students through the Society of Women Engineers at Columbia, and the hosting of high school students from under-represented minorities through Columbia University's Young Scholars Program.\r\n\r\nWhile providing important tools for solving real world problems, the project is also expected to have a major impact on the theoretical underpinnings of the Alternating Direction Method of Multipliers (ADMM) and an understanding of the optimization landscape of multiaffine problems arising in data analysis. ADMM has become a major algorithmic approach for solving problems in both parallel and distributed computational settings, because of its ability to transform the computationally intensive process of solving a difficult problem into an iterative procedure that involves solving simpler problems that are coupled by a system of linear equations. The project will expand ADMMs applicability by enabling these problems to be coupled by multiaffine constraints. The project will combine this multiaffine ADMM (M-ADMM) approach with stochastic and/or distributed approaches that are provably efficient and scalable.  For stochastic M-ADMM methods, how to reduce variance and importance sampling will be studied. For distributed settings, how to incorporate both centralized and decentralized concensus constraints into an M-ADMM framework will be investigated, as will asynchronous variants. The project will empirically study how to distribute the the computational effort of M-ADMM, both according to blocks of data and groups of parameters in high dimensional models. Because multiaffine problems are highly nonconvex, the solutions obtained by M-ADMM are in general only guaranteed to be local optima. It is known however, that for certain  multiaffine optimization problems, every local minimum is a global minimum and every saddle point has a direction of strict negative curvature under reasonable assumptions. The project will attempt to expand these kinds of results to more general multiaffine constrained problems and study the ability of M-ADMM for avoiding stagnating near saddle points.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Donald",
   "pi_last_name": "Goldfarb",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Donald Goldfarb",
   "pi_email_addr": "goldfarb@columbia.edu",
   "nsf_id": "000118999",
   "pi_start_date": "2018-09-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Wright",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "John N Wright",
   "pi_email_addr": "jw2966@columbia.edu",
   "nsf_id": "000630253",
   "pi_start_date": "2018-09-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "500 W. 120th",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276623",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 699952.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project developed efficient, scalable methods for solving optimization problems involving multiaffine constraints. This class of optimization problems includes many critical problems in machine learning, imaging, vision and beyond. There is a need for methods which scale to massive, practical datasets, while still producing high-quality solutions. Leveraging problem structure is critical to achieving these goals. Specifically, the project has developed practical methods for training deep neural networks (DNNs), which make efficient use of curvature information, leading to significant improvements in both speed, scalability and ease of deployment. These include the following methods.</p>\n<p>(i) Kronecker-Factored Quasi-Newton (e.g., K-BFGS) methods. These <span>approximate the Hessian of the DNN loss function by a layer-wise block-diagonal matrix and use the structure of the gradient and Hessian to further approximate each of these blocks as the Kronecker product of two much smaller matrices. Variants for both feed-forward, fully-connected (FF-FC) and convolutional NNs have been developed.</span></p>\n<p>(ii) Tensor Normal Training (TNT) methods.&nbsp;<span dir=\"ltr\">&nbsp;These approximate the probabilistically based Fisher&nbsp;</span><span dir=\"ltr\">matrix, by estimating the layer-</span><span dir=\"ltr\">wise covariance of the sampling based gradient as a block-diagonal pre-conditioning matrix, where these blocks have a&nbsp;</span><span dir=\"ltr\">Kronecker separable structure. This results in<span dir=\"ltr\">&nbsp;a tractable approximation to the Fisher matrix. Consequently, TNT&rsquo;s&nbsp;</span><span dir=\"ltr\">memory requirements and per-iterat<span dir=\"ltr\">ion computational costs are only slightly higher&nbsp;</span><span dir=\"ltr\">than those for first-order methods. Moreover, TNT o</span></span></span><span dir=\"ltr\">nly requires knowledge on the shape</span><span dir=\"ltr\">8&nbsp;</span>of the training parameters, and hence is applicable to all types of DNNs.</p>\n<p>(iii)<span dir=\"ltr\">&nbsp;Mini-block Fisher (MBF) methods. These are preconditioned&nbsp;</span>stochastic gradient methods, that lie in between first-order methods and the&nbsp;<span dir=\"ltr\">methods in (i) and (ii) above. Specifically, they</span><span dir=\"ltr\">&nbsp;use a block-diagonal approximation to&nbsp;</span><span dir=\"ltr\">the empirical Fisher matrix, where for each DNN layer</span><span dir=\"ltr\">, whether it is convolutional or FF-FC</span><span dir=\"ltr\">, the associated diago</span><span dir=\"ltr\">nal block is itself block-diagonal and is composed&nbsp;</span><span dir=\"ltr\">of a large number of mini-blocks of modest size.&nbsp;</span><span dir=\"ltr\">Our novel approach utilizes the parallelism of&nbsp;</span><span dir=\"ltr\">GPUs to efficiently perform computations on the&nbsp;</span><span dir=\"ltr\">large number of matrices in each layer. Conse</span><span dir=\"ltr\"><span dir=\"ltr\">quently, MBF's </span></span><span dir=\"ltr\">memory requirements and per-iteration computational costs are only slightly higher&nbsp;</span><span dir=\"ltr\">than those for first-order methods.&nbsp;</span></p>\n<p><span dir=\"ltr\"><span dir=\"ltr\">(iv)&nbsp;<span dir=\"ltr\">Layer-wise Adaptive Learning-Rate&nbsp;</span><span dir=\"ltr\">First-Order method. This is a&nbsp;</span></span></span>per-layer adaptive step-size procedure for stochastic first-order optimization methods for training DNNs. It eliminate the need for the user to tune the learning rate (LR) by exploiting the layer-wise stochastic curvature information contained in the diagonal blocks of the Hessian corresponding to each layer, and the self-concordant nature to which typical losss/activation functions give rise. The method has memory requirements that are comparable to those of first-order methods, while its per-iteration time complexity is only increased by an amount that is roughly equivalent to an additional gradient computation. Numerical experiments show that SGD with momentum and AdamW combined with the proposed per-layer step-sizes are able to choose effective LR schedules and outperform fine-tuned LR versions of these methods as well as popular second-order algorithms for training DNNs.&nbsp;We are expanding these results by approximating the local norm of both layer-wise and component-wise directions needed by our approach to computing adaptive LRs, so that our methods have essentially the same memory and computatuional requirements as the most popular first-order methods.</p>\n<p>The project has contributed broadly applicable analyses of convergence for problems involving multiaffine objective functions and constraints. It has also developed fine-grained geometric analyses of particular problems of practical importance, including deconvolution problems (imaging, neuroscience, materials science), dictionary learning problems (imaging, data compression) and deep learning, which show that under certain hypotheses, simple, scalable optimization methods obtain globally optimal solutions. These results include end-to-end guarantees for training deep neural networks, which prove that an appropriately trained deep network will generalize to unseen data, provided the data distribution has low-dimensional structure. We are leveraging these results to derive new optimization-inspired learning architectures, with improved efficiency and interpretability.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/11/2023<br>\n\t\t\t\t\tModified by: Donald&nbsp;Goldfarb</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project developed efficient, scalable methods for solving optimization problems involving multiaffine constraints. This class of optimization problems includes many critical problems in machine learning, imaging, vision and beyond. There is a need for methods which scale to massive, practical datasets, while still producing high-quality solutions. Leveraging problem structure is critical to achieving these goals. Specifically, the project has developed practical methods for training deep neural networks (DNNs), which make efficient use of curvature information, leading to significant improvements in both speed, scalability and ease of deployment. These include the following methods.\n\n(i) Kronecker-Factored Quasi-Newton (e.g., K-BFGS) methods. These approximate the Hessian of the DNN loss function by a layer-wise block-diagonal matrix and use the structure of the gradient and Hessian to further approximate each of these blocks as the Kronecker product of two much smaller matrices. Variants for both feed-forward, fully-connected (FF-FC) and convolutional NNs have been developed.\n\n(ii) Tensor Normal Training (TNT) methods.  These approximate the probabilistically based Fisher matrix, by estimating the layer-wise covariance of the sampling based gradient as a block-diagonal pre-conditioning matrix, where these blocks have a Kronecker separable structure. This results in a tractable approximation to the Fisher matrix. Consequently, TNT\u2019s memory requirements and per-iteration computational costs are only slightly higher than those for first-order methods. Moreover, TNT only requires knowledge on the shape8 of the training parameters, and hence is applicable to all types of DNNs.\n\n(iii) Mini-block Fisher (MBF) methods. These are preconditioned stochastic gradient methods, that lie in between first-order methods and the methods in (i) and (ii) above. Specifically, they use a block-diagonal approximation to the empirical Fisher matrix, where for each DNN layer, whether it is convolutional or FF-FC, the associated diagonal block is itself block-diagonal and is composed of a large number of mini-blocks of modest size. Our novel approach utilizes the parallelism of GPUs to efficiently perform computations on the large number of matrices in each layer. Consequently, MBF's memory requirements and per-iteration computational costs are only slightly higher than those for first-order methods. \n\n(iv) Layer-wise Adaptive Learning-Rate First-Order method. This is a per-layer adaptive step-size procedure for stochastic first-order optimization methods for training DNNs. It eliminate the need for the user to tune the learning rate (LR) by exploiting the layer-wise stochastic curvature information contained in the diagonal blocks of the Hessian corresponding to each layer, and the self-concordant nature to which typical losss/activation functions give rise. The method has memory requirements that are comparable to those of first-order methods, while its per-iteration time complexity is only increased by an amount that is roughly equivalent to an additional gradient computation. Numerical experiments show that SGD with momentum and AdamW combined with the proposed per-layer step-sizes are able to choose effective LR schedules and outperform fine-tuned LR versions of these methods as well as popular second-order algorithms for training DNNs. We are expanding these results by approximating the local norm of both layer-wise and component-wise directions needed by our approach to computing adaptive LRs, so that our methods have essentially the same memory and computatuional requirements as the most popular first-order methods.\n\nThe project has contributed broadly applicable analyses of convergence for problems involving multiaffine objective functions and constraints. It has also developed fine-grained geometric analyses of particular problems of practical importance, including deconvolution problems (imaging, neuroscience, materials science), dictionary learning problems (imaging, data compression) and deep learning, which show that under certain hypotheses, simple, scalable optimization methods obtain globally optimal solutions. These results include end-to-end guarantees for training deep neural networks, which prove that an appropriately trained deep network will generalize to unseen data, provided the data distribution has low-dimensional structure. We are leveraging these results to derive new optimization-inspired learning architectures, with improved efficiency and interpretability. \n\n \n\n\t\t\t\t\tLast Modified: 06/11/2023\n\n\t\t\t\t\tSubmitted by: Donald Goldfarb"
 }
}