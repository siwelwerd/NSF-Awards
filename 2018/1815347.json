{
 "awd_id": "1815347",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Small: Robust Eye Tracking and Facial Context Sensing for Mobile Augmented Reality and Virtual Reality",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 516000.0,
 "awd_min_amd_letter_date": "2018-08-02",
 "awd_max_amd_letter_date": "2019-05-16",
 "awd_abstract_narration": "Virtual and Augmented Reality headsets may soon become a major mass-market mobile consumer electronics device. However, there are challenges in making these headsets more portable and less power hungry, and in enabling a more immersive user experience by increasing awareness of the user's attention and emotional state. This project's goals are two-fold. The first goal is to enhance systems' ability to track the eye in a low-power yet robust manner by exploring new eye tracker designs combined with machine learning approaches. The second goal is to study methods to infer facial expressions and user emotion from wearable headsets by combining multiple sensing modalities including facial muscle activity and cameras. The project will also have educational impact on middle-school students and under-represented students through workshops. \r\n\r\nThe project involves activities and innovations in several areas. On the eye tracking side, the project will explore new hardware designs with stereo cameras and machine learning approaches to enhance robustness to face and eye shapes as well as eyeglass movements. On the facial expression sensing side, the team will explore multimodal sensing methods that combine electrooculography (EOG) and multiple camera views to infer expressions as well as to reduce power consumption. On the networked systems side, the project will look at leveraging eye tracking and facial expression sensing to enable wireless offload of rendering to a nearby compute node. The techniques to be researched can significantly lower the power consumed by AR and VR systems, enhance their ability to sense human expressions to enable more immersive experience, and reduce computational complexity by enabling predictive pre-fetch from a wirelessly connected edge cloud.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Deepak",
   "pi_last_name": "Ganesan",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Deepak K Ganesan",
   "pi_email_addr": "dganesan@cs.umass.edu",
   "nsf_id": "000486260",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "",
  "perf_city_name": "Amherst",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039264",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 500000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">This project enables enable better tracking of eye movements, facial gestures, and user emotions on headworn wearables like AR/VR headsets. Towards this goal, we go beyond traditional eye sensing methods and explore new sensing modalities and machine learning methods to improve robustness, reduce power consumption and reduce obtrusiveness.&nbsp;</p>\n<p class=\"p1\">The project involved activities and innovations in several areas. The first direction that we explored was unobtrusively and continuously monitoring one&rsquo;s facial expressions with headworn wearables. The standard Facial Action Coding System (FACS) along with camera based methods have been shown to provide objective indicators of facial expressions; however, these approaches can also be fairly limited for mobile applications due to privacy concerns and awkward positioning of the camera. To bridge this gap, we designed W!NCE which leverages two EoG electrodes placed solely at the nose bridge for continuously and unobtrusively sensing of upper facial action units with high fidelity. W!NCE detects facial gestures using a two-stage processing pipeline involving motion artifact removal and facial action detection. We validated our system&rsquo;s applicability through extensive evaluation on data from 17 users under stationary and ambulatory settings. W!NCE was published in IMWUT 2019.&nbsp;</p>\n<p class=\"p1\">The second direction that we explored is how the heavy computation needs required to process multi-modal information from sensors such as cameras can be offloaded to the edge to enable lower power execution. We have developed several computational pipelines that enable splitting deep learning models across an edge device and edge cloud while gracefully degrading performance in the presence of limited resources. These include CLIO, a novel approach to split a machine learning computation between a wearable device and cloud in a progressive&nbsp; manner (i.e., one that can adapt the size of intermediate results to deal with wireless dynamics), and FLEET, a new approach for offloading early exit computation to the cloud which allows the wearable device to leverage cloud resources even when operating with low-power radios. CLIO and FLEET were published at ACM Mobicom 2020 and 2023 respectively.</p>\n<p class=\"p1\">The third direction that we have explored is the design of wearable EEG systems that can be embedded in comfortable headwear while also concurrently monitor brain activity, eye movement, muscle activity, cardio-respiratory features and gross body movements. Normally, this requires multiple sensors to be worn at different locations as well as uncomfortable adhesives and discrete electronic components to be placed on the head. As a result, existing wearables either compromise comfort or compromise accuracy in tracking sleep variables. We proposed PhyMask, an all-textile sleep monitoring solution that is practical and comfortable for continuous use and that acquires all signals of interest to sleep solely using comfortable textile sensors placed on the head, and validated our solution against clinical grade polysomnography. This work was published at ACM Health 2022.</p>\n<p class=\"p1\">Put together, the work done in this project explores several new directions in sensing and computation offload from headworn wearables.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/20/2023<br>\nModified by: Deepak&nbsp;K&nbsp;Ganesan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project enables enable better tracking of eye movements, facial gestures, and user emotions on headworn wearables like AR/VR headsets. Towards this goal, we go beyond traditional eye sensing methods and explore new sensing modalities and machine learning methods to improve robustness, reduce power consumption and reduce obtrusiveness.\n\n\nThe project involved activities and innovations in several areas. The first direction that we explored was unobtrusively and continuously monitoring ones facial expressions with headworn wearables. The standard Facial Action Coding System (FACS) along with camera based methods have been shown to provide objective indicators of facial expressions; however, these approaches can also be fairly limited for mobile applications due to privacy concerns and awkward positioning of the camera. To bridge this gap, we designed W!NCE which leverages two EoG electrodes placed solely at the nose bridge for continuously and unobtrusively sensing of upper facial action units with high fidelity. W!NCE detects facial gestures using a two-stage processing pipeline involving motion artifact removal and facial action detection. We validated our systems applicability through extensive evaluation on data from 17 users under stationary and ambulatory settings. W!NCE was published in IMWUT 2019.\n\n\nThe second direction that we explored is how the heavy computation needs required to process multi-modal information from sensors such as cameras can be offloaded to the edge to enable lower power execution. We have developed several computational pipelines that enable splitting deep learning models across an edge device and edge cloud while gracefully degrading performance in the presence of limited resources. These include CLIO, a novel approach to split a machine learning computation between a wearable device and cloud in a progressive manner (i.e., one that can adapt the size of intermediate results to deal with wireless dynamics), and FLEET, a new approach for offloading early exit computation to the cloud which allows the wearable device to leverage cloud resources even when operating with low-power radios. CLIO and FLEET were published at ACM Mobicom 2020 and 2023 respectively.\n\n\nThe third direction that we have explored is the design of wearable EEG systems that can be embedded in comfortable headwear while also concurrently monitor brain activity, eye movement, muscle activity, cardio-respiratory features and gross body movements. Normally, this requires multiple sensors to be worn at different locations as well as uncomfortable adhesives and discrete electronic components to be placed on the head. As a result, existing wearables either compromise comfort or compromise accuracy in tracking sleep variables. We proposed PhyMask, an all-textile sleep monitoring solution that is practical and comfortable for continuous use and that acquires all signals of interest to sleep solely using comfortable textile sensors placed on the head, and validated our solution against clinical grade polysomnography. This work was published at ACM Health 2022.\n\n\nPut together, the work done in this project explores several new directions in sensing and computation offload from headworn wearables.\n\n\n\t\t\t\t\tLast Modified: 12/20/2023\n\n\t\t\t\t\tSubmitted by: DeepakKGanesan\n"
 }
}