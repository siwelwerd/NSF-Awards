{
 "awd_id": "1749833",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER:Towards Perceptual Agents That See and Reason Like Humans",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 545586.0,
 "awd_amount": 545586.0,
 "awd_min_amd_letter_date": "2018-06-01",
 "awd_max_amd_letter_date": "2018-06-01",
 "awd_abstract_narration": "Recent advancements in computer vision systems have enabled their widespread deployment in areas like social media, healthcare, robotics, and ecology, among many others. While such applications hold exceptional promise for improving our well-being and advancing scientific discovery, the ubiquity of these intelligent systems presents new technical, social, and cultural challenges for their wide-scale adoption. This project leads an integrated effort of research, teaching, and outreach to address some of these challenges. The project develops architectures that are substantially more accurate and capable of extracting detailed information from perceptual data across different modalities. An emphasis of this work is to develop computer vision systems that can reason about data in ways that are interpretable by humans. This project also promotes diversity, engages high school, undergraduate, and graduate students in research activities, and fosters collaborations with industry and researchers in areas such as ecology and biology through workshops.\r\n\r\nThis research explores new directions that improve the capabilities of visual perception and reasoning systems for analyzing image data, spatio-temporal data, and depth data. The research develops a novel class of graph-based and factorized architectures for 3D shape and spatio-temporal analysis that provide better tradeoffs between computational cost, memory overhead, and accuracy than existing models. The research develops weakly supervised techniques for learning shape and motion representations from large amounts of unlabeled data. The research also develops a novel class of techniques for transforming visual data to semantic representations such as attributes, natural language, and symbolic programs. These techniques will improve the interpretability of machine learning models and enable collaborative learning and inference between humans and AI agents.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Subhransu",
   "pi_last_name": "Maji",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Subhransu Maji",
   "pi_email_addr": "smaji@cs.umass.edu",
   "nsf_id": "000699819",
   "pi_start_date": "2018-06-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "100 Venture Way, Suite 201",
  "perf_city_name": "Hadley",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010359450",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 545586.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The research from our team has made significant advancements in understanding 3D shapes, identifying detailed visual differences (like telling apart similar animal or plant species), and improving how computers combine language and visual information to solve complex problems. These innovations can be used in areas ranging from animation and virtual reality to identifying animal species in environmental studies.</p>\n<h3>Understanding 3D Shapes</h3>\n<p>A major focus of our research has been figuring out how to represent 3D shapes more accurately. For example, our work has made it easier to break down complex 3D objects into simpler parts, which helps in tasks like editing and animation. We also developed methods that allow computers to learn 3D shapes just by looking at regular 2D photos or videos, without needing any special 3D data. This is important for creating realistic 3D models in virtual environments or reconstructing objects from images in fields like archaeology or robotics.</p>\n<h3>Identifying Fine Details</h3>\n<p>Our research has also made it easier for computers to identify small details that help differentiate similar objects or species. For instance, if you have a dataset of bird images, traditional AI models might struggle to tell the difference between two similar-looking species. We improved these models by teaching them to understand text descriptions (e.g., a bird&rsquo;s beak length or feather color) and apply this knowledge to new images they haven&rsquo;t seen before. This helps in scenarios where there isn&rsquo;t enough labeled data, making the models better at recognizing new species or identifying the parts of an object even with minimal information.</p>\n<h3>Connecting Language and Vision</h3>\n<p>We also explored how to make AI systems that understand both images and text better. One project, called <em>PhraseCut</em>, introduced a new dataset to help computers figure out how to pick out specific objects in an image based on a natural language description (e.g., &ldquo;the red book on the left shelf&rdquo;). This makes it easier to interact with computers using everyday language, which could be useful for tasks like visually guided search or even helping visually impaired individuals by describing their surroundings.</p>\n<h3>Educational and Community Impact</h3>\n<p>Beyond the research itself, the project has supported the education of many students. Five PhD students completed their research, and more than a dozen Master&rsquo;s and undergraduate students gained valuable experience through this project. Our team has also shared findings at major academic conferences and made our software and datasets available to everyone, so other researchers can build on our work.</p>\n<p>We have organized workshops that bring together researchers from different fields&mdash;like computer science and biology&mdash;alongside industry experts to share ideas and tackle tough challenges in visual recognition (e.g., recognizing rare species or understanding complex objects in images). As a result, more researchers are now working on solving these problems, building a strong community around this topic.</p>\n<h3>Summary</h3>\n<p>In short, our research has pushed the boundaries of how AI understands shapes, distinguishes small visual details, and connects visual information with language. These advancements are helping to build smarter, more flexible AI systems that can work in a variety of real-world scenarios, from environmental monitoring to making everyday interactions with technology smoother and more intuitive.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/29/2024<br>\nModified by: Subhransu&nbsp;Maji</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634757834_3d--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634757834_3d--rgov-800width.png\" title=\"3D shape understanding\"><img src=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634757834_3d--rgov-66x44.png\" alt=\"3D shape understanding\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Architectures for 3D shape understanding</div>\n<div class=\"imageCredit\">Subhransu Maji</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Subhransu&nbsp;Maji\n<div class=\"imageTitle\">3D shape understanding</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634493800_bcnn--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634493800_bcnn--rgov-800width.png\" title=\"bilinear CNNs\"><img src=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634493800_bcnn--rgov-66x44.png\" alt=\"bilinear CNNs\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">bilinear CNNs for fine-grained recognition</div>\n<div class=\"imageCredit\">Subhransu Maji</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Subhransu&nbsp;Maji\n<div class=\"imageTitle\">bilinear CNNs</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634670176_prgan--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634670176_prgan--rgov-800width.png\" title=\"Learning 3D from images\"><img src=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634670176_prgan--rgov-66x44.png\" alt=\"Learning 3D from images\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Inferring 3D shapes from images using projective geometry and geometric priors</div>\n<div class=\"imageCredit\">Subhransu Maji</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Subhransu&nbsp;Maji\n<div class=\"imageTitle\">Learning 3D from images</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634587200_coarse_sup--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634587200_coarse_sup--rgov-800width.png\" title=\"Learning from coarse supervision\"><img src=\"/por/images/Reports/POR/2024/1749833/1749833_10547607_1727634587200_coarse_sup--rgov-66x44.png\" alt=\"Learning from coarse supervision\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Techniques to infer detailed models from coarse supervision</div>\n<div class=\"imageCredit\">Subhransu Maji</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Subhransu&nbsp;Maji\n<div class=\"imageTitle\">Learning from coarse supervision</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe research from our team has made significant advancements in understanding 3D shapes, identifying detailed visual differences (like telling apart similar animal or plant species), and improving how computers combine language and visual information to solve complex problems. These innovations can be used in areas ranging from animation and virtual reality to identifying animal species in environmental studies.\nUnderstanding 3D Shapes\n\n\nA major focus of our research has been figuring out how to represent 3D shapes more accurately. For example, our work has made it easier to break down complex 3D objects into simpler parts, which helps in tasks like editing and animation. We also developed methods that allow computers to learn 3D shapes just by looking at regular 2D photos or videos, without needing any special 3D data. This is important for creating realistic 3D models in virtual environments or reconstructing objects from images in fields like archaeology or robotics.\nIdentifying Fine Details\n\n\nOur research has also made it easier for computers to identify small details that help differentiate similar objects or species. For instance, if you have a dataset of bird images, traditional AI models might struggle to tell the difference between two similar-looking species. We improved these models by teaching them to understand text descriptions (e.g., a birds beak length or feather color) and apply this knowledge to new images they havent seen before. This helps in scenarios where there isnt enough labeled data, making the models better at recognizing new species or identifying the parts of an object even with minimal information.\nConnecting Language and Vision\n\n\nWe also explored how to make AI systems that understand both images and text better. One project, called PhraseCut, introduced a new dataset to help computers figure out how to pick out specific objects in an image based on a natural language description (e.g., the red book on the left shelf). This makes it easier to interact with computers using everyday language, which could be useful for tasks like visually guided search or even helping visually impaired individuals by describing their surroundings.\nEducational and Community Impact\n\n\nBeyond the research itself, the project has supported the education of many students. Five PhD students completed their research, and more than a dozen Masters and undergraduate students gained valuable experience through this project. Our team has also shared findings at major academic conferences and made our software and datasets available to everyone, so other researchers can build on our work.\n\n\nWe have organized workshops that bring together researchers from different fieldslike computer science and biologyalongside industry experts to share ideas and tackle tough challenges in visual recognition (e.g., recognizing rare species or understanding complex objects in images). As a result, more researchers are now working on solving these problems, building a strong community around this topic.\nSummary\n\n\nIn short, our research has pushed the boundaries of how AI understands shapes, distinguishes small visual details, and connects visual information with language. These advancements are helping to build smarter, more flexible AI systems that can work in a variety of real-world scenarios, from environmental monitoring to making everyday interactions with technology smoother and more intuitive.\n\n\n\t\t\t\t\tLast Modified: 09/29/2024\n\n\t\t\t\t\tSubmitted by: SubhransuMaji\n"
 }
}