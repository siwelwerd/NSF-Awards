{
 "awd_id": "1763569",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Scalable Integration of Data-Driven and Model-Based Methods for Large Vocabulary Sign Recognition and Search",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 209896.0,
 "awd_amount": 209896.0,
 "awd_min_amd_letter_date": "2018-07-21",
 "awd_max_amd_letter_date": "2018-07-21",
 "awd_abstract_narration": "It is surprisingly difficult to look up an unfamiliar sign in American Sign Language (ASL). Most ASL dictionaries list signs in alphabetical order based on approximate English translations, so a user who does not understand a sign or know its English translation would not know how to find it. ASL lacks a written form or intuitive \"alphabetical sorting\" based on such a writing system. Although some dictionaries make available alternative ways to search for a sign, based on explicit specification of various properties, a user must often still look through hundreds of pictures of signs to find a match to the unfamiliar sign (if it is present at all in that dictionary). This research will create a framework that will enable the development of a user-friendly, video-based sign-lookup interface, for use with online ASL video dictionaries and resources, and for facilitation of ASL annotation.  Input will consist of either a webcam recording of a sign by the user, or user identification of the start and end frames of a sign from a digital video. To test the efficacy of the new tools in real-world applications, the team will partner with the leading producer of pedagogical materials for ASL instruction in high schools and colleges, which is developing the first multimedia ASL dictionary with video-based ASL definitions for signs. The lookup interface will be used experimentally to search the ASL dictionary in ASL classes at Boston University and RIT. Project outcomes will revolutionize how deaf children, students learning ASL, or families with deaf children search ASL dictionaries. They will accelerate research on ASL linguistics and technology, by increasing efficiency, accuracy, and consistency of annotations of ASL videos through video-based sign lookup. And they will lay the groundwork for future technologies to benefit deaf users, such as search by video example through ASL video collections, or ASL-to-English translation, for which sign-recognition is a precursor. The new linguistically annotated video data and software tools will be shared publicly, for use by others in linguistic and computer science research, as well as in education. \r\n\r\nSign recognition from video is still an open and difficult problem because of the nonlinearities involved in recognizing 3D structures from 2D video, and the complex linguistic organization of sign languages. The linguistic parameters relevant to sign production and discrimination include hand configuration and orientation, location relative to the body or in signing space, movement trajectory, and in some cases, facial expressions/head movements. An additional complication is that signs belonging to different classes have distinct internal structures, and are thus subject to different linguistic constraints and require distinct recognition strategies; yet prior research has generally failed to address these distinctions. The challenges are compounded by inter- and intra- signer variations, and, in continuous signing, by co-articulation effects (i.e., influence from adjacent signs) with respect to several of the above parameters. Purely data-driven approaches are ill-suited to sign recognition given the limited quantities of available, consistently annotated data and the complexity of the linguistic structures involved, which are hard to infer. Prior research has, for this reason, generally focused on selected aspects of the problem, often restricting the work to a limited vocabulary, and therefore resulting in methods that are not scalable. More importantly, few if any methods involve 4D (spatio-temporal) modeling and attention to the linguistic properties of specific types of signs. A new approach to computer-based recognition of ASL from video is needed. In this research, the approach will be to build a new hybrid, scalable, computational framework for sign identification from a large vocabulary, which has never before been achieved. This research will strategically combine state-of-the-art computer vision, machine-learning methods, and linguistic modeling. It will leverage the team's existing publicly shared ASL corpora and Sign Bank - linguistically annotated and categorized video recordings produced by native signers - which will be augmented to meet the requirements of this project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matt",
   "pi_last_name": "Huenerfauth",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matt Huenerfauth",
   "pi_email_addr": "matt.huenerfauth@rit.edu",
   "nsf_id": "000220138",
   "pi_start_date": "2018-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rochester Institute of Tech",
  "inst_street_address": "1 LOMB MEMORIAL DR",
  "inst_street_address_2": "",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5854757987",
  "inst_zip_code": "146235603",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "ROCHESTER INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "J6TWTRKC1X14"
 },
 "perf_inst": {
  "perf_inst_name": "Rochester Institute of Tech",
  "perf_str_addr": "1 Lomb Memorial Drive",
  "perf_city_name": "Rochester",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146235603",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 209896.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>It is surprisingly difficult to look up an unfamiliar sign in American Sign Language (ASL). Most ASL dictionaries list signs in alphabetical order based on approximate English translations, so a user who does not understand a sign or know its English translation would have no way to find it. ASL lacks a written form or intuitive \"alphabetical sorting.\" &nbsp;Although some dictionaries offer alternative ways to search for a sign, based on explicit specification of various properties, a user must often still look through hundreds of pictures of signs to find the sign of interest, if it is present at all in that dictionary.&nbsp;</p>\n<p>In this collaborative research project with Rochester Institute of Technology (RIT), Boston University, and Rutgers University, we have created a framework and prototype for user-friendly search by video example for access to online ASL video dictionaries and other digital resources, and for facilitation of ASL annotation. A user can conduct a search by inputting a video from a webcam (with the user performing the sign whose meaning they do not know) or submitting a recording of a sign or a clip from a digital video. User studies to guide the development of the lookup interface have been conducted at RIT, and our collaborators at Boston University and Rutgers University conducted research on the linguistic analysis and annotation of video data from ASL signers, as well as sign recognition research on identification of signs from video input.</p>\n<p>This research leveraged the team's publicly shared ASL corpora and Sign Bank -- linguistically annotated and categorized video recordings of Deaf signers -- which were augmented through a substantial amount of new video data collected at RIT and through videos contributed by DawnSignPress. These new video files were annotated at Boston University using SignStream(R), their software for linguistic annotation of visual language data.&nbsp;</p>\n<p>Our new linguistically annotated video data and our software tools are shared publicly, for use by others in linguistic and computer science research, as well as in education.&nbsp;</p>\n<p>On the RIT side of the project, in addition to the collection and labeling of video data of ASL, the research focus has been on the design of user-interfaces for how to best enable an individual who does not know the meaning of an ASL sign to determine its meaning.&nbsp;</p>\n<p>Our research findings included how the satisfaction of users with an ASL dictionary lookup system declines as the placement of the desired item appears lower in the list of results returned, as well as how the overall linguistic similarity of the items in the results list influences users' impression of the quality of the underlying sign-recognition technology.&nbsp; We also investigated how when the desired item appears on the second page of results from a dictionary search, there is a notable drop-off in the satisfaction of users with the quality of the results.&nbsp;</p>\n<p>In a subsequent study, we investigated how providing users with filtering options to narrow the set of results that are returned after an initial dictionary search influenced the satisfaction of users with the system and their success at finding the meaning of a desired ASL sign. We investigated a variety of designs and linguistic filter parameters for such a system and provided guidance on how to best incorporate a filtering step in a search tool. In a comparison study, we measured advantages of providing this filtering step after an initial dictionary search based on video input.&nbsp;</p>\n<p>In a final study, we investigate how integrating an ASL dictionary search tool into the user-interface of a video player system led to further benefits for individuals who need to look up the meaning of ASL signs while watching a longer video.&nbsp; All this work provides guidance for the designers of ASL dictionary systems, to best support the lookup of unfamiliar signs.</p>\n<p>This overall collaborative research project paves the way for applications with the potential to revolutionize how deaf children, students learning ASL, or families with deaf children can search ASL dictionaries and other digital resources. This will also enable development of tools that will accelerate research on ASL linguistics and technology, by increasing efficiency, accuracy, and consistency of annotations of ASL videos through video-based sign lookup. This research also lays the groundwork for future technologies to benefit deaf users, such as search by video example through ASL video collections, or ASL-to-English translation, for which sign-recognition is a precursor.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/13/2023<br>\nModified by: Matt&nbsp;Huenerfauth</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIt is surprisingly difficult to look up an unfamiliar sign in American Sign Language (ASL). Most ASL dictionaries list signs in alphabetical order based on approximate English translations, so a user who does not understand a sign or know its English translation would have no way to find it. ASL lacks a written form or intuitive \"alphabetical sorting.\" Although some dictionaries offer alternative ways to search for a sign, based on explicit specification of various properties, a user must often still look through hundreds of pictures of signs to find the sign of interest, if it is present at all in that dictionary.\n\n\nIn this collaborative research project with Rochester Institute of Technology (RIT), Boston University, and Rutgers University, we have created a framework and prototype for user-friendly search by video example for access to online ASL video dictionaries and other digital resources, and for facilitation of ASL annotation. A user can conduct a search by inputting a video from a webcam (with the user performing the sign whose meaning they do not know) or submitting a recording of a sign or a clip from a digital video. User studies to guide the development of the lookup interface have been conducted at RIT, and our collaborators at Boston University and Rutgers University conducted research on the linguistic analysis and annotation of video data from ASL signers, as well as sign recognition research on identification of signs from video input.\n\n\nThis research leveraged the team's publicly shared ASL corpora and Sign Bank -- linguistically annotated and categorized video recordings of Deaf signers -- which were augmented through a substantial amount of new video data collected at RIT and through videos contributed by DawnSignPress. These new video files were annotated at Boston University using SignStream(R), their software for linguistic annotation of visual language data.\n\n\nOur new linguistically annotated video data and our software tools are shared publicly, for use by others in linguistic and computer science research, as well as in education.\n\n\nOn the RIT side of the project, in addition to the collection and labeling of video data of ASL, the research focus has been on the design of user-interfaces for how to best enable an individual who does not know the meaning of an ASL sign to determine its meaning.\n\n\nOur research findings included how the satisfaction of users with an ASL dictionary lookup system declines as the placement of the desired item appears lower in the list of results returned, as well as how the overall linguistic similarity of the items in the results list influences users' impression of the quality of the underlying sign-recognition technology. We also investigated how when the desired item appears on the second page of results from a dictionary search, there is a notable drop-off in the satisfaction of users with the quality of the results.\n\n\nIn a subsequent study, we investigated how providing users with filtering options to narrow the set of results that are returned after an initial dictionary search influenced the satisfaction of users with the system and their success at finding the meaning of a desired ASL sign. We investigated a variety of designs and linguistic filter parameters for such a system and provided guidance on how to best incorporate a filtering step in a search tool. In a comparison study, we measured advantages of providing this filtering step after an initial dictionary search based on video input.\n\n\nIn a final study, we investigate how integrating an ASL dictionary search tool into the user-interface of a video player system led to further benefits for individuals who need to look up the meaning of ASL signs while watching a longer video. All this work provides guidance for the designers of ASL dictionary systems, to best support the lookup of unfamiliar signs.\n\n\nThis overall collaborative research project paves the way for applications with the potential to revolutionize how deaf children, students learning ASL, or families with deaf children can search ASL dictionaries and other digital resources. This will also enable development of tools that will accelerate research on ASL linguistics and technology, by increasing efficiency, accuracy, and consistency of annotations of ASL videos through video-based sign lookup. This research also lays the groundwork for future technologies to benefit deaf users, such as search by video example through ASL video collections, or ASL-to-English translation, for which sign-recognition is a precursor.\n\n\n\t\t\t\t\tLast Modified: 11/13/2023\n\n\t\t\t\t\tSubmitted by: MattHuenerfauth\n"
 }
}