{
 "awd_id": "1847853",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research:Automated Instruction Assistant for Argumentative Essays",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 143000.0,
 "awd_amount": 159000.0,
 "awd_min_amd_letter_date": "2018-08-31",
 "awd_max_amd_letter_date": "2019-02-21",
 "awd_abstract_narration": "Development of students' writing skills promotes critical thinking across disciplines, and professional success. Yet the past decade of the Nation's Report Card on students' writing points to a long-standing crisis in writing instruction that persists through post-secondary school. Students need more instruction on how to write, and instructors need more support to provide students with comprehensive, targeted feedback. This project will develop an Automated Instruction Assistant (AIA) to provide post-secondary instructors with feedback on essays while they grade them, through the application of natural language processing and machine learning techniques to the analysis of essay content and argumentation. The project will apply an iterative design process to a sequence of two argumentative essay assignments in the context of a freshman course on academic skills in a computer science department, where the enrollment is in the hundreds. It will integrate state-of-the art methods in content analysis and argument mining of text to model text meaning more deeply than in previous work. Automated support for application of educational rubrics to argumentative essays will help instructors to provide more comprehensive, standardized feedback for students, and foster transparent and supportive writing instruction. \r\n\r\nThis project will develop an AIA that assigns both a total score to an essay, and individual score dimensions, such as how well an argumentative essay articulates a major claim. The scores will be supported by pointers into the text that provide score justification. As a result, the AIA output can be linked directly to a rubric used by the instructor, which facilitates instructor reflection, training for graders, and feedback for students. The technology will integrate and extend the researchers' previous work on content analysis and argument mining. The content analysis will take as input a small number of reference essays to generate a model of the ideas (propositions) in the domain, weighted by importance within and across essays. The argument mining will identify the propositions that play a role in an argument, and their argument relations. It will produce an argument graph in which the nodes are propositions and edges are argument relations. Integration with the content analysis will ground the propositions in the domain ideas, and make it possible to exploit the role of importance of ideas in the domain, and their prominence within an essay. Methods will include novel applications of dynamic programming and integer linear programming combined with deep learning of rich, low dimensional semantic vectors.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Smaranda",
   "pi_last_name": "Muresan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Smaranda Muresan",
   "pi_email_addr": "smuresan@barnard.edu",
   "nsf_id": "000542607",
   "pi_start_date": "2018-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 143000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Written work is an important vehicle in undergraduate education. Through writing, students learn to articulate and explain what they have learned, and instructors can assess both whether students are ready to move on to the next learning objectives, and how well the instructional goals of the course are being met. Analytic rubrics support the use of writing as a vehicle for learning and instruction by breaking down the expectations for a writing assignment into distinct analytic components that are each given a rating.&nbsp; On the one hand, analytic rubrics have been found to support the use of writing for learning and instruction in the context of studies of educational interventions, where these studies build in the resources needed to apply the rubrics in a reliable way. On the other hand, designing and measuring the reliability of such rubrics has very high overhead for instructors in ordinary classroom practice. This project investigated two argumentative writing assignments in a large academic skills class for first year college students in a computer science program, addressing two research questions. The first question was whether teaching assistants could provide accurate assessments of students' written work, based on an analytic rubric. The second question was whether automated methods could be used to enhance the feasibility of analytic rubrics for ordinary classroom practice.</span></p>\n<p>&nbsp;</p>\n<p><span>For the first question of the reliability of a rubric developed for this project, we found that raters could be trained to reliably apply the rubric, and we found that grades assigned during the course by TAs were not reliable.&nbsp;This is an important finding that motivates further investigation of how to utilize analytic rubrics in ordinary classroom practice, such as through automation, or through human-machine collaboration.</span></p>\n<p>&nbsp;</p>\n<p><span>For the second question of the feasibility of automated methods, we found that existing automated methods could not on their own apply the rubric reliably. However, we found that automated results were promising enough to motivate future work on automating aspects of the rubric, particularly models that integrate content analysis with argument structure.</span></p>\n<p><span>&nbsp;</span></p>\n<p><span>This project supported the development of new datasets and computational tools.&nbsp; We assembled a small dataset of students' argumentative essays for two assignments, and annotated the assignments&nbsp;for content and argumentation as data for testing automated natural language processing methods to replicate the annotations. The dataset also includes a new rubric for assessment of content and argumentation of source-based argumentative essays, and application of the rubric to the essays.&nbsp; Moreover, we developed a small set of news articles on climate change that were fact-checked by experts with argument structure. &nbsp;We developed two kinds of software. First, we improved and extended the functionality of existing software for automated assessment of argument structure. As a scientific advance, we study the connection between discourse structure and argument structure and show that modeling discourse structure helps improve models for detection of argument structure. &nbsp;Second, we develop computational models for check-worthiness of claims that models the argument structure. The latter has the potential to help journalists in guiding them what to fact-check.</span></p>\n<p><span>&nbsp;</span></p>\n<p><span>The project produced a journal publication in a major educational technology journal, two papers in conference proceedings for major natural language processing (NLP) conferences, and a paper in the premier NLP workshop devoted to Innovative use of NLP for Building Educational Applications.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/23/2022<br>\n\t\t\t\t\tModified by: Smaranda&nbsp;Muresan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWritten work is an important vehicle in undergraduate education. Through writing, students learn to articulate and explain what they have learned, and instructors can assess both whether students are ready to move on to the next learning objectives, and how well the instructional goals of the course are being met. Analytic rubrics support the use of writing as a vehicle for learning and instruction by breaking down the expectations for a writing assignment into distinct analytic components that are each given a rating.  On the one hand, analytic rubrics have been found to support the use of writing for learning and instruction in the context of studies of educational interventions, where these studies build in the resources needed to apply the rubrics in a reliable way. On the other hand, designing and measuring the reliability of such rubrics has very high overhead for instructors in ordinary classroom practice. This project investigated two argumentative writing assignments in a large academic skills class for first year college students in a computer science program, addressing two research questions. The first question was whether teaching assistants could provide accurate assessments of students' written work, based on an analytic rubric. The second question was whether automated methods could be used to enhance the feasibility of analytic rubrics for ordinary classroom practice.\n\n \n\nFor the first question of the reliability of a rubric developed for this project, we found that raters could be trained to reliably apply the rubric, and we found that grades assigned during the course by TAs were not reliable. This is an important finding that motivates further investigation of how to utilize analytic rubrics in ordinary classroom practice, such as through automation, or through human-machine collaboration.\n\n \n\nFor the second question of the feasibility of automated methods, we found that existing automated methods could not on their own apply the rubric reliably. However, we found that automated results were promising enough to motivate future work on automating aspects of the rubric, particularly models that integrate content analysis with argument structure.\n\n \n\nThis project supported the development of new datasets and computational tools.  We assembled a small dataset of students' argumentative essays for two assignments, and annotated the assignments for content and argumentation as data for testing automated natural language processing methods to replicate the annotations. The dataset also includes a new rubric for assessment of content and argumentation of source-based argumentative essays, and application of the rubric to the essays.  Moreover, we developed a small set of news articles on climate change that were fact-checked by experts with argument structure.  We developed two kinds of software. First, we improved and extended the functionality of existing software for automated assessment of argument structure. As a scientific advance, we study the connection between discourse structure and argument structure and show that modeling discourse structure helps improve models for detection of argument structure.  Second, we develop computational models for check-worthiness of claims that models the argument structure. The latter has the potential to help journalists in guiding them what to fact-check.\n\n \n\nThe project produced a journal publication in a major educational technology journal, two papers in conference proceedings for major natural language processing (NLP) conferences, and a paper in the premier NLP workshop devoted to Innovative use of NLP for Building Educational Applications.\n\n \n\n\t\t\t\t\tLast Modified: 12/23/2022\n\n\t\t\t\t\tSubmitted by: Smaranda Muresan"
 }
}