{
 "awd_id": "1822865",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Designing and Evaluating a Naturalistic Platform for Collaborative Learning About Spatial Reasonings",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928447",
 "po_email": "cshen@nsf.gov",
 "po_sign_block_name": "Chia Shen",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 749849.0,
 "awd_amount": 781849.0,
 "awd_min_amd_letter_date": "2018-08-27",
 "awd_max_amd_letter_date": "2020-07-02",
 "awd_abstract_narration": "Spatial reasoning and computational thinking are essential knowledge for future workers in Science, Technology, Engineering and Mathematics (STEM). Researchers in this project will design and study an intelligent multimodal interface to support elementary and middle school students to learn these cognitive skills on a collaborative digital platform based on Minecraft. The new interface will include audio and speech, gaze with eye tracking, and tangible physical objects. The combination of these human-computer interaction modalities may support learning for a broader diversity of learners and facilitate collaboration between peers in a more naturalistic and productive setting. Enabling interacting with computers not only through keyboard commands but also through speech, physical blocks and eye gaze, learners will have the opportunity to practice and improve their spatial reasoning and computational thinking skills in an informal, flexible and accessible context. More broadly, this project investigates the future of multimodal interfaces. Some of the key components of the next-generation multimodal, voice-enabled interface that this project explores include the ability to interpret everyday human language and to express an idea using speech or gesture during collaboration. In addition to developing new technologies, this project examines how to structure different digital learning experiences in ways that promote spatial reasoning and computational thinking. Several hypotheses related to spatial reasoning and computational thinking will be tested through laboratory studies and afterschool learning club studies in partnership with the Evanston Public School District in Illinois. This project is funded by the Cyberlearning for Work at the Human-Technology Frontier program, which seeks to fund exploratory and synergistic research in learning technologies to prepare learners to excel in work at the human-technology frontier.\r\n\r\nThe research project will investigate four important research questions. (1) In what ways does a multimodal interface enable or promote increased participation in the afterschool game-based learning club among current non-participants, especially younger students, people from underrepresented groups and girls? (2) How might a multimodal interface change existing patterns of behavior among current learners; specifically, in what ways do multimodal interfaces introduce, support, and disrupt collaborative learning and building practices? (3) Do these changes in participation and in-game practices correlate with the development of spatial thinking, computational thinking and the pursuit of computing careers? (4) What design principles and technological and/or algorithmic developments are needed to support collaborative work through multimodal interfaces? Researchers will employ an iterative design process. Design modifications will be informed by mixed-method analyses of video data, surveys, learning assessments, user activity logs, and multimodal sensory data on how learners engage in multimodal, collaborative problem solving . This research has the potential to discover learning pathways to STEM careers for underrepresented young learners. Around 400 students will participate and experience the proposed multimodal learning system during the three years. The resulting software and design documents will be open source for the public in order to reach a much larger number of institutes to broadening participation of STEM learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marcelo",
   "pi_last_name": "Worsley",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Marcelo Worsley",
   "pi_email_addr": "marcelo.worsley@northwestern.edu",
   "nsf_id": "000695833",
   "pi_start_date": "2018-08-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Uttal",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "David H Uttal",
   "pi_email_addr": "duttal@northwestern.edu",
   "nsf_id": "000245900",
   "pi_start_date": "2018-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "2120 Campus Drive",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602082610",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "153600",
   "pgm_ele_name": "S-STEM-Schlr Sci Tech Eng&Math"
  },
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "7218",
   "pgm_ref_txt": "RET SUPP-Res Exp for Tchr Supp"
  },
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "13XX",
   "app_name": "H-1B FUND, EHR, NSF",
   "app_symb_id": "045176",
   "fund_code": "1300XXXXDB",
   "fund_name": "H-1B FUND, EDU, NSF",
   "fund_symb_id": "045176"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 749849.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 32000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This award helped to advance the field&rsquo;s knowledge about learning spatial reasoning and computational thinking through virtual learning environments. The award included two well aligned threads (1) understanding how students practice spatial reasoning and computational thinking in popular virtual learning environments, and (2) identifying strategies for how using multimodal data (eye tracking, speech, tangibles, and gestures) might contribute to, and reflect, participant learning in virtual learning environments. Importantly, the use of multimodal data provided a richer means for the project team to study participant spatial reasoning and computational thinking practices. Regarding the first thread, the research team identified a combination of traditional and novel ways that users practice spatial reasoning and computational thinking within virtual learning environments. Virtual learning environments provide some affordances that are not available in the material world, and users often take advantage of those affordances to practice spatial skills in new ways. A combination of screen recording with eye tracking data helped the research team notice the specific spatial reasoning skills participants were utilizing. In terms of the second thread, the research team completed multiple iterations of the multimodal platform and observed considerable variation in youth preferences for the different modalities. Young learners (e.g., kindergarten to 2<sup>nd</sup> grade) were particularly drawn to using speech capabilities to overcome difficulties with spelling and typing. Our user studies with youth also surfaced considerable diversity in the types of gestures people wanted to use to control the virtual learning environments. Some students preferred static gestures while others preferred dynamic gestures. There was also variability in youth preferences for small-scale gestures (using one&rsquo;s finger) and whole-body gestures (using a combination of body parts). Future implementations of the multimodal interface will privilege the ability for participants to select and customize which gestures they want to utilize as they participate with the virtual learning environment. Finally, we observed adults being willing to participate in the virtual learning experience alongside their children when they could manipulate tangible blocks to control the virtual learning environment interface. All told, the choices that designers make around the input modalities for the virtual learning environment can play a significant role in how learners engage with the platforms, and the field&rsquo;s ability to study and understand participant learning.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/26/2023<br>\n\t\t\t\t\tModified by: Marcelo&nbsp;Worsley</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis award helped to advance the field\u2019s knowledge about learning spatial reasoning and computational thinking through virtual learning environments. The award included two well aligned threads (1) understanding how students practice spatial reasoning and computational thinking in popular virtual learning environments, and (2) identifying strategies for how using multimodal data (eye tracking, speech, tangibles, and gestures) might contribute to, and reflect, participant learning in virtual learning environments. Importantly, the use of multimodal data provided a richer means for the project team to study participant spatial reasoning and computational thinking practices. Regarding the first thread, the research team identified a combination of traditional and novel ways that users practice spatial reasoning and computational thinking within virtual learning environments. Virtual learning environments provide some affordances that are not available in the material world, and users often take advantage of those affordances to practice spatial skills in new ways. A combination of screen recording with eye tracking data helped the research team notice the specific spatial reasoning skills participants were utilizing. In terms of the second thread, the research team completed multiple iterations of the multimodal platform and observed considerable variation in youth preferences for the different modalities. Young learners (e.g., kindergarten to 2nd grade) were particularly drawn to using speech capabilities to overcome difficulties with spelling and typing. Our user studies with youth also surfaced considerable diversity in the types of gestures people wanted to use to control the virtual learning environments. Some students preferred static gestures while others preferred dynamic gestures. There was also variability in youth preferences for small-scale gestures (using one\u2019s finger) and whole-body gestures (using a combination of body parts). Future implementations of the multimodal interface will privilege the ability for participants to select and customize which gestures they want to utilize as they participate with the virtual learning environment. Finally, we observed adults being willing to participate in the virtual learning experience alongside their children when they could manipulate tangible blocks to control the virtual learning environment interface. All told, the choices that designers make around the input modalities for the virtual learning environment can play a significant role in how learners engage with the platforms, and the field\u2019s ability to study and understand participant learning.\n\n \n\n\t\t\t\t\tLast Modified: 06/26/2023\n\n\t\t\t\t\tSubmitted by: Marcelo Worsley"
 }
}