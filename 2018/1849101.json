{
 "awd_id": "1849101",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Pilot Study on Bias and Trust in AI Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2020-02-29",
 "tot_intn_awd_amt": 74529.0,
 "awd_amount": 74529.0,
 "awd_min_amd_letter_date": "2018-09-15",
 "awd_max_amd_letter_date": "2018-09-15",
 "awd_abstract_narration": "Robots and other autonomous systems are proliferating rapidly, and are being used in the public domain for such purposes as companionship, facilitating interactive learning, and making recommendations to users. However, little is understood about how trust, including mistrust and over-trust, and bias develop as humans interact with these systems. The investigators of this project will use basic real-life scenarios to understand human-human interaction in the context of trust and bias, and then will translate these data into quantized attributes.  This data will be used to develop general algorithms that will be useful in programming robots in ways that ensure human safety and well-being when interacting with robots. This project will conduct a series of pilot tests to explore fundamental research questions to find ways to minimize potential negative impacts and consequences of human-robot interaction. Additionally, the investigators will integrate the research with teaching and training of graduate and undergraduate students.\r\n\r\nThe aim of this project is to gain knowledge that informs the design of future robot systems by understanding how trust is established and how bias impacts human perception and algorithmic performance. As part of this work, the team also aims to establish baseline algorithms that mitigate the impacts of bias in autonomous systems. The specific research objectives of this project are to quantify the impact of trust and bias in human-robot interaction scenarios where the algorithms are designed based on learned data from human experts; and to develop methods for objectively mitigating bias, while still optimizing for robot performance.  The pilot tests are designed to contribute to understanding of social cognition in human-robot interaction. Direct societal befits will be gained as robots and other autonomous systems proliferate in the public domain.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ayanna",
   "pi_last_name": "Howard",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Ayanna M Howard",
   "pi_email_addr": "howard.1727@osu.edu",
   "nsf_id": "000108981",
   "pi_start_date": "2018-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Borenstein",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Jason D Borenstein",
   "pi_email_addr": "jason.borenstein@pubpolicy.gatech.edu",
   "nsf_id": "000265890",
   "pi_start_date": "2018-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue, NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 74529.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>There is a strong and ongoing push for robots to be used in more and more sectors in society. Key ethical problems related to the current use of robots and Artificial Intelligence (AI) are trust and bias; these problems are interconnected and must be addressed.&nbsp; Thus, the primary aim of this project was to obtain knowledge that could inform the design of future robotic and AI systems. &nbsp;Key objectives were to 1) quantify the impact of trust and bias in scenarios where AI algorithms have been trained based on historical data obtained from human beings and 2) develop algorithms that could lessen biases found in the resulting algorithms. In order to accomplish these objectives, the research team conducted four main studies.&nbsp; Results from these studies were published in a series of publications.&nbsp;</p>\n<p>The first study sought to quantify biases found in facial recognition algorithms, specifically those that might be used to scan the faces of children.&nbsp; The team found that emotion recognition systems display subpar performance on datasets of children's expressions. Based on these findings, the team developed a preliminary matrix that can be utilized for assessing whether there is sufficient diversity of people represented in a set of images used as a basis for emotion recognition.</p>\n<p>The second study involved quantifying the level of trust clinicians and parents have in pediatric robotic exoskeletons, specifically a robotic device attached to the child?s legs that can assist with their movements. &nbsp;Included in this assessment were clinicians that treated children who had any form of disability that affected movement, muscle control, and/or balance. Through study results, the team discovered that most clinicians were somewhat to very concerned that a child might not safely use a robotic exoskeleton outside of the clinical setting. Parents, on the other hand, reported higher trust (in other words, lower concern) about their child using an exoskeleton outside of the clinical setting.</p>\n<p>The third study sought to develop methods to mitigate bias by changing how robots communicate with people.&nbsp; This study examined the best way for a robot to communicate information to people while they performed a memory-based task and the impact on their performance if the robot provided incorrect information during the task. &nbsp;&nbsp;Findings indicated that the best form of communication from a robot, after it makes a mistake, occurs when a person directly asks the robot for help.</p>\n<p>The fourth study examined the impact of robot gender on people?s perception of a robot?s ability to perform a task. The research team discovered that perceived job competency is a better predictor for human trust than robot gender or the gender of the participant in the study. As such, developers should critically reconsider how they design robots in order to reduce the potential for the technology to perpetuate gender stereotypes.</p>\n<p>Results from this collection of research are designed to make important contributions by providing methods for understanding and quantifying the potential harmful effects that robots could have on society and providing a foundation for increasing the technology?s positive benefits. Project results could have significant value especially considering that placing too much trust in robots may place vulnerable populations at risk, including children and those with disabilities.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/01/2020<br>\n\t\t\t\t\tModified by: Ayanna&nbsp;M&nbsp;Howard</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1849101/1849101_10583171_1588375736153_GenderStudy--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1849101/1849101_10583171_1588375736153_GenderStudy--rgov-800width.jpg\" title=\"Gender Study for Robot\"><img src=\"/por/images/Reports/POR/2020/1849101/1849101_10583171_1588375736153_GenderStudy--rgov-66x44.jpg\" alt=\"Gender Study for Robot\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Robot used for two of the studies: Gender and Trust Study</div>\n<div class=\"imageCredit\">Georgia Institute of Technology</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Ayanna&nbsp;M&nbsp;Howard</div>\n<div class=\"imageTitle\">Gender Study for Robot</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThere is a strong and ongoing push for robots to be used in more and more sectors in society. Key ethical problems related to the current use of robots and Artificial Intelligence (AI) are trust and bias; these problems are interconnected and must be addressed.  Thus, the primary aim of this project was to obtain knowledge that could inform the design of future robotic and AI systems.  Key objectives were to 1) quantify the impact of trust and bias in scenarios where AI algorithms have been trained based on historical data obtained from human beings and 2) develop algorithms that could lessen biases found in the resulting algorithms. In order to accomplish these objectives, the research team conducted four main studies.  Results from these studies were published in a series of publications. \n\nThe first study sought to quantify biases found in facial recognition algorithms, specifically those that might be used to scan the faces of children.  The team found that emotion recognition systems display subpar performance on datasets of children's expressions. Based on these findings, the team developed a preliminary matrix that can be utilized for assessing whether there is sufficient diversity of people represented in a set of images used as a basis for emotion recognition.\n\nThe second study involved quantifying the level of trust clinicians and parents have in pediatric robotic exoskeletons, specifically a robotic device attached to the child?s legs that can assist with their movements.  Included in this assessment were clinicians that treated children who had any form of disability that affected movement, muscle control, and/or balance. Through study results, the team discovered that most clinicians were somewhat to very concerned that a child might not safely use a robotic exoskeleton outside of the clinical setting. Parents, on the other hand, reported higher trust (in other words, lower concern) about their child using an exoskeleton outside of the clinical setting.\n\nThe third study sought to develop methods to mitigate bias by changing how robots communicate with people.  This study examined the best way for a robot to communicate information to people while they performed a memory-based task and the impact on their performance if the robot provided incorrect information during the task.   Findings indicated that the best form of communication from a robot, after it makes a mistake, occurs when a person directly asks the robot for help.\n\nThe fourth study examined the impact of robot gender on people?s perception of a robot?s ability to perform a task. The research team discovered that perceived job competency is a better predictor for human trust than robot gender or the gender of the participant in the study. As such, developers should critically reconsider how they design robots in order to reduce the potential for the technology to perpetuate gender stereotypes.\n\nResults from this collection of research are designed to make important contributions by providing methods for understanding and quantifying the potential harmful effects that robots could have on society and providing a foundation for increasing the technology?s positive benefits. Project results could have significant value especially considering that placing too much trust in robots may place vulnerable populations at risk, including children and those with disabilities.\n\n\t\t\t\t\tLast Modified: 05/01/2020\n\n\t\t\t\t\tSubmitted by: Ayanna M Howard"
 }
}