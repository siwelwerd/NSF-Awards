{
 "awd_id": "1755992",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: SaTC: Democratizing Differential Privacy via Algorithms for Hybrid Models",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "James Joshi",
 "awd_eff_date": "2018-06-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2018-04-10",
 "awd_max_amd_letter_date": "2018-04-10",
 "awd_abstract_narration": "Individuals generate enormous amounts of personal data that are subsequently collected and stored by organizations and governments. The data powers many innovative applications in areas such as web services, health care, and transportation, but they also increase privacy risks. Differential privacy, a framework to rigorously reason about privacy properties of algorithms, holds tremendous promise for enabling privacy-preserving yet useful data analyses. However, its adoption has been limited to entities with massive user bases. This project aims to democratize the ability to deploy differential privacy by making it practical for entities with smaller user bases. \r\n\r\nResearch activities in this project include formulation of a new, hybrid, privacy model that models heterogeneous privacy preferences of individuals and current industry practices. The project also develops novel algorithms that preserve differential privacy while taking advantage of the hybrid model to improve utility outcomes, and evaluation of their performance. The work makes progress towards eliminating one of the significant barriers to data-driven innovation by expanding the applicability of differential privacy to a wider range of entities.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aleksandra",
   "pi_last_name": "Korolova",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aleksandra Korolova",
   "pi_email_addr": "korolova@princeton.edu",
   "nsf_id": "000702499",
   "pi_start_date": "2018-04-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S. Flower St.",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900890001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "CA37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "026Y00",
   "pgm_ele_name": "CRII CISE Research Initiation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The local model of differential privacy has recently emerged as one of the promising and desirable approaches for enabling data-driven innovation while preserving a rigorous and quantifiable notion of privacy, differential privacy. The local aspect of the model guarantees that privacy is protected even if the data curator's data store is fully breached. In parallel, a significant body of academic and industry work highlighted the wide theoretical and practical gaps between utility achievable in the traditional trusted curator model of differential privacy (where the curator ensures the privacy of its output but has access and can perform computations on raw individual data) and that achievable in the local model.</p>\n<p>Motivated by the desire to bridge the utility gap between local and trusted curator models of differential privacy, the project introduced a new model of trust -- the hybrid model. In this model, a majority of individuals contribute their data while receiving the local model's differential privacy guarantee, and a small minority -- via a trusted curator that ensures differential privacy after the data analysis.</p>\n<p>The project then performed both a theoretical and empirical study of the extent to which the hybrid model can enable an improvement on the utility of the local model for a variety of tasks. It developed and evaluated the performance of algorithms that can take advantage of the model, demonstrating the model's power for certain tasks, under a range of computational and interaction assumptions.&nbsp;<span>The affirmative findings of the model's power establish it, and the algorithms developed for it, as part of a promising toolkit for designing and implementing differential privacy.</span></p>\n<p>In addition to making steps that benefit society through democratizing differential privacy, the project&nbsp;<span>provided training and professional development to students through research opportunities, developed new teaching materials related to data privacy, and engaged in efforts to broaden participation in computing.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/03/2022<br>\n\t\t\t\t\tModified by: Aleksandra&nbsp;Korolova</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe local model of differential privacy has recently emerged as one of the promising and desirable approaches for enabling data-driven innovation while preserving a rigorous and quantifiable notion of privacy, differential privacy. The local aspect of the model guarantees that privacy is protected even if the data curator's data store is fully breached. In parallel, a significant body of academic and industry work highlighted the wide theoretical and practical gaps between utility achievable in the traditional trusted curator model of differential privacy (where the curator ensures the privacy of its output but has access and can perform computations on raw individual data) and that achievable in the local model.\n\nMotivated by the desire to bridge the utility gap between local and trusted curator models of differential privacy, the project introduced a new model of trust -- the hybrid model. In this model, a majority of individuals contribute their data while receiving the local model's differential privacy guarantee, and a small minority -- via a trusted curator that ensures differential privacy after the data analysis.\n\nThe project then performed both a theoretical and empirical study of the extent to which the hybrid model can enable an improvement on the utility of the local model for a variety of tasks. It developed and evaluated the performance of algorithms that can take advantage of the model, demonstrating the model's power for certain tasks, under a range of computational and interaction assumptions. The affirmative findings of the model's power establish it, and the algorithms developed for it, as part of a promising toolkit for designing and implementing differential privacy.\n\nIn addition to making steps that benefit society through democratizing differential privacy, the project provided training and professional development to students through research opportunities, developed new teaching materials related to data privacy, and engaged in efforts to broaden participation in computing.\n\n\t\t\t\t\tLast Modified: 02/03/2022\n\n\t\t\t\t\tSubmitted by: Aleksandra Korolova"
 }
}