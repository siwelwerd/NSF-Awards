{
 "awd_id": "1764077",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium: Interactive Debegging for Big Data Analytics",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 900000.0,
 "awd_amount": 900000.0,
 "awd_min_amd_letter_date": "2018-05-18",
 "awd_max_amd_letter_date": "2021-05-18",
 "awd_abstract_narration": "An abundance of data in science, engineering, national security, and health care has led to the emerging field of big data analytics.  To process massive quantities of data, developers leverage data-intensive scalable computing (DISC) systems in the cloud, such as Google's MapReduce, Apache Hadoop, and Apache Spark. While DISC systems help to address the scalability challenges of big data analytics, they also introduce an enormous challenge for data scientists in understanding and resolving errors. This project addresses the severe lack of debugging support in DISC systems today, which makes it difficult for data scientists to understand their applications, determine the causes of identified errors, and ensure that such errors are properly repaired. \r\n \r\nThe research provides two kinds of debugging support for big data processing programs in modern DISC systems like Apache Spark: new interactive, real-time debugging primitives for large-scale distributed processing and tool-assisted fault-localization services for big data. Technical approaches include a new data provenance technique for providing fine-grained visibility into large-scale distributed data processing and runtime optimizations for iterative development and debugging workloads. Tool-assisted fault localization services leverage these underlying provenance and optimization techniques to pinpoint and characterize the root causes of errors efficiently. Big data analytics is increasingly important in the 21st century, where daily lives leave behind a detailed digital record and decision-makers of all kinds, from companies to government agencies, would like to base their actions on data. The research contributes to improving productivity and correctness of big data applications, which is crucial for many disciplines that distill terabytes of low-value data into high-value insights.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Miryung",
   "pi_last_name": "Kim",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Miryung Kim",
   "pi_email_addr": "miryung@cs.ucla.edu",
   "nsf_id": "000676266",
   "pi_start_date": "2018-05-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Tyson",
   "pi_last_name": "Condie",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tyson Condie",
   "pi_email_addr": "tconde@cs.ucla.edu",
   "nsf_id": "000629463",
   "pi_start_date": "2018-05-18",
   "pi_end_date": "2020-08-06"
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "UCLA Computer Science Dept.",
  "perf_str_addr": "420 Westwood Plaza, 4532-H BH",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951596",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7798",
   "pgm_ref_txt": "SOFTWARE & HARDWARE FOUNDATION"
  },
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 436197.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 228282.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 235521.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p dir=\"ltr\"><span>To process massive quantities of data, developers leverage data-intensive scalable computing (DISC) systems in the cloud, such as Google's MapReduce, Apache Hadoop, and Apache Spark. While DISC systems address the scalability challenges of big data analytics, they also introduce new challenges in ensuring software correctness.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The goal of this project is to improve developer productivity during the debugging and testing of DISC analytics applications. This will enable developers to enhance software correctness, gain visibility into large-scale data processing, quickly diagnose the root causes of errors, and repair the discovered issues. To achieve this goal, we took a four-pronged approach:&nbsp;</span></p>\n<ol>\n<li>Data provenance for data-intensive scalable computing frameworks: To understand the flow of individual records within a data-parallel pipeline, we designed a data provenance capability called <a href=\"https://doi.org/10.1007/s00778-017-0474-5\">Titian</a>, which helps trace how errors propagate through data processing steps.&nbsp;</li>\n<li>Scalable debugging primitives for data-intensive scalable computing applications: &nbsp;Diagnosing errors in big data analytics applications is challenging due to the unclean nature of datasets, incorrect assumptions about the data, and bugs in program logic. By leveraging data provenance, we developed new scalable fault localization techniques, s such as <a href=\"https://doi.org/10.1145/2950290.2983930\">BigDebug</a> and <a href=\"https://doi.org/10.1145/3127479.3131624\">BigSift</a> that combines data provenance, database system optimizations, and delta-debugging to identify subsets of input data responsible for errors.&nbsp;</li>\n<li>Automated correctness and performance debugging techniques for data-intensive scalable computing applications: Performance debugging is difficult for big data applications. To address this, we developed a post-mortem performance debugging tool that computes and propagates record-level computation latencies to keep track of abnormally expensive records throughout the data provenance pipeline, such as <a href=\"https://doi.org/10.1145/3357223.3362727\">PerfDebug</a>. We also designed dynamic taint analysis approaches that improve precision of identified faults such as <a href=\"https://doi.org/10.1145/3472883.3487016\">OptDebug</a>, and <a href=\"https://doi.org/10.1145/3419111.3421292\">FlowDebug</a>.</li>\n<li>Automated testing for data-intensive scable computing applications: Modern big data applications pose new challenges for exhaustive, automated testing because they consist of dataflow operators, and user-defined functions (UDF), which are more complex than SQL queries. We design new white-box symbolic execution based testing approaches, called <a href=\"https://doi.org/10.1145/3338906.3338953\">BigTest</a> and <a href=\"https://dl.acm.org/doi/10.1145/3660825\">NaturalSym</a> to analyze the internal semantics of UDFs in tandem with the equivalence classes created by each dataflow and relational operator. To complement symbolic execution based exhaustive testing, we also developed new fuzz testing approaches such as <a href=\"https://doi.org/10.1145/3324884.3416641\">BigFuzz</a>, <a href=\"https://dl.acm.org/doi/10.1145/3611643.3616298\">DepFuzz</a>, and <a href=\"https://dl.acm.org/doi/10.1109/ASE56229.2023.00034\">NaturalFuzz</a> that increase test generation speed, improve bug detection capability, and improve naturalness of generated test inputs. &nbsp; &nbsp;</li>\n</ol>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\"><span>The project resulted in over twenty three publications in top conferences and journals, and it supported eight PhD students and postdoc researchers. The students and postdoc supported by this grant have gone on to become tenure-track faculty members at Virginia Tech and UC Riverside. The PI, Miryung Kim, delivered distinguished lectures on the &ldquo;Software Engineering for Big Data Analytics&rdquo; at institutions such as CMU and UIUC. She gave multiple keynote speaches, including at the 34th IEEE/ACM International Conference on Automated Software Engineering (Novemeber, 2019), titled as &ldquo;Software Engineering in a Data-Centric World.&rdquo;&nbsp;</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/20/2024<br>\nModified by: Miryung&nbsp;Kim</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nTo process massive quantities of data, developers leverage data-intensive scalable computing (DISC) systems in the cloud, such as Google's MapReduce, Apache Hadoop, and Apache Spark. While DISC systems address the scalability challenges of big data analytics, they also introduce new challenges in ensuring software correctness.\n\n\nThe goal of this project is to improve developer productivity during the debugging and testing of DISC analytics applications. This will enable developers to enhance software correctness, gain visibility into large-scale data processing, quickly diagnose the root causes of errors, and repair the discovered issues. To achieve this goal, we took a four-pronged approach:\n\nData provenance for data-intensive scalable computing frameworks: To understand the flow of individual records within a data-parallel pipeline, we designed a data provenance capability called Titian, which helps trace how errors propagate through data processing steps.\nScalable debugging primitives for data-intensive scalable computing applications: Diagnosing errors in big data analytics applications is challenging due to the unclean nature of datasets, incorrect assumptions about the data, and bugs in program logic. By leveraging data provenance, we developed new scalable fault localization techniques, s such as BigDebug and BigSift that combines data provenance, database system optimizations, and delta-debugging to identify subsets of input data responsible for errors.\nAutomated correctness and performance debugging techniques for data-intensive scalable computing applications: Performance debugging is difficult for big data applications. To address this, we developed a post-mortem performance debugging tool that computes and propagates record-level computation latencies to keep track of abnormally expensive records throughout the data provenance pipeline, such as PerfDebug. We also designed dynamic taint analysis approaches that improve precision of identified faults such as OptDebug, and FlowDebug.\nAutomated testing for data-intensive scable computing applications: Modern big data applications pose new challenges for exhaustive, automated testing because they consist of dataflow operators, and user-defined functions (UDF), which are more complex than SQL queries. We design new white-box symbolic execution based testing approaches, called BigTest and NaturalSym to analyze the internal semantics of UDFs in tandem with the equivalence classes created by each dataflow and relational operator. To complement symbolic execution based exhaustive testing, we also developed new fuzz testing approaches such as BigFuzz, DepFuzz, and NaturalFuzz that increase test generation speed, improve bug detection capability, and improve naturalness of generated test inputs.  \n\n\n\n\n\n\nThe project resulted in over twenty three publications in top conferences and journals, and it supported eight PhD students and postdoc researchers. The students and postdoc supported by this grant have gone on to become tenure-track faculty members at Virginia Tech and UC Riverside. The PI, Miryung Kim, delivered distinguished lectures on the Software Engineering for Big Data Analytics at institutions such as CMU and UIUC. She gave multiple keynote speaches, including at the 34th IEEE/ACM International Conference on Automated Software Engineering (Novemeber, 2019), titled as Software Engineering in a Data-Centric World.\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 11/20/2024\n\n\t\t\t\t\tSubmitted by: MiryungKim\n"
 }
}