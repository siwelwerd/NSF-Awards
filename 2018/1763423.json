{
 "awd_id": "1763423",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium: Fairness in Software Systems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2018-09-15",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 1050000.0,
 "awd_amount": 1107329.0,
 "awd_min_amd_letter_date": "2018-05-03",
 "awd_max_amd_letter_date": "2021-05-28",
 "awd_abstract_narration": "Software impacts society in many ways and increasingly automates decision-making. For example, software transcribes videos, translates documents, selects what news articles are promoted, and determines who gets a loan or gets hired. It is possible for software to exhibit bias in its operation, whether or not it is intended by the customers or developers of the software. For example, software might be more accurate at transcribing male voices than female ones. Or software may inject societal stereotypes into automated translations, and risk-assessment computations may exhibit racial bias. As more societal functions operate in cyberspace, the importance of software fairness increases. In these settings, data-driven software has the ability to shape human behavior: it affects the products we view and purchase, the news articles we read, the social interactions we engage in, and, ultimately, the opinions we form. Biases in data and software risk forming, propagating, and perpetuating biases in society. This project develops theory, techniques and tools to enable software designers and engineers to describe fairness requirements, test the software for fairness properties, and debug fairness defects. The outcomes of this project will help increase the society's trust in software decisions and in the data the software uses, in turn, increasing potential impact and benefits the software can bring to society.\r\n\r\nThe project addresses scientific questions behind efficiently and effectively measuring potential bias and helping stakeholders make informed decisions about software. It is not the project's aim to devise policies or eliminate bias in software. Instead, the aim is to provide software testing tools and measures that can be validated for formally specified software fairness properties. To measure bias, the project develops a novel approach for measuring causal relationships between program inputs and outputs. Software testing enables conducting causal experiments consisting of running the software with nearly identical inputs that vary only in a key input characteristic under test. Variations in an input characteristic that affect execution behavior provide evidence of a causal relationship. The project identifies when causal relationships are appropriate for measuring potential bias, develops efficient testing methods for measuring these relationships, and creates tools and techniques to help engineers identify and modify the causes of these relationships.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yuriy",
   "pi_last_name": "Brun",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuriy Brun",
   "pi_email_addr": "brun@cs.umass.edu",
   "nsf_id": "000559414",
   "pi_start_date": "2018-05-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alexandra",
   "pi_last_name": "Meliou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alexandra Meliou",
   "pi_email_addr": "ameli@cs.umass.edu",
   "nsf_id": "000637003",
   "pi_start_date": "2018-05-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039242",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  },
  {
   "pgm_ref_code": "2878",
   "pgm_ref_txt": "SPECIAL PROJECTS - CCF"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  },
  {
   "pgm_ref_code": "CL10",
   "pgm_ref_txt": "CLB-Career Life Balance"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 594224.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 455776.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 57329.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern software systems routinely rely on machine learning and artificial intelligence. Such systems play an influential role in making important decisions in our society, including in healthcare, criminal justice, finance, governance, and education. Unfortunately, abundant evidence exists that such systems can exhibit unsafe or discriminatory behavior, such as suggesting potentially fatal medical treatments, making racist or sexist predictions, exhibiting worse performance for people of some races or genders than others, or facilitating radicalization and polarization. This project has created a formal foundation of safety and bias in software systems and has developed technology for detecting and mitigating unsafe and discriminatory behavior in software systems.</p>\n<p>Intellectual Merit: The project has developed novel technology for increasing safety and fairness in software systems. This technology includes (1) methods for domain experts to specify desirable safety and fairness software properties; (2) methods for auditing and testing existing software systems for evidence of unsafe and discriminatory behavior; (3) efficient algorithms for providing diversification guarantees in data selection; (4) methods for training machine-learning-based software components while providing high-probability guarantees that the components will not violate specified safety and fairness properties; (5) data-driven re-weighting methods for mitigating bias in learned classifiers; and (6) methodology for measuring how unsafe and unfair behavior affect users' trust in software systems. Importantly, the technology was specifically designed such that its users are not required to be experts in machine learning, enabling domain experts, such as doctors, lawyers, and policy makers, to specify desired properties and software engineers to build and use the safe and fair software components. The technology was shown to work in situations where the available training data come from a different distribution than the data available during deployment, and where definitions of safety and fairness are complex and require the measurement of a delayed impact of decisions made by the software.</p>\n<p>Broader Impacts: Techniques to improve safety and fairness in software can lead to higher-quality software and more responsible use of artificial intelligence and machine learning. Software systems that provide guarantees on their behavior can improve outcomes and increase users' trust in those systems. Additionally, this project developed educational modules for use in software engineering and data science classes to improve understanding of responsible artificial intelligence use, better preparing students for jobs in the software engineering industry. Finally, this project supported the training of multiple PhD, MS, and undergraduate students.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/18/2024<br>\nModified by: Yuriy&nbsp;Brun</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nModern software systems routinely rely on machine learning and artificial intelligence. Such systems play an influential role in making important decisions in our society, including in healthcare, criminal justice, finance, governance, and education. Unfortunately, abundant evidence exists that such systems can exhibit unsafe or discriminatory behavior, such as suggesting potentially fatal medical treatments, making racist or sexist predictions, exhibiting worse performance for people of some races or genders than others, or facilitating radicalization and polarization. This project has created a formal foundation of safety and bias in software systems and has developed technology for detecting and mitigating unsafe and discriminatory behavior in software systems.\n\n\nIntellectual Merit: The project has developed novel technology for increasing safety and fairness in software systems. This technology includes (1) methods for domain experts to specify desirable safety and fairness software properties; (2) methods for auditing and testing existing software systems for evidence of unsafe and discriminatory behavior; (3) efficient algorithms for providing diversification guarantees in data selection; (4) methods for training machine-learning-based software components while providing high-probability guarantees that the components will not violate specified safety and fairness properties; (5) data-driven re-weighting methods for mitigating bias in learned classifiers; and (6) methodology for measuring how unsafe and unfair behavior affect users' trust in software systems. Importantly, the technology was specifically designed such that its users are not required to be experts in machine learning, enabling domain experts, such as doctors, lawyers, and policy makers, to specify desired properties and software engineers to build and use the safe and fair software components. The technology was shown to work in situations where the available training data come from a different distribution than the data available during deployment, and where definitions of safety and fairness are complex and require the measurement of a delayed impact of decisions made by the software.\n\n\nBroader Impacts: Techniques to improve safety and fairness in software can lead to higher-quality software and more responsible use of artificial intelligence and machine learning. Software systems that provide guarantees on their behavior can improve outcomes and increase users' trust in those systems. Additionally, this project developed educational modules for use in software engineering and data science classes to improve understanding of responsible artificial intelligence use, better preparing students for jobs in the software engineering industry. Finally, this project supported the training of multiple PhD, MS, and undergraduate students.\n\n\n\t\t\t\t\tLast Modified: 11/18/2024\n\n\t\t\t\t\tSubmitted by: YuriyBrun\n"
 }
}