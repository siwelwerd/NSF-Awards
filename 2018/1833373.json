{
 "awd_id": "1833373",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CSR: Medium: Collaborative Research: Scale-Out Near-Data Acceleration of Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2018-02-07",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-04-20",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "A growing number of commercial and enterprise systems increasingly rely on machine learning algorithms. This shift is, on the one hand, due to the breakthroughs in machine learning algorithms that extract insights from massive amounts of data. Therefore, such systems need to process ever-increasing amounts of data, demanding higher memory bandwidth and capacity. However, the bandwidth between processors and off-chip memory has not increased due to various stringent physical constraints. Besides, data transfers between the processors and the off-chip memory consume orders of magnitude more energy than on-chip computation due to the disparity between interconnection and transistor scaling.\r\n\r\nExploiting recent 3D-stacking technology, the researcher community has explored near-data processing architectures that place processors and memory on the same chip. However, it is unclear whether or not such processing-in-memory (PIM) attempts will be successful for commodity computing systems due to the high cost of 3D-stacking technology and demanded change in existing processor, memory and/or applications. Faced with these challenges, the PIs are to investigate near-data processing platforms that do not require any change in processor, memory and applications, exploiting deep insights on commodity memory subsystems and network software stack. The success of this project will produce inexpensive but powerful near-data processing platforms that can directly run existing machine learning applications without any modification.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hadi",
   "pi_last_name": "Esmaeilzadeh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hadi Esmaeilzadeh",
   "pi_email_addr": "hadi@eng.ucsd.edu",
   "nsf_id": "000653497",
   "pi_start_date": "2018-04-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 235643.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 130040.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 134317.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-7bd5b0a3-7fff-590b-2a48-67a4300af365\">\n<p dir=\"ltr\"><span>Advances in Machine Learning (ML) are revolutionizing every aspect of our lives. However, such transformative effects are predicated on providing (1) high-performance computing capabilities that enable the learning algorithms and (2) trustable learning algorithms that can adapt to adversarial conditions. This project takes a holistic approach to building a novel, specialized, and automated compute stack for immersive machine intelligence. This project redefines the data/software/hardware abstractions and enables seamless utilization of hardware specialization from domain-specific programming languages by backing them with automated hardware generation toolsets and novel system software primitives. Our end-to-end innovations push the boundary on all aspects of the computing stack including model optimizations and inference privacy, multi-target cross-domain compilation, and template reconfigurable architectures.</span></p>\n<p dir=\"ltr\"><span>Model Pruning and Quantization reduce the model size and improve inference efficiency. SnaPEA(ISCA&rsquo;18) dynamically prunes the CNN by cutting convolution operations short if it determines that the output will be negative. LeOPArd (ISCA&rsquo;22) dynamically prunes attention computation by formulating the finding of the layer-wise pruning thresholds as a differentiable regularizer. DCQ(PMLR&rsquo;19) accelerates quantized model training by dividing a full-precision network into multiple sections, each of which exposes intermediate features to train a team of students parallelly in the quantized domain and stitching them afterward. ReLeQ(IEEE Micro&rsquo;20) uses Reinforcement Learning to learn the sensitivity of final classification accuracy with respect to the bit-width of each layer while keeping classification accuracy.</span></p>\n<p dir=\"ltr\"><span>Privacy is a central concern for ML applications. To improve inference privacy in the cloud, Shredder (ASPLOS&rsquo;20) learns and applies additive noise distributions that significantly reduce the information content of communicated data without altering the network. Cloak (WWW&rsquo;21) sifts out data irrelevant to the prediction service prior to sending the data to the cloud, thus enabling access to the services with much greater privacy.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Compiling high-performance code to a diverse set of accelerators is an integral part of unlocking the power of acceleration. To enable efficient compilation to a single target, Glimpse (DAC&rsquo;22, Best Paper Nomination) incorporates the mathematical embedding of the hardware specification to better guide the compiler search algorithm and improve the sample efficiency and the search. To enable cross-domain acceleration, PolyMath(HPCA&rsquo;21) introduces a cross-domain compilation stack that builds upon 1) a cross-domain language that exploits the mathematical similarities across multiple domains, and 2) an intermediate representation that enables simultaneous access to all levels of operation granularity to provide flexible compilation to different accelerators. The Codelet Compiler proposes Architecture Covenant Graph, an architecture abstraction, and Codelets, a DNN layer representation, to enable the compiler to simultaneously compile to different architectures while reusing the compilation paths. Other compiler work includes SERENITY(MLSys&rsquo;20) and Yin-Yang(IEEE Micro&rsquo;22).&nbsp;</span></p>\n<p dir=\"ltr\"><span>Our innovations in architecture span across the entire hardware stack. At the bit level, Bit Fusion(ISCA&rsquo;18) introduces a DNN accelerator that constitutes an array of bit-level processing elements that dynamically fuse to match the bit-width of individual DNN layers to enable dynamic bit-level fusion. At the circuit level, SiMul((IEEE Micro&rsquo;18) proposed an approximate multiplier that supports a tradeoff between compute precision and energy consumption at runtime. At the architecture level, Planaria(MICRO&rsquo;20) introduced the first spacial simultaneous multi-tenant DNN accelerator that can dynamically break into multiple smaller yet full-fledged DNN engines at runtime. At the system level, INCEPTION (MICRO&rsquo;18) accelerates distributed training by embedding data compression accelerators running a hardware-friendly lossy-compression algorithm in the Network Interface Cards. Other architecture works include AxMemo(ISCA&rsquo;18), AXRAM(PACT&rsquo;18), BIHIWE(PACT&rsquo;20), Bit-Parallel Vector Composability(DAC&rsquo;20), and Tandem Processor (ASPLOS&rsquo;24).&nbsp;</span></p>\n<p dir=\"ltr\"><span>Our end-to-end frameworks builds upon innovations in each layer of the computing stack to tackle challenges in ML acceleration. DNNWEAVER(MICRO&rsquo;16) introduced a framework that automatically generates a synthesizable accelerator for a given (DNN, FPGA) pair from a high-level specification. CoSMIC(MICRO&rsquo;17) introduced a full computing stack constituting language, compiler, system software, template architecture, and circuit generators, that enable programmable acceleration of learning at scale. Other End-to-End framework covers application domains such as robotics (RoboX, ISCA&rsquo;18), data analytics (DAnA, VLDB&rsquo;18), Reinforcement Learning (FlexiGan, FCCM&rsquo;18), and Automatic Circuit Generation (VeriGood-ML, ICCAD&rsquo;21).</span></p>\n<p dir=\"ltr\"><span>Our work is a multidisciplinary work that brings together elements from computer architecture, software engineering, and machine learning. Our full-stack, scale-out, near-data acceleration systems cost-effectively improve the performance and energy efficiency of a wide range of emerging machine-learning applications. Such benefits provide economic, environmental, and social advantages as the IT industry is moving toward harvesting insights from the ever-increasing corpus of data. Besides over 20 publications in top research venues, we organized workshops and open-sourced our software and hardware platform artifacts for experimentation and further research in academia and industry. Furthermore, this interdisciplinary project enables us to teach the key concepts of synergistic interactions amongst machine learning algorithms, processor architecture, memory systems, networks, and operating systems to many students. Finally, we involved under-represented students through tasks designed to foster their interest in advanced degrees in computing.</span></p>\n</span></p><br>\n<p>\n Last Modified: 12/27/2023<br>\nModified by: Hadi&nbsp;Esmaeilzadeh</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nAdvances in Machine Learning (ML) are revolutionizing every aspect of our lives. However, such transformative effects are predicated on providing (1) high-performance computing capabilities that enable the learning algorithms and (2) trustable learning algorithms that can adapt to adversarial conditions. This project takes a holistic approach to building a novel, specialized, and automated compute stack for immersive machine intelligence. This project redefines the data/software/hardware abstractions and enables seamless utilization of hardware specialization from domain-specific programming languages by backing them with automated hardware generation toolsets and novel system software primitives. Our end-to-end innovations push the boundary on all aspects of the computing stack including model optimizations and inference privacy, multi-target cross-domain compilation, and template reconfigurable architectures.\n\n\nModel Pruning and Quantization reduce the model size and improve inference efficiency. SnaPEA(ISCA18) dynamically prunes the CNN by cutting convolution operations short if it determines that the output will be negative. LeOPArd (ISCA22) dynamically prunes attention computation by formulating the finding of the layer-wise pruning thresholds as a differentiable regularizer. DCQ(PMLR19) accelerates quantized model training by dividing a full-precision network into multiple sections, each of which exposes intermediate features to train a team of students parallelly in the quantized domain and stitching them afterward. ReLeQ(IEEE Micro20) uses Reinforcement Learning to learn the sensitivity of final classification accuracy with respect to the bit-width of each layer while keeping classification accuracy.\n\n\nPrivacy is a central concern for ML applications. To improve inference privacy in the cloud, Shredder (ASPLOS20) learns and applies additive noise distributions that significantly reduce the information content of communicated data without altering the network. Cloak (WWW21) sifts out data irrelevant to the prediction service prior to sending the data to the cloud, thus enabling access to the services with much greater privacy.\n\n\nCompiling high-performance code to a diverse set of accelerators is an integral part of unlocking the power of acceleration. To enable efficient compilation to a single target, Glimpse (DAC22, Best Paper Nomination) incorporates the mathematical embedding of the hardware specification to better guide the compiler search algorithm and improve the sample efficiency and the search. To enable cross-domain acceleration, PolyMath(HPCA21) introduces a cross-domain compilation stack that builds upon 1) a cross-domain language that exploits the mathematical similarities across multiple domains, and 2) an intermediate representation that enables simultaneous access to all levels of operation granularity to provide flexible compilation to different accelerators. The Codelet Compiler proposes Architecture Covenant Graph, an architecture abstraction, and Codelets, a DNN layer representation, to enable the compiler to simultaneously compile to different architectures while reusing the compilation paths. Other compiler work includes SERENITY(MLSys20) and Yin-Yang(IEEE Micro22).\n\n\nOur innovations in architecture span across the entire hardware stack. At the bit level, Bit Fusion(ISCA18) introduces a DNN accelerator that constitutes an array of bit-level processing elements that dynamically fuse to match the bit-width of individual DNN layers to enable dynamic bit-level fusion. At the circuit level, SiMul((IEEE Micro18) proposed an approximate multiplier that supports a tradeoff between compute precision and energy consumption at runtime. At the architecture level, Planaria(MICRO20) introduced the first spacial simultaneous multi-tenant DNN accelerator that can dynamically break into multiple smaller yet full-fledged DNN engines at runtime. At the system level, INCEPTION (MICRO18) accelerates distributed training by embedding data compression accelerators running a hardware-friendly lossy-compression algorithm in the Network Interface Cards. Other architecture works include AxMemo(ISCA18), AXRAM(PACT18), BIHIWE(PACT20), Bit-Parallel Vector Composability(DAC20), and Tandem Processor (ASPLOS24).\n\n\nOur end-to-end frameworks builds upon innovations in each layer of the computing stack to tackle challenges in ML acceleration. DNNWEAVER(MICRO16) introduced a framework that automatically generates a synthesizable accelerator for a given (DNN, FPGA) pair from a high-level specification. CoSMIC(MICRO17) introduced a full computing stack constituting language, compiler, system software, template architecture, and circuit generators, that enable programmable acceleration of learning at scale. Other End-to-End framework covers application domains such as robotics (RoboX, ISCA18), data analytics (DAnA, VLDB18), Reinforcement Learning (FlexiGan, FCCM18), and Automatic Circuit Generation (VeriGood-ML, ICCAD21).\n\n\nOur work is a multidisciplinary work that brings together elements from computer architecture, software engineering, and machine learning. Our full-stack, scale-out, near-data acceleration systems cost-effectively improve the performance and energy efficiency of a wide range of emerging machine-learning applications. Such benefits provide economic, environmental, and social advantages as the IT industry is moving toward harvesting insights from the ever-increasing corpus of data. Besides over 20 publications in top research venues, we organized workshops and open-sourced our software and hardware platform artifacts for experimentation and further research in academia and industry. Furthermore, this interdisciplinary project enables us to teach the key concepts of synergistic interactions amongst machine learning algorithms, processor architecture, memory systems, networks, and operating systems to many students. Finally, we involved under-represented students through tasks designed to foster their interest in advanced degrees in computing.\n\t\t\t\t\tLast Modified: 12/27/2023\n\n\t\t\t\t\tSubmitted by: HadiEsmaeilzadeh\n"
 }
}