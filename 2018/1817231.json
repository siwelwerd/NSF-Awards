{
 "awd_id": "1817231",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Fast and Accurate Natural Language Parsing and Generation by Marrying Deep Learning with Dynamic Programming",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2018-07-29",
 "awd_max_amd_letter_date": "2019-07-18",
 "awd_abstract_narration": "This grant aims to improve automatic understanding, generation, and translation of natural language by machines. Automated natural language processing has already changed the way we interact with digital assistants such as Amazon Echo and Apple Siri on smartphones and other devices.  Automated machine translation reduces information barriers on the Web, where most information is inaccessible to most users because it is in a language they do not understand.  Today's natural language systems, however, are limited to short exchanges and often make errors.  These limitations are due to need for the systems to respond very quickly: current methods for more accurate understanding and generation take too long.  This project will overcome this problem by developing new, fast, principled algorithms for these tasks.  This project also supports STEM education of underrepresented minorities (who do not speak English natively) by recruiting them in machine translation studies.\r\n\r\nThis grant aims to construct fast (linear-time) and accurate natural language parsers and generators (including translators) that utilize the power of both deep learning (for accurate and automatic feature engineering) and dynamic programming (to speed up the search). This project focuses on Recurrent Neural Network-based models (RNNs) such as Long Short Term Memory (LSTMs). In particular, this project aims to (1) Develop linear-time dynamic programming-based neural parsers by using RNNs to summarize the input text and extend them to joint syntactic-discourse parsing and predictive parsing.  (2) Develop approximate dynamic programming algorithms and principled beam search methods for text generation and machine translation systems that use RNN-based decoders to model output text.  (3) Combine the above two directions with an innovative application of simultaneous translation using predictive parsing.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Liang",
   "pi_last_name": "Huang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Liang Huang",
   "pi_email_addr": "huanlian@oregonstate.edu",
   "nsf_id": "000629761",
   "pi_start_date": "2018-07-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon State University",
  "inst_street_address": "1500 SW JEFFERSON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CORVALLIS",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5417374933",
  "inst_zip_code": "973318655",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OR04",
  "org_lgl_bus_name": "OREGON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MZ4DYXE1SL98"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon State University",
  "perf_str_addr": "1148 Kelley Engineering Center",
  "perf_city_name": "Corvallis",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "973315501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OR04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 127456.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 272544.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-e9edc370-7fff-84b4-945f-1425555af29c\"> </span></p>\n<p dir=\"ltr\"><span>This project aims to improve automatic understanding, generation, and translation of natural language by combining modern deep learning models with classical algorithms and techniques such as grammar formalisms and dynamic programming.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Specifically, we achieved the following results:</span></p>\n<ol>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We made major advances in natural language generation and generation machine translation, including learning to stop (NAACL 2019), multireference training (EMNLP 2018), speeding up neural translation (EMNLP 2018), better beam search (EMNLP 2018), and ensemble sequence level training for multimodal translation (WMT 2018).</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We made significant improvements in simultaneous translation, including opportunistic decoding and timely correction (ACL 2020), direct speech-to-text simultaneous translation (ACL 2021 findings) and creating a new training dataset for simultaneous translation by rewriting existing parallel text corpora to reduce unnecessary reorderings (EMNLP 2021).</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We also applied linear-time parsing and classical machine learning techniques to RNA biology, which proved to be extremely relevant during the COVID-19 pandemic because of the extreme length of SARS-CoV-2. Its super long genome, with ~30,000 nucleotides, rendered classical cubic-time folding algorithms intractable. We successfully adapted linear-time parsing algorithms to RNA folding and devised the first lineart-time RNA folding algorithm, LinearFold, which takes only 27 seconds to fold the full-length SARS-CoV-2 genome. Based on this breakthrough, we further developed LinearPartition, LinearSampling, and most importantly, LinearTurboFold. The last one was the first homologous folding algorithm to scale to full-length SARS-CoV-2 variants, and found 30+ &ldquo;druggable&rdquo; regions. It was published by the Proceedings of National Academy of Sciences (PNAS), one of the top three general science journals.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Results from this grant were published in 8 refereed conference papers, 4 high impact journal papers, and 4 dissertations.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>This grant partially supported four (4) PhD students, one of whom are female. All of these students have successfully graduated and are working in research and development after graduation.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>This grant supported an undergraduate research assistant, who co-authored the EMNLP 2021 paper and went to graduate school at a top university.</span></p>\n</li>\n</ol>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/23/2023<br>\n\t\t\t\t\tModified by: Liang&nbsp;Huang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis project aims to improve automatic understanding, generation, and translation of natural language by combining modern deep learning models with classical algorithms and techniques such as grammar formalisms and dynamic programming. \n\n \nSpecifically, we achieved the following results:\n\n\nWe made major advances in natural language generation and generation machine translation, including learning to stop (NAACL 2019), multireference training (EMNLP 2018), speeding up neural translation (EMNLP 2018), better beam search (EMNLP 2018), and ensemble sequence level training for multimodal translation (WMT 2018).\n\n\nWe made significant improvements in simultaneous translation, including opportunistic decoding and timely correction (ACL 2020), direct speech-to-text simultaneous translation (ACL 2021 findings) and creating a new training dataset for simultaneous translation by rewriting existing parallel text corpora to reduce unnecessary reorderings (EMNLP 2021).\n\n\nWe also applied linear-time parsing and classical machine learning techniques to RNA biology, which proved to be extremely relevant during the COVID-19 pandemic because of the extreme length of SARS-CoV-2. Its super long genome, with ~30,000 nucleotides, rendered classical cubic-time folding algorithms intractable. We successfully adapted linear-time parsing algorithms to RNA folding and devised the first lineart-time RNA folding algorithm, LinearFold, which takes only 27 seconds to fold the full-length SARS-CoV-2 genome. Based on this breakthrough, we further developed LinearPartition, LinearSampling, and most importantly, LinearTurboFold. The last one was the first homologous folding algorithm to scale to full-length SARS-CoV-2 variants, and found 30+ \"druggable\" regions. It was published by the Proceedings of National Academy of Sciences (PNAS), one of the top three general science journals.\n\n\nResults from this grant were published in 8 refereed conference papers, 4 high impact journal papers, and 4 dissertations.\n\n\nThis grant partially supported four (4) PhD students, one of whom are female. All of these students have successfully graduated and are working in research and development after graduation.\n\n\nThis grant supported an undergraduate research assistant, who co-authored the EMNLP 2021 paper and went to graduate school at a top university.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 02/23/2023\n\n\t\t\t\t\tSubmitted by: Liang Huang"
 }
}