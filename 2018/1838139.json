{
 "awd_id": "1838139",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Privacy in Unsupervised Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928950",
 "po_email": "rwachter@nsf.gov",
 "po_sign_block_name": "Ralph Wachter",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 911398.0,
 "awd_amount": 911398.0,
 "awd_min_amd_letter_date": "2018-09-11",
 "awd_max_amd_letter_date": "2020-11-23",
 "awd_abstract_narration": "Modern data sets are largely unlabeled. Unsupervised learning of useful representations to better understand the structure in data is a critical challenge in data science and machine learning; it finds application in computational and social science, including information retrieval, web mining, and recommendation systems. As we progress further into the age of Big data, and the amount of data to be processed grows faster than the growth in our computational resources, better and faster ways for performing unsupervised learning and data analysis on such big data sets become ever more necessary. Furthermore, with the advent of the internet of things, private data is collected rather ubiquitously and seamlessly through devices such as smartphones, cameras, microphones, radio-frequency identification (RFID) readers, and social networks, raising serious concerns about an individual's privacy. Therefore, in this project, we initiate a formal investigation into privacy-aware unsupervised learning for Big data applications.\r\n\r\nTaking a stochastic optimization view of unsupervised learning, we capture more general learning problems than previously studied in the privacy literature. One such class of learning problems is non-convex problems, such as matrix learning, tensor factorization, deep learning, and many more. While most of these problems are NP-hard, in practice we find that we can efficiently find solutions to these problems. We conjecture that noisy stochastic gradient descent updates that have recently been shown to efficiently find local minima for a large class of non-convex problems also guarantees privacy implicitly. Finally, we consider extensions of the privacy model from that of a single curator to those to distributed learning, continual release model, streaming model, and a novel sliding window model.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Raman",
   "pi_last_name": "Arora",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Raman Arora",
   "pi_email_addr": "arora@cs.jhu.edu",
   "nsf_id": "000656458",
   "pi_start_date": "2018-09-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400, N Charles St",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 911398.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">With machine learning and artificial intelligence (AI) becoming uber pervasive and increasingly woven into our social fabric, there is a growing concern about its potential misuse, wittingly or unwittingly. There have been documented incidents of machine learning models encoding user data and various stereotypes, racial identities, and other sensitive attributes, leading to erosion of personal privacy, spread of false information (e.g., deepfake), and a general distrust of data-based systems as biased, unfair, and vulnerable to attacks. It is paramount, therefore, to carefully re-examine the machine learning pipeline from data collection to field deployment and develop mathematical and algorithmic foundations for responsible computing.&nbsp;<span class=\"s1\">To that end, in this project, we focused on the design of algorithms with provable learning guarantees while ensuring privacy.<span>&nbsp;</span></span></p>\n<p class=\"p1\"><strong>Private Unsupervised Learning:</strong>&nbsp;The techniques developed as part of this project have advanced our understanding of private algorithms for machine learning. We have presented private algorithms for unsupervised approaches such as subspace learning and robust principal component analysis (PCA), which are ubiquitous tools in much of machine learning and data science. Private graph sparsification enables a host of private analysis techniques for graph-related queries. The sliding window model is relevant to many real-world time series applications.&nbsp;&nbsp;</p>\n<p class=\"p1\"><strong>Differentially Private Stochastic Convex Optimization:</strong>&nbsp;Our work on differentially private algorithms for stochastic convex optimization fills several gaps in the existing literature and provides provably optimal algorithms for a large class of convex learning problems. We provide state-of-the-art algorithms for approximating stationary points of the population risk for non-convex loss functions.</p>\n<p class=\"p1\"><strong>Machine Unlearning/Data Deletion:</strong>&nbsp;Another major effort on this project was towards designing machine learning algorithms that also admit optimal and efficient unlearning, i.e., when a request is made to delete or remove a specific example from the training set, such an operation can be completed efficiently. The motivation for this problem stems from recent awareness of data ownership and privacy concerns. Various laws and regulations now grant a user the right to request any entity holding its data be deleted from their database, thereby upholding a user's \"<em>right to be forgotten</em>.\" We give the first algorithms for data deletion in machine learning problems that are posed as empirical risk minimization by identifying new algorithmic principles based on algorithmic stability. In particular, we show that if an algorithm is Total Variation (TV) stable, then there exists an exact unlearning method. For risk minimization problems, we give TV stable learning algorithms based on noisy Stochastic Gradient descent (SGD), a popular algorithm used in differential privacy. We design corresponding efficient unlearning algorithms based on constructing a (maximal) coupling of Markov chains for the noisy SGD procedure. For stochastic convex optimization, we show that this method has a smaller runtime than retraining, thereby establishing a novel separation between retraining and efficient unlearning.</p>\n<p class=\"p1\"><strong>Practical Implications:</strong>&nbsp;While being very theoretical, our work has clear, practical, real-world implications. For example, with the Machine UnLearning problem, we have made seminal contributions by advocating for the strongest possible notion of exact unlearning and also showing how to design novel unlearning algorithms by making a very nice connection with techniques in optimal transport and leveraging a notion of total variation stability. This work has received much attention and has been the basis of numerous follow-up studies.</p>\n<p class=\"p1\"><strong>Human Resource Development:&nbsp;</strong>The project requested funding for two graduate students and a postdoc. One of the graduate students who was supported by this award successfully defended his thesis and has started a position at Google NYC as a research scientist in the learning theory group. The other graduate student recently defended his thesis titled \"<em>Statistical Learning via Stochastic&nbsp;Optimization under Data Privacy Considerations</em>\" and has joined Meta as a research scientist, focusing on developing a theoretical foundation for machine learning with privacy considerations. The award has also supported two excellent postdocs. The postdoc who worked on the project during the first year has started a tenure track faculty at Rutgers University after a short stint at Apple Research working with the privacy team. A second postdoc who worked on this project over the past year has accepted a research scientist position at Google, Mountain View.</p>\n<p class=\"p1\"><strong>Curriculum Development:&nbsp;</strong>A new interdisciplinary course titled \"<em>Advanced Topics in Machine Learning\"</em>&nbsp;developed by PI as part of the curriculum development activities has attracted students from computer science, electrical and computer engineering, applied math, and statistics. One of the goals of this course is to impart students with the necessary skills to thrive in today's job market. The utility of the course to a wide audience will be evaluated through periodic feedback from students and an open exhibition of the course projects. The PI also organized a year-long weekly lecture series focusing on the foundations of differential privacy with the purpose of training graduate students to conduct research at the forefront of learning with privacy considerations.</p><br>\n<p>\n Last Modified: 02/12/2024<br>\nModified by: Raman&nbsp;Arora</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nWith machine learning and artificial intelligence (AI) becoming uber pervasive and increasingly woven into our social fabric, there is a growing concern about its potential misuse, wittingly or unwittingly. There have been documented incidents of machine learning models encoding user data and various stereotypes, racial identities, and other sensitive attributes, leading to erosion of personal privacy, spread of false information (e.g., deepfake), and a general distrust of data-based systems as biased, unfair, and vulnerable to attacks. It is paramount, therefore, to carefully re-examine the machine learning pipeline from data collection to field deployment and develop mathematical and algorithmic foundations for responsible computing.To that end, in this project, we focused on the design of algorithms with provable learning guarantees while ensuring privacy.\n\n\nPrivate Unsupervised Learning:The techniques developed as part of this project have advanced our understanding of private algorithms for machine learning. We have presented private algorithms for unsupervised approaches such as subspace learning and robust principal component analysis (PCA), which are ubiquitous tools in much of machine learning and data science. Private graph sparsification enables a host of private analysis techniques for graph-related queries. The sliding window model is relevant to many real-world time series applications.\n\n\nDifferentially Private Stochastic Convex Optimization:Our work on differentially private algorithms for stochastic convex optimization fills several gaps in the existing literature and provides provably optimal algorithms for a large class of convex learning problems. We provide state-of-the-art algorithms for approximating stationary points of the population risk for non-convex loss functions.\n\n\nMachine Unlearning/Data Deletion:Another major effort on this project was towards designing machine learning algorithms that also admit optimal and efficient unlearning, i.e., when a request is made to delete or remove a specific example from the training set, such an operation can be completed efficiently. The motivation for this problem stems from recent awareness of data ownership and privacy concerns. Various laws and regulations now grant a user the right to request any entity holding its data be deleted from their database, thereby upholding a user's \"right to be forgotten.\" We give the first algorithms for data deletion in machine learning problems that are posed as empirical risk minimization by identifying new algorithmic principles based on algorithmic stability. In particular, we show that if an algorithm is Total Variation (TV) stable, then there exists an exact unlearning method. For risk minimization problems, we give TV stable learning algorithms based on noisy Stochastic Gradient descent (SGD), a popular algorithm used in differential privacy. We design corresponding efficient unlearning algorithms based on constructing a (maximal) coupling of Markov chains for the noisy SGD procedure. For stochastic convex optimization, we show that this method has a smaller runtime than retraining, thereby establishing a novel separation between retraining and efficient unlearning.\n\n\nPractical Implications:While being very theoretical, our work has clear, practical, real-world implications. For example, with the Machine UnLearning problem, we have made seminal contributions by advocating for the strongest possible notion of exact unlearning and also showing how to design novel unlearning algorithms by making a very nice connection with techniques in optimal transport and leveraging a notion of total variation stability. This work has received much attention and has been the basis of numerous follow-up studies.\n\n\nHuman Resource Development:The project requested funding for two graduate students and a postdoc. One of the graduate students who was supported by this award successfully defended his thesis and has started a position at Google NYC as a research scientist in the learning theory group. The other graduate student recently defended his thesis titled \"Statistical Learning via StochasticOptimization under Data Privacy Considerations\" and has joined Meta as a research scientist, focusing on developing a theoretical foundation for machine learning with privacy considerations. The award has also supported two excellent postdocs. The postdoc who worked on the project during the first year has started a tenure track faculty at Rutgers University after a short stint at Apple Research working with the privacy team. A second postdoc who worked on this project over the past year has accepted a research scientist position at Google, Mountain View.\n\n\nCurriculum Development:A new interdisciplinary course titled \"Advanced Topics in Machine Learning\"developed by PI as part of the curriculum development activities has attracted students from computer science, electrical and computer engineering, applied math, and statistics. One of the goals of this course is to impart students with the necessary skills to thrive in today's job market. The utility of the course to a wide audience will be evaluated through periodic feedback from students and an open exhibition of the course projects. The PI also organized a year-long weekly lecture series focusing on the foundations of differential privacy with the purpose of training graduate students to conduct research at the forefront of learning with privacy considerations.\t\t\t\t\tLast Modified: 02/12/2024\n\n\t\t\t\t\tSubmitted by: RamanArora\n"
 }
}