{
 "awd_id": "1827361",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Temporal dynamics of phonetic perceptual organization",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 536227.0,
 "awd_amount": 545797.0,
 "awd_min_amd_letter_date": "2018-08-21",
 "awd_max_amd_letter_date": "2019-07-06",
 "awd_abstract_narration": "Scientists and engineers have studied speech to understand why this form of communication is so effective. They have also sought to create speaking and listening devices that approach the accuracy and ease of everyday communication, with modest success. The research problem has been easy to define: English is composed of more than 100,000 words created from over 16,000 different syllables and syllables are composed from a small inventory of several dozen consonants and vowels. Automatic speech recognition would be remarkably easy if these linguistic properties - words, syllables, consonants, vowels -produced uniformity in the sounds that talkers actually make. In fact, each utterance is also physically unique, whether in its sound pattern or in the visible movements of the speaker's face.  Different vocal anatomy in men, women and children causes complex variation in sound production even when the linguistic message is the same. Moreover, aspects of a talker's productions may express the dialect and speaking style of their family and linguistic community. Human listeners readily attend to the acoustic hints of these individual and social markers while also listening for the message. This project will examine how these different sources of perceptual information for speech are organized, how they are integrated over time, and how they allow perceptual tuning to the speech of individual talkers. Ultimately, a more complete account of the perception of speech can lead to improvement in recognition technology and to the creation of assistive devices.\r\n\r\n\r\nThree experimental projects will be performed: 1) to estimate the temporal dynamics of auditory sensory integration; 2) to determine the dimensions of exposure-based perceptual tuning to the characteristics of individual talkers; and, 3) to describe and model the intrinsic differences in auditory and visual temporal sensitivity and persistence that affect audiovisual speech perception. In each instance, the perceptual sensitivity to linguistic properties, talker characteristics, and language general features of spoken language will be assayed using discriminating and robust measures of auditory and audiovisual resolution. The studies explore the versatility of perceptual faculties applied to speech and provide an opportunity to identify the principles underlying the remarkably robust perceptual abilities that support and sustain communication. The overall goal is a formal and functional characterization of the cognitive resources that insure the perceptual stability of spoken communication in natural environments, whether the source of speech is visible or not, whether the talker is familiar or not, and whether the quality of the sensory samples of speech is natural or not.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Remez",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Robert E Remez",
   "pi_email_addr": "rremez@barnard.edu",
   "nsf_id": "000177674",
   "pi_start_date": "2018-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Barnard College",
  "inst_street_address": "3009 BROADWAY",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128542708",
  "inst_zip_code": "100276909",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "BARNARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LPQ1NHRK78M9"
 },
 "perf_inst": {
  "perf_inst_name": "Barnard College",
  "perf_str_addr": "3009 Broadway",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276598",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 536227.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 9570.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aimed to advance the scientific understanding of perception, focusing on speech. In part, the goal of studying the way people understand spoken messages is motivated from the importance of spoken communication in human affairs. Speech is everywhere. And, although spoken communication is typically robust even under adverse conditions, there are many circumstances in everyday experience that obscure the acoustic features of speech, and make it difficult to notice the details that a clear message depends on. This project specifically examined the temporal characteristics of speech, considering the brief and rapidly changing properties both visible and audible that the message depends on; and, on the temporal characteristics of the fading of sensory impressions of speech. We use speech synthesis to present sounds to listeners that let us identify the persistence of the sounds within the auditory nervous system, after the physical sounds have faded. Can a listener take advantage of this form of memory to recover the acoustic properties of a spoken message?&nbsp;</p>\n<p>Our aim in these studies is to see if there is a way for a listener to squeeze a bit more of the spoken message from a sensory memory of the speech that just occurred. Yet, this is a tough challenge because the auditory sensory memory itself lasts only about one-tenth of a second. One of the finding of this project is that attention plays a critical role in this recovery, and a listener must focus on the audible properties of speech very quickly to get any benefit from the rapidly fading sounds in the mind&rsquo;s ear. Prediction seems not to help much in this unless the message is a familiar one. It is as if a perceiver is helped most by knowing when to listen, and this is a solvable kind of prediction, far more amenable to attention than the prospect of guessing the linguistic properties &ndash; the exact words and syllables of the message. And, although the auditory nervous system rapidly analyzes the spatial properties of sounds that strike the ears &ndash; and does this automatically, requiring neither attention nor thought &ndash; the sound of a single voice is impossible to pull out of a busy acoustic environment without deliberate attention. At least, our measures suggest that without attention, no message gets in.&nbsp;</p>\n<p>One finding of this project is that the persistence of the visible aspects of speech can help recover acoustic properties. The reason for this, according to our measures in two completely different procedures, is that visual persistence is greater than auditory persistence. This means that temporal distortion due to echoes, or to delay in transmission whether electronic or physical, might deliver auditory properties well after the visible properties have been sampled by the eye. Yet, because the persistence of visual sensory storage is almost three times as long as auditory storage in the nervous system, the auditory sensory properties arrive well before the visual sense becomes inaccessible due to fading. There is little possibility of the complementary effect, in which a late arriving visible impression is saved by an unfaded auditory trace beyond a tenth of a second. Knowing this permits us to use it to our advantage, potentially.</p>\n<p>For instance, these measures can help design enclosures that are more conducive to spoken communication, whether a stadium concert of or an office cubicle. And, the findings might be useful to guide broadcast standards for compressing televised content &ndash; and, altering the temporal characteristics &ndash; while preserving the intelligibility of the programming in order to allow more commercial messages per unit time. It is also possible that refining the scientific understanding fo the interplay of attention and sensitivity will lead to theoretical developments the spur advances in our general understanding of perception, action, and cognition, which is always the hope of a scientific investigation of this kind.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/26/2023<br>\nModified by: Robert&nbsp;E&nbsp;Remez</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project aimed to advance the scientific understanding of perception, focusing on speech. In part, the goal of studying the way people understand spoken messages is motivated from the importance of spoken communication in human affairs. Speech is everywhere. And, although spoken communication is typically robust even under adverse conditions, there are many circumstances in everyday experience that obscure the acoustic features of speech, and make it difficult to notice the details that a clear message depends on. This project specifically examined the temporal characteristics of speech, considering the brief and rapidly changing properties both visible and audible that the message depends on; and, on the temporal characteristics of the fading of sensory impressions of speech. We use speech synthesis to present sounds to listeners that let us identify the persistence of the sounds within the auditory nervous system, after the physical sounds have faded. Can a listener take advantage of this form of memory to recover the acoustic properties of a spoken message?\n\n\nOur aim in these studies is to see if there is a way for a listener to squeeze a bit more of the spoken message from a sensory memory of the speech that just occurred. Yet, this is a tough challenge because the auditory sensory memory itself lasts only about one-tenth of a second. One of the finding of this project is that attention plays a critical role in this recovery, and a listener must focus on the audible properties of speech very quickly to get any benefit from the rapidly fading sounds in the minds ear. Prediction seems not to help much in this unless the message is a familiar one. It is as if a perceiver is helped most by knowing when to listen, and this is a solvable kind of prediction, far more amenable to attention than the prospect of guessing the linguistic properties  the exact words and syllables of the message. And, although the auditory nervous system rapidly analyzes the spatial properties of sounds that strike the ears  and does this automatically, requiring neither attention nor thought  the sound of a single voice is impossible to pull out of a busy acoustic environment without deliberate attention. At least, our measures suggest that without attention, no message gets in.\n\n\nOne finding of this project is that the persistence of the visible aspects of speech can help recover acoustic properties. The reason for this, according to our measures in two completely different procedures, is that visual persistence is greater than auditory persistence. This means that temporal distortion due to echoes, or to delay in transmission whether electronic or physical, might deliver auditory properties well after the visible properties have been sampled by the eye. Yet, because the persistence of visual sensory storage is almost three times as long as auditory storage in the nervous system, the auditory sensory properties arrive well before the visual sense becomes inaccessible due to fading. There is little possibility of the complementary effect, in which a late arriving visible impression is saved by an unfaded auditory trace beyond a tenth of a second. Knowing this permits us to use it to our advantage, potentially.\n\n\nFor instance, these measures can help design enclosures that are more conducive to spoken communication, whether a stadium concert of or an office cubicle. And, the findings might be useful to guide broadcast standards for compressing televised content  and, altering the temporal characteristics  while preserving the intelligibility of the programming in order to allow more commercial messages per unit time. It is also possible that refining the scientific understanding fo the interplay of attention and sensitivity will lead to theoretical developments the spur advances in our general understanding of perception, action, and cognition, which is always the hope of a scientific investigation of this kind.\n\n\n\t\t\t\t\tLast Modified: 12/26/2023\n\n\t\t\t\t\tSubmitted by: RobertERemez\n"
 }
}