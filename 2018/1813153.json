{
 "awd_id": "1813153",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "NSF-BSF: RI: Small: Collaborative Research: Modeling Crosslinguistic Influences Between Language Varieties",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 167464.0,
 "awd_amount": 167464.0,
 "awd_min_amd_letter_date": "2018-07-27",
 "awd_max_amd_letter_date": "2019-06-25",
 "awd_abstract_narration": "Most people in the world today are multilingual. Though multilingualism is a gradual phenomenon, previous research has primarily examined text from second language learners who have not yet achieved fluency. This project focuses on text produced by nonnative but highly fluent speakers. Fluent but nonnative language differs subtly from native, monolingual language in the frequencies of certain concepts, constructions, and collocations. This raises the possibility that language technologies -- typically trained on \"standard\" native language -- are systematically biased in ways that render them less useful for the majority of users.  This project will develop methods to examine large datasets of fluent nonnative language to detect the subtle influences of the native language and deliver natural language processing (NLP) tools for these language varieties. Its methods will be applicable beyond the populations in this study, including NLP-based measurement for social science and research seeking to better understand cognition in the bilingual mind. Native language identification will enable potential applications in language learning, cybersecurity, geolocation, personalization, and more. The project will openly share implementations and data, and will include educational activities that bring research into education.\r\n\r\nThis project will advance natural language processing techniques to shed light on the differences in language use by fluent speakers with varying linguistic backgrounds:  native speakers, highly fluent nonnative speakers, and translators when translating from another language into English.  It is known that classifiers can be trained to discriminate with high accuracy among these populations, even though humans have difficulty telling them apart. This project will focus on semantic phenomena, which can confound even fluent nonnative speakers. If current NLP models are biased toward native language, then they may not support accurate measurement in nonnative text; the project will develop new techniques to mitigate this bias. This project will deliver a range of new models for native language identification, new measurement models and multi-variety models for language-variety-aware NLP tools, new semantic annotations in several Englishes, and a study on nonnative annotation.  These novel methods for studying variation within a language and building such variation into our NLP systems will lead to unprecedented flexibility in computational models of natural language semantics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Noah",
   "pi_last_name": "Smith",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Noah A Smith",
   "pi_email_addr": "noah@allenai.org",
   "nsf_id": "000228357",
   "pi_start_date": "2018-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "185 Stevens Way",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 82144.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 85320.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span style=\"font-size: 11.000000pt; font-family: 'NimbusRomNo9L';\">Natural  language processing (NLP) methods are typically developed for &ldquo;standard&rdquo; language. Focusing on English, most  NLP resources are targeted for a very specific variant of  English (broadly speaking, standard newspaper American English).  However, it is obvious that there are many &ldquo;Englishes.&rdquo; Various dialects  of English are spoken in the U.S. alone, and more across the  English-speaking world (the UK, Ireland, Australia, New Zealand and  South Africa, but also India, Singapore, and elsewhere in Asia and  Africa). Furthermore, while English is the language of the Internet,  comprising over 50% of the content on the top 10 million websites,</span><span style=\"font-size: 8.000000pt; font-family: 'NimbusRomNo9L'; color: rgb(0.000000%, 0.000000%, 100.000000%); vertical-align: 4.000000pt;\"> </span><span style=\"font-size: 11.000000pt; font-family: 'NimbusRomNo9L';\">native  English speakers are far outnumbered by speakers of English as a  foreign language. Consequently, a vast amount of static and dynamic web  content (e.g., in online communities) is generated by nonnative writers.  This calls for a quantification of the degradation of existing NLP  tools on such content, and subsequent development of NLP resources that  are tuned for a wider spectrum of Englishes (assuming such degradation  is significant, as we expect). This project targeted adapting  models from &ldquo;standard&rdquo; English to the English of, for example, highly advanced  nonnative speakers, aiming to <span style=\"font-size: 11.000000pt; font-family: 'NimbusRomNo9L';\">support a much larger population of users.</span></span></p>\n<p><span style=\"font-size: 11.000000pt; font-family: 'NimbusRomNo9L';\"><span style=\"font-size: 11.000000pt; font-family: 'NimbusRomNo9L';\">Concretely, this project took important steps in that direction.&nbsp; First, a new, large six-language dataset that will encourage work on multilingual NLP (the Multilingual Amazon Reviews Corpus), especially transfer of methods from one language to another, was created and released to the research community.&nbsp; Second, new methods were introduced for adapting existing models to low-resource language varieties, particularly those unrepresented in conventional web-based corpora (e.g., Singaporean English).&nbsp; Third, a particular application area, the detection of toxic text, was considered experimentally for sensitivity toward dialectal bias.&nbsp; Existing methods for removing such biases were found insufficient, and a new dialect-aware data correction method was explored and evaluated.<br /></span></span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/13/2021<br>\n\t\t\t\t\tModified by: Noah&nbsp;A&nbsp;Smith</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nNatural  language processing (NLP) methods are typically developed for \"standard\" language. Focusing on English, most  NLP resources are targeted for a very specific variant of  English (broadly speaking, standard newspaper American English).  However, it is obvious that there are many \"Englishes.\" Various dialects  of English are spoken in the U.S. alone, and more across the  English-speaking world (the UK, Ireland, Australia, New Zealand and  South Africa, but also India, Singapore, and elsewhere in Asia and  Africa). Furthermore, while English is the language of the Internet,  comprising over 50% of the content on the top 10 million websites, native  English speakers are far outnumbered by speakers of English as a  foreign language. Consequently, a vast amount of static and dynamic web  content (e.g., in online communities) is generated by nonnative writers.  This calls for a quantification of the degradation of existing NLP  tools on such content, and subsequent development of NLP resources that  are tuned for a wider spectrum of Englishes (assuming such degradation  is significant, as we expect). This project targeted adapting  models from \"standard\" English to the English of, for example, highly advanced  nonnative speakers, aiming to support a much larger population of users.\n\nConcretely, this project took important steps in that direction.  First, a new, large six-language dataset that will encourage work on multilingual NLP (the Multilingual Amazon Reviews Corpus), especially transfer of methods from one language to another, was created and released to the research community.  Second, new methods were introduced for adapting existing models to low-resource language varieties, particularly those unrepresented in conventional web-based corpora (e.g., Singaporean English).  Third, a particular application area, the detection of toxic text, was considered experimentally for sensitivity toward dialectal bias.  Existing methods for removing such biases were found insufficient, and a new dialect-aware data correction method was explored and evaluated.\n\n\n\t\t\t\t\tLast Modified: 12/13/2021\n\n\t\t\t\t\tSubmitted by: Noah A Smith"
 }
}