{
 "awd_id": "1751365",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Physically-Motivated Learning of 3D Shape and Semantics",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-04-01",
 "awd_exp_date": "2023-03-31",
 "tot_intn_awd_amt": 548581.0,
 "awd_amount": 548581.0,
 "awd_min_amd_letter_date": "2018-03-16",
 "awd_max_amd_letter_date": "2022-04-11",
 "awd_abstract_narration": "A system that navigates or interacts with the real world must reason about 3D geometric properties such as distances or orientations of objects, as well as semantic properties such as part locations or object types. This project combines physics-based modeling of images and shapes, with the versatility of robust optimization and deep learning, to recover 3D shape and semantic information. The work establishes connections between computer vision, machine learning, computer graphics and perception. Vision is a powerful sensing modality since images encode rich information about shape and semantics. However, image formation is a physical phenomenon that often includes complex factors like shape deformations, occlusions, material properties and participating media. Consequently, practical deployment of autonomous or intelligent vision-based systems requires robustness to the effects of diverse physical factors. Such effects may be inverted by modeling the image formation process, but hand-crafted features and hard-coded rules face limitations for data inconsistent with the model. Recent advances in deep learning have led to impressive performances, but generalization of a purely data-driven approach to handle such complex effects is expensive. To address these challenges, this project develops technologies of handling the diversity of real-world images through incorporation of physical models of image formation within deep learning frameworks. The project creates a cross-disciplinary educational program in vision, graphics, learning and perception through coursework that draws connections across wide areas such as physically-based modeling, deep learning, 3D reconstruction and semantic understanding. The program also develops K-12 educative modules that provide experiential insight into novel technologies such as virtual reality or self-driving, with a focus on outreach to students from under-represented backgrounds.\r\n\r\nThis research lays the foundations for physically-motivated learning of 3D shape and semantics, with benefits such as higher accuracy, better generalization or greater ease of training. It develops theoretical frameworks that relate unknown material behavior to 3D shape, which allows robust optimization frameworks and convolutional neural network architectures for material-invariant shape estimation. It designs novel network structures that model complex transformations, to generalize recovery of shape or semantics across non-rigid and articulated deformations, or distortions due to refraction and participating media. Further, it uses physical models of appearance or motion to bridge the domain gap between simulations and real images, leading to weakly supervised frameworks that mitigate the expense of data annotation. These advances enable novel applications for light field imaging, augmented reality, self-driving in challenging weather, or underwater robotic exploration.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Manmohan",
   "pi_last_name": "Chandraker",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Manmohan K Chandraker",
   "pi_email_addr": "mkchandraker@eng.ucsd.edu",
   "nsf_id": "000727279",
   "pi_start_date": "2018-03-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Drive MC0934",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 126886.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 113228.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 116796.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 100228.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 91443.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project has developed novel deep learning frameworks that recover 3D shape, along with material and lighting, through physically-motivated insights that allow handling the complexities in image formation, such as unknown spatially-varying material, shadows, inter-reflections and global light transport. We establish that such a modeling allows higher accuracy, better generalization and greater ease of training than black-box approaches. Through a series of six papers published between 2018 to 2022 (all of them highly selective oral presentations at venues like CVPR or ECCV, with acceptance rates between 3-5%), the PI and team solve several longstanding challenges of: (a) recovering spatially-varying material properties, (b) joint shape and material recovery, (c) joint shape, material and spatially-varying lighting estimation in complex indoor scenes, (d) recovery of light sources in indoor scenes. Together, these results have allowed applications in single-image mobile augmented reality such as virtual object insertion, material replacement and editing of visible and invisible light sources to achieve unprecedented levels of photorealism.<br /><br />A new framework called OpenRooms was created with (a) ground truth data for shape, material and complex light transport including shadows and interreflections, (b) tools for reconstruction, annotation and rendering needed for users to create their own data using commodity scans as input, (c) trained models that solve problems of shape, material and lighting estimation, (d) code for augmented reality applications such as object insertion and material editing. Methods trained on OpenRooms achieve state-of-the-art performances. Code and data is publicly released as open source. This is a significant step in democratization of research in augmented reality and related areas.<br /><br />The grant was instrumental in shaping the PhD thesis research of multiple students advised by the PI, one of which was recognized with the CSE Dissertation Award at UCSD and a nomination for the ACM Dissertation Award. An ECCV 2022 paper also won the Best Paper Honorable Mention. Five of the PhD students supervised by the PI won prestigious industry fellowships from Google and Qualcomm.<br /><br />The grant supported the development of new coursework at UCSD through its research outcomes, consisting of graduate CSE 252D and undergraduate CSE 152B, which focus on recent advances in computer vision. Both classes are popular, with high student approvals for the instructor and the course, which also resulted in the PI receiving the 2022 Teacher of the Year Award at UCSD.<br /><br />The grant enabled participation in the ENLACE program, where students from the US and Mexico team up on a research project to foster collaboration across geographic boundaries. The grant also shaped a project under the Early Research Scholars Program, which provides research mentoring to students from backgrounds under-represented in computer science.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/23/2023<br>\n\t\t\t\t\tModified by: Manmohan&nbsp;K&nbsp;Chandraker</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698087353378_04_shapematerial--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698087353378_04_shapematerial--rgov-800width.jpg\" title=\"Physically-Motivated Estimation of Shape and Material\"><img src=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698087353378_04_shapematerial--rgov-66x44.jpg\" alt=\"Physically-Motivated Estimation of Shape and Material\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We develop a physically-motivated cascaded CNN architecture for the extremely ill-posed problem of recovering shape and spatially-varying BRDF from a single mobile phone image, which allows visualization under a novel illumination condition.</div>\n<div class=\"imageCredit\">Z. Li, Z. Xu, R. Ramamoorthi, K. Sunkavalli and M.K. Chandraker, Learning to Reconstruct Shape and Spatially-Varying Reflectance with a Single Image, SIGGRAPH Asia 2018.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Manmohan&nbsp;K&nbsp;Chandraker</div>\n<div class=\"imageTitle\">Physically-Motivated Estimation of Shape and Material</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698086790408_02_invindoor--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698086790408_02_invindoor--rgov-800width.jpg\" title=\"Shape, Material and Lighting Estimation in Indoor Scenes\"><img src=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698086790408_02_invindoor--rgov-66x44.jpg\" alt=\"Shape, Material and Lighting Estimation in Indoor Scenes\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Given a single image of an indoor scene, we recover diffuse albedo, normals, specularity, depth and spatially-varying lighting. Incorporating physical insights into our network allows high-quality applications like object insertion, even for specular objects in a single real image.</div>\n<div class=\"imageCredit\">Z. Li, M. Shafiei, R. Ramamoorthi, K. Sunkavalli, M.K. Chandraker, Inverse Rendering for Complex Indoor Scenes: Shape, Spatially-Varying Lighting and SVBRDF from a Single Image, CVPR 2020.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Manmohan&nbsp;K&nbsp;Chandraker</div>\n<div class=\"imageTitle\">Shape, Material and Lighting Estimation in Indoor Scenes</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698087749881_05_material--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698087749881_05_material--rgov-800width.jpg\" title=\"Physically-Motivated Estimation of Material Properties\"><img src=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698087749881_05_material--rgov-66x44.jpg\" alt=\"Physically-Motivated Estimation of Material Properties\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We develop a physically-motivated network to estimate spatially-varying BRDF from a single mobile phone image. The input images in the first three rows are acquired using a hand-held commodity mobile phone, with a Google Tango in the fourth row and an iPhone 6s for the fifth row.</div>\n<div class=\"imageCredit\">Z. Li, K. Sunkavalli and M.K. Chandraker, Materials for Masses: SVBRDF Acquisition with a Single Mobile Phone Image, ECCV 2018.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Manmohan&nbsp;K&nbsp;Chandraker</div>\n<div class=\"imageTitle\">Physically-Motivated Estimation of Material Properties</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698088064580_07_augmentedreality--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698088064580_07_augmentedreality--rgov-800width.jpg\" title=\"Photorealistic Augmented Reality Applications\"><img src=\"/por/images/Reports/POR/2023/1751365/1751365_10534948_1698088064580_07_augmentedreality--rgov-66x44.jpg\" alt=\"Photorealistic Augmented Reality Applications\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our physically-motivated networks enable high-quality augmented reality applications like object insertion and material editing. Note the consistency of the shading, shadowing and reflections in the outputs, despite the intertwined influence of material, geometry and lighting in a single image.</div>\n<div class=\"imageCredit\">Z. Li, M. Shafiei, R. Ramamoorthi, K. Sunkavalli, M.K. Chandraker, Inverse Rendering for Complex Indoor Scenes: Shape, Spatially-Varying Lighting and SVBRDF from a Single Image, CVPR 2020.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Manmohan&nbsp;K&nbsp;Chandraker</div>\n<div class=\"imageTitle\">Photorealistic Augmented Reality Applications</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe project has developed novel deep learning frameworks that recover 3D shape, along with material and lighting, through physically-motivated insights that allow handling the complexities in image formation, such as unknown spatially-varying material, shadows, inter-reflections and global light transport. We establish that such a modeling allows higher accuracy, better generalization and greater ease of training than black-box approaches. Through a series of six papers published between 2018 to 2022 (all of them highly selective oral presentations at venues like CVPR or ECCV, with acceptance rates between 3-5%), the PI and team solve several longstanding challenges of: (a) recovering spatially-varying material properties, (b) joint shape and material recovery, (c) joint shape, material and spatially-varying lighting estimation in complex indoor scenes, (d) recovery of light sources in indoor scenes. Together, these results have allowed applications in single-image mobile augmented reality such as virtual object insertion, material replacement and editing of visible and invisible light sources to achieve unprecedented levels of photorealism.\n\nA new framework called OpenRooms was created with (a) ground truth data for shape, material and complex light transport including shadows and interreflections, (b) tools for reconstruction, annotation and rendering needed for users to create their own data using commodity scans as input, (c) trained models that solve problems of shape, material and lighting estimation, (d) code for augmented reality applications such as object insertion and material editing. Methods trained on OpenRooms achieve state-of-the-art performances. Code and data is publicly released as open source. This is a significant step in democratization of research in augmented reality and related areas.\n\nThe grant was instrumental in shaping the PhD thesis research of multiple students advised by the PI, one of which was recognized with the CSE Dissertation Award at UCSD and a nomination for the ACM Dissertation Award. An ECCV 2022 paper also won the Best Paper Honorable Mention. Five of the PhD students supervised by the PI won prestigious industry fellowships from Google and Qualcomm.\n\nThe grant supported the development of new coursework at UCSD through its research outcomes, consisting of graduate CSE 252D and undergraduate CSE 152B, which focus on recent advances in computer vision. Both classes are popular, with high student approvals for the instructor and the course, which also resulted in the PI receiving the 2022 Teacher of the Year Award at UCSD.\n\nThe grant enabled participation in the ENLACE program, where students from the US and Mexico team up on a research project to foster collaboration across geographic boundaries. The grant also shaped a project under the Early Research Scholars Program, which provides research mentoring to students from backgrounds under-represented in computer science.\n\n\t\t\t\t\tLast Modified: 10/23/2023\n\n\t\t\t\t\tSubmitted by: Manmohan K Chandraker"
 }
}