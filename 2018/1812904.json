{
 "awd_id": "1812904",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Design of Gradient-Based Methods for Solving General and Huge Convex Optimization Problems",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Pedro Embid",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 316798.0,
 "awd_amount": 316798.0,
 "awd_min_amd_letter_date": "2018-08-14",
 "awd_max_amd_letter_date": "2018-08-14",
 "awd_abstract_narration": "Optimization modeling, and algorithms for solving the models, have been key to gains in efficiency in the US economy since the late 1940's.  The relevance of optimization has increased manyfold alongside advances in computer design, and recently alongside the availability of vast and complex datasets arising from internet applications.  Optimization has become a central part of machine learning.  However, even the most efficient optimization algorithms are unable to succeed for complicated models populated by huge datasets possessing little special structure, the main problem being the limited core memory available on (even large) computers.  While Moore's Law accurately predicted exponentially-increasing computing power, the surprise has been that the sizes of datasets are increasing much faster.  A central focus of the project is the design of algorithms capable of solving a complicated optimization model populated with a huge dataset, by breaking apart the model into a sequence of computational problems, each relying on only a portion of the data, not too much for core memory.  Graduate students participate in the research.\r\n\r\nThe general approach makes use of the most elemental of algorithms for convex optimization, namely, subgradient methods, dating to the 1960's.  In tandem, the approach makes use of -- and advances -- a framework promoted by the investigator in recent years, whereby a general convex optimization problem is transformed into an equivalent convex optimization problem whose only constraints are linear equations and for which the objective function is Lipschitz continuous (thereby allowing direct application of subgradient methods).  Exploration is being done initially for linear programming, an ambitious goal being to solve problems even beyond the reach of the simplex method (in cases where the basis-inverse matrix is larger than core memory permits).  Focus also is being given to problems involving an objective function that is itself the sum of many functions, a common setting in machine learning.  Here the goal is to devise algorithms that are able to choose the summand functions in a principled (and efficient) manner, unlike incremental (sub)gradient methods, which choose a summand uniformly at random.  Additionally, attempts are being made to extend the investigator's framework so as to provide, for example, a way to transform a continuously-differentiable objective function, possibly with bounded domain, into an entire function possessing Lipschitz-continuous gradient, thereby allowing accelerated methods to be applied easily.  A particularly important aspect of the project is the design of practical schemes for speeding up first order methods when the optimization problem being solved has some particular kind of geometrical structure (such as \"sharpness,\" where the objective function grows linearly with the distance to optimality).  The goal is to design schemes that require no knowledge of parameters governing the geometrical structure, and yet that are guaranteed to achieve optimal speedup whenever the structure is present (regardless of whether the user knows the structure is present).  Graduate students participate in the research.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Renegar",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "James M Renegar",
   "pi_email_addr": "renegar@orie.cornell.edu",
   "nsf_id": "000362038",
   "pi_start_date": "2018-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "136 Hoy Rd",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148533801",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126600",
   "pgm_ele_name": "APPLIED MATHEMATICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 316798.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Optimization is concerned with developing mathematical models which capture the essence of complicated real-world problems, the models being simple enough that answers can be gleaned from the models by automated computational procedures known as algorithms. For each kind of algorithm, there is a range of models to which it can be applied, although its speed in \"solving\" the model typically depends on specific characteristics of the individual model. If relevant characteristics for the model are known and quantified, algorithms can often be fine-tuned for the model, leading to a speed-up in the solving the model. Especially for large models, unfortunately, the relevant characteristics are usually unknown. A key outcome of the project is the development of a procedure by which multiple copies of an algorithm are run in parallel, each with an assigned task, which when completed leads the algorithm to tell the others what it has found in case the discovery might be of use for them in completing their tasks. Once an algorithm completes its task, it is restarted with a new task. The project established that for a highly important class of algorithms (first-order methods), the procedure speeds-up the time required for solving models without a need to fine-tune the algorithms to the specific characteristics of the model being solved.</p>\n<p>Another important outcome of the project is the development of new algorithms for a classical problem in optimization, namely, computing a point in the intersection of finitely many \"convex\" sets, assuming for each of the sets being intersected, a point in it is known. The new algorithms are provably faster than previous algorithms similarly designed to be applied to intersections of convex sets quite generally. Moreover, the new algorithms depend only on elementary computations, making them typically be straightforward to implement in practice.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/28/2022<br>\n\t\t\t\t\tModified by: James&nbsp;M&nbsp;Renegar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOptimization is concerned with developing mathematical models which capture the essence of complicated real-world problems, the models being simple enough that answers can be gleaned from the models by automated computational procedures known as algorithms. For each kind of algorithm, there is a range of models to which it can be applied, although its speed in \"solving\" the model typically depends on specific characteristics of the individual model. If relevant characteristics for the model are known and quantified, algorithms can often be fine-tuned for the model, leading to a speed-up in the solving the model. Especially for large models, unfortunately, the relevant characteristics are usually unknown. A key outcome of the project is the development of a procedure by which multiple copies of an algorithm are run in parallel, each with an assigned task, which when completed leads the algorithm to tell the others what it has found in case the discovery might be of use for them in completing their tasks. Once an algorithm completes its task, it is restarted with a new task. The project established that for a highly important class of algorithms (first-order methods), the procedure speeds-up the time required for solving models without a need to fine-tune the algorithms to the specific characteristics of the model being solved.\n\nAnother important outcome of the project is the development of new algorithms for a classical problem in optimization, namely, computing a point in the intersection of finitely many \"convex\" sets, assuming for each of the sets being intersected, a point in it is known. The new algorithms are provably faster than previous algorithms similarly designed to be applied to intersections of convex sets quite generally. Moreover, the new algorithms depend only on elementary computations, making them typically be straightforward to implement in practice.\n\n\t\t\t\t\tLast Modified: 11/28/2022\n\n\t\t\t\t\tSubmitted by: James M Renegar"
 }
}