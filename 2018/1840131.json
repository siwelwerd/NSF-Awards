{
 "awd_id": "1840131",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FW-HTF: Collaborative Research: Enhancing Human Capabilities through Virtual Personal Embodied Assistants in Self-Contained Eyeglasses-Based Augmented Reality (AR) Systems",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032925365",
 "po_email": "jberg@nsf.gov",
 "po_sign_block_name": "Jordan Berg",
 "awd_eff_date": "2018-09-15",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 2190000.0,
 "awd_amount": 2190000.0,
 "awd_min_amd_letter_date": "2018-09-13",
 "awd_max_amd_letter_date": "2021-03-17",
 "awd_abstract_narration": "The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. \r\n\r\nThis award supports basic research underpinning development of an eyeglass-based 3D mobile telepresence system with integrated virtual personal assistant. This technology will increase worker productivity and improve skills. The system automatically adjusts visual focus and places virtual elements in the image without eye strain.  The user will be able to communicate to the system by speech.  The system also uses sensors to keep track of the user's surroundings and provide the relevant information to the user automatically.  The project will explore two of the many possible uses of the system: amplifying a workers capabilities (such as a physical therapist interacting with a remote patient), and accelerating post-injury return to work through telepresence (such as a burn victim reintegrating into his/her workplace). The project will advance the national interest by allowing the right person to be virtually in the right place at the right time. The project also includes an education and outreach component wherein undergraduate and graduate students shall receive training in engineering and research methods. Course curriculum at Stanford University and the University of North Carolina at Chapel Hill shall be updated to include project-related content and examples. \r\n\r\nThis project comprises fundamental research activities needed to develop an embodied Intelligent Cognitive Assistant (GLASS-X) that will amplify the capabilities of workers in a way that will increase productivity and improve quality of life. GLASS-X is conceived of as an eyeglass-based 3D mobile telepresence system with integrated virtual personal assistant. Methods include: body and environment reconstruction (situation awareness) from a fusion of images provided by an eyeglass frame-based camera array and limb motion data provided by inertial measurement units; fundamental research on adaptive focus displays capable to reduce eye strain when using augmented reality displays; dialog-based communication with a virtual personal assistant, including transformations from visual input to dialog and vice versa; human subject evaluations of GLASS-X technology in two workplace domains (remote interactions between a physical therapist and his/her patient; burn survivor remote return-to-work). This research promises to push the state of the art in core areas including: computer vision; augmented reality; accommodating displays; and natural language and dialogue models.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Henry",
   "pi_last_name": "Fuchs",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Henry Fuchs",
   "pi_email_addr": "fuchs@cs.unc.edu",
   "nsf_id": "000451367",
   "pi_start_date": "2018-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jan-Michael",
   "pi_last_name": "Frahm",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jan-Michael Frahm",
   "pi_email_addr": "jmf@cs.unc.edu",
   "nsf_id": "000427356",
   "pi_start_date": "2018-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Lewek",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Lewek",
   "pi_email_addr": "mlewek@med.unc.edu",
   "nsf_id": "000688588",
   "pi_start_date": "2021-03-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mohit",
   "pi_last_name": "Bansal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mohit Bansal",
   "pi_email_addr": "mbansal@cs.unc.edu",
   "nsf_id": "000689943",
   "pi_start_date": "2018-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Prudence",
   "pi_last_name": "Plummer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Prudence Plummer",
   "pi_email_addr": "pplummer@med.unc.edu",
   "nsf_id": "000780980",
   "pi_start_date": "2018-09-13",
   "pi_end_date": "2021-03-17"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Felicia",
   "pi_last_name": "Williams",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Felicia N Williams",
   "pi_email_addr": "fnwmd@med.unc.edu",
   "nsf_id": "000780967",
   "pi_start_date": "2018-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Chapel Hill",
  "perf_str_addr": "201 S. Columbia St.",
  "perf_city_name": "Chapel Hill",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275993175",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "082y00",
   "pgm_ele_name": "FW-HTF-Adv Cogn & Phys Capblty"
  },
  {
   "pgm_ele_code": "082Y00",
   "pgm_ele_name": "FW-HTF-Adv Cogn & Phys Capblty"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 2190000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project sought to extend the capabilities needed in a future system that would be integrated into a person's ordinary eyeglasses. Such a \"smart eyeglasses\" system with a see-through display, and miniature cameras and other sensors, would provide most (perhaps all) the capabilities of today's smart phones, plus many additional capabilties enabled by its awareness of the user's body and the user's physical surroundings.&nbsp; To advance the prospect of such a promising future system, this project developed new capabilities in multiple necessary techologies: 1) automatic techiques to detect and estimate the user's body pose and appearance using only tiny cameras mounted in eyeglass frames and tiny motion sesors worn on the wrists and legs, 2) improved techniques for computer-generated holograms (the most promising technology for imagery in such future eyeglasses), 3) improved natural language techniques for the system to interact with the user via speech while simultaneously being aware of the user's surroundings from the camera imagery, 4) improved 2D and 3D teleconferencing systems that would be feasible and immersive with such \"smart eyeglasses.\"</p>\n<p>This project also investigated two applications of such future \"smart eyeglasses\": 1) remote Physical Therapy and 2) early return to work via telepresence for survivors of burns for whom risk of infection often necessitates prolong isolation from the wider world.&nbsp;</p>\n<p>For the Physical Therapy application, this project developed and studied, with collaborators from Stanford University, the trade-offs between several variations of promising 2D and 3D teleconferencing systems, using multiple 2D screens with multiple cameras in some subject groups, and VR headsets and color-plus-depth cameras for other subject groups.&nbsp;</p>\n<p>For the possibilitiy of early return to work for burn survivors using such future \"smart eyeglasses,\" we studies the occupations of patients in the NC Jaycee Burn Center at UNC Chapel Hill. We selected a representative period (March10 to May 22, 2020) and reviewed the occupations of all patients of working age (18-64) admitted during that period -- 105 patients.&nbsp; From our review, we estimated that approximately 25% of the patients may (in the future) benefit from the 3D presence technology being developed.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/12/2023<br>\n\t\t\t\t\tModified by: Henry&nbsp;Fuchs</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project sought to extend the capabilities needed in a future system that would be integrated into a person's ordinary eyeglasses. Such a \"smart eyeglasses\" system with a see-through display, and miniature cameras and other sensors, would provide most (perhaps all) the capabilities of today's smart phones, plus many additional capabilties enabled by its awareness of the user's body and the user's physical surroundings.  To advance the prospect of such a promising future system, this project developed new capabilities in multiple necessary techologies: 1) automatic techiques to detect and estimate the user's body pose and appearance using only tiny cameras mounted in eyeglass frames and tiny motion sesors worn on the wrists and legs, 2) improved techniques for computer-generated holograms (the most promising technology for imagery in such future eyeglasses), 3) improved natural language techniques for the system to interact with the user via speech while simultaneously being aware of the user's surroundings from the camera imagery, 4) improved 2D and 3D teleconferencing systems that would be feasible and immersive with such \"smart eyeglasses.\"\n\nThis project also investigated two applications of such future \"smart eyeglasses\": 1) remote Physical Therapy and 2) early return to work via telepresence for survivors of burns for whom risk of infection often necessitates prolong isolation from the wider world. \n\nFor the Physical Therapy application, this project developed and studied, with collaborators from Stanford University, the trade-offs between several variations of promising 2D and 3D teleconferencing systems, using multiple 2D screens with multiple cameras in some subject groups, and VR headsets and color-plus-depth cameras for other subject groups. \n\nFor the possibilitiy of early return to work for burn survivors using such future \"smart eyeglasses,\" we studies the occupations of patients in the NC Jaycee Burn Center at UNC Chapel Hill. We selected a representative period (March10 to May 22, 2020) and reviewed the occupations of all patients of working age (18-64) admitted during that period -- 105 patients.  From our review, we estimated that approximately 25% of the patients may (in the future) benefit from the 3D presence technology being developed.\n\n\t\t\t\t\tLast Modified: 01/12/2023\n\n\t\t\t\t\tSubmitted by: Henry Fuchs"
 }
}