{
 "awd_id": "1763469",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Manipulation Assistance for Activities of Daily Living in Everyday Environments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 475000.0,
 "awd_amount": 539000.0,
 "awd_min_amd_letter_date": "2018-07-30",
 "awd_max_amd_letter_date": "2022-06-29",
 "awd_abstract_narration": "While many people with disabilities need help with activities of daily living (ADLs) in their homes or at other locations, they care deeply about maintaining their sense of independence, which implies limiting the tasks that professional or family caregivers are asked to provide. There is the potential for robots to have a huge impact here, by enabling people to live independently for longer. The goal of this research is to develop a robotic wheelchair-manipulator system (RoWMan) consisting of a power wheelchair with a robotic arm mounted on it, that will help its user perform ADLs either as an assistive device or by performing manipulation tasks autonomously. In assistive mode, the user would ride in the wheelchair, with the RoWMan system manipulating items as requested. Whereas in autonomous mode, the user could ask RoWMan to navigate on its own through the house, retrieve items, and place them as directed. This project will necessitate the development of new user interfaces as well as an array of new machine learning and robotics techniques that will enable successful autonomous robotic navigation and manipulation in unstructured environments. To ensure broad impact, project outcomes will be evaluated with a user population at Crotched Mountain Rehabilitation Center.\r\n\r\nIn recent focus groups it was found that users want a number of capabilities, including the ability to pick up something from the floor, the ability to unlock and open a door, the ability to manipulate items on a tightly packed shelf, etc. RoWMan will be designed so as to enable users to perform these sorts of tasks, by focusing on two areas: robotic manipulation and human-robot interaction. The manipulation work will develop new algorithms that perform well with novel objects in unstructured environments. Traditionally, manipulation planners assume that the shapes of the objects involved are known in advance or can be estimated on the fly, but these assumptions often cause problems in practice. The focus here will be to develop new algorithms based on deep reinforcement learning that can perform manipulation tasks reliably even when the geometry of the world is unknown in advance. The project will also support research into a new class of human-robot interaction based on laser pointers. Recent work suggests that laser pointing can be very effective for the target user community because it enables users to point directly in the environment rather than on a screen which induces additional cognitive load. This project will develop new ways of communicating sophisticated intent using a combination of environmental context, laser pointing, and laser gestures.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Holly",
   "pi_last_name": "Yanco",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Holly A Yanco",
   "pi_email_addr": "holly@cs.uml.edu",
   "nsf_id": "000278965",
   "pi_start_date": "2018-07-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Lowell",
  "inst_street_address": "220 PAWTUCKET ST STE 400",
  "inst_street_address_2": "",
  "inst_city_name": "LOWELL",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "9789344170",
  "inst_zip_code": "018543573",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MA03",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS LOWELL",
  "org_prnt_uei_num": "",
  "org_uei_num": "LTNVSTJ3R6D5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Lowell",
  "perf_str_addr": "One University Avenue",
  "perf_city_name": "Lowell",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "018542827",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "MA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 273269.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 115749.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 117982.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>At the entrance of a grocery store, there is often a mobility scooter available for people who have trouble walking. However, that scooter does not help people to reach items on the shelves of the store. For this research project, we added a robot arm and gripper to a scooter. Our collaborators at Northeastern University developed the ability for a robot to grasp an object selected by the person riding the scooter. Our focus was discovering the best way for a person to select desired objects in the real world.</p>\r\n<p>We developed two interfaces with different selection methods. The graphical user interface (GUI) shows a picture of the world on a 10-inch touchscreen. The tangible user interface (TUI) has a joystick, a box of buttons, and a projector, which are designed to keep the user's attention in the real world unlike the GUI. The projection system on the TUI is used to simulate a laser pointer, moved around by the person using the joystick, and for highlighting the selected object before starting the grasp, to allow the user to correct the system if an error was made.</p>\r\n<p>We conducted a within-subjects user study with 27 participants over the age of 60, comparing the use of the two different user interfaces for our assistive robot scooter. We found that people could use the GUI without much learning but that it did not perform as well when people had to select an object surrounded by clutter. Instead, while the TUI required additional learning, performance was nearly identical in both low and high clutter. When designing an assistive robot system for picking up objects in highly cluttered environments, turning the world into the interface can make it easier for people to quickly select the thing that they want in three dimensions, rather than a flat image of the world.</p>\r\n<p>The projection system designed for the tangible user interface has had broader impacts in other areas of robotics. For example, it has been used to allow a mobile robot to show its planned path, to provide additional safety to people in the environment. It has also been used as part of a failure explanation system, to allow the robot to show a person where it is having problems, whether because an object has been moved out of reach or a part is missing. As robots become more common in the world, they need to have the ability to work safely around people and to explain what they are doing. Our projection system enables robots to have this capability.</p><br>\n<p>\n Last Modified: 02/10/2025<br>\nModified by: Holly&nbsp;A&nbsp;Yanco</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/1763469/1763469_10563424_1739202502657_NERVE_scooter_smaller--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1763469/1763469_10563424_1739202502657_NERVE_scooter_smaller--rgov-800width.png\" title=\"Assistive scooter system with two of the student researchers\"><img src=\"/por/images/Reports/POR/2025/1763469/1763469_10563424_1739202502657_NERVE_scooter_smaller--rgov-66x44.png\" alt=\"Assistive scooter system with two of the student researchers\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Two student researchers demonstrating the assistive robot scooter developed in collaboration with Northeastern University. The projector on the top of the scooter turns the world into the interface, allowing for direct selection of items to grasp in the real world.</div>\n<div class=\"imageCredit\">UMass Lowell NERVE Center</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Holly&nbsp;A&nbsp;Yanco\n<div class=\"imageTitle\">Assistive scooter system with two of the student researchers</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nAt the entrance of a grocery store, there is often a mobility scooter available for people who have trouble walking. However, that scooter does not help people to reach items on the shelves of the store. For this research project, we added a robot arm and gripper to a scooter. Our collaborators at Northeastern University developed the ability for a robot to grasp an object selected by the person riding the scooter. Our focus was discovering the best way for a person to select desired objects in the real world.\r\n\n\nWe developed two interfaces with different selection methods. The graphical user interface (GUI) shows a picture of the world on a 10-inch touchscreen. The tangible user interface (TUI) has a joystick, a box of buttons, and a projector, which are designed to keep the user's attention in the real world unlike the GUI. The projection system on the TUI is used to simulate a laser pointer, moved around by the person using the joystick, and for highlighting the selected object before starting the grasp, to allow the user to correct the system if an error was made.\r\n\n\nWe conducted a within-subjects user study with 27 participants over the age of 60, comparing the use of the two different user interfaces for our assistive robot scooter. We found that people could use the GUI without much learning but that it did not perform as well when people had to select an object surrounded by clutter. Instead, while the TUI required additional learning, performance was nearly identical in both low and high clutter. When designing an assistive robot system for picking up objects in highly cluttered environments, turning the world into the interface can make it easier for people to quickly select the thing that they want in three dimensions, rather than a flat image of the world.\r\n\n\nThe projection system designed for the tangible user interface has had broader impacts in other areas of robotics. For example, it has been used to allow a mobile robot to show its planned path, to provide additional safety to people in the environment. It has also been used as part of a failure explanation system, to allow the robot to show a person where it is having problems, whether because an object has been moved out of reach or a part is missing. As robots become more common in the world, they need to have the ability to work safely around people and to explain what they are doing. Our projection system enables robots to have this capability.\t\t\t\t\tLast Modified: 02/10/2025\n\n\t\t\t\t\tSubmitted by: HollyAYanco\n"
 }
}