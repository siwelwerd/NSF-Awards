{
 "awd_id": "1839379",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Signing Avatars & Immersive Learning (SAIL): Development and Testing of a Novel Embodied Learning Environment",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2018-08-16",
 "awd_max_amd_letter_date": "2018-08-16",
 "awd_abstract_narration": "Improved resources for learning American Sign Language (ASL) are in high demand. Traditional educational materials for ASL tend to include books and videos, but there has been limited progress in using cutting-edge technologies to harness the visual-spatial nature of ASL for improved learning outcomes. Interactive speaking avatars have become valuable learning tools for spoken language instruction, whereas the potential uses of signing avatars have not been adequately explored. The aim of this EArly Grant for Exploratory Research is to investigate the feasibility of a system in which signing avatars (computer-animated virtual humans built from motion capture recordings) teach users ASL in an immersive virtual environment. The system is called Signing Avatars & Immersive Learning (SAIL). The project focuses on developing and testing this entirely novel ASL learning tool, fostering the inclusion of underrepresented minorities in STEM. This work has the potential to substantially advance the fields of virtual reality, ASL instruction, and embodied learning.\r\n\r\nThis project leverages the cognitive neuroscience of embodied learning to test the SAIL system. The ultimate goal is to develop a prototype of the system and test its use in a sample of hearing non-signers. Signing avatars are created using motion capture recordings of native deaf signers signing in ASL. The avatars are placed in a virtual reality landscape accessed via head-mounted goggles. Users enter the virtual reality environment, and the user's own movements are captured via a gesture-tracking system. A \"teacher\" avatar guides users through an interactive ASL lesson involving both the observation and production of signs. Users learn ASL signs from both the first-person perspective and the third-person perspective. The inclusion of the first-person perspective may enhance the potential for embodied learning processes. Following the development of SAIL, the project involves conducting an electroencephalography (EEG) experiment to examine how the sensorimotor systems of the brain are engaged by the embodied experiences provided in SAIL. The extent of neural activity in the sensorimotor cortex during viewing of another person signing provides insight into how the observer is processing the signs within SAIL. The project team pioneers the integration of multiple technologies: avatars, motion capture systems, virtual reality, gesture tracking, and EEG with the goal of making progress toward an improved tool for sign language learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lorna",
   "pi_last_name": "Quandt",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lorna Quandt",
   "pi_email_addr": "lorna.quandt@gallaudet.edu",
   "nsf_id": "000719891",
   "pi_start_date": "2018-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Melissa",
   "pi_last_name": "Malzkuhn",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Melissa Malzkuhn",
   "pi_email_addr": "melissa.malzkuhn@gallaudet.edu",
   "nsf_id": "000719889",
   "pi_start_date": "2018-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Gallaudet University",
  "inst_street_address": "800 FLORIDA AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "WASHINGTON",
  "inst_state_code": "DC",
  "inst_state_name": "District of Columbia",
  "inst_phone_num": "2026515497",
  "inst_zip_code": "200023600",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DC00",
  "org_lgl_bus_name": "GALLAUDET UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TQCJUED1WEF9"
 },
 "perf_inst": {
  "perf_inst_name": "Gallaudet University",
  "perf_str_addr": "800 Florida Ave, NE",
  "perf_city_name": "Washington",
  "perf_st_code": "DC",
  "perf_st_name": "District of Columbia",
  "perf_zip_code": "200023695",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DC00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Improved resources for learning American Sign Language (ASL) are in high demand. There has been limited progress in using cutting-edge technologies to harness the visual-spatial nature of ASL for improved learning outcomes. At the same time, interactive speaking avatars have become valuable learning tools for spoken language instruction. But the potential educational uses of signing avatars have not been adequately explored. Our project, named Signing Avatars &amp; Immersive Learning (SAIL) investigated the feasibility of a system in which signing avatars (computer-animated virtual humans built from motion capture recordings) teach users ASL in an immersive virtual environment. The project focused on developing and testing this entirely novel ASL learning tool, while fostering the inclusion of underrepresented minorities in STEM.<br /><br />This project leveraged the theory of embodied learning to design the SAIL system. We developed a way that new ASL users can enter an immersive 3D environment and interact with a &ldquo;Teacher&rdquo; avatar to learn introductory ASL. We used novel motion capture technology to record a native deaf signer and processed the recordings to allow for high-fidelity representations of the signer&rsquo;s hands, fingers, and facial expressions. The Teacher was placed in a virtual reality environment accessed via head-mounted goggles (e.g., Oculus Rift S). Users enter the virtual reality environment, and the user's own movements are displayed via a gesture-tracking system. We intentionally included a three-dimensional view of the signed content so that users can better see and understand the handshapes and movements which are integral to signed communication. Following the development of SAIL, we conducted a large-scale online rating study to gather information about how signing avatars are perceived by ASL users with various language backgrounds. We found that overall, ASL users preferred our motion-capture avatar over a computer-synthesized avatar who moved in a more robotic way. We identified several significant relationships between how an ASL user&rsquo;s own ASL fluency changes their acceptance of signing avatars. Overall, the SAIL team developed a proof-of-concept version of a novel ASL learning system and conducted a large scale assessment of how potential users react to signing avatars. This research is critical for informing future design of signing avatars. The project team pioneered the integration of multiple technologies: avatars, motion capture systems, virtual reality, and gesture tracking with the goal of making progress toward an improved tool for sign language learning.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/29/2021<br>\n\t\t\t\t\tModified by: Lorna&nbsp;Quandt</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nImproved resources for learning American Sign Language (ASL) are in high demand. There has been limited progress in using cutting-edge technologies to harness the visual-spatial nature of ASL for improved learning outcomes. At the same time, interactive speaking avatars have become valuable learning tools for spoken language instruction. But the potential educational uses of signing avatars have not been adequately explored. Our project, named Signing Avatars &amp; Immersive Learning (SAIL) investigated the feasibility of a system in which signing avatars (computer-animated virtual humans built from motion capture recordings) teach users ASL in an immersive virtual environment. The project focused on developing and testing this entirely novel ASL learning tool, while fostering the inclusion of underrepresented minorities in STEM.\n\nThis project leveraged the theory of embodied learning to design the SAIL system. We developed a way that new ASL users can enter an immersive 3D environment and interact with a \"Teacher\" avatar to learn introductory ASL. We used novel motion capture technology to record a native deaf signer and processed the recordings to allow for high-fidelity representations of the signer\u2019s hands, fingers, and facial expressions. The Teacher was placed in a virtual reality environment accessed via head-mounted goggles (e.g., Oculus Rift S). Users enter the virtual reality environment, and the user's own movements are displayed via a gesture-tracking system. We intentionally included a three-dimensional view of the signed content so that users can better see and understand the handshapes and movements which are integral to signed communication. Following the development of SAIL, we conducted a large-scale online rating study to gather information about how signing avatars are perceived by ASL users with various language backgrounds. We found that overall, ASL users preferred our motion-capture avatar over a computer-synthesized avatar who moved in a more robotic way. We identified several significant relationships between how an ASL user\u2019s own ASL fluency changes their acceptance of signing avatars. Overall, the SAIL team developed a proof-of-concept version of a novel ASL learning system and conducted a large scale assessment of how potential users react to signing avatars. This research is critical for informing future design of signing avatars. The project team pioneered the integration of multiple technologies: avatars, motion capture systems, virtual reality, and gesture tracking with the goal of making progress toward an improved tool for sign language learning.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/29/2021\n\n\t\t\t\t\tSubmitted by: Lorna Quandt"
 }
}