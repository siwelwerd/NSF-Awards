{
 "awd_id": "1823562",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FoMR: Collaborative Research: Single-Thread Multi-Accelerator Execution to Close the Dennard Scaling Gap",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 37000.0,
 "awd_amount": 37000.0,
 "awd_min_amd_letter_date": "2018-08-10",
 "awd_max_amd_letter_date": "2018-08-10",
 "awd_abstract_narration": "With ever-slowing scaling trends in microprocessor technologies, traditional techniques of improving processor performance are no longer viable, and achieving higher performance requires a dramatically different approach. This project develops a multicore chip microarchitecture using specialized accelerators and code-injection techniques without needing to modify user-level software, compilers and operating systems.  The impact of this research is to help steer microprocessor design in novel ways that can help sustain performance improvements, especially for datacenter and big-data computing.  \r\n\r\nThis project builds on a recent promising technique involves offloading program phases onto specialized processors (accelerators) which are tuned to execute programs with specific characteristics  (i.e., parallelism, control dependence, memory behavior) at extremely high efficiency. There are two main challenges which motivate the major thrusts of this work.  The first is the question of how to design a practical system for managing the execution of heterogeneous accelerators and dynamic translation.  The second is how to design a set of accelerators which provide integer factors of improvement over general purpose processors' performance and energy efficiency.  This work addresses the first challenge by designing a disaggregated translation subsystem, including region detection hardware at each core, a set of disaggregated compiler cores and a translation cache, and a hardware/software layer which dynamically re-maps logical threads to physical cores based on dynamic code properties and load balancing.  To design a set of balanced accelerators, this work is analyzing programs to identify key program behaviors, and developing targeted accelerators for each.  Finally, this includes the design of synthesis-time resource allocation algorithms which will co-optimize the choice of cache interface, general core attributes, and accelerator execution model.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anthony",
   "pi_last_name": "Nowatzki",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anthony Nowatzki",
   "pi_email_addr": "tjn@cs.ucla.edu",
   "nsf_id": "000752659",
   "pi_start_date": "2018-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "UCLA Computer Science Dept.",
  "perf_str_addr": "420 Westwood Plaza,484 Engr. IV",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951596",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "021Z",
   "pgm_ref_txt": "Industry Partnerships"
  },
  {
   "pgm_ref_code": "2878",
   "pgm_ref_txt": "SPECIAL PROJECTS - CCF"
  },
  {
   "pgm_ref_code": "7798",
   "pgm_ref_txt": "SOFTWARE & HARDWARE FOUNDATION"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 37000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-46313647-7fff-2377-e18a-a6a9ce0a7bab\"> </span></p>\n<p dir=\"ltr\"><span>The slowing of transistor scaling (Dennard Scaling &amp; Moore's Law) has made evident the necessity to innovate at the architecture level to meaningfully improve computational ability. The goal of this project was to explore the paradigm of \"transparent specialization\" as a means of mitigating these effects. There are two key aspects of this paradigm: first, to augment existing ISAs with expressive abstractions that can both enable new microarchitecture optimizations and allow offloading of some program components onto specialized accelerators.&nbsp; The second aspect of transparent specialization is to automatically transform programs to use these abstractions using innovative compilers, all without programmer involvement.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>To understand the potential of this approach, we first undertook a modeling effort, which let us explore the possible dimensions of transparent specialization. Specifically, we built a trace-based program simulator using LLVM and gem5, which could optimistically simulate hybrid CPU/accelerator systems with varying degrees of optimism -- this tool is called GemForge.&nbsp; The study confirmed previous results that perfect accelerator execution (perfect dataflow + no reduction/induction dependences) can slightly outperform conventional processor cores and significantly reduce power consumption.&nbsp; However, our surprising new finding was that even with perfect this accelerator execution, the memory latency and throughput were still the primary bottlenecks.&nbsp; Thus, this defined our key focus, to accelerate memory.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Based on this new insight, this work developed the paradigm of \"memory specialization\" -- to encode coarse grain memory patterns as first-class ISA constructs, which we call streams.&nbsp; To summarize how they work, streams can encode common memory access patterns (linear, indirect, pointer-chasing) in a single instruction, which are generally executed before loops.&nbsp; Ordinary instructions within the loop can consume data pulled in by streams directly from standard registers (implemented through modifications to register renaming).</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>We found that the paradigm of memory specialization enables four unique opportunities.&nbsp; First, streams reduce the total number of instructions to be expressed, reducing core pipeline power and energy.&nbsp; Second, explicit memory patterns in streams can enable \"perfect\" prefetching (perfect accuracy and timeliness). Third, streams expose high-level information which the hardware does not have access too, e.g. reuse distance of a memory access pattern, and this information can be used to make more intelligent decisions about cache policies like replacement, and bypassing.&nbsp; Fourth, streams can be offloaded into the memory system, which heavily reduce the request traffic, and further reduce memory latency.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>In terms of quantitative results, on challenging SPEC 2017 workloads, the design improved large out-of-order core performance by ~1.4x, and enabled a dual-issue core to perform as well as a six-issue core, all while consuming 25% less energy.&nbsp; On data-processing workloads from Rodinia, the benefits are even more dramatic: a stream-specialized inorder core can beat an out-of-order core by 15%, using 3x less energy.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>To support other researchers and facilitate the use of our tools in an educational setting, our GemForge framework is released open source: </span><a href=\"https://github.com/PolyArch/gem-forge-framework\"><span>https://github.com/PolyArch/gem-forge-framework</span></a><span>.&nbsp; It includes support for fast exploration of accelerator architectures, as well as compilers and simulators for memory specialization.&nbsp; This tool has been used in graduate-level computer architecture courses to teach the principles of compiler/hardware codesign. Finally, this work supported one graduate research student, which enabled him to dedicate research time, attend conferences, and gain experience presenting his work in two major computer architecture conferences.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/17/2022<br>\n\t\t\t\t\tModified by: Tony&nbsp;Nowatzki</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe slowing of transistor scaling (Dennard Scaling &amp; Moore's Law) has made evident the necessity to innovate at the architecture level to meaningfully improve computational ability. The goal of this project was to explore the paradigm of \"transparent specialization\" as a means of mitigating these effects. There are two key aspects of this paradigm: first, to augment existing ISAs with expressive abstractions that can both enable new microarchitecture optimizations and allow offloading of some program components onto specialized accelerators.  The second aspect of transparent specialization is to automatically transform programs to use these abstractions using innovative compilers, all without programmer involvement.\n\n \nTo understand the potential of this approach, we first undertook a modeling effort, which let us explore the possible dimensions of transparent specialization. Specifically, we built a trace-based program simulator using LLVM and gem5, which could optimistically simulate hybrid CPU/accelerator systems with varying degrees of optimism -- this tool is called GemForge.  The study confirmed previous results that perfect accelerator execution (perfect dataflow + no reduction/induction dependences) can slightly outperform conventional processor cores and significantly reduce power consumption.  However, our surprising new finding was that even with perfect this accelerator execution, the memory latency and throughput were still the primary bottlenecks.  Thus, this defined our key focus, to accelerate memory.\n\n \nBased on this new insight, this work developed the paradigm of \"memory specialization\" -- to encode coarse grain memory patterns as first-class ISA constructs, which we call streams.  To summarize how they work, streams can encode common memory access patterns (linear, indirect, pointer-chasing) in a single instruction, which are generally executed before loops.  Ordinary instructions within the loop can consume data pulled in by streams directly from standard registers (implemented through modifications to register renaming).\n\n \nWe found that the paradigm of memory specialization enables four unique opportunities.  First, streams reduce the total number of instructions to be expressed, reducing core pipeline power and energy.  Second, explicit memory patterns in streams can enable \"perfect\" prefetching (perfect accuracy and timeliness). Third, streams expose high-level information which the hardware does not have access too, e.g. reuse distance of a memory access pattern, and this information can be used to make more intelligent decisions about cache policies like replacement, and bypassing.  Fourth, streams can be offloaded into the memory system, which heavily reduce the request traffic, and further reduce memory latency.\n\n \nIn terms of quantitative results, on challenging SPEC 2017 workloads, the design improved large out-of-order core performance by ~1.4x, and enabled a dual-issue core to perform as well as a six-issue core, all while consuming 25% less energy.  On data-processing workloads from Rodinia, the benefits are even more dramatic: a stream-specialized inorder core can beat an out-of-order core by 15%, using 3x less energy.\n\n \nTo support other researchers and facilitate the use of our tools in an educational setting, our GemForge framework is released open source: https://github.com/PolyArch/gem-forge-framework.  It includes support for fast exploration of accelerator architectures, as well as compilers and simulators for memory specialization.  This tool has been used in graduate-level computer architecture courses to teach the principles of compiler/hardware codesign. Finally, this work supported one graduate research student, which enabled him to dedicate research time, attend conferences, and gain experience presenting his work in two major computer architecture conferences.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 03/17/2022\n\n\t\t\t\t\tSubmitted by: Tony Nowatzki"
 }
}