{
 "awd_id": "1763268",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI:Medium:Collaborative Research: Object-Centric Inference of Actionable Information from Visual Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 475000.0,
 "awd_amount": 475000.0,
 "awd_min_amd_letter_date": "2018-08-14",
 "awd_max_amd_letter_date": "2018-08-14",
 "awd_abstract_narration": "This project will create novel algorithms and learning architectures suitable for understanding how to plan and execute actions in an environment for purposeful object manipulation. Such understanding is indispensable for autonomous agents operating in unstructured environments, and it is also valuable in providing automated assistance to humans during the execution of various physical tasks. The project will computationally \"imagine\" changes that actors with human-like manipulation capabilities can make on that environment and generate plans that can accomplish the desired manipulations. Such tools facilitate the creation of smart environments, where for example a perception system watching an elderly person can infer the task the person is trying to accomplish and offer advice/assistance. They also allow the creation of automated instructional videos customized to a particular environment that can be used for efficient training of unskilled workers. The project will provide mentoring and research opportunities for a diverse set of students, including members of groups typically under-represented in computer science.\r\n\r\nThis research will study environments formed by objects, some of which can be manipulated, while others define obstacles to be avoided or support surfaces to be used. Manipulating an object typically means interacting with small parts of the object, referred to as its active sites: handles, buttons, levers, graspable or pushable regions, etc. A deep challenge is to develop tools for identifying and classifying these active sites on objects at large scale, and to codify the types of interactions they partake of based on dynamic 2D/3D imagery, building a vocabulary of elementary actions. This requires novel machine learning methods and deep architectures for processing large-scale dynamic visual and geometric data. It also requires characterizing manipulations at a more abstract level so that they can be used by a variety of effectors, robotic or human, on different object geometries and physical characteristics. A further challenge is the accumulation and update of actionable information as more visual data is received in online object model repositories, such as ShapeNet. A final but key step of the approach will be the development of tools for transporting such action knowledge to new settings that are similar but not identical to the capture settings, using a variety of mathematical tools including functional maps.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Leonidas",
   "pi_last_name": "Guibas",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Leonidas J Guibas",
   "pi_email_addr": "guibas@cs.stanford.edu",
   "nsf_id": "000467730",
   "pi_start_date": "2018-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "450 Serra Mall",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 475000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of environment perception is often the planning and execution of actions ? for both humans and autonomous robotic agents. While traditional computer vision aims to understand the content of visual data representing an environment as it currently is, the focus of this project has been on ?what could be? -- to imagine what changes actors with human-like manipulation capabilities can make on that environment, and&nbsp;<em>what information is needed to effect these changes</em>. Towards this end we have developed data sets, algorithms, and learning architectures that tailor&nbsp;<strong>perception for action</strong>, delivering&nbsp;<strong>actionable information</strong>&nbsp;for object-centric task planning and execution.</p>\n<p>On the dataset front, the project has provided&nbsp;<strong>PartNet</strong>, the first large-scale space data set with fine-grained part annotations (over 500K parts among over 26K models) for over 25 common indoor object categories. A following refinement was&nbsp;<strong>PartNet-Mobility</strong>, which also encodes object articulations, including joint types and limits (14K articulated parts over 2,346 object models from 46 categories). The project also developed&nbsp;<strong>SAPIEN</strong>, a part-based interactive physics simulator for various robotic vision and interaction tasks. A second key line of contributions addresses object and camera pose estimation ? a key requirement for 3D action. The&nbsp;<strong>CAPTRA&nbsp;</strong>system significantly advanced the state of the art on articulated object pose estimation, without the need of prior CAD models. The project also contributed (1) to semi-supervised 3D object detection, requiring significantly less training data, (2) to multi-body segmentation and motion estimation with strong generalizability across different object categories, and (3) to the generation of novel tools for 3D point cloud processing that enable improved 3D object reconstruction ? including the ability retrieve relevant CAD models and deform them in semantically meaningful ways to match a desired target.</p>\n<p>Finally, by exploiting the SAPIEN simulator and PartNet-Mobility dataset, the project developed novel perception methods for learning object affordances and robot-object interactions&nbsp;<em>from experiments in simulation, with no further human supervision</em>. These include the automatic location of object regions suitable for certain actions (e.g., pulling or pushing when opening or closing a door), interaction trajectory planning, the execution of multi-object tasks (e.g., placing a bottle in a refrigerator), and the planning of sparse ?probing? interactions to learn object affordances.</p>\n<p>We expect that the techniques developed under this project can have many other applications in robotics as well as in AI-assisted smart environments for the elderly or for humans with disabilities.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/26/2022<br>\n\t\t\t\t\tModified by: Leonidas&nbsp;J&nbsp;Guibas</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of environment perception is often the planning and execution of actions ? for both humans and autonomous robotic agents. While traditional computer vision aims to understand the content of visual data representing an environment as it currently is, the focus of this project has been on ?what could be? -- to imagine what changes actors with human-like manipulation capabilities can make on that environment, and what information is needed to effect these changes. Towards this end we have developed data sets, algorithms, and learning architectures that tailor perception for action, delivering actionable information for object-centric task planning and execution.\n\nOn the dataset front, the project has provided PartNet, the first large-scale space data set with fine-grained part annotations (over 500K parts among over 26K models) for over 25 common indoor object categories. A following refinement was PartNet-Mobility, which also encodes object articulations, including joint types and limits (14K articulated parts over 2,346 object models from 46 categories). The project also developed SAPIEN, a part-based interactive physics simulator for various robotic vision and interaction tasks. A second key line of contributions addresses object and camera pose estimation ? a key requirement for 3D action. The CAPTRA system significantly advanced the state of the art on articulated object pose estimation, without the need of prior CAD models. The project also contributed (1) to semi-supervised 3D object detection, requiring significantly less training data, (2) to multi-body segmentation and motion estimation with strong generalizability across different object categories, and (3) to the generation of novel tools for 3D point cloud processing that enable improved 3D object reconstruction ? including the ability retrieve relevant CAD models and deform them in semantically meaningful ways to match a desired target.\n\nFinally, by exploiting the SAPIEN simulator and PartNet-Mobility dataset, the project developed novel perception methods for learning object affordances and robot-object interactions from experiments in simulation, with no further human supervision. These include the automatic location of object regions suitable for certain actions (e.g., pulling or pushing when opening or closing a door), interaction trajectory planning, the execution of multi-object tasks (e.g., placing a bottle in a refrigerator), and the planning of sparse ?probing? interactions to learn object affordances.\n\nWe expect that the techniques developed under this project can have many other applications in robotics as well as in AI-assisted smart environments for the elderly or for humans with disabilities.\n\n \n\n\t\t\t\t\tLast Modified: 11/26/2022\n\n\t\t\t\t\tSubmitted by: Leonidas J Guibas"
 }
}