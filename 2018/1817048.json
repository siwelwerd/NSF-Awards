{
 "awd_id": "1817048",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Algorithms and Software for Scalable Kernel Methods",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 476172.0,
 "awd_amount": 485562.0,
 "awd_min_amd_letter_date": "2018-06-25",
 "awd_max_amd_letter_date": "2019-05-06",
 "awd_abstract_narration": "Scientists and engineers are increasingly interested in using machine learning methods on huge datasets that cannot be processed on a single workstation.  At the same time public and private institutions are making significant investments on high-performance computing (HPC) clusters equipped with thousands of leading edge processors and network connectivity. However, despite the availability of such HPC systems, data analysis tasks are mostly restricted to a single or a few workstations. The reason is that, with few exceptions, existing machine learning software does not scale efficiently on HPC systems. The need to process in-situ large scientific and engineering datasets is not met with current software and significant downsampling is required in order to use existing tools. A serious bottleneck in current artificial intelligence (AI) workflows is the significant cost of training for large scale problems. The slow convergence of existing methods and the large number of calibration hyper-parameters (learning rate, batch size, and other knobs that control the performance of the AI system) make training extremely expensive. Design and analysis of scalable optimization algorithms for faster training, that is the fitting of the machine learning (ML) model parameters to the data, are needed for analytics in real time  and at scale, which is the goal of this project.\r\n\r\nThe proposed research will introduce novel numerical methods and parallel algorithms for second-order/Newton methods that will be tailored to machine learning (ML) models and will be many orders of magnitude faster than the existing state of-the-art (first-order methods like steepest descent). The researchers plan to design, analyze, and implement robust approximations for covariance matrices, a class of matrices in AI and computational statistics, used in statistical analysis (e.g., sampling, risk assessment, and uncertainty quantification). The investigators plan to design, analyze, and implement scalable fast algorithms in the context of high-performance computing for the so called nearest-neighbor problem, a particular method in ML, data analysis, and information retrieval. The resulting software library will provide a means for end-to-end tools for discovery and innovation and provide new capabilities in the NSF XSEDE infrastructure project. Along with  research activities, an educational and dissemination program is designed to communicate the results of this work to both students and researchers, as well as a more general audience of computational and application scientists.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "George",
   "pi_last_name": "Biros",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "George Biros",
   "pi_email_addr": "gbiros@gmail.com",
   "nsf_id": "000209886",
   "pi_start_date": "2018-06-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Texas at Austin",
  "perf_str_addr": "201, E 24th street",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121229",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "778100",
   "pgm_ele_name": "Leadership-Class Computing"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 476172.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 9390.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project we targeted a broad range of problems in the intersection of kernel methods, machine learning, deep learning, randomized linear algebra, and high-performance computing. In the course of the project we studied nearest neighbor algorithms, hierarchical clustering, hierarchical matrix approximation, approximation of deep learning operators, and active learning. Below we give a brief summary of our most important contributions.&nbsp;</p>\n<p><br />First, we introduced a new doubly-randomized method for approximating the Gauss-Newton operator of fully connected neural networks. We provided theoretical and experimental evidence to show the benefits of the new method. Second, we introduced algorithms and software for randomized k-nearest-neighbor searches and hierarchical density-based clustering of datasets with sparse coordinates in very high dimensions. Third, we proposed a new scheme for compressing arbitrary rectangular matrices by generalizing methods originally targeted to square kernel matrices. Fourth, we proposed a new randomized Cholesky factorization for sparse kernel matrices and graph Laplacians. Last, we considered the integration of the nearest-neighbor searches, clustering, and kernel-matrix approximation with deep learning in the context of active semi-supervised learning. Taken together this award resulted in new fundamental results in machine learning and high-dimensional data analysis; and in new HPC machine learning software. The nearest-neighbor algorithms, clustering, Cholesky factorization, and matrix approximation have been parallelized to scale on distributed memory, heterogeneous architectures.</p>\n<p>This award partially supported three graduate students, one postdoctoral researcher, and one undergraduate student through summer internships and a senior year project. The results were disseminated using peer-reviewed journals, conference presentations and invited talks by students and faculty, and through open-source software releases.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/29/2022<br>\n\t\t\t\t\tModified by: George&nbsp;Biros</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this project we targeted a broad range of problems in the intersection of kernel methods, machine learning, deep learning, randomized linear algebra, and high-performance computing. In the course of the project we studied nearest neighbor algorithms, hierarchical clustering, hierarchical matrix approximation, approximation of deep learning operators, and active learning. Below we give a brief summary of our most important contributions. \n\n\nFirst, we introduced a new doubly-randomized method for approximating the Gauss-Newton operator of fully connected neural networks. We provided theoretical and experimental evidence to show the benefits of the new method. Second, we introduced algorithms and software for randomized k-nearest-neighbor searches and hierarchical density-based clustering of datasets with sparse coordinates in very high dimensions. Third, we proposed a new scheme for compressing arbitrary rectangular matrices by generalizing methods originally targeted to square kernel matrices. Fourth, we proposed a new randomized Cholesky factorization for sparse kernel matrices and graph Laplacians. Last, we considered the integration of the nearest-neighbor searches, clustering, and kernel-matrix approximation with deep learning in the context of active semi-supervised learning. Taken together this award resulted in new fundamental results in machine learning and high-dimensional data analysis; and in new HPC machine learning software. The nearest-neighbor algorithms, clustering, Cholesky factorization, and matrix approximation have been parallelized to scale on distributed memory, heterogeneous architectures.\n\nThis award partially supported three graduate students, one postdoctoral researcher, and one undergraduate student through summer internships and a senior year project. The results were disseminated using peer-reviewed journals, conference presentations and invited talks by students and faculty, and through open-source software releases. \n\n \n\n\t\t\t\t\tLast Modified: 04/29/2022\n\n\t\t\t\t\tSubmitted by: George Biros"
 }
}