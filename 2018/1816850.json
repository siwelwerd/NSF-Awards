{
 "awd_id": "1816850",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Lightweight Virtualization Driven Elastic Memory Management and Cluster Scheduling",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Danella Zhao",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 376000.0,
 "awd_amount": 376000.0,
 "awd_min_amd_letter_date": "2018-06-21",
 "awd_max_amd_letter_date": "2018-06-21",
 "awd_abstract_narration": "Data-centers are evolving to host heterogeneous workloads on shared clusters to reduce the operational cost and achieve high resource utilization. However, it is challenging to schedule heterogeneous workloads with diverse resource requirements and performance constraints on heterogeneous hardware. Data parallel processing often suffers from interference and significant memory pressure, resulting in excessive garbage collection and out-of-memory errors that harm application performance and reliability. Cluster memory management and scheduling is still inefficient, leading to low utilization and poor multi-service support. Existing approaches either focus on application awareness or operating system awareness, thus are not well positioned to address the semantic gap between application run-times and the operating system. This project aims to improve application performance and cluster efficiency via lightweight virtualization-enabled elastic memory management and cluster scheduling. It combines system experimentation with rigorous design and analyses to improve performance and efficiency, and tackle memory pressure of data-parallel processing. Developed system software will be open-sourced, providing opportunities to foster a large ecosystem that spans system software providers and customers. \r\n\r\nEnabled by lightweight containers, cluster scheduling and the underlying operating system can cooperate synergistically, such that, the dynamic resource demand of an application can be exposed to the operating system, and the cluster memory manager and scheduler can be assisted with rich run-time information retrieved from performance counters and operating system.  Towards this end, the project aims to devise a distributed memory manager for data-parallel programs that can survive from memory pressure and enable elastic cluster memory management with architecture-aware container placement, design a cooperative paging to improve performance of memory swapping by extending the current virtual memory reclaim mechanism in Linux kernel, enable memory over-commitment for elastic cluster scheduling with a new service that can detect and exploit the over-commitment opportunities, and design a multi-queue based distributed task scheduler to manage performance interference and hardware heterogeneity. The contributions include a library of developed mechanisms and open-source system software at cluster and kernel levels that can significantly improve cluster utilization and application performance.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiaobo",
   "pi_last_name": "Zhou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaobo Zhou",
   "pi_email_addr": "xzhou@uccs.edu",
   "nsf_id": "000330885",
   "pi_start_date": "2018-06-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Colorado at Colorado Springs",
  "inst_street_address": "1420 AUSTIN BLUFFS PKWY",
  "inst_street_address_2": "",
  "inst_city_name": "COLORADO SPRINGS",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "7192553153",
  "inst_zip_code": "809183733",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "CO05",
  "org_lgl_bus_name": "THE REGENTS OF THE UNIVERSITY OF COLORADO",
  "org_prnt_uei_num": "",
  "org_uei_num": "RH87YDXC1AY5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Colorado at Colorado Springs",
  "perf_str_addr": "1420 Austin Bluffs Parkway",
  "perf_city_name": "Colorado Springs",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "809183733",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "CO05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 376000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Datacenters are evolving to co-locate heterogeneous workloads on shared clusters to reduce the operational cost and achieve high resource utilization. However, data parallel processing in datacenters often suffers from performance interference and significant memory pressure, resulting in excessive garbage collection and out-of-memory (OOM) errors that harm application performance and reliability. In this project, we proposed a new design space that exploits lightweight virtualization paradigm to improve application performance and cluster utilization by elastic memory management and scheduling. The research centers on two key designs and the integration, 1) elastic memory management runtimes that can survive from memory pressure, improve performance of memory swapping, and enable memory over-commitment; 2) elastic cluster scheduling that can detect and exploit the memory over-commitment opportunities and tackle interferences for efficient job co-location. We obtained following outcomes in research, student training, and curricula integration.</p>\n<p>We identified two major challenges in cluster memory management for data-intensive applications: (1) improper memory setting causes individual tasks suffering from performance degradation or even OOM errors; (2) dynamic and unpredictable task memory behaviors under static memory management cause significant resource wastage and queuing delay, while memory over-provision often leads to under-utilized cluster resource and significant job queuing delay. We designed <strong><em>Pufferfish</em></strong>, a container-driven application agnostic elastic memory manager. Pufferfish is transparent to applications and compatible to existing cluster schedulers. We observed that a user space implementation of OOM killer fails to do that and results in significant memory thrashing because of lacking the memory pressure knowledge from OS. We designed a user-assisted OOM killer (namely UA killer) in kernel space, an OS augmentation for accurate thrashing detection and agile task killing. Upon the UA killer, we developed <strong><em>Charon</em></strong>, a cluster scheduler for oversubscription of opportunistic memory in an on-demand manner. Charon is implemented upon Mercury. We found that the current memory reclaim mechanism in Linux OS significantly degrades the performance of latency-sensitive services under memory pressure, which makes co-location inefficient or even ineffective. We designed and developed <strong><em>Hermes</em></strong>, a library-level mechanism for fast memory allocation for latency-sensitive services in a shared environment based on resource reservation and proactive reclamation.&nbsp; Hermes is implemented in library Glibc. We further tackled interference between the concurrent garbage collection (GC) threads and the worker threads of latency-critical services. We designed a lightweight native storage <strong><em>FlashByte</em></strong>, which can efficiently cache the intermediate data with low overhead of GC. We proposed<strong><em> iGC</em></strong>, a middleware that bridges the semantic gap between JVM and the OS and improves concurrent GC performance in multi-tenant systems.</p>\n<p>Efficient co-location of different workloads lies in how to minimize the queuing delays of short jobs while maximizing cluster utilization. We proposed <strong><em>BIG-C</em></strong>, a container-based resource management framework for data-intensive cluster computing and preemptive fair share cluster scheduling. BIG-C was implemented upon YARN. Although memory and CPU isolation have been extensively studied in co-located servers, we found that Simultaneous Multi-Threading (SMT) technology imposes non-trivial interference on memory access which jeopardizes efficient co-location and performance assurance of latency-critical services. We proposed a statistic approach to select the appropriate hardware performance events and form a metric for SMT interference quantification. We developed <strong><em>Holmes</em></strong>, a user-space approach to SMT interference diagnosis and adaptive CPU scheduling for efficient job co-location in multi-tenant systems. Understanding and troubleshooting distributed data analytics systems is critical to ensure reliable production environments. We designed and developed <strong><em>IntelLog</em></strong>, a semantic-aware non-intrusive workflow reconstruction tool for distributed data analytics systems. Leveraging natural language processing, IntelLog automatically can extract and format semantic information in each log message.</p>\n<p>In addition to the two primary research themes (memory management and cluster scheduling), our research was also extended to cluster support to iterative machine learning (ML) workloads. We &nbsp;designed <strong><em>EPS</em></strong>, an augmented and lightweight Parameter Sever that allow to adjust the number of workers and servers for a ML job at runtime and significantly accelerates feedback-driven exploration and distributed ML model training. Furthermore, we developed an efficient, nonintrusive GPU scheduling framework that employs a combination of an adaptive GPU scheduler and an elastic GPU allocation mechanism to reduce job makespan and improve cluster resource utilization.</p>\n<p>Overall, this project combines system experimentation with rigorous design and analyses to improve performance and efficiency and tackle memory pressure of data-parallel processing.</p>\n<p>This project also provided significant training and professional development for students. It supported four PhD students in conducting quality research. It also engaged one REU undergraduate student and one Master student in research experience. Three students have obtained PhD degrees and joined industry including Nvidia and Facebook. The Master student was recruited to an NSF SFS program and joined MITRE.</p>\n<p>The outcomes were disseminated to research communities by presentations/publications in ACM/IEEE conferences, publications in IEEE Transactions, PhD dissertations, and open-source artifacts at GitHub. We integrated research results with curricula innovation by integrating some research results into one graduate-level course in Computer Communications.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/01/2022<br>\n\t\t\t\t\tModified by: Xiaobo&nbsp;Zhou</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDatacenters are evolving to co-locate heterogeneous workloads on shared clusters to reduce the operational cost and achieve high resource utilization. However, data parallel processing in datacenters often suffers from performance interference and significant memory pressure, resulting in excessive garbage collection and out-of-memory (OOM) errors that harm application performance and reliability. In this project, we proposed a new design space that exploits lightweight virtualization paradigm to improve application performance and cluster utilization by elastic memory management and scheduling. The research centers on two key designs and the integration, 1) elastic memory management runtimes that can survive from memory pressure, improve performance of memory swapping, and enable memory over-commitment; 2) elastic cluster scheduling that can detect and exploit the memory over-commitment opportunities and tackle interferences for efficient job co-location. We obtained following outcomes in research, student training, and curricula integration.\n\nWe identified two major challenges in cluster memory management for data-intensive applications: (1) improper memory setting causes individual tasks suffering from performance degradation or even OOM errors; (2) dynamic and unpredictable task memory behaviors under static memory management cause significant resource wastage and queuing delay, while memory over-provision often leads to under-utilized cluster resource and significant job queuing delay. We designed Pufferfish, a container-driven application agnostic elastic memory manager. Pufferfish is transparent to applications and compatible to existing cluster schedulers. We observed that a user space implementation of OOM killer fails to do that and results in significant memory thrashing because of lacking the memory pressure knowledge from OS. We designed a user-assisted OOM killer (namely UA killer) in kernel space, an OS augmentation for accurate thrashing detection and agile task killing. Upon the UA killer, we developed Charon, a cluster scheduler for oversubscription of opportunistic memory in an on-demand manner. Charon is implemented upon Mercury. We found that the current memory reclaim mechanism in Linux OS significantly degrades the performance of latency-sensitive services under memory pressure, which makes co-location inefficient or even ineffective. We designed and developed Hermes, a library-level mechanism for fast memory allocation for latency-sensitive services in a shared environment based on resource reservation and proactive reclamation.  Hermes is implemented in library Glibc. We further tackled interference between the concurrent garbage collection (GC) threads and the worker threads of latency-critical services. We designed a lightweight native storage FlashByte, which can efficiently cache the intermediate data with low overhead of GC. We proposed iGC, a middleware that bridges the semantic gap between JVM and the OS and improves concurrent GC performance in multi-tenant systems.\n\nEfficient co-location of different workloads lies in how to minimize the queuing delays of short jobs while maximizing cluster utilization. We proposed BIG-C, a container-based resource management framework for data-intensive cluster computing and preemptive fair share cluster scheduling. BIG-C was implemented upon YARN. Although memory and CPU isolation have been extensively studied in co-located servers, we found that Simultaneous Multi-Threading (SMT) technology imposes non-trivial interference on memory access which jeopardizes efficient co-location and performance assurance of latency-critical services. We proposed a statistic approach to select the appropriate hardware performance events and form a metric for SMT interference quantification. We developed Holmes, a user-space approach to SMT interference diagnosis and adaptive CPU scheduling for efficient job co-location in multi-tenant systems. Understanding and troubleshooting distributed data analytics systems is critical to ensure reliable production environments. We designed and developed IntelLog, a semantic-aware non-intrusive workflow reconstruction tool for distributed data analytics systems. Leveraging natural language processing, IntelLog automatically can extract and format semantic information in each log message.\n\nIn addition to the two primary research themes (memory management and cluster scheduling), our research was also extended to cluster support to iterative machine learning (ML) workloads. We  designed EPS, an augmented and lightweight Parameter Sever that allow to adjust the number of workers and servers for a ML job at runtime and significantly accelerates feedback-driven exploration and distributed ML model training. Furthermore, we developed an efficient, nonintrusive GPU scheduling framework that employs a combination of an adaptive GPU scheduler and an elastic GPU allocation mechanism to reduce job makespan and improve cluster resource utilization.\n\nOverall, this project combines system experimentation with rigorous design and analyses to improve performance and efficiency and tackle memory pressure of data-parallel processing.\n\nThis project also provided significant training and professional development for students. It supported four PhD students in conducting quality research. It also engaged one REU undergraduate student and one Master student in research experience. Three students have obtained PhD degrees and joined industry including Nvidia and Facebook. The Master student was recruited to an NSF SFS program and joined MITRE.\n\nThe outcomes were disseminated to research communities by presentations/publications in ACM/IEEE conferences, publications in IEEE Transactions, PhD dissertations, and open-source artifacts at GitHub. We integrated research results with curricula innovation by integrating some research results into one graduate-level course in Computer Communications.\n\n\t\t\t\t\tLast Modified: 07/01/2022\n\n\t\t\t\t\tSubmitted by: Xiaobo Zhou"
 }
}