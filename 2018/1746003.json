{
 "awd_id": "1746003",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Gripper-integrated proximity, contact and force sensing for collaborative robots",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Muralidharan Nair",
 "awd_eff_date": "2018-01-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 224999.0,
 "awd_amount": 224999.0,
 "awd_min_amd_letter_date": "2017-12-21",
 "awd_max_amd_letter_date": "2017-12-21",
 "awd_abstract_narration": "The broader impact/commercial potential of this project will be to greatly advance the application of robots and improve robots safety when working together with humans. Integration of embedded proximity sensors into robotic end-effectors with an advanced user-interface control system provides a significant leap from current sensory and control systems available. The advances of this project in sensing and control will expand the current boundaries which limit human interaction with the power of robots. The improvements to both control and sensing in collaborative robots will advance the use of robots in industrial, medical and household applications. Improved accuracy will expand robot use and significantly lower manufacturing costs in many advanced industries. Vast increases in robot safety with this project?s system will allow expanded human interaction including robot care for the needs of the growing senior population.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project will advance our understanding of the role tactile sensation plays in robust manipulation and lead to novel means for in-gripper 3D sensing, complementing and in some cases replacing external 3D perception systems. Specifically, this project will demonstrate the role that proximity sensing and zero-force contact sensing play in grasp planning, object registration, and object recognition. A key challenge will be to merge information from two different sensor technologies that both excel in different range regimes, but have poor accuracy otherwise. The proposed covariance-based sensor fusion algorithms will be benchmarked by comparison with 3D models obtained from high-fidelity laser and 3D-scanning systems. A second challenge will be to abstract the resulting capability into easy-to-use programming environments for robotic grasp and manipulation planning, which will be tested with customers in the field.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nicolaus",
   "pi_last_name": "Correll",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Nicolaus J Correll",
   "pi_email_addr": "ncorrell@colorado.edu",
   "nsf_id": "000546404",
   "pi_start_date": "2017-12-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Robotic Materials Inc",
  "inst_street_address": "3080 VALMONT RD # 100",
  "inst_street_address_2": "",
  "inst_city_name": "BOULDER",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3037171436",
  "inst_zip_code": "803012152",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "ROBOTIC MATERIALS INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "DAVKSDE6BGW4"
 },
 "perf_inst": {
  "perf_inst_name": "Robotic Materials Inc",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "803041147",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "8034",
   "pgm_ref_txt": "Hardware Components"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 224999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This Phase I SBIR project has resulted in a novel robotic grasping solution that tightly integrates tactile, force, and proximity sensing into the fingers and palm of a robotic hand to perform robust manipulation in unstructured environments. Information from different sensing modalities are fused in an embedded computing system co-located with the gripper, which is sufficient to control modern robotic arms and mobile platforms in real-time. The embedded 3D perception system is able to perceive objects from a range of a few meters down to 11cm, which is sufficient for localizing and approaching objects of interest. Once the object is grasped, proximity, contact and force sensing are used to make up for inaccuracies in the initial localization and gently manipulate it.</p>\n<p>Specifically, this project has demonstrated the hand's ability to pick-up unknown objects ranging in sizes from a few millimeters to multiple centimeters, objects that are fragile and require handling with a minimum of force, and the suitability of the technology in a mobile manipulation framework that is prone to location uncertainty. This project has also led to a graphical programming environment that allows to abstract complex computer vision, motion planning, and machine learning tasks and can be easily reconfigured by an unexperienced user. This enables applications in low-volume, high-mix manufacturing tasks that require fast reconfiguration, handling and manipulating of fruit in an agricultural setting, or delivery of items in a warehouse environment, thereby complementing the skills of a human worker.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/01/2018<br>\n\t\t\t\t\tModified by: Nicolaus&nbsp;J&nbsp;Correll</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1746003/1746003_10526631_1533133849438_rm2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1746003/1746003_10526631_1533133849438_rm2--rgov-800width.jpg\" title=\"bin-picking\"><img src=\"/por/images/Reports/POR/2018/1746003/1746003_10526631_1533133849438_rm2--rgov-66x44.jpg\" alt=\"bin-picking\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Robotic Materials' hand on an autonomous cart by Canvas Technology performing bin picking and kitting.</div>\n<div class=\"imageCredit\">Austin Miller</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nicolaus&nbsp;J&nbsp;Correll</div>\n<div class=\"imageTitle\">bin-picking</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1746003/1746003_10526631_1533133731194_rm1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1746003/1746003_10526631_1533133731194_rm1--rgov-800width.jpg\" title=\"Ring-picking\"><img src=\"/por/images/Reports/POR/2018/1746003/1746003_10526631_1533133731194_rm1--rgov-66x44.jpg\" alt=\"Ring-picking\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Robotic Materials Inc.' autonomous hand picking rings that are difficult to perceive with overhead vision and hard to grasp using suction.</div>\n<div class=\"imageCredit\">Robotic Materials Inc.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nicolaus&nbsp;J&nbsp;Correll</div>\n<div class=\"imageTitle\">Ring-picking</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1746003/1746003_10526631_1533133934006_rm3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1746003/1746003_10526631_1533133934006_rm3--rgov-800width.jpg\" title=\"strawberry\"><img src=\"/por/images/Reports/POR/2018/1746003/1746003_10526631_1533133934006_rm3--rgov-66x44.jpg\" alt=\"strawberry\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Robotic Materials' hand using contact and force sensing for gentle manipulation of fruits.</div>\n<div class=\"imageCredit\">Robotic Materials Inc.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nicolaus&nbsp;J&nbsp;Correll</div>\n<div class=\"imageTitle\">strawberry</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis Phase I SBIR project has resulted in a novel robotic grasping solution that tightly integrates tactile, force, and proximity sensing into the fingers and palm of a robotic hand to perform robust manipulation in unstructured environments. Information from different sensing modalities are fused in an embedded computing system co-located with the gripper, which is sufficient to control modern robotic arms and mobile platforms in real-time. The embedded 3D perception system is able to perceive objects from a range of a few meters down to 11cm, which is sufficient for localizing and approaching objects of interest. Once the object is grasped, proximity, contact and force sensing are used to make up for inaccuracies in the initial localization and gently manipulate it.\n\nSpecifically, this project has demonstrated the hand's ability to pick-up unknown objects ranging in sizes from a few millimeters to multiple centimeters, objects that are fragile and require handling with a minimum of force, and the suitability of the technology in a mobile manipulation framework that is prone to location uncertainty. This project has also led to a graphical programming environment that allows to abstract complex computer vision, motion planning, and machine learning tasks and can be easily reconfigured by an unexperienced user. This enables applications in low-volume, high-mix manufacturing tasks that require fast reconfiguration, handling and manipulating of fruit in an agricultural setting, or delivery of items in a warehouse environment, thereby complementing the skills of a human worker. \n\n\t\t\t\t\tLast Modified: 08/01/2018\n\n\t\t\t\t\tSubmitted by: Nicolaus J Correll"
 }
}