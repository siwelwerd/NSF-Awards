{
 "awd_id": "1820462",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Low-cost real-time perception system for self-driving consumer cars",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Muralidharan Nair",
 "awd_eff_date": "2018-06-15",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 225000.0,
 "awd_amount": 225000.0,
 "awd_min_amd_letter_date": "2018-06-15",
 "awd_max_amd_letter_date": "2018-06-15",
 "awd_abstract_narration": "The broader impact/commercial potential of this project is the practical deployment of a low-cost and low-power real-time perception system in self-driving consumer cars. This edge computing functionality in sensors enables higher reliability and lower cost of overall sensing and computing needed for truly autonomous self-driving. Such innovation will contribute significantly to the early and widespread availability of safety and convenience benefits to consumers. Furthermore, the advanced perception system will have a potential long-term impact on robotics in general, which can lead to creation of new markets and new lifestyles.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project aims to develop efficient algorithms and software implementation of a real-time perception system to enable the use of low-cost computing systems for self-driving cars. The algorithms provide a novel way of using image features to perform simultaneous localization and mapping (SLAM) with 100 times less computational costs than the existing algorithms. They also include a truly novel neural network to fuse the image feature and light detection and ranging (LiDAR) features and perform object detection, which has 100 times less complexity compared to the state-of-the-art method. These reduced-complexity algorithms can be implemented on low-power and low-cost SoC processors.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Koji",
   "pi_last_name": "Seto",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Koji Seto",
   "pi_email_addr": "kojiseto.ai@gmail.com",
   "nsf_id": "000761619",
   "pi_start_date": "2018-06-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Apollo AI Inc.",
  "inst_street_address": "1267 Willis Street, STE 200",
  "inst_street_address_2": "",
  "inst_city_name": "Redding",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "4087581593",
  "inst_zip_code": "960010400",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "CA01",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": "WL24LCQ2P4N3"
 },
 "perf_inst": {
  "perf_inst_name": "Apollo AI Inc.",
  "perf_str_addr": "97 S. 2nd St. Suite 100",
  "perf_city_name": "San Jose",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "951132512",
  "perf_ctry_code": "US",
  "perf_cong_dist": "18",
  "perf_st_cong_dist": "CA18",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "8034",
   "pgm_ref_txt": "Hardware Components"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 225000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Autonomous driving technology will enable a new \"Passenger Economy\" worth US$7 trillion. Strategy Analytics expects that: 1. Conservatively, 585,000 lives can be saved due to pilotless vehicles in the era of the Passenger Economy from 2035 to 2045. 2. Pilotless vehicles will free more than 250 million hours of consumers' commuting time per year in the most congested cities in the world. 3. Reductions in public safety costs related to traffic accidents will amount to more than US$234 billion over the Passenger Economy era from 2035-2045. While self-driving technology is expected to generate huge savings when it is deployed, it is super expensive and out-of-reach to consumers. Two pain points of the customers in bringing the fully autonomous self-driving consumer cars to the market are: 1. Autonomous driving systems are super-expensive and cost $250K for the sensors and computing in addition to the car itself. 2. Some of today's prototypes for fully autonomous systems consume two to four kilowatts of electricity.</p>\n<p>The Phase I project aimed to develop the efficient algorithms and software implementation of real-time perception system to enable the use of 100 times lower-cost computing systems for self-driving cars. The system provides real-time simultaneous localization and mapping (SLAM) with object detection, classification, and tracking for self-driving applications. It features the efficient algorithm and implementation for SLAM using real-time fusion of light detection and ranging (LiDAR) data and high definition image data and the efficient object detection using reduced complexity neural network called ApolloPerceptionNet which enables real-time implementation of this system on high-performance and low-power processors.</p>\n<p>The ApolloPerceptionNet features low-complexity algorithm using a fusion of LiDAR point cloud and camera images. Whereas a LiDAR-and-camera fusion helps it increase performance, the drastic reduction of computational complexity makes it extremely harder to achieve the target accuracy of object detection. We then turned our attention to perception system and proposed a novel low-cost real-time perception system that can achieve high object detection accuracy in real driving conditions.</p>\n<p>As a result, we were able to achieve the significant progress through Phase I efforts.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/02/2019<br>\n\t\t\t\t\tModified by: Koji&nbsp;Seto</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAutonomous driving technology will enable a new \"Passenger Economy\" worth US$7 trillion. Strategy Analytics expects that: 1. Conservatively, 585,000 lives can be saved due to pilotless vehicles in the era of the Passenger Economy from 2035 to 2045. 2. Pilotless vehicles will free more than 250 million hours of consumers' commuting time per year in the most congested cities in the world. 3. Reductions in public safety costs related to traffic accidents will amount to more than US$234 billion over the Passenger Economy era from 2035-2045. While self-driving technology is expected to generate huge savings when it is deployed, it is super expensive and out-of-reach to consumers. Two pain points of the customers in bringing the fully autonomous self-driving consumer cars to the market are: 1. Autonomous driving systems are super-expensive and cost $250K for the sensors and computing in addition to the car itself. 2. Some of today's prototypes for fully autonomous systems consume two to four kilowatts of electricity.\n\nThe Phase I project aimed to develop the efficient algorithms and software implementation of real-time perception system to enable the use of 100 times lower-cost computing systems for self-driving cars. The system provides real-time simultaneous localization and mapping (SLAM) with object detection, classification, and tracking for self-driving applications. It features the efficient algorithm and implementation for SLAM using real-time fusion of light detection and ranging (LiDAR) data and high definition image data and the efficient object detection using reduced complexity neural network called ApolloPerceptionNet which enables real-time implementation of this system on high-performance and low-power processors.\n\nThe ApolloPerceptionNet features low-complexity algorithm using a fusion of LiDAR point cloud and camera images. Whereas a LiDAR-and-camera fusion helps it increase performance, the drastic reduction of computational complexity makes it extremely harder to achieve the target accuracy of object detection. We then turned our attention to perception system and proposed a novel low-cost real-time perception system that can achieve high object detection accuracy in real driving conditions.\n\nAs a result, we were able to achieve the significant progress through Phase I efforts.\n\n\t\t\t\t\tLast Modified: 11/02/2019\n\n\t\t\t\t\tSubmitted by: Koji Seto"
 }
}