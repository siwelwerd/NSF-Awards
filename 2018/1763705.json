{
 "awd_id": "1763705",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Robots That Learn From Description Through Synthesis and Analysis",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-09-15",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 1197449.0,
 "awd_amount": 1213449.0,
 "awd_min_amd_letter_date": "2018-09-07",
 "awd_max_amd_letter_date": "2022-07-06",
 "awd_abstract_narration": "Many types of physical work are performed most efficiently in collaboration. For example, garage mechanics, electricians, surgeons, and carpenters often employ an assistant or an apprentice to help them perform their jobs. The goal of the proposed project is to develop the underlying principles that would allow an intelligent robotic system to naturally collaborate with a human partner in such tasks.  In particular, the proposed research studies the problem of performing an action in response to a complex command; for example, \"When I hold these two pieces together with the holes aligned, place a 3 inch screw through the hole.\"  The results of this research will be demonstrated in a system that is able to assist in typical assembly or cooking tasks.\r\n\r\nThe research explores the development of perception-based classifiers that are composed, on demand, from the structure of the command itself. This classifier is constructed from pre-trained generic components and is then fine-tuned using simulation-generated data also derived from the structure of the command.  This poses several challenging technical problems. The first problem is to create general-purpose perceptual components that correspond to the language units of the query's objects, actions, relationships, and activities. The second problem is to compose these units to form a classifier and to create methods for on-demand fine-tuning of the classifier from simulation data.  The system must then be able to command a robot to perform the correct action in response to the command. Measures of success of this research will be 1) the breadth of capabilities the system is able to provide; 2) the specificity it is able to achieve when applied to representative video data; and 3) its ability to naturally interact with a human user to perform a collaborative manipulation task.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Hager",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory D Hager",
   "pi_email_addr": "hager@cs.jhu.edu",
   "nsf_id": "000385453",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Yuille",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Alan L Yuille",
   "pi_email_addr": "ayuille1@jhu.edu",
   "nsf_id": "000107159",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Austin",
   "pi_last_name": "Reiter",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Austin Reiter",
   "pi_email_addr": "areiter2@jhu.edu",
   "nsf_id": "000677328",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 1197449.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Future collaborative robots will, by necessity, need to be able to accept and perceptually ground instructions from co-workers. A robot in a small manufacturing enterprise might be instructed to &ldquo;...vacuum the top of the vise while I am drilling,&rdquo; or a robot assisting in food preparation may be asked to &ldquo;...transfer the onion to the pan once I&rsquo;m finished cutting it.&rdquo; In each case, the robot has to both develop a shared perceptual understanding of the scene defined by the content of the request, and then detect when certain events occur to trigger or synchronize an action.&nbsp;&nbsp;</p>\n<p>Our project has advanced our vision for interactive perception-based robotics along several dimensions.&nbsp;&nbsp;With regard to perception, in order to ground and react to language instructions, a robot must be able to divide (segment) a scene into relevant objects, and it must be able to understand and react to activity relative to those objects.&nbsp;In recent years, video instance segmentation (VIS) has been largely advanced by offline models, while online models gradually attracted less attention possibly due to their inferior performance. However, online methods are necessary to handle long, ongoing video sequences common in robotic interaction. We developed an online framework for VIAS based on contrastive learning that is able to learn more discriminative instance embeddings for association and fully exploit history information for stability. Despite its simplicity, our method outperforms all online and offline methods on three benchmarks. We developed complementary methods for motion estimation (optical flow) that take advantage of a new method for self-generated labels, and we also developed methods that are able to refine human poses over time through a unique method of constraining them to a dictionary of local pose manifolds. Finally, we developed an approach to fine-grained activity recognition that models activities as compositions of&nbsp;<em>dynamic action signatures</em>. This compositional approach allows us to reframe fine-grained recognition as&nbsp;<em>zero-shot&nbsp;</em>activity recognition, where a detector is composed &ldquo;on the fly&rdquo; from simple first-principles state machines supported by deep-learned components. We were able to show state of the art performance for activity recognition, and also to show that we can use off-the-shelf object detectors to recognize newly described activities in completely de-novo settings with no additional training.&nbsp;</p>\n<p>With regard to robotic task performance, we note that modern reinforcement learning (RL) algorithms are not sample- efficient to train on multi-step tasks in complex domains, impeding their wider deployment in the real world. We address this problem by leveraging the insight that RL models trained to complete one set of tasks can be re-purposed to complete related tasks when given just a handful of demonstrations. Based upon this insight, we propose See-SPOT-Run (SSR), a new computational approach to robot learning that enables a robot to complete a variety of real robot tasks in novel problem domains&nbsp;<em>without&nbsp;</em>task-specific training. Complementing this, we investigated ways to efficiently enable human operators to interact with robotic agents using natural language would allow non-experts to intuitively instruct these agents. We developed a novel transformer-based model which enables a user to guide a robot arm through a 3D multi-step manipulation task with natural language commands. Our system maps images and commands to masks over grasp or place locations, grounding the language directly in perceptual space. In a suite of block rearrangement tasks, we showeda that these masks can be combined with an existing manipulation framework without re-training, greatly improving learning efficiency. Our modular design allows us to leverage supervised and reinforcement learning, providing an easy interface for experimentation with different architectures and it completes block manipulation tasks with synthetic commands 530% more often than a comparable UNet-based baseline.</p>\n<p>The&nbsp;<strong>Broader Impact</strong>&nbsp;of our work is primarily through two avenues. First, we have had the opportunity to train seven graduate students plus additional master&rsquo;s and undergraduate students during the course of this project. These students have gone on to both industry and academic positions, building on the work they did in this project. Second, the impact of vision-based robotics broadly is growing rapidly in the form of automated driving, the increasing use of vision-based robotics in manufacturing and in other warehouse applications. However, they still lack the flexibility and accuracy necessary to operate as everyday interactive assistants. Our methods have been published in leading conferences and will be available to support those use-cases, as well as providing a basis for advances in other future research efforts.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/13/2024<br>\nModified by: Gregory&nbsp;D&nbsp;Hager</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nFuture collaborative robots will, by necessity, need to be able to accept and perceptually ground instructions from co-workers. A robot in a small manufacturing enterprise might be instructed to ...vacuum the top of the vise while I am drilling, or a robot assisting in food preparation may be asked to ...transfer the onion to the pan once Im finished cutting it. In each case, the robot has to both develop a shared perceptual understanding of the scene defined by the content of the request, and then detect when certain events occur to trigger or synchronize an action.\n\n\nOur project has advanced our vision for interactive perception-based robotics along several dimensions.With regard to perception, in order to ground and react to language instructions, a robot must be able to divide (segment) a scene into relevant objects, and it must be able to understand and react to activity relative to those objects.In recent years, video instance segmentation (VIS) has been largely advanced by offline models, while online models gradually attracted less attention possibly due to their inferior performance. However, online methods are necessary to handle long, ongoing video sequences common in robotic interaction. We developed an online framework for VIAS based on contrastive learning that is able to learn more discriminative instance embeddings for association and fully exploit history information for stability. Despite its simplicity, our method outperforms all online and offline methods on three benchmarks. We developed complementary methods for motion estimation (optical flow) that take advantage of a new method for self-generated labels, and we also developed methods that are able to refine human poses over time through a unique method of constraining them to a dictionary of local pose manifolds. Finally, we developed an approach to fine-grained activity recognition that models activities as compositions ofdynamic action signatures. This compositional approach allows us to reframe fine-grained recognition aszero-shotactivity recognition, where a detector is composed on the fly from simple first-principles state machines supported by deep-learned components. We were able to show state of the art performance for activity recognition, and also to show that we can use off-the-shelf object detectors to recognize newly described activities in completely de-novo settings with no additional training.\n\n\nWith regard to robotic task performance, we note that modern reinforcement learning (RL) algorithms are not sample- efficient to train on multi-step tasks in complex domains, impeding their wider deployment in the real world. We address this problem by leveraging the insight that RL models trained to complete one set of tasks can be re-purposed to complete related tasks when given just a handful of demonstrations. Based upon this insight, we propose See-SPOT-Run (SSR), a new computational approach to robot learning that enables a robot to complete a variety of real robot tasks in novel problem domainswithouttask-specific training. Complementing this, we investigated ways to efficiently enable human operators to interact with robotic agents using natural language would allow non-experts to intuitively instruct these agents. We developed a novel transformer-based model which enables a user to guide a robot arm through a 3D multi-step manipulation task with natural language commands. Our system maps images and commands to masks over grasp or place locations, grounding the language directly in perceptual space. In a suite of block rearrangement tasks, we showeda that these masks can be combined with an existing manipulation framework without re-training, greatly improving learning efficiency. Our modular design allows us to leverage supervised and reinforcement learning, providing an easy interface for experimentation with different architectures and it completes block manipulation tasks with synthetic commands 530% more often than a comparable UNet-based baseline.\n\n\nTheBroader Impactof our work is primarily through two avenues. First, we have had the opportunity to train seven graduate students plus additional masters and undergraduate students during the course of this project. These students have gone on to both industry and academic positions, building on the work they did in this project. Second, the impact of vision-based robotics broadly is growing rapidly in the form of automated driving, the increasing use of vision-based robotics in manufacturing and in other warehouse applications. However, they still lack the flexibility and accuracy necessary to operate as everyday interactive assistants. Our methods have been published in leading conferences and will be available to support those use-cases, as well as providing a basis for advances in other future research efforts.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 01/13/2024\n\n\t\t\t\t\tSubmitted by: GregoryDHager\n"
 }
}