{
 "awd_id": "1830711",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Collaborative Research: Algorithmic and Computational Frontiers of MapReduce for Big Data Analysis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2018-01-01",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 114469.0,
 "awd_amount": 114469.0,
 "awd_min_amd_letter_date": "2018-03-22",
 "awd_max_amd_letter_date": "2018-03-22",
 "awd_abstract_narration": "Modern science and engineering heavily relies on processing massive data sets and the size of the data requires applications to run using distributed computing frameworks.  However, many existing methods essential to the applications are not easily adapted to work in distributed settings. This project aims to develop new efficient ways of processing large data sets in widely used distributed computing platforms. The project will reveal new methods for processing diverse and complex data sets of massive size and allow for various applications scale to large inputs. The work has the potential to fundamentally change algorithmic techniques used in distributed computing, helping to shape big data research, the computing industry, and the growing economy reliant on big data analysis. Research outcomes will be integrated with education by writing an extensive survey/tutorial on the core algorithmic ideas used in the new discoveries to make the ideas transparent to the algorithmic developers and practitioners. The PIs will make some of the discovered algorithmic ideas accessible even to undergraduate students, helping them get prepared to cope with algorithmic challenges in distributed computing for large data sets. Special efforts will be made to include women and minorities in advising and mentoring plans.\r\n\r\nThe main goal of the project is to find new ways of unlocking the underlying power of MapReduce, a popular distributed platform, through the development of new algorithmics. The developed algorithms should have provably strong guarantees and demonstrate the effectiveness via empirical experiments. Considering the increasing demand for large data analysis, establishing a solid theoretical MapReduce model and developing new algorithmic ideas will have the potential to establish faster and memory efficient algorithms for distributed computing. The PIs will consider a collection of carefully chosen problems to understand in the MapReduce setting that not only have strong connections to theoretical work but also have the potential for high impact in real world Big Data applications: Clustering, Distributed Dynamic Programming, and Limitations of MapReduce. This will be done in parallel with the attempt to better understand the currently accepted MapReduce models that have been developed and to perhaps further refine them to better connect models with practice.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Moseley",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin J Moseley",
   "pi_email_addr": "moseleyb85@gmail.com",
   "nsf_id": "000671246",
   "pi_start_date": "2018-03-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7934",
   "pgm_ref_txt": "PARAL/DISTRIBUTED ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 114469.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern science and engineering has recently been thriving by exploiting the knowledge hidden in abundant available. This advance further has accelerated the growth of the data, so called Big Data, and brought various challenges to be addressed to sustain the advance. One of the main challenges with processing Big Data is that the size of the data renders known sequential data processing algorithms ineffective. A key technology for analyzing large data sets is distributed computing technologies powered by data centers and parallel computing systems.<br /><br />Distributed computing holds the promise of allowing the processing of large data sets. To realize this promise, there is a critical need to develop algorithmic tools to solve various optimization problems making full use of the new technologies.&nbsp; In particular, distributed computing requires new algorithms and software to process data sets that are fundamentally different from the methods used today. The long term research goal of the community is to develop software and a theoretical foundation that will allow the processing of large data sets across science and engineering.<br /><br />This project focused on the development of new algorithmic tools that can be used to solve optimization problems in the distributed setting, in particular, for the popular software frameworks, such as MapReduce and Spark.&nbsp; The key outcomes of the project are summarized as follows.<br /><br />First, the project developed a distributed dynamic programming framework.&nbsp; Dynamic programming is one of the key software design techniques used in computer science and operations research.&nbsp; It is one of fundamental optimization methods to be included in undergraduate curriculums. This project showed a framework for implementing dynamic programs in the distributed setting and it is the first of its kind.&nbsp;&nbsp;<br /><br />Second, the project developed fast distributed clustering frameworks. Clustering is a key data analysis tool and is often cited as the most used software in industry. The project discovered systematic ways to implement distributed clustering, allowing for the understanding of larger data sets. The methods developed in this project have been adopted by industry and are included in one of the most popular data science tool kits, Spark's ML library.<br /><br />Third, the project gave theoretical lower bounds on computations in the distributed setting. This helps algorithm and software designers understand the limitations of distributed computing and what problems we cannot hope to solve. The limitations of such systems have been a difficult hurdle for the algorithms community and our work makes progress on this important problem.<br /><br />Finally, the work developed various techniques for optimizing the infrastructure of distributed computing systems. This includes better scheduling of distributed machines, improving cache performances and ensuring high system utilization.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/24/2019<br>\n\t\t\t\t\tModified by: Benjamin&nbsp;J&nbsp;Moseley</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModern science and engineering has recently been thriving by exploiting the knowledge hidden in abundant available. This advance further has accelerated the growth of the data, so called Big Data, and brought various challenges to be addressed to sustain the advance. One of the main challenges with processing Big Data is that the size of the data renders known sequential data processing algorithms ineffective. A key technology for analyzing large data sets is distributed computing technologies powered by data centers and parallel computing systems.\n\nDistributed computing holds the promise of allowing the processing of large data sets. To realize this promise, there is a critical need to develop algorithmic tools to solve various optimization problems making full use of the new technologies.  In particular, distributed computing requires new algorithms and software to process data sets that are fundamentally different from the methods used today. The long term research goal of the community is to develop software and a theoretical foundation that will allow the processing of large data sets across science and engineering.\n\nThis project focused on the development of new algorithmic tools that can be used to solve optimization problems in the distributed setting, in particular, for the popular software frameworks, such as MapReduce and Spark.  The key outcomes of the project are summarized as follows.\n\nFirst, the project developed a distributed dynamic programming framework.  Dynamic programming is one of the key software design techniques used in computer science and operations research.  It is one of fundamental optimization methods to be included in undergraduate curriculums. This project showed a framework for implementing dynamic programs in the distributed setting and it is the first of its kind.  \n\nSecond, the project developed fast distributed clustering frameworks. Clustering is a key data analysis tool and is often cited as the most used software in industry. The project discovered systematic ways to implement distributed clustering, allowing for the understanding of larger data sets. The methods developed in this project have been adopted by industry and are included in one of the most popular data science tool kits, Spark's ML library.\n\nThird, the project gave theoretical lower bounds on computations in the distributed setting. This helps algorithm and software designers understand the limitations of distributed computing and what problems we cannot hope to solve. The limitations of such systems have been a difficult hurdle for the algorithms community and our work makes progress on this important problem.\n\nFinally, the work developed various techniques for optimizing the infrastructure of distributed computing systems. This includes better scheduling of distributed machines, improving cache performances and ensuring high system utilization. \n\n\t\t\t\t\tLast Modified: 10/24/2019\n\n\t\t\t\t\tSubmitted by: Benjamin J Moseley"
 }
}