{
 "awd_id": "1826258",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Achieving Autonomy by Learning from Sensor-Assisted Control in a Wheelchair-Based Human-Robot Collaborative System",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922633",
 "po_email": "aleoness@nsf.gov",
 "po_sign_block_name": "Alex Leonessa",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 496383.0,
 "awd_amount": 592383.0,
 "awd_min_amd_letter_date": "2018-08-16",
 "awd_max_amd_letter_date": "2023-11-14",
 "awd_abstract_narration": "Individuals with sensorimotor impairment must often rely on others to help them perform common activities of daily living. The goal of this project is to improve independence and quality of life by creating an adaptive human-robot collaborative system (a wheelchair mounted robotic arm) that learns from example to assist its user perform instrumental activities in a way that requires minimal user guidance. The project has a novel intention recognition framework that learns user goals despite imprecision of telemanipulation cues provided by the user during object interactions. The project also implements and tests a novel form of shared control authority that adaptively allocates workload between the human and robot to optimally leverage the physical capabilities and cognitive resources of the user. By doing so, this project will improve the independence of individuals with sensorimotor impairment and increase the autonomy of a wheelchair mounted assistive robot, thereby advancing NSF's mission by promoting the progress of science and advancing the national health and welfare.  The project will involve an educational component that provides training to graduate students in conducting research. The project also develops a hands-on exhibit in collaboration with the Tampa Museum of Science and Industry, allowing visitors to explore the field of rehabilitation engineering.\r\n\r\nThis research investigates new control methodologies that promise improved autonomy in the control of a wheelchair mounted robotic arm operated by individuals with sensorimotor impairment.  The project addresses key steps in the dexterous telemanipulation of objects: object detection, classification, and affordance modeling; user intention estimation; user / robot workload distribution; and algorithm training through teleoperation. Methods include computer vision, machine learning, probabilistic graphical modeling, and human subject experimentation to develop and test the collaborative human / robot system during performance of activities such as opening/closing doors with pull and knob style handles, and fetching objects in an unstructured populated environment.  Effective application of the technology promises persons with physical disabilities opportunity to achieve a high level of independence, dignity, and quality of life.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rajiv",
   "pi_last_name": "Dubey",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rajiv Dubey",
   "pi_email_addr": "dubey@usf.edu",
   "nsf_id": "000256572",
   "pi_start_date": "2018-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sudeep",
   "pi_last_name": "Sarkar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sudeep Sarkar",
   "pi_email_addr": "sarkar@usf.edu",
   "nsf_id": "000285699",
   "pi_start_date": "2018-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Redwan",
   "pi_last_name": "Alqasemi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Redwan Alqasemi",
   "pi_email_addr": "ralqasemi@gmail.com",
   "nsf_id": "000558405",
   "pi_start_date": "2018-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kyle",
   "pi_last_name": "Reed",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Kyle B Reed",
   "pi_email_addr": "kylereed@usf.edu",
   "nsf_id": "000560399",
   "pi_start_date": "2018-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of South Florida",
  "inst_street_address": "4202 E FOWLER AVE",
  "inst_street_address_2": "",
  "inst_city_name": "TAMPA",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "8139742897",
  "inst_zip_code": "336205800",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "FL15",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTH FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NKAZLXLL7Z91"
 },
 "perf_inst": {
  "perf_inst_name": "University of South Florida",
  "perf_str_addr": "3702 Spectrum Blvd.",
  "perf_city_name": "Tampa",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "336129446",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "FL15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "058Y00",
   "pgm_ele_name": "M3X - Mind, Machine, and Motor"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "070E",
   "pgm_ref_txt": "INTEG OF HUMAN & COGNITIVE"
  },
  {
   "pgm_ref_code": "116E",
   "pgm_ref_txt": "RESEARCH EXP FOR UNDERGRADS"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9231",
   "pgm_ref_txt": "SUPPL FOR UNDERGRAD RES ASSIST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 496383.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of the proposed research is to create a human-robot collaborative system that learns from sensor-assisted teleoperation to reach the maximum possible autonomy requiring minimal user input. We created an intelligent control system with several algorithms to control the robotic arm mounted on a power wheelchair to make intelligent object identification for tasks related to activities of daily living (ADL) and perform intelligent grasping related to the tasks.</p>\n<p>&nbsp;</p>\n<p><strong>Research Activities:</strong></p>\n<p>1-&nbsp;&nbsp;&nbsp;&nbsp; In order to make it easier for human operators to control robotic arms, a control model was developed that aligns better with human perception of direction and movement. As such, a patent was granted describing a system for controlling robotic arms using a \"hybrid inverse kinematics\" model combined with an \"intuitive reference frame.\" Traditional control methods use either a ground-based reference frame or an end-effector reference frame, which can confuse operators because these frames don&rsquo;t match the operator&rsquo;s perspective, requiring additional cognitive effort. This hybrid model allows operators to perform complex tasks with robotic arms, such as lifting, rotating, and placing objects, by translating input commands from devices like joysticks into intuitive, easy-to-understand movements for the robot. This technology can be widely applied in areas requiring precise and intuitive robotic manipulation, like industrial settings, healthcare, and space exploration.</p>\n<p>&nbsp;</p>\n<p>2-&nbsp;&nbsp;&nbsp;&nbsp; To improve a user&rsquo;s ability to interact with and control a robot, it was opted to take advantage of the availability and versatility of modern-day smartphones, such as their ability to inform you of their orientation, to manipulate a robot for activities of the daily living. Three control interfaces were developed and compared for these activities and demonstrated promising results.</p>\n<p>&nbsp;</p>\n<p>3-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; An algorithm was developed to accurately determine the pose and orientation of an object presented in either a video or a series of images, even when parts of the object are obscured. This algorithm demonstrated up to 4% higher accuracy compared to similar methods. Recognizing an object&rsquo;s orientation enhances a robot&rsquo;s ability to interact with that object. Building on this method, a framework was created to improve a robot&rsquo;s ability to grasp, focusing on stability and reducing movements that can lead to drops or spills. While prior AI-based methods can identify potential grasp points, they often lacked precision, resulting in objects shifting or rotating upon lifting. The developed framework refines these AI-suggested grasp points, stabilizing the grasp and minimizing unwanted movement to prevent collisions.</p>\n<p>&nbsp;</p>\n<p>4-&nbsp;&nbsp;&nbsp;&nbsp; A simple, yet effective, task learning and planning algorithm was developed that allowed the robot to learn tasks from the user. As an example, if the robot is used for a task, such as making a bowl of cereal, the robot will learn and remember steps required in order to complete this task and would be able to repeat these actions again, even when the environment and the object placement have changed. Additionally, the robot would be able to combine smaller tasks in order to complete more complex tasks. Depending on whether the task has been previously completed or not, the robot may ask for help from the user in order to either structure a full task or to compensate for system failures, such as the inability to identify the objects in the environment. In the case of the former, the user may use voice commands to control the robot, and for the latter, it might be required for the user to point where the object is on a touch screen.</p>\n<p>&nbsp;</p>\n<p><strong>Education, Training and Development Opportunities and Outreach Activities:</strong></p>\n<p>1-&nbsp;&nbsp;&nbsp;&nbsp; An undergraduate Independent Study course has been offered for the past four semesters to engage students in research related to this project.</p>\n<p>2-&nbsp;&nbsp;&nbsp;&nbsp; Many graduate and undergraduate students were fully involved in learning and practicing in the knowledge gathered from the diverse activities associated with the design and implementation of the new methods and algorithms generated through this work. Some of these students graduated and continued working on similar research projects at their work site.</p>\n<p>3-&nbsp;&nbsp;&nbsp;&nbsp; Regular visits by prospective college students to induct students in science and technology program at USF have been performed regularly. There have been active participations by the lab members to showcased their work to elementary, middle, and high school students. This project has been demonstrated every year through USF&rsquo;s annual Engineering EXPO with participants from general public to college and elementary education students and families. Minorities and underprivileged groups were also included in this project through scheduled visits and lab tours. This includes girl scouts and special-interest groups.</p>\n<p>4-&nbsp;&nbsp;&nbsp;&nbsp; Several high school students spent their internship at our lab assisting our students on various aspects of this project.</p>\n<p>5-&nbsp;&nbsp;&nbsp;&nbsp; Presentations of this work have been made at many professional meetings and have been published in many peer-reviewed international journals and conferences.</p><br>\n<p>\n Last Modified: 11/07/2024<br>\nModified by: Redwan&nbsp;Alqasemi</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011050743_5--rgov-214x142.JPG\" original=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011050743_5--rgov-800width.JPG\" title=\"Figure 2\"><img src=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011050743_5--rgov-66x44.JPG\" alt=\"Figure 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Grasp refinement - the red and green rectangles display the initial and the refined grasp positions, respectively</div>\n<div class=\"imageCredit\">CARRT</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Redwan&nbsp;Alqasemi\n<div class=\"imageTitle\">Figure 2</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011340647_1--rgov-214x142.JPG\" original=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011340647_1--rgov-800width.JPG\" title=\"Figure 6\"><img src=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011340647_1--rgov-66x44.JPG\" alt=\"Figure 6\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Pouring water task - using smartphone interface</div>\n<div class=\"imageCredit\">CARRT</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Redwan&nbsp;Alqasemi\n<div class=\"imageTitle\">Figure 6</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731010940150_3--rgov-214x142.JPG\" original=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731010940150_3--rgov-800width.JPG\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731010940150_3--rgov-66x44.JPG\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Arm looking at an object from multiple views for pose estimation</div>\n<div class=\"imageCredit\">CARRT</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Redwan&nbsp;Alqasemi\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011272301_2--rgov-214x142.JPG\" original=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011272301_2--rgov-800width.JPG\" title=\"Figure 5\"><img src=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011272301_2--rgov-66x44.JPG\" alt=\"Figure 5\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Picking and placing dishes task - using smartphone interface</div>\n<div class=\"imageCredit\">CARRT</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Redwan&nbsp;Alqasemi\n<div class=\"imageTitle\">Figure 5</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011149476_4--rgov-214x142.JPG\" original=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011149476_4--rgov-800width.JPG\" title=\"Figure 3\"><img src=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011149476_4--rgov-66x44.JPG\" alt=\"Figure 3\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Grasp refinement - pictures (a) and (c) show the unrefined grasps, and pictures (b) and (d) show the refined grasps, respectively</div>\n<div class=\"imageCredit\">CARRT</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Redwan&nbsp;Alqasemi\n<div class=\"imageTitle\">Figure 3</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011215666_6--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011215666_6--rgov-800width.png\" title=\"Figure 4\"><img src=\"/por/images/Reports/POR/2024/1826258/1826258_10572039_1731011215666_6--rgov-66x44.png\" alt=\"Figure 4\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Pre-learning teleoperation using three smartphone interfaces</div>\n<div class=\"imageCredit\">CARRT</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Redwan&nbsp;Alqasemi\n<div class=\"imageTitle\">Figure 4</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of the proposed research is to create a human-robot collaborative system that learns from sensor-assisted teleoperation to reach the maximum possible autonomy requiring minimal user input. We created an intelligent control system with several algorithms to control the robotic arm mounted on a power wheelchair to make intelligent object identification for tasks related to activities of daily living (ADL) and perform intelligent grasping related to the tasks.\n\n\n\n\n\nResearch Activities:\n\n\n1- In order to make it easier for human operators to control robotic arms, a control model was developed that aligns better with human perception of direction and movement. As such, a patent was granted describing a system for controlling robotic arms using a \"hybrid inverse kinematics\" model combined with an \"intuitive reference frame.\" Traditional control methods use either a ground-based reference frame or an end-effector reference frame, which can confuse operators because these frames dont match the operators perspective, requiring additional cognitive effort. This hybrid model allows operators to perform complex tasks with robotic arms, such as lifting, rotating, and placing objects, by translating input commands from devices like joysticks into intuitive, easy-to-understand movements for the robot. This technology can be widely applied in areas requiring precise and intuitive robotic manipulation, like industrial settings, healthcare, and space exploration.\n\n\n\n\n\n2- To improve a users ability to interact with and control a robot, it was opted to take advantage of the availability and versatility of modern-day smartphones, such as their ability to inform you of their orientation, to manipulate a robot for activities of the daily living. Three control interfaces were developed and compared for these activities and demonstrated promising results.\n\n\n\n\n\n3- An algorithm was developed to accurately determine the pose and orientation of an object presented in either a video or a series of images, even when parts of the object are obscured. This algorithm demonstrated up to 4% higher accuracy compared to similar methods. Recognizing an objects orientation enhances a robots ability to interact with that object. Building on this method, a framework was created to improve a robots ability to grasp, focusing on stability and reducing movements that can lead to drops or spills. While prior AI-based methods can identify potential grasp points, they often lacked precision, resulting in objects shifting or rotating upon lifting. The developed framework refines these AI-suggested grasp points, stabilizing the grasp and minimizing unwanted movement to prevent collisions.\n\n\n\n\n\n4- A simple, yet effective, task learning and planning algorithm was developed that allowed the robot to learn tasks from the user. As an example, if the robot is used for a task, such as making a bowl of cereal, the robot will learn and remember steps required in order to complete this task and would be able to repeat these actions again, even when the environment and the object placement have changed. Additionally, the robot would be able to combine smaller tasks in order to complete more complex tasks. Depending on whether the task has been previously completed or not, the robot may ask for help from the user in order to either structure a full task or to compensate for system failures, such as the inability to identify the objects in the environment. In the case of the former, the user may use voice commands to control the robot, and for the latter, it might be required for the user to point where the object is on a touch screen.\n\n\n\n\n\nEducation, Training and Development Opportunities and Outreach Activities:\n\n\n1- An undergraduate Independent Study course has been offered for the past four semesters to engage students in research related to this project.\n\n\n2- Many graduate and undergraduate students were fully involved in learning and practicing in the knowledge gathered from the diverse activities associated with the design and implementation of the new methods and algorithms generated through this work. Some of these students graduated and continued working on similar research projects at their work site.\n\n\n3- Regular visits by prospective college students to induct students in science and technology program at USF have been performed regularly. There have been active participations by the lab members to showcased their work to elementary, middle, and high school students. This project has been demonstrated every year through USFs annual Engineering EXPO with participants from general public to college and elementary education students and families. Minorities and underprivileged groups were also included in this project through scheduled visits and lab tours. This includes girl scouts and special-interest groups.\n\n\n4- Several high school students spent their internship at our lab assisting our students on various aspects of this project.\n\n\n5- Presentations of this work have been made at many professional meetings and have been published in many peer-reviewed international journals and conferences.\t\t\t\t\tLast Modified: 11/07/2024\n\n\t\t\t\t\tSubmitted by: RedwanAlqasemi\n"
 }
}