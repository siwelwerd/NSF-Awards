{
 "awd_id": "1801495",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Medium: Collaborative: Towards Trustworthy Deep Neural Network Based AI: A Systems Approach",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032920000",
 "po_email": "doliveir@nsf.gov",
 "po_sign_block_name": "Daniela Oliveira",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 899990.0,
 "awd_amount": 899990.0,
 "awd_min_amd_letter_date": "2018-07-20",
 "awd_max_amd_letter_date": "2018-07-20",
 "awd_abstract_narration": "Artificial intelligence (AI) is poised to revolutionize the world in fields ranging from technology to medicine, physics and the social sciences. Yet as AI is deployed in these domains, recent work has shown that systems may be vulnerable to different types of attacks that cause them to misbehave; for instance, attacks that cause an AI system to recognize a stop sign as a speed-limit sign. The project seeks to develop methodologies for testing, verifying and debugging AI systems, with a specific focus on deep neural network (DNN)-based AI systems, to ensure their safety and security. \r\n\r\nThe intellectual merits of the proposed research are encompassed in four new software tools that will be developed: (1) DeepXplore, a tool for automated and systematic testing of DNNs that discovers erroneous behavior that might be either inadvertently or maliciously introduced; (2) BadNets, a framework that automatically generated DNNs with known and stealthy misbehaviours in order to stress-test DeepXplore; (3) SafetyNets; a low-overhead scheme for safe and verifiable execution of DNNs in the cloud; and (4) VisualBackProp; a visual debugging tool for DNNs. The synergistic use of these tools for secure deployment of an AI system for autonomous driving will be demonstrated.\r\n\r\nThe project outcomes will significantly improve the security and safety of AI systems and increase their deployment in safety- and security-critical settings, resulting in broad societal impact. The results of the project will be widely disseminated via publications, talks, open access code, and competitions hosted on sites such as Kaggle and NYU's annual Cyber-Security Awareness Week (CSAW). Furthermore, students from under-represented minority groups in science, technology, engineering and mathematics (STEM) will be actively recruited and mentored to be leaders in this critical area.  \r\n\r\nThe code for this project will be made publicly available via github.com. Preliminary code for the tools that will be developed is already hosted on this website, including DeepXplore (https://github.com/peikexin9/deepxplore) and BadNets (https://github.com/Kooscii/BadNets/). These repositories will be linked to from a homepage that describes the entire project. The project homepage will be hosted on wp.nyu.edu/mlsecproject.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Siddharth",
   "pi_last_name": "Garg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Siddharth Garg",
   "pi_email_addr": "sg175@nyu.edu",
   "nsf_id": "000680915",
   "pi_start_date": "2018-07-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Brendan",
   "pi_last_name": "Dolan-Gavitt",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brendan Dolan-Gavitt",
   "pi_email_addr": "bd52@nyu.edu",
   "nsf_id": "000701772",
   "pi_start_date": "2018-07-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anna",
   "pi_last_name": "Choromanska",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Anna E Choromanska",
   "pi_email_addr": "ac5455@nyu.edu",
   "nsf_id": "000736446",
   "pi_start_date": "2018-07-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "70 Washington Square S",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100121019",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 899990.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The projects goals were to secure AI against hackers who can, via even subtle modifications to the data on which AI is trained or its inputs, cause the AI to misbehave with potentially disastrous consequences. To this end, our project aimed to develop methods for the design of robust AI technologies to ensure that the AI behaves safely and securely, especially when deployed in applications that can impact human health and safety, for example, autonomous driving.</p>\n<p>The project has made significant progress in achieving thse goals. We have developed new ways of detecting and quarantining malicious inputs to deep networks, and demonstrated how these quarantined inputs can be used to repair vulnerable nets. We have also demonstrated new attack vectors and corresponding defenses on autonomous driving systems, for instance, how an attacker can deploy bildboards near traffic signs in a city and fool an autonomous driving system to incorrectly identify specific billboard ads as correlates for red/green traffic signals. In parallel, we stuidied the robustness of deep learning methods used in the domain of AI-enbabled chip design, and demonstrated new attacks and defenses.</p>\n<p>A number of PHD students have been trained via this project, three of whom have graduated (one of these three is a woman). We have conducted an annual AI/ML summer school for K-2 students, and help ML hacking competitions at NYU's Cyber-security Awareness Week.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/16/2023<br>\n\t\t\t\t\tModified by: Siddharth&nbsp;Garg</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe projects goals were to secure AI against hackers who can, via even subtle modifications to the data on which AI is trained or its inputs, cause the AI to misbehave with potentially disastrous consequences. To this end, our project aimed to develop methods for the design of robust AI technologies to ensure that the AI behaves safely and securely, especially when deployed in applications that can impact human health and safety, for example, autonomous driving.\n\nThe project has made significant progress in achieving thse goals. We have developed new ways of detecting and quarantining malicious inputs to deep networks, and demonstrated how these quarantined inputs can be used to repair vulnerable nets. We have also demonstrated new attack vectors and corresponding defenses on autonomous driving systems, for instance, how an attacker can deploy bildboards near traffic signs in a city and fool an autonomous driving system to incorrectly identify specific billboard ads as correlates for red/green traffic signals. In parallel, we stuidied the robustness of deep learning methods used in the domain of AI-enbabled chip design, and demonstrated new attacks and defenses.\n\nA number of PHD students have been trained via this project, three of whom have graduated (one of these three is a woman). We have conducted an annual AI/ML summer school for K-2 students, and help ML hacking competitions at NYU's Cyber-security Awareness Week.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/16/2023\n\n\t\t\t\t\tSubmitted by: Siddharth Garg"
 }
}