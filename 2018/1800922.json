{
 "awd_id": "1800922",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Augmented Reality Agents with Pervasive Awareness, Appearance and Abilities",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 262727.0,
 "awd_amount": 262727.0,
 "awd_min_amd_letter_date": "2018-08-15",
 "awd_max_amd_letter_date": "2018-08-15",
 "awd_abstract_narration": "Voice-based assistants that respond to commands of people can be thought of as virtual companions that are always standing by to play music, tell us the weather, turn the lights off, etc. While their powers to respond and act on our behalf are increasing, unlike real companions they are largely unaware of our presence, take little initiative, look like appliances rather than interaction partners, and have limited abilities to both respond to queries and to sense and control real objects around us. This project will develop Augmented Reality Agents (ARAs) to embody these voice-based assistants, making them more are aware of our appearance, emotions, and behaviors, giving dynamic visual representations that make us aware of their state and behaviors; and leveraging the growth in \"Internet of Things\" (IoT) infrastructure and devices to increase the breadth and depth of their awareness. Together, these advances will lead to more effective and accepted voice-based assistants, both in the home and beyond. Such ARAs have a number of potential applications, including healthcare (by increasing the realism of clinical simulation and training, or providing support for caregiving through remote communication and virtual companionship) and education (by being more engaging tutors or representing historically important individuals). \r\n \r\nTo develop embodied Augmented Reality Agents (ARAs) with pervasive contextual awareness, appearance, and abilities the researchers will undertake a program of research aimed at the nexus of concepts and technologies associated with Augmented Reality (AR), Intelligent Virtual Agents (IVA), and the Internet of Things (IoT). To maximize the expected knowledge outcomes the researchers have organized their plans into three categories. First, they will develop new understanding of and priorities for ARA awareness, appearance, and abilities in a manner that does not require, nor depend on, a specific technological realization of automated behaviors. Second, they will use off-the-shelf and custom components to realize pervasive ARA functionality, to facilitate formative experiments related to basic ARA behaviors, and develop domain-specific applications and experiments. Third, the researchers will use application-specific realizations to assess the potential usefulness related to companionship and two areas of healthcare training: pediatric patient simulators and wide-area team-based medical training. The healthcare-focused work will leverage relevant courses at UF and UCF, and UCF's NSF REU center on the Internet of Things to engage students beyond their core team, in meaningful research.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeremy",
   "pi_last_name": "Bailenson",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Jeremy N Bailenson",
   "pi_email_addr": "bailenso@stanford.edu",
   "nsf_id": "000158257",
   "pi_start_date": "2018-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943041212",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 262727.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><dl class=\"clearing\"><br /></dl><dl class=\"clearing\"><dd>\n<div class=\"tinyMCEContent\">\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">PI Bailenson&rsquo;s research was focused on understanding the efficacy of AR simulations with embodied agents, and to gain a better understanding of how nonverbal communication happens within a virtual space, including through the in-depth analysis of human motion tracking data. To assist with these goals, PI Bailenson supervised Stanford PhD students Mark R. Miller, Hanseul Jun, and other students who contributed to the design and coding of human subject studies that resulted in large datasets that revealed insights into nonverbal communication in virtual reality.&nbsp; In particular we studied how people interact with embodied agents, for example task performance, nonverbal gestures, and social attitudes.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Extending beyond the headset, PI Bailenson also spearheaded research into understanding nonverbal cues and human fatigue while using video conferencing platforms, which are rapidly becoming ubiquitous tools for education, health, and workforce communication. The results of these studies showed ARAs as a possible solution to &ldquo;Zoom fatigue.&rdquo;&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">PI Bailenson also assisted with the studies that observed the efficacy of non-human augmented reality companions, conducted with collaborators at University of Central Florida.</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\">&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The objective of PI Bailenson&rsquo;s research was to gain in-depth understanding of human interactions with virtual agents and avatars. Before widespread use of ARAs it is necessary to have a firm grasp on the implications of interacting with virtual agents. This included examining specific media, such as augmented and virtual reality to understand how embodied agents are treated by humans.&nbsp; However, it also involved examining the basic processes behind how people interact with mediated humans, for example, perceptual features involved in videoconferencing.&nbsp; In general our work was to build technology, run experiments on how people interact with that technology, and to use those findings from studies to iterate on design of human/agent systems.</span></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\">&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Video conference platforms have been a crucial communication technology to maintain social connections during the COVID-19 pandemic. It is possible that video conferencing will keep playing an important role in interpersonal communication in the post-pandemic age. The rise of any new communication tool comes with challenges that need to be understood and mitigated in order to maximize the benefit drawn from its use. Similarly to other media, video conferencing can trigger fatigue that may be experienced by many but whose causes and antecedents are yet to be uncovered. The research on Zoom fatigue supported by this grant provides a first step in this direction by providing a short questionnaire that can now be used widely by researchers to measure Zoom fatigue. Understanding the social impacts of new media technology, such as video conferencing, and its effects on human health will help future researchers gain insights into how ARAs can help improve upon existing communication technologies.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In that same vein, this grant has also sponsored research on the use of AR for social use. Imagine a cocktail party, with dozens of people socializing, about half of whom are wearing AR headsets. Perhaps some people will choose to invite other virtual avatars into the party, and these can only be seen by those wearing headsets. The presence of avatars might change the way people talk, gesture, and socialize. For example, what happens when an AR user at the party is forced to violate the personal space of either an avatar who is registered in a specific location or a real human who is not aware of the avatar? While similar issues occur with phone calls and videoconferencing, the unique aspect of AR is that these avatars are grounded spatially in the room in a set position among the physical people. We expected virtual objects and the AR device itself to hinder communication as they prevented eye-contact and disrupted common ground between people. Results from this study showed that participants wearing headsets felt significantly less connected to their partners and rated their non-headset partner with significantly lower social presence, even though both interactants were in the same physical room when compared to their partners who were not wearing the headset.&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\">&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The grant includes many studies on the efficacy of AR simulations with embodied agents.&nbsp; In order to understand these interactions, we need to develop new tools to associate head movements with various outcomes related to interaction, and these three papers provide innovation on the tools to use head movements as a proxy for mental states, individual differences, and outcomes.</span></span></p>\n</div>\n</dd></dl><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/14/2022<br>\n\t\t\t\t\tModified by: Jeremy&nbsp;N&nbsp;Bailenson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\nPI Bailenson\u2019s research was focused on understanding the efficacy of AR simulations with embodied agents, and to gain a better understanding of how nonverbal communication happens within a virtual space, including through the in-depth analysis of human motion tracking data. To assist with these goals, PI Bailenson supervised Stanford PhD students Mark R. Miller, Hanseul Jun, and other students who contributed to the design and coding of human subject studies that resulted in large datasets that revealed insights into nonverbal communication in virtual reality.  In particular we studied how people interact with embodied agents, for example task performance, nonverbal gestures, and social attitudes.\n\n \nExtending beyond the headset, PI Bailenson also spearheaded research into understanding nonverbal cues and human fatigue while using video conferencing platforms, which are rapidly becoming ubiquitous tools for education, health, and workforce communication. The results of these studies showed ARAs as a possible solution to \"Zoom fatigue.\" \n\n \nPI Bailenson also assisted with the studies that observed the efficacy of non-human augmented reality companions, conducted with collaborators at University of Central Florida.\n \nThe objective of PI Bailenson\u2019s research was to gain in-depth understanding of human interactions with virtual agents and avatars. Before widespread use of ARAs it is necessary to have a firm grasp on the implications of interacting with virtual agents. This included examining specific media, such as augmented and virtual reality to understand how embodied agents are treated by humans.  However, it also involved examining the basic processes behind how people interact with mediated humans, for example, perceptual features involved in videoconferencing.  In general our work was to build technology, run experiments on how people interact with that technology, and to use those findings from studies to iterate on design of human/agent systems.\n \nVideo conference platforms have been a crucial communication technology to maintain social connections during the COVID-19 pandemic. It is possible that video conferencing will keep playing an important role in interpersonal communication in the post-pandemic age. The rise of any new communication tool comes with challenges that need to be understood and mitigated in order to maximize the benefit drawn from its use. Similarly to other media, video conferencing can trigger fatigue that may be experienced by many but whose causes and antecedents are yet to be uncovered. The research on Zoom fatigue supported by this grant provides a first step in this direction by providing a short questionnaire that can now be used widely by researchers to measure Zoom fatigue. Understanding the social impacts of new media technology, such as video conferencing, and its effects on human health will help future researchers gain insights into how ARAs can help improve upon existing communication technologies. \n\n \nIn that same vein, this grant has also sponsored research on the use of AR for social use. Imagine a cocktail party, with dozens of people socializing, about half of whom are wearing AR headsets. Perhaps some people will choose to invite other virtual avatars into the party, and these can only be seen by those wearing headsets. The presence of avatars might change the way people talk, gesture, and socialize. For example, what happens when an AR user at the party is forced to violate the personal space of either an avatar who is registered in a specific location or a real human who is not aware of the avatar? While similar issues occur with phone calls and videoconferencing, the unique aspect of AR is that these avatars are grounded spatially in the room in a set position among the physical people. We expected virtual objects and the AR device itself to hinder communication as they prevented eye-contact and disrupted common ground between people. Results from this study showed that participants wearing headsets felt significantly less connected to their partners and rated their non-headset partner with significantly lower social presence, even though both interactants were in the same physical room when compared to their partners who were not wearing the headset. \n \nThe grant includes many studies on the efficacy of AR simulations with embodied agents.  In order to understand these interactions, we need to develop new tools to associate head movements with various outcomes related to interaction, and these three papers provide innovation on the tools to use head movements as a proxy for mental states, individual differences, and outcomes.\n\n\n\n\t\t\t\t\tLast Modified: 11/14/2022\n\n\t\t\t\t\tSubmitted by: Jeremy N Bailenson"
 }
}