{
 "awd_id": "1763199",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Wearable Sound Sensing and Feedback Techniques for Persons who are Deaf or Hard of Hearing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 899994.0,
 "awd_amount": 915994.0,
 "awd_min_amd_letter_date": "2018-07-21",
 "awd_max_amd_letter_date": "2019-04-18",
 "awd_abstract_narration": "The goal of this research is to advance wearable sound sensing and feedback technology for users who are deaf or hard of hearing, along with appropriate techniques for human-computer interaction.  A wearable device with multiple microphones and audio processing algorithms to automatically sense, localize, and identify sounds will be developed.  Means to discreetly provide this information to the user via emerging wearable technologies such as head-mounted displays and smartwatches will also be implemented.  Evaluations will include lab and field studies, everyday tasks such as noticing sounds and participating in oral conversations, and objective and subjective measures. Project outcomes will have broad impact by enabling new sound awareness options for users who are deaf or hard of hearing, thereby augmenting the wearer's existing strategies with additional, unobtrusive information. The new assistive technologies will have the potential to improve the lives of a large portion of the population, in particular the growing number of older adults with hearing loss in the United States.\r\n\r\nTo these ends, major subgoals will include: understanding user needs for wearable sound sensing and feedback, including prioritizing the importance of different sounds across a variety of contexts and based on an individual user's level of hearing loss; developing and evaluating a lightweight wearable sound sensing platform and accompanying algorithms, including both new sound scene analysis algorithms for a microphone array conformal on, or in proximity to, a complex-shaped baffle (the wearer's head) and that take into account how sound scatters off the wearer's body, along with adaptive state-of-the-art sound classification and speech recognition approaches to work with this processed audio; and developing and evaluating visual and haptic or vibrational feedback of the sensed sound via wearable prototypes.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Leah",
   "pi_last_name": "Findlater",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Leah Findlater",
   "pi_email_addr": "leahkf@uw.edu",
   "nsf_id": "000636876",
   "pi_start_date": "2018-07-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jon",
   "pi_last_name": "Froehlich",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jon Froehlich",
   "pi_email_addr": "jonf@cs.washington.edu",
   "nsf_id": "000630231",
   "pi_start_date": "2018-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 899994.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-e8f73da3-7fff-d0be-c423-898ce0171822\">\r\n<p dir=\"ltr\"><span>This project focused on advancing sound sensing and feedback methods for users who are Deaf or Hard of Hearing. Using novel sound processing algorithms and mobile and wearable displays, the project aimed to provide always available, unobtrusive, and glanceable support to augment the user&rsquo;s existing sound awareness strategies.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>The intellectual merit contributions across the three sites in this collaborative project included published papers at top venues in human-computer interaction and accessibility (e.g., CHI, ASSETS, CSCW) and sound processing (e.g., ICASSP). Outcomes included:</span></p>\r\n<ol>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>An understanding of user needs and barriers for sound sensing and feedback across contexts such as social settings, while mobile, or at home, and across technology platforms such as head-worn displays, smartwatches, smartphones, and video conferencing.&nbsp;</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>New wearable and mobile sound sensing technologies and algorithms to recognize sounds on a variety of platforms (e.g., watch, smarthome, mobile), including approaches to allow end users themselves to personalize what sounds the system recognizes.</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Findings from lab-based user evaluations and field studies with participants who are Deaf or Hard of Hearing.</span></p>\r\n</li>\r\n</ol>\r\n<p dir=\"ltr\"><span>In terms of broader impacts, the effort included release of an Android smartwatch app and findings have been shared with industry stakeholders who are working on similar products. Three PhD dissertations were partially funded by this grant, focusing respectively on building and evaluating novel sound recognition systems in the lab and field, designing new technologies to allow Deaf and Hard of Hearing users to personalize how their sound recognition tool works, and understanding and designing for social and other contextual factors affecting speech captioning use.</span></p>\r\n</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/17/2024<br>\nModified by: Leah&nbsp;Findlater</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\r\n\n\nThis project focused on advancing sound sensing and feedback methods for users who are Deaf or Hard of Hearing. Using novel sound processing algorithms and mobile and wearable displays, the project aimed to provide always available, unobtrusive, and glanceable support to augment the users existing sound awareness strategies.\r\n\n\nThe intellectual merit contributions across the three sites in this collaborative project included published papers at top venues in human-computer interaction and accessibility (e.g., CHI, ASSETS, CSCW) and sound processing (e.g., ICASSP). Outcomes included:\r\n\r\n\r\n\n\nAn understanding of user needs and barriers for sound sensing and feedback across contexts such as social settings, while mobile, or at home, and across technology platforms such as head-worn displays, smartwatches, smartphones, and video conferencing.\r\n\r\n\r\n\n\nNew wearable and mobile sound sensing technologies and algorithms to recognize sounds on a variety of platforms (e.g., watch, smarthome, mobile), including approaches to allow end users themselves to personalize what sounds the system recognizes.\r\n\r\n\r\n\n\nFindings from lab-based user evaluations and field studies with participants who are Deaf or Hard of Hearing.\r\n\r\n\r\n\n\nIn terms of broader impacts, the effort included release of an Android smartwatch app and findings have been shared with industry stakeholders who are working on similar products. Three PhD dissertations were partially funded by this grant, focusing respectively on building and evaluating novel sound recognition systems in the lab and field, designing new technologies to allow Deaf and Hard of Hearing users to personalize how their sound recognition tool works, and understanding and designing for social and other contextual factors affecting speech captioning use.\r\n\r\n\n\n\t\t\t\t\tLast Modified: 12/17/2024\n\n\t\t\t\t\tSubmitted by: LeahFindlater\n"
 }
}