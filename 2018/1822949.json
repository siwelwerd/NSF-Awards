{
 "awd_id": "1822949",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: Collaborative Research: Rethinking Data Center Abstractions Utilizing Warehouse-Scale Shared Memory",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-08-30",
 "awd_max_amd_letter_date": "2018-08-30",
 "awd_abstract_narration": "Warehouse-scale computers (or data centers) are essential for the technology that people rely on every day: from making purchases, to hailing rides, to sharing experiences with friends.  Programming warehouse-scale computers requires specialized software that coordinates the many individual computers that make up the data center, while ensuring that the system will continue to operate if any machine fails.  Unfortunately, this data center software has fundamental differences from that developed for single computers (i.e., that is taught to most computer science students) resulting in long development times and poor performance.  The proposed work will bridge the gap between the software systems used in current data centers and what is available to most programmers (and computing students). The proposed work will allow individual computers within a data center to communicate through \"shared memory\"---the same mechanism used within small-scale computers from phones to laptops to individual servers. The project has the potential to make warehouse scale computing much more accessible to everyone. In particular, it will allow an easy transition for software that runs on common machines (laptops, desktops) to the datacenter. Additionally, the project will create many educational opportunities through enhanced classroom projects and creation of research opportunities for undergraduates.\r\n\r\nThe project's distinguishing feature is a holistic design of new computing hardware and operating systems to allow this \"shared memory\" abstraction to provide both the scale and failure tolerance of specialized data center software.  While prior hardware takes an approach of \"share all\" or \"share nothing\", the proposed hardware will allow subsets of data to be shared across subsets of hardware.  Then, the operating system will be extended to automatically manage the access to this shared memory, so that programmers do not need to be aware of the difference.  By coordinating software and hardware management, the project will overcome prior scalability and failure tolerance challenges of sharing memory. It will also allow easy sharing of datacenter resources, preventing fragmentation and reducing the cost of using datacenters and cloud computing.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Wentzlaff",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "David M Wentzlaff",
   "pi_email_addr": "wentzlaf@princeton.edu",
   "nsf_id": "000602658",
   "pi_start_date": "2018-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "87 Prospect Avenue, 2nd floor",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This work has studied how computer architecture can be enhanced for the data center and how machine learning can be used to accelerate computing in the data center.&nbsp;&nbsp;This work has particularly focused on the memory system of computers used in the data center.&nbsp;&nbsp;By optimizing computer architectures and memory systems for the data center, many of the computations that occur in the data center can potentially be accelerated thereby reducing the time humanity spends waiting for results, can reduce the cost of the computation, and the energy used for the computation.&nbsp;&nbsp;As part of this project, we studied how the increasing memory bandwidth both on a single chip (or system in package) and in the data center can be leveraged to increase performance.&nbsp;&nbsp;We found that in many cases there is additional, underutilized bandwidth in the memory system that can be used to increase performance.&nbsp;&nbsp;If trends continue, this underutilized bandwidth may even continue to increase.&nbsp;&nbsp;By moving from cache coherence protocols that invalidate the data in other caches to ones that update the data in the caches of other processors, in many circumstances, the unused/underutilized available bandwidth can be leveraged to increase performance.&nbsp;&nbsp;This work has also studied how computer architectures and computer systems can leverage machine learning to build practical hardware and software systems to improve performance and this work has studied how to create better functions to decide how to prune neural networks.</p>\n<p>In this work, we have also investigated how computer architectures can be optimized for Function as a Service (FaaS) workloads.&nbsp;&nbsp;Function as a Service are emerging cloud computing applications that execute in the data center where users pay on a per function execution basis.&nbsp;&nbsp;We have explored how to optimize computer architectures for this emerging set of workloads and found that there is potential to optimizing data prefetching and branch prediction hardware for these applications.&nbsp;&nbsp;In addition, we created a simulator which enables computer architecture research in FaaS which has traditionally been challenging with prior simulators due to the complex and large software stacks used in FaaS environments.</p>\n<p>This work has had broader impacts by funding students to travel to top conferences and present their work and discuss their work with top researchers in the field of computer architecture.&nbsp;&nbsp;This work can also have broader impact by leading to more efficient computing systems in the data center.&nbsp;&nbsp;By having more efficient computer systems, humanity can answer computational questions quicker and potentially save energy.&nbsp;&nbsp;This work has also influenced the classroom through the integration of ideas from this work into a graduate course at Princeton University.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/06/2024<br>\nModified by: David&nbsp;M&nbsp;Wentzlaff</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis work has studied how computer architecture can be enhanced for the data center and how machine learning can be used to accelerate computing in the data center.This work has particularly focused on the memory system of computers used in the data center.By optimizing computer architectures and memory systems for the data center, many of the computations that occur in the data center can potentially be accelerated thereby reducing the time humanity spends waiting for results, can reduce the cost of the computation, and the energy used for the computation.As part of this project, we studied how the increasing memory bandwidth both on a single chip (or system in package) and in the data center can be leveraged to increase performance.We found that in many cases there is additional, underutilized bandwidth in the memory system that can be used to increase performance.If trends continue, this underutilized bandwidth may even continue to increase.By moving from cache coherence protocols that invalidate the data in other caches to ones that update the data in the caches of other processors, in many circumstances, the unused/underutilized available bandwidth can be leveraged to increase performance.This work has also studied how computer architectures and computer systems can leverage machine learning to build practical hardware and software systems to improve performance and this work has studied how to create better functions to decide how to prune neural networks.\n\n\nIn this work, we have also investigated how computer architectures can be optimized for Function as a Service (FaaS) workloads.Function as a Service are emerging cloud computing applications that execute in the data center where users pay on a per function execution basis.We have explored how to optimize computer architectures for this emerging set of workloads and found that there is potential to optimizing data prefetching and branch prediction hardware for these applications.In addition, we created a simulator which enables computer architecture research in FaaS which has traditionally been challenging with prior simulators due to the complex and large software stacks used in FaaS environments.\n\n\nThis work has had broader impacts by funding students to travel to top conferences and present their work and discuss their work with top researchers in the field of computer architecture.This work can also have broader impact by leading to more efficient computing systems in the data center.By having more efficient computer systems, humanity can answer computational questions quicker and potentially save energy.This work has also influenced the classroom through the integration of ideas from this work into a graduate course at Princeton University.\n\n\n\t\t\t\t\tLast Modified: 03/06/2024\n\n\t\t\t\t\tSubmitted by: DavidMWentzlaff\n"
 }
}