{
 "awd_id": "1822809",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: Collaborative Research: Moving Towards Secure and Massive Parallel Computing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2020-09-30",
 "tot_intn_awd_amt": 68331.0,
 "awd_amount": 68331.0,
 "awd_min_amd_letter_date": "2018-08-29",
 "awd_max_amd_letter_date": "2018-08-29",
 "awd_abstract_narration": "Modern computing systems have moved beyond single-core, single-processor devices to more modern multi-core parallel processors operating in networked systems and available in warehouse-scale clouds popularized by industries and the government. This new parallel, interconnected, big-data world requires fundamental research on multiple levels from algorithms to systems and computer architecture. This project seeks to take initial steps in the study of the expansive set of algorithms and systems issues in this important research challenge by building and developing new general frameworks for massive parallel computation, often involving privacy and security, in real-life scenarios. \r\n\r\nThe investigators? long-term goals include two directions.  As the first thrust of this effort, the investigators aim to design fundamental and efficient algorithms for massive parallel computations in the practical MapReduce framework, in particular by reducing the number of rounds in this framework. As the second thrust of this effort, the investigators aim to augment current parallel environments and architectures with better data structures and abstractions to develop simplified and fast implementations of fundamental algorithms such that everyone can use them in practice.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Clifford",
   "pi_last_name": "Stein",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Clifford S Stein",
   "pi_email_addr": "cliff@ieor.columbia.edu",
   "nsf_id": "000193678",
   "pi_start_date": "2018-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "2960 Broadway",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 68331.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The scale of modern datasets requires that the computation is done in a more structured/restricted settings than on a standard single computer. One major such setting is the massive parallel computing model (MPC), which models cluster computing systems such as the influential MapReduce or many systems that followed, where a computational task is performed collaboratively by a number of machines.&nbsp; &nbsp;</p>\n<p>&nbsp;</p>\n<p>We study algorithms for massive graphs in this setting.&nbsp; The key point in designing algorithms for massive graphs is that each machine only \"sees\" a small fraction of the data.&nbsp; Thus, the communication between machines, an operation that is orders-of-magnitude slower than local computation, becomes the bottleneck.&nbsp; We have to design algorithms that are nothing like their sequential counterparts, and must work to minimize communication and total storage, in addition to time.</p>\n<p>&nbsp;</p>\n<p>In this work, we make significant progress on several important graph problems.&nbsp; These include</p>\n<p>- connectivity : figuring out if a graph is connected, that is can you reach every node from every other node</p>\n<p>- biconnectivity : finding components in a graph that are biconnected.&nbsp; Each pair of nodes in a biconnected component has two distinct paths between them.</p>\n<p>- matching:&nbsp; figuring out an optimal way to pair up nodes, or to assign entities from one set of objects to another</p>\n<p>- shortest paths : figuring out the path of least total distance between two points.</p>\n<p>&nbsp;</p>\n<p>For each of these problems, we advance the state of the art in MPC algorithms, and for several answer long-standing open questions about the complexity of such computations.</p>\n<p>&nbsp;</p>\n<p>The techniques introduced in the course of this work will have broader impacts thanjust the results described here.&nbsp; The techniques will be used for other related problems and also can be applied to other models.&nbsp; For example, our work on matching also has direct relevance to the streaming model of computing, and we suspect that other results will have similar broader reach.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/02/2021<br>\n\t\t\t\t\tModified by: Clifford&nbsp;S&nbsp;Stein</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe scale of modern datasets requires that the computation is done in a more structured/restricted settings than on a standard single computer. One major such setting is the massive parallel computing model (MPC), which models cluster computing systems such as the influential MapReduce or many systems that followed, where a computational task is performed collaboratively by a number of machines.   \n\n \n\nWe study algorithms for massive graphs in this setting.  The key point in designing algorithms for massive graphs is that each machine only \"sees\" a small fraction of the data.  Thus, the communication between machines, an operation that is orders-of-magnitude slower than local computation, becomes the bottleneck.  We have to design algorithms that are nothing like their sequential counterparts, and must work to minimize communication and total storage, in addition to time.\n\n \n\nIn this work, we make significant progress on several important graph problems.  These include\n\n- connectivity : figuring out if a graph is connected, that is can you reach every node from every other node\n\n- biconnectivity : finding components in a graph that are biconnected.  Each pair of nodes in a biconnected component has two distinct paths between them.\n\n- matching:  figuring out an optimal way to pair up nodes, or to assign entities from one set of objects to another\n\n- shortest paths : figuring out the path of least total distance between two points.\n\n \n\nFor each of these problems, we advance the state of the art in MPC algorithms, and for several answer long-standing open questions about the complexity of such computations.\n\n \n\nThe techniques introduced in the course of this work will have broader impacts thanjust the results described here.  The techniques will be used for other related problems and also can be applied to other models.  For example, our work on matching also has direct relevance to the streaming model of computing, and we suspect that other results will have similar broader reach.\n\n\t\t\t\t\tLast Modified: 05/02/2021\n\n\t\t\t\t\tSubmitted by: Clifford S Stein"
 }
}