{
 "awd_id": "1816010",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Extracting affect and interaction information from primary care visits to support patient-provider interactions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922568",
 "po_email": "wnilsen@nsf.gov",
 "po_sign_block_name": "Wendy Nilsen",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2018-08-08",
 "awd_max_amd_letter_date": "2022-09-12",
 "awd_abstract_narration": "Primary care is an integral part of an effective health care system that emphasizes continuous and preventive care. However, the United States faces a shortage of primary care physicians that is projected to get worse, a shortage most pronounced in clinics that serve rural and minority patients. This shortage can lead to stress and physician burnout, affecting both the shortage itself and the quality of patient-provider interactions. This project will develop methods for semi-automated analysis of primary care visits, with the long-term goal of detecting both momentary and longer-term stress, then providing feedback and reflection systems to help reduce stress and improve the quality of patient care and retention of physicians. This will require both advances in video processing to recognize aspects of how humans interact with both each other and with technology, novel models of how these interactions affect the group's affective state and relationship, and initial design inquiry into patients' and providers' needs during primary care visits. This, in turn, will require providing interdisciplinary educational opportunities for health service and engineering students. \r\n\r\nThe project has two main aims. The first focuses on developing methods to extract data from clinical encounters about stress, burnout, and interaction, leveraging a large existing dataset of recorded primary care visits collected in several contexts. The team will develop techniques to recognize both non-verbal and verbal cues in interactions, including eye contact, facial expressions, posture and body language, turn-taking behaviors, and indicators of socio-emotional exchange such as facial mimicry. The team will also extract cues about how the people involved interact with technology through gaze analysis, evidence of typing or touchscreen use, and screen sharing/co-referencing behaviors. These methods will be validated by comparing their performance against human annotations, with the extracted data informing the second main aim around determining opportunities and requirements for developing tools to provide feedback and support reflection. This second aim will be pursued through user-centered design methods, including reviewing extracted data with clinical collaborators to evaluate its potential value, workflow analysis of patient-provider interactions informed by ethnographic observational methods, and thematic analysis of focus groups including both patients and care providers.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jacob",
   "pi_last_name": "Furst",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Jacob D Furst",
   "pi_email_addr": "jfurst@cdm.depaul.edu",
   "nsf_id": "000176791",
   "pi_start_date": "2022-06-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Enid",
   "pi_last_name": "Montague",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Enid N Montague",
   "pi_email_addr": "emontag1@depaul.edu",
   "nsf_id": "000516740",
   "pi_start_date": "2018-08-08",
   "pi_end_date": "2022-06-07"
  }
 ],
 "inst": {
  "inst_name": "DePaul University",
  "inst_street_address": "1 E JACKSON BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3123627388",
  "inst_zip_code": "606042287",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "IL07",
  "org_lgl_bus_name": "DEPAUL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MNZ8KMRWTDB6"
 },
 "perf_inst": {
  "perf_inst_name": "DePaul University",
  "perf_str_addr": "243 S Wabash Ave, Chicago, IL 60",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606042287",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "IL07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The \"Extracing affect and interaction infromation from primary care visits to support patient-provider interactions\" supported students and faculty to investigate what we could learn from recording interactions between patients and physicians.&nbsp; There were two classes of outcomes: one regarding the design of systems and spaces to support better interactions, and one regarding what could be learned automatically from the video recordings.&nbsp; The main outcomes have been published in 9 conference and journal articles and two master's theses.&nbsp; In summary, we discovered that 1) both human and computer analyses are extremely hard to perform in the absence of a standardized method of recording,&nbsp; 2) human analysis is best when focused on evaluating large scale trends of entire videos, 3) computer analysis is best when focused on analysis of short video segments and 4) computer vision strategies can do an excellent job of evaluating the target of eye gaze for patients and physicians.&nbsp; While the first three are important findings for planning video recording strategies for any kind of human interaction, could be helpful in providing realtime feedback to physicians about whether they are looking at patients frequently or infrequently during a visit.&nbsp; As eye gaze is believed to be important in health outcomes, this could improve patient healthcare.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/24/2023<br>\n\t\t\t\t\tModified by: Jacob&nbsp;D&nbsp;Furst</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe \"Extracing affect and interaction infromation from primary care visits to support patient-provider interactions\" supported students and faculty to investigate what we could learn from recording interactions between patients and physicians.  There were two classes of outcomes: one regarding the design of systems and spaces to support better interactions, and one regarding what could be learned automatically from the video recordings.  The main outcomes have been published in 9 conference and journal articles and two master's theses.  In summary, we discovered that 1) both human and computer analyses are extremely hard to perform in the absence of a standardized method of recording,  2) human analysis is best when focused on evaluating large scale trends of entire videos, 3) computer analysis is best when focused on analysis of short video segments and 4) computer vision strategies can do an excellent job of evaluating the target of eye gaze for patients and physicians.  While the first three are important findings for planning video recording strategies for any kind of human interaction, could be helpful in providing realtime feedback to physicians about whether they are looking at patients frequently or infrequently during a visit.  As eye gaze is believed to be important in health outcomes, this could improve patient healthcare.\n\n\t\t\t\t\tLast Modified: 04/24/2023\n\n\t\t\t\t\tSubmitted by: Jacob D Furst"
 }
}