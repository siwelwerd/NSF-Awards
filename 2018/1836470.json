{
 "awd_id": "1836470",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Defining Almost Correct: Quantifying Student Understanding Hidden in Wrong Answers",
 "cfda_num": "47.076",
 "org_code": "11040000",
 "po_phone": "7032924625",
 "po_email": "chovis@nsf.gov",
 "po_sign_block_name": "R. Corby Hovis",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 206876.0,
 "awd_amount": 206876.0,
 "awd_min_amd_letter_date": "2018-08-13",
 "awd_max_amd_letter_date": "2023-09-11",
 "awd_abstract_narration": "This project aims to transform the way faculty assess student learning in undergraduate physics courses. Quantitative assessments of student learning in physics have generally focused on whether a student got the \"right\" answer on a multiple-choice exam. This kind of analysis fails to capture how close a student's understanding is to being \"right.\" It therefore cannot track whether or how students' understanding improves or progresses.  This project will develop sophisticated scoring methods for multiple-choice tests that can reveal students' productive, but \"wrong\" ideas. This method should result in better-informed and more equitable decisions regarding instructional practices. For instance, this kind of analysis could determine if students have improved their understanding after instruction, even if they still do not get the right answer. Such information could be particularly helpful with less-prepared students who have further to go in their learning process. Since a disproportionate number of these students are from groups that are under-represented in physics, using a simple right/wrong analysis treats these groups inequitably. More fully representing student learning is vitally important for making better decisions regarding instructional practices. This project will use two kinds of analysis to determine which wrong answers from a set of choices are better than others. The first process is a statistical analysis that determines the most common sequence of wrong answers students traverse before arriving at the correct answer. The second process involves qualitative interviews to uncover student thinking about the reasons for their choices.\r\n\r\nBetter understanding the progression of student learning across multiple disciplines should help develop a larger and more inclusive STEM work force to meet the growing needs of the U.S. economy. This project aims to meet the critical need for a more complete scoring metric for research-based assessment instruments, by developing a new assessment tool to measure student learning in novel ways. The new approach is based on data from a commonly used research-based assessment instruments in physics, the Force and Motion Conceptual Evaluation. This project will begin by developing a ranking of incorrect responses to each question on the Force and Motion Conceptual Evaluation, based on quantitative analyses of student responses. Next, it will reconcile rankings from multiple analyses to generate a unified ranking for each question. From this unified ranking, the project will define a metric to represent overall student knowledge. Then, the project will develop a user-friendly assessment tool (software) to analyze student response data and calculate the new learning metric. Next, by applying the new assessment tool to existing data, the project will identify patterns in student response progressions that differ based on instructional factors or student demographics. Finally, the project will connect the unified rankings to the learning progressions literature by interviewing students about why they choose various responses. The outcome will be a new assessment tool that should allow researchers and instructors to more deeply analyze data about their students' learning. This knowledge could lead to more informed decisions regarding instructional choices. The results of this project may be applied to any multiple-choice, research-based assessment instruments in any discipline. Thus, the project has the potential to significantly improve the ways in which data about learning are interpreted in many different contexts.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Trevor",
   "pi_last_name": "Smith",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Trevor I Smith",
   "pi_email_addr": "smithtr@rowan.edu",
   "nsf_id": "000692496",
   "pi_start_date": "2018-08-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Nasrine",
   "pi_last_name": "Bendjilali",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nasrine Bendjilali",
   "pi_email_addr": "bendjilali@rowan.edu",
   "nsf_id": "000661447",
   "pi_start_date": "2018-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rowan University",
  "inst_street_address": "201 MULLICA HILL RD",
  "inst_street_address_2": "",
  "inst_city_name": "GLASSBORO",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8562564057",
  "inst_zip_code": "080281702",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NJ01",
  "org_lgl_bus_name": "ROWAN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "DMDEQP66JL85"
 },
 "perf_inst": {
  "perf_inst_name": "Rowan University",
  "perf_str_addr": "201 Mullica Hill Rd",
  "perf_city_name": "Glassboro",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "080281701",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NJ01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "199800",
   "pgm_ele_name": "IUSE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8209",
   "pgm_ref_txt": "Improv Undergrad STEM Ed(IUSE)"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0418",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001819DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 206876.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to identify a ranking of incorrect response options for each item on a commonly used physics multiple-choice test: the Force and Motion Conceptual Evaluation (FMCE). A consistent ranking of both correct and incorrect response options would allow physics instructors to get a better measurement of their students' knowledge and understanding. This measurement would be based on all the responses that a student selects, rather than only paying attention to whether a student was right or wrong.</p>\r\n<p>To achieve this goal we analyzed two large data sets of student responses to FMCE items. Each data set consisted of responses from more than 5,000 students collected before and after physics instruction at various colleges and universities from across the United States. We analyzed the data using methods from item response theory (IRT) that treat each response option separately. The output of these analyses provided an indication of how well each response option for a particular item aligns with the correct response for that item. We analyzed our data using both unidimensional IRT models (assuming that the test measures students&rsquo; knowledge of a single overarching topic) and multidimensional IRT models (assuming the test measures students&rsquo; understanding of several smaller topics).</p>\r\n<p><span>We identified a preliminary ranking of FMCE response options based on analysis of one of our data sets using a unidimensional IRT model. Analyses of our second data set produced results that were similar, but not identical, to our preliminary ranking. These inconsistencies persisted when we analyzed both data sets using multidimensional IRT models as well. These results emphasize the need for careful analysis to ensure the generalizability of research results. The outcomes of research conducted within one student population should not be assumed to hold within all populations without rigorous confirmation of their fidelity and consistency.</span></p><br>\n<p>\n Last Modified: 01/28/2025<br>\nModified by: Trevor&nbsp;I&nbsp;Smith</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of this project was to identify a ranking of incorrect response options for each item on a commonly used physics multiple-choice test: the Force and Motion Conceptual Evaluation (FMCE). A consistent ranking of both correct and incorrect response options would allow physics instructors to get a better measurement of their students' knowledge and understanding. This measurement would be based on all the responses that a student selects, rather than only paying attention to whether a student was right or wrong.\r\n\n\nTo achieve this goal we analyzed two large data sets of student responses to FMCE items. Each data set consisted of responses from more than 5,000 students collected before and after physics instruction at various colleges and universities from across the United States. We analyzed the data using methods from item response theory (IRT) that treat each response option separately. The output of these analyses provided an indication of how well each response option for a particular item aligns with the correct response for that item. We analyzed our data using both unidimensional IRT models (assuming that the test measures students knowledge of a single overarching topic) and multidimensional IRT models (assuming the test measures students understanding of several smaller topics).\r\n\n\nWe identified a preliminary ranking of FMCE response options based on analysis of one of our data sets using a unidimensional IRT model. Analyses of our second data set produced results that were similar, but not identical, to our preliminary ranking. These inconsistencies persisted when we analyzed both data sets using multidimensional IRT models as well. These results emphasize the need for careful analysis to ensure the generalizability of research results. The outcomes of research conducted within one student population should not be assumed to hold within all populations without rigorous confirmation of their fidelity and consistency.\t\t\t\t\tLast Modified: 01/28/2025\n\n\t\t\t\t\tSubmitted by: TrevorISmith\n"
 }
}