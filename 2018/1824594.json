{
 "awd_id": "1824594",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Cognitive Barriers to Understanding Complexity in Human-Technical Systems: Evidence from Engineering Students and Practitioners",
 "cfda_num": "47.041",
 "org_code": "07050000",
 "po_phone": "7032927708",
 "po_email": "jladejio@nsf.gov",
 "po_sign_block_name": "Kemi Ladeji-Osias",
 "awd_eff_date": "2018-09-15",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 403178.0,
 "awd_amount": 403178.0,
 "awd_min_amd_letter_date": "2018-09-07",
 "awd_max_amd_letter_date": "2018-09-07",
 "awd_abstract_narration": "According to the National Academy of Engineering, poor understanding of complex human-technical systems, i.e., systems that have many interacting parts, has been a major cause of \"man made disasters\" that include, for example, the Fukushima Daiichi nuclear accident and the Deepwater Horizon oil spill in the Gulf of Mexico. Various studies show that even well-schooled engineers have difficulty understanding basic concepts of complex human-technical systems. This research will provide insights of the important cognitive (e.g. reasoning, thinking) skills for the understanding of complex systems for both engineering students and working professionals. Examples of cognitive barriers are for example, the experts' tendency to look at details at the expense of looking at the big picture, and the human tendency to focus on short-term as opposed to the long-term outcomes, among others. This research will address the needs of industry and government to educate and develop complex problem solvers for the US workforce so that the US maintains its economic competitiveness, national security, and position as a global leader in innovation. Given that engineers design, build and manage human-technical systems throughout their careers, it is important to study the effect of the cognitive barriers during and after their formal education. From an educational point of view, the research will integrate the results into engineering courses, case studies, team assignments and simulation platforms. From an outreach point of view, the research will use the results for the design and offering of company, government agency, and University workshops.\r\n\r\nThis is a multi-disciplinary project, which lies at the intersection of complex systems and engineering education and will study undergraduate students with different educational experiences as well as professionals with different work experiences. Central conceptualizations of systems thinking is the focus on understanding the workings of complex interconnected socio-technical systems. In our project, our specific definition aligns with the system dynamics school of thought, which provides established methods for analyzing system behavior whose final behavior over time might be counter-intuitive. The research assumes that engineers can recognize and manage system complexity and that classroom education and field experiences can be essential for learning how to understand complex systems. The central research hypothesis is that education in engineering programs and real world experiences influence (positively or negatively) engineers' understanding of complex systems. In order to investigate the main hypothesis, four specific research questions (RQ1-RQ4) are studied. Three vignettes with different levels of structuredness and complexity representing engineering tasks will be used. The research questions are as follows: RQ1: What is the relationship between engineering students'/professionals' level of education/expertise and their performance on three vignettes that vary with respect to problem structuredness and complexity? RQ2: What is the relationship between engineering students'/professionals' perceptions of their own systems thinking competencies and their performance on three vignettes that vary with respect to problem structuredness and complexity? RQ3: How do engineering students and professionals differ in their approaches to solving problems that vary with respect to problem stucturedness and complexity? and RQ4: How do engineering students and professionals describe how and where they developed their cognitive skills of understanding complex systems?\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "EEC",
 "org_div_long_name": "Division of Engineering Education and Centers",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Konstantinos",
   "pi_last_name": "Triantis",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Konstantinos P Triantis",
   "pi_email_addr": "triantis@vt.edu",
   "nsf_id": "000150493",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Knight",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Knight",
   "pi_email_addr": "dbknight@vt.edu",
   "nsf_id": "000649118",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jacob",
   "pi_last_name": "Grohs",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jacob Grohs",
   "pi_email_addr": "jrgrohs@vt.edu",
   "nsf_id": "000665682",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Navid",
   "pi_last_name": "Ghaffarzadegan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Navid Ghaffarzadegan",
   "pi_email_addr": "navidg@vt.edu",
   "nsf_id": "000736437",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Niyousha",
   "pi_last_name": "Hosseinichimeh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Niyousha Hosseinichimeh",
   "pi_email_addr": "niyousha@vt.edu",
   "nsf_id": "000755268",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "7054 Haycock Road",
  "perf_city_name": "Falls Church",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "220432311",
  "perf_ctry_code": "US",
  "perf_cong_dist": "08",
  "perf_st_cong_dist": "VA08",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "134000",
   "pgm_ele_name": "EngEd-Engineering Education"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "110E",
   "pgm_ref_txt": "EDUCATION RESEARCH"
  },
  {
   "pgm_ref_code": "1340",
   "pgm_ref_txt": "ENGINEERING EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 403178.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>There are six outcomes associated with our award. The first outcome is the design of a tool&mdash;the Lake Urmia Vignette, or LUV&mdash;that can be used in any classroom, training setting, or experimental study that assesses individuals&rsquo; understanding of complexity in socio-environmental systems. This vignette describes a real-world environmental catastrophe, which includes various stakeholders along with social, political, economic and environmental variables. The case is a rich framework for experimentation and can help trigger class discussions. Our research includes the scenario, a rubric and detailed process by which coders can score responses. Our assessment tool has also proven to be useful in grounding class discussion as students reflect on their answers to prompts. We have used the tool in undergraduate and graduate classes.</p>\n<p>The second outcome is the comparison of self-reporting assessments and scenario-based assessments of Systems Thinking (ST) Competence. Self-report assessments are used frequently in higher education to assess a variety of constructs, including attitudes, opinions, knowledge, and competence. Systems thinking (ST) is an example of one competence often measured using self-report assessments where individuals answer several questions about their perceptions of their own skills, habits, or daily decisions. In our research, we define systems thinking (ST) as the ability to see the world as a complex interconnected system where different parts can influence each other, and different interrelationships determine system outcomes. An alternative, less-common, assessment approach is to measure skills directly by providing a scenario about an unstructured problem and evaluating respondents&rsquo; judgment or analysis of the scenario (scenario-based assessment). Our research explored the relationships between engineering students&rsquo; performance on self-report assessments and scenario-based assessments of systems thinking (ST), finding that there were no significant relationships between the two assessment techniques. These results suggest that there may be limitations to using self-report assessments as a method to assess systems thinking (ST) and other competencies in educational research and evaluation, which could be addressed by incorporating alternative formats for assessing competence. Future work could explore these findings further and support the development of alternative assessment approaches.</p>\n<p class=\"Default\">The third outcome is the investigation of student approaches to scenario-based assessment of systems thinking (ST). Existing assessments often fail to accurately measure teachable knowledge or skills that constitute systems thinking (ST). To investigate this issue, we compared students&rsquo; performance on two previously and independently peer-reviewed scenario-based assessments for systems thinking (ST). We found that the way a scenario is presented to students impacts their subsequent problem-solving approach, which complicates the assessment of systems thinking (ST). Additionally, students identified only limited opportunities for the development of ill-structured problem-solving skills necessary for systems thinking (ST). Our findings inform future work on improving systems thinking (ST) assessments and emphasize the importance of intentionally supplying opportunities for students to practice solving ill-structured problems throughout the curriculum.</p>\n<p>The fourth outcome of our research is the investigation of cognitive maps, or mental maps, which are externalized portrayals of mental models, i.e., peoples&rsquo; mental representations of reality and their presumptions about how the world works. They are often used as the intermediate step toward uncovering individuals&rsquo; presumptions of the outside world. However, once one&rsquo;s understanding of the real world is mapped, how can researchers systematically evaluate the maps and compare and contrast them? In our research, we review several common approaches to analyzing cognitive maps, some rooted in network theories. Our analysis shows that these methods provide inconsistent results and often fall short of capturing variations in mental models. The analysis points to a lack of effective methods for examining these maps and helps articulate a major research problem for systems thinking (ST) scholars.</p>\n<p>The fifth outcome relates to the measurement of systems thinking (ST) skills. Despite all the attention to systems thinking (ST), there is less consensus when it comes to evaluating and assessing systems thinking (ST) skills. Particularly, a quantitative assessment approach that captures system thinking&rsquo;s (ST&rsquo;s) multiple dimensions is crucial when evaluating the degree to which one has learned and utilizes systems thinking (ST). Our research proposes a systematic approach to create such a Multi-dimensional Index of Systems Thinking (ST) from textual data.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The sixth and final outcome pertains to the unique dataset that we collected during the project with the goal of using and validating our Lake Urmia Vignette (LUV). We recruited 30 graduate students at Virginia Tech from across two departments in the College of Engineering (Industrial and Systems Engineering, and Engineering Education). We also collected data from 20 undergraduate students across the Mechanical Engineering and Civil engineering departments. The graduate students responded to a survey that included our LUV vignette among several other measures of systems thinking and critical thinking. Finally, we completed the data collection where professionals were surveyed and were interviewed.</p><br>\n<p>\n Last Modified: 12/06/2023<br>\nModified by: Konstantinos&nbsp;P&nbsp;Triantis</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThere are six outcomes associated with our award. The first outcome is the design of a toolthe Lake Urmia Vignette, or LUVthat can be used in any classroom, training setting, or experimental study that assesses individuals understanding of complexity in socio-environmental systems. This vignette describes a real-world environmental catastrophe, which includes various stakeholders along with social, political, economic and environmental variables. The case is a rich framework for experimentation and can help trigger class discussions. Our research includes the scenario, a rubric and detailed process by which coders can score responses. Our assessment tool has also proven to be useful in grounding class discussion as students reflect on their answers to prompts. We have used the tool in undergraduate and graduate classes.\n\n\nThe second outcome is the comparison of self-reporting assessments and scenario-based assessments of Systems Thinking (ST) Competence. Self-report assessments are used frequently in higher education to assess a variety of constructs, including attitudes, opinions, knowledge, and competence. Systems thinking (ST) is an example of one competence often measured using self-report assessments where individuals answer several questions about their perceptions of their own skills, habits, or daily decisions. In our research, we define systems thinking (ST) as the ability to see the world as a complex interconnected system where different parts can influence each other, and different interrelationships determine system outcomes. An alternative, less-common, assessment approach is to measure skills directly by providing a scenario about an unstructured problem and evaluating respondents judgment or analysis of the scenario (scenario-based assessment). Our research explored the relationships between engineering students performance on self-report assessments and scenario-based assessments of systems thinking (ST), finding that there were no significant relationships between the two assessment techniques. These results suggest that there may be limitations to using self-report assessments as a method to assess systems thinking (ST) and other competencies in educational research and evaluation, which could be addressed by incorporating alternative formats for assessing competence. Future work could explore these findings further and support the development of alternative assessment approaches.\n\n\nThe third outcome is the investigation of student approaches to scenario-based assessment of systems thinking (ST). Existing assessments often fail to accurately measure teachable knowledge or skills that constitute systems thinking (ST). To investigate this issue, we compared students performance on two previously and independently peer-reviewed scenario-based assessments for systems thinking (ST). We found that the way a scenario is presented to students impacts their subsequent problem-solving approach, which complicates the assessment of systems thinking (ST). Additionally, students identified only limited opportunities for the development of ill-structured problem-solving skills necessary for systems thinking (ST). Our findings inform future work on improving systems thinking (ST) assessments and emphasize the importance of intentionally supplying opportunities for students to practice solving ill-structured problems throughout the curriculum.\n\n\nThe fourth outcome of our research is the investigation of cognitive maps, or mental maps, which are externalized portrayals of mental models, i.e., peoples mental representations of reality and their presumptions about how the world works. They are often used as the intermediate step toward uncovering individuals presumptions of the outside world. However, once ones understanding of the real world is mapped, how can researchers systematically evaluate the maps and compare and contrast them? In our research, we review several common approaches to analyzing cognitive maps, some rooted in network theories. Our analysis shows that these methods provide inconsistent results and often fall short of capturing variations in mental models. The analysis points to a lack of effective methods for examining these maps and helps articulate a major research problem for systems thinking (ST) scholars.\n\n\nThe fifth outcome relates to the measurement of systems thinking (ST) skills. Despite all the attention to systems thinking (ST), there is less consensus when it comes to evaluating and assessing systems thinking (ST) skills. Particularly, a quantitative assessment approach that captures system thinkings (STs) multiple dimensions is crucial when evaluating the degree to which one has learned and utilizes systems thinking (ST). Our research proposes a systematic approach to create such a Multi-dimensional Index of Systems Thinking (ST) from textual data.\n\n\n The sixth and final outcome pertains to the unique dataset that we collected during the project with the goal of using and validating our Lake Urmia Vignette (LUV). We recruited 30 graduate students at Virginia Tech from across two departments in the College of Engineering (Industrial and Systems Engineering, and Engineering Education). We also collected data from 20 undergraduate students across the Mechanical Engineering and Civil engineering departments. The graduate students responded to a survey that included our LUV vignette among several other measures of systems thinking and critical thinking. Finally, we completed the data collection where professionals were surveyed and were interviewed.\t\t\t\t\tLast Modified: 12/06/2023\n\n\t\t\t\t\tSubmitted by: KonstantinosPTriantis\n"
 }
}