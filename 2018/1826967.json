{
 "awd_id": "1826967",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CC* NPEO: Toward the National Research Platform",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924220",
 "po_email": "kthompso@nsf.gov",
 "po_sign_block_name": "Kevin Thompson",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 2500000.0,
 "awd_amount": 2548000.0,
 "awd_min_amd_letter_date": "2018-09-12",
 "awd_max_amd_letter_date": "2022-01-11",
 "awd_abstract_narration": "Academic researchers need a simple data sharing architecture with end-to-end 10-to-100Gbps performance to enable virtual co-location of large amounts of data with computing. End-to-end is a difficult problem to solve in general because the networks between ends (campuses, data repositories, etc.) typically traverse multiple network management domains: campus, regional, and national.  No one organization owns the responsibility for providing scientists with high-bandwidth disk-to-disk performance. Toward the National Research Platform (TNRP), addresses issues critical to scaling end-to-end data sharing. TNRP will instrument a large federation of heterogeneous \"national-regional-state\" networks (NRSNs) to greatly improve end-to-end network performance across the nation. \r\n\r\nThe goal of improving end-to-end network performance across the nation requires active participation of these distributed intermediate-level entities to reach out to their campuses. They are trusted conveners of their member institutions, contributing effectively to the \"people networking\" that is as necessary to the development of a full National Research Platform as is the stability, deployment, and performance of technology. TNRP's collaborating NRSNs structure leads to engagement of a large set of science applications, identified by the participating NRSNs and the Open Science Grid. \r\n\r\nTNRP is highly instrumented to directly measure performance. Visualizations of disk-to-disk performance with passive and active network monitoring show intra- and inter-NSRN end-to-end performance. Internet2, critical for interconnecting regional networks, will provide an instrumented dedicated virtual network instance for the interconnection of TNRP's NRSNs. Cybersecurity is a continuing concern; evaluations of advanced containerized orchestration, hardware crypto engines, and novel IPv6 strategies are part of the TNRP plan.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Larry",
   "pi_last_name": "Smarr",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Larry L Smarr",
   "pi_email_addr": "lsmarr@ucsd.edu",
   "nsf_id": "000396797",
   "pi_start_date": "2018-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Philip",
   "pi_last_name": "Papadopoulos",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Philip M Papadopoulos",
   "pi_email_addr": "ppapadopoulos@ucsd.edu",
   "nsf_id": "000462781",
   "pi_start_date": "2018-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Wuerthwein",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Frank Wuerthwein",
   "pi_email_addr": "fkw@ucsd.edu",
   "nsf_id": "000144338",
   "pi_start_date": "2018-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tajana",
   "pi_last_name": "Rosing",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Tajana S Rosing",
   "pi_email_addr": "tajana@ucsd.edu",
   "nsf_id": "000485892",
   "pi_start_date": "2018-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ilkay",
   "pi_last_name": "Altintas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ilkay Altintas",
   "pi_email_addr": "altintas@sdsc.edu",
   "nsf_id": "000487407",
   "pi_start_date": "2018-09-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Drive",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808000",
   "pgm_ele_name": "Campus Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 2500000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Academic researchers currently want end-to-end (E2E) 10- to 100-Gbps network connections so they can virtually co-locate large amounts of data and computing, and there is a growing demand for even greater 200-400 Gbps performance as technology and applications, such as AI, increase data acquisition needs and capabilities. E2E is a difficult problem to solve because the networks between ends typically traverse multiple network management domains &ndash; campus, regional, and national &ndash; and no single organization has the responsibility for providing high-bandwidth data transfers. The <em>Toward the National Research Platform </em>project is addressing the issues of scaling E2E sharing by instrumenting a large federation of heterogeneous networks and data transfer nodes (DTNs) to gain knowledge to improve national E2E performance.</p>\n<p>The National Research Platform (NRP) evolved from the NSF-funded Pacific Research Platform (PRP), which created a high-speed big-data sharing architecture enabled by end-to-end 10- to 100-Gbps connections. &nbsp;It was discovered that by measuring and monitoring network usage,&nbsp; PRP innovations could improve data-transfer speeds between deployed campus DTNs by up to a thousand times. Building on this knowledge, we worked directly with network leaders, campus CIOs, and supercomputer center directors to refine and improve measurement and monitoring technology for the distributed cluster of computer nodes called Nautilus that is the core of the NRP. Nautilus is a national-scale instrument for network measurement and monitoring. It joins software-defined compute, storage, and networking into one highly distributed cluster at 50+ locations, with approximately 1,300 GPUs and 21,000 CPU cores plus over 5PB of high-speed storage. The TNRP project deployed an additional 145 DTNs specialized for measuring and monitoring, fast data transfer, interactive visualization, and storage management. A campus can become a co-owner of the NRP by adding resources (GPU, CPU, FPGA, storage, or other equipment) to Nautilus. NRP partner data transfer and compute nodes are all instrumented and directly measure performance, and visualizations of memory-to-memory performance show E2E performance.</p>\n<p>Because improving end-to-end network performance across the nation requires active participation of regional distributed intermediate-level networking entities that connect campuses, the NRP increased its collaboration with&nbsp; several national research and education networks, including Internet2 and ESnet, as well as state and regional networks, such as the Great Plains Network, the Albuquerque GigaPoP, CENIC, Pacific Wave, Front Range GigaPoP, SoX, Florida Lambda Rail, the Chicago-based MREN, and PIREN in Hawaii and Guam. These and other grassroot efforts on campuses allowed us to successfully meet our goal of developing a community of Nautilus users. Our logs show that the number of Nautilus users has grown from an original 30 in 2018 to nearly 3,000 faculty, researchers, and students on over 100 campuses today. Nautilus allows these users to get their research results back much faster by giving them access to multiple -- sometimes 100s -- of computers at once instead of limiting them to a single computer under their desk. Using the monitoring infrastructure created by this project, researchers in many disciplines have learned how to monitor and measure their resource utilization and take advantage of high-throughput and distributed computing, as well as high-speed sharing, over 10, 40, and 100Gb/s connections to maintain high performance at all times. As a result, the Nautilus distributed hyper-converged cluster is now a model for use and replication on various research platforms around the nation and globally.</p><br>\n<p>\n Last Modified: 12/19/2023<br>\nModified by: Larry&nbsp;L&nbsp;Smarr</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAcademic researchers currently want end-to-end (E2E) 10- to 100-Gbps network connections so they can virtually co-locate large amounts of data and computing, and there is a growing demand for even greater 200-400 Gbps performance as technology and applications, such as AI, increase data acquisition needs and capabilities. E2E is a difficult problem to solve because the networks between ends typically traverse multiple network management domains  campus, regional, and national  and no single organization has the responsibility for providing high-bandwidth data transfers. The Toward the National Research Platform project is addressing the issues of scaling E2E sharing by instrumenting a large federation of heterogeneous networks and data transfer nodes (DTNs) to gain knowledge to improve national E2E performance.\n\n\nThe National Research Platform (NRP) evolved from the NSF-funded Pacific Research Platform (PRP), which created a high-speed big-data sharing architecture enabled by end-to-end 10- to 100-Gbps connections. It was discovered that by measuring and monitoring network usage, PRP innovations could improve data-transfer speeds between deployed campus DTNs by up to a thousand times. Building on this knowledge, we worked directly with network leaders, campus CIOs, and supercomputer center directors to refine and improve measurement and monitoring technology for the distributed cluster of computer nodes called Nautilus that is the core of the NRP. Nautilus is a national-scale instrument for network measurement and monitoring. It joins software-defined compute, storage, and networking into one highly distributed cluster at 50+ locations, with approximately 1,300 GPUs and 21,000 CPU cores plus over 5PB of high-speed storage. The TNRP project deployed an additional 145 DTNs specialized for measuring and monitoring, fast data transfer, interactive visualization, and storage management. A campus can become a co-owner of the NRP by adding resources (GPU, CPU, FPGA, storage, or other equipment) to Nautilus. NRP partner data transfer and compute nodes are all instrumented and directly measure performance, and visualizations of memory-to-memory performance show E2E performance.\n\n\nBecause improving end-to-end network performance across the nation requires active participation of regional distributed intermediate-level networking entities that connect campuses, the NRP increased its collaboration with several national research and education networks, including Internet2 and ESnet, as well as state and regional networks, such as the Great Plains Network, the Albuquerque GigaPoP, CENIC, Pacific Wave, Front Range GigaPoP, SoX, Florida Lambda Rail, the Chicago-based MREN, and PIREN in Hawaii and Guam. These and other grassroot efforts on campuses allowed us to successfully meet our goal of developing a community of Nautilus users. Our logs show that the number of Nautilus users has grown from an original 30 in 2018 to nearly 3,000 faculty, researchers, and students on over 100 campuses today. Nautilus allows these users to get their research results back much faster by giving them access to multiple -- sometimes 100s -- of computers at once instead of limiting them to a single computer under their desk. Using the monitoring infrastructure created by this project, researchers in many disciplines have learned how to monitor and measure their resource utilization and take advantage of high-throughput and distributed computing, as well as high-speed sharing, over 10, 40, and 100Gb/s connections to maintain high performance at all times. As a result, the Nautilus distributed hyper-converged cluster is now a model for use and replication on various research platforms around the nation and globally.\t\t\t\t\tLast Modified: 12/19/2023\n\n\t\t\t\t\tSubmitted by: LarryLSmarr\n"
 }
}