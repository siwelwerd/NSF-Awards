{
 "awd_id": "1830421",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Robust Inverse Learning for Human-Robot Collaboration",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 644182.0,
 "awd_amount": 660182.0,
 "awd_min_amd_letter_date": "2018-08-18",
 "awd_max_amd_letter_date": "2021-02-25",
 "awd_abstract_narration": "Collaborative robots are robots that share with humans their work and personal spaces. These robots are expected to work with humans on a variety of tasks in various situations with few changes to their software and hardware. To do this, the robot must understand what is it that the human or other robot is doing, how is the human or robot performing the task, and then personalize its interaction. Currently, robots are programmed with much manual effort to perform specific tasks in controlled environments. This research is studying ways that will substantially advance a robot's capabilities in all these aspects, to enable a collaboration that is as automatic and seamless as possible. It is building methods, which allow the robot to observe the human or robot perform the task, understand the human's preferences and intent in the task, and then spontaneously collaborate with the human on the task. This approach relies on the insight that observing a human or robot perform the task provides information and facilitates learning the task. An application considered in this project is an agricultural robot that will observe and autonomously collaborate with a human in grading and packing onions in postharvest processing sheds. This has the potential to augment scarce human labor in our nation's farms in performing this repetitive task. \r\n\r\nInverse reinforcement learning (IRL) refers to both the problem and method by which an agent learns the goals and preferences of another agent that explain the latter's observed behavior. The technical approach to this research is first identifying the challenges that IRL is facing in its use toward inferring the goals and preferences of the observed agent, human or robot, in real-world contexts. The research is tractably generalizing IRL to meet key unmet challenges. It is developing new methods that will make IRL robust to real-world uncertainties involving hidden variables, occlusions, and imperfect observations by the robot. Typically, IRL is one sided and the reward is learned with the aim of imitating the observed behavior. This research will go a step further and investigate how the dynamics and learned preferences can be revised and incorporated in the robot's collaborative decision making and planning, to enable the robot to spontaneously collaborate with the previously observed agent on the task. Consequently, the focus is on domains where the subject robot can observe an agent performing well-defined tasks that benefit from teamwork. The research plan is expected to yield a portfolio of algorithms that take key steps toward enabling robots to autonomously learn how to perform tasks and deploy this knowledge toward optimally collaborating with others on the task. Being able to learn tasks simply from passive demonstrations provides greater appeal to this research as it minimizes costly human interventions.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Prashant",
   "pi_last_name": "Doshi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Prashant Doshi",
   "pi_email_addr": "pdoshi@uga.edu",
   "nsf_id": "000219732",
   "pi_start_date": "2018-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Yi",
   "pi_last_name": "Hong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yi Hong",
   "pi_email_addr": "yi.hong@uga.edu",
   "nsf_id": "000732678",
   "pi_start_date": "2018-08-18",
   "pi_end_date": "2021-02-25"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kenneth",
   "pi_last_name": "Bogert",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kenneth Bogert",
   "pi_email_addr": "kbogert@unca.edu",
   "nsf_id": "000741083",
   "pi_start_date": "2018-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Georgia Research Foundation Inc",
  "inst_street_address": "310 E CAMPUS RD RM 409",
  "inst_street_address_2": "",
  "inst_city_name": "ATHENS",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "7065425939",
  "inst_zip_code": "306021589",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "GA10",
  "org_lgl_bus_name": "UNIVERSITY OF GEORGIA RESEARCH FOUNDATION, INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "NMJHD63STRC5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Georgia",
  "perf_str_addr": "310 East Campus Road",
  "perf_city_name": "Athens",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "306021589",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "GA10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 644182.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project yielded a portfolio of new methods that brings inverse reinforcement learning (IRL) significantly closer to being used for real-world applications in programming robots. IRL is about automatically learning the underlying task parameters from demonstrations of the task. As such, it offers a way for a robot to learn to perform the task, which was demonstrated to it. This project developed a perception pipeline that processes a video of the task being performed and identifies the task states and actions in the video. It also developed several methods that let the learning occur even when some portions of the demonstration are hidden from view and when the perception sensors and models result in output that is noisy. This is anticipated in real-world use cases of IRL. The project's broader impacts led to applications of this technology toward deploying collaborative robotic arms that can safely sort produce such as sweet onions on a conveyance line alongside humans and the transfer of this technology to a startup company, which aims to commercialize it. Multiple graduate students who assisted on this project received training in AI and contributed to building up the AI workforce of the nation. The products of this project such as publications and techniques were also integrated into the courses and the tutorials that the investigators taught. Overall, the project's outcomes led to important realizations of intellectual and broader impacts in a field of national priority.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2022<br>\n\t\t\t\t\tModified by: Prashant&nbsp;Doshi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project yielded a portfolio of new methods that brings inverse reinforcement learning (IRL) significantly closer to being used for real-world applications in programming robots. IRL is about automatically learning the underlying task parameters from demonstrations of the task. As such, it offers a way for a robot to learn to perform the task, which was demonstrated to it. This project developed a perception pipeline that processes a video of the task being performed and identifies the task states and actions in the video. It also developed several methods that let the learning occur even when some portions of the demonstration are hidden from view and when the perception sensors and models result in output that is noisy. This is anticipated in real-world use cases of IRL. The project's broader impacts led to applications of this technology toward deploying collaborative robotic arms that can safely sort produce such as sweet onions on a conveyance line alongside humans and the transfer of this technology to a startup company, which aims to commercialize it. Multiple graduate students who assisted on this project received training in AI and contributed to building up the AI workforce of the nation. The products of this project such as publications and techniques were also integrated into the courses and the tutorials that the investigators taught. Overall, the project's outcomes led to important realizations of intellectual and broader impacts in a field of national priority.\n\n\t\t\t\t\tLast Modified: 12/30/2022\n\n\t\t\t\t\tSubmitted by: Prashant Doshi"
 }
}