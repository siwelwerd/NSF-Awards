{
 "awd_id": "1763191",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: MEDIUM: Collaborative Research: Foundations of Adaptive Data Analysis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2018-03-01",
 "awd_exp_date": "2021-02-28",
 "tot_intn_awd_amt": 276000.0,
 "awd_amount": 276000.0,
 "awd_min_amd_letter_date": "2018-02-14",
 "awd_max_amd_letter_date": "2020-04-25",
 "awd_abstract_narration": "Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. \r\n\r\nThe technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of \"over-fitting\" and \"false discovery.\" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Zou",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "James Y Zou",
   "pi_email_addr": "jamesz@stanford.edu",
   "nsf_id": "000728417",
   "pi_start_date": "2018-02-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "365 Lasuen St",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943058700",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 89310.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 91970.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 94720.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-fbe0d07b-7fff-4168-d48b-f906f878a9ed\">\n<p dir=\"ltr\"><span>The goal of this project is to advance the science of adaptive data analysis. Adaptive data analysis is ubiquitous in modern data science. Researchers often iteratively explore the data. They can use the data to test different hypotheses depending on their previous findings on the same data. This iterative process is what we call adaptive data analysis; it is adaptive because which questions or hypotheses are tested next depends on the previous results. Adaptivity creates complex interdependencies and correlations which render invalid standard statistical guarantees, which typically assume that the analyses done on the data are independent.&nbsp;&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>In our project, we developed new theories and algorithms that enable researchers to perform flexible adaptive data analysis while maintaining some of the good statistical properties such as generalization and robustness that are often useful in practice. We achieved this by bringing together ideas from privacy, information theory, robust optimization and machine learning. This work establishes a solid foundation for adaptive and interactive data science.&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>Through this project, we have published many research papers on adaptive data analysis. For example, we showed how data collected through adaptive processes can contain inadvertent statistical biases, and proposed methods to reduce such bias. We also proposed new approaches using data augmentation to make downstream analysis more robust. The PIs have also given numerous talks, organized workshops, taught classes and published open-source code on this topic. The science of adaptive data analysis is still an active and relatively early area of research. The tools and foundation developed in this project can benefit the broader data science community.&nbsp;</span></p>\n<div><span><br /></span></div>\n</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/26/2021<br>\n\t\t\t\t\tModified by: James&nbsp;Y&nbsp;Zou</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThe goal of this project is to advance the science of adaptive data analysis. Adaptive data analysis is ubiquitous in modern data science. Researchers often iteratively explore the data. They can use the data to test different hypotheses depending on their previous findings on the same data. This iterative process is what we call adaptive data analysis; it is adaptive because which questions or hypotheses are tested next depends on the previous results. Adaptivity creates complex interdependencies and correlations which render invalid standard statistical guarantees, which typically assume that the analyses done on the data are independent.  \n\n\nIn our project, we developed new theories and algorithms that enable researchers to perform flexible adaptive data analysis while maintaining some of the good statistical properties such as generalization and robustness that are often useful in practice. We achieved this by bringing together ideas from privacy, information theory, robust optimization and machine learning. This work establishes a solid foundation for adaptive and interactive data science. \n\n\nThrough this project, we have published many research papers on adaptive data analysis. For example, we showed how data collected through adaptive processes can contain inadvertent statistical biases, and proposed methods to reduce such bias. We also proposed new approaches using data augmentation to make downstream analysis more robust. The PIs have also given numerous talks, organized workshops, taught classes and published open-source code on this topic. The science of adaptive data analysis is still an active and relatively early area of research. The tools and foundation developed in this project can benefit the broader data science community. \n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 08/26/2021\n\n\t\t\t\t\tSubmitted by: James Y Zou"
 }
}