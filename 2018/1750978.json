{
 "awd_id": "1750978",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Structured Scientific Evidence Extraction: Models and Corpora",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 549933.0,
 "awd_amount": 565933.0,
 "awd_min_amd_letter_date": "2018-02-05",
 "awd_max_amd_letter_date": "2020-06-01",
 "awd_abstract_narration": "Scientific evidence is primarily disseminated in free-text journal articles. Drawing upon this evidence to make decisions or inform policies therefore requires perusing relevant articles and manually extracting the findings of interest. Unfortunately, this process is time-consuming and has not scaled to meet the demands imposed by the torrential expansion of the scientific evidence base. This work seeks to design novel Natural Language Processing (NLP) methods that can automatically \"read\" and make sense of unstructured published scientific evidence. This is critically important because decisions by policy-makers, care-givers and individuals should be informed by the entirety of the relevant published scientific evidence; but because evidence is predominantly unstructured -- and hence not directly actionable -- this is currently impossible in practice. Consider clinical medicine, an important example which serves as the target domain of this proposal (although the framework and models will generalize to other scientific areas). Roughly 100 articles describing trials were published every single day in 2015. Healthcare professionals cannot possibly make sense of this, and thus treatment decisions must be made without full consideration of the available evidence. Methods that can automatically infer from this torrential mass of unstructured literature which treatments are actually supported by the evidence would facilitate better, evidence-based decisions. Toward this end, this research seeks to design NLP models capable of mapping from natural language scientific articles describing studies or trials to structured \"evidence frames\" that codify the interventions and outcomes studied, and the reported findings concerning these. NLP technology is not presently up to this task. Therefore, this project will support core methodological contributions that will advance systems for data extraction and machine reading of lengthy articles; these will have impact beyond the present motivating application. \r\n\r\nFrom a technical perspective, the focus of this work concerns developing novel, interpretable (transparent) neural network models for extraction from and inference over lengthy articles. Specifically, this project aims to design models that can automatically identify treatments and associated outcomes from free-texts, and then infer the reported comparative effects of the former with respect to the latter. This pushes against limits of existing language technology capabilities. In particular, this necessitates models that perform deep analysis of individual, potentially lengthy, technical documents. Furthermore, model transparency is critical here, as domain experts must be able to recover from where in documents evidential claims were inferred. New corpora curated for this project (to be shared with the broader community) will facilitate core NLP research on such models. To realize the aforementioned methodological aims, the researchers leading this project will develop conditional and dynamic \"attentive\" neural models. Specific methodological lines of research to be explored include: (i) Models equipped with conditional, sparse attention mechanisms over textual units that reflect scientific discourse structure to achieve accurate and transparent extraction of, and inference concerning, reported evidence. (ii) Neural sequence tagging models that take multiple 'reads' of a text, exploiting iteratively adjusted conditional document representations as global context to inform local predictions. A project website (http://www.byronwallace.com/evidence-extraction) provides access to papers, datasets and other project outputs.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Byron",
   "pi_last_name": "Wallace",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Byron Wallace",
   "pi_email_addr": "b.wallace@northeastern.edu",
   "nsf_id": "000627515",
   "pi_start_date": "2018-02-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 115343.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 132659.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 317931.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>To determine whether a particular intervention (e.g., educational tool or medical treatment) actually works, one would ideally consult all available evidence from relevant trials. Unfortunately, such results are primarily disseminated in natural language scientific articles, imposing substantial burden on domain experts trying to make sense of them. <strong>This project supported development of novel Natural Language Processing (NLP) methods and datasets that enabled substantial progress towards making unstructured published scientific evidence actionable</strong>.</p>\n<p>Specifically, the project supported the formalization of the novel task of structured <em>scientific evidence extraction</em>, i.e., mapping from natural language scientific articles describing trials to structured evidence frames codifying the interventions and outcomes studied, and the reported findings concerning these. The resulting models are able to infer&mdash;with some accuracy&mdash;if a given article provides evidence supporting the use of a particular treatment (say, aspirin) to affect some outcome of interest (e.g., reduce risk of stroke). Models for this important and challenging task were developed using a new dataset collected as part of this project, which has also been released to (and used by) the broader research community.&nbsp;</p>\n<p>Identifying treatments and outcomes in free-texts, and then inferring the reported efficacy of the former with respect to the latter, was well beyond the scope of language technologies at the onset of this project. Furthermore, the ability for domain experts to <em>verify </em>model outputs is a critical need in this domain&mdash;as domain experts must be able to recover from where in documents evidential claims were inferred&mdash;but many machine learning models are opaque. Between technical innovations and dataset development supported by this project specifically, and advances in the field of NLP more generally, <strong>this project has yielded methods which permit (at least in some cases) fully automatic meta-analysis</strong>. The project has also supported development of open-source software and data resources which have been shared with the larger research community.&nbsp;</p>\n<p>In addition to these application-oriented outputs, the project has supported core technical work on NLP models that permit <em>verification </em>of outputs by users, and new approaches to <em>relation extraction </em>that jointly extract interventions, comparators, and outcomes from trials, and then infer relationships between these. Relevant to the former, we introduced a now widely used benchmark (ERASER) that supports development and evaluation of models which provide evidence for predictions.</p>\n<p>The papers resulting from this project and describing these methods have won awards and have been highly cited (collectively over 1000 times) by members of the NLP community and beyond.&nbsp;&nbsp;</p>\n<p>In terms of <strong>broader impacts</strong>, the developed models are poised to make evidence synthesis more efficient. This is important because decisions by policymakers, care-givers and individuals should be informed by the entirety of the published scientific evidence. This is currently impossible, because evidence is primarily reported in unstructured articles. Culling and synthesizing evidence relevant to a specific question is thus an onerous task. The models and datasets developed under this project have supported development of novel technologies which semi-automate extraction of key findings from published scientific literature, in turn (potentially) informing evidence-based guidelines and practice. The PI has worked with the non-profit Reboot Rx (formerly Cures Within Reach) to use the methods developed in this project to help discover candidates for drug repurposing, and with specialized software companies (e.g., Sciome and CapeStart, amongst others) to facilitate transfer of knowledge and methods into practice. Moreover, the Trialstreamer database (elements of which were supported by this project) is now routinely highlighted as a trustworthy source by University library guidelines.</p>\n<p>Finally, this project supported critical <strong>educational and outreach initiatives</strong>. The PI partnered with faculty and administration at Roxbury Community College (RCC), a minority-serving (majority Black) institute in Boston less than a mile from Northeastern to recruit an RCC student as an REU in 2019, providing his first research experience. In addition, as originally proposed, the PI developed <em>DS 4440: Practical Neural Networks </em>as part of this project, which is now a popular course regularly offered (and required for Data Science majors). The PI received the 2021 <em>Joel and Ruth Spira Excellence in&nbsp;Teaching Award</em>, partly on the basis of this course, which was supported through this CAREER proposal.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Depiction of the problem framining, as reflected in the <em>Evidence Inference</em> dataset collected for this project.</p>\n<p>&nbsp;</p>\n<p>Recent results from our work using Large Language Models (LLMs) to extract the fine-grained data necessary to perform statistical meta-analysis of results.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/16/2024<br>\nModified by: Byron&nbsp;Wallace</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nTo determine whether a particular intervention (e.g., educational tool or medical treatment) actually works, one would ideally consult all available evidence from relevant trials. Unfortunately, such results are primarily disseminated in natural language scientific articles, imposing substantial burden on domain experts trying to make sense of them. This project supported development of novel Natural Language Processing (NLP) methods and datasets that enabled substantial progress towards making unstructured published scientific evidence actionable.\n\n\nSpecifically, the project supported the formalization of the novel task of structured scientific evidence extraction, i.e., mapping from natural language scientific articles describing trials to structured evidence frames codifying the interventions and outcomes studied, and the reported findings concerning these. The resulting models are able to inferwith some accuracyif a given article provides evidence supporting the use of a particular treatment (say, aspirin) to affect some outcome of interest (e.g., reduce risk of stroke). Models for this important and challenging task were developed using a new dataset collected as part of this project, which has also been released to (and used by) the broader research community.\n\n\nIdentifying treatments and outcomes in free-texts, and then inferring the reported efficacy of the former with respect to the latter, was well beyond the scope of language technologies at the onset of this project. Furthermore, the ability for domain experts to verify model outputs is a critical need in this domainas domain experts must be able to recover from where in documents evidential claims were inferredbut many machine learning models are opaque. Between technical innovations and dataset development supported by this project specifically, and advances in the field of NLP more generally, this project has yielded methods which permit (at least in some cases) fully automatic meta-analysis. The project has also supported development of open-source software and data resources which have been shared with the larger research community.\n\n\nIn addition to these application-oriented outputs, the project has supported core technical work on NLP models that permit verification of outputs by users, and new approaches to relation extraction that jointly extract interventions, comparators, and outcomes from trials, and then infer relationships between these. Relevant to the former, we introduced a now widely used benchmark (ERASER) that supports development and evaluation of models which provide evidence for predictions.\n\n\nThe papers resulting from this project and describing these methods have won awards and have been highly cited (collectively over 1000 times) by members of the NLP community and beyond.\n\n\nIn terms of broader impacts, the developed models are poised to make evidence synthesis more efficient. This is important because decisions by policymakers, care-givers and individuals should be informed by the entirety of the published scientific evidence. This is currently impossible, because evidence is primarily reported in unstructured articles. Culling and synthesizing evidence relevant to a specific question is thus an onerous task. The models and datasets developed under this project have supported development of novel technologies which semi-automate extraction of key findings from published scientific literature, in turn (potentially) informing evidence-based guidelines and practice. The PI has worked with the non-profit Reboot Rx (formerly Cures Within Reach) to use the methods developed in this project to help discover candidates for drug repurposing, and with specialized software companies (e.g., Sciome and CapeStart, amongst others) to facilitate transfer of knowledge and methods into practice. Moreover, the Trialstreamer database (elements of which were supported by this project) is now routinely highlighted as a trustworthy source by University library guidelines.\n\n\nFinally, this project supported critical educational and outreach initiatives. The PI partnered with faculty and administration at Roxbury Community College (RCC), a minority-serving (majority Black) institute in Boston less than a mile from Northeastern to recruit an RCC student as an REU in 2019, providing his first research experience. In addition, as originally proposed, the PI developed DS 4440: Practical Neural Networks as part of this project, which is now a popular course regularly offered (and required for Data Science majors). The PI received the 2021 Joel and Ruth Spira Excellence inTeaching Award, partly on the basis of this course, which was supported through this CAREER proposal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDepiction of the problem framining, as reflected in the Evidence Inference dataset collected for this project.\n\n\n\n\n\nRecent results from our work using Large Language Models (LLMs) to extract the fine-grained data necessary to perform statistical meta-analysis of results.\n\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 07/16/2024\n\n\t\t\t\t\tSubmitted by: ByronWallace\n"
 }
}