{
 "awd_id": "1830345",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: INT: COLLAB: Shared Autonomy for Unstructured Underwater Environments through Vision and Language",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Juan Wachs",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 348407.0,
 "awd_amount": 348407.0,
 "awd_min_amd_letter_date": "2018-08-31",
 "awd_max_amd_letter_date": "2018-08-31",
 "awd_abstract_narration": "Existing underwater robotic systems typically provide one of two operating modes---full teleoperation or full autonomy. Teleoperation is by far the most common, particularly for tasks involving interaction with the environment, such as grasping and manipulation. Autonomy is restricted to non-contact survey missions and to controlled laboratory settings. The ability to operate between teleoperation and autonomy will improve the efficiency and effectiveness of tasks performed in underwater environments. This research will develop and evaluate a novel shared autonomy framework. The research leverages the different nature of humans and robots. This work will reduce the need for multiple, highly trained operators. It has the potential to drastically reduce the cost of underwater missions. The contributions of this research will impact the way in which humans work together with robots within a wide variety of applications, including space exploration, disaster relief, and assistive robotics.\r\n\r\nAs robotic systems play an ever-larger role as our surrogates for marine science and exploration, the ability to leverage the complementary nature of humans and robots becomes critical for scientific discovery. This research will develop new models and algorithms that exploit multiple non-commensurate sensing and control modalities to realize intelligent shared autonomy in complex unstructured environments. Novel to this research is the use of natural language and vision as complementary forms of weak supervision to enable robots to learn human-collaborative sensorimotor manipulation policies opportunistically from narrated human demonstrations. Fundamental to these methods is their ability to then refine these policies in situ based upon interaction with a human operator. Together, these models and algorithms will enhance the efficiency and effectiveness of underwater scientific exploration.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Johnson-Roberson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew Johnson-Roberson",
   "pi_email_addr": "mkj@andrew.cmu.edu",
   "nsf_id": "000649944",
   "pi_start_date": "2018-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091274",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 348407.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has developed a set of automated tools for performing science tasks with a Remotely Operated Vehicle (ROV) underwater. This system can be used for water sampling, grabbing and using tools and picking and placing objects. Currently, the system uses a fisheye camera an additional pair of stereo cameras that are similar to how humans understand depth. The system builds a 3D reconstruction of the scene and then the manipulator is dispatched to automatically grab and move object in the field of view. We have a proof-of-concept demonstration of the command interface, which allows a pilot to step through the maneuver and visualize and oversee execution at every step.</p>\n<p>An interactive marker allows the user to easily set the goal sample location in the scene. The demo shows that the calibration of the different camera systems, the manipulator, and the computer's understanding of the scene is accurate enough to achieve our manipulation goals. This was demonstrated on a real vehicle. The ROV used in this mission was the Nereid Under Ice hybrid ROV for missions under ice and equipped with an ultra-low power hydraulic manipulator.&nbsp;</p>\n<p>Testing and field trials were carried out in progressively challenging environments, initially in laboratory settings and tank testing, followed by 11 dive missions at the Central American Pacific shelf margin of Costa Rica to operational depths of approximately 1800m. This area of the Costa Rican accretionary prism is a well-studied region.</p>\n<p>Following completion of the Costa Rica expedition, our team conducted a series of five dive missions to depths of 500m within the potentially hazardous craters of the Kolumbo and Santorini calderas. These sites of active volcanism contain localized areas of high-temperature hydrothermal venting, which is host to organisms that may resemble those which arose on early Earth, prior to atmosphere we now breath.&nbsp;</p>\n<p>In summary, automated exploration of unstructured seafloor environments is within reach of current underwater robotic technology. More development is needed, particularly in methods for scene reconstruction and understanding, to make this technology sufficiently reliable for fully automated deployment, but results from our oceanographic expeditions in this project has demonstrated that a wide range of existing ROVs and manipulator systems can be inexpensively adapted for high level automation capabilities.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/29/2021<br>\n\t\t\t\t\tModified by: Matthew&nbsp;Johnson-Roberson</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796195607_audio_slurp_b--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796195607_audio_slurp_b--rgov-800width.jpg\" title=\"Audio Slurping\"><img src=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796195607_audio_slurp_b--rgov-66x44.jpg\" alt=\"Audio Slurping\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Using natural language to perform slurp sampling</div>\n<div class=\"imageCredit\">Matthew Walter</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Matthew&nbsp;Johnson-Roberson</div>\n<div class=\"imageTitle\">Audio Slurping</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796259692_automated_slurp--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796259692_automated_slurp--rgov-800width.jpg\" title=\"Automated Slurping\"><img src=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796259692_automated_slurp--rgov-66x44.jpg\" alt=\"Automated Slurping\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Model of ROV to perform automated sampling</div>\n<div class=\"imageCredit\">Gideon Billings</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Matthew&nbsp;Johnson-Roberson</div>\n<div class=\"imageTitle\">Automated Slurping</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796339908_pointclouds_a--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796339908_pointclouds_a--rgov-800width.jpg\" title=\"Pointcloud\"><img src=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796339908_pointclouds_a--rgov-66x44.jpg\" alt=\"Pointcloud\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Underwater manipulation scene</div>\n<div class=\"imageCredit\">Gideon Billings</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Matthew&nbsp;Johnson-Roberson</div>\n<div class=\"imageTitle\">Pointcloud</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796462672_fisheye_reconstruction--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796462672_fisheye_reconstruction--rgov-800width.jpg\" title=\"3D Reconstruction\"><img src=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796462672_fisheye_reconstruction--rgov-66x44.jpg\" alt=\"3D Reconstruction\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Fisheye 3D reconstruction</div>\n<div class=\"imageCredit\">Gideon Billings</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Matthew&nbsp;Johnson-Roberson</div>\n<div class=\"imageTitle\">3D Reconstruction</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796538821_uwhandles--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796538821_uwhandles--rgov-800width.jpg\" title=\"Underwater T-handles\"><img src=\"/por/images/Reports/POR/2021/1830345/1830345_10579021_1640796538821_uwhandles--rgov-66x44.jpg\" alt=\"Underwater T-handles\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Handles for underwater manipulation using a robot arm</div>\n<div class=\"imageCredit\">Gideon Billings</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Matthew&nbsp;Johnson-Roberson</div>\n<div class=\"imageTitle\">Underwater T-handles</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project has developed a set of automated tools for performing science tasks with a Remotely Operated Vehicle (ROV) underwater. This system can be used for water sampling, grabbing and using tools and picking and placing objects. Currently, the system uses a fisheye camera an additional pair of stereo cameras that are similar to how humans understand depth. The system builds a 3D reconstruction of the scene and then the manipulator is dispatched to automatically grab and move object in the field of view. We have a proof-of-concept demonstration of the command interface, which allows a pilot to step through the maneuver and visualize and oversee execution at every step.\n\nAn interactive marker allows the user to easily set the goal sample location in the scene. The demo shows that the calibration of the different camera systems, the manipulator, and the computer's understanding of the scene is accurate enough to achieve our manipulation goals. This was demonstrated on a real vehicle. The ROV used in this mission was the Nereid Under Ice hybrid ROV for missions under ice and equipped with an ultra-low power hydraulic manipulator. \n\nTesting and field trials were carried out in progressively challenging environments, initially in laboratory settings and tank testing, followed by 11 dive missions at the Central American Pacific shelf margin of Costa Rica to operational depths of approximately 1800m. This area of the Costa Rican accretionary prism is a well-studied region.\n\nFollowing completion of the Costa Rica expedition, our team conducted a series of five dive missions to depths of 500m within the potentially hazardous craters of the Kolumbo and Santorini calderas. These sites of active volcanism contain localized areas of high-temperature hydrothermal venting, which is host to organisms that may resemble those which arose on early Earth, prior to atmosphere we now breath. \n\nIn summary, automated exploration of unstructured seafloor environments is within reach of current underwater robotic technology. More development is needed, particularly in methods for scene reconstruction and understanding, to make this technology sufficiently reliable for fully automated deployment, but results from our oceanographic expeditions in this project has demonstrated that a wide range of existing ROVs and manipulator systems can be inexpensively adapted for high level automation capabilities.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/29/2021\n\n\t\t\t\t\tSubmitted by: Matthew Johnson-Roberson"
 }
}