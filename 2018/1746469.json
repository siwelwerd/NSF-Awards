{
 "awd_id": "1746469",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Processor Architecture for Radically Improved Performance and Energy Efficiency on Sparse Machine Learning",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rick Schwerdtfeger",
 "awd_eff_date": "2018-01-01",
 "awd_exp_date": "2018-09-30",
 "tot_intn_awd_amt": 224643.0,
 "awd_amount": 224643.0,
 "awd_min_amd_letter_date": "2017-12-21",
 "awd_max_amd_letter_date": "2017-12-21",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to expand the capability of modern computer systems to execute machine learning applications and enable new uses of machine learning in the everyday lives of people.\u00a0\u00a0Due to widespread success, machine learning is being applied to automate tasks across most modern businesses.\u00a0 However, the increasingly computationally complex and data intensive nature of these new problems is rapidly increasing. The scale of current problems stress the computational abilities and memory requirements of modern systems.\u00a0 The technology to be developed under this Phase I project, will enable computer systems with radically higher performance and energy-efficiency while performing these machine learning tasks.\u00a0 Enabling efficient machine learning will enable complex tasks to fit within modern mobile devices while simultaneously enabling computers within datacenters to solve increasingly large problems.\u00a0 Finally, as businesses rush to deploy hardware for machine learning, the underlying algorithms and techniques are rapidly evolving.\u00a0 The technology to be developed is highly adaptable, enabling high efficiency on current machine learning techniques while mitigating risks for businesses likely to adapt new machine learning algorithms.\r\n\r\nThe proposed project introduces a new hardware architecture for the execution of data and control intensive machine learning workloads.\u00a0 As machine learning has expanded in use, increasing data sizes have brought about the use of compressed data representations.\u00a0 However, modern computational devices like microprocessors or graphics processors are highly inefficient when working on problems using these compressed representations due to irregular control and data access patterns.\u00a0 This Phase I project introduces an adaptable architecture that excels at irregular computing and can dynamically re-allocate resources to hasten execution.\u00a0 To demonstrate the capabilities of the new architecture, key execution kernels from modern machine learning applications will be adapted and developed to operate on compressed representations.\u00a0 An existing simulation infrastructure will be extended to model key hardware requirements and gather performance estimations of the newly proposed hardware architectures.\u00a0 Preliminary estimates demonstrate that multiple factors of improvement in energy efficiency and performance are expected across the key operations of machine learning applications.\u00a0",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mitchell",
   "pi_last_name": "Hayenga",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Mitchell B Hayenga",
   "pi_email_addr": "mitch.hayenga@gmail.com",
   "nsf_id": "000749099",
   "pi_start_date": "2017-12-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Revolution Computing Incorporated",
  "inst_street_address": "9308 Springwood Drive",
  "inst_street_address_2": "",
  "inst_city_name": "Austin",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9794504469",
  "inst_zip_code": "787502940",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": "F4QQLMJ9CR63"
 },
 "perf_inst": {
  "perf_inst_name": "Revolution Computing Incorporated",
  "perf_str_addr": "9308 Springwood Drive",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787502940",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "8035",
   "pgm_ref_txt": "Hardware Devices"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 224643.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Machine learning algorithms, including neural networks, have proven useful in solving problems from a broad range of applications such as image recognition, machine translation, and targeted marketing. Due to this widespread success, deep neural networks are being used to address increasingly computationally complex and data intensive tasks. The scale of these tasks is now pushing the computational abilities and memory requirements of modern systems.&nbsp; To solve these increasingly large problems, the use of sparse data representations has emerged. Sparse representations exploit high frequencies of zeros within datasets to drastically reduce the memory footprint and operations required for many operations.&nbsp; However, sparse representations suffer from a lack of regularity in data access patterns and program control flow, resulting in poor compute efficiency on both modern microprocessors and graphic processors.&nbsp; Similarly, alternative machine learning algorithms, including random forests and other tree-based approaches, suffer from intense and unpredictable control flow. The Revolution Architecture represents a novel way to construct a general-purpose microprocessor that excels on irregular and heavily-threaded sparse machine learning applications in both mobile and server environments.</p>\n<p>&nbsp;</p>\n<p><span>Sparse machine learning is a prominent area of industrial and academic research with many new algorithms and customized accelerators recently being proposed.&nbsp; Revolution Computing&rsquo;s value proposition is the ability to use a new general purpose microprocessor design to accelerate these applications by over three times and at twice the energy efficiency within an equivalent silicon area of conventional processor designs.&nbsp; Unlike dedicated accelerators, the Revolution Architecture runs the commercial RISC-V instruction set architecture giving it the ability to execute conventional programs and readily adapt as machine learning algorithms evolve.&nbsp; This general-purpose design greatly mitigates customer risk by allowing large algorithmic changes not possible with dedicated accelerators that only represent the current state of knowledge in the rapidly progressing field of machine learning.</span></p>\n<p>&nbsp;</p>\n<p>Although modern microprocessors benefit from decades of optimization, their fundamental structures were defined in a time when transistors were expensive and energy efficiency was not a primary design constraint. The Revolution Architecture reinvents the microprocessor by relying on a principle of in-place execution. Rather than delivering instructions and operands to a small set of heavily-pipelined execution units and constantly evaluating complex scheduling logic, instructions retain simple dedicated execution resources and simply wait for their operands to arrive.&nbsp; The use of in-place execution removes the need for commonly employed register renaming techniques, allowing dedicated decoded instruction caches to be placed directly alongside execution units.&nbsp; For codes like sparse machine learning with few, but unpredictable control flow paths these caches enable greater energy efficiency, the ability to service the instruction bandwidth required by many threads, and single-cycle branch misprediction recovery.</p>\n<p>&nbsp;</p>\n<p>The use of machine learning to automate tasks and solve complicated problems is spreading rapidly to business and consumer applications.&nbsp; However, for these applications to scale and address larger problems, more efficient means must be found to accelerate machine learning as the performance of modern compute is not increasing as it once was.&nbsp; The proposed architecture will enable these larger problems to be efficiently solved using fewer resources, resulting in energy and monetary savings.</p>\n<p>&nbsp;</p>\n<p>Under this NSF SBIR Phase I grant, Revolulution Computing sucessfully demonstrated through software emulation and hardware prototyping, that the Revolution Architecture was capable of potentially 17x performance-per-area speedups over conventional microprocessor designs.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/17/2018<br>\n\t\t\t\t\tModified by: Mitchell&nbsp;B&nbsp;Hayenga</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMachine learning algorithms, including neural networks, have proven useful in solving problems from a broad range of applications such as image recognition, machine translation, and targeted marketing. Due to this widespread success, deep neural networks are being used to address increasingly computationally complex and data intensive tasks. The scale of these tasks is now pushing the computational abilities and memory requirements of modern systems.  To solve these increasingly large problems, the use of sparse data representations has emerged. Sparse representations exploit high frequencies of zeros within datasets to drastically reduce the memory footprint and operations required for many operations.  However, sparse representations suffer from a lack of regularity in data access patterns and program control flow, resulting in poor compute efficiency on both modern microprocessors and graphic processors.  Similarly, alternative machine learning algorithms, including random forests and other tree-based approaches, suffer from intense and unpredictable control flow. The Revolution Architecture represents a novel way to construct a general-purpose microprocessor that excels on irregular and heavily-threaded sparse machine learning applications in both mobile and server environments.\n\n \n\nSparse machine learning is a prominent area of industrial and academic research with many new algorithms and customized accelerators recently being proposed.  Revolution Computing?s value proposition is the ability to use a new general purpose microprocessor design to accelerate these applications by over three times and at twice the energy efficiency within an equivalent silicon area of conventional processor designs.  Unlike dedicated accelerators, the Revolution Architecture runs the commercial RISC-V instruction set architecture giving it the ability to execute conventional programs and readily adapt as machine learning algorithms evolve.  This general-purpose design greatly mitigates customer risk by allowing large algorithmic changes not possible with dedicated accelerators that only represent the current state of knowledge in the rapidly progressing field of machine learning.\n\n \n\nAlthough modern microprocessors benefit from decades of optimization, their fundamental structures were defined in a time when transistors were expensive and energy efficiency was not a primary design constraint. The Revolution Architecture reinvents the microprocessor by relying on a principle of in-place execution. Rather than delivering instructions and operands to a small set of heavily-pipelined execution units and constantly evaluating complex scheduling logic, instructions retain simple dedicated execution resources and simply wait for their operands to arrive.  The use of in-place execution removes the need for commonly employed register renaming techniques, allowing dedicated decoded instruction caches to be placed directly alongside execution units.  For codes like sparse machine learning with few, but unpredictable control flow paths these caches enable greater energy efficiency, the ability to service the instruction bandwidth required by many threads, and single-cycle branch misprediction recovery.\n\n \n\nThe use of machine learning to automate tasks and solve complicated problems is spreading rapidly to business and consumer applications.  However, for these applications to scale and address larger problems, more efficient means must be found to accelerate machine learning as the performance of modern compute is not increasing as it once was.  The proposed architecture will enable these larger problems to be efficiently solved using fewer resources, resulting in energy and monetary savings.\n\n \n\nUnder this NSF SBIR Phase I grant, Revolulution Computing sucessfully demonstrated through software emulation and hardware prototyping, that the Revolution Architecture was capable of potentially 17x performance-per-area speedups over conventional microprocessor designs.\n\n\t\t\t\t\tLast Modified: 11/17/2018\n\n\t\t\t\t\tSubmitted by: Mitchell B Hayenga"
 }
}