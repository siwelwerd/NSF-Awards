{
 "awd_id": "1816500",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NeTS: Small:  Dynamic Predictive Streaming of 360 Degree Video",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Darleen Fisher",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 499901.0,
 "awd_amount": 515901.0,
 "awd_min_amd_letter_date": "2018-08-21",
 "awd_max_amd_letter_date": "2021-04-23",
 "awd_abstract_narration": "Virtual Reality (VR) and Augmented Reality (AR) applications are projected to be the next wave of \"Killer Apps\" in the future Internet. VR/AR applications facilitate vivid immersive virtual and augmented reality experience and create tremendous new opportunities in many domains, including education, business, healthcare, and entertainment, etc. Many VR/AR applications involve streaming of 360-degree video scenes. Compared with the traditional video streaming, 360-degree video streaming requires much higher network bandwidth and much lower packet delivery latency, and user's quality of experience is highly sensitive to the dynamics in both network environment and user viewing behaviors. Addressing these unique challenges, this project will develop novel 360-degree video coding and delivery solutions to enable high quality interactive, on-demand, and live video streaming.   \r\n\r\nThe project includes several research thrusts to enable novel joint coding-and-delivery solutions for high quality and robust 360-degree video streaming. For interactive streaming, novel Field-of-View (FoV) adaptive coding structure will be designed to achieve low encoding and decoding latency. Realtime joint optimization of streaming rate adaption and video coding bits allocation based on the predicted FoV will be studied to maximize the rendered video quality.  For on-demand streaming, a two-tier video coding and delivery framework will be developed, and the rate allocation and video chunk scheduling between the two tiers will be investigated to strike the desired balance between the rendered video quality and streaming robustness. To facilitate predictive coding and delivery, the project will develop effective algorithms for predicting user FoVs, based on the past FoV trajectory and the audio and visual content through deep learning architectures. Personalized FoV prediction based on other users' view trajectories will also be explored under the framework of recommender systems. Fully-functional 360 video streaming prototypes will be developed and tested in controlled and real network environments to validate and improve the new designs. If successful, the research will lead to new theory and designs for 360-degree video coding and help enable the wide-spread deployment of high-quality and robust 360-video streaming systems. The research findings will be made available through publications, talks, open protocols, and open-source codes, allowing a multitude of developers, researchers, and companies to evolve 360-video streaming. The project will also create valuable research opportunities for graduate and undergraduate students, especially women and minority students. Interactions with industry will be facilitated through workshops and several research centers at the New York University.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yong",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yong Liu",
   "pi_email_addr": "yongliu@nyu.edu",
   "nsf_id": "000105707",
   "pi_start_date": "2018-08-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yao",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yao Wang",
   "pi_email_addr": "yw523@nyu.edu",
   "nsf_id": "000467592",
   "pi_start_date": "2018-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "2 MetroTech Center",
  "perf_city_name": "Brooklyn",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "112013848",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "NY07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "736300",
   "pgm_ele_name": "Networking Technology and Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499901.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>360 degree video streaming requires much higher network bandwidth and much lower packet delivery latency, and user?s quality of experience is highly sensitive to the dynamics in both network environment and user viewing behaviors. Meanwhile, in 360 degree video streaming, a user only watches a video scene within a field of view (FoV) centered in a certain direction.</p>\n<p>&nbsp;Intellectual Merits Outcomes:</p>\n<p>&nbsp;The major finding of this project is that predictive FoV-adaptive coding and streaming of 360 degree video that adapt the number of bits used to code and deliver 360 degree video based on user FoV can significantly reduce the bandwidth consumption and delivery latency of 360 degree video while maintaining the streaming robustness.</p>\n<p>&nbsp;1. We first demonstrated that a user's FoV in the near future (ranging from tens of milli-seconds to ten seconds) can be accurately predicted using deep learning models that consider the target user's past FoV trajectory as well as the FoV trajectories of other users watching the same video. We also developed deep learning models to predict the available mobile network bandwidth in the near future with high accuracy.</p>\n<p>&nbsp;2. For on-demand 360 degree video streaming, we developed a novel two-tier coding-streaming solution, where the base tier delivers the entire 360 degree span at a lower quality with a long prefetching buffer, and the enhancement tier delivers the predicted FoV at a higher quality using a short buffer. The base tier provides robustness to both network bandwidth variations and FoV prediction errors. The enhancement tier improves the video quality if it is delivered in time and FoV prediction is accurate. We demonstrated that the proposed two-tier systems can achieve a high-level of Quality-of-Experience (QoE) in the face of network bandwidth and user FoV dynamics.</p>\n<p>&nbsp;3. For live streaming of 360 degree video, we propose to organize users watching the same live event into a ?streaming flock? by manipulating their playback latencies. We demonstrated that&nbsp; user flocking facilitates collaborative FoV prediction where the actual FoV information of users in the front of the flock are utilized to predict FoV of users behind them. We further designed a network condition aware flocking strategy to reduce the video freeze and increase the chance for collaborative FoV prediction on all users. Flocking also facilitates caching as video tiles downloaded by the front users can be cached by an edge server to serve the users at the back of the flock, thereby reducing the traffic in the core network.</p>\n<p>&nbsp;4. In 360 degree video interactive streaming, it is critical to minimize the end-to-end frame delay.&nbsp;&nbsp; We designed a frame-level FoV-adaptive inter-coding structure in which regions in a frame that are in or near the predicted FoV are coded using temporal and spatial prediction, while a small rotating region is coded with spatial prediction only. This rotating intra region periodically refreshes the entire frame, thereby providing robustness to both FoV prediction errors and frame losses due to transmission errors. The system adapts the sizes and rates of different regions for each video segment to maximize the rendered video quality under the predicted bandwidth constraint and FoV prediction accuracy.</p>\n<p>&nbsp;5. Through some preliminary studies, we also demonstrated that our proposed FoV-adaptive coding and streaming designs can be extended to volumetric point cloud videos.</p>\n<p>&nbsp;</p>\n<p>Broader Impact Outcomes:</p>\n<p>&nbsp;1. Four PhD students and four master students have participated in this research. They all gained valuable experience to use optimization and machine learning tools to solve practical problems involving video streaming and user behaviors. They are now employed by top IT companies in the USA. Two graduated PhD students are working as video coding and streaming research scientists in Apple and ByteDance, respectively.&nbsp;</p>\n<p>&nbsp;2. Two undergraduate students joined the project through REU supplement support. Both students gained valuable hands-on research experience.&nbsp;&nbsp;</p>\n<p>3. Our results were published in top-tier conferences and journal. We have presented our results through conferences and seminar talks to academia and industry. PI Wang delivered a keynote on 360 video streaming in ACM MMSys 2020.&nbsp;</p>\n<p>4. We developed a web-based 360 video streaming system which allows users to change their view angles using mouse on computers, and finger swipes/phone rotations on cell phone (https://www.360videodemo.com). Our collected mobile bandwidth traces were shared publicly at: https://github.com/NYU-METS/Main</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/17/2023<br>\n\t\t\t\t\tModified by: Yong&nbsp;Liu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n360 degree video streaming requires much higher network bandwidth and much lower packet delivery latency, and user?s quality of experience is highly sensitive to the dynamics in both network environment and user viewing behaviors. Meanwhile, in 360 degree video streaming, a user only watches a video scene within a field of view (FoV) centered in a certain direction.\n\n Intellectual Merits Outcomes:\n\n The major finding of this project is that predictive FoV-adaptive coding and streaming of 360 degree video that adapt the number of bits used to code and deliver 360 degree video based on user FoV can significantly reduce the bandwidth consumption and delivery latency of 360 degree video while maintaining the streaming robustness.\n\n 1. We first demonstrated that a user's FoV in the near future (ranging from tens of milli-seconds to ten seconds) can be accurately predicted using deep learning models that consider the target user's past FoV trajectory as well as the FoV trajectories of other users watching the same video. We also developed deep learning models to predict the available mobile network bandwidth in the near future with high accuracy.\n\n 2. For on-demand 360 degree video streaming, we developed a novel two-tier coding-streaming solution, where the base tier delivers the entire 360 degree span at a lower quality with a long prefetching buffer, and the enhancement tier delivers the predicted FoV at a higher quality using a short buffer. The base tier provides robustness to both network bandwidth variations and FoV prediction errors. The enhancement tier improves the video quality if it is delivered in time and FoV prediction is accurate. We demonstrated that the proposed two-tier systems can achieve a high-level of Quality-of-Experience (QoE) in the face of network bandwidth and user FoV dynamics.\n\n 3. For live streaming of 360 degree video, we propose to organize users watching the same live event into a ?streaming flock? by manipulating their playback latencies. We demonstrated that  user flocking facilitates collaborative FoV prediction where the actual FoV information of users in the front of the flock are utilized to predict FoV of users behind them. We further designed a network condition aware flocking strategy to reduce the video freeze and increase the chance for collaborative FoV prediction on all users. Flocking also facilitates caching as video tiles downloaded by the front users can be cached by an edge server to serve the users at the back of the flock, thereby reducing the traffic in the core network.\n\n 4. In 360 degree video interactive streaming, it is critical to minimize the end-to-end frame delay.   We designed a frame-level FoV-adaptive inter-coding structure in which regions in a frame that are in or near the predicted FoV are coded using temporal and spatial prediction, while a small rotating region is coded with spatial prediction only. This rotating intra region periodically refreshes the entire frame, thereby providing robustness to both FoV prediction errors and frame losses due to transmission errors. The system adapts the sizes and rates of different regions for each video segment to maximize the rendered video quality under the predicted bandwidth constraint and FoV prediction accuracy.\n\n 5. Through some preliminary studies, we also demonstrated that our proposed FoV-adaptive coding and streaming designs can be extended to volumetric point cloud videos.\n\n \n\nBroader Impact Outcomes:\n\n 1. Four PhD students and four master students have participated in this research. They all gained valuable experience to use optimization and machine learning tools to solve practical problems involving video streaming and user behaviors. They are now employed by top IT companies in the USA. Two graduated PhD students are working as video coding and streaming research scientists in Apple and ByteDance, respectively. \n\n 2. Two undergraduate students joined the project through REU supplement support. Both students gained valuable hands-on research experience.  \n\n3. Our results were published in top-tier conferences and journal. We have presented our results through conferences and seminar talks to academia and industry. PI Wang delivered a keynote on 360 video streaming in ACM MMSys 2020. \n\n4. We developed a web-based 360 video streaming system which allows users to change their view angles using mouse on computers, and finger swipes/phone rotations on cell phone (https://www.360videodemo.com). Our collected mobile bandwidth traces were shared publicly at: https://github.com/NYU-METS/Main\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/17/2023\n\n\t\t\t\t\tSubmitted by: Yong Liu"
 }
}