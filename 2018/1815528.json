{
 "awd_id": "1815528",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Reliable and Generalizable Neural Search Engine Architectures",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 499659.0,
 "awd_amount": 499659.0,
 "awd_min_amd_letter_date": "2018-08-02",
 "awd_max_amd_letter_date": "2018-08-02",
 "awd_abstract_narration": "Scientists need to frequently search the scientific literature on the subject they are studying.  Despite the availability of papers and citation databases on the Web, the enormous growth of scientific publications in all disciplines makes this a daunting task.  Traditional commerical search engines, such as Google, often fail to include the most important documents in the first few pages of returned results - in other words, they do not do a good enough job of ranking scientific papers for a given query.  Recently, new algorithms for search based on artificial neural network techniques have emerged as an alternative to traditional search architectures. These new neural search architectures are more accurate, but must be first trained with millions of example queries and answers from user interactions; this limits their usefulness for many tasks.  This project will overcome this problem by developing new methods of training neural search engines that reduce the need for training examples by integrating explicit knowledge resources for a given discipline.  The new techniques will be disseminated in freely available open-source search software for both university and industry researchers, thus broadly benefiting scientific advancement. In addition, the project will broaden participation by under-represented groups by creating research opportunities for female and undergraduate students and technology transfer opportunities for industry.\r\n\r\nThis research develops new methods of training neural ranking architectures when a massive amount of training data is not available for the target application; integrates external knowledge resources to provide more information for making accurate ranking decisions; and applies the architecture to a domain-specific search task such as retrieving tabular data from scientific documents. This collection of problems is chosen to increase the practicality of neural ranking architectures outside of high-traffic commercial search environments, and to investigate and exploit the strengths of neural ranking architectures at using attention mechanisms to manage evidence, soft-matching across different types of evidence, and learning sophisticated nonlinear decision models. This research furthers the development of neural ranking architectures that are generally applicable and more reliable than current systems due to their ability to integrate a broader range of evidence in a predictable manner. Neural ranking architectures have generated much excitement and skepticism during the last several years. This research extends a recently-developed neural ranking system that is already able to beat strong learning-to-rank systems under specific conditions. It addresses one of the main obstacles to wider use of these models -- the availability of large amounts of training data. It integrates information from external semi-structured knowledge resources, because such information is effective in other ranking architectures and because it is likely to benefit from how neural ranking architectures manage and use diverse evidence of varying quality. Finally, it stress tests the architecture by applying it to a domain-specific task such as table retrieval from scientific documents, that requires the search engine to use several parts of the document selectively, rather than the entire document. These activities are designed to produce a neural ranking architecture capable of managing diverse evidence and document structure so as to provide greater knowledge about the particular strengths and weaknesses of neural ranking architectures. \r\n\r\nThis research develops new methods of training neural ranking architectures when a massive amount of training data is not available for the target application; integrates external knowledge resources to provide more information for making accurate ranking decisions; and applies the architecture to a domain-specific search task such as retrieving tabular data from scientific documents. This collection of problems is chosen to increase the practicality of neural ranking architectures outside of high-traffic commercial search environments, and to investigate and exploit the strengths of neural ranking architectures at using attention mechanisms to manage evidence, soft-matching across different types of evidence, and learning sophisticated nonlinear decision models. This research furthers the development of neural ranking architectures that are generally applicable and more reliable than current systems due to their ability to integrate a broader range of evidence in a predictable manner. Neural ranking architectures have generated much excitement and skepticism during the last several years. This research extends a recently-developed neural ranking system that is already able to beat strong learning-to-rank systems under specific conditions. It addresses one of the main obstacles to wider use of these models -- the availability of large amounts of training data. It integrates information from external semi-structured knowledge resources, because such information is effective in other ranking architectures and because it is likely to benefit from how neural ranking architectures manage and use diverse evidence of varying quality. Finally, it stress tests the architecture by applying it to a domain-specific task such as table retrieval from scientific documents, that requires the search engine to use several parts of the document selectively, rather than the entire document. These activities are designed to produce a neural ranking architecture capable of managing diverse evidence and document structure so as to provide greater knowledge about the particular strengths and weaknesses of neural ranking architectures. The project website (http://www.cs.cmu.edu/~callan/Projects/IIS-1815528/)  describes recent activities and provides access to research publications, experimental results, datasets, and open-sources software produced by the project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jamie",
   "pi_last_name": "Callan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jamie Callan",
   "pi_email_addr": "callan@cs.cmu.edu",
   "nsf_id": "000173762",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499659.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Search engines based on neural network techniques have emerged as an alternative to traditional search engine architectures. These new <em>neural ranking</em> <em>architectures</em> enable more sophisticated reasoning about the meaning of a text than older, <em>lexical-match</em> systems. When this project began, neural ranking architectures were just beginning to be better than heuristic and feature-based lexical-match models when designed carefully and trained with massive amounts of training data, which limited their use to organizations with highly trained personnel and large amounts of search traffic. This project was designed to further the development of neural ranking architectures that are broadly applicable and more reliable than previous systems. Research questions were chosen to increase the practicality of neural ranking architectures outside of high-traffic commercial search environments and to investigate and exploit the strengths of neural ranking architectures.</p>\n<p>The project produced a broad range of work that can be organized into four general area: Design of neural ranking architectures; training of neural ranking architectures; use of neural text analysis to upgrade lexical-match models, which are still widely used; application of neural architectures to a broader range of information retrieval tasks; and development of tools and techniques that make neural models easier to use or to use efficiently.</p>\n<p>The research on neural ranking architecture design included some of the earliest work on reranking with a large language model, which became a standard technique; investigation of several types of modular architecture to improve efficiency or shift computation from online to offline; and investigation of integrated lexical-match and neural ranking architectures that exploited the strengths of each approach.</p>\n<p>The research on training neural ranking architectures developed unsupervised methods of using document structure to train models; investigated the use of complex (slower) models to train simpler (faster) models (&ldquo;distillation&rdquo;); investigated the value of different types of supervised and unsupervised training data; developed Condenser and coCondenser, two techniques specifically designed to train effective vector-based (&ldquo;dense&rdquo;) rankers; and developed Hyde, a technique that exploits the hallucination capabilities of large language models to bridge representational gaps between queries and documents while still returning actual documents.</p>\n<p>Lexical-match search engines are still widely used and far less expensive to operate than neural ranking architectures, thus there is great value in improving their accuracy. The research on using neural models to improve lexical-match search developed several methods of using transformer models to improve the term weighting that powers lexical-match systems; added context-specific term representations that enabled better discrimination of different uses of the same term when matching queries to documents (e.g., apple, which has several common meanings); and enriched the vocabularies of queries and documents to reduce vocabulary mismatch.</p>\n<p>Although text retrieval was the focus of the project, it showed that with modest modifications, a neural ranking architecture could be applied to retrieval of engineering diagrams; neural models could be used to summarize tables of numbers for retrieval by a conversational assistant; and that answering certain kinds of logical and mathematical reasoning questions can be done using an off-the-shelf large language model to decompose an information need into a series of procedural problems that can be accomplished with small programs that it also writes.</p>\n<p>The project also produced Tevatron, a toolkit that lowers the barrier-to-entry for work on vector-based (&ldquo;dense&rdquo;) retrieval systems; and GradCache, a technique that allows graphical processing units (GPUs) with small amounts of memory to produce the same results as GPUs with large amounts of memory with only a modest increase in computation time, enabling organizations with modest resources to reproduce work done by organizations with the latest hardware.</p>\n<p>&nbsp;The results of the project were disseminated in a Ph.D. dissertation (and eventually a second Ph.D. dissertation), twenty-five peer-reviewed publications, and an open-source software toolkit. Many of the research artifacts (datasets, intermediate results, software) are distributed from project websites to enable other researchers and practitioners in industry to build upon our work. Several of the project&rsquo;s techniques (e.g., Condenser/coCondenser, Hyde, GradCache) have been adopted widely. Several others inspired research by other organizations that produced more effective methods.The project also partially supported the graduate education of four Computer Science students and provided state-of-the-art research experience to several other graduate and undergraduate students who went on to graduate school or jobs at American companies.</p>\n<p>Text search is one of the most widely used Computer Science technologies. Improving the effectiveness of full-text search engines affects a worldwide population. This project helped to make neural ranking architectures accessible to broad range of organizations, advanced the state-of-the-art in directions not being explored by commercial web search companies, and introduced the use of neural text analysis for upgrading the effectiveness of lexical-match search engines, which still make up the majority of deployed search engines.</p><br>\n<p>\n Last Modified: 12/15/2023<br>\nModified by: Jamie&nbsp;Callan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nSearch engines based on neural network techniques have emerged as an alternative to traditional search engine architectures. These new neural ranking architectures enable more sophisticated reasoning about the meaning of a text than older, lexical-match systems. When this project began, neural ranking architectures were just beginning to be better than heuristic and feature-based lexical-match models when designed carefully and trained with massive amounts of training data, which limited their use to organizations with highly trained personnel and large amounts of search traffic. This project was designed to further the development of neural ranking architectures that are broadly applicable and more reliable than previous systems. Research questions were chosen to increase the practicality of neural ranking architectures outside of high-traffic commercial search environments and to investigate and exploit the strengths of neural ranking architectures.\n\n\nThe project produced a broad range of work that can be organized into four general area: Design of neural ranking architectures; training of neural ranking architectures; use of neural text analysis to upgrade lexical-match models, which are still widely used; application of neural architectures to a broader range of information retrieval tasks; and development of tools and techniques that make neural models easier to use or to use efficiently.\n\n\nThe research on neural ranking architecture design included some of the earliest work on reranking with a large language model, which became a standard technique; investigation of several types of modular architecture to improve efficiency or shift computation from online to offline; and investigation of integrated lexical-match and neural ranking architectures that exploited the strengths of each approach.\n\n\nThe research on training neural ranking architectures developed unsupervised methods of using document structure to train models; investigated the use of complex (slower) models to train simpler (faster) models (distillation); investigated the value of different types of supervised and unsupervised training data; developed Condenser and coCondenser, two techniques specifically designed to train effective vector-based (dense) rankers; and developed Hyde, a technique that exploits the hallucination capabilities of large language models to bridge representational gaps between queries and documents while still returning actual documents.\n\n\nLexical-match search engines are still widely used and far less expensive to operate than neural ranking architectures, thus there is great value in improving their accuracy. The research on using neural models to improve lexical-match search developed several methods of using transformer models to improve the term weighting that powers lexical-match systems; added context-specific term representations that enabled better discrimination of different uses of the same term when matching queries to documents (e.g., apple, which has several common meanings); and enriched the vocabularies of queries and documents to reduce vocabulary mismatch.\n\n\nAlthough text retrieval was the focus of the project, it showed that with modest modifications, a neural ranking architecture could be applied to retrieval of engineering diagrams; neural models could be used to summarize tables of numbers for retrieval by a conversational assistant; and that answering certain kinds of logical and mathematical reasoning questions can be done using an off-the-shelf large language model to decompose an information need into a series of procedural problems that can be accomplished with small programs that it also writes.\n\n\nThe project also produced Tevatron, a toolkit that lowers the barrier-to-entry for work on vector-based (dense) retrieval systems; and GradCache, a technique that allows graphical processing units (GPUs) with small amounts of memory to produce the same results as GPUs with large amounts of memory with only a modest increase in computation time, enabling organizations with modest resources to reproduce work done by organizations with the latest hardware.\n\n\nThe results of the project were disseminated in a Ph.D. dissertation (and eventually a second Ph.D. dissertation), twenty-five peer-reviewed publications, and an open-source software toolkit. Many of the research artifacts (datasets, intermediate results, software) are distributed from project websites to enable other researchers and practitioners in industry to build upon our work. Several of the projects techniques (e.g., Condenser/coCondenser, Hyde, GradCache) have been adopted widely. Several others inspired research by other organizations that produced more effective methods.The project also partially supported the graduate education of four Computer Science students and provided state-of-the-art research experience to several other graduate and undergraduate students who went on to graduate school or jobs at American companies.\n\n\nText search is one of the most widely used Computer Science technologies. Improving the effectiveness of full-text search engines affects a worldwide population. This project helped to make neural ranking architectures accessible to broad range of organizations, advanced the state-of-the-art in directions not being explored by commercial web search companies, and introduced the use of neural text analysis for upgrading the effectiveness of lexical-match search engines, which still make up the majority of deployed search engines.\t\t\t\t\tLast Modified: 12/15/2023\n\n\t\t\t\t\tSubmitted by: JamieCallan\n"
 }
}