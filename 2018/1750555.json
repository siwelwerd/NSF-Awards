{
 "awd_id": "1750555",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Stable Foundations for Reliable Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2018-02-01",
 "awd_exp_date": "2023-01-31",
 "tot_intn_awd_amt": 499434.0,
 "awd_amount": 499434.0,
 "awd_min_amd_letter_date": "2018-01-08",
 "awd_max_amd_letter_date": "2021-09-14",
 "awd_abstract_narration": "Across all sciences, researchers hope to use algorithms and machine learning to derive reliable insights from data, but often research findings turn out to be false or hard to replicate. Indeed, assessing the validity of insights suggested by data is presently a difficult and error-prone task. Even in industry, where machine learning has fueled dramatic advances, more principled ways of benchmarking and improving the performance of a machine learning system would make a major difference. What often work best in practice are poorly understood heuristics, leading to much guesswork with varying results. This inscrutable behavior of machine learning also has repercussions on society at large as more and more people struggle with the implications of algorithmic decisions in their daily lives. Fairness, interpretability, and transparency have become major talking points as algorithms increasingly aid or replace human judgment.\r\n\r\nThe PI aims to build guiding theory alongside scalable algorithms that make the practice of machine learning more reliable, transparent, and aligned with societal values. Focusing on algorithmic stability as a unifying technical framework, this proposal targets several foundational challenges including the design of a robust methodology to address the reliability crisis in data science, a working theory for why and when large artificial neural networks train and generalize well, and a universal framework to reason about generalization in unsupervised learning as is presently lacking. A particular emphasis is on application domains of societal impact. The PI has long been invested in topics such as privacy, fairness, accountability and transparency in machine learning not only through academic publications, but also through workshops, mentorship, teaching, and interdisciplinary engagements.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Moritz",
   "pi_last_name": "Hardt",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Moritz Hardt",
   "pi_email_addr": "hardt@eecs.berkeley.edu",
   "nsf_id": "000747429",
   "pi_start_date": "2018-01-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "676 Soda Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947201776",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 90466.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 95197.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 100235.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 213536.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong id=\"docs-internal-guid-25a240a6-7fff-8662-7efa-69558e1306f6\" style=\"font-weight: normal;\"> </strong></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong id=\"docs-internal-guid-25a240a6-7fff-8662-7efa-69558e1306f6\" style=\"font-weight: normal;\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The central goal of this project was to strengthen the empirical and theoretical foundations of machine learning, aiming to make the practice of machine learning more reliable. The objectives included mitigating the replication crisis, ensuring societal goals such as fairness, and understanding difficult non-convex optimization problems as they arise in deep learning.</span></strong></p>\n<p><strong id=\"docs-internal-guid-25a240a6-7fff-8662-7efa-69558e1306f6\" style=\"font-weight: normal;\"> <br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The results under this grant made progress on all three central objectives. Moreover, since the beginning of the project, these objectives have only grown in importance. Machine learning has continued to grow its far-reaching societal impact with entirely new transformative applications in recent years. The proposal, as written in 2017, remains timely in all of its components.</span></p>\n<br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">More specifically, results from this grant include work on&nbsp;</span></p>\n<br /><ol style=\"margin-top: 0; margin-bottom: 0; padding-inline-start: 48px;\">\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\">\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">ensuring fairness in machine learning,</span></p>\n</li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\">\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">dealing with strategic behavior,</span></p>\n</li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\">\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">optimization and generalization in deep learning,</span></p>\n</li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\">\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">machine learning benchmarks and reliable evaluation.</span></p>\n</li>\n</ol><br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The outcomes span more than a dozen results, published in leading computer science conferences, as well as publicly available code where applicable. The grant supported numerous graduate student researchers at the University of California, Berkeley. The grant moreover partially supported the PI.&nbsp;</span></p>\n<br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The results of this grant also became the basis of new teaching materials, and found their way into two new textbooks:</span></p>\n<br /><ol style=\"margin-top: 0; margin-bottom: 0; padding-inline-start: 48px;\">\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\">\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Patterns, predictions, and actions: Foundations of machine learning. Princeton University Press, 2022. Hardt and Recht.</span></p>\n</li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\">\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Fairness and Machine Learning: Limitations and Opportunities. MIT Press, 2023. Barocas, Hardt, Narayanan.</span></p>\n</li>\n</ol><br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A scientific highlight of the work within this grant is a deep understanding of the problem of overfitting in machine learning benchmarks. Machine learning benchmarks form the centerpiece of the machine learning community and fueled significant progress over the last two decades. At the outset, machine learning benchmarks are based on the holdout method.&nbsp; A test set is put aside for evaluation purposes. Researchers and engineers train models on the training set and then evaluate, rank, and compare performance on the test set. ImageNet is one example of a central benchmark that fueled the deep learning revolution of the decade following 2012. A major concern with benchmarks was that participants might begin to overfit their models to the test set. That is, by repeated evaluation on the test set, researchers find ways to score higher on the test set without improving other external performance indicators. Many test sets have been used tens of thousands of times, making this concern a real possibility based on first principles. The theoretical and empirical work within this grant, however, identified several important reasons why the extent of overfitting to machine learning benchmarks is less severe than feared. Moreover, the work demonstrated this observation empirically in a broad meta study of machine learning competitions. These newly identified forces against overfitting are not purely statistical. They involve in intriguing ways both the behavior of the machine learning community, as well as the mind of the machine learning researcher. Nonetheless, we found elegant mathematical assumptions that capture these sociotechnical conditions. Under these assumptions, we provided new theory that shows the limited extent of overfitting.&nbsp;</span></p>\n<br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Another highlight includes work on strategic behavior in machine learning. Real-world applications often face the problem that participants respond strategically to a machine learning classifier, changing data in a way that improves their classification. The results under this grant contributed to a growing literature on the topic. Specifically, our work identified the important difference between gaming and self-improvement when it comes to strategic behavior. We used tools from causal modeling to formalize this distinction and work out some of its consequences.</span></p>\n<br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Fairness in machine learning was another central component of the grant. One new result demonstrated that a condition, considered a fairness criterion by some researchers, actually follows from unconstrained optimization. Another influential result proposed the study of fairness in a dynamic classification setting, where decisions have a long-term impact on a population. Here, it turns out that standard fairness interventions may cause harm relative to an unconstrained objective. These results make important progress on our understanding of fairness criteria and interventions. The grant also spanned several topics adjacent to fairness, such as explainability and welfare maximization.&nbsp;</span></p>\n<br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">These highlights demonstrate the broad scope of the grant proposal and its successful execution.</span></p>\n</strong><br class=\"Apple-interchange-newline\" /></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/03/2024<br>\nModified by: Moritz&nbsp;Hardt</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThe central goal of this project was to strengthen the empirical and theoretical foundations of machine learning, aiming to make the practice of machine learning more reliable. The objectives included mitigating the replication crisis, ensuring societal goals such as fairness, and understanding difficult non-convex optimization problems as they arise in deep learning.\n\n\n \n\n\n\nThe results under this grant made progress on all three central objectives. Moreover, since the beginning of the project, these objectives have only grown in importance. Machine learning has continued to grow its far-reaching societal impact with entirely new transformative applications in recent years. The proposal, as written in 2017, remains timely in all of its components.\n\n\n\n\nMore specifically, results from this grant include work on\n\n\n\n\n\nensuring fairness in machine learning,\n\n\n\n\ndealing with strategic behavior,\n\n\n\n\noptimization and generalization in deep learning,\n\n\n\n\nmachine learning benchmarks and reliable evaluation.\n\n\n\n\n\nThe outcomes span more than a dozen results, published in leading computer science conferences, as well as publicly available code where applicable. The grant supported numerous graduate student researchers at the University of California, Berkeley. The grant moreover partially supported the PI.\n\n\n\n\nThe results of this grant also became the basis of new teaching materials, and found their way into two new textbooks:\n\n\n\n\n\nPatterns, predictions, and actions: Foundations of machine learning. Princeton University Press, 2022. Hardt and Recht.\n\n\n\n\nFairness and Machine Learning: Limitations and Opportunities. MIT Press, 2023. Barocas, Hardt, Narayanan.\n\n\n\n\n\nA scientific highlight of the work within this grant is a deep understanding of the problem of overfitting in machine learning benchmarks. Machine learning benchmarks form the centerpiece of the machine learning community and fueled significant progress over the last two decades. At the outset, machine learning benchmarks are based on the holdout method. A test set is put aside for evaluation purposes. Researchers and engineers train models on the training set and then evaluate, rank, and compare performance on the test set. ImageNet is one example of a central benchmark that fueled the deep learning revolution of the decade following 2012. A major concern with benchmarks was that participants might begin to overfit their models to the test set. That is, by repeated evaluation on the test set, researchers find ways to score higher on the test set without improving other external performance indicators. Many test sets have been used tens of thousands of times, making this concern a real possibility based on first principles. The theoretical and empirical work within this grant, however, identified several important reasons why the extent of overfitting to machine learning benchmarks is less severe than feared. Moreover, the work demonstrated this observation empirically in a broad meta study of machine learning competitions. These newly identified forces against overfitting are not purely statistical. They involve in intriguing ways both the behavior of the machine learning community, as well as the mind of the machine learning researcher. Nonetheless, we found elegant mathematical assumptions that capture these sociotechnical conditions. Under these assumptions, we provided new theory that shows the limited extent of overfitting.\n\n\n\n\nAnother highlight includes work on strategic behavior in machine learning. Real-world applications often face the problem that participants respond strategically to a machine learning classifier, changing data in a way that improves their classification. The results under this grant contributed to a growing literature on the topic. Specifically, our work identified the important difference between gaming and self-improvement when it comes to strategic behavior. We used tools from causal modeling to formalize this distinction and work out some of its consequences.\n\n\n\n\nFairness in machine learning was another central component of the grant. One new result demonstrated that a condition, considered a fairness criterion by some researchers, actually follows from unconstrained optimization. Another influential result proposed the study of fairness in a dynamic classification setting, where decisions have a long-term impact on a population. Here, it turns out that standard fairness interventions may cause harm relative to an unconstrained objective. These results make important progress on our understanding of fairness criteria and interventions. The grant also spanned several topics adjacent to fairness, such as explainability and welfare maximization.\n\n\n\n\nThese highlights demonstrate the broad scope of the grant proposal and its successful execution.\n\n\n\n\n\t\t\t\t\tLast Modified: 04/03/2024\n\n\t\t\t\t\tSubmitted by: MoritzHardt\n"
 }
}