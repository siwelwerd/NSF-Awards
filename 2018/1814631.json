{
 "awd_id": "1814631",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI:Small: Dynamic and Statistical Based Invariants on Manifolds for Video Analysis",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-07-26",
 "awd_max_amd_letter_date": "2018-08-19",
 "awd_abstract_narration": "Computer vision systems can benefit society in many ways.  For example, spatially distributed vision sensors endowed with activity analysis capabilities can prevent crime, help optimize resource use in smart buildings, and give early warning of serious medical conditions.  The most powerful computer vision systems employ an approach called \"deep learning\", in which simulated networks of neurons transform the input video pixels into high-level concepts. For example, in the crime example, the high-level concept might be \"someone breaking into a building\".  A major impediment to building computer vision is that great expertise and trial-and-error is required for a programmer to design a neural network that can teach itself to recognize the goal concepts.  This project will reduce this barrier by creating a set of well-designed neural network modules, or \"layers\", that a programmer can snap together to build a working computer vision system.  Education is proactively integrated into this project, starting with STEM summer camps projects for urban middle school students and continuing at the college level with a multi-disciplinary program that uses the grand challenge of aware environments to link a full range of distinct subjects ranging from computer vision and machine learning to systems theory and optimization. At the graduate level, these activities are complemented by recruitment efforts that leverage the resources at Northeastern's University Program in Multicultural Engineering to broaden the participation of underrepresented groups in research. \r\n\r\nComputer vision has made tremendous progress in the era of deep learning. However, training of deep architectures requires learning the optimal value of a very large number of parameters through the numerical minimization of a non-convex loss function. While in practice, using stochastic gradient descent to solve this problem often \"works\", the analysis of what the network learned or why it failed to do so, remains an a-posteriori task requiring visualization tools to inspect which neurons are firing and possibly to look at intermediate results. This research seeks to address this issue by incorporating a set of structured layers to current deep architectures, designed using dynamical systems theory and statistics fundamentals, which capture spatio-temporal information across multiple scales. At its core is a unified vision, invariants on latent space manifolds as information encapsulators, that emphasizes robustness and computational complexity issues. Advantages of the proposed layers include the ability to easily understand what they learn, since they are based on first principles; shallower networks with a reduction of the number of parameters that needs to be learned due to the high expressive power of the new layers; and requiring less annotated data, by providing efficient ways to transfer knowledge between domains and to synthesize realistic data.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Octavia",
   "pi_last_name": "Camps",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Octavia I Camps",
   "pi_email_addr": "camps@ece.neu.edu",
   "nsf_id": "000429038",
   "pi_start_date": "2018-07-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mario",
   "pi_last_name": "Sznaier",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mario Sznaier",
   "pi_email_addr": "msznaier@coe.neu.edu",
   "nsf_id": "000428363",
   "pi_start_date": "2018-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave, 540-177",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Summary:</strong></p>\n<p class=\"p1\">This research developed a framework for a principled design of a class of deep networks, capable of exploiting dynamic and statistical invariants that capture spatio-temporal information across multiple scales. At its core is a unified vision, invariants on latent space manifolds as information encapsulators, that emphasizes robustness and computational complexity issues.</p>\n<p>&nbsp;</p>\n<p><strong>Intellectual Merit:&nbsp;</strong></p>\n<p>Undeniably, in the era of deep learning, computer vision has made tremendous progress. However, training of deep architectures requires learning a very large number of optimal parameters through the optimization of a non-convex loss function computed for a set of given data, which can have multiple modalities, including color, optical flow, range, both real and synthetically generated. Unfortunately, this minimization problem cannot be solved in closed form nor there are good convex relaxations available guaranteed to find&nbsp; globally optimal solutions. While in practice, using SGD often \"works'', the analysis of what the network learned or why failed to do so, remains an a-posteriori task requiring visualization tools to inspect which neurons are firing and possibly to look at intermediate results.</p>\n<p>This research addressed this issue by incorporating a set of structured layers to current deep architectures, which are designed using dynamical systems theory and statistics fundamentals. Advantages of using these layers include:</p>\n<p>(a)&nbsp;&nbsp; the ability to easily understand what they learn, since they are based on first principles;</p>\n<p>(b)&nbsp;&nbsp; shallower networks with a reduction of the number of parameters that needs to be learned due to the high expressive power of the new layers;</p>\n<p>(c)&nbsp;&nbsp; requiring less annotated data, by providing efficient ways to transfer knowledge between domains and to synthesize realistic data; and</p>\n<p>(d)&nbsp;&nbsp; broad applicability to efficiently solve a wide range of problems arising in the context of aware environments, including video semantic segmentation and activity detection and recognition.</p>\n<p><strong>Broader Impacts:</strong></p>\n<p class=\"p1\">The recent exponential growth in data collection capabilities and the accelerated progress in computer vision have the potential to profoundly impact society, with benefits ranging from safer, self aware environments, to sustainable use of scarce resources. A major impediment to realizing this vision stems from the current need for annotated data to train deep network architectures. This research reduced this need by developing a new class of layers with substantially enhanced information extraction and summarization capabilities, which can be used to learn in semi-supervised and unsupervised settings. Such a capability has the potential to significantly benefit society. Spatially distributed vision sensors endowed with activity analysis capabilities can prevent crime, help optimize resource use in smart buildings, and give early warning of serious medical conditions. In addition, the&nbsp; research carried out in this grant has the potential for significant cross-fertilization with other branches of engineering and applied mathematics. An example is the connection between high dimensional data analysis (computer vision, machine learning), dynamic invariants (systems theory) and computational linear algebra (optimization, matrix sketching).</p>\n<p class=\"p1\"><strong>Outcomes:</strong></p>\n<p class=\"p1\">We&nbsp; designed and implemented a dynamic atoms-based network utilizing a dictionary of Linear Time Invariant (LTI) low order systems for predicting future measurements. <span>Utilizing this network as a backbone, we introduced unsupervised neural networks </span>&nbsp;that use these dynamics atom-based networks as backbone, for efficient human pose estimation from video frames, and for cross-view and cross-subject action recognition from RGB videos. We introduced novel techniques for visually explaining variational autoencoders using gradient-based attention, anomaly localization in images and improving feature disentanglement. We also developed computationally tractable algorithms for subspace clustering and&nbsp;a fast two-view motion segmentation algorithm based on Christoffel polynomials. Our efforts included a self-supervised method for video decomposition into interpretable components, and Neural Constraint Inference (NCI) for unsupervised discovery of interactions between dynamic objects. Additional accomplishments involved the development of a framework for dynamic mode decomposition, an egocentric hand action recognition pipeline for mixed reality applications, a transformer-based architecture for action segmentation, and an alternative, computationally efficient approach for error-in-variables identification of switched affine models for action segmentation. Lastly, we contributed to the field with a difussion-based framework for solving image and video puzzles and contibuted to the theory of non-parametric identification of low order dynamic models.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/30/2023<br>\nModified by: Octavia&nbsp;I&nbsp;Camps</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1814631/1814631_10561798_1703982234597_FigureOutcomes--rgov-214x142.jpeg\" original=\"/por/images/Reports/POR/2023/1814631/1814631_10561798_1703982234597_FigureOutcomes--rgov-800width.jpeg\" title=\"Frugal Interpretable Dynamics Autoencoder\"><img src=\"/por/images/Reports/POR/2023/1814631/1814631_10561798_1703982234597_FigureOutcomes--rgov-66x44.jpeg\" alt=\"Frugal Interpretable Dynamics Autoencoder\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Frugal Interpretable Dynamics Autoencoder.</div>\n<div class=\"imageCredit\">Octavia Camps</div>\n<div class=\"imagePermisssions\">Royalty-free (restricted use - cannot be shared)</div>\n<div class=\"imageSubmitted\">Octavia&nbsp;I&nbsp;Camps\n<div class=\"imageTitle\">Frugal Interpretable Dynamics Autoencoder</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nSummary:\n\n\nThis research developed a framework for a principled design of a class of deep networks, capable of exploiting dynamic and statistical invariants that capture spatio-temporal information across multiple scales. At its core is a unified vision, invariants on latent space manifolds as information encapsulators, that emphasizes robustness and computational complexity issues.\n\n\n\n\n\nIntellectual Merit:\n\n\nUndeniably, in the era of deep learning, computer vision has made tremendous progress. However, training of deep architectures requires learning a very large number of optimal parameters through the optimization of a non-convex loss function computed for a set of given data, which can have multiple modalities, including color, optical flow, range, both real and synthetically generated. Unfortunately, this minimization problem cannot be solved in closed form nor there are good convex relaxations available guaranteed to find globally optimal solutions. While in practice, using SGD often \"works'', the analysis of what the network learned or why failed to do so, remains an a-posteriori task requiring visualization tools to inspect which neurons are firing and possibly to look at intermediate results.\n\n\nThis research addressed this issue by incorporating a set of structured layers to current deep architectures, which are designed using dynamical systems theory and statistics fundamentals. Advantages of using these layers include:\n\n\n(a) the ability to easily understand what they learn, since they are based on first principles;\n\n\n(b) shallower networks with a reduction of the number of parameters that needs to be learned due to the high expressive power of the new layers;\n\n\n(c) requiring less annotated data, by providing efficient ways to transfer knowledge between domains and to synthesize realistic data; and\n\n\n(d) broad applicability to efficiently solve a wide range of problems arising in the context of aware environments, including video semantic segmentation and activity detection and recognition.\n\n\nBroader Impacts:\n\n\nThe recent exponential growth in data collection capabilities and the accelerated progress in computer vision have the potential to profoundly impact society, with benefits ranging from safer, self aware environments, to sustainable use of scarce resources. A major impediment to realizing this vision stems from the current need for annotated data to train deep network architectures. This research reduced this need by developing a new class of layers with substantially enhanced information extraction and summarization capabilities, which can be used to learn in semi-supervised and unsupervised settings. Such a capability has the potential to significantly benefit society. Spatially distributed vision sensors endowed with activity analysis capabilities can prevent crime, help optimize resource use in smart buildings, and give early warning of serious medical conditions. In addition, the research carried out in this grant has the potential for significant cross-fertilization with other branches of engineering and applied mathematics. An example is the connection between high dimensional data analysis (computer vision, machine learning), dynamic invariants (systems theory) and computational linear algebra (optimization, matrix sketching).\n\n\nOutcomes:\n\n\nWe designed and implemented a dynamic atoms-based network utilizing a dictionary of Linear Time Invariant (LTI) low order systems for predicting future measurements. Utilizing this network as a backbone, we introduced unsupervised neural networks that use these dynamics atom-based networks as backbone, for efficient human pose estimation from video frames, and for cross-view and cross-subject action recognition from RGB videos. We introduced novel techniques for visually explaining variational autoencoders using gradient-based attention, anomaly localization in images and improving feature disentanglement. We also developed computationally tractable algorithms for subspace clustering anda fast two-view motion segmentation algorithm based on Christoffel polynomials. Our efforts included a self-supervised method for video decomposition into interpretable components, and Neural Constraint Inference (NCI) for unsupervised discovery of interactions between dynamic objects. Additional accomplishments involved the development of a framework for dynamic mode decomposition, an egocentric hand action recognition pipeline for mixed reality applications, a transformer-based architecture for action segmentation, and an alternative, computationally efficient approach for error-in-variables identification of switched affine models for action segmentation. Lastly, we contributed to the field with a difussion-based framework for solving image and video puzzles and contibuted to the theory of non-parametric identification of low order dynamic models.\n\n\n\t\t\t\t\tLast Modified: 12/30/2023\n\n\t\t\t\t\tSubmitted by: OctaviaICamps\n"
 }
}