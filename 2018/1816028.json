{
 "awd_id": "1816028",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: New Approaches to Decentralized Differential Privacy",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "James Joshi",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-08-24",
 "awd_max_amd_letter_date": "2018-08-24",
 "awd_abstract_narration": "Recently, Google and Apple have deployed large systems for differentially private collection and analysis of decentralized user data.  These systems use a local model of privacy in which no sensitive user data is collected. This local model enjoys many implementation advantages, but does not capture the most expressive private algorithms.  These more expressive private algorithms inherently require a central model of privacy, in which a trusted party agrees to collect the sensitive data and reveal only the outcome of some private algorithm. Finding a trusted aggregator can be problematic in many applications. This project specifically addresses this tension by using cryptography to design and implement scalable secure protocols for the statistical analysis of decentralized user data that combine the best features of the central and local models of privacy.\r\n\r\nThis project introduces and studies a novel intermediate model for decentralized privacy based on anonymous computation.  Protocols in this model enjoy the same simplicity as protocols in the local model of privacy, but bypass some of the limitations of protocols in the local model.  For functionalities that cannot be achieved in this intermediate model, this project designs tailored secure cryptographic to implement these functionalities without a trusted data collector, while overcoming the high communication, computation, and round complexity overheads of generic protocols. The investigators will involve  graduate and undergraduate students in this research.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jonathan",
   "pi_last_name": "Ullman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jonathan Ullman",
   "pi_email_addr": "jullman@ccs.neu.edu",
   "nsf_id": "000710812",
   "pi_start_date": "2018-08-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Abhi",
   "pi_last_name": "Shelat",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Abhi Shelat",
   "pi_email_addr": "a.shelat@northeastern.edu",
   "nsf_id": "000376004",
   "pi_start_date": "2018-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Avenue",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-708af84d-7fff-db7b-d503-ed984d0fdb30\">&nbsp;</span></p>\n<p dir=\"ltr\"><span>Large technology companies collect data from users at massive scale and use it for statistical analysis and machine learning, raising questions about how to protect users? privacy.&nbsp; While differential privacy has emerged as the de facto standard of individual privacy for statistics and machine learning, the fact that the data is decentralized across millions of devices makes it impossible to deploy many of the most expressive differentially private algorithms.&nbsp; These more expressive private algorithms inherently require a central model of privacy, in which a trusted party agrees to collect the sensitive data and reveal only the outcome of some private algorithm.&nbsp; Since the organization is often not viewed as a trusted party, many deployments use a more restrictive model called local differential privacy, where privacy is enforced entirely on the users? devices without a trusted party.&nbsp; While this type of differential privacy is desirable, it severely restricts the utility of the data collected.&nbsp; This project specifically addressed this tension by using cryptography to design and implement scalable secure protocols for the most expressive differentially private algorithms without relying on a trusted party.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The main outcome of this project was to introduce and develop the foundations of a new model for decentralized differential privacy based on anonymous computation and communication, called the shuffle model of differential privacy.&nbsp; The shuffle model abstracts the capabilities of scalable systems for secure multi-party computation and federated learning, and thus algorithms designed for this model can be implemented in practice on decentralized data without a trusted party.&nbsp; This project developed a suite of new algorithmic tools for the shuffle model, which enable organizations to analyze massive, decentralized collections of sensitive data with strong guarantees of individual privacy.&nbsp; The project also demonstrated inherent limitations of algorithms in this model, which demonstrated the need for more powerful forms of cryptography to solve certain problems.&nbsp; To this end, the project developed the most practical protocols to-date for implementing certain differentially private algorithms without a trusted curator.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/05/2023<br>\n\t\t\t\t\tModified by: Abhi&nbsp;Shelat</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nLarge technology companies collect data from users at massive scale and use it for statistical analysis and machine learning, raising questions about how to protect users? privacy.  While differential privacy has emerged as the de facto standard of individual privacy for statistics and machine learning, the fact that the data is decentralized across millions of devices makes it impossible to deploy many of the most expressive differentially private algorithms.  These more expressive private algorithms inherently require a central model of privacy, in which a trusted party agrees to collect the sensitive data and reveal only the outcome of some private algorithm.  Since the organization is often not viewed as a trusted party, many deployments use a more restrictive model called local differential privacy, where privacy is enforced entirely on the users? devices without a trusted party.  While this type of differential privacy is desirable, it severely restricts the utility of the data collected.  This project specifically addressed this tension by using cryptography to design and implement scalable secure protocols for the most expressive differentially private algorithms without relying on a trusted party.\n\n \nThe main outcome of this project was to introduce and develop the foundations of a new model for decentralized differential privacy based on anonymous computation and communication, called the shuffle model of differential privacy.  The shuffle model abstracts the capabilities of scalable systems for secure multi-party computation and federated learning, and thus algorithms designed for this model can be implemented in practice on decentralized data without a trusted party.  This project developed a suite of new algorithmic tools for the shuffle model, which enable organizations to analyze massive, decentralized collections of sensitive data with strong guarantees of individual privacy.  The project also demonstrated inherent limitations of algorithms in this model, which demonstrated the need for more powerful forms of cryptography to solve certain problems.  To this end, the project developed the most practical protocols to-date for implementing certain differentially private algorithms without a trusted curator.\n\n \n\n\t\t\t\t\tLast Modified: 01/05/2023\n\n\t\t\t\t\tSubmitted by: Abhi Shelat"
 }
}