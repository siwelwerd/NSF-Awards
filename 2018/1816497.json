{
 "awd_id": "1816497",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: Adversarial Learning via Modeling Interpretation",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-07-23",
 "awd_max_amd_letter_date": "2022-01-27",
 "awd_abstract_narration": "Machine learning (ML) models are increasingly important in society, with applications including malware detection, online content filtering and ranking, and self-driving cars. However, these models are vulnerable to adversaries attacking them by submitting incorrect or manipulated data with the goal of causing errors, causing potential harm to both the decisions the models make and the systems and people who rely on them. Further, many common ML models make decisions in ways that are hard for humans to understand, leading to calls to develop modeling techniques that make the models more explainable and interpretable. This project sits at the intersection of adversarial and explainable ML, with the key insight that as models become more interpretable in terms of both the individual decisions they make and the rules they use to distinguish between different decisions, this interpretability will likely provide additional information that can be used to both create and defend against adversarial attacks.  The overall project goal is to test this insight and contribute to both the security and data mining communities by developing an adversarial learning framework that leverages interpretability of ML models and results to both identify and mitigate the risks of adversarial attacks, especially in the context of big data. The project also contains a significant educational component, including incorporating the research into curriculum development and providing research opportunities to undergraduate and underrepresented students.\r\n\r\nThe project consists of three research thrusts. The first is to develop effective attacking strategies by analyzing modeling interpretation from three aspects including instance level, class level, and a specific group of deep neural networks. This enables more effective attacks to be initiated through understanding the underlying working mechanisms of ML models. The second thrust is to focus on developing defensive strategies to improve the robustness of ML models against these adversarial attacks. The proposed defensive strategies are aimed at the three major steps in a typical knowledge discovery pipeline including training data refinement, model architecture modification, and test data filtering. While existing efforts are based on continuously probing built systems and updating model parameters once prediction mistakes are discovered, the proposed work provides a proactive way to tackle the problem. The third thrust is to develop adversarial learning algorithms to deal with challenges and take advantage of opportunities brought by big data. Specifically, the developed adversarial attacking and defensive algorithms will deal with large-scale, heterogeneous, and relational data. This will enable the proposed algorithms to scale to real-world applications demonstrating challenging data characteristics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Guofei",
   "pi_last_name": "Gu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Guofei Gu",
   "pi_email_addr": "guofei@cse.tamu.edu",
   "nsf_id": "000537957",
   "pi_start_date": "2022-01-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Xia",
   "pi_last_name": "Hu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xia Hu",
   "pi_email_addr": "xia.hu@rice.edu",
   "nsf_id": "000703493",
   "pi_start_date": "2018-07-23",
   "pi_end_date": "2022-01-27"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Caverlee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "James Caverlee",
   "pi_email_addr": "caverlee@tamu.edu",
   "nsf_id": "000357482",
   "pi_start_date": "2018-07-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Guofei",
   "pi_last_name": "Gu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Guofei Gu",
   "pi_email_addr": "guofei@cse.tamu.edu",
   "nsf_id": "000537957",
   "pi_start_date": "2018-07-23",
   "pi_end_date": "2022-01-27"
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "MS 4645",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778454645",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-720f6f9d-7fff-a6be-a476-164e3ec2f083\"> </span></p>\n<p dir=\"ltr\"><span>We have produced the following main outcomes in this project:</span></p>\n<p>&nbsp;</p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>A GCN-based interpretable recommendation model by resolving latent representations into semantic aspects.</span><span> Recommendation systems play a pivotal role in a wide range of web applications such as user response prediction, like click-through-rate prediction and conversion rate prediction. Based on a user-item interaction network augmented by a knowledge graph, we propose a novel explainable recommendation model through improving the transparency of representation learning. Different from previous work, our model conducts factor discovery and representation learning simultaneously. In this way, our model learns interpretable and meaningful representations for both users and items. Unlike traditional methods in which there is a trade-off between interpretability and effectiveness, considering explainability will not negatively affect the performance of our model. Finally, comprehensive experiments are conducted to validate the performance of our model and explanation quality (both quantitatively and qualitatively).</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>An autoencoder-based detection framework against fake images and videos by regularizing local interpretation.</span><span> With advancements of deep learning techniques, it is now possible to generate super-realistic images and videos, i.e., deepfakes. Although lots of efforts have been devoted to detect deepfakes, their performance drops significantly on previously unseen but related manipulations and the detection generalization capability remains a problem. Motivated by the fine-grained nature and spatial locality characteristics of deepfakes, we propose Locality-Aware AutoEncoder (LAE) to bridge the generalization gap. We propose an active learning framework to select the challenging candidates for labeling, which requires human masks for less than 3% of the training data, dramatically reducing the annotation efforts to regularize interpretations. Experimental results on three deepfake detection tasks indicate that LAE could focus on the forgery regions to make decisions. The analysis further shows that LAE outperforms the state-of-the-arts by 6.52%, 12.03%, and 3.08% respectively on three deepfake detection tasks in terms of generalization accuracy on previously unseen manipulations.</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>A training-free backdoor attack framework in deep neural networks.</span><span> We investigate a specific security problem called backdoor attack (or trojan attack), which aims to attack deployed DNN systems relying on the hidden trigger patterns inserted by malicious hackers. We propose a training-free attack approach which is different from previous work. Specifically, we do not change parameters in the original model but insert a tiny backdoor module TrojanNet into the target model. The infected model with a malicious backdoor can misclassify inputs into a target label when the inputs are stamped with the special triggers. We also propose a new neuron-level backdoor detection algorithm. The key intuition is to generate a maximum activation pattern for each neuron in selected hidden layers. Since backdoor related neurons can be activated by small triggers, their activation patterns are much smaller than normal ones. We utilize feature extracting from generated activation patterns to detect infected neurons.</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>An Ensemble Approach for Explanation-based Adversarial Detection. </span><span>Recent research has shown Deep Neural Networks (DNNs) to be vulnerable to adversarial examples that induce desired misclassifications in the models. Such risks impede the application of machine learning in security-sensitive domains. We propose ExAD, a framework to detect adversarial examples using an ensemble of explanation techniques. Each explanation technique in ExAD produces an explanation map identifying the relevance of input variables for the model&rsquo;s classification. We evaluate our approach using six state-of-the-art adversarial attacks on three image datasets. Our extensive evaluation shows that our mechanism can effectively detect these attacks under blackbox threat model with limited false-positives. Furthermore, we find that our approach achieves promising results in limiting the success rate of whitebox attacks.&nbsp;</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>A representation neutralization algorithm is proposed by leverage model interpretation for bias mitigation.</span><span> While existing bias mitigation methods for DNN models primarily work on learning debiased encoders, we investigated the following research problem that whether we could reduce the discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs. Specifically, we propose a new mitigation technique, namely RNF, that achieves fairness by debiasing only the task-specific classification head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classification head of the DNN model. The proposed RNF framework has demonstrated to be effective on real-world benchmark datasets.</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>A new voice command disruption attack by exploiting speaker biases in automatic speech recognition. In the study of potential bias in modern voice assistant devices, our experimental results confirm both racial and gender bias that is still present in the speech recognition systems of two modern smart speaker devices. </span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/29/2023<br>\nModified by: Guofei&nbsp;Gu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nWe have produced the following main outcomes in this project:\n\n\n\n\n\n\n\nA GCN-based interpretable recommendation model by resolving latent representations into semantic aspects. Recommendation systems play a pivotal role in a wide range of web applications such as user response prediction, like click-through-rate prediction and conversion rate prediction. Based on a user-item interaction network augmented by a knowledge graph, we propose a novel explainable recommendation model through improving the transparency of representation learning. Different from previous work, our model conducts factor discovery and representation learning simultaneously. In this way, our model learns interpretable and meaningful representations for both users and items. Unlike traditional methods in which there is a trade-off between interpretability and effectiveness, considering explainability will not negatively affect the performance of our model. Finally, comprehensive experiments are conducted to validate the performance of our model and explanation quality (both quantitatively and qualitatively).\n\n\n\n\n\n\n\n\n\nAn autoencoder-based detection framework against fake images and videos by regularizing local interpretation. With advancements of deep learning techniques, it is now possible to generate super-realistic images and videos, i.e., deepfakes. Although lots of efforts have been devoted to detect deepfakes, their performance drops significantly on previously unseen but related manipulations and the detection generalization capability remains a problem. Motivated by the fine-grained nature and spatial locality characteristics of deepfakes, we propose Locality-Aware AutoEncoder (LAE) to bridge the generalization gap. We propose an active learning framework to select the challenging candidates for labeling, which requires human masks for less than 3% of the training data, dramatically reducing the annotation efforts to regularize interpretations. Experimental results on three deepfake detection tasks indicate that LAE could focus on the forgery regions to make decisions. The analysis further shows that LAE outperforms the state-of-the-arts by 6.52%, 12.03%, and 3.08% respectively on three deepfake detection tasks in terms of generalization accuracy on previously unseen manipulations.\n\n\n\n\n\n\n\n\n\nA training-free backdoor attack framework in deep neural networks. We investigate a specific security problem called backdoor attack (or trojan attack), which aims to attack deployed DNN systems relying on the hidden trigger patterns inserted by malicious hackers. We propose a training-free attack approach which is different from previous work. Specifically, we do not change parameters in the original model but insert a tiny backdoor module TrojanNet into the target model. The infected model with a malicious backdoor can misclassify inputs into a target label when the inputs are stamped with the special triggers. We also propose a new neuron-level backdoor detection algorithm. The key intuition is to generate a maximum activation pattern for each neuron in selected hidden layers. Since backdoor related neurons can be activated by small triggers, their activation patterns are much smaller than normal ones. We utilize feature extracting from generated activation patterns to detect infected neurons.\n\n\n\n\n\n\n\n\n\nAn Ensemble Approach for Explanation-based Adversarial Detection. Recent research has shown Deep Neural Networks (DNNs) to be vulnerable to adversarial examples that induce desired misclassifications in the models. Such risks impede the application of machine learning in security-sensitive domains. We propose ExAD, a framework to detect adversarial examples using an ensemble of explanation techniques. Each explanation technique in ExAD produces an explanation map identifying the relevance of input variables for the models classification. We evaluate our approach using six state-of-the-art adversarial attacks on three image datasets. Our extensive evaluation shows that our mechanism can effectively detect these attacks under blackbox threat model with limited false-positives. Furthermore, we find that our approach achieves promising results in limiting the success rate of whitebox attacks.\n\n\n\n\n\n\n\n\n\nA representation neutralization algorithm is proposed by leverage model interpretation for bias mitigation. While existing bias mitigation methods for DNN models primarily work on learning debiased encoders, we investigated the following research problem that whether we could reduce the discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs. Specifically, we propose a new mitigation technique, namely RNF, that achieves fairness by debiasing only the task-specific classification head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classification head of the DNN model. The proposed RNF framework has demonstrated to be effective on real-world benchmark datasets.\n\n\n\n\n\n\n\n\n\nA new voice command disruption attack by exploiting speaker biases in automatic speech recognition. In the study of potential bias in modern voice assistant devices, our experimental results confirm both racial and gender bias that is still present in the speech recognition systems of two modern smart speaker devices. \n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 11/29/2023\n\n\t\t\t\t\tSubmitted by: GuofeiGu\n"
 }
}