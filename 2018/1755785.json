{
 "awd_id": "1755785",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: Representation Learning and Adaptation using Unlabeled Videos",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-06-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 172903.0,
 "awd_amount": 172903.0,
 "awd_min_amd_letter_date": "2018-04-13",
 "awd_max_amd_letter_date": "2018-04-13",
 "awd_abstract_narration": "Recent success in visual recognition relies on training deep neural networks (DNNs) on a large-scale annotated image classification dataset in a fully supervised fashion. The learned representation encoded in the parameters of DNNs have shown remarkable transferability to a wide range of tasks. However, the dependency on supervised learning substantially limits the scalability to new problem domains because manual labeling is often expensive and in some cases requires expertise. In contrast, a massive amount of free unlabeled images and videos are readily available on the Internet. This project develops algorithms to capitalize on large amounts of unlabeled videos for representation learning and adaptation. The developed methods significantly alleviate the high cost and scarcity of manual annotations for constructing large-scale datasets. The project involves both graduate and undergraduate students in the research. The research materials are also integrated to curriculum development in courses on deep learning for machine perception. Results will be disseminated through scientific publications, open-source software, and dataset releases.\r\n\r\nThis research tackles two key problems in representation learning. In the first research aim, the project simultaneously leverages spatial and temporal contexts in videos to learn generalizable representation. The research takes advantages of rich supervisory signals for representation learning from appearance variations and temporal coherence in videos. Compared to the supervised counterpart (which requires millions of manually labeled images), learning from unlabeled videos is inexpensive and is not limited in scope. The project also seeks to adapt the learned representation to handle appearance variations in new domains with minimal manual supervision. The effectiveness of representation adaptation is validated in the context of instance-level video object segmentation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jia-Bin",
   "pi_last_name": "Huang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jia-Bin Huang",
   "pi_email_addr": "jbhuang0604@gmail.com",
   "nsf_id": "000732126",
   "pi_start_date": "2018-04-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "1185 Perry Street",
  "perf_city_name": "Blacksburg",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "240610101",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "VA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 172903.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project is to tackle two key problems in representation learning. First, the project&nbsp;simultaneously leverages spatial and temporal contexts in videos to learn generalizable representation for computer vision tasks.&nbsp;Second,&nbsp;the project also seeks to&nbsp;adapt&nbsp;the learned representation to handle appearance variations in new domains with minimal manual supervision.</p>\n<p><span>** Intellectual merit</span>:&nbsp;</p>\n<p>Through the project, we studied the&nbsp;representation learning and adaption from videos and dessimiated our findings via top-tier publications at Computer Vision and Machine Learning&nbsp;conferneces. Our key activities include the following research findings.</p>\n<p>We first demonstrate we can learn monocular depth estimation and optical flow estimation networks from unlabeled videos by leveraging temporal contexts. The core idea is to use cross-task geometric consistency to train the models jointly in a self-supervised manner. The resulting models lead to the state-of-the-art performance without using any manually labeled training data.&nbsp;[1]</p>\n<p>We then study the problem of representation learning for video activity recognition.&nbsp;Training a model using existing video datasets inevitably captures and leverages unwanted scene bias. The learned representation may not generalize well to new action classes or different tasks. We propose to mitigate scene bias for video representation learning.&nbsp;Our proposed method show consistent improvement over the baseline model without debiasing. [2]</p>\n<p>For representation adaptation, we show that adapting a carefully pretrained model with standard multiclass loss can achieve highly competitive results compared with complicated meta-learning algorithms. Our findings have high-impact in the field of meta learning ([3] cited over 1052 times as of Aug 2022).</p>\n<p>We also propose a representation adaptation method across domains for activity recognition in videos. Specially, we design self-supervised loss using temporal contexts and an attention mechanism to filter out uninformative video clips during domain adaptation.</p>\n<p><span>[1] Zou, Yuliang and Luo, Zelunl and Huang, Jia-Bin (2018). DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency. ECCV 2018</span></p>\n<p><span>[2] Choi, Jinwoo and Gao, Chen and Messou, Joseph and Huang, Jia-Bin (2019).&nbsp;Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition. NeurIPS 2019</span></p>\n<p><span>[3]&nbsp;<span>Chen, Wei-Yu and Liu, Yen-Cheng and Kira, Zsolt and Wang, Yu-Chiang Frank and Huang, Jia-Bin (2019). A Closer Look at Few-shot Classification, ICLR 2019</span></span></p>\n<p><span><span>[4]&nbsp;Choi,&nbsp;<span>Jinwoo, Sharma, Gaurav, Schulter, Samuel, and Huang, Jia-Bin (2020) <span>Shuffle and Attend: Video Domain Adaptation.&nbsp;</span>ECCV 2020</span></span></span></p>\n<p>The project funding provided graduate research assistantship and conference travel support for two PhD students at Virginia Tech. Both are now graudated with their PhD.&nbsp;</p>\n<p>&nbsp;</p>\n<p><span>** Broader of impacts:</span></p>\n<p>For representation learning, our work on few-shot learning (publisehd at ICLR 2019) advance the field by providing a carefully designed benchmark, a strong baseline without complex meta learning algorithm, and introducing a new problem on cross-domain few-shot recognition.&nbsp;</p>\n<p>To date (08/08/2022), according to Google Scholar, the paper has been cited 1052 times. There are several notable follow-up work based on our research, including&nbsp;</p>\n<p>our own follow-up research: Tseng, Hung-Yu and Lee, Hsin-Ying and Huang, Jia-Bin and Yang, Ming-Hsuan (2020). Cross-Domain Few-Shot Classification via Learned Feature-Wise Transform. ICLR 2020</p>\n<p>and other groups, e.g.,</p>\n<p>from MIT: Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, Phillip Isola. Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need? ECCV 2020.</p>\n<p>from UCSD/UC Berkeley:&nbsp;<span>Yinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, Xiaolong Wang.</span><br /><em>Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning.</em><span>&nbsp;ICCV</span><span>, 2021</span></p>\n<div>Our research on video domain adaptation is integrated into \"Video Recognition\" in the ECE 5554 Computer Vision course and \"Domain Adaptation\" in ECE 5524 Machine Learning courses at Virginia Tech. These courses often have high student enrollment (around 70 graduate students and 30 undergraduate students) taught in Fall 2018 and Fall 2019.&nbsp;</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/09/2022<br>\n\t\t\t\t\tModified by: Jia-Bin&nbsp;Huang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project is to tackle two key problems in representation learning. First, the project simultaneously leverages spatial and temporal contexts in videos to learn generalizable representation for computer vision tasks. Second, the project also seeks to adapt the learned representation to handle appearance variations in new domains with minimal manual supervision.\n\n** Intellectual merit: \n\nThrough the project, we studied the representation learning and adaption from videos and dessimiated our findings via top-tier publications at Computer Vision and Machine Learning conferneces. Our key activities include the following research findings.\n\nWe first demonstrate we can learn monocular depth estimation and optical flow estimation networks from unlabeled videos by leveraging temporal contexts. The core idea is to use cross-task geometric consistency to train the models jointly in a self-supervised manner. The resulting models lead to the state-of-the-art performance without using any manually labeled training data. [1]\n\nWe then study the problem of representation learning for video activity recognition. Training a model using existing video datasets inevitably captures and leverages unwanted scene bias. The learned representation may not generalize well to new action classes or different tasks. We propose to mitigate scene bias for video representation learning. Our proposed method show consistent improvement over the baseline model without debiasing. [2]\n\nFor representation adaptation, we show that adapting a carefully pretrained model with standard multiclass loss can achieve highly competitive results compared with complicated meta-learning algorithms. Our findings have high-impact in the field of meta learning ([3] cited over 1052 times as of Aug 2022).\n\nWe also propose a representation adaptation method across domains for activity recognition in videos. Specially, we design self-supervised loss using temporal contexts and an attention mechanism to filter out uninformative video clips during domain adaptation.\n\n[1] Zou, Yuliang and Luo, Zelunl and Huang, Jia-Bin (2018). DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency. ECCV 2018\n\n[2] Choi, Jinwoo and Gao, Chen and Messou, Joseph and Huang, Jia-Bin (2019). Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition. NeurIPS 2019\n\n[3] Chen, Wei-Yu and Liu, Yen-Cheng and Kira, Zsolt and Wang, Yu-Chiang Frank and Huang, Jia-Bin (2019). A Closer Look at Few-shot Classification, ICLR 2019\n\n[4] Choi, Jinwoo, Sharma, Gaurav, Schulter, Samuel, and Huang, Jia-Bin (2020) Shuffle and Attend: Video Domain Adaptation. ECCV 2020\n\nThe project funding provided graduate research assistantship and conference travel support for two PhD students at Virginia Tech. Both are now graudated with their PhD. \n\n \n\n** Broader of impacts:\n\nFor representation learning, our work on few-shot learning (publisehd at ICLR 2019) advance the field by providing a carefully designed benchmark, a strong baseline without complex meta learning algorithm, and introducing a new problem on cross-domain few-shot recognition. \n\nTo date (08/08/2022), according to Google Scholar, the paper has been cited 1052 times. There are several notable follow-up work based on our research, including \n\nour own follow-up research: Tseng, Hung-Yu and Lee, Hsin-Ying and Huang, Jia-Bin and Yang, Ming-Hsuan (2020). Cross-Domain Few-Shot Classification via Learned Feature-Wise Transform. ICLR 2020\n\nand other groups, e.g.,\n\nfrom MIT: Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, Phillip Isola. Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need? ECCV 2020.\n\nfrom UCSD/UC Berkeley: Yinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, Xiaolong Wang.\nMeta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning. ICCV, 2021\nOur research on video domain adaptation is integrated into \"Video Recognition\" in the ECE 5554 Computer Vision course and \"Domain Adaptation\" in ECE 5524 Machine Learning courses at Virginia Tech. These courses often have high student enrollment (around 70 graduate students and 30 undergraduate students) taught in Fall 2018 and Fall 2019. \n\n\t\t\t\t\tLast Modified: 08/09/2022\n\n\t\t\t\t\tSubmitted by: Jia-Bin Huang"
 }
}