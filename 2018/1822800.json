{
 "awd_id": "1822800",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "A Remote Multimodal Learning Environment to Increase Graphical Information Access for Blind and Visually Impaired Students",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928447",
 "po_email": "cshen@nsf.gov",
 "po_sign_block_name": "Chia Shen",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 747894.0,
 "awd_amount": 779254.0,
 "awd_min_amd_letter_date": "2018-08-16",
 "awd_max_amd_letter_date": "2020-12-18",
 "awd_abstract_narration": "There are many limitations for students who are blind or visually impaired (BVI) in accessing complex STEM graphical information in the classroom or workplace. This longstanding problem arises due to reliance on inaccessible and outdated learning materials, the need for costly specialized devices, and an adherence to an outdated educational service model. To address these issues, this project will investigate the development and evaluation of an innovative remote learning system based on the use of multiple sensory channels to strategically present information from auditory, linguistic, touch, and enhanced visual sensing. The research will focus specifically on the optimization of multimodal information presentation and perception, separating sensory output based on its unique information processing characteristics for conveying different types of stimuli. The first project goal is to increase the quality of STEM instruction for BVI students by determining perceptually motivated learning supports that promote non-visual knowledge acquisition of STEM graphical and spatial information (learning goal). The second project goal is to increase access to graphical and spatial STEM content through creation of an innovative remote multimodal interface for communicating the conceptual meaning of visual information (technology goal). The project outcomes will contribute to theories of non-visual learning and multisensory processing, and a clear translational path to development of more efficient, intuitive, and usable multimodal interfaces for both blind and sighted users. The application of the results will help to address the severe under-representation of BVI individuals in STEM-related disciplines, and the 70% unemployment rate of this demographic, by providing a new, low-cost, and accessible technology platform for communicating non-visual graphical STEM materials. \r\n\r\nThe researchers will answer the following inter-connected questions: 1) What is the best information content to be conveyed by different modal outputs for maximizing perceptual saliency, learnability, interpretation, and representation of STEM graphical materials? Once optimized in the lab, 2) How well does the optimized multimodal learning system perform in a remote deployment environment in conveying graphical STEM materials to BVI learners; and 3) Does the remote learning system increase the level of comprehension of STEM graphical content as compared to traditional BVI instructional methods? Both quantitative and qualitative data about the optimization process and the remote technology system will be collected and analyzed including user response metrics on speed/accuracy, user experience data, and STEM graphical assessment instruments adapted for BVI students. The first phase of the research will investigate multimodal information processing in order to establish best practices for information delivery and non-visual graphical learning efficiency with experiments comparing graphical information presented in different modalities for three core STEM graphical themes: graphs, diagrams, and maps. The second phase of experiments will investigate the remote learning system's efficacy as well as evaluating user performance on graphical STEM learning measures and key usability and satisfaction metrics. This project has the broader goals of increasing independence for BVI learners and other students with or without disabilities who might benefit from a remote multimodal learning environment, and the development of a new tool for supporting large-scale research and assistive technology evaluation with BVI human subjects, thereby dramatically increasing scientists' ability to recruit and work with a much larger population of BVI users than is currently possible from lab-based studies.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nicholas",
   "pi_last_name": "Giudice",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nicholas Giudice",
   "pi_email_addr": "nicholas.giudice@maine.edu",
   "nsf_id": "000503809",
   "pi_start_date": "2018-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Justin",
   "pi_last_name": "Dimmel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Justin Dimmel",
   "pi_email_addr": "justin.dimmel@maine.edu",
   "nsf_id": "000747931",
   "pi_start_date": "2018-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Stacy",
   "pi_last_name": "Doore",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Stacy A Doore",
   "pi_email_addr": "sadoore@colby.edu",
   "nsf_id": "000767938",
   "pi_start_date": "2018-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maine",
  "inst_street_address": "5717 CORBETT HALL",
  "inst_street_address_2": "",
  "inst_city_name": "ORONO",
  "inst_state_code": "ME",
  "inst_state_name": "Maine",
  "inst_phone_num": "2075811484",
  "inst_zip_code": "044695717",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "ME02",
  "org_lgl_bus_name": "UNIVERSITY OF MAINE SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "PB3AJE5ZEJ59"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maine",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "ME",
  "perf_st_name": "Maine",
  "perf_zip_code": "044695717",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "ME02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "798000",
   "pgm_ele_name": "ECR-EDU Core Research"
  },
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "1545",
   "pgm_ref_txt": "Research in Disabilties Education"
  },
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "8212",
   "pgm_ref_txt": "Broaden Particip STEM Resrch"
  },
  {
   "pgm_ref_code": "8817",
   "pgm_ref_txt": "STEM Learning & Learning Environments"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0418",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001819DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 747894.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 15360.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Traditional learning materials used in mainstream science, engineering, technology, and mathematics (STEM) classrooms rely heavily on graphics and images to efficiently convey complex concepts. However, these materials are often inaccessible to students with blindness or visual impairment (BVI), leading to inequities in information access and posing significant barriers for success in STEM educational and career pathways.</p>\n<p>&nbsp;</p>\n<p>Although graphical information is ubiquitous in today&rsquo;s digital world, the vast majority of this content is highly visual, regardless of setting. While many visual products can be accessed through alternative means - figures have captions, web-based images have labels, social media photos are tagged, and so on - these text-based descriptions are only sometimes present, and often do not tell the whole story. With more content moving to the electronic space, it is paramount that new solutions for graphical information access are explored in the digital domain. The primary research goals of this project were to 1) investigate perceptually motivated learning strategies that promote non-visual knowledge acquisition of STEM graphical and spatial information, and 2) develop and evaluate a multimodal learning environment to increase information access for blind and visually impaired students to STEM graphical curricula content.</p>\n<p>&nbsp;</p>\n<p>The research findings from this project provide strong evidence of the concept of functional equivalence between two types of interface conditions, natural language (NL) spatial descriptions and haptic (vibro-tactile) stimuli with&nbsp; a NL overview, based on multiple study outcomes where participant performance resulted in similar measures of accuracy, response time, and recreation tasks. Aggregate results demonstrate our prototype multimodal learning system reliably increased access to graphical information and promoted Universal Design in inclusive learning environments. We began the research by investigating the language used by secondary and post-secondary math instructional experts describing geometry diagrams. The results of this study suggest that instructional experts use different strategies, vocabulary, and ordering of information in describing the images for the two groups of learners.</p>\n<p>&nbsp;</p>\n<p>We used these results to develop NL descriptions for the design of the multimodal vibro-audio interface learning environment and conducted the next study using simple geometric figures made up of points, lines, and regions comparing three interface conditions, a complete natural language description, haptic only, and haptic with a short NL overview. The results of this study found statistically-similar performance for time and accuracy between the natural language and haptics with overview conditions, but reliably worse performance for haptic-only learning, suggesting the NL overview help guide the haptic learning. However, analyzing data from a figure recreation task based on narrative descriptions suggested that the natural language condition produced the most accurate descriptions over the other two conditions (which was supported by user preference surveys). The next set of studies, focused on comparing only two conditions with more complex graphics conveyed using a longer natural language description interface and the vibro-audio interface combining haptic stimuli with a NL overview. Three types of graphical images were tested: pie charts, Venn diagrams, and line graphs for both BLV and sighted participants. Although results suggested differences between the two groups in response time (with BVI performance being faster), both groups showed similar accuracy between the two presentation conditions and three graph types. Both groups reported a preference for receiving the graphical information through the natural language interface; however, the finding of near identical accuracy suggests that they were able to access the information and learn the instructional concepts similarly in either condition. The aggregate results across these studies support evolving theories of functional equivalence for multimodal information channels and highlight the importance of layering graphical information to provide learners with information access tools that best suit their unique sensory needs.</p>\n<p>&nbsp;</p>\n<p>The guidelines disseminated help establish a framework for remotely deployed, multimodal graphics to increase accessibility of the digital world, a necessary component for inclusive design that is currently lacking. This project also enables both content creators and technology designers to consider universal design from the onset, providing open-source tools to make electronic content truly accessible on existing commercial hardware. The infrastructure created will provide a learner-centered framework for promoting knowledge acquisition and literacy of multimodal, digital graphics. This literacy is critical for all students in today&rsquo;s world and is likely to benefit students of multiple learning styles, creating new dimensions of learning right at our fingertips. The broader impacts of this project will help to address the graphical access problem for individuals with visual impairments and overcome the barriers of graphical information transfer and accessibility currently impeding students with visual impairments (a significantly underrepresented population in STEM). This research provides the foundation for a transformed classroom experience in which students with visual impairments can independently interact with their sighted peers and primary classroom teacher, in real-time, a significant benefit over the current practices.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/08/2023<br>\nModified by: Nicholas&nbsp;Giudice</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1822800/1822800_10572246_1699472571937_figure_2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1822800/1822800_10572246_1699472571937_figure_2--rgov-800width.jpg\" title=\"Description of the differences between conditions.\"><img src=\"/por/images/Reports/POR/2023/1822800/1822800_10572246_1699472571937_figure_2--rgov-66x44.jpg\" alt=\"Description of the differences between conditions.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Description of the differences between conditions and example of what users would see on device</div>\n<div class=\"imageCredit\">Justin Brown</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Nicholas&nbsp;Giudice\n<div class=\"imageTitle\">Description of the differences between conditions.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1822800/1822800_10572246_1699472686749_figure_1--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1822800/1822800_10572246_1699472686749_figure_1--rgov-800width.png\" title=\"Example of use on device\"><img src=\"/por/images/Reports/POR/2023/1822800/1822800_10572246_1699472686749_figure_1--rgov-66x44.png\" alt=\"Example of use on device\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Example of use on device</div>\n<div class=\"imageCredit\">Justin Brown</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Nicholas&nbsp;Giudice\n<div class=\"imageTitle\">Example of use on device</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nTraditional learning materials used in mainstream science, engineering, technology, and mathematics (STEM) classrooms rely heavily on graphics and images to efficiently convey complex concepts. However, these materials are often inaccessible to students with blindness or visual impairment (BVI), leading to inequities in information access and posing significant barriers for success in STEM educational and career pathways.\n\n\n\n\n\nAlthough graphical information is ubiquitous in todays digital world, the vast majority of this content is highly visual, regardless of setting. While many visual products can be accessed through alternative means - figures have captions, web-based images have labels, social media photos are tagged, and so on - these text-based descriptions are only sometimes present, and often do not tell the whole story. With more content moving to the electronic space, it is paramount that new solutions for graphical information access are explored in the digital domain. The primary research goals of this project were to 1) investigate perceptually motivated learning strategies that promote non-visual knowledge acquisition of STEM graphical and spatial information, and 2) develop and evaluate a multimodal learning environment to increase information access for blind and visually impaired students to STEM graphical curricula content.\n\n\n\n\n\nThe research findings from this project provide strong evidence of the concept of functional equivalence between two types of interface conditions, natural language (NL) spatial descriptions and haptic (vibro-tactile) stimuli with a NL overview, based on multiple study outcomes where participant performance resulted in similar measures of accuracy, response time, and recreation tasks. Aggregate results demonstrate our prototype multimodal learning system reliably increased access to graphical information and promoted Universal Design in inclusive learning environments. We began the research by investigating the language used by secondary and post-secondary math instructional experts describing geometry diagrams. The results of this study suggest that instructional experts use different strategies, vocabulary, and ordering of information in describing the images for the two groups of learners.\n\n\n\n\n\nWe used these results to develop NL descriptions for the design of the multimodal vibro-audio interface learning environment and conducted the next study using simple geometric figures made up of points, lines, and regions comparing three interface conditions, a complete natural language description, haptic only, and haptic with a short NL overview. The results of this study found statistically-similar performance for time and accuracy between the natural language and haptics with overview conditions, but reliably worse performance for haptic-only learning, suggesting the NL overview help guide the haptic learning. However, analyzing data from a figure recreation task based on narrative descriptions suggested that the natural language condition produced the most accurate descriptions over the other two conditions (which was supported by user preference surveys). The next set of studies, focused on comparing only two conditions with more complex graphics conveyed using a longer natural language description interface and the vibro-audio interface combining haptic stimuli with a NL overview. Three types of graphical images were tested: pie charts, Venn diagrams, and line graphs for both BLV and sighted participants. Although results suggested differences between the two groups in response time (with BVI performance being faster), both groups showed similar accuracy between the two presentation conditions and three graph types. Both groups reported a preference for receiving the graphical information through the natural language interface; however, the finding of near identical accuracy suggests that they were able to access the information and learn the instructional concepts similarly in either condition. The aggregate results across these studies support evolving theories of functional equivalence for multimodal information channels and highlight the importance of layering graphical information to provide learners with information access tools that best suit their unique sensory needs.\n\n\n\n\n\nThe guidelines disseminated help establish a framework for remotely deployed, multimodal graphics to increase accessibility of the digital world, a necessary component for inclusive design that is currently lacking. This project also enables both content creators and technology designers to consider universal design from the onset, providing open-source tools to make electronic content truly accessible on existing commercial hardware. The infrastructure created will provide a learner-centered framework for promoting knowledge acquisition and literacy of multimodal, digital graphics. This literacy is critical for all students in todays world and is likely to benefit students of multiple learning styles, creating new dimensions of learning right at our fingertips. The broader impacts of this project will help to address the graphical access problem for individuals with visual impairments and overcome the barriers of graphical information transfer and accessibility currently impeding students with visual impairments (a significantly underrepresented population in STEM). This research provides the foundation for a transformed classroom experience in which students with visual impairments can independently interact with their sighted peers and primary classroom teacher, in real-time, a significant benefit over the current practices.\n\n\n\t\t\t\t\tLast Modified: 11/08/2023\n\n\t\t\t\t\tSubmitted by: NicholasGiudice\n"
 }
}