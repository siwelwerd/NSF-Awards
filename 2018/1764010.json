{
 "awd_id": "1764010",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Neuromorphic and Data-Driven Speech Segregation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 851674.0,
 "awd_amount": 851674.0,
 "awd_min_amd_letter_date": "2018-09-17",
 "awd_max_amd_letter_date": "2018-09-17",
 "awd_abstract_narration": "This project investigates how neural representations of speech and music in the cortex can be adapted and applied to overcome the challenge of robust perception in extremely noisy and cluttered environments, mimicking processing and capabilities of the brain. More specifically, the project will formulate algorithms inspired by the architecture of the brain to segregate and track targeted speakers or sound sources, test their performance, and relate them to state-of-the-art approaches that utilize deep artificial neural networks to accomplish these tasks. Human psychoacoustic and physiological experiments with these algorithms will be conducted to test the validity of these ideas for mimicking human abilities. This effort will spur the development of new neuromorphic computational tools modeled after the brain and its cognitive functions. In turn, these will provide a theoretical framework to guide future experiments into how complex cognitive functions originate and how they influence sensory perception and lead to robust behavioral performance.\r\n\r\nThe planned projects will be organized into two flavors. The first attempts to borrow from existing neuromorphic approaches that rely on cortical representations to develop new embeddings within the deep neural networks framework, which will in turn endow the latter with brain-like robustness in challenging unanticipated environments. Three specific efforts within this flavor will be conducted: Learning DNN embeddings using cortical representations of speech and music, exploring unsupervised clustering of cortical features using adversarial auto-encoders, and exploiting pitch and timbre representations to enhance segregation of sound. The second flavor of projects borrows from the DNN approach to build into neuromorphic algorithms the desirable performance and flexibility attained by training on available databases. Two broad areas of studies are planned: one focuses on questions of neuromorphic implementations that benefit from DNN toolboxes and ideas, especially in segregation and reconstruction. The other focuses on investigating how autoencoders can be exploited to implement feature reduction and clustering efficiently.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shihab",
   "pi_last_name": "Shamma",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Shihab A Shamma",
   "pi_email_addr": "sas@isr.umd.edu",
   "nsf_id": "000461861",
   "pi_start_date": "2018-09-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Carol",
   "pi_last_name": "Espy-Wilson",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "Carol Y Espy-Wilson",
   "pi_email_addr": "espy@eng.umd.edu",
   "nsf_id": "000258918",
   "pi_start_date": "2018-09-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "AV Williams Bldg",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207423285",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "MD",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 851674.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Humans can easily attend to a target speaker in a complex acoustic environment with concurrent sounds, even when these sounds consist of the speech from other talkers.&nbsp; Advances in deep learning models have led to drastic improvements in disentangling source mixtures relative to the performance of previously proposed computational models developed for this purpose.&nbsp;&nbsp; A series of experiments were conducted in this grant to understand the mechanisms employed by deep learning models and compare them with what we know about the properties used by the human brain for this task.&nbsp; The experiments showed that deep learning technologies focus on the harmonicity in speech for segregation.&nbsp; If any of the harmonics are masked which happens in normal noisy environments, the deep learning algorithms fail. This failure is particularly true when the masking occurs in the low frequency range.&nbsp; On the other hand, missing harmonics at low frequency or at any other frequencies does not affect the ability of humans to perform segregation.&nbsp; Another finding was that humans and deep learning algorithms diverge when it comes to the use of temporal coherence.&nbsp; &nbsp;&nbsp;Studies have shown that humans rely on the coincidence of the onsets and offsets of harmonics to group harmonics in forming a signal source.&nbsp; This explains why humans are not That is, timing information is more important than harmonicity.&nbsp; This timing information is not used at all by deep learning models.&nbsp; &nbsp;&nbsp;This difference between humans and deep learning models points to the need to incorporate temporal coherence in deep learning models if they are to be as robust as humans in everyday noisy environments.</p>\n<p>Another aspect of this work focused on a Mirror network (Mirrornet) to model how children learn to speak.&nbsp; Most organisms including humans function by coordinating and integrating sensory signals with motor actions to survive and accomplish desired tasks.&nbsp; Human infants learn to speak by first going through a &ldquo;babbling&rdquo; stage as they learn the &ldquo;feel&rdquo; or the range and limitations of their articulatory commands (controlled by the motor system in the brain). They also listen carefully to the speech around them, initially implicitly learning it without necessarily producing any of it (controlled by the auditory system in the brain). When infants are ready to learn to speak, they utter incomplete malformed replica of the speech they hear. They also sense these errors (unsupervised) or are told about some of them (semi-supervised) and proceed to adapt the articulatory commands to minimize the errors and slowly converge on the desired auditory signal. In other words, learning these complex sensorimotor mappings proceeds simultaneously and often in an unsupervised manner by listening and speaking all at once.&nbsp; In this work, we explored a constrained autoencoder architecture inspired by sensorimotor interactions, to learn meaningful articulatory representations. An</p>\n<p>articulatory synthesizer based on deep learning was custom developed and trained in a supervised and speaker-independent fashion to be used as the vocal tract model in the MirrorNet. When incorporated with this articulatory synthesizer, the MirrorNet estimates the movement of the articulators in an unsupervised fashion, mimicking how children learn to speak.&nbsp; Note that an &lsquo;initialization phase&rsquo; was needed to constrain the MirrorNet&rsquo;s encoder and decoder coefficients to converge to the range of values expected for the movements of the articulators.&nbsp; &nbsp;&nbsp;This stage is not surprising given humans only articulate a small set of speech sounds (around 46 in American English) in any language even though the vocal tract can be manipulated to produce a great deal more.&nbsp; This initialization was followed by conventional &lsquo;learning&rsquo; procedures that produced physiologically meaningful predictions of the articulatory variables, resulting in natural sounding conversational speech.</p>\n<p>Finally, in part of this project, we tackled the problem of how humans segregate multiple simultaneous speakers using specifically binaural cues and attention. The project explored the question of whether selective attention to location cues (while ignoring less reliable cues such as pitch, or vice versa) in fact offered advantages. We have shown in detail how selective attention can sometimes offer the advantages always anticipated, but in many cases it does not as it excludes other cues in favor of the attended ones.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/19/2024<br>\nModified by: Shihab&nbsp;A&nbsp;Shamma</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nHumans can easily attend to a target speaker in a complex acoustic environment with concurrent sounds, even when these sounds consist of the speech from other talkers. Advances in deep learning models have led to drastic improvements in disentangling source mixtures relative to the performance of previously proposed computational models developed for this purpose. A series of experiments were conducted in this grant to understand the mechanisms employed by deep learning models and compare them with what we know about the properties used by the human brain for this task. The experiments showed that deep learning technologies focus on the harmonicity in speech for segregation. If any of the harmonics are masked which happens in normal noisy environments, the deep learning algorithms fail. This failure is particularly true when the masking occurs in the low frequency range. On the other hand, missing harmonics at low frequency or at any other frequencies does not affect the ability of humans to perform segregation. Another finding was that humans and deep learning algorithms diverge when it comes to the use of temporal coherence. Studies have shown that humans rely on the coincidence of the onsets and offsets of harmonics to group harmonics in forming a signal source. This explains why humans are not That is, timing information is more important than harmonicity. This timing information is not used at all by deep learning models. This difference between humans and deep learning models points to the need to incorporate temporal coherence in deep learning models if they are to be as robust as humans in everyday noisy environments.\n\n\nAnother aspect of this work focused on a Mirror network (Mirrornet) to model how children learn to speak. Most organisms including humans function by coordinating and integrating sensory signals with motor actions to survive and accomplish desired tasks. Human infants learn to speak by first going through a babbling stage as they learn the feel or the range and limitations of their articulatory commands (controlled by the motor system in the brain). They also listen carefully to the speech around them, initially implicitly learning it without necessarily producing any of it (controlled by the auditory system in the brain). When infants are ready to learn to speak, they utter incomplete malformed replica of the speech they hear. They also sense these errors (unsupervised) or are told about some of them (semi-supervised) and proceed to adapt the articulatory commands to minimize the errors and slowly converge on the desired auditory signal. In other words, learning these complex sensorimotor mappings proceeds simultaneously and often in an unsupervised manner by listening and speaking all at once. In this work, we explored a constrained autoencoder architecture inspired by sensorimotor interactions, to learn meaningful articulatory representations. An\n\n\narticulatory synthesizer based on deep learning was custom developed and trained in a supervised and speaker-independent fashion to be used as the vocal tract model in the MirrorNet. When incorporated with this articulatory synthesizer, the MirrorNet estimates the movement of the articulators in an unsupervised fashion, mimicking how children learn to speak. Note that an initialization phase was needed to constrain the MirrorNets encoder and decoder coefficients to converge to the range of values expected for the movements of the articulators. This stage is not surprising given humans only articulate a small set of speech sounds (around 46 in American English) in any language even though the vocal tract can be manipulated to produce a great deal more. This initialization was followed by conventional learning procedures that produced physiologically meaningful predictions of the articulatory variables, resulting in natural sounding conversational speech.\n\n\nFinally, in part of this project, we tackled the problem of how humans segregate multiple simultaneous speakers using specifically binaural cues and attention. The project explored the question of whether selective attention to location cues (while ignoring less reliable cues such as pitch, or vice versa) in fact offered advantages. We have shown in detail how selective attention can sometimes offer the advantages always anticipated, but in many cases it does not as it excludes other cues in favor of the attended ones.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 03/19/2024\n\n\t\t\t\t\tSubmitted by: ShihabAShamma\n"
 }
}