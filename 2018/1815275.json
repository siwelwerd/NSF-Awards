{
 "awd_id": "1815275",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: SMALL: Robust Reinforcement Learning Using Bayesian Models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 437753.0,
 "awd_amount": 437753.0,
 "awd_min_amd_letter_date": "2018-07-31",
 "awd_max_amd_letter_date": "2019-03-15",
 "awd_abstract_narration": "Basing decisions on data is preferable to relying on heuristics or rules of thumb. Using data effectively, however, can be challenging. In domains like agriculture or medicine, datasets are usually small, biased, and noisy. For instance, the full effects of reduced pesticide applications depend on the weather and the impacts on yield may not be known until the harvest. Reducing pesticide applications reduces costs and provides ecological and consumer benefits, but using too little of it can easily cause a crop failure and significant financial losses. These dual problems of limited data availability and a high cost of failure are also common in manufacturing, maintenance, and even robotics. Because most existing reinforcement learning methods assume large datasets, stakeholders often dismiss data-driven methods and rely on heuristics to make decisions that are apparently safe but quite sub-optimal. This research develops new robust methods for data-driven decision making that can recommend good actions that are also safe even when data is limited. The new reinforcement learning methods use prior domain knowledge to estimate the confidence in possible outcomes to prevent catastrophic failure when predictions are incorrect. The practical viability of these methods is tested on the problem of using historical data to recommending improved pesticide schedules for fruit orchards and is disseminated to practitioners.\r\n\r\nThis research targets reinforcement learning problems with 1) limited or expensive data and 2) a high cost of failure. When bad decisions cause large losses, injury, or death, then having confidence in a policy's quality is more important than its optimality gap. Computing high-confidence policies in reinforcement learning is difficult. Even small errors can quickly accumulate through positive feedback loops and covariate shift. Therefore, more robust methods are needed to convince practitioners to benefit from data instead of relying on heuristics. The project combines robust optimization with model-based reinforcement learning to compute good policies that are resistant to data errors. Robust optimization has achieved successes in many areas but can be difficult to use with reinforcement learning. It requires a model of plausible uncertainty levels, so-called ambiguity sets, to properly balance solution?s quality and confidence. Constructing good ambiguity sets manually in sequential decision problems is very difficult even for robust optimization experts. This research investigates a new data-driven Bayesian approach to robust reinforcement learning. It combines hierarchical Bayesian models with robust optimization to leverage powerful hierarchical modeling techniques while avoiding the computational complexity often associated with Bayesian reinforcement learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marek",
   "pi_last_name": "Petrik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Marek Petrik",
   "pi_email_addr": "mpetrik@cs.unh.edu",
   "nsf_id": "000736649",
   "pi_start_date": "2018-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shady",
   "pi_last_name": "Atallah",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Shady S Atallah",
   "pi_email_addr": "shady.atallah@unh.edu",
   "nsf_id": "000711038",
   "pi_start_date": "2019-03-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of New Hampshire",
  "inst_street_address": "51 COLLEGE RD",
  "inst_street_address_2": "BLDG 107",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NH",
  "inst_state_name": "New Hampshire",
  "inst_phone_num": "6038622172",
  "inst_zip_code": "038242620",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NH01",
  "org_lgl_bus_name": "UNIVERSITY SYSTEM OF NEW HAMPSHIRE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GBNGC495XA67"
 },
 "perf_inst": {
  "perf_inst_name": "University of New Hampshire",
  "perf_str_addr": "33 Academic Way",
  "perf_city_name": "Durham",
  "perf_st_code": "NH",
  "perf_st_name": "New Hampshire",
  "perf_zip_code": "038242619",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NH01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 437753.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research deepened the understanding of robustness in reinforcement learning and used it to develop new robust reinforcement learning algorithms that are faster and perform better than the prior art. These robust reinforcement learning algorithms aim to enable reliable data-driven decision-making in the presence of significant data limitations in high-stake decision domains.<br /><br />The research studies robust optimization techniques, which compute the best decisions for the worst plausible realization of uncertain parameters. For example, when deciding to use a pesticide, a robust algorithm would assume the lowest effectiveness consistent with the historical data. These methods have been introduced and widely studied in the operations research community, but their properties are poorly understood when applied in reinforcement learning. This research project has alleviated the limitations of robust algorithms in reinforcement learning in four principal directions. <br /><br />The first limitation of robust reinforcement learning algorithms this project addresses is the use of plausible parameter values that are too broad to be practical. Existing algorithms allow for worst-case realizations of uncertain model parameters that lead to overly pessimistic assumptions and give rise to excessively conservative decisions. Such decisions are robust to model errors, as desired, but their performance when the models are correct is inadequate. In this project, we proposed a class of techniques that use hierarchical Bayesian models to leverage prior domain knowledge to balance carefully the decisions' robustness with their average performance. These algorithms can compute decisions robust to model and data errors and incur only minimum penalty in their average performance. <br /><br />The second limitation of robust reinforcement learning is that existing algorithms construct ambiguity sets, which represent the scope of plausible parameter values, without regard for the problem structure. Although this approach is conceptually simple, it can negatively impact the algorithms' performance and robustness. In particular, using ambiguity sets defined in terms of simple distance metrics, such as Lp vector norms or KL divergence, can lead to decisions that pay a performance penalty to be robust to an improvement in the quality of the outcomes when parameter values are more favorable than expected. For example, a robust algorithm with the generic distance metric will treat the model error of a pesticide working better than expected as undesirable as the model error of the pesticide working worse than expected. We developed robust reinforcement learning algorithms that use problem structure to focus on avoiding only uncertainty with negative impacts on solution quality. For example, these methods can make better decisions because they only seek to compensate for the pesticide underperforming and ignore the situation when it overperforms.&nbsp; &nbsp;<br /><br />The third limitation is that the computational complexity of robust optimization in reinforcement learning can be significant. The increased computation complexity of robust optimization leads to solution times being thousands of times slower than traditional reinforcement learning methods. We developed a new class of algorithms that can bring the computational time of robust solutions much closer in line with conventional algorithms. In our experimental results, these algorithms improve the runtime of existing robust reinforcement learning algorithms by over 1000 times, even in problems with several hundred states and actions. We also proposed a fundamentally new approach to solving robust reinforcement learning that formulates it as a convex optimization problem. This formulation allows leveraging the vast array of convex optimization algorithms when solving and analyzing robust reinforcement learning algorithms. <br /><br />Our final goal was to evaluate robust algorithms on a range of domains inspired by practice and inform the community about the expected real-world performance of existing reinforcement learning algorithms. For this purpose, we defined a set of new domains based on a pest management problem in agriculture and adapted numerous domains from reinforcement learning, operations research, health care, and other areas. For instance, we have evaluated robust algorithms in standard inventory management problems, cancer treatment, and problems based on simple reinforcement learning benchmarks. These numerical results have helped us and the community to gauge the importance of improvements achieved by several algorithms proposed as a part of this research and other groups. <br /><br />Overall, this research project helped to advance the theory and practice of robust reinforcement learning. It has enabled the practical use of robust algorithms in reinforcement learning in domains with modest state and decision spaces.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/06/2023<br>\n\t\t\t\t\tModified by: Marek&nbsp;Petrik</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis research deepened the understanding of robustness in reinforcement learning and used it to develop new robust reinforcement learning algorithms that are faster and perform better than the prior art. These robust reinforcement learning algorithms aim to enable reliable data-driven decision-making in the presence of significant data limitations in high-stake decision domains.\n\nThe research studies robust optimization techniques, which compute the best decisions for the worst plausible realization of uncertain parameters. For example, when deciding to use a pesticide, a robust algorithm would assume the lowest effectiveness consistent with the historical data. These methods have been introduced and widely studied in the operations research community, but their properties are poorly understood when applied in reinforcement learning. This research project has alleviated the limitations of robust algorithms in reinforcement learning in four principal directions. \n\nThe first limitation of robust reinforcement learning algorithms this project addresses is the use of plausible parameter values that are too broad to be practical. Existing algorithms allow for worst-case realizations of uncertain model parameters that lead to overly pessimistic assumptions and give rise to excessively conservative decisions. Such decisions are robust to model errors, as desired, but their performance when the models are correct is inadequate. In this project, we proposed a class of techniques that use hierarchical Bayesian models to leverage prior domain knowledge to balance carefully the decisions' robustness with their average performance. These algorithms can compute decisions robust to model and data errors and incur only minimum penalty in their average performance. \n\nThe second limitation of robust reinforcement learning is that existing algorithms construct ambiguity sets, which represent the scope of plausible parameter values, without regard for the problem structure. Although this approach is conceptually simple, it can negatively impact the algorithms' performance and robustness. In particular, using ambiguity sets defined in terms of simple distance metrics, such as Lp vector norms or KL divergence, can lead to decisions that pay a performance penalty to be robust to an improvement in the quality of the outcomes when parameter values are more favorable than expected. For example, a robust algorithm with the generic distance metric will treat the model error of a pesticide working better than expected as undesirable as the model error of the pesticide working worse than expected. We developed robust reinforcement learning algorithms that use problem structure to focus on avoiding only uncertainty with negative impacts on solution quality. For example, these methods can make better decisions because they only seek to compensate for the pesticide underperforming and ignore the situation when it overperforms.   \n\nThe third limitation is that the computational complexity of robust optimization in reinforcement learning can be significant. The increased computation complexity of robust optimization leads to solution times being thousands of times slower than traditional reinforcement learning methods. We developed a new class of algorithms that can bring the computational time of robust solutions much closer in line with conventional algorithms. In our experimental results, these algorithms improve the runtime of existing robust reinforcement learning algorithms by over 1000 times, even in problems with several hundred states and actions. We also proposed a fundamentally new approach to solving robust reinforcement learning that formulates it as a convex optimization problem. This formulation allows leveraging the vast array of convex optimization algorithms when solving and analyzing robust reinforcement learning algorithms. \n\nOur final goal was to evaluate robust algorithms on a range of domains inspired by practice and inform the community about the expected real-world performance of existing reinforcement learning algorithms. For this purpose, we defined a set of new domains based on a pest management problem in agriculture and adapted numerous domains from reinforcement learning, operations research, health care, and other areas. For instance, we have evaluated robust algorithms in standard inventory management problems, cancer treatment, and problems based on simple reinforcement learning benchmarks. These numerical results have helped us and the community to gauge the importance of improvements achieved by several algorithms proposed as a part of this research and other groups. \n\nOverall, this research project helped to advance the theory and practice of robust reinforcement learning. It has enabled the practical use of robust algorithms in reinforcement learning in domains with modest state and decision spaces.\n\n\t\t\t\t\tLast Modified: 09/06/2023\n\n\t\t\t\t\tSubmitted by: Marek Petrik"
 }
}