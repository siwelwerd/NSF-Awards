{
 "awd_id": "1750640",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: A Stable Foundation for Trustworthy Data Analysis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2018-02-01",
 "awd_exp_date": "2024-01-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 508000.0,
 "awd_min_amd_letter_date": "2018-01-08",
 "awd_max_amd_letter_date": "2022-02-11",
 "awd_abstract_narration": "Every day, massive amounts of data are collected, analyzed, and used to make high-stakes decisions, raising many questions about how to use this data in a trustworthy manner.  This project is about two such questions: (1) How can researchers prevent false discovery, and use data to learn meaningful facts about a population without overfitting to that data?  Despite decades of research into methods for preventing false discovery, it remains a vexing problem for the scientific community.  (2) How can researchers use valuable but sensitive data to learn about a population without compromising the privacy of individuals in that data?  This task has proven to be quite delicate, and there have been several high profile attacks on supposedly anonymous datasets, causing a lack of confidence in the most commonly used approaches.  \r\n\r\nAlthough they may seem unrelated, surprisingly, both of these questions can be addressed using stable algorithms---algorithms that are insensitive to small changes in their inputs.  In the past decade, differential privacy emerged as a strong form of algorithmic stability that guarantees a high degree of individual privacy, yet admits highly accurate data analysis.  More recently, differential privacy has been shown to prevent false discovery in interactive data analysis---the common scenario where the same dataset is analyzed repeatedly, which has been implicated in a \"statistical crisis in science.\"\r\n\r\nThis project will take a unified approach to advancing the state-of-the-art in privacy and false discovery via algorithmic stability.  The main outcomes of this project will be building the theoretical foundations of interactive data analysis, developing new computationally efficient stable algorithms for central problems in these areas, understanding the limits of privacy and interactive data analysis both in theory and in practice, and broadening the reach of algorithmic stability to address other challenges in trustworthy data analysis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jonathan",
   "pi_last_name": "Ullman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jonathan Ullman",
   "pi_email_addr": "jullman@ccs.neu.edu",
   "nsf_id": "000710812",
   "pi_start_date": "2018-01-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 102716.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 105798.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 108972.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 97925.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 92589.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-6757e4f9-7fff-99c0-897e-1f9bdefce33b\"> </span></p>\n<p dir=\"ltr\"><span>Every day, massive amounts of data are collected, analyzed, and used to make high-stakes decisions, raising many questions about how to use this data in a trustworthy manner.&nbsp; This project made a unified attack on two such questions:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>How can researchers prevent false discovery, and use data to learn meaningful facts about a population without overfitting to that data?</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>How can researchers use valuable yet sensitive data to learn about a population without compromising the privacy of the individuals in that data?</span></p>\n</li>\n</ul>\n<p dir=\"ltr\"><span>Although they seem unrelated, these questions can both be addressed using stable algorithms&mdash;algorithms whose outputs are insensitive to small changes in their inputs. Specifically, differential privacy is a strong form of algorithmic stability that guarantees a high degree of individual privacy yet admits highly accurate data analysis. Moreover, differential privacy prevents false discovery in interactive data analysis&mdash;the common scenario where the same dataset is analyzed repeatedly, which has been implicated in a &ldquo;statistical crisis in science&rdquo; by Gelman and Loken. This connection has led to dramatically more accurate algorithms for interactive data analysis.</span></p>\n<p dir=\"ltr\"><span><span> </span></span></p>\n<p dir=\"ltr\"><span>This project made several contributions to resolving these two problems.</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>The project developed foundations of interactive data analysis&mdash;identifying new techniques and algorithms with optimal sample complexity.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>This project introduced new computationally efficient private algorithms for natural data analysis tasks in machine learning and statistics.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>This project introduced methodology for empirically auditing differentially private algorithms to understand how their real-world privacy guarantees correspond to their theoretical guarantees.</span></p>\n</li>\n</ul>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 05/08/2024<br>\nModified by: Jonathan&nbsp;Ullman</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nEvery day, massive amounts of data are collected, analyzed, and used to make high-stakes decisions, raising many questions about how to use this data in a trustworthy manner. This project made a unified attack on two such questions:\n\n\n\n\nHow can researchers prevent false discovery, and use data to learn meaningful facts about a population without overfitting to that data?\n\n\n\n\nHow can researchers use valuable yet sensitive data to learn about a population without compromising the privacy of the individuals in that data?\n\n\n\n\nAlthough they seem unrelated, these questions can both be addressed using stable algorithmsalgorithms whose outputs are insensitive to small changes in their inputs. Specifically, differential privacy is a strong form of algorithmic stability that guarantees a high degree of individual privacy yet admits highly accurate data analysis. Moreover, differential privacy prevents false discovery in interactive data analysisthe common scenario where the same dataset is analyzed repeatedly, which has been implicated in a statistical crisis in science by Gelman and Loken. This connection has led to dramatically more accurate algorithms for interactive data analysis.\n\n\n \n\n\nThis project made several contributions to resolving these two problems.\n\n\n\n\nThe project developed foundations of interactive data analysisidentifying new techniques and algorithms with optimal sample complexity.\n\n\n\n\nThis project introduced new computationally efficient private algorithms for natural data analysis tasks in machine learning and statistics.\n\n\n\n\nThis project introduced methodology for empirically auditing differentially private algorithms to understand how their real-world privacy guarantees correspond to their theoretical guarantees.\n\n\n\n\n\t\t\t\t\tLast Modified: 05/08/2024\n\n\t\t\t\t\tSubmitted by: JonathanUllman\n"
 }
}