{
 "awd_id": "1816264",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: Collaborative: Algorithms Everywhere: Identifying and Designing for Data Privacy Styles",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 227800.0,
 "awd_amount": 227800.0,
 "awd_min_amd_letter_date": "2018-07-26",
 "awd_max_amd_letter_date": "2018-07-26",
 "awd_abstract_narration": "The computational algorithms that analyze our personal data online and in myriad medical, credit card, and other databases can make it increasingly easy to infer personal, intimate details about us (such as our personality, political ideology, or sexual preference) from seemingly mundane data (such as which pages someone has \"Liked\" on Facebook). People may not notice or know about these risks, and if they do, they must make ongoing decisions about which algorithms they may be providing with their personal information, which to ignore, and which to decry as invasive or unethical. Currently, the behaviors that emerge around these systems are poorly accounted for in existing technology design practices. To address this shortcoming, this work will answer the questions: How do people navigate a world in which data collection and analysis is a continuous feature of their environment? What strategies do people adopt based on their varied privacy perceptions, attitudes, and needs? How can interfaces and other features of internet systems be designed to support different styles of data privacy?\r\n\r\nThis proposal uses search-related behaviors as a research and design context to examine user adaptation and response to pervasive data collection. Through a series of surveys, the project team will develop a validated measure of privacy patterns or styles related to the technologies, tools, and beliefs that people habitually use to guide their data privacy behaviors. The researchers also will conduct qualitative interviews with users of data protection tools. These interviews will establish what specific strategies these users employ to protect their privacy from algorithmic analysis, if any, as well as the mental models that give rise to efficacy beliefs about those strategies. They will create a collection of experimental prototype tools to explore the design space around algorithmic privacy. Collectively, this work will expand our technology design vocabulary in terms of being able to account for different styles of privacy practices. This research has the potential to significantly improve privacy policies and technologies by describing the specific challenges that people face as they attempt to protect their privacy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrea",
   "pi_last_name": "Forte",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andrea Forte",
   "pi_email_addr": "fortea@umich.edu",
   "nsf_id": "000561997",
   "pi_start_date": "2018-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Drexel University",
  "inst_street_address": "3141 CHESTNUT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158956342",
  "inst_zip_code": "191042875",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "DREXEL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "XF3XM9642N96"
 },
 "perf_inst": {
  "perf_inst_name": "Drexel University",
  "perf_str_addr": "3141 Chestnut",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191042816",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "065Z",
   "pgm_ref_txt": "Human factors for security research"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 227800.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-f3a38392-7fff-e7c6-02af-49115dcb45aa\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">People need to achieve privacy goals in a world filled not only with new data collection and surveillance technologies, but also with algorithms that can infer and reveal information about individuals. Society has a great need to understand privacy protection not only as it pertains to controlling the bits and pieces of information that people share, but also as it pertains to the possible future uses of that information to learn things that were not intentionally shared. Understanding how people protect their privacy in such a complex information environment is critical for their continued safety and security online.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project has enabled students and faculty at Drexel University and Lehigh University to collaborate on scientific studies that uncovered people&rsquo;s privacy-related beliefs, particularly where inferential algorithms are concerned. We used a variety of research methods to conduct several studies over the course of the project. These included: surveying and interviewing experts and laypersons to better understand how to support a range of privacy practices in design practice; examining the practices of privacy-concerned citizens who organize privacy groups and how their practices changed over the course of the pandemic to inform policy, research and design; engaging in participatory research with an online drug harm reduction community for whom privacy is a significant concern; conducting technical investigations to ascertain the degree and nature of the privacy risks posed by inferential algorithms; and collecting data on how people understand the potential for algorithms to reveal information about them that they may believe to be protected.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We characterized differences between privacy experts and laypersons that show they have different styles of reasoning when considering privacy-enhancing technologies: experts think of them as technologies whose primary function is enhancing privacy, whereas laypersons conceptualize privacy enhancement as a supplemental function incorporated into other technologies. This has implications for the development of privacy tools that will succeed in the market. Our studies with privacy groups confirmed prior findings about how different individuals often conceptualize privacy in different ways. However, by studying these groups, we were also able to identify a series of practical strategies by which designers, policy makers, and other researchers might account for these pluralistic formulations of privacy. Finally, our participatory engagement revealed the ease with which anonymous online accounts from different platforms can be linked. In bringing these results back to community members, we found that they did not think about the threats that inferential algorithms posed in a strictly private-vs-public fashion. Rather, their reasoning focused more on various ways that they could introduce friction. Doing so might not prevent account linking entirely but rather might slow it down or make it more difficult. In this way, these studies also led to enhanced theoretical understandings of privacy in a world filled with algorithms. These studies were not only productive, but generative; studies conducted with the support of this grant inspired additional work that is ongoing and will continue beyond the lifetime of the grant itself. </span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/05/2023<br>\n\t\t\t\t\tModified by: Andrea&nbsp;Forte</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "People need to achieve privacy goals in a world filled not only with new data collection and surveillance technologies, but also with algorithms that can infer and reveal information about individuals. Society has a great need to understand privacy protection not only as it pertains to controlling the bits and pieces of information that people share, but also as it pertains to the possible future uses of that information to learn things that were not intentionally shared. Understanding how people protect their privacy in such a complex information environment is critical for their continued safety and security online.\n\n \nThis project has enabled students and faculty at Drexel University and Lehigh University to collaborate on scientific studies that uncovered people\u2019s privacy-related beliefs, particularly where inferential algorithms are concerned. We used a variety of research methods to conduct several studies over the course of the project. These included: surveying and interviewing experts and laypersons to better understand how to support a range of privacy practices in design practice; examining the practices of privacy-concerned citizens who organize privacy groups and how their practices changed over the course of the pandemic to inform policy, research and design; engaging in participatory research with an online drug harm reduction community for whom privacy is a significant concern; conducting technical investigations to ascertain the degree and nature of the privacy risks posed by inferential algorithms; and collecting data on how people understand the potential for algorithms to reveal information about them that they may believe to be protected. \n\n \nWe characterized differences between privacy experts and laypersons that show they have different styles of reasoning when considering privacy-enhancing technologies: experts think of them as technologies whose primary function is enhancing privacy, whereas laypersons conceptualize privacy enhancement as a supplemental function incorporated into other technologies. This has implications for the development of privacy tools that will succeed in the market. Our studies with privacy groups confirmed prior findings about how different individuals often conceptualize privacy in different ways. However, by studying these groups, we were also able to identify a series of practical strategies by which designers, policy makers, and other researchers might account for these pluralistic formulations of privacy. Finally, our participatory engagement revealed the ease with which anonymous online accounts from different platforms can be linked. In bringing these results back to community members, we found that they did not think about the threats that inferential algorithms posed in a strictly private-vs-public fashion. Rather, their reasoning focused more on various ways that they could introduce friction. Doing so might not prevent account linking entirely but rather might slow it down or make it more difficult. In this way, these studies also led to enhanced theoretical understandings of privacy in a world filled with algorithms. These studies were not only productive, but generative; studies conducted with the support of this grant inspired additional work that is ongoing and will continue beyond the lifetime of the grant itself. \n\n\t\t\t\t\tLast Modified: 01/05/2023\n\n\t\t\t\t\tSubmitted by: Andrea Forte"
 }
}