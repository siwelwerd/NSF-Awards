{
 "awd_id": "1813049",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Robust and Secure Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-05-24",
 "awd_max_amd_letter_date": "2018-05-24",
 "awd_abstract_narration": "Machine learning (ML) systems play an increasingly central role in society--from ubiquitous speech recognition systems, to navigation systems, product recommendation systems, and deployed learning systems across manufacturing, industry, and healthcare.  The near future, with complex computer vision systems, self-driving cars, and ML driven medical care and patient monitoring, promises a nearly pervasive presence of ML systems in our society.  Despite promising performance in idealized settings, current ML systems are often brittle--they are sensitive to slight changes in the input data, and often have weaknesses that can be easily exploited by a malicious adversary.  Resolving these current shortcomings is a necessary step in ensuring the stability, safety, and security of a society that relies heavily on machine learning.  The central goal of this project is to develop learning algorithms that are robust, and secure.  These go beyond the traditional goal of developing learning algorithms that achieve high accuracy, and address the broad need for reliability and safety in critical deployed systems.  As an extension of the research component of the project, the investigator will continue education and outreach efforts.  These include disseminating the research publications and code produced by this project, continuing to develop new courses and teaching materials on data-centric algorithms, machine learning, and related topics, and organizing a semi-annual forum for the exchange of ideas between industry and academia.    \r\n\r\nThe research core of this project addresses the lack of robustness of current learning and optimization algorithms. This lack of robustness takes the following two distinct forms. First, current algorithms are sensitive to changes in even a very small portion of the data-set on which they are trained.  Second, even when trained on legitimate data, the learned models are often susceptible to \"adversarial examples\" in the sense that for the vast majority of data points--even data points in the training set--a small adversarial perturbation of the data point in question will result in the model outputting a completely different label.  The presence of these two types of fragility in current learning systems raises the possibility of vulnerabilities to two new sorts of security threats: 1) the threat that a portion of the training data is either extremely biased and unreliable, or worse--that it has been generated by an adversary whose goal is to mislead the machine learning system, and 2) the threat that deployed machine learning systems can be tricked via minute but carefully generated adversarial modifications in their test points--modifications that are essentially invisible to humans. The project seeks to address these two critical weaknesses of current systems, by : 1) developing new algorithms that are robust to the presence of significant fractions of arbitrary -- including adversarial -- data, which can be applied to a number of fundamental estimation, machine learning, and optimization tasks, and 2) developing a rigorous understanding of why certain training algorithms yield models that are inherently vulnerable to adversarial examples, and develop tools for reducing this vulnerability.  Additionally, this project investigates the computational, and information theoretic aspects of robust and secure learning, including developing an understanding of any potential trade-offs, for example between the amount of training data and computation time, and robustness or security of the resulting trained model.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Valiant",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory J Valiant",
   "pi_email_addr": "gvaliant@cs.stanford.edu",
   "nsf_id": "000603941",
   "pi_start_date": "2018-05-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "353 Serra Mall, Gates 470",
  "perf_city_name": "Hayward",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943055008",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<div class=\"gmail_default\">This project resulted in a number of new algorithms and advances&nbsp;in understanding of robust and secure learning and estimation.&nbsp; Some of the main research contributions are summarized below.&nbsp; Beyond the research contributions, this project supported a number of PhD students and postdoctoral researchers, undergraduate researchers, and outreach efforts including a Summer school for high-schoolers on machine learning with a focus on robustness.&nbsp; Four of the&nbsp;PhD students directly funded by this project have now graduated, and gone on to begin&nbsp;faculty positions at top universities or research scientist&nbsp;positions in industry.&nbsp;&nbsp;<br /><br />One fundamental question, from the security side of machine learning, is how to allow trained models to \"forget\" certain training datapoints.&nbsp;&nbsp;There has been much recent discussions on how to provide individuals with control over when their data can and cannot be used (e.g. the EU?s Right To Be Forgotten regulation). In results&nbsp;published in NeurIPS'19, we initiated a formal framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulated the problem of efficiently deleting individual data points from trained machine learning models. For many deep learning based ML models, the only way to completely remove an individual?s data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. In this work, beyond describing a framework in which to consider these questions, we introduced several algorithmic principles that enable efficient data deletion in ML.&nbsp; This was the first work in this area, and has sparked significant interest from the academic and industrial ML community.</div>\n<div class=\"gmail_default\">On the side of robust learning, one of the significant research directions supported by this project was an effort to develop learning and estimation algorithms that are robust (both in theory and in practice) to significant deviations from the idealized setting where data has strong distributional assumptions.&nbsp; Within this vein, there was progress on several fronts.&nbsp; &nbsp;One line of results (appearing in COLT?19 and COLT'21) formulated the problem of \"selective prediction\": observations arrive sequentially, and the goal will be to accurately estimate some statistic (or deploy a trained model) for some window of future observations.&nbsp; Crucially, we make no assumptions about how the future is related to the past, and posit no distributional assumptions on the data.&nbsp; Despite this lack of assumptions, the punchline is that provided the predictor/learner is allowed to select 1) when the prediction is made, and 2) the window-length to which the prediction pertains, accurate prediction is possible even for worst-case data.&nbsp; The high-level intuition underlying these startling results is a sort of Ramsey-theoretic argument: every sequence must have some predictable structure at some scale---if a sequence is random, then it is predictable and concentrates at longer timescales, whereas if it has a few isolated changes in behavior, then at most timesteps, it is predictable at shorter timeframes.&nbsp;&nbsp;</div>\n<div class=\"gmail_default\">These surprising results on accurately predicting the future without assumptions on how the future is related to the past, motivated the consideration of a general formulation where accurate inferences are possible for worst-case data. For example, given worst case data but an understanding of the process by which the data is partitioned into a test and training set, when is accurate prediction or learning possible? Work appearing at Neurips'20 described a framework for thinking about such generalizations, and gave a practical and theoretically near-optimal algorithm for predictions in such settings.&nbsp;&nbsp;</div>\n<div class=\"gmail_default\"><br />Beyond the above lines of research, this project supported a number of other successful research efforts, including introducing new algorithms for training deep neural networks that enforce structural invariants (ICML'19), work to understand the implicit regularization (COLT'19) and data augmentation (ICML'20) in deep neural networks, efforts to formalize the problem of generating more data (ICML'20), work to understand the role of memory in optimization and learning (STOC'19, COLT'22), characterization of the effects of model misspecification to slow down learning and how to be robust to such misspecification (AISTATS'21), and the introduction of a new problem \"online pen testing\" and associated algorithms that model decision making under uncertainty, where there is a significant cost for obtaining additional information (ITCS'21),&nbsp; and providing a rigorous framework in which to probe the ability of Transformer networks to perform \"in context\" learning (Neurips'22).</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/06/2023<br>\n\t\t\t\t\tModified by: Gregory&nbsp;J&nbsp;Valiant</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis project resulted in a number of new algorithms and advances in understanding of robust and secure learning and estimation.  Some of the main research contributions are summarized below.  Beyond the research contributions, this project supported a number of PhD students and postdoctoral researchers, undergraduate researchers, and outreach efforts including a Summer school for high-schoolers on machine learning with a focus on robustness.  Four of the PhD students directly funded by this project have now graduated, and gone on to begin faculty positions at top universities or research scientist positions in industry.  \n\nOne fundamental question, from the security side of machine learning, is how to allow trained models to \"forget\" certain training datapoints.  There has been much recent discussions on how to provide individuals with control over when their data can and cannot be used (e.g. the EU?s Right To Be Forgotten regulation). In results published in NeurIPS'19, we initiated a formal framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulated the problem of efficiently deleting individual data points from trained machine learning models. For many deep learning based ML models, the only way to completely remove an individual?s data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. In this work, beyond describing a framework in which to consider these questions, we introduced several algorithmic principles that enable efficient data deletion in ML.  This was the first work in this area, and has sparked significant interest from the academic and industrial ML community.\nOn the side of robust learning, one of the significant research directions supported by this project was an effort to develop learning and estimation algorithms that are robust (both in theory and in practice) to significant deviations from the idealized setting where data has strong distributional assumptions.  Within this vein, there was progress on several fronts.   One line of results (appearing in COLT?19 and COLT'21) formulated the problem of \"selective prediction\": observations arrive sequentially, and the goal will be to accurately estimate some statistic (or deploy a trained model) for some window of future observations.  Crucially, we make no assumptions about how the future is related to the past, and posit no distributional assumptions on the data.  Despite this lack of assumptions, the punchline is that provided the predictor/learner is allowed to select 1) when the prediction is made, and 2) the window-length to which the prediction pertains, accurate prediction is possible even for worst-case data.  The high-level intuition underlying these startling results is a sort of Ramsey-theoretic argument: every sequence must have some predictable structure at some scale---if a sequence is random, then it is predictable and concentrates at longer timescales, whereas if it has a few isolated changes in behavior, then at most timesteps, it is predictable at shorter timeframes.  \nThese surprising results on accurately predicting the future without assumptions on how the future is related to the past, motivated the consideration of a general formulation where accurate inferences are possible for worst-case data. For example, given worst case data but an understanding of the process by which the data is partitioned into a test and training set, when is accurate prediction or learning possible? Work appearing at Neurips'20 described a framework for thinking about such generalizations, and gave a practical and theoretically near-optimal algorithm for predictions in such settings.  \n\nBeyond the above lines of research, this project supported a number of other successful research efforts, including introducing new algorithms for training deep neural networks that enforce structural invariants (ICML'19), work to understand the implicit regularization (COLT'19) and data augmentation (ICML'20) in deep neural networks, efforts to formalize the problem of generating more data (ICML'20), work to understand the role of memory in optimization and learning (STOC'19, COLT'22), characterization of the effects of model misspecification to slow down learning and how to be robust to such misspecification (AISTATS'21), and the introduction of a new problem \"online pen testing\" and associated algorithms that model decision making under uncertainty, where there is a significant cost for obtaining additional information (ITCS'21),  and providing a rigorous framework in which to probe the ability of Transformer networks to perform \"in context\" learning (Neurips'22).\n\n \n\n \n\n\t\t\t\t\tLast Modified: 07/06/2023\n\n\t\t\t\t\tSubmitted by: Gregory J Valiant"
 }
}