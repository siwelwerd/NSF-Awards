{
 "awd_id": "1750430",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Foundations of Information Theory: Information Inequalities and Dimension-Free Phenomena",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 550000.0,
 "awd_amount": 550000.0,
 "awd_min_amd_letter_date": "2018-02-02",
 "awd_max_amd_letter_date": "2022-07-22",
 "awd_abstract_narration": "Information inequalities stemming from entropy and mutual information form the basis for Shannon's mathematical theory of communication, data transmission and storage, the vast consequences of which have ushered in the modern information age. While originally bounding theoretically the rate at which data may be transmitted over an imperfect channel, or how far an information source may be compressed, the foundational nature of these inequalities has led to their widespread applications in quantitative fields ranging from computer science to physics. This may be attributed, in part, to these inequalities being dimension-free, that is, the sharpness of the estimate does not degrade with the data dimension, thus making them suitable for analysis and inference in high-dimensional settings that are characteristic of modern problems in statistics, optimization and data science.  Broadly speaking, this project seeks to further elucidate these foundational underpinnings of information theory, and to extend their applicability to modern problems in statistics and data analysis that seek to uncover information from large data sets. The research is coupled with a plan to integrate research and education at multiple levels: the project will train researchers at the interface of statistics, information theory and mathematics, preparing them to enter academic and industrial careers in data science, and skillfully adapt to new fields as national priorities change. Other aims are to promote collaboration within the broader research community through development of thematic workshops and tutorials. \r\n\r\nThis project will undertake a systematic investigation of information inequalities. This goal will be achieved through an integrated research agenda that seeks to: (i) quantify high-dimensional statistical phenomena; (ii) characterize extremal properties of information inequalities; and (iii) discover new concentration and isoperimetric phenomena through linking information- and transportation-based quantities on graphs and other discrete spaces.  Through these aims, this project will promote the influence of information theory across traditional boundaries and enrich the set of intellectual questions addressed.  Given the intrinsic importance of quantifying statistical and probabilistic phenomena throughout science and engineering, the results will have broad and lasting impact.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas A",
   "pi_last_name": "Courtade",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas A Courtade",
   "pi_email_addr": "courtade@eecs.berkeley.edu",
   "nsf_id": "000665428",
   "pi_start_date": "2018-02-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "265 Cory Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947201770",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 174282.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 254607.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 121111.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Much like the mathematical laws of physics allow us to predict and quantify natural phenomena, the mathematical framework of information theory provides tools to rigorously analyze the transmission and representation of information as a quantitative object. Central to this framework are information inequalities -- quantitative relationships involving mutual information, entropy, Fisher information, and related measures -- which have played a foundational role in the field since its inception. These inequalities have found broad utility across disciplines, including statistics, computer science, mathematics, and related areas, largely due to their power in characterizing high-dimensional phenomena. The overarching goal of this project was to systematically investigate information inequalities from a perspective independent of the classical Shannon paradigms of compression and transmission.</p>\r\n<p><br /> One major outcome was the development of a unified theory of multimarginal entropy inequalities, along with a characterization of their extremal properties. This comprehensive framework integrates a wide range of existing results across information theory, mathematics, and statistics. Notably, it subsumes many classical inequalities -- such as the entropy power inequality, the Brunn-Minkowski inequality, and Brascamp-Lieb-type inequalities -- as special cases. Importantly, the results hold in arbitrary dimensions, expanding their applicability.</p>\r\n<p><br /> Beyond this, the project established new connections between information theory and statistics, leading to the formulation of a novel class of Bayesian Cramer-Rao-type bounds. These bounds provide general-purpose, easily computable lower bounds on the performance of statistical estimators under arbitrary loss functions, making them especially practical in real-world applications.</p>\r\n<p><br /> Another significant contribution was the development of general methods for analyzing the stability of Gaussian inequalities in both information theory and functional analysis. Stability, in this context, refers to understanding the behavior of near-extremal configurations -- those that nearly achieve equality in an inequality. Using the techniques we developed, the project achieved the first quantitative stability results for several key inequalities, including the entropy power inequality and the Bakry-Emery theorem.</p>\r\n<p><br /> Finally, the project also made a lasting impact through the interdisciplinary training of undergraduate and graduate students. These students have since advanced to leadership roles in both academia and industry, reflecting the broad and enduring impact of this work. Furthermore, the project contributed to the development of both a research monograph and a graduate-level textbook, extending its educational reach and providing valuable resources for future researchers and students in the field.</p><br>\n<p>\n Last Modified: 04/23/2025<br>\nModified by: Thomas A&nbsp;Courtade</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nMuch like the mathematical laws of physics allow us to predict and quantify natural phenomena, the mathematical framework of information theory provides tools to rigorously analyze the transmission and representation of information as a quantitative object. Central to this framework are information inequalities -- quantitative relationships involving mutual information, entropy, Fisher information, and related measures -- which have played a foundational role in the field since its inception. These inequalities have found broad utility across disciplines, including statistics, computer science, mathematics, and related areas, largely due to their power in characterizing high-dimensional phenomena. The overarching goal of this project was to systematically investigate information inequalities from a perspective independent of the classical Shannon paradigms of compression and transmission.\r\n\n\n\n One major outcome was the development of a unified theory of multimarginal entropy inequalities, along with a characterization of their extremal properties. This comprehensive framework integrates a wide range of existing results across information theory, mathematics, and statistics. Notably, it subsumes many classical inequalities -- such as the entropy power inequality, the Brunn-Minkowski inequality, and Brascamp-Lieb-type inequalities -- as special cases. Importantly, the results hold in arbitrary dimensions, expanding their applicability.\r\n\n\n\n Beyond this, the project established new connections between information theory and statistics, leading to the formulation of a novel class of Bayesian Cramer-Rao-type bounds. These bounds provide general-purpose, easily computable lower bounds on the performance of statistical estimators under arbitrary loss functions, making them especially practical in real-world applications.\r\n\n\n\n Another significant contribution was the development of general methods for analyzing the stability of Gaussian inequalities in both information theory and functional analysis. Stability, in this context, refers to understanding the behavior of near-extremal configurations -- those that nearly achieve equality in an inequality. Using the techniques we developed, the project achieved the first quantitative stability results for several key inequalities, including the entropy power inequality and the Bakry-Emery theorem.\r\n\n\n\n Finally, the project also made a lasting impact through the interdisciplinary training of undergraduate and graduate students. These students have since advanced to leadership roles in both academia and industry, reflecting the broad and enduring impact of this work. Furthermore, the project contributed to the development of both a research monograph and a graduate-level textbook, extending its educational reach and providing valuable resources for future researchers and students in the field.\t\t\t\t\tLast Modified: 04/23/2025\n\n\t\t\t\t\tSubmitted by: Thomas ACourtade\n"
 }
}