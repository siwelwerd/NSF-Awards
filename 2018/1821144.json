{
 "awd_id": "1821144",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CDS&E: Efficient and Robust Recurrent Neural Networks",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Christopher Stark",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2018-06-14",
 "awd_max_amd_letter_date": "2018-06-14",
 "awd_abstract_narration": "Deep neural networks have emerged over the last decade as one of the most powerful machine learning methods. Recurrent neural networks (RNNs) are special neural networks that are designed to efficiently model sequential data such as speech and text data by exploiting temporal connections within a sequence and handling varying sequence lengths in a dataset. While RNN and its variants have found success in many real-world applications, there are various issues that make them difficult to use in practice. This project will systematically address some of these difficulties and develop an efficient and robust RNN. Computer codes derived in this project will be made freely available. The research results will have applications in a variety of areas involving sequential data learning, including computer vision, speech recognition, natural language processing, financial data analysis, and bioinformatics.\r\n\r\nAs in other neural networks, training of RNNs typically involves some variants of gradient descent optimization, which is prone to so-called vanishing or exploding gradient problems. Regularization of RNNs, which refers to techniques used to prevent the model from overfitting the raining data and hence poor generalization to new data, is also challenging. The current preferred RNN architectures such as the Long-Short-Term-Memory networks have highly complex structures with numerous additional interacting elements that are not easy to understand. This project develops an RNN that extends a recent orthogonal/unitary RNNs to more effectively model long and short term dependency of sequential data. Through an indirect parametrization of recurrent matrix, dropout regularization techniques will be developed. The network developed in this project will retain the simplicity and efficiency of basic RNNs but enhance some key capabilities for robust applications. In particular, the project will include a study of applications of RNNs to some bioinformatics problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Qiang",
   "pi_last_name": "Ye",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qiang Ye",
   "pi_email_addr": "qye3@uky.edu",
   "nsf_id": "000482987",
   "pi_start_date": "2018-06-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Kentucky Research Foundation",
  "inst_street_address": "500 S LIMESTONE",
  "inst_street_address_2": "109 KINKEAD HALL",
  "inst_city_name": "LEXINGTON",
  "inst_state_code": "KY",
  "inst_state_name": "Kentucky",
  "inst_phone_num": "8592579420",
  "inst_zip_code": "405260001",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "KY06",
  "org_lgl_bus_name": "UNIVERSITY OF KENTUCKY RESEARCH FOUNDATION, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "H1HYA8Z1NTM5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Kentucky Research Foundation",
  "perf_str_addr": "500 S Limestone 109 Kinkead Hall",
  "perf_city_name": "Lexington",
  "perf_st_code": "KY",
  "perf_st_name": "Kentucky",
  "perf_zip_code": "405260001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "KY06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Intellectual Merit:<br /><br />This project systematically studies several challenges arising in training of a recurrent neural network (RNN), a fundamental model for sequential data such as speech and text data. Several new RNN models have been developed to address vanishing or exploding gradient problems in training of RNNs as well as to effectively carry short term and long term memory of the networks. They include a scaled Cayley unitary recurrent neural network (scuRNN), eigenvalue normalized recurrent neural network (ENRNN), and an orthogonal recurrent gated unit through Neumann-Cayley transform (NC-GRU).<br /><br />The project has also investigated new optimization techniques and new model architectures to accelerate and stabilize training of neural networks. A new class of preconditioning methods, called batch normalization preconditioning, have been developed for fully-connected networks and convolutional neural networks that can significantly accelerates the training. A conjugate gradient momentum method has been proposed to address the difficulty of selecting momentum parameters in momentum methods. An adaptive weighted discriminator loss function has been developed to stabilize the training of generative adversarial neural networks (GANs). A new integral-based approach has been introduced as a more versatile and efficient way to construct normalizing flows. A convolutional neural network with suitable structures has been derived to produce symmetric feature maps that can better model tasks with such a structure.<br /><br />Broader Impact:<br /><br />The project has implemented some new RNN models for three applications: the state inference of RNA secondary structure, automatic detections of epileptic seizures through analysis of the electroencephalography signals, and molecular property predictions in drug design. It has further developed a GAN model with adaptive weighted discriminator and a consistency loss functions for speech enhancements. New state-of-the-art results have been obtained for several datasets in the tasks of molecular property predictions and speech enhancements.<br /><br />The project has resulted in seven computer codes that are freely distributed at the open source platform GitHub. It has also produced three Ph.D. dissertations directed by PI.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/22/2022<br>\n\t\t\t\t\tModified by: Qiang&nbsp;Ye</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIntellectual Merit:\n\nThis project systematically studies several challenges arising in training of a recurrent neural network (RNN), a fundamental model for sequential data such as speech and text data. Several new RNN models have been developed to address vanishing or exploding gradient problems in training of RNNs as well as to effectively carry short term and long term memory of the networks. They include a scaled Cayley unitary recurrent neural network (scuRNN), eigenvalue normalized recurrent neural network (ENRNN), and an orthogonal recurrent gated unit through Neumann-Cayley transform (NC-GRU).\n\nThe project has also investigated new optimization techniques and new model architectures to accelerate and stabilize training of neural networks. A new class of preconditioning methods, called batch normalization preconditioning, have been developed for fully-connected networks and convolutional neural networks that can significantly accelerates the training. A conjugate gradient momentum method has been proposed to address the difficulty of selecting momentum parameters in momentum methods. An adaptive weighted discriminator loss function has been developed to stabilize the training of generative adversarial neural networks (GANs). A new integral-based approach has been introduced as a more versatile and efficient way to construct normalizing flows. A convolutional neural network with suitable structures has been derived to produce symmetric feature maps that can better model tasks with such a structure.\n\nBroader Impact:\n\nThe project has implemented some new RNN models for three applications: the state inference of RNA secondary structure, automatic detections of epileptic seizures through analysis of the electroencephalography signals, and molecular property predictions in drug design. It has further developed a GAN model with adaptive weighted discriminator and a consistency loss functions for speech enhancements. New state-of-the-art results have been obtained for several datasets in the tasks of molecular property predictions and speech enhancements.\n\nThe project has resulted in seven computer codes that are freely distributed at the open source platform GitHub. It has also produced three Ph.D. dissertations directed by PI.\n\n\t\t\t\t\tLast Modified: 12/22/2022\n\n\t\t\t\t\tSubmitted by: Qiang Ye"
 }
}