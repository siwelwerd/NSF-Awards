{
 "awd_id": "1807563",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CDS&E: Optimization of Advanced Cyberinfrastructure through Data-Driven Computational Modeling",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927116",
 "po_email": "sghafoor@nsf.gov",
 "po_sign_block_name": "Sheikh Ghafoor",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 523644.0,
 "awd_amount": 523644.0,
 "awd_min_amd_letter_date": "2018-08-31",
 "awd_max_amd_letter_date": "2018-12-11",
 "awd_abstract_narration": "Modern scientific and big-data computing systems must balance multiple system attributes such as power, \r\nperformance, and reliability to meet application science demands. These systems and their designers are, however,\r\nconstrained by the lack of clear and well-defined methods to guide system design and tuning. This reduces the \r\noverall effectiveness of modern strategic computing systems. This project is developing new\r\ntechniques for quantitatively characterizing and optimizing system tradeoffs in a wide range of\r\nmodern strategic computing systems. The short term goal of this project is to develop models, analyses,\r\nand optimizations that will enable system software, applications, and system architects to make effective \r\nend-to-end performance tradeoffs. The desired long-term impact of this research is to increase the\r\noverall efficiency and effectiveness of current and emerging strategic computing systems. The techniques\r\ndeveloped as part of this project will also be integrated with large-scale computing educational programs\r\nat the University of New Mexico and across the country.\r\n\r\nThis research is grounded in stochastic inter-collective-interval model for characterizing \r\nthe performance of application/system-software configurations. To this end, the project is \r\ninvestigating techniques for estimating how different system software and applications mechanisms change \r\napplication model parameters, providing a general means for understanding application/system software \r\nperformance tradeoffs. In addition, the project is examining optimization techniques that leverage \r\nthese models to improve resource allocation decisions in system schedulers and runtime systems. \r\nThese techniques are being evaluated based on their ability to improve the overall performance of modern \r\nstrategic computing systems in areas such as scheduling, power management, and data locality. The techniques\r\ndeveloped as part of this project will also be integrated with large-scale computing educational programs\r\nat the University of New Mexico and across the country.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Patrick",
   "pi_last_name": "Bridges",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Patrick G Bridges",
   "pi_email_addr": "patrickb@unm.edu",
   "nsf_id": "000351181",
   "pi_start_date": "2018-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Majeed",
   "pi_last_name": "Hayat",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Majeed M Hayat",
   "pi_email_addr": "majeed.hayat@marquette.edu",
   "nsf_id": "000463779",
   "pi_start_date": "2018-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Trilce",
   "pi_last_name": "Estrada",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Trilce Estrada",
   "pi_email_addr": "estrada@cs.unm.edu",
   "nsf_id": "000656205",
   "pi_start_date": "2018-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of New Mexico",
  "inst_street_address": "1 UNIVERSITY OF NEW MEXICO",
  "inst_street_address_2": "",
  "inst_city_name": "ALBUQUERQUE",
  "inst_state_code": "NM",
  "inst_state_name": "New Mexico",
  "inst_phone_num": "5052774186",
  "inst_zip_code": "871310001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NM01",
  "org_lgl_bus_name": "UNIVERSITY OF NEW MEXICO",
  "org_prnt_uei_num": "",
  "org_uei_num": "F6XLTRUQJEN4"
 },
 "perf_inst": {
  "perf_inst_name": "University of New Mexico",
  "perf_str_addr": "",
  "perf_city_name": "Albuquerque",
  "perf_st_code": "NM",
  "perf_st_name": "New Mexico",
  "perf_zip_code": "871310001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NM01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808400",
   "pgm_ele_name": "CDS&E"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "8084",
   "pgm_ref_txt": "CDS&E"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 523644.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The major goal of this research is characterizing, predicting, and analyzing application/system-software performance interactions at scale. This goal is achieved in three major steps. First, by performing stochastic modeling of extreme-scale software through their inter-collective intervals (ICI) in two ways: one based on the direct estimation of the distribution of system software-perturbed ICI lengths, and one based on deriving this quantity from separate characterizations of the stochastic behaviors of the application and system software.&nbsp;Second, by integrating representative decision support analysis that may include estimating scaling impacts (e.g., evaluating how changes in the number of processes running in an application impact application runtime), detecting performance outlier events, and estimating system-software impact on interval distributions. Third, by evaluating the performance modeling abstractions and analysis techniques on a range of extreme-scale applications and programming models.&nbsp;</p>\n<p>USing this approach, the project&nbsp;designed a new, scalable, and statistically robust approach for effective modeling, measurement, and analysis of large-scale performance variation in HPC systems. This approach avoided the need to reason about complex distributions of runtimes among large numbers of individual application processes by focusing instead on the maximum length of distributed workload intervals. The evaluation of this approach included examining&nbsp; and quantifying the strenghts and limitations of this modeling approach,</p>\n<p>The project complemented this approach by studying the analytical modeling of large-scale application performance using a novel, composite stochastic model to characterize interference and compute-time individually. This approach provided a framework to correctly quantify and predict the impact of rare, colossal interference at scale on the HPC application performance, as represented by the mean interval length. The results showed that the reported modeling approach accurately estimates the mean interval-length characteristics at scale when compared to other modeling approaches such as, Generalized Pareto Distribution, Generalized Extreme Value, exponential distribution, and Normal. The availability of an analytical model that permits the understanding of the salient characteristics of the mean interval length at scale, which is a major contribution of this paper, would be a key enabler to understanding the interaction patterns between processors in the application. Another benefit of the this modeling approach is understanding the impact of interference mitigation techniques, such as task scheduling techniques (EDF scheduling, for example) on HPC performance.</p>\n<p>The project also examoined the the design and performance of cyberinfrastructure for high performance processing of Big Data, in particular real time processing of large amounts of distributed data through graph analysis.To facilitate this research the project studied a high-performance computing pipeline for graph clustering analysis of streaming data. In this systems, consumer containers retrieve data clusters and process them to create a weighted word co-occurrence graph, which is then the input to application containers that perform the graph clustering analysis. To facilitate efficient communication between the application and consumer components, both the consumer and application modules were housed within the same pod, enabling the use of a shared persistent volume for rapid data transfer from the consumer to the application container. To integrate these and other individual results from the project into a comprehensive output, an additional merge pod has been introduced to combine the results of consumer-app pods.</p>\n<p>To evaluate the accuracy and performance of the pipeline in scenarios such as the spread of information in crises in the political world, data from X, formerly known and referred to here as Twitter, was used as an input source. One data set included data from the Github Russo Ukrainian War Dataset repository, while another used COVID-19 Tweets. The final design enhanced the efficiency and effectiveness of the data processing pipeline but also exposed issues related to data reliability/performance tradeoffs. The results suggest future research in this direction is especially important in light of the data loss challenges posed by large, distributed cloud environments. Additional results also indicate that the parallel version has a faster run time compared to the serial one on larger networks. For smaller networks, however, parallelism adds a significant overhead that is not worth it for smaller datasets.</p>\n<p>The project also provided training of three female graduate students at the University of New Mexico (UNM) and Marquette University. The training included mentoring on the design and development of cloud-based systems and data processing pipelines, the systems issues related to these pipelines, as well as on experiment design for large-scale distributed systems, as well as developing large-scale distributed analysis algorithm.&nbsp;Training on analytical modeling included pertinent concepts from probability and stochastic processes as well as large-deviation theory and the connection to extreme value theory. In addition, weekly seminars were conducted at UNM including all the investigators and students, as well as participants from Sandia National Labs, Los Alamos National Laboratory, and alumni from the laboratory at UNM.</p><br>\n<p>\n Last Modified: 02/27/2024<br>\nModified by: Patrick&nbsp;G&nbsp;Bridges</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe major goal of this research is characterizing, predicting, and analyzing application/system-software performance interactions at scale. This goal is achieved in three major steps. First, by performing stochastic modeling of extreme-scale software through their inter-collective intervals (ICI) in two ways: one based on the direct estimation of the distribution of system software-perturbed ICI lengths, and one based on deriving this quantity from separate characterizations of the stochastic behaviors of the application and system software.Second, by integrating representative decision support analysis that may include estimating scaling impacts (e.g., evaluating how changes in the number of processes running in an application impact application runtime), detecting performance outlier events, and estimating system-software impact on interval distributions. Third, by evaluating the performance modeling abstractions and analysis techniques on a range of extreme-scale applications and programming models.\n\n\nUSing this approach, the projectdesigned a new, scalable, and statistically robust approach for effective modeling, measurement, and analysis of large-scale performance variation in HPC systems. This approach avoided the need to reason about complex distributions of runtimes among large numbers of individual application processes by focusing instead on the maximum length of distributed workload intervals. The evaluation of this approach included examining and quantifying the strenghts and limitations of this modeling approach,\n\n\nThe project complemented this approach by studying the analytical modeling of large-scale application performance using a novel, composite stochastic model to characterize interference and compute-time individually. This approach provided a framework to correctly quantify and predict the impact of rare, colossal interference at scale on the HPC application performance, as represented by the mean interval length. The results showed that the reported modeling approach accurately estimates the mean interval-length characteristics at scale when compared to other modeling approaches such as, Generalized Pareto Distribution, Generalized Extreme Value, exponential distribution, and Normal. The availability of an analytical model that permits the understanding of the salient characteristics of the mean interval length at scale, which is a major contribution of this paper, would be a key enabler to understanding the interaction patterns between processors in the application. Another benefit of the this modeling approach is understanding the impact of interference mitigation techniques, such as task scheduling techniques (EDF scheduling, for example) on HPC performance.\n\n\nThe project also examoined the the design and performance of cyberinfrastructure for high performance processing of Big Data, in particular real time processing of large amounts of distributed data through graph analysis.To facilitate this research the project studied a high-performance computing pipeline for graph clustering analysis of streaming data. In this systems, consumer containers retrieve data clusters and process them to create a weighted word co-occurrence graph, which is then the input to application containers that perform the graph clustering analysis. To facilitate efficient communication between the application and consumer components, both the consumer and application modules were housed within the same pod, enabling the use of a shared persistent volume for rapid data transfer from the consumer to the application container. To integrate these and other individual results from the project into a comprehensive output, an additional merge pod has been introduced to combine the results of consumer-app pods.\n\n\nTo evaluate the accuracy and performance of the pipeline in scenarios such as the spread of information in crises in the political world, data from X, formerly known and referred to here as Twitter, was used as an input source. One data set included data from the Github Russo Ukrainian War Dataset repository, while another used COVID-19 Tweets. The final design enhanced the efficiency and effectiveness of the data processing pipeline but also exposed issues related to data reliability/performance tradeoffs. The results suggest future research in this direction is especially important in light of the data loss challenges posed by large, distributed cloud environments. Additional results also indicate that the parallel version has a faster run time compared to the serial one on larger networks. For smaller networks, however, parallelism adds a significant overhead that is not worth it for smaller datasets.\n\n\nThe project also provided training of three female graduate students at the University of New Mexico (UNM) and Marquette University. The training included mentoring on the design and development of cloud-based systems and data processing pipelines, the systems issues related to these pipelines, as well as on experiment design for large-scale distributed systems, as well as developing large-scale distributed analysis algorithm.Training on analytical modeling included pertinent concepts from probability and stochastic processes as well as large-deviation theory and the connection to extreme value theory. In addition, weekly seminars were conducted at UNM including all the investigators and students, as well as participants from Sandia National Labs, Los Alamos National Laboratory, and alumni from the laboratory at UNM.\t\t\t\t\tLast Modified: 02/27/2024\n\n\t\t\t\t\tSubmitted by: PatrickGBridges\n"
 }
}