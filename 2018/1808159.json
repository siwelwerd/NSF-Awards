{
 "awd_id": "1808159",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Multimodal Sensing and Analytics at Scale: Algorithms and Applications",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": "7032924568",
 "po_email": "hdai@nsf.gov",
 "po_sign_block_name": "Huaiyu Dai",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 249991.0,
 "awd_amount": 265991.0,
 "awd_min_amd_letter_date": "2018-08-21",
 "awd_max_amd_letter_date": "2020-04-27",
 "awd_abstract_narration": "Finding highly correlated latent factors in multimodal signals and data: Scalable algorithms and applications in sensing, imaging, and language processing\r\n\r\nAbstract: Multimodal signals and data arise naturally in many walks of science and engineering, and our digital society presents ever-increasing opportunities to collect and extract useful information from such data. For example, brain magnetic resonance imaging and electro-encephalography are two modes of sensing brain activity that can offer different \"views\" of the same set of patients (entities). Co-occurrence frequencies of a given set of words in different languages is another example. Crime, poverty, welfare, income, tax, school, unemployment, and other types of social data offer different views of a given set of municipalities. Integrating multiple views to extract meaningful common information is of great interest, and finds a vast amount of timely applications -- in brain imaging, machine translation, landscape change detection in remote sensing, and social science research, to name a few.  However, existing multiview analytics tools -- notably (generalized) canonical correlation analysis [(G)CCA] -- are struggling to keep pace with the size of today's datasets, and the problem is only getting worse. Furthermore, the complex structure and dynamic nature of some of the underlying phenomena are not accounted for in classical GCCA. This project will provide much needed scalable and flexible computational tools for GCCA-based multimodal sensing and analytics, thereby benefiting a large variety of scientific and engineering applications. It will produce a framework allowing for plug-and-play incorporation of application-specific prior information, and distributed implementation. Beyond linear and batch GCCA, nonlinear GCCA and streaming GCCA will be considered. These are appealing and timely for many applications, but associated computational tools are sorely missing.\r\n\r\nIn terms of theory and methods, many key aspects of GCCA (such as convergence properties, distributed implementation, and streaming variants) are still poorly understood. The research will provide a set of high-performance computational tools that are backed by advanced optimization theory and rigorous convergence guarantees. The research will evolve along the following synergistic thrusts: 1) scalable and stochastic GCCA algorithms; 2) distributed, streaming and nonlinear GCCA algorithms; and 3) validation, using a series of timely and important applications in remote sensing, brain imaging, natural language processing, and sensor array processing. Devising scalable, flexible, streaming, and nonlinear GCCA algorithms is very well-motivated for modern sensing and analytics problems which involve rapidly increasing amounts of data with unknown underlying dynamics. Using GCCA for large-scale dynamic and complex data also poses very challenging and exciting modeling and optimization problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiao",
   "pi_last_name": "Fu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiao Fu",
   "pi_email_addr": "xiao.fu@oregonstate.edu",
   "nsf_id": "000709147",
   "pi_start_date": "2018-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon State University",
  "inst_street_address": "1500 SW JEFFERSON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CORVALLIS",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5417374933",
  "inst_zip_code": "973318655",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OR04",
  "org_lgl_bus_name": "OREGON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MZ4DYXE1SL98"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "973318507",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OR04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "153E",
   "pgm_ref_txt": "Wireless comm & sig processing"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 249991.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-eb471438-7fff-52b5-79fd-50dbf02e3e4a\"> </span></p>\n<p dir=\"ltr\"><span>Multimodal signals and data are ubiquitously present in the real world. A modality can be regarded as a \"view\" of a certain entity or event (e.g., picture and audio of a cat are two modalities/views of the cat). It is widely believed that common information from multimodalities represents the essential part of the data entity/event. Effectively and efficiently learning or extracting such common information from real world signals/data has been a long-existing aspiration of the signal processing community at large. Multimodal sensing and analysis is concerned with this task, which </span><span>finds a vast amount of timely applications -- in brain imaging, machine translation, remote sensing, and social science research, to name a few. </span><span>&nbsp;This project developed a series of algorithmic and analytical frameworks to solve challenging problems arising in multimodal sensing and analysis. First, a series of efficient, lightweight, flexible, and privacy-preserving algorithms were developed to address the scalability challenge---which had become a key barrier for processing and analyzing the massive amount of multimodal data in modern days. Second, this project developed an in-depth understanding of a number of multimodal sensing and analysis paradigms, which substantially enriched the analytical toolbox and solidified the theoretical foundations of the pertinent domains. Third, this project developed specialized and principled methodologies for timely multimodal sensing problems, e.g., hyperspectral imaging/remote sensing, wireless communications, brain signal processing, and language data analysis.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The outcomes of this project already showed positive impacts on other disciplines. The computational principles developed in this project were used in various domains in science and engineering, e.g., medical imaging. The theoretical results were leveraged to analyze long-lingering challenges in machine learning (e.g., to establish finite-sample identifiability of independent component analysis). This project involved 4 REU students, among whom 3 were from historically underrepresented groups in computing. A Ph.D. student was supported and graduated under this project. These showed positive contributions to building a diverse and competitive workforce in engineering. In addition, this project enriched educational resources in multiple ways.&nbsp; It generated a tutorial article published in IEEE Signal Processing Magazine. The outcomes were integrated into two graduate courses at Oregon State University.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/14/2022<br>\n\t\t\t\t\tModified by: Xiao&nbsp;Fu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nMultimodal signals and data are ubiquitously present in the real world. A modality can be regarded as a \"view\" of a certain entity or event (e.g., picture and audio of a cat are two modalities/views of the cat). It is widely believed that common information from multimodalities represents the essential part of the data entity/event. Effectively and efficiently learning or extracting such common information from real world signals/data has been a long-existing aspiration of the signal processing community at large. Multimodal sensing and analysis is concerned with this task, which finds a vast amount of timely applications -- in brain imaging, machine translation, remote sensing, and social science research, to name a few.  This project developed a series of algorithmic and analytical frameworks to solve challenging problems arising in multimodal sensing and analysis. First, a series of efficient, lightweight, flexible, and privacy-preserving algorithms were developed to address the scalability challenge---which had become a key barrier for processing and analyzing the massive amount of multimodal data in modern days. Second, this project developed an in-depth understanding of a number of multimodal sensing and analysis paradigms, which substantially enriched the analytical toolbox and solidified the theoretical foundations of the pertinent domains. Third, this project developed specialized and principled methodologies for timely multimodal sensing problems, e.g., hyperspectral imaging/remote sensing, wireless communications, brain signal processing, and language data analysis. \nThe outcomes of this project already showed positive impacts on other disciplines. The computational principles developed in this project were used in various domains in science and engineering, e.g., medical imaging. The theoretical results were leveraged to analyze long-lingering challenges in machine learning (e.g., to establish finite-sample identifiability of independent component analysis). This project involved 4 REU students, among whom 3 were from historically underrepresented groups in computing. A Ph.D. student was supported and graduated under this project. These showed positive contributions to building a diverse and competitive workforce in engineering. In addition, this project enriched educational resources in multiple ways.  It generated a tutorial article published in IEEE Signal Processing Magazine. The outcomes were integrated into two graduate courses at Oregon State University.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/14/2022\n\n\t\t\t\t\tSubmitted by: Xiao Fu"
 }
}