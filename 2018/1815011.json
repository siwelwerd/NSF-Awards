{
 "awd_id": "1815011",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Foundations for Collaborative and Information-Limited Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 324884.0,
 "awd_amount": 324884.0,
 "awd_min_amd_letter_date": "2018-08-28",
 "awd_max_amd_letter_date": "2018-08-28",
 "awd_abstract_narration": "Machine learning increasingly is being used throughout society, and in a wide range of applications.  Businesses use machine learning systems for decision support, internet sites use machine learning to better interact with users, our personal devices use machine learning to adapt to our needs, and our cars are beginning to use data-trained systems to improve safety.  These applications bring up new opportunities as well as new concerns. Opportunities include the potential for systems to more rapidly learn and adapt through collaboration, and concerns include privacy and the fairness of algorithmically-made decisions.  This project is aimed at developing new foundational understanding of these opportunities and concerns, to help guide the development of more efficient, more adaptive, and fairer, machine learning methods.  This project additionally will support educational workshops on these issues, and more broadly will support the education and training of young scientists on these topics.\r\n\r\nSpecifically, this project has the following four main thrusts: (1) Collaborative Machine Learning. How can devices with related learning tasks best collaborate to learn efficiently from only a modest amount of data, and how can privacy and related concerns be addressed?  (2) Property Testing and Error Extrapolation.  This thrust aims to develop methods that, from a small amount of labeled data, can reliably estimate how well a given learning algorithm or representation class would perform if given a much larger labeled data sample.  (3) Semi-Supervised Learning. Semi-supervised learning refers to methods that combine labeled and unlabeled data, to learn well even when labeled data is limited. This work aims to develop theoretical foundations for an approach based on explicitly learning regularities within the unlabeled data and then using these to guide how learning is performed over the labeled data. (4)  Fairness in Learning. There has recently been substantial concern about algorithmic decisions (such as whether to offer an applicant a loan) that could unfairly discriminate against certain classes of people. This work aims to develop improved theoretical understanding, tools, and guarantees for tackling these kinds of problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Avrim",
   "pi_last_name": "Blum",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Avrim Blum",
   "pi_email_addr": "avrim@ttic.edu",
   "nsf_id": "000445508",
   "pi_start_date": "2018-08-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Toyota Technological Institute at Chicago",
  "inst_street_address": "6045 S KENWOOD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7738340409",
  "inst_zip_code": "606372803",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO",
  "org_prnt_uei_num": "ERBJF4DMW6G4",
  "org_uei_num": "ERBJF4DMW6G4"
 },
 "perf_inst": {
  "perf_inst_name": "Toyota Technological Institute at Chicago",
  "perf_str_addr": "6045 S. Kenwood Ave.",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606372803",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 324884.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project advanced the theoretical foundations for several core challenges in developing socially-responsible and robust machine learning systems.</p>\n<p>One main direction of this project addressed the problem of improving fairness in machine learning algorithms. Towards this end, we developed an algorithm that can make existing algorithmic decision-making systems more fair by seamlessly incorporating improved predictors for mistreated subgroups when such predictors are found.&nbsp; Importantly, our incorporation procedure is able to do so while provably not sacrificing performance on other groups even when the various subgroups overlap.&nbsp; We also performed a mathematical analysis of how biased training data can affect the classifiers produced by machine learning algorithms, and the extent to which different fairness constraints can alleviate (or worsen) the problems caused by different kinds of bias that may be present in data.&nbsp; Finally, we developed methods for improving fairness in multi-stage selection processes, where individuals must pass a series of tests or evaluations that might each have some bias in them.</p>\n<p>A second direction involved collaborative learning, and specifically the development of algorithms that aim to reduce both communication costs and sample-size requirements when multiple agents collaborate to learn a task together.&nbsp; This work also addressed the challenge of incentivizing agents to maintain their collaboration.</p>\n<p>A third direction involved development of algorithms for strategic classification.&nbsp; Strategic classification refers to settings where data points correspond to entities (e.g., individuals or companies) who have the ability to manipulate their observable features to a limited extent, and who will do so if it allows them to be classified in the way they desire.&nbsp; Thus, the decision-maker must use a decision rule that factors this ability to manipulate into the decision process.&nbsp; We developed algorithms for a challenging adaptive setting of this problem, in which the decision-maker has limited information about true feature values because even the training data has been manipulated.</p>\n<p>Finally, this work investigated the challenge of designing machine learning algorithms that are robust to data-poisoning attacks, in which corrupted data points have been secretly added to a training set by an attacker in order to induce various kinds of misbehaviors.&nbsp; Our work developed new understanding of several forms of such attacks including (1) clean-label data poisoning, where the adversarial training points may cause damage even if they are provided with the correct labels, (2) backdoor data poisoning attacks, where the adversarial training points are designed to only influence misbehavior when specific triggers are observed, and (3) targeted dirty-label data poisoning, where the adversarial data is designed to cause mistakes on specific test examples. &nbsp;One key contribution of this work is introducing the notion of robustly-reliable predictions, in which predictions are provided with certificates that indicate the conditions under which they can be confidently relied upon.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/18/2022<br>\n\t\t\t\t\tModified by: Avrim&nbsp;Blum</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project advanced the theoretical foundations for several core challenges in developing socially-responsible and robust machine learning systems.\n\nOne main direction of this project addressed the problem of improving fairness in machine learning algorithms. Towards this end, we developed an algorithm that can make existing algorithmic decision-making systems more fair by seamlessly incorporating improved predictors for mistreated subgroups when such predictors are found.  Importantly, our incorporation procedure is able to do so while provably not sacrificing performance on other groups even when the various subgroups overlap.  We also performed a mathematical analysis of how biased training data can affect the classifiers produced by machine learning algorithms, and the extent to which different fairness constraints can alleviate (or worsen) the problems caused by different kinds of bias that may be present in data.  Finally, we developed methods for improving fairness in multi-stage selection processes, where individuals must pass a series of tests or evaluations that might each have some bias in them.\n\nA second direction involved collaborative learning, and specifically the development of algorithms that aim to reduce both communication costs and sample-size requirements when multiple agents collaborate to learn a task together.  This work also addressed the challenge of incentivizing agents to maintain their collaboration.\n\nA third direction involved development of algorithms for strategic classification.  Strategic classification refers to settings where data points correspond to entities (e.g., individuals or companies) who have the ability to manipulate their observable features to a limited extent, and who will do so if it allows them to be classified in the way they desire.  Thus, the decision-maker must use a decision rule that factors this ability to manipulate into the decision process.  We developed algorithms for a challenging adaptive setting of this problem, in which the decision-maker has limited information about true feature values because even the training data has been manipulated.\n\nFinally, this work investigated the challenge of designing machine learning algorithms that are robust to data-poisoning attacks, in which corrupted data points have been secretly added to a training set by an attacker in order to induce various kinds of misbehaviors.  Our work developed new understanding of several forms of such attacks including (1) clean-label data poisoning, where the adversarial training points may cause damage even if they are provided with the correct labels, (2) backdoor data poisoning attacks, where the adversarial training points are designed to only influence misbehavior when specific triggers are observed, and (3) targeted dirty-label data poisoning, where the adversarial data is designed to cause mistakes on specific test examples.  One key contribution of this work is introducing the notion of robustly-reliable predictions, in which predictions are provided with certificates that indicate the conditions under which they can be confidently relied upon.\n\n\t\t\t\t\tLast Modified: 12/18/2022\n\n\t\t\t\t\tSubmitted by: Avrim Blum"
 }
}