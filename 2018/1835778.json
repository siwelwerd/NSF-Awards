{
 "awd_id": "1835778",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927092",
 "po_email": "alsuarez@nsf.gov",
 "po_sign_block_name": "Alejandro Suarez",
 "awd_eff_date": "2018-11-01",
 "awd_exp_date": "2022-10-31",
 "tot_intn_awd_amt": 254103.0,
 "awd_amount": 254103.0,
 "awd_min_amd_letter_date": "2018-08-23",
 "awd_max_amd_letter_date": "2018-08-23",
 "awd_abstract_narration": "This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation.  A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. \r\n\r\nThe goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations.  The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems.  Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis.   The software framework from this project is expected to handle petascale to exascale datasets for users.  Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store.  The broader target is next generation simulation software in the geosciences and other disciplines.  \r\n\r\nThis award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ryan",
   "pi_last_name": "Abernathey",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ryan Abernathey",
   "pi_email_addr": "ra2697@columbia.edu",
   "nsf_id": "000650948",
   "pi_start_date": "2018-08-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University Lamont-Doherty Earth Observatory",
  "perf_str_addr": "61 Route 9W",
  "perf_city_name": "Palisades",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "109648000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "17",
  "perf_st_cong_dist": "NY17",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "772600",
   "pgm_ele_name": "Data Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "077Z",
   "pgm_ref_txt": "CSSI-1: Cyberinfr for Sustained Scientif"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 254103.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project aimed to address the growing challenge posed by the data produced by modern high-resolution ocean models. There are two main aspects to this challenge:</p>\n<ul>\n<li>The raw size of the outputs. For example, the model we worked with in this project, MITgcm LL4320, comprised over 2 Petabytes of data.</li>\n<li>The complexity of the data. The LLC4320 uses a complex lat-lon cap geometry to cover the sphere, leading to difficulty ing analyzing and visualization the fields</li>\n</ul>\n<p>We prototyped solutions to both of these challenges.</p>\n<p>For the <em>data volume challenge</em>, we sought to move beyond the \"download model\" of distributing ocean model data. Recognizing that downloading data files is cumbersome and infeasable at the petabyte scale, we explored different approaches to support data-proximate computing and cloud-native data access. This included exploring new file formats, such as Zarr, which are optimized for efficient access over very large arrays. We deployed a data-proximate computing environment--SciServer--at Johns Hopkins, which provides users with the ability to run calculations on a system directly connected to the data. We also explored using commercial cloud storage and cloud computing to address data access. Specifically, we placed a subset of the data in Google Cloud Storage and, in collaboration with the Pangeo Project, deployed computing environments next to the cloud data. Both approaches provide a feasible alternative to the download model.</p>\n<p>For the <em>data complexity challenge</em>, we developed a range of open-source software solutions aimed at making it easier for regular scientists to anaylze and visualize simulation data. These tools include Python package such as xgcm, OceanSpy, Poseidon-Viewer, and the SciServer computing system.</p>\n<p>We hope that these prototypes and tools can aid the community in transitioning to a more scalable approach to ocean model data infrastructure.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/19/2023<br>\n\t\t\t\t\tModified by: Ryan&nbsp;Abernathey</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project aimed to address the growing challenge posed by the data produced by modern high-resolution ocean models. There are two main aspects to this challenge:\n\nThe raw size of the outputs. For example, the model we worked with in this project, MITgcm LL4320, comprised over 2 Petabytes of data.\nThe complexity of the data. The LLC4320 uses a complex lat-lon cap geometry to cover the sphere, leading to difficulty ing analyzing and visualization the fields\n\n\nWe prototyped solutions to both of these challenges.\n\nFor the data volume challenge, we sought to move beyond the \"download model\" of distributing ocean model data. Recognizing that downloading data files is cumbersome and infeasable at the petabyte scale, we explored different approaches to support data-proximate computing and cloud-native data access. This included exploring new file formats, such as Zarr, which are optimized for efficient access over very large arrays. We deployed a data-proximate computing environment--SciServer--at Johns Hopkins, which provides users with the ability to run calculations on a system directly connected to the data. We also explored using commercial cloud storage and cloud computing to address data access. Specifically, we placed a subset of the data in Google Cloud Storage and, in collaboration with the Pangeo Project, deployed computing environments next to the cloud data. Both approaches provide a feasible alternative to the download model.\n\nFor the data complexity challenge, we developed a range of open-source software solutions aimed at making it easier for regular scientists to anaylze and visualize simulation data. These tools include Python package such as xgcm, OceanSpy, Poseidon-Viewer, and the SciServer computing system.\n\nWe hope that these prototypes and tools can aid the community in transitioning to a more scalable approach to ocean model data infrastructure. \n\n\t\t\t\t\tLast Modified: 04/19/2023\n\n\t\t\t\t\tSubmitted by: Ryan Abernathey"
 }
}