{
 "awd_id": "1816148",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Uncovering Dynamics from Internet Imagery",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2018-08-10",
 "awd_max_amd_letter_date": "2019-07-22",
 "awd_abstract_narration": "Virtual- and augmented reality (VR/AR) technologies have the promise to enable new and exciting ways of perceiving the world from the comfort of our homes and desks. Among the current applications of 3D VR visualizations, obtaining realistic depictions of actual real-world environments is highly desired in educational experiences. This project will develop scalable algorithms for computing \"living 3D models\" that can represent elements such as people and cars moving around the scene, water flowing in fountains, or chairs outside cafes being placed in different places on different days. To overcome the need for dedicated capture, the project targets publicly available Internet photo collections, which have the requisite data diversity to drive large-scale, cost-effective VR/AR content generation. The research not only supports the field of VR/AR but also provides improved analysis methods for a broad range of other applications, including forensic analysis, cultural heritage conversation, city planning, virtual training, and education, with particularly potential impact in enhancing social study experiences for economically disadvantaged students. \r\n\r\nThis project will aggregate object instances in the individual 2D images of the photo collection to infer the motion dynamics of the entire class of objects in the scene, e.g., all cars or all people. The method will thus infer and model the motion dynamics without ever seeing the motion of these objects, since there is typically only one observation per object instance available due to the uncontrolled, crowd-sourced capture. The key information for the inference will be the observation of the varying densities of the dynamic scene elements in the scene. The novel scene representation stores the accumulated dynamics in object class scene occupancy maps, as well as object class motion flows for the scene, e.g., the information where pedestrians move to in the scene and how they move within the scene. The developed methodology will open new and exciting avenues for research on jointly recovering semantic labels and 3D geometry in the wild, a task that is one of the currently most challenging problems in 3D computer vision.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Henry",
   "pi_last_name": "Fuchs",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Henry Fuchs",
   "pi_email_addr": "fuchs@cs.unc.edu",
   "nsf_id": "000451367",
   "pi_start_date": "2019-07-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Jan-Michael",
   "pi_last_name": "Frahm",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jan-Michael Frahm",
   "pi_email_addr": "jmf@cs.unc.edu",
   "nsf_id": "000427356",
   "pi_start_date": "2018-08-10",
   "pi_end_date": "2019-07-22"
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Chapel Hill",
  "perf_str_addr": "201 S. Columbia St",
  "perf_city_name": "Chapel Hill",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275993175",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">This project focused on uncovering dynamics from video imagery, to recover motion of humans from videos found on the internet and videos taken in the laboratory or in a home or outdoors.<span>&nbsp; </span>One result was to reconstruct, from videos found on the internet, crowds of pedestrians walking around in outdoor spaces. Previous techniques only reconstructed the streets and buildings, not the people walking around them. The new techniques developed in this project added the people (the pedestrians) to bring these 3D reconstructions ?to life,? so the observer can view the outdoor space, with moving pedestrians, from a variety of viewpoints, like the view from flying a helicopter around that space.</p>\n<p class=\"p1\">To help these 3D reconstruction of outdoor human-inhabited environments, this project also developed new techniques for reconstructing 3D buildings when there is available only a single overhead view.</p>\n<p class=\"p2\">This project also developed improved reconstruction of humans from multiple nearby cameras, as in indoor scenes where a few cameras are set up around a space.</p>\n<p class=\"p1\">Finally, this project developed improved methods for 3D reconstructing of a moving human in the situation where the <em>only</em> technology available is that which is worn unobtrusively by that human ? specifically a few tiny cameras embedded in the frames of the user?s eyeglasses and miniature motion sensors (IMUs) embedded in the user?s watch, and/or fitness band, and/or shoes or belt buckles. (Thus, for example, there is no need for cameras mounted on tripods around the space that the user is moving in.) This severe limitation of relying only on unobtrusive wearable technology was adopted so that the 3D reconstruction of the moving user can be performed anywhere, anytime, indoors and outdoors, as the user goes about their daily life. The initial application focus in this project was on healthcare applications, for which the user may be a patient whose motions are to be monitored by a physical therapist or a neurologist.<span>&nbsp; </span>Prior methods either relied on some external technology, such as an external nearby camera. Such a camera might <em>not</em> be available in certain important situations, for example, when the user/patient is attempting to get into or out of a car in a parking lot. Further, prior methods that relied only on wearable technology but not wearable cameras, such as only motion sensors, would not be able to reconstruct the scene around the user/patient, for example if the user were holding on to the car handle while trying to get out of the car, or the user holding some bulky object while trying to get out of the car. The techniques developed in this project, combining downward- and forward-looking camera imagery in eyeglass frames with wearable motion sensors, enabled 3D reconstruction both of the user movement and reconstruction of the appearance of the user?s body and (in the future) reconstruction of nearby objects.</p>\n<p class=\"p2\">With the anticipated developments in still smaller, miniature cameras and increased computation power, we expect that future eyeglasses with embedded tiny cameras and motion sensors will allow detailed 3D reconstruction of both the dynamics of the users movement as well as realistic reconstruction of the user?s appearance and the 3D reconstruction and appearance of nearby objects. These capabilities will open vast possibilities in our daily lives, including in healthcare, in early detection of abnormalities, as well as 3D immersive teleconferencing, anywhere, anytime.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/10/2023<br>\n\t\t\t\t\tModified by: Henry&nbsp;Fuchs</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "This project focused on uncovering dynamics from video imagery, to recover motion of humans from videos found on the internet and videos taken in the laboratory or in a home or outdoors.  One result was to reconstruct, from videos found on the internet, crowds of pedestrians walking around in outdoor spaces. Previous techniques only reconstructed the streets and buildings, not the people walking around them. The new techniques developed in this project added the people (the pedestrians) to bring these 3D reconstructions ?to life,? so the observer can view the outdoor space, with moving pedestrians, from a variety of viewpoints, like the view from flying a helicopter around that space.\nTo help these 3D reconstruction of outdoor human-inhabited environments, this project also developed new techniques for reconstructing 3D buildings when there is available only a single overhead view.\nThis project also developed improved reconstruction of humans from multiple nearby cameras, as in indoor scenes where a few cameras are set up around a space.\nFinally, this project developed improved methods for 3D reconstructing of a moving human in the situation where the only technology available is that which is worn unobtrusively by that human ? specifically a few tiny cameras embedded in the frames of the user?s eyeglasses and miniature motion sensors (IMUs) embedded in the user?s watch, and/or fitness band, and/or shoes or belt buckles. (Thus, for example, there is no need for cameras mounted on tripods around the space that the user is moving in.) This severe limitation of relying only on unobtrusive wearable technology was adopted so that the 3D reconstruction of the moving user can be performed anywhere, anytime, indoors and outdoors, as the user goes about their daily life. The initial application focus in this project was on healthcare applications, for which the user may be a patient whose motions are to be monitored by a physical therapist or a neurologist.  Prior methods either relied on some external technology, such as an external nearby camera. Such a camera might not be available in certain important situations, for example, when the user/patient is attempting to get into or out of a car in a parking lot. Further, prior methods that relied only on wearable technology but not wearable cameras, such as only motion sensors, would not be able to reconstruct the scene around the user/patient, for example if the user were holding on to the car handle while trying to get out of the car, or the user holding some bulky object while trying to get out of the car. The techniques developed in this project, combining downward- and forward-looking camera imagery in eyeglass frames with wearable motion sensors, enabled 3D reconstruction both of the user movement and reconstruction of the appearance of the user?s body and (in the future) reconstruction of nearby objects.\nWith the anticipated developments in still smaller, miniature cameras and increased computation power, we expect that future eyeglasses with embedded tiny cameras and motion sensors will allow detailed 3D reconstruction of both the dynamics of the users movement as well as realistic reconstruction of the user?s appearance and the 3D reconstruction and appearance of nearby objects. These capabilities will open vast possibilities in our daily lives, including in healthcare, in early detection of abnormalities, as well as 3D immersive teleconferencing, anywhere, anytime.\n\n \n\n\t\t\t\t\tLast Modified: 01/10/2023\n\n\t\t\t\t\tSubmitted by: Henry Fuchs"
 }
}