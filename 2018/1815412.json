{
 "awd_id": "1815412",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Data Parallel Frameworks for Large-scale Machine Learning through Sync-on-the-Fly",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032920000",
 "po_email": "doliveir@nsf.gov",
 "po_sign_block_name": "Daniela Oliveira",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 489999.0,
 "awd_amount": 489999.0,
 "awd_min_amd_letter_date": "2018-07-29",
 "awd_max_amd_letter_date": "2018-07-29",
 "awd_abstract_narration": "The advances in sensing, storage, and networking technologies have led to the collections of high-volume, high-dimensional data. Making sense of these data is critical for companies and organizations to make better business decisions, to bring convenience to our daily life, and even enable better health through biological information and drug discovery. Recent advances in machine learning have led to a flurry of data analytic techniques that typically require an iterative refinement process. However, the massive amount of data involved and potentially numerous iterations required make performing data analytics in a timely manner challenging.\r\n\r\nThis project aims to design and implement a data parallel programming framework called Sync-on-the-fly.  The framework enables machine learning computations within cloud computing environments to establish synchronization barriers during the execution of the computations. Since synchronization barriers are established for building consistent model parameters, they do not need to be predefined.  The barriers can be established during the computation based on the progress of the computation. This data parallel programming model preserves the semantics of machine learning algorithms. The goals are to build theoretical foundations and create efficient distributed frameworks for a series of well-known machine learning algorithms, and establish the programming models for these computations.\r\n\r\nThe technologies developed from this project will have immediate important applications on road traffic prediction, biological information discovery, online marketing, and computer forensic analysis. This project will bring fast, accurate, and cost-effective processing of massive data to users. This project will also train new graduate engineers in distributed framework design, machine learning algorithms, and big data analytics. All of these skillsets are in broad demand in US industry.\r\n\r\nThe data and software codes produced for the distributed framework and distributed machine learning algorithms will be made publicly available at the research website http://rio.ecs.umass.edu/html/research/index.html. The website will be retained for at least three years after the conclusion of the project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lixin",
   "pi_last_name": "Gao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lixin Gao",
   "pi_email_addr": "lgao@ecs.umass.edu",
   "nsf_id": "000483181",
   "pi_start_date": "2018-07-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039284",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 489999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The research in this project has lead to the development of a series of new theory, techniques, and technologies for the development of the data parallel frameworks for large-scale machine learning. The research directly and substantially advances the literature in the principal disciplines, noticeably including machine learning, distributed computing, and bigdata analytics. Meanwhile, the research in this project may also benefit the advance in the research in the other disciplines noticeably including but not necessarily limited to biological science, geographical and geological sciences, library sciences, social sciences, and bioinformatics. Examples of part of the outcomes directly resulting from the research of the project include:</p>\n<p>-Fault-tolerant framework for Asynchronous Iterative Computations. We capture the messages by leading the asynchronous iterative computation system into a state where inflight messages are absorbed by workers. And workers keep on computing during that period. Instead of forcing all workers to reach such a state at the same time, we design a method that captures the messages and local state on each worker independently. We make a checkpoint of the system by constructing these local states captured at different moment into a <em>virtual snapshot</em>.</p>\n<p>-Distributed framework that accelerates the training process of deep learning models through prioritizing the execution of gradient descent. The proposed distributed framework automatically identifies and utilizes the most important data points to update the gradient of the model parameters. &nbsp;Our distributed framework can be applied to a large variety of deep learning models solved with stochastic gradient descent, including multi-layer perceptron (MLP), convolutional neural network (CNN), and recurrent neural network (RNN).</p>\n<p>-Distributed Framework for Graph Convolutional Networks using cohesive mini-batches to accelerate large-scale GCN training in CPU clusters. The cohesive mini-batches group nodes are tightly connected in the graph. These nodes will share the common neighbors to reduce the dependent nodes in GCN training. With the reduction of dependent nodes, we can reduce more than half of the computation required in each epoch. We propose a computation cost function to efficiently derive the required computation for each mini-batch. After proving the submodular property of the computation cost function, we develop an efficient algorithm to partition the graph nodes into cohesive mini-batches in polynomial time.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2023<br>\n\t\t\t\t\tModified by: Lixin&nbsp;Gao</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe research in this project has lead to the development of a series of new theory, techniques, and technologies for the development of the data parallel frameworks for large-scale machine learning. The research directly and substantially advances the literature in the principal disciplines, noticeably including machine learning, distributed computing, and bigdata analytics. Meanwhile, the research in this project may also benefit the advance in the research in the other disciplines noticeably including but not necessarily limited to biological science, geographical and geological sciences, library sciences, social sciences, and bioinformatics. Examples of part of the outcomes directly resulting from the research of the project include:\n\n-Fault-tolerant framework for Asynchronous Iterative Computations. We capture the messages by leading the asynchronous iterative computation system into a state where inflight messages are absorbed by workers. And workers keep on computing during that period. Instead of forcing all workers to reach such a state at the same time, we design a method that captures the messages and local state on each worker independently. We make a checkpoint of the system by constructing these local states captured at different moment into a virtual snapshot.\n\n-Distributed framework that accelerates the training process of deep learning models through prioritizing the execution of gradient descent. The proposed distributed framework automatically identifies and utilizes the most important data points to update the gradient of the model parameters.  Our distributed framework can be applied to a large variety of deep learning models solved with stochastic gradient descent, including multi-layer perceptron (MLP), convolutional neural network (CNN), and recurrent neural network (RNN).\n\n-Distributed Framework for Graph Convolutional Networks using cohesive mini-batches to accelerate large-scale GCN training in CPU clusters. The cohesive mini-batches group nodes are tightly connected in the graph. These nodes will share the common neighbors to reduce the dependent nodes in GCN training. With the reduction of dependent nodes, we can reduce more than half of the computation required in each epoch. We propose a computation cost function to efficiently derive the required computation for each mini-batch. After proving the submodular property of the computation cost function, we develop an efficient algorithm to partition the graph nodes into cohesive mini-batches in polynomial time.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/30/2023\n\n\t\t\t\t\tSubmitted by: Lixin Gao"
 }
}