{
 "awd_id": "1815358",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Collaborative Research: Explainable Natural Language Inference",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 244537.0,
 "awd_amount": 252537.0,
 "awd_min_amd_letter_date": "2018-07-27",
 "awd_max_amd_letter_date": "2020-06-26",
 "awd_abstract_narration": "Natural language inference (NLI) can support decision-making using information contained in natural language texts (e.g, detecting undiagnosed medical conditions in medical records, finding alternate treatments from scientific literature). This requires gathering facts extracted from text and reasoning over them. Current automated solutions for NLI are largely incapable of producing explanations for their inferences, but this capacity is essential for users to trust their reasoning in domains such as scientific discovery and medicine where the cost of making errors is high. This project develops natural language inference methods that are both accurate and explainable. They are accurate because they build on state-of-the-art deep learning frameworks which use powerful, automatically learned, representations of text. They are explainable because they aggregate information in units that can be represented in both a human readable explanation and a machine-usable vector representation. This project will advance methods in explainable natural language inference to enable the application of automated inference methods in critical domains such as medical knowledge extraction. The project will also evaluate the explainability of the inference decisions in collaboration with domain experts.\r\n\r\nThis project reframes natural language inference as the task of constructing and reasoning over explanations. In particular, inference assembles smaller component facts into a graph (explanation graph) that it reasons over to make decisions. In this view, generating explanations is an integral part of the inference process and not a separate post-hoc mechanism. The project has three main goals: (a) Develop multiagent reinforcement learning models that can effectively and efficiently explore the space of explanation graphs, (b) Develop deep learning based aggregation mechanisms that can prevent inference from combining semantically incompatible evidence, and (c) Build a continuum of hypergraph based text representations that combine discrete forms of structured knowledge with their continuous embedding based representations. The techniques will be evaluated on three application domains: complex question answering, medical relation extraction, and clinical event detection from medical records. The results of the project will be disseminated through the project website, scholarly venues, and the software and datasets will be made available to the public.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Niranjan",
   "pi_last_name": "Balasubramanian",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Niranjan Balasubramanian",
   "pi_email_addr": "niranjan@cs.stonybrook.edu",
   "nsf_id": "000678413",
   "pi_start_date": "2018-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "Department of Computer Science,",
  "perf_city_name": "Stony Brook",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117946999",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 244537.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-bc31e2be-7fff-e043-679c-09f32e0921ab\"> </span></p>\n<p dir=\"ltr\">Explainable natural language inference is critical for building Natural Language Processing systems that are both reliable and trustworthy. This research aimed at developing methods and datasets that can help build explainable NLP systems in information access applications such as&nbsp; question answering and relation extraction. As an example, consider a question answering (QA) system that provides an answer to a question. If it is also able to provide the pieces of information that it used to arrive at its answer then a user can assess if the answer is correct. And when the system returns incorrect&nbsp; answers, a deployer or developer of such a system can attempt to debug why the system failed thus providing a way to improve the system.</p>\n<p dir=\"ltr\"><span>One of the main difficulties in developing explainable models is the lack of training&nbsp; data -- i.e. examples of the input, output and desired explanations. While there were plenty of question and&nbsp; answer pair datasets, obtaining explanations is much more laborious to create. A related difficulty this presents is that when we build systems that are only trained to only identify the correct answers, they tend to figure out any artifact or spurious correlation that might exist between question and answers in the datasets that they train on. This then results in models that are (A) unreliable -- i.e. ones that don&rsquo;t do the correct reasoning we expect of them, and (B) not explainable -- i.e. ones that can only provide answers but no useful explanations. This project made multiple contributions in addressing this challenge which can be described in four main threads.</span></p>\n<p dir=\"ltr\"><span>Thread one focused on building QA models that are structured in such a way that they are forced to take good intermediate steps i.e. finding useful sentences that contain necessary information for arriving at the final answer. This naturally lends itself to building models that can provide high quality explanations. We showed how existing ideas in verifying entailment (figuring out if one piece of text supports information in another) can be used to assemble a QA system of this kind.</span></p>\n<p dir=\"ltr\"><span>Thread two focused on minimizing unreliable reasoning by formalizing one type of bad reasoning in QA models and developing new datasets to discourage this behavior. When there is no supervision for intermediate steps models can latch on to any artifact in the data and perform what we call disconnected reasoning. If we can somehow catch when models are not identifying or using all the information they are supposed to use when answering a question then we can address this problem. We developed ways to transform existing datasets to detect and discourage disconnected reasoning.</span></p>\n<p dir=\"ltr\"><span>Another important contribution we make here is to show how to construct reliable and explainable QA datasets.&nbsp;</span>A multihop question, i.e. one which requires multiple pieces of information, can be seen as composed of multiple single-hop questions. This naturally provides a way to construct (and filter) a large collection of multihop questions by connecting single-hop questions where the answer of one question is part of another question. More importantly, this gives us questioons with identified sub-steps, thus providing examples for models to learn to do the intermediate steps and provide supervision for explanations.&nbsp;</p>\n<p dir=\"ltr\"><span>Thread three looked at pushing the scope for what constitutes an explanation for multihop question answering. Previous datasets mostly identified existing spans of text within the inputs to the QA system as explanations. However, this is neither adequate nor concise.&nbsp; A human providing an explanation would summarize the relevant information and not simply read out the relevant sentences. Again we need examples of such explanations at scale which is expensive and time consuming to obtain. We show that we can make use of existing ideas in abstractive summarization to provide useful compressed summaries which can then be used by a QA model. Further, we introduce a reinforcement&nbsp; learning framework where we turn notions of explanation quality into rewards to improve the quality of the models ability to find most useful information for answering a question.</span></p>\n<p>Thread four looked at explainable biomedical relation extraction. The main contribution is in formalizing a new task forcing models to learn to not only identify relations between biomedical entities, but also to figure out what is the biomedical mechanism that connects and justifies the inferred relation. The key challenge is the lack of large scale datasets. We show how relatively small amounts of domain expert time can be used to first identify examples of these biomedical mechanisms and then use this to bootstrap creation of a large weakly labeled dataset for this task.&nbsp;</p>\n<p dir=\"ltr\"><span>Overall this project made useful algorithmic advances in terms of methods for building explainable NLP systems and resulted in resources via large scale datasets for question answering and relation extraction which we hope will further advances in these areas.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2021<br>\n\t\t\t\t\tModified by: Niranjan&nbsp;Balasubramanian</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nExplainable natural language inference is critical for building Natural Language Processing systems that are both reliable and trustworthy. This research aimed at developing methods and datasets that can help build explainable NLP systems in information access applications such as  question answering and relation extraction. As an example, consider a question answering (QA) system that provides an answer to a question. If it is also able to provide the pieces of information that it used to arrive at its answer then a user can assess if the answer is correct. And when the system returns incorrect  answers, a deployer or developer of such a system can attempt to debug why the system failed thus providing a way to improve the system.\nOne of the main difficulties in developing explainable models is the lack of training  data -- i.e. examples of the input, output and desired explanations. While there were plenty of question and  answer pair datasets, obtaining explanations is much more laborious to create. A related difficulty this presents is that when we build systems that are only trained to only identify the correct answers, they tend to figure out any artifact or spurious correlation that might exist between question and answers in the datasets that they train on. This then results in models that are (A) unreliable -- i.e. ones that don\u2019t do the correct reasoning we expect of them, and (B) not explainable -- i.e. ones that can only provide answers but no useful explanations. This project made multiple contributions in addressing this challenge which can be described in four main threads.\nThread one focused on building QA models that are structured in such a way that they are forced to take good intermediate steps i.e. finding useful sentences that contain necessary information for arriving at the final answer. This naturally lends itself to building models that can provide high quality explanations. We showed how existing ideas in verifying entailment (figuring out if one piece of text supports information in another) can be used to assemble a QA system of this kind.\nThread two focused on minimizing unreliable reasoning by formalizing one type of bad reasoning in QA models and developing new datasets to discourage this behavior. When there is no supervision for intermediate steps models can latch on to any artifact in the data and perform what we call disconnected reasoning. If we can somehow catch when models are not identifying or using all the information they are supposed to use when answering a question then we can address this problem. We developed ways to transform existing datasets to detect and discourage disconnected reasoning.\nAnother important contribution we make here is to show how to construct reliable and explainable QA datasets. A multihop question, i.e. one which requires multiple pieces of information, can be seen as composed of multiple single-hop questions. This naturally provides a way to construct (and filter) a large collection of multihop questions by connecting single-hop questions where the answer of one question is part of another question. More importantly, this gives us questioons with identified sub-steps, thus providing examples for models to learn to do the intermediate steps and provide supervision for explanations. \nThread three looked at pushing the scope for what constitutes an explanation for multihop question answering. Previous datasets mostly identified existing spans of text within the inputs to the QA system as explanations. However, this is neither adequate nor concise.  A human providing an explanation would summarize the relevant information and not simply read out the relevant sentences. Again we need examples of such explanations at scale which is expensive and time consuming to obtain. We show that we can make use of existing ideas in abstractive summarization to provide useful compressed summaries which can then be used by a QA model. Further, we introduce a reinforcement  learning framework where we turn notions of explanation quality into rewards to improve the quality of the models ability to find most useful information for answering a question.\n\nThread four looked at explainable biomedical relation extraction. The main contribution is in formalizing a new task forcing models to learn to not only identify relations between biomedical entities, but also to figure out what is the biomedical mechanism that connects and justifies the inferred relation. The key challenge is the lack of large scale datasets. We show how relatively small amounts of domain expert time can be used to first identify examples of these biomedical mechanisms and then use this to bootstrap creation of a large weakly labeled dataset for this task. \nOverall this project made useful algorithmic advances in terms of methods for building explainable NLP systems and resulted in resources via large scale datasets for question answering and relation extraction which we hope will further advances in these areas.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/30/2021\n\n\t\t\t\t\tSubmitted by: Niranjan Balasubramanian"
 }
}