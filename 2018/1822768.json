{
 "awd_id": "1822768",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Teachers are the Learners: Providing Automated Feedback on Classroom Inter-Personal Dynamics",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925126",
 "po_email": "abaylor@nsf.gov",
 "po_sign_block_name": "Amy Baylor",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 749969.0,
 "awd_amount": 775969.0,
 "awd_min_amd_letter_date": "2018-07-29",
 "awd_max_amd_letter_date": "2021-05-19",
 "awd_abstract_narration": "The quality of teacher-student interactions in school classrooms both predicts and impacts students' learning outcomes. Training teachers to perceive subtle interactions and interpersonal classroom dynamics more accurately can help them to implement more effective interactions in their own classrooms. Contemporary methods of training teachers to understand classroom interactions are based mostly on watching classroom observation videos of other teachers, which have been annotated for different dimensions (\"positive climate\", \"teacher sensitivity\", etc.). Only rarely do teachers receive personalized feedback on their own classroom interactions captured in video, and when they do, it is sparse - typically one comment for every 15-minute video segment without any details. This project will automate classroom observations using a system called Automatic Classroom Observation Recognition neural Network (ACORN). This system will integrate multimodal features consisting of facial expression, eye gaze, auditory emotion, speech, and language in order to assess classroom dynamics automatically. As a complement to ACORN, the researchers will also develop a Classroom Observation Interactive Learning System (COILS) that trains teachers to perceive classroom dynamics more precisely.\r\n\r\nACORN will be trained and tested on two coded classroom observation datasets of hundreds of pre-school and elementary school teachers across the USA. Moreover, based on the ACORN prototype, COILS will be developed. COILS will then be evaluated in a study on 50 pre-service teachers. The research questions are: 1) Will the observation training with COILS help them perceive classroom interactions more precisely? 2) How well will ACORN perform vs human coders? and 3) How well can the machine  learned automated subjective activity perform in the new domain of classroom dynamics?  The researchers will also explore different machine learning computational architectures that can utilize modest-sized data sets to accurately learn from multi-modal data. If successful, both ACORN and COILS can be extended from pre-service teachers to train in-service teachers in understanding classroom dynamics to improve their teaching.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jacob",
   "pi_last_name": "Whitehill",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jacob Whitehill",
   "pi_email_addr": "jrwhitehill@wpi.edu",
   "nsf_id": "000709937",
   "pi_start_date": "2018-07-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Erin",
   "pi_last_name": "Ottmar",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Erin R Ottmar",
   "pi_email_addr": "erottmar@wpi.edu",
   "nsf_id": "000661763",
   "pi_start_date": "2018-07-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Lane",
   "pi_last_name": "Harrison",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lane Harrison",
   "pi_email_addr": "ltharrison@wpi.edu",
   "nsf_id": "000710896",
   "pi_start_date": "2018-07-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Worcester Polytechnic Institute",
  "inst_street_address": "100 INSTITUTE RD",
  "inst_street_address_2": "",
  "inst_city_name": "WORCESTER",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "5088315000",
  "inst_zip_code": "016092280",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "WORCESTER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJNQME41NBU4"
 },
 "perf_inst": {
  "perf_inst_name": "Worcester Polytechnic Institute",
  "perf_str_addr": "100 Institute Road",
  "perf_city_name": "Worcester",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "016092280",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "798000",
   "pgm_ele_name": "ECR-EDU Core Research"
  },
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "7218",
   "pgm_ref_txt": "RET SUPP-Res Exp for Tchr Supp"
  },
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0418",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001819DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 749969.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 10000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span class=\"s1\">With the goal of providing more specific, comprehensive, and frequent feedback to teachers about the quality of interpersonal dynamics in their classrooms, the investigator team -- consisting of computer and data scientists, learning scientists, and educational researchers -- developed novel artificial intelligence (AI) techniques to analyze classroom observation videos automatically. Various AI architectures, based on multi-modal machine learning, were explored, ranging from convolutional and recurrent neural networks to ordinary linear regression, to aggregate multiple features over each classroom video. Specific features included teachers' facial emotion, speech and auditory emotion, and keyphrases of language content. The team built systems to (1) predict established measures of teaching quality such as the Classroom Assessment Scoring System (CLASS) that is widely used in educational research, as well as (2) to detect moments of classroom negativity that may be useful for the teacher to review. The collection of AI tools developed by the team were dubbed the Automatic Classroom Observation Recognition Network (ACORN). Based on the ACORN, the research team also developed a software prototype -- called the Classroom Observation Interactive Learning System (COILS) -- to visualize the automatically extracted information about a classroom video and to engage teachers in interpreting it. Specifically, the emotion of the students and teachers and eye-gaze attention of where each person is looking were rendered in a web-based interactive software tool. Based on this prototype, the researchers conducted experiments among undergraduate students, pre-service, and in-service teachers to examine how such information is valued and processed.</span></p>\n<p class=\"p1\"><span class=\"s1\">While pursuing this use-inspired research on how AI can benefit classroom observation and feedback, the team also investigated more general AI, machine learning, and speech analysis topics. For example, they developed more accurate speaker recognition models (used to identify who-is-speaking) by collecting larger speech datasets from YouTube automatically. They identified a new kind of clustering problem, called \"compositional clustering\", that is useful for determining who-is-speaking-when (known as \"speaker diarization\") when the input audio may contain multiple speakers speaking simultaneously (as often occurs in classroom speech). Moreover, they devised several novel algorithms that can tackle this new clustering problem. Finally, they identified a fundamental limitation (related to \"trivariate correlation\") in how AI can be used to develop scientific instruments to conduct basic research on the relationship between two phenomena (e.g., students' emotion and their learning outcomes). Specifically, the researchers proved that the expected correlation between the two phenomena that are measured with an AI-based instrument is reduced based on how accurate the instrument itself is. This has implications for researchers in various fields who use AI-based tools as a convenient tool for data collection.</span></p>\n<p class=\"p1\"><span class=\"s1\">The research products of this grant at its conclusion in July 2023 -- including a total of 9 conference, workshop, and journal papers -- were published at venues that spanned computer vision, speech analysis, learning analytics, and machine learning communities.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/24/2023<br>\n\t\t\t\t\tModified by: Jacob&nbsp;Whitehill</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1822768/1822768_10562955_1692917428628_classroom--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1822768/1822768_10562955_1692917428628_classroom--rgov-800width.jpg\" title=\"Classroom speech analysis\"><img src=\"/por/images/Reports/POR/2023/1822768/1822768_10562955_1692917428628_classroom--rgov-66x44.jpg\" alt=\"Classroom speech analysis\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Neural network-based keyphrase detectors were used to detect specific phrases of classroom speech that are associated with supportive language by the teacher.</div>\n<div class=\"imageCredit\">DOI: 10.1109/ICASSP40776.2020.9053173</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jacob&nbsp;Whitehill</div>\n<div class=\"imageTitle\">Classroom speech analysis</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1822768/1822768_10562955_1692917292502_comp--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1822768/1822768_10562955_1692917292502_comp--rgov-800width.jpg\" title=\"Compositional embeddings\"><img src=\"/por/images/Reports/POR/2023/1822768/1822768_10562955_1692917292502_comp--rgov-66x44.jpg\" alt=\"Compositional embeddings\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Compositional embeddings can be used for speaker diarization in scenarios where multiple speakers may be speaking simultaneously.</div>\n<div class=\"imageCredit\">DOI: 10.1109/ICASSP39728.2021.9413752</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jacob&nbsp;Whitehill</div>\n<div class=\"imageTitle\">Compositional embeddings</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "With the goal of providing more specific, comprehensive, and frequent feedback to teachers about the quality of interpersonal dynamics in their classrooms, the investigator team -- consisting of computer and data scientists, learning scientists, and educational researchers -- developed novel artificial intelligence (AI) techniques to analyze classroom observation videos automatically. Various AI architectures, based on multi-modal machine learning, were explored, ranging from convolutional and recurrent neural networks to ordinary linear regression, to aggregate multiple features over each classroom video. Specific features included teachers' facial emotion, speech and auditory emotion, and keyphrases of language content. The team built systems to (1) predict established measures of teaching quality such as the Classroom Assessment Scoring System (CLASS) that is widely used in educational research, as well as (2) to detect moments of classroom negativity that may be useful for the teacher to review. The collection of AI tools developed by the team were dubbed the Automatic Classroom Observation Recognition Network (ACORN). Based on the ACORN, the research team also developed a software prototype -- called the Classroom Observation Interactive Learning System (COILS) -- to visualize the automatically extracted information about a classroom video and to engage teachers in interpreting it. Specifically, the emotion of the students and teachers and eye-gaze attention of where each person is looking were rendered in a web-based interactive software tool. Based on this prototype, the researchers conducted experiments among undergraduate students, pre-service, and in-service teachers to examine how such information is valued and processed.\nWhile pursuing this use-inspired research on how AI can benefit classroom observation and feedback, the team also investigated more general AI, machine learning, and speech analysis topics. For example, they developed more accurate speaker recognition models (used to identify who-is-speaking) by collecting larger speech datasets from YouTube automatically. They identified a new kind of clustering problem, called \"compositional clustering\", that is useful for determining who-is-speaking-when (known as \"speaker diarization\") when the input audio may contain multiple speakers speaking simultaneously (as often occurs in classroom speech). Moreover, they devised several novel algorithms that can tackle this new clustering problem. Finally, they identified a fundamental limitation (related to \"trivariate correlation\") in how AI can be used to develop scientific instruments to conduct basic research on the relationship between two phenomena (e.g., students' emotion and their learning outcomes). Specifically, the researchers proved that the expected correlation between the two phenomena that are measured with an AI-based instrument is reduced based on how accurate the instrument itself is. This has implications for researchers in various fields who use AI-based tools as a convenient tool for data collection.\nThe research products of this grant at its conclusion in July 2023 -- including a total of 9 conference, workshop, and journal papers -- were published at venues that spanned computer vision, speech analysis, learning analytics, and machine learning communities.\n\n \n\n\t\t\t\t\tLast Modified: 08/24/2023\n\n\t\t\t\t\tSubmitted by: Jacob Whitehill"
 }
}