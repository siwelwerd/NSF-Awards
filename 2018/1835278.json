{
 "awd_id": "1835278",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NCS-FO: Engineering Living Neural Networks for Learning",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": "7032925394",
 "po_email": "rnash@nsf.gov",
 "po_sign_block_name": "Richard Nash",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 501038.0,
 "awd_amount": 517038.0,
 "awd_min_amd_letter_date": "2018-08-31",
 "awd_max_amd_letter_date": "2022-03-15",
 "awd_abstract_narration": "Recent developments in optogenetics, patterned optical stimulation, and high-speed optical detection enable simultaneous stimulation and recording of thousands of living neurons. Connected biological living neurons naturally exhibit the ability to perform computations and to learn. The proposed project will engineer living neural network to compute for learning task. Experimental testbed will be built to allow optical stimulation and detection. Algorithms will be developed to train the living neuron networks. The proposed testbed can be used by neuroscientists to verify network-level hypotheses. Insights learned from the proposed research can inspire other neuromorphic architectures based on solid state devices. Throughout this project, graduate students will be trained in computer engineering, bioengineering, and signal processing. Students will have the opportunity to work on interdisciplinary research in these fields. New courses based on the results from the proposed work will be introduced and new modules will be added to existing curriculum. The proposed outreach activities aim to attract interest to computer engineering and neural engineering.\r\n\r\nThe goal of this project is to use optogenetic in vitro neural network to run learning applications. Living neural networks have spontaneous activities, which can interfere with precise modification of synaptic strength. This research will study how to stabilize the living neural network such that a Spike Time Dependent Plasticity (STDP)-based programming protocol can imprint the desired synaptic strengths onto a living neural network. This research will also investigate how to strategically design and apply an STDP-based protocol to maximize programming throughput and optimize convergence rate of the network states. On the algorithm side, the proposed research will study data representation and training algorithms that consider various constraints of the proposed wetware system. Learning algorithms will be designed to work on random neural networks of unknown topology. Observable details of neuron activities will be used to improve accuracy of learning tasks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yevgeny",
   "pi_last_name": "Berdichevsky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yevgeny Berdichevsky",
   "pi_email_addr": "yeb211@lehigh.edu",
   "nsf_id": "000607529",
   "pi_start_date": "2022-03-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Xiaochen",
   "pi_last_name": "Guo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaochen Guo",
   "pi_email_addr": "xig515@lehigh.edu",
   "nsf_id": "000704529",
   "pi_start_date": "2018-08-31",
   "pi_end_date": "2022-03-15"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Zhiyuan",
   "pi_last_name": "Yan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhiyuan Yan",
   "pi_email_addr": "yan@lehigh.edu",
   "nsf_id": "000308858",
   "pi_start_date": "2018-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Yevgeny",
   "pi_last_name": "Berdichevsky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yevgeny Berdichevsky",
   "pi_email_addr": "yeb211@lehigh.edu",
   "nsf_id": "000607529",
   "pi_start_date": "2018-08-31",
   "pi_end_date": "2022-03-15"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Xiaochen",
   "pi_last_name": "Guo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaochen Guo",
   "pi_email_addr": "xig515@lehigh.edu",
   "nsf_id": "000704529",
   "pi_start_date": "2022-03-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Lehigh University",
  "inst_street_address": "526 BRODHEAD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BETHLEHEM",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "6107583021",
  "inst_zip_code": "180153008",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "PA07",
  "org_lgl_bus_name": "LEHIGH UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "E13MDBKHLDB5"
 },
 "perf_inst": {
  "perf_inst_name": "Lehigh University",
  "perf_str_addr": "19 Memorial Drive West, Departme",
  "perf_city_name": "Bethlehem",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "180153006",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "PA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8551",
   "pgm_ref_txt": "IntgStrat Undst Neurl&Cogn Sys"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "090E",
   "pgm_ref_txt": "Chem/Bio and Physical Diagnostics"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 501038.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project included computational and experimental thrusts.&nbsp; Experimental results were used to inform computational models, and results from computational models then guided new experiments. &nbsp;We used experimental data from whole cell recordings in primary rat cortical neurons to fit parameters of computational spiking neural networks (SNNs).&nbsp; These SNNs showed good performance on a delay task, where the network needs to output values that were input into the network in the recent past.&nbsp;&nbsp; Simulations revealed that conventional frequency-adapting spiking neural models are computationally expensive.&nbsp; We therefore developed a novel model of spike frequency adaptation that was efficient and accurate to the experimental data.&nbsp; &nbsp;We then studied whether living neural networks can be used for computing and performing machine learning tasks.&nbsp; Some inherent characteristics of living neural networks that may be obstacles to achieving this include variability of neuronal properties and random connectivity.&nbsp;&nbsp; We created a two-layer hybrid bio-silicon platform for performing computation, and neuronal variations and dynamics were verified by fitting model parameters with experimental results.&nbsp;&nbsp; Various types of random connectivity were generated.&nbsp; We found that reasonable inference accuracy could be achieved on recognition of handwritten digits (MNIST dataset) despite neuronal variability and random connectivity.&nbsp; Living neural networks are a type of a recurrent spiking neural network (RSNN), which are used to develop energy efficient neuromorphic systems, and to achieve good performance on tasks where time is a variable.&nbsp;&nbsp; Speech recognition is an example of such a task.&nbsp;&nbsp; Living neurons possess several time constants (membrane, synapse, and frequency adaptation) that determine their temporal characteristics.&nbsp; We reasoned that the presence of these constants may improve performance of a recurrent spiking neural network.&nbsp; We created an RSNN using leaky integrate and fire neuron model with dynamic synapses and spike frequency adaptation.&nbsp;&nbsp; Step-by-step experiments were designed to understand the impact of recurrent connections, synapse model, and adaptation model on network accuracy.&nbsp;&nbsp; Sequential handwritten digit recognition (MNIST dataset) and spoken digit (Ti46-Digit) tasks were used to evaluate network performance.&nbsp; Results suggested that dynamic synapses were more efficient than adaptation in improving the network&rsquo;s learning capability.&nbsp;&nbsp; When incorporating adaptation and synapse model together, the network was able to achieve an accuracy similar to state-of-the-art RSNNs while requiring fewer neurons and smaller constants.&nbsp;&nbsp; We then explored whether neuronal dynamic properties and network recurrent connections endowed living neural networks with temporal memory.&nbsp;&nbsp; We experimentally determined whether the state of an isolated cortical network could be used to accurately determine the timing of occurrence of an input pattern &ndash; essentially to spatially embed temporal features of the input.&nbsp; We developed an experimental system based on patterned optogenetic stimulation of dissociated primary rat cortical cultures, and read out activity via fluorescent calcium indicator.&nbsp;&nbsp; We delivered input sequences of patterns such that a pattern of interest occurred at different times.&nbsp;&nbsp; We found that the state of the experimental living neural networks contained information about inputs for at least 900 msec, and state of the network could be used to determine timing of input pattern occurrence with 100 msec precision.&nbsp;&nbsp;&nbsp; These experiments were conducted in 2D living neural networks.&nbsp; 3D networks represent an environment that is closer to that found in the intact brain. We therefore developed a platform to carry out computational tasks with 3D networks.&nbsp; The platform used microchannels that contained axons and dendrites extended by neurons in the 3D network.&nbsp; These microchannel-contained axons and dendrites were used to deliver optical stimuli pattern.&nbsp; We found that neurons in the 3D environment, stimulated via their exposed axons and dendrites, possessed features of neuronal population code in the intact cortex.&nbsp; Together, our work on this NSF-funded project paved the way for the use of a living neural network as a biological computational unit.</p><br>\n<p>\n Last Modified: 02/02/2025<br>\nModified by: Yevgeny&nbsp;Berdichevsky</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project included computational and experimental thrusts. Experimental results were used to inform computational models, and results from computational models then guided new experiments. We used experimental data from whole cell recordings in primary rat cortical neurons to fit parameters of computational spiking neural networks (SNNs). These SNNs showed good performance on a delay task, where the network needs to output values that were input into the network in the recent past. Simulations revealed that conventional frequency-adapting spiking neural models are computationally expensive. We therefore developed a novel model of spike frequency adaptation that was efficient and accurate to the experimental data. We then studied whether living neural networks can be used for computing and performing machine learning tasks. Some inherent characteristics of living neural networks that may be obstacles to achieving this include variability of neuronal properties and random connectivity. We created a two-layer hybrid bio-silicon platform for performing computation, and neuronal variations and dynamics were verified by fitting model parameters with experimental results. Various types of random connectivity were generated. We found that reasonable inference accuracy could be achieved on recognition of handwritten digits (MNIST dataset) despite neuronal variability and random connectivity. Living neural networks are a type of a recurrent spiking neural network (RSNN), which are used to develop energy efficient neuromorphic systems, and to achieve good performance on tasks where time is a variable. Speech recognition is an example of such a task. Living neurons possess several time constants (membrane, synapse, and frequency adaptation) that determine their temporal characteristics. We reasoned that the presence of these constants may improve performance of a recurrent spiking neural network. We created an RSNN using leaky integrate and fire neuron model with dynamic synapses and spike frequency adaptation. Step-by-step experiments were designed to understand the impact of recurrent connections, synapse model, and adaptation model on network accuracy. Sequential handwritten digit recognition (MNIST dataset) and spoken digit (Ti46-Digit) tasks were used to evaluate network performance. Results suggested that dynamic synapses were more efficient than adaptation in improving the networks learning capability. When incorporating adaptation and synapse model together, the network was able to achieve an accuracy similar to state-of-the-art RSNNs while requiring fewer neurons and smaller constants. We then explored whether neuronal dynamic properties and network recurrent connections endowed living neural networks with temporal memory. We experimentally determined whether the state of an isolated cortical network could be used to accurately determine the timing of occurrence of an input pattern  essentially to spatially embed temporal features of the input. We developed an experimental system based on patterned optogenetic stimulation of dissociated primary rat cortical cultures, and read out activity via fluorescent calcium indicator. We delivered input sequences of patterns such that a pattern of interest occurred at different times. We found that the state of the experimental living neural networks contained information about inputs for at least 900 msec, and state of the network could be used to determine timing of input pattern occurrence with 100 msec precision. These experiments were conducted in 2D living neural networks. 3D networks represent an environment that is closer to that found in the intact brain. We therefore developed a platform to carry out computational tasks with 3D networks. The platform used microchannels that contained axons and dendrites extended by neurons in the 3D network. These microchannel-contained axons and dendrites were used to deliver optical stimuli pattern. We found that neurons in the 3D environment, stimulated via their exposed axons and dendrites, possessed features of neuronal population code in the intact cortex. Together, our work on this NSF-funded project paved the way for the use of a living neural network as a biological computational unit.\t\t\t\t\tLast Modified: 02/02/2025\n\n\t\t\t\t\tSubmitted by: YevgenyBerdichevsky\n"
 }
}