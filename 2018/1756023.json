{
 "awd_id": "1756023",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: Explaining Decisions of Black-box Models via Input Perturbations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 174942.0,
 "awd_amount": 174942.0,
 "awd_min_amd_letter_date": "2018-03-30",
 "awd_max_amd_letter_date": "2018-03-30",
 "awd_abstract_narration": "Machine learning is at the forefront of many recent advances in science and technology, enabled in part by complex models and algorithms. However, as a consequence of this complexity, machine learning systems essentially act as \"black-boxes\" as far as users are concerned. Thus, it is incredibly difficult to predict what they will do when deployed, understand why they are making the decisions, guarantee their robustness, or broadly speaking, trust their behavior. As these algorithms become an increasing part of our society, our financial systems, our healthcare providers, our scientific advances, and our defense systems, it is crucial to address this challenge. In this work, the PI and his team will develop algorithms that explain why any classifier is making its decisions, without any access to its underlying implementation, in order to make the inner workings understandable to the users. Such explanations make machine learning more transparent, leading to a more robust evaluation pipeline, reduced debugging efforts, and increased ease of use (and of trust) of these complex, black-box systems.\r\n\r\nFor a decision made by a machine learning classifier, the team will develop methods that accurately characterize the relationship between the input instance and the algorithm's prediction, and present it in an intuitive manner. The primary intuition is to estimate the instance-specific behavior of the predictor by observing the output of the classifier as the input instance is perturbed. The first proposed thrust of this work extends this basic framework by considering rules that define counter-examples, and summarize the behavior over multiple instances, providing detailed and accurate insights into the behavior with minimal effort on the users' part. The second thrust identifies automated ways to learn domain-specific perturbation functions that generate realistic instances to compute the explanations. The team proposes a comprehensive evaluation of these explainers consisting of user experiments in comparing, trusting, and modifying machine learning algorithms, with applications to diverse tasks such as sentiment analysis, machine translation, time series, visual question answering, and object detection.\r\n\r\nDue to the many potential applications of this work, both for machine learning practitioners and end-users, dissemination of the results is a key focus, and the team will augment standard channels (such as publications) with novel ones that include open-source software, jargon-free documentation, and interactive tutorials/demonstrations to encourage application of machine learning to novel domains.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sameer",
   "pi_last_name": "Singh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sameer Singh",
   "pi_email_addr": "sameer@uci.edu",
   "nsf_id": "000727594",
   "pi_start_date": "2018-03-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "4204 Donald Bren Hall",
  "perf_city_name": "Irvine",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926973425",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 174942.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-e10d2524-7fff-02d7-faa2-e96226b77585\"> </span></p>\n<p dir=\"ltr\"><span>AI and machine learning are becoming an increasing part of our society, with applications that span medicine, finance, education, entertainment, law, and advertising. This proliferation has been achieved partly by recent advances in machine learning that have resulted in underlying algorithms becoming increasingly complex. However, this very complexity has also made these algorithms opaque and difficult to analyze. For example, it is unclear whether the machine learning algorithms competitively perform the intelligence tasks that we expect they undertake. Further, it is difficult to understand, debug, and develop them; thus, they are challenging to use correctly and easy to be misused. Model-agnostic explanations have recently gained prominence as a potential solution to this problem. By perturbing the input and observing the output of the model, these explanations allow us to understand whether the behavior of the model is as desired.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>In this project, we focus on the model-agnostic techniques for analyzing, testing, and characterizing the behavior of black-box machine learning models. We aim to identify the limitations of existing posthoc explanation techniques and develop extensions that are more faithful to the dataset, the model, and the analysis goals.In particular, existing posthoc explanation techniques use simplistic approximations, like linear models, of the complex model behavior, and use impractical and unrealistic assumptions when perturbing the input, such as changing features uniformly at random, which do not hold for most real-world scenarios.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Our first body of work focused on identifying the limitations of existing analysis tools of the model behavior. We show that existing posthoc explanation techniques, in particular, are manipulable; the model can hide aspects of its behavior that are not detectable by existing tools. We show that tools such as LIME and SHAP, by making linear assumptions and perturbing the input uniformly, can not detect discriminatory behaviors of adversarially created models. Similarly, we show that gradients of a model can be arbitrarily modified without changing the model behavior, thus rendering gradient-based analysis, such as the use of saliency maps for interpretation or adversarial attacks for reliability, ineffective. Apart from feature-based explanations, several explanation techniques identify data points in the training data that are most important for a test prediction. Through a thorough evaluation, we show that simple (and much faster) approximations essentially perform the same, or better, than more complex ones. Altogether, these show that explanations and analysis tools for model behavior are not all-encompassing, and should be used with clear caveats to their limitations.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>With these concerns with model analysis identified, we also introduced several techniques for analyzing specific behaviors of the black-box models, designed to be specific to their domains. First, we designed saliency-based explanation techniques for agents that operate in a dynamic environment, such as reinforcement learning agents. By explicitly incorporating relevance and specificity of the selected features to the chosen action (and not to others), the explanations produced are more accurate to the agent and more useful and interpretable to users. Second, we design tools for analyzing the model behavior by making specific adversarial changes specific to the domain. For knowledge graph completion, we design methods that identify candidates for fake facts that should be added, or true facts that should be removed, to change a specific target prediction. For NLP tasks, we identify phrases that, when added to most inputs, cause the model to change its prediction. We use these techniques not only to evaluate the sensitivity of models to adversarial changes but, more importantly, to characterize and understand the model behavior.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>In this project, we also identify the importance of perturbing data in specific ways to characterize and evaluate the model. For natural language processing, in particular, we propose several techniques, guidelines, and tools that use manual expertise to create diagnostic datasets for evaluating black box models. We introduce contrast sets as instances that are minimally changed from the existing ones by a domain expert, to create challenging evaluation benchmarks. We also introduce CheckList, which builds upon software testing fundamentals to create a framework for testing machine learning. CheckList aids users in identifying the desired capabilities for a task of interest, quickly generating large and diverse test sets for each capability, and evaluating the models for the task. Altogether, these approaches provide a more realistic and comprehensive evaluation and a better understanding of the capabilities and limitations of machine learning models.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/08/2022<br>\n\t\t\t\t\tModified by: Sameer&nbsp;Singh</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nAI and machine learning are becoming an increasing part of our society, with applications that span medicine, finance, education, entertainment, law, and advertising. This proliferation has been achieved partly by recent advances in machine learning that have resulted in underlying algorithms becoming increasingly complex. However, this very complexity has also made these algorithms opaque and difficult to analyze. For example, it is unclear whether the machine learning algorithms competitively perform the intelligence tasks that we expect they undertake. Further, it is difficult to understand, debug, and develop them; thus, they are challenging to use correctly and easy to be misused. Model-agnostic explanations have recently gained prominence as a potential solution to this problem. By perturbing the input and observing the output of the model, these explanations allow us to understand whether the behavior of the model is as desired.\n\n \nIn this project, we focus on the model-agnostic techniques for analyzing, testing, and characterizing the behavior of black-box machine learning models. We aim to identify the limitations of existing posthoc explanation techniques and develop extensions that are more faithful to the dataset, the model, and the analysis goals.In particular, existing posthoc explanation techniques use simplistic approximations, like linear models, of the complex model behavior, and use impractical and unrealistic assumptions when perturbing the input, such as changing features uniformly at random, which do not hold for most real-world scenarios.\n\n \nOur first body of work focused on identifying the limitations of existing analysis tools of the model behavior. We show that existing posthoc explanation techniques, in particular, are manipulable; the model can hide aspects of its behavior that are not detectable by existing tools. We show that tools such as LIME and SHAP, by making linear assumptions and perturbing the input uniformly, can not detect discriminatory behaviors of adversarially created models. Similarly, we show that gradients of a model can be arbitrarily modified without changing the model behavior, thus rendering gradient-based analysis, such as the use of saliency maps for interpretation or adversarial attacks for reliability, ineffective. Apart from feature-based explanations, several explanation techniques identify data points in the training data that are most important for a test prediction. Through a thorough evaluation, we show that simple (and much faster) approximations essentially perform the same, or better, than more complex ones. Altogether, these show that explanations and analysis tools for model behavior are not all-encompassing, and should be used with clear caveats to their limitations.\n\n \nWith these concerns with model analysis identified, we also introduced several techniques for analyzing specific behaviors of the black-box models, designed to be specific to their domains. First, we designed saliency-based explanation techniques for agents that operate in a dynamic environment, such as reinforcement learning agents. By explicitly incorporating relevance and specificity of the selected features to the chosen action (and not to others), the explanations produced are more accurate to the agent and more useful and interpretable to users. Second, we design tools for analyzing the model behavior by making specific adversarial changes specific to the domain. For knowledge graph completion, we design methods that identify candidates for fake facts that should be added, or true facts that should be removed, to change a specific target prediction. For NLP tasks, we identify phrases that, when added to most inputs, cause the model to change its prediction. We use these techniques not only to evaluate the sensitivity of models to adversarial changes but, more importantly, to characterize and understand the model behavior.\n\n \nIn this project, we also identify the importance of perturbing data in specific ways to characterize and evaluate the model. For natural language processing, in particular, we propose several techniques, guidelines, and tools that use manual expertise to create diagnostic datasets for evaluating black box models. We introduce contrast sets as instances that are minimally changed from the existing ones by a domain expert, to create challenging evaluation benchmarks. We also introduce CheckList, which builds upon software testing fundamentals to create a framework for testing machine learning. CheckList aids users in identifying the desired capabilities for a task of interest, quickly generating large and diverse test sets for each capability, and evaluating the models for the task. Altogether, these approaches provide a more realistic and comprehensive evaluation and a better understanding of the capabilities and limitations of machine learning models.\n\n \n\n\t\t\t\t\tLast Modified: 08/08/2022\n\n\t\t\t\t\tSubmitted by: Sameer Singh"
 }
}