{
 "awd_id": "1758628",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase II:  A Security, Privacy and Governance Policy Enforcement Framework for Big Data",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2018-04-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 749993.0,
 "awd_amount": 759993.0,
 "awd_min_amd_letter_date": "2018-04-04",
 "awd_max_amd_letter_date": "2021-07-01",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be the creation of a new tool that could prevent the loss of sensitive data stored in big data management systems due to cyber-attacks. Furthermore, the proposed cybersecurity tool can allow organizations to audit their big data usage to prevent data misuse and comply with various privacy regulations. Recent attacks have shown that the leakage/stealing of stored data may result in enormous monetary loss and damage to organizational reputation, and increased identity theft risks for individuals. Furthermore, in the age of big data, protecting the security and privacy of stored data is paramount for maintaining public trust, and getting the full value from the collected data. The company's proposed tool will potentially have significant impact by addressing these important societal needs with respect to big data security and privacy. Based on customer discovery findings, this tool will also address an important customer need found in many different industries and has the potential to have significant commercial impact as more and more companies are adopting big data technologies.\r\n\r\nThis Small Business Innovation Research Phase II project will commercialize a novel big data privacy, security and governance management tool that provides efficient data sanitization, attribute-based access control, accountability and governance policy enforcement capabilities for protecting sensitive data stored in big data management systems. In addition, the proposed product will provide novel data sensitivity aware intrusion detection capabilities. The Phase II research objectives are: 1) to develop an efficient attribute-based access control framework to prevent unauthorized access to sensitive data; 2) to develop data sanitization capabilities for complying with various regulations; 3) to develop a scalable audit log capture, storage and querying framework for increasing accountability for big data usage; and 4) to develop a data sensitivity aware intrusion detection framework to quickly detect potential attacks against sensitive data. These objectives pose significant research challenges with respect to scaling to big data without impacting the existing workflow of the companies. The company proposes to address these challenges by using novel code injection techniques combined with risk aware audit log generation and data sensitivity aware machine learning based intrusion detection techniques.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Fahad",
   "pi_last_name": "Shaon",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fahad Shaon",
   "pi_email_addr": "fahad@datasectech.com",
   "nsf_id": "000725072",
   "pi_start_date": "2018-04-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Data Security Technologies LLC",
  "inst_street_address": "17217 WATERVIEW PKWY",
  "inst_street_address_2": "# 1202",
  "inst_city_name": "DALLAS",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9727299582",
  "inst_zip_code": "752528004",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "TX04",
  "org_lgl_bus_name": "DATA SECURITY TECHNOLOGIES LLC",
  "org_prnt_uei_num": "",
  "org_uei_num": "JQ17UZPLVNL5"
 },
 "perf_inst": {
  "perf_inst_name": "Data Security Technologies LLC",
  "perf_str_addr": "17217 Waterview parkway",
  "perf_city_name": "Dallas",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "752528004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "TX04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5373",
   "pgm_ref_txt": "SMALL BUSINESS PHASE II"
  },
  {
   "pgm_ref_code": "8032",
   "pgm_ref_txt": "Software Services and Applications"
  },
  {
   "pgm_ref_code": "8240",
   "pgm_ref_txt": "SBIR/STTR CAP"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 749993.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 10000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-d3c2a952-7fff-8f7b-59ab-2e0456e6e58e\" dir=\"ltr\"><span>As demand for machine learning and data science continues to proliferate verticals in virtually every industry, protecting sensitive data stored in various data stores while complying with privacy and security regulations has emerged as one of the primary technical and business challenges of the decade. Companies need data scientists to perform experiments with data that produce insightful models, but experimentation can lead to risky behavior because identifying sensitive data and managing how it?s accessed becomes incredibly difficult in cloud and hybrid cloud settings with hundreds of SaaS applications and users. Existing Data Loss Prevention solutions focus heavily on data encryption and activity level control (determining who can share, edit, upload, download) resources. These solutions do not provide granular control over data - customers now need data to be protected at the row and column level with robust policies that can be based on any logic (e.g., identity, sensitivity, context of both the user submitting the request and the data being requested.) Additionally, existing solutions are focused exclusively on the reactive component of the security problem (e.g., access rules). The combination of a lack of granular control, and the inability to proactively protect resources from threats leaves customers especially vulnerable to data loss risk.&nbsp;&nbsp;</span></p>\n<p>To address these challenges, as a part of this Phase 2 grant, we developed SecureDL. It is a platform for unified protection and governance across all SQL and NoSQL (Hadoop, Spark, HBase, Hive etc.) databases and is purpose-built to supercharge the workflows of AI and Machine Learning (ML) teams building the products of tomorrow. SecureDL sits in between the database and applications and intercepts malicious queries in real time using a combination of our novel sensitivity analysis and admin defined policies. First, SecureDL finds and classifies sensitive data with cutting edge ML techniques, and this sensitivity estimation is used in the masking and query rewriting functionality. Second, SecureDL provides an automated proactive protection by 1) intercepting and rewriting malicious or sensitive SQL queries and programmatic data requests, 2) detecting malicious Scala and Java code with on-the-fly program analysis. We believe that within the decade, query and job analysis will be a customary part of any data analysis workflow, and DataSecTech?s SecureDL is the differentiated solution that leads this category.&nbsp; Finally, SecureDL&nbsp; provides reactive protection by&nbsp; allowing to write granular attribute based access control policies based on the context, identity and sensitivity of the data and user to control data access across all data stores and ML pipelines.&nbsp;</p>\n<p dir=\"ltr\"><span>Due to these unique capabilities, SecureDL gives customers the structure and confidence to secure their data by combining proactive and reactive security on top of all data stores. Admins can see detailed logs of all activity against the data store and use this to periodically create more granular policies. As more requests are submitted, SecureDL?s sensitivity analysis also gets better.This positive feedback loop that combines automated proactive protection with granular, admin defined policies is the essence of SecureDL?s customer driven approach to data-centric audit and protection.&nbsp;</span></p>\n<p dir=\"ltr\"><span>As a result, SecureDL reduces the cost compliance with existing data privacy regulations, and sensitive data exfiltration due to cyber attacks. In addition, it helps to easily scale data science workflow and ML pipelines and reduces cost of data security by having a standardized data centric audit and protection framework across cloud, hybrid and mixed cloud environments.&nbsp;</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/02/2021<br>\n\t\t\t\t\tModified by: Fahad&nbsp;Shaon</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "As demand for machine learning and data science continues to proliferate verticals in virtually every industry, protecting sensitive data stored in various data stores while complying with privacy and security regulations has emerged as one of the primary technical and business challenges of the decade. Companies need data scientists to perform experiments with data that produce insightful models, but experimentation can lead to risky behavior because identifying sensitive data and managing how it?s accessed becomes incredibly difficult in cloud and hybrid cloud settings with hundreds of SaaS applications and users. Existing Data Loss Prevention solutions focus heavily on data encryption and activity level control (determining who can share, edit, upload, download) resources. These solutions do not provide granular control over data - customers now need data to be protected at the row and column level with robust policies that can be based on any logic (e.g., identity, sensitivity, context of both the user submitting the request and the data being requested.) Additionally, existing solutions are focused exclusively on the reactive component of the security problem (e.g., access rules). The combination of a lack of granular control, and the inability to proactively protect resources from threats leaves customers especially vulnerable to data loss risk.  \n\nTo address these challenges, as a part of this Phase 2 grant, we developed SecureDL. It is a platform for unified protection and governance across all SQL and NoSQL (Hadoop, Spark, HBase, Hive etc.) databases and is purpose-built to supercharge the workflows of AI and Machine Learning (ML) teams building the products of tomorrow. SecureDL sits in between the database and applications and intercepts malicious queries in real time using a combination of our novel sensitivity analysis and admin defined policies. First, SecureDL finds and classifies sensitive data with cutting edge ML techniques, and this sensitivity estimation is used in the masking and query rewriting functionality. Second, SecureDL provides an automated proactive protection by 1) intercepting and rewriting malicious or sensitive SQL queries and programmatic data requests, 2) detecting malicious Scala and Java code with on-the-fly program analysis. We believe that within the decade, query and job analysis will be a customary part of any data analysis workflow, and DataSecTech?s SecureDL is the differentiated solution that leads this category.  Finally, SecureDL  provides reactive protection by  allowing to write granular attribute based access control policies based on the context, identity and sensitivity of the data and user to control data access across all data stores and ML pipelines. \nDue to these unique capabilities, SecureDL gives customers the structure and confidence to secure their data by combining proactive and reactive security on top of all data stores. Admins can see detailed logs of all activity against the data store and use this to periodically create more granular policies. As more requests are submitted, SecureDL?s sensitivity analysis also gets better.This positive feedback loop that combines automated proactive protection with granular, admin defined policies is the essence of SecureDL?s customer driven approach to data-centric audit and protection. \nAs a result, SecureDL reduces the cost compliance with existing data privacy regulations, and sensitive data exfiltration due to cyber attacks. In addition, it helps to easily scale data science workflow and ML pipelines and reduces cost of data security by having a standardized data centric audit and protection framework across cloud, hybrid and mixed cloud environments. \n\n\t\t\t\t\tLast Modified: 07/02/2021\n\n\t\t\t\t\tSubmitted by: Fahad Shaon"
 }
}