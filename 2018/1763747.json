{
 "awd_id": "1763747",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium: Training Sparse Neural Networks with Co-Designed Hardware Accelerators: Enabling Model Optimization and Scientific Exploration",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 1199849.0,
 "awd_amount": 1199849.0,
 "awd_min_amd_letter_date": "2018-07-02",
 "awd_max_amd_letter_date": "2020-06-22",
 "awd_abstract_narration": "Machine learning systems are critical drivers of new technologies such as near-perfect automatic speech recognition, autonomous vehicles, computer vision, and natural language understanding.  The underlying inference engine for many of these systems is based on neural networks.  Before a neural network can be used for these inference tasks, it must be trained using a data corpus of known input-output pairs.  This training process is very computationally intensive with current systems requiring weeks to months of time on graphic processing units (GPUs) or central processing units in the cloud.  As more data becomes available, this problem of long training time is further exacerbated because larger, more effective network models become desirable.  The theoretical understanding of neural networks is limited, so experimentation and empirical optimization remains the primary tool for understanding deep neural networks and innovating in the field.   However, the ability to conduct larger scale experiments is becoming concentrated with a few large entities with the necessary financial and computational resources.  Even for those with such resources, the painfully long experimental cycle for training neural networks means that large-scale searches and optimizations over the neural network model structure are not performed.  The ultimate goal of this research project is to democratize and distribute the ability to conduct large scale neural network training and model optimizations at high speed, using hardware accelerators.  Reducing the training time from weeks to hours will allow researchers to run many more experiments, gaining knowledge into the fundamental inner workings of deep learning systems.  The hardware accelerators are also much more energy efficient than the existing GPU-based training paradigm, so advances made in this project can significantly reduce the energy consumption required for neural network training tasks.\r\n\r\nThis project comprises an interdisciplinary research plan that spans theory, hardware architecture and design, software control, and system integration.  A new class of neural networks that have pre-defined sparsity is being explored.  These sparse neural networks are co-designed with a very flexible, high-speed, energy-efficient hardware architecture that maximizes circuit speed for any model size in a given Field Programmable Gate Array (FPGA) chip.  This algorithm-hardware co-design is a key research theme that differentiates this approach from previous research that enforces some sparsity during the training process in a manner incompatible with parallel hardware acceleration. In particular, the proposed architecture operates on each network layer simultaneously, executing the forward- and back-propagation in parallel and pipelined fully across layers.  With high precision arithmetic, a speed-up of about 5X relative to GPUs is expected.  Using log-domain arithmetic, these gains are expected to increase to 100X or larger. Software and algorithms are being developed to manage multiple FPGA boards, simplifying and automating the model search and training process. These algorithms exploit the ability to reconfigure the FPGAs to trade speed for accuracy, a capability lacking in GPUs.  These software tools will also serve as a bridge to popular Python libraries used by the machine learning community.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Keith",
   "pi_last_name": "Chugg",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Keith M Chugg",
   "pi_email_addr": "chugg@usc.edu",
   "nsf_id": "000440155",
   "pi_start_date": "2018-07-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Beerel",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Peter A Beerel",
   "pi_email_addr": "pabeerel@usc.edu",
   "nsf_id": "000177318",
   "pi_start_date": "2018-07-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Leana",
   "pi_last_name": "Golubchik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Leana Golubchik",
   "pi_email_addr": "leana@cs.usc.edu",
   "nsf_id": "000445297",
   "pi_start_date": "2018-07-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Panayiotis",
   "pi_last_name": "Georgiou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Panayiotis Georgiou",
   "pi_email_addr": "georgiou@sipi.usc.edu",
   "nsf_id": "000487058",
   "pi_start_date": "2018-07-02",
   "pi_end_date": "2019-08-15"
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3740 McClintock Avenue",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900892565",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "CA37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 758600.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 441249.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>We developed new methods for advancing the training of large neural networks using application specific hardware. &nbsp;This contrasts with the conventional approach of using general purpose processors, such as graphics processing units (GPUs).&nbsp;</span></p>\n<p><span>Our work was the first to introduce the concept of pre-defined, structured sparsity. &nbsp;Conventional neural networks have fully-connected or dense architectures, but after training, previous research has shown that many of these connections can be disregarded. &nbsp;This results in sparse connectivity pattern that reduce complexity, but do not map well to custom, highly parallel circuit architectures. &nbsp;Our work demonstrated that one can pre-define a structured sparse connection pattern and still maintain excellent learning performance. &nbsp;We also showed how one can co-design such a pre-defined, structured sparsity pattern with a highly parallel circuit architecture for training neural networks. &nbsp;This has the potential to significantly reduce the energy cost of large-scale training and/or enable embedded systems to train large neural networks at the edge. &nbsp;</span></p>\n<p><span>Our work also explores automated model search and training hyper-parameter optimization. &nbsp;We released an open-source software package for broader use in the research and industry communities. &nbsp;</span></p>\n<p><span>Our work also conducted some of the earliest work in log-number system (LNS) computational approaches for training neural networks, which eliminate costly multiplier circuits. &nbsp;These LNS approaches have the potential to reduce the area and/or energy consumption of training circuitry by a factor of two.</span></p>\n<p><span>Both pre-defined, structured sparsity and LNS approaches have become widely studied topics in the machine learning field and have received significant uptake in industry. &nbsp;</span></p>\n<p><span>Beyond the research component of our project, this collaboration led to significant advances in the curriculum at the USC Ming Hsieh Department of Electrical and Computing Engineering. &nbsp;Specifically, this project directly led to the creation of four new graduate level course in deep learning and software skills for machine learning, the first undergraduate machine learning class in the department, and a significantly revised MS degree program in Machine Learning and Data Sciences. &nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/08/2023<br>\nModified by: Keith&nbsp;M&nbsp;Chugg</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nWe developed new methods for advancing the training of large neural networks using application specific hardware. This contrasts with the conventional approach of using general purpose processors, such as graphics processing units (GPUs).\n\n\nOur work was the first to introduce the concept of pre-defined, structured sparsity. Conventional neural networks have fully-connected or dense architectures, but after training, previous research has shown that many of these connections can be disregarded. This results in sparse connectivity pattern that reduce complexity, but do not map well to custom, highly parallel circuit architectures. Our work demonstrated that one can pre-define a structured sparse connection pattern and still maintain excellent learning performance. We also showed how one can co-design such a pre-defined, structured sparsity pattern with a highly parallel circuit architecture for training neural networks. This has the potential to significantly reduce the energy cost of large-scale training and/or enable embedded systems to train large neural networks at the edge. \n\n\nOur work also explores automated model search and training hyper-parameter optimization. We released an open-source software package for broader use in the research and industry communities. \n\n\nOur work also conducted some of the earliest work in log-number system (LNS) computational approaches for training neural networks, which eliminate costly multiplier circuits. These LNS approaches have the potential to reduce the area and/or energy consumption of training circuitry by a factor of two.\n\n\nBoth pre-defined, structured sparsity and LNS approaches have become widely studied topics in the machine learning field and have received significant uptake in industry. \n\n\nBeyond the research component of our project, this collaboration led to significant advances in the curriculum at the USC Ming Hsieh Department of Electrical and Computing Engineering. Specifically, this project directly led to the creation of four new graduate level course in deep learning and software skills for machine learning, the first undergraduate machine learning class in the department, and a significantly revised MS degree program in Machine Learning and Data Sciences. \n\n\n\t\t\t\t\tLast Modified: 12/08/2023\n\n\t\t\t\t\tSubmitted by: KeithMChugg\n"
 }
}