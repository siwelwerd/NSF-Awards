{
 "awd_id": "1810975",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Cluster Validation Without Model Assumptiions",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 275000.0,
 "awd_amount": 275000.0,
 "awd_min_amd_letter_date": "2018-05-11",
 "awd_max_amd_letter_date": "2020-07-18",
 "awd_abstract_narration": "This project uses tools from optimization and statistics to put in the hands of practitioners a suite of methods to validate the results of clustering. Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups. The methods to be developed will recognize the cases when the data is well clustered and, in these cases, will provide a \"certificate of stability\" guaranteeing this. This research will develop validation methods for several widely used clustering paradigms (such as K-means, Spectral Clustering, finite Mixture Models) in realistic data scenarios. These methods will be integrated and disseminated with the big data open source platform megaman, developed and maintained by the group of PI Meila.\r\n\r\nThe proposed approach is based on the notions of good and stable clustering. ``Good'' means that clustering C fits the data well, according to the current clustering paradigm. ``Stable'' means that the only clusterings that fit well are small perturbations of C. This can only happen when C captures structure present in the data. While goodness can be easily checked in practice, stability is not a property that can be verified directly. The core of this project is to find conditions on the data and C that guarantee stability AND are practically verifyable. Such results are known as stability results. The project outlines two novel approaches to this goal. The first approach is based on using convex relaxations to the original clustering problem. The second approach is based on ``recycling'' existing theoretical results proved under assumptions about the data generating model. Often, the proof of such a result contains the elements of a model free stability proof. Thus, this project will be using existing statistical theory in a novel way. Among convex relaxations for clustering, the relaxations to a Semi-Definite Program (SDP) are especially promising. Therefore, ways to accelerate the validation algorithms by exploiting the special structure of the SDPs will be explored.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marina",
   "pi_last_name": "Meila",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Marina Meila",
   "pi_email_addr": "mmp@stat.washington.edu",
   "nsf_id": "000100267",
   "pi_start_date": "2018-05-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Zaid",
   "pi_last_name": "Harchaoui",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zaid Harchaoui",
   "pi_email_addr": "zaid@uw.edu",
   "nsf_id": "000730146",
   "pi_start_date": "2018-05-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn AVE NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981950001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 92068.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 85382.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 97550.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Clustering is ubiquitous in &nbsp;data exploration, or embedded in a variety of other scientific algorithms. Yet, for practitioners, clustering has for too long remained an art. Specifically, the practical question `<em>`given a specific data set D &nbsp;and clustering C of these data, how to know whether to trust this clustering to be good?'' </em>had remained until now unanswered. &nbsp;In the sciences, clustering often&nbsp;helps identify new classes of diseases, subpopulations, or energy wells in a molecular system. In this context, the clusters need to be &nbsp;<em>meaningful</em>, and for this, C must be <em>unique, up to small perturbations </em>(otherwise, the clusters would be<em> irreproducible</em>).</p>\n<p>In this project, PI Meila with co-PI Harchaoui <em>introduced a novel original approach</em> to ascertain that a clustering is approximately optimal and stable.&nbsp;<br />The central original idea is the existence of and <strong>Optimality Certificate (OC).</strong> An &nbsp;OC $\\delta$ is an error margin for C. If $\\delta=0.03\\%$ it means that the unknown optimal clustering is no more than $\\delta$ different from C (hence C is nearly optimal), and moreover, any other nearly-optimal C' &nbsp;is also within $\\delta$. ``Optimal'' and ``good'' are w.r.t. the clustering paradigm chosen by the user, e.g. K-means.&nbsp;</p>\n<p>What distinguishes this work is that the OC d<em>oes not depend on&nbsp;assumptions that cannot be verified in practice</em> (e.g. on assumptions about the data source) and must be <em>&nbsp;practically computable from the data</em> &nbsp;and clustering at hand. This project has developed <em>the first paradigm in which obtaining such OC is possible.</em></p>\n<p>Subsequently, this project developed theory and implementable, tractable OC algorithms for <strong>cost based clusterin</strong>g. The theory allows a user to create an OC algorithm for new or&nbsp;existing clustering costs, in a large class of cost-based clustering&nbsp;methods, namely any clustering cost that admits a convex&nbsp;relaxation; <em>K-means and spectral clustering</em> (with several&nbsp;variants) are included in this&nbsp;class.<br />For K-means and spectral clustering, the OC algorithms were implemented and made available in the package &nbsp;admm_ss_sdp.</p>\n<p>The PI and co-PI also developed tractable optimality certificates for <strong>mixtures of Gaussians</strong>. The approach &nbsp;is completely new, different from previous work on mixtures, as well as from the approaches above. Specifically, it was shown that, if the data distribution is closely approximated by &nbsp;a mixture model with &nbsp;<em>well separated, not too small clusters</em>, then the mixture parameters are unique up to small perturbations.The OC are tractably computable, and do not depend on unknown parameters, with the exception of the distance to the data distribution, which must be estimated.&nbsp;</p>\n<p>If an OC for a given clustering is found, this implicitly validates not just the clustering, but also the number of clusters K and the clustering paradigm. Thus, the OC algorithms perform model selection for clustering in a principled and general way. Even when an OC is not found, the results of the OC algorithm can serve as heuristics for choosing the number of clusters K, in line with stability selection, the most successful paradigm to date for selecting K.</p>\n<p><br />Meila develped the course `<em>`Classic and Modern Data Clustering''</em>, consisting &nbsp;of three modules. They have been tested by the PI as stand-alone tutorials and as part of graduate courses.<br />This project is bringing practical validation methodologies, grounded in theory, to the clustering algorithms that are already in wide use, thus impacting potentially a large number of users.&nbsp;<br />Meila and Harchaoui &nbsp;applied OC to the validation of energy wells (representing intermediate reaction products or meta-stable states) in &nbsp;Molecular Dynamics (MD) simulations. The&nbsp;CytoSegmenter softare allows to locate abrupt changes in phytoplankton community structure.&nbsp;<br /><br />PhD student Hanyu Zhang completed his dissertation \"Interpretation and validation in unsupervised learning\".&nbsp;</p>\n<p>The GDA group lead by Meila in collaboration with Yen-Chi Chen is a focal point for the Geometric DataAnalysis community across Seattle, integrating MS, PhD, postdoc researchers with external members and UW faculty.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/16/2023<br>\n\t\t\t\t\tModified by: Marina&nbsp;Meila</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1810975/1810975_10543455_1684261377050_fig-ssmethod-noSS--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1810975/1810975_10543455_1684261377050_fig-ssmethod-noSS--rgov-800width.jpg\" title=\"Optimality Certificate (OC) illustration\"><img src=\"/por/images/Reports/POR/2023/1810975/1810975_10543455_1684261377050_fig-ssmethod-noSS--rgov-66x44.jpg\" alt=\"Optimality Certificate (OC) illustration\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An  OC algorithm takes as input the clustered data  and outputs the value of the OC. When a clustering is not stable, the algorithm returns a ``no certificate'' message.</div>\n<div class=\"imageCredit\">Meila</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Marina&nbsp;Meila</div>\n<div class=\"imageTitle\">Optimality Certificate (OC) illustration</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1810975/1810975_10543455_1684260460304_fig-outcomes-screenshot-edited--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1810975/1810975_10543455_1684260460304_fig-outcomes-screenshot-edited--rgov-800width.jpg\" title=\"OC calculated for K-means (left) and Spectral clustering (right) by 2 methods\"><img src=\"/por/images/Reports/POR/2023/1810975/1810975_10543455_1684260460304_fig-outcomes-screenshot-edited--rgov-66x44.jpg\" alt=\"OC calculated for K-means (left) and Spectral clustering (right) by 2 methods\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Left: Clustered data and K-means  CO's. Right: real MD data (2 energy wells) and spectral clustering CO's. Arrows point to OC for case shown.</div>\n<div class=\"imageCredit\">Meila</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Marina&nbsp;Meila</div>\n<div class=\"imageTitle\">OC calculated for K-means (left) and Spectral clustering (right) by 2 methods</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nClustering is ubiquitous in  data exploration, or embedded in a variety of other scientific algorithms. Yet, for practitioners, clustering has for too long remained an art. Specifically, the practical question ``given a specific data set D  and clustering C of these data, how to know whether to trust this clustering to be good?'' had remained until now unanswered.  In the sciences, clustering often helps identify new classes of diseases, subpopulations, or energy wells in a molecular system. In this context, the clusters need to be  meaningful, and for this, C must be unique, up to small perturbations (otherwise, the clusters would be irreproducible).\n\nIn this project, PI Meila with co-PI Harchaoui introduced a novel original approach to ascertain that a clustering is approximately optimal and stable. \nThe central original idea is the existence of and Optimality Certificate (OC). An  OC $\\delta$ is an error margin for C. If $\\delta=0.03\\%$ it means that the unknown optimal clustering is no more than $\\delta$ different from C (hence C is nearly optimal), and moreover, any other nearly-optimal C'  is also within $\\delta$. ``Optimal'' and ``good'' are w.r.t. the clustering paradigm chosen by the user, e.g. K-means. \n\nWhat distinguishes this work is that the OC does not depend on assumptions that cannot be verified in practice (e.g. on assumptions about the data source) and must be  practically computable from the data  and clustering at hand. This project has developed the first paradigm in which obtaining such OC is possible.\n\nSubsequently, this project developed theory and implementable, tractable OC algorithms for cost based clustering. The theory allows a user to create an OC algorithm for new or existing clustering costs, in a large class of cost-based clustering methods, namely any clustering cost that admits a convex relaxation; K-means and spectral clustering (with several variants) are included in this class.\nFor K-means and spectral clustering, the OC algorithms were implemented and made available in the package  admm_ss_sdp.\n\nThe PI and co-PI also developed tractable optimality certificates for mixtures of Gaussians. The approach  is completely new, different from previous work on mixtures, as well as from the approaches above. Specifically, it was shown that, if the data distribution is closely approximated by  a mixture model with  well separated, not too small clusters, then the mixture parameters are unique up to small perturbations.The OC are tractably computable, and do not depend on unknown parameters, with the exception of the distance to the data distribution, which must be estimated. \n\nIf an OC for a given clustering is found, this implicitly validates not just the clustering, but also the number of clusters K and the clustering paradigm. Thus, the OC algorithms perform model selection for clustering in a principled and general way. Even when an OC is not found, the results of the OC algorithm can serve as heuristics for choosing the number of clusters K, in line with stability selection, the most successful paradigm to date for selecting K.\n\n\nMeila develped the course ``Classic and Modern Data Clustering'', consisting  of three modules. They have been tested by the PI as stand-alone tutorials and as part of graduate courses.\nThis project is bringing practical validation methodologies, grounded in theory, to the clustering algorithms that are already in wide use, thus impacting potentially a large number of users. \nMeila and Harchaoui  applied OC to the validation of energy wells (representing intermediate reaction products or meta-stable states) in  Molecular Dynamics (MD) simulations. The CytoSegmenter softare allows to locate abrupt changes in phytoplankton community structure. \n\nPhD student Hanyu Zhang completed his dissertation \"Interpretation and validation in unsupervised learning\". \n\nThe GDA group lead by Meila in collaboration with Yen-Chi Chen is a focal point for the Geometric DataAnalysis community across Seattle, integrating MS, PhD, postdoc researchers with external members and UW faculty.\n\n \n\n\t\t\t\t\tLast Modified: 05/16/2023\n\n\t\t\t\t\tSubmitted by: Marina Meila"
 }
}