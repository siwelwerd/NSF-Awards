{
 "awd_id": "1841456",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032928235",
 "po_email": "bmihaila@nsf.gov",
 "po_sign_block_name": "Bogdan Mihaila",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 499872.0,
 "awd_amount": 499872.0,
 "awd_min_amd_letter_date": "2018-09-07",
 "awd_max_amd_letter_date": "2018-09-07",
 "awd_abstract_narration": "The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of \"real\" data collected from the experiments with \"synthetic\" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.\r\n\r\nThe main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the \"lingua franca\" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.\r\n\r\nThis project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Neubauer",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Mark S Neubauer",
   "pi_email_addr": "msn@illinois.edu",
   "nsf_id": "000509237",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Katz",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel S Katz",
   "pi_email_addr": "dskatz@illinois.edu",
   "nsf_id": "000312962",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 South Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "768400",
   "pgm_ele_name": "CESER-Cyberinfrastructure for"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "020Z",
   "pgm_ref_txt": "OAC Facility Cyberinfrastructure"
  },
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499872.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-c84dd067-7fff-7e4f-b810-ced460747d12\"> </span></p>\n<p dir=\"ltr\"><span>The goal of particle physics is to understand nature at its most fundamental level by studying its building blocks and interactions. Major multi-user research facilities such as the Large Hadron Collider (LHC) at CERN provide a unique window into the subatomic world and form the foundation for a broad, data-intensive science program in pursuit of this goal. It is imperative that the physics community maximize the scientific potential of the data resulting from these facilities. The scientific potential can be enhanced by developing&nbsp;</span>performant and sustainable cyberinfrastructure that streamlines high-impact, but computationally-intensive, analysis workflows such as comparisons of <em>real</em> data collected from the experiments with <em>synthetic</em> data produced from simulations to extract scientific results. These comparisons increasingly utilize artificial intelligence (AI) methods to enhance the analysis of data from facilities such as the LHC.</p>\n<p dir=\"ltr\"><span>The main goal of the project was to develop and deploy AI and likelihood-free inference methods and software which can increase the discovery reach of data-intensive science using scalable cyberinfrastructure (CI) that is integrated into existing CI elements and can be executed on large systems such as HPCs. Activities in pursuit of this goal were undertaken by students, postdoctoral fellows, a research software engineer and faculty members supported on the project.</span></p>\n<p dir=\"ltr\"><span>REANA is </span><span>a reusable and reproducible research data analysis platform which helps researchers to structure their input data, analysis code, containerized environments and computational workflows so that analyses can be executed on heterogeneous computing platforms.</span><span> We deployed and utilized a REANA cluster at NCSA and generated detailed public documentation of our experience, providing feedback to the REANA team and others using the REANA platform for their science. We </span><span>explored how to use Parsl, a system that expresses workflows as code, to create REANA reproducible research objects. We found that Parsl's workflow-as-code model made it easier to learn and read than REANA's supported Yadage YAML model, but its dependence on iPython for execution made it more invasive to the deployed Docker containers. We also encountered dependencies on the Parsl package and Python 3 for the containers we studied that made execution of some of the legacy analysis code we were orchestrating difficult or impossible without significant code changes to upgrade them to Python 3.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We explored several open-source technologies for machine learning workflow environments and performed detailed studies using a package called <em>MLFlow</em>. MLFlow is a toolkit that provides tracking of training sessions with searchable history of code, hyperparameters and fit results, packaging of workflows into reproducible runs and tools for sending trained models to standard run-time environments. </span><span>We worked to integrate MLFlow with several workflows in HEP including charged particle tracking using Graph Neural Networks and MadMiner for ML-based inference, also working with MadMiner developers to generate a REANA-MLFlow-Madminer singularity container. Our studies demonstrated that MLFlow is a powerful framework to enable ML workflows in high-energy physics to be more reproducible and sustainable. In the course of these studies, we developed a new feature for MLFlow which would make development easier for users across all domains, especially for Kubernetes-based project.&nbsp; We submitted a feature request for this development to the MLFlow team that was integrated into their main project.</span></p>\n<p dir=\"ltr\"><span>We developed CI for physics simulation for use in LHC data analyses, particularly to support sustainable matrix element method (MEM) calculations on HPCs using an AI approach based on deep learning. The matrix element (ME) method is based on an </span><em>ab initio</em><span> approximate calculation of the probability density of an event with observed final-state particle momenta to be due to any physics process of interest that can be specified in the context of quantum field theory. We found that the MEM can be accurately modeled by treating it as a regression problem within the context of deep neural networks (DNNs) and successfully demonstrated this approach using simulated Drell-Yan (DY) events at the LHC. During the course of this work, we developed a set of integrated Docker images that provide publicly-available images for an end-to-end simulation workflow which we used for our MEM-based analysis pipeline. This pipeline was executed on the Blue Waters supercomputer at the University of Illinois to calculate ME-based probability densities for 10 million DY events. Using our DNN-based MEM method (<strong>DeepMEM</strong>), we demonstrated a <strong>17-fold improvement</strong> in the evaluation speed over traditional methods without loss of modeling accuracy or generalization in near-by kinematic regions. DeepMEM&nbsp;</span>is an&nbsp;open-source python library distributed on PyPI&nbsp;that can be used to study additional physics processes and datasets.</p>\n<p><span>In the process of our DNN-based MEM work, we saw a need for preserving and accessing Universal FeynRules Output (UFO) models that are used in physics generators such as Madgraph. We developed </span><span>two public Github code repositories with software to support end-to-end support infrastructure for preserving and accessing UFO models based on </span><span>Findable, Accessible, Interoperable, and Reusable (FAIR)</span><span> principles.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/02/2023<br>\n\t\t\t\t\tModified by: Mark&nbsp;S&nbsp;Neubauer</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675388064512_DeepMEMmodeling--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675388064512_DeepMEMmodeling--rgov-800width.jpg\" title=\"DeepMEM modeling of simulated Drell-Yan events at the LHC\"><img src=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675388064512_DeepMEMmodeling--rgov-66x44.jpg\" alt=\"DeepMEM modeling of simulated Drell-Yan events at the LHC\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Comparison of our DeepMEM MEmethod modeling of the probability density for simulated Drell-Yan events with calculations from the MoMEMta package</div>\n<div class=\"imageCredit\">Mihir Katare</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mark&nbsp;S&nbsp;Neubauer</div>\n<div class=\"imageTitle\">DeepMEM modeling of simulated Drell-Yan events at the LHC</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675387687482_MEMcontainers--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675387687482_MEMcontainers--rgov-800width.jpg\" title=\"Containerized workflow developed for Matrix Element Method\"><img src=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675387687482_MEMcontainers--rgov-66x44.jpg\" alt=\"Containerized workflow developed for Matrix Element Method\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Containerized simulation+calculation pipeline developed and executedon the Blue Waters Supercomputer at Illinois to simulate and calculate ME probability densities for 300kDrell-Yan events at the LHC in 45 minutes (for comparison, single-threaded execution time is 150 hrs)</div>\n<div class=\"imageCredit\">Matthew Feickert</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mark&nbsp;S&nbsp;Neubauer</div>\n<div class=\"imageTitle\">Containerized workflow developed for Matrix Element Method</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675387912913_MEMcontainersDNN--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675387912913_MEMcontainersDNN--rgov-800width.jpg\" title=\"Containerized workflow developed for DNN-based Matrix Element Method\"><img src=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675387912913_MEMcontainersDNN--rgov-66x44.jpg\" alt=\"Containerized workflow developed for DNN-based Matrix Element Method\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Containerized workflow with the addition of our DeepMEM approximation. After a one-time training investment of 18 hours, 300k events were evaluated in 2 minutes with DeepMEM compared to 34 minutes for MoMEMta</div>\n<div class=\"imageCredit\">Matthew Feickert</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mark&nbsp;S&nbsp;Neubauer</div>\n<div class=\"imageTitle\">Containerized workflow developed for DNN-based Matrix Element Method</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675387507951_DeepMEM_Flow--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675387507951_DeepMEM_Flow--rgov-800width.jpg\" title=\"DeepMEM workflow\"><img src=\"/por/images/Reports/POR/2023/1841456/1841456_10580699_1675387507951_DeepMEM_Flow--rgov-66x44.jpg\" alt=\"DeepMEM workflow\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Example analysis workflow using the proposed DeepMEM approach to the Matrix Element Method</div>\n<div class=\"imageCredit\">Mark Neubauer</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mark&nbsp;S&nbsp;Neubauer</div>\n<div class=\"imageTitle\">DeepMEM workflow</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nThe goal of particle physics is to understand nature at its most fundamental level by studying its building blocks and interactions. Major multi-user research facilities such as the Large Hadron Collider (LHC) at CERN provide a unique window into the subatomic world and form the foundation for a broad, data-intensive science program in pursuit of this goal. It is imperative that the physics community maximize the scientific potential of the data resulting from these facilities. The scientific potential can be enhanced by developing performant and sustainable cyberinfrastructure that streamlines high-impact, but computationally-intensive, analysis workflows such as comparisons of real data collected from the experiments with synthetic data produced from simulations to extract scientific results. These comparisons increasingly utilize artificial intelligence (AI) methods to enhance the analysis of data from facilities such as the LHC.\nThe main goal of the project was to develop and deploy AI and likelihood-free inference methods and software which can increase the discovery reach of data-intensive science using scalable cyberinfrastructure (CI) that is integrated into existing CI elements and can be executed on large systems such as HPCs. Activities in pursuit of this goal were undertaken by students, postdoctoral fellows, a research software engineer and faculty members supported on the project.\nREANA is a reusable and reproducible research data analysis platform which helps researchers to structure their input data, analysis code, containerized environments and computational workflows so that analyses can be executed on heterogeneous computing platforms. We deployed and utilized a REANA cluster at NCSA and generated detailed public documentation of our experience, providing feedback to the REANA team and others using the REANA platform for their science. We explored how to use Parsl, a system that expresses workflows as code, to create REANA reproducible research objects. We found that Parsl's workflow-as-code model made it easier to learn and read than REANA's supported Yadage YAML model, but its dependence on iPython for execution made it more invasive to the deployed Docker containers. We also encountered dependencies on the Parsl package and Python 3 for the containers we studied that made execution of some of the legacy analysis code we were orchestrating difficult or impossible without significant code changes to upgrade them to Python 3. \nWe explored several open-source technologies for machine learning workflow environments and performed detailed studies using a package called MLFlow. MLFlow is a toolkit that provides tracking of training sessions with searchable history of code, hyperparameters and fit results, packaging of workflows into reproducible runs and tools for sending trained models to standard run-time environments. We worked to integrate MLFlow with several workflows in HEP including charged particle tracking using Graph Neural Networks and MadMiner for ML-based inference, also working with MadMiner developers to generate a REANA-MLFlow-Madminer singularity container. Our studies demonstrated that MLFlow is a powerful framework to enable ML workflows in high-energy physics to be more reproducible and sustainable. In the course of these studies, we developed a new feature for MLFlow which would make development easier for users across all domains, especially for Kubernetes-based project.  We submitted a feature request for this development to the MLFlow team that was integrated into their main project.\nWe developed CI for physics simulation for use in LHC data analyses, particularly to support sustainable matrix element method (MEM) calculations on HPCs using an AI approach based on deep learning. The matrix element (ME) method is based on an ab initio approximate calculation of the probability density of an event with observed final-state particle momenta to be due to any physics process of interest that can be specified in the context of quantum field theory. We found that the MEM can be accurately modeled by treating it as a regression problem within the context of deep neural networks (DNNs) and successfully demonstrated this approach using simulated Drell-Yan (DY) events at the LHC. During the course of this work, we developed a set of integrated Docker images that provide publicly-available images for an end-to-end simulation workflow which we used for our MEM-based analysis pipeline. This pipeline was executed on the Blue Waters supercomputer at the University of Illinois to calculate ME-based probability densities for 10 million DY events. Using our DNN-based MEM method (DeepMEM), we demonstrated a 17-fold improvement in the evaluation speed over traditional methods without loss of modeling accuracy or generalization in near-by kinematic regions. DeepMEM is an open-source python library distributed on PyPI that can be used to study additional physics processes and datasets.\n\nIn the process of our DNN-based MEM work, we saw a need for preserving and accessing Universal FeynRules Output (UFO) models that are used in physics generators such as Madgraph. We developed two public Github code repositories with software to support end-to-end support infrastructure for preserving and accessing UFO models based on Findable, Accessible, Interoperable, and Reusable (FAIR) principles.\n\n\t\t\t\t\tLast Modified: 02/02/2023\n\n\t\t\t\t\tSubmitted by: Mark S Neubauer"
 }
}