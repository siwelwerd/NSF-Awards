{
 "awd_id": "1755705",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CIF: New Structure-Exploiting and Memory-Efficient Methods for Large-Scale Optimization and Data Analysis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2018-01-29",
 "awd_max_amd_letter_date": "2018-01-29",
 "awd_abstract_narration": "Large-scale optimization methods have been paramount to the successes of recent applications of machine learning and data analysis in a wide variety of domains. At the same time, certain structural properties of statistical models, such as sparsity or low-rank structure, have proven to be crucial for obtaining meaningful and accurate results in high dimensions. In addition to being highly scalable to large datasets, some optimization algorithms have the desirable property that they directly promote the aforementioned valuable structural properties of models. This project involves developing, analyzing, and implementing novel optimization algorithms that have such beneficial structure-exploiting and also memory-efficiency properties. This project directly involves the mentoring of graduate students, as well as integration of research results into an undergraduate level machine learning course and a graduate level course in optimization and statistical learning.\r\n\r\nThe foundation for this project is the Frank-Wolfe Method, a particular structure-exploiting first-order gradient optimization algorithm, and the related methodology of in-face directions. In-face directions automatically promote well-structured near-optimal solutions and have encouraging memory-efficiency properties. This research will investigate conditions whereby methods with in-face directions, as applied to convex relaxations of matrix completion and more general atomic norm regularization problems, are guaranteed to have a low memory footprint. Furthermore, this project will extend the reach of methods that incorporate in-face directions to new problem classes, including non-smooth objective functions, non-convex objective functions, and stochastic gradient estimates. The proposed optimization framework and in-face methodology applies very generally, and has potential for broader impact in several areas, including recommender systems, bioinformatics, customer segmentation, sentiment analysis, and medical imaging.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Paul",
   "pi_last_name": "Grigas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Paul Grigas",
   "pi_email_addr": "pgrigas@berkeley.edu",
   "nsf_id": "000754417",
   "pi_start_date": "2018-01-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California Berkeley",
  "perf_str_addr": "4177 Etcheverry Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947201776",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project developed new methodologies for large-scale optimization and decision-making problems. The methodologies apply to general classes of problems across the domains of machine learning and data analysis and can deliver algorithms with improved efficiency in several application areas, including recommender systems, bioinformatics, customer segmentation, sentiment analysis, medical imaging, and more.</p>\n<p>This project involved developing, analyzing, and implementing novel optimization algorithms that directly promote valuable structural properties of models. Specifically, one of the main outcomes of the project was the development of a joint online learning and decision-making procedure via the optimization algorithm of dual mirror descent. This procedure involves mixing decision-making steps via optimization tools with a generic parameter learning method. The procedure was analyzed theoretically, wherein it was shown to have desirable low regret properties, and was shown to be competitive experimentally in joint allocation and bidding problems.</p>\n<p>Another major outcome of this project was a new methodology for developing and analyzing several different (second order) algorithms for computing an approximately optimal solution path of a parameterized optimization problem.&nbsp;Such parametric optimization problems arise naturally and abundantly in machine learning when turning regularization parameters for training models. The proposed algorithms are inspired by a differential equations perspective on the parametric solution path. Unlike most methods for computing the so-called regularization path, the developed algorithms are quite general and do not rely on the specific structure of the regularizing function.</p>\n<p>This project directly involved the mentoring of graduate students, as well as integration of research results into an undergraduate/masters level machine learning course and a graduate level course in optimization and statistical learning that brings new Ph.D. students close to the research frontier.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/14/2022<br>\n\t\t\t\t\tModified by: Paul&nbsp;Grigas</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project developed new methodologies for large-scale optimization and decision-making problems. The methodologies apply to general classes of problems across the domains of machine learning and data analysis and can deliver algorithms with improved efficiency in several application areas, including recommender systems, bioinformatics, customer segmentation, sentiment analysis, medical imaging, and more.\n\nThis project involved developing, analyzing, and implementing novel optimization algorithms that directly promote valuable structural properties of models. Specifically, one of the main outcomes of the project was the development of a joint online learning and decision-making procedure via the optimization algorithm of dual mirror descent. This procedure involves mixing decision-making steps via optimization tools with a generic parameter learning method. The procedure was analyzed theoretically, wherein it was shown to have desirable low regret properties, and was shown to be competitive experimentally in joint allocation and bidding problems.\n\nAnother major outcome of this project was a new methodology for developing and analyzing several different (second order) algorithms for computing an approximately optimal solution path of a parameterized optimization problem. Such parametric optimization problems arise naturally and abundantly in machine learning when turning regularization parameters for training models. The proposed algorithms are inspired by a differential equations perspective on the parametric solution path. Unlike most methods for computing the so-called regularization path, the developed algorithms are quite general and do not rely on the specific structure of the regularizing function.\n\nThis project directly involved the mentoring of graduate students, as well as integration of research results into an undergraduate/masters level machine learning course and a graduate level course in optimization and statistical learning that brings new Ph.D. students close to the research frontier.\n\n\t\t\t\t\tLast Modified: 07/14/2022\n\n\t\t\t\t\tSubmitted by: Paul Grigas"
 }
}