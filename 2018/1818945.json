{
 "awd_id": "1818945",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Geometric Analysis and Computation for Generative Models",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 100000.0,
 "awd_amount": 100000.0,
 "awd_min_amd_letter_date": "2018-06-21",
 "awd_max_amd_letter_date": "2018-06-21",
 "awd_abstract_narration": "Research in unsupervised learning and generative models is concerned with uncovering structure and relationships in data with the intent of being able to generate new, as yet unseen, examples of the data set. Generative models learn the distribution of a data set from finite samples and provide an efficient sampler of the approximated density, rather than relying on labels for supervision. These models are a powerful tool for analyzing large volume, high-dimensional data in an unsupervised way. While generative models are an active research topic in machine learning, many theoretical and computational questions for such models remain unclear. This collaborative research project will study generative models from a geometric perspective, focusing on both performance guarantees and efficient implementations. The ability to efficiently create new data points that are guaranteed to be similar to the existing data has important implications in a variety of applications, including medical data analysis and privacy, bioinformatics, modeling of image and audio signals, and general high-dimensional data analysis in which it is difficult to collect labeled data for supervised algorithms.\r\n\r\nThe ideas and approaches in this research project center around the techniques that have evolved in the manifold learning field over the past decade. These mathematical tools, in particular local neighborhood preserving maps, approximation analysis in terms of intrinsic dimensionality, and construction of global coordinate systems based upon local affinity, have natural applications in the study of generative models. The project is comprised of four fundamental questions that arise in the field: (a) What are the types of distributions that generative networks are capable of learning efficiently, and how does the intrinsic dimensionality of the distribution affect convergence? (b) How can non-parametric generative models be created for dimension-reduced representations that arise in manifold learning, and which only depend on the intrinsic geometry of the data? (c) How can efficiently-computed metrics be defined between high-dimensional distributions for use in assessing the validity of various generative models? (d) How can these metrics be used to examine the various paths generative models take through the parameter space while being trained, and what clusters of starting points give optimal generators? The project will focus on both mathematical and computational aspects of these problems, aiming at resolving fundamental questions about these tools that are widely used in various data analysis and signal processing applications in science and industry.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiuyuan",
   "pi_last_name": "Cheng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiuyuan Cheng",
   "pi_email_addr": "xiuyuan.cheng@duke.edu",
   "nsf_id": "000708670",
   "pi_start_date": "2018-06-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "2200 W. Main St, Suite 710",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-e69e392a-7fff-5f51-bbba-2f53e9ded507\"> </span></p>\n<p dir=\"ltr\"><span>Recent years have witnessed a revolution in the advent of generative models.&nbsp; These are models capable of learning a distribution from training data, and also capable of efficiently resampling from that distribution in order to generate novel samples.&nbsp; Our work, funded by this award, focuses on several theoretical and computational questions that arise in this field, namely, the effects of the intrinsic dimension of the data on the learning process, learning methods for detecting global and local deviations between the generated data and the given training data, and measures of quality of a given generative model for a given task.</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span>By nature of its construction, generative models inherently generate data of a lower intrinsic dimension than the dimension of the ambient space the data lives in.&nbsp; This means they are best suited to fitting training data distributions that lie near lower-dimensional surfaces.&nbsp; Much of our research has focused on developing algorithms and theoretical analyses that recognize and exploit this fact.&nbsp; As part of this award, we have developed novel generative models that learn local diffusion-based resampling methods that are better suited to fitting low-dimensional surfaces.&nbsp; Similarly, we've developed several approaches to discriminating generative models that adapt to the intrinsic dimension of the data, and thus require significantly fewer samples to converge.&nbsp; We've applied these methods to image and video generation, learning low-dimensional embeddings of data, generation of examples of satellite images that have similar predicted income levels, and motion transfer.</p>\n<p dir=\"ltr\">A key aspect of generative models is the discriminator used to determine whether the generated data agrees with the unknown training data distribution.&nbsp; As a part of this award, we've developed maximum mean discrepancy, optimal transport, and deep learning based methods of defining statistical distances between distributions, with theoretical guarantees focusing on how many points are needed to learn a stable estimate as a function of the intrinsic dimension of the data.&nbsp; We've also developed several methods for detecting local statistics that inform where the two distributions deviate from one another, along with theoretical analyses of the consistency and power of these statistics.&nbsp; Our results have been applied in a number of settings, including hyperspectral image analysis, detecting fake images, and detection of cell subpopulations in scRNA data.</p>\n<p dir=\"ltr\">Finally, we've developed methods to assess the quality of a generative model at given tasks.&nbsp; This includes methods for developing distances between generative models by examining the geometry between the points induced by the network weights.&nbsp; This is of particular benefit for measuring whether a particular method of training converges to consistent latent spaces, which we apply to detecting disentangled factors.&nbsp; This also included modeling the null hypothesis of several visual perception based discriminators on state-of-the-art generative models, and characterizing features of these models that do not agree with the training data.</p>\n<p dir=\"ltr\">The research we conducted involved 8 PhD students, one undergraduate, and three postdoctoral researchers at UCSD, one graduate student and three undergraduate summer interns at Duke University.&nbsp; Our results for this collaborative award were published over 26 national and international venues, and presented in over 42 seminars and conferences, and 8 workshops and minisymposium organized by the authors on topics connected to this grant.&nbsp; Additionally, some of the mathematical methods we used and developed were taught in graduate courses developed by the PIs.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/28/2022<br>\n\t\t\t\t\tModified by: Xiuyuan&nbsp;Cheng</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nRecent years have witnessed a revolution in the advent of generative models.  These are models capable of learning a distribution from training data, and also capable of efficiently resampling from that distribution in order to generate novel samples.  Our work, funded by this award, focuses on several theoretical and computational questions that arise in this field, namely, the effects of the intrinsic dimension of the data on the learning process, learning methods for detecting global and local deviations between the generated data and the given training data, and measures of quality of a given generative model for a given task.\n By nature of its construction, generative models inherently generate data of a lower intrinsic dimension than the dimension of the ambient space the data lives in.  This means they are best suited to fitting training data distributions that lie near lower-dimensional surfaces.  Much of our research has focused on developing algorithms and theoretical analyses that recognize and exploit this fact.  As part of this award, we have developed novel generative models that learn local diffusion-based resampling methods that are better suited to fitting low-dimensional surfaces.  Similarly, we've developed several approaches to discriminating generative models that adapt to the intrinsic dimension of the data, and thus require significantly fewer samples to converge.  We've applied these methods to image and video generation, learning low-dimensional embeddings of data, generation of examples of satellite images that have similar predicted income levels, and motion transfer.\nA key aspect of generative models is the discriminator used to determine whether the generated data agrees with the unknown training data distribution.  As a part of this award, we've developed maximum mean discrepancy, optimal transport, and deep learning based methods of defining statistical distances between distributions, with theoretical guarantees focusing on how many points are needed to learn a stable estimate as a function of the intrinsic dimension of the data.  We've also developed several methods for detecting local statistics that inform where the two distributions deviate from one another, along with theoretical analyses of the consistency and power of these statistics.  Our results have been applied in a number of settings, including hyperspectral image analysis, detecting fake images, and detection of cell subpopulations in scRNA data.\nFinally, we've developed methods to assess the quality of a generative model at given tasks.  This includes methods for developing distances between generative models by examining the geometry between the points induced by the network weights.  This is of particular benefit for measuring whether a particular method of training converges to consistent latent spaces, which we apply to detecting disentangled factors.  This also included modeling the null hypothesis of several visual perception based discriminators on state-of-the-art generative models, and characterizing features of these models that do not agree with the training data.\nThe research we conducted involved 8 PhD students, one undergraduate, and three postdoctoral researchers at UCSD, one graduate student and three undergraduate summer interns at Duke University.  Our results for this collaborative award were published over 26 national and international venues, and presented in over 42 seminars and conferences, and 8 workshops and minisymposium organized by the authors on topics connected to this grant.  Additionally, some of the mathematical methods we used and developed were taught in graduate courses developed by the PIs.\n\n\t\t\t\t\tLast Modified: 10/28/2022\n\n\t\t\t\t\tSubmitted by: Xiuyuan Cheng"
 }
}