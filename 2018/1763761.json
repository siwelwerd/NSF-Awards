{
 "awd_id": "1763761",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium: Time Based Deep Neural Networks: An Integrated Hardware-Software Approach",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2018-05-01",
 "awd_exp_date": "2024-04-30",
 "tot_intn_awd_amt": 900000.0,
 "awd_amount": 900000.0,
 "awd_min_amd_letter_date": "2018-04-04",
 "awd_max_amd_letter_date": "2020-07-28",
 "awd_abstract_narration": "Recent advancements in deep learning hardware and algorithms are providing computers with unprecedented levels of human-like intelligence for applications such as self-driving cars, patient diagnosis and treatment, speech processing, strategy games, and education. Traditional deep learning algorithms rely on powerful computers tethered to the cloud which incurs a large communication overhead, requires extensive computing resources, and compromises privacy and security. There is a strong consensus among experts that the next frontier in deep learning will be highly-efficient neural network processors running on mobile platforms. This project aims at developing a compact and low power alternative to conventional deep learning computing hardware, specifically targeted for edge devices. The proposed approach is based on a novel computing concept called time-based circuits, which can deliver a similar level of inference performance at only a fraction of the power consumption compared to traditional methods. Throughout the project, the investigators will consider transferring the new neural network computing methods to industry. The new time-based deep learning computation methods will be incorporated into the graduate and undergraduate curricula, as well as K-12 outreach activities, of the electrical engineering and computer science departments at the University of Minnesota.\r\n\r\nThis project will focus on both hardware and software techniques for enabling deep learning applications on resource-constrained mobile platforms. On the hardware side, the team will demonstrate a prototype low-power deep neural network processor where internal operations such as convolution, pooling, and activation functions are performed entirely in the time domain. On the software side, the team will develop pruning, approximation, and hybrid approaches that can effectively reduce the complexity of deep neural networks with minimal impact on the overall inference accuracy. A unique aspect of this project is the continual interaction between the hardware and software groups to deliver the first fully time-based deep neural network engine targeted for edge devices.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Sachin",
   "pi_last_name": "Sapatnekar",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Sachin S Sapatnekar",
   "pi_email_addr": "sachin@umn.edu",
   "nsf_id": "000161507",
   "pi_start_date": "2019-08-29",
   "pi_end_date": "2019-10-31"
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Chris",
   "pi_last_name": "Kim",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chris Kim",
   "pi_email_addr": "chriskim@umn.edu",
   "nsf_id": "000289080",
   "pi_start_date": "2018-04-04",
   "pi_end_date": "2019-08-29"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chris",
   "pi_last_name": "Kim",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chris Kim",
   "pi_email_addr": "chriskim@umn.edu",
   "nsf_id": "000289080",
   "pi_start_date": "2019-10-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Sachin",
   "pi_last_name": "Sapatnekar",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Sachin S Sapatnekar",
   "pi_email_addr": "sachin@umn.edu",
   "nsf_id": "000161507",
   "pi_start_date": "2018-04-04",
   "pi_end_date": "2019-08-29"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sachin",
   "pi_last_name": "Sapatnekar",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Sachin S Sapatnekar",
   "pi_email_addr": "sachin@umn.edu",
   "nsf_id": "000161507",
   "pi_start_date": "2019-10-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Qi",
   "pi_last_name": "Zhao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qi Zhao",
   "pi_email_addr": "qzhao@umn.edu",
   "nsf_id": "000753440",
   "pi_start_date": "2018-04-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota",
  "perf_str_addr": "200 Union Street SE",
  "perf_city_name": "Minneapolis",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554550167",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 443392.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 456608.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The goal of this project is to develop innovative hardware and software techniques for a time-based deep neural network hardware system, aimed at creating compact and energy-efficient inference engines. To achieve this, we have focused on several key advancements. Firstly, we developed a partial on-board deep neural network. This reduces the need for extensive communication between the&nbsp;edge and cloud computing resources, thereby lowering overall computation power requirements. This enhancement boosts efficiency and processing speed, making the system more suitable for real-time applications.&nbsp;Secondly, we implemented layer-wise pruning methods. This involves selectively removing redundant or less significant neurons and connections within the neural network layers. By optimizing and simplifying the network, we maintain satisfactory performance while reducing computational load and energy consumption. Lastly, we incorporated approximate computing design principles into our neural network framework. Approximate computing allows for trade-offs between computation accuracy and hardware complexity, further enhancing system efficiency. By approximating calculations where exact precision is not critical, we save power and processing capacity, which is especially beneficial for applications prioritizing speed and energy efficiency over perfect accuracy.&nbsp;The project contributed to the training of graduate and undergraduate&nbsp;students in neural network chip design, algorithm development, and hardware-software co-design methodologies. A number of real chips were built in advanced CMOS technologies for concept verification.&nbsp;</span></p><br>\n<p>\n Last Modified: 06/09/2024<br>\nModified by: Chris&nbsp;Kim</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of this project is to develop innovative hardware and software techniques for a time-based deep neural network hardware system, aimed at creating compact and energy-efficient inference engines. To achieve this, we have focused on several key advancements. Firstly, we developed a partial on-board deep neural network. This reduces the need for extensive communication between theedge and cloud computing resources, thereby lowering overall computation power requirements. This enhancement boosts efficiency and processing speed, making the system more suitable for real-time applications.Secondly, we implemented layer-wise pruning methods. This involves selectively removing redundant or less significant neurons and connections within the neural network layers. By optimizing and simplifying the network, we maintain satisfactory performance while reducing computational load and energy consumption. Lastly, we incorporated approximate computing design principles into our neural network framework. Approximate computing allows for trade-offs between computation accuracy and hardware complexity, further enhancing system efficiency. By approximating calculations where exact precision is not critical, we save power and processing capacity, which is especially beneficial for applications prioritizing speed and energy efficiency over perfect accuracy.The project contributed to the training of graduate and undergraduatestudents in neural network chip design, algorithm development, and hardware-software co-design methodologies. A number of real chips were built in advanced CMOS technologies for concept verification.\t\t\t\t\tLast Modified: 06/09/2024\n\n\t\t\t\t\tSubmitted by: ChrisKim\n"
 }
}