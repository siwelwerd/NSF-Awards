{
 "awd_id": "1841471",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032928235",
 "po_email": "bmihaila@nsf.gov",
 "po_sign_block_name": "Bogdan Mihaila",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 486879.0,
 "awd_amount": 486879.0,
 "awd_min_amd_letter_date": "2018-09-07",
 "awd_max_amd_letter_date": "2018-09-07",
 "awd_abstract_narration": "The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of \"real\" data collected from the experiments with \"synthetic\" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.\r\n\r\nThe main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the \"lingua franca\" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.\r\n\r\nThis project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kyle",
   "pi_last_name": "Cranmer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kyle Cranmer",
   "pi_email_addr": "kyle.cranmer@wisc.edu",
   "nsf_id": "000357252",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Heiko",
   "pi_last_name": "Mueller",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Heiko Mueller",
   "pi_email_addr": "hm74@nyu.edu",
   "nsf_id": "000739498",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "726 Broadway",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100039616",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "768400",
   "pgm_ele_name": "CESER-Cyberinfrastructure for"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "020Z",
   "pgm_ref_txt": "OAC Facility Cyberinfrastructure"
  },
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 486879.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>Extracting scientific results from major multi-user research facilities such as the Large Hadron Collider (LHC) involves the comparison of large volumes of data collected from the instruments with \"synthetic\" data produced from computationally-intensive simulations. New data analysis techniques that leverage machine learning (ML) can help address this challenge to arrive at timely and sound scientific conclusions and to enhance the discovery potential of data-intensive science. The main objective of this work&nbsp;was to employ artificial intelligence techniques and existing computing resources to more thoroughly analyze LHC data. This required the development of scalable cyberinfrastructure (CI) that is an extension to existing CI elements such as MadMiner and the REANA system and the adaption of cutting-edge ML algorithms to run on high-performance and high-throughput computing systems.</span></p>\n<p><span>One tool called MadMiner uses AI to enhance the sensitivity of LHC measurements that might reveal the presence of new fundamental particles or interactions. The tool is computationally intensive, so it was important to leverage&nbsp;high-performance and high-throughput computing systems. Our team developed a workflow that can be interpreted by the REANA system to distribute the necessary computations across many workers in a computing cluster. We also developed a web-based tutorial so that the community could learn how to use this new and powerful tool.</span></p>\n<p><span>Another major effort within the LHC experiments&nbsp; has been to capture and preserve analysis workflows so that they can be re-used to test more theories of fundamental particle physics. Once these analysis workflows are preserved, there are still challenges that are encountered when using them. In particular, most particle physics theories have many adjustable parameters like the masses of hypothetical particles. Efficiently exploring these theories is difficult, but we've developed AI-based algorithms to help. However, additional cyberinfrastructure was needed to close the loop and turn this into a fully functional system.&nbsp;</span></p>\n<p><span>The tools that were developed have been the subject of tutorials for hundreds of PhD students around the world and are impacting data analysis practices at CERN and the LHC. These efforts have also strengthened and led to new interactions with data science and machine learning researchers in industry.&nbsp;</span></p>\n<p><span><br /></span></p>\n</div>\n</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/19/2022<br>\n\t\t\t\t\tModified by: Kyle&nbsp;Cranmer</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618124995_2020-02-27-iris-hep-posters-2020-main--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618124995_2020-02-27-iris-hep-posters-2020-main--rgov-800width.jpg\" title=\"Irina Espejo\"><img src=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618124995_2020-02-27-iris-hep-posters-2020-main--rgov-66x44.jpg\" alt=\"Irina Espejo\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Irina Espejo, a graduate student at NYU, with the poster for Scalable Cyberinfrastructure Applications</div>\n<div class=\"imageCredit\">Kyle Cranmer</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Kyle&nbsp;Cranmer</div>\n<div class=\"imageTitle\">Irina Espejo</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618207296_ER3p1AyWkAEdGp---rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618207296_ER3p1AyWkAEdGp---rgov-800width.jpg\" title=\"Sinclert\"><img src=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618207296_ER3p1AyWkAEdGp---rgov-66x44.jpg\" alt=\"Sinclert\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Sinclert Perez presents a poster on cyberinfrastructure components.</div>\n<div class=\"imageCredit\">Kyle Cranmer</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Kyle&nbsp;Cranmer</div>\n<div class=\"imageTitle\">Sinclert</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618323670_2020-02-17-preservation-thumbs-up-2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618323670_2020-02-17-preservation-thumbs-up-2--rgov-800width.jpg\" title=\"Analysis-Preservation-Bootcamp\"><img src=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618323670_2020-02-17-preservation-thumbs-up-2--rgov-66x44.jpg\" alt=\"Analysis-Preservation-Bootcamp\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Participants in Analysis Preservation Bootcamp showing off their ability to reproduce an LHC analysis</div>\n<div class=\"imageCredit\">Samuel Meehan</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Kyle&nbsp;Cranmer</div>\n<div class=\"imageTitle\">Analysis-Preservation-Bootcamp</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618499571_ScreenShot2022-01-19at1.44.22PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618499571_ScreenShot2022-01-19at1.44.22PM--rgov-800width.jpg\" title=\"MadMiner tutorial website\"><img src=\"/por/images/Reports/POR/2022/1841471/1841471_10580702_1642618499571_ScreenShot2022-01-19at1.44.22PM--rgov-66x44.jpg\" alt=\"MadMiner tutorial website\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A screenshot of the MadMiner tutorial website, which is interactive and allows students and researchers to use the tool without any local software installation requirements.</div>\n<div class=\"imageCredit\">Kyle Cranmer</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Kyle&nbsp;Cranmer</div>\n<div class=\"imageTitle\">MadMiner tutorial website</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n\n\n\nExtracting scientific results from major multi-user research facilities such as the Large Hadron Collider (LHC) involves the comparison of large volumes of data collected from the instruments with \"synthetic\" data produced from computationally-intensive simulations. New data analysis techniques that leverage machine learning (ML) can help address this challenge to arrive at timely and sound scientific conclusions and to enhance the discovery potential of data-intensive science. The main objective of this work was to employ artificial intelligence techniques and existing computing resources to more thoroughly analyze LHC data. This required the development of scalable cyberinfrastructure (CI) that is an extension to existing CI elements such as MadMiner and the REANA system and the adaption of cutting-edge ML algorithms to run on high-performance and high-throughput computing systems.\n\nOne tool called MadMiner uses AI to enhance the sensitivity of LHC measurements that might reveal the presence of new fundamental particles or interactions. The tool is computationally intensive, so it was important to leverage high-performance and high-throughput computing systems. Our team developed a workflow that can be interpreted by the REANA system to distribute the necessary computations across many workers in a computing cluster. We also developed a web-based tutorial so that the community could learn how to use this new and powerful tool.\n\nAnother major effort within the LHC experiments  has been to capture and preserve analysis workflows so that they can be re-used to test more theories of fundamental particle physics. Once these analysis workflows are preserved, there are still challenges that are encountered when using them. In particular, most particle physics theories have many adjustable parameters like the masses of hypothetical particles. Efficiently exploring these theories is difficult, but we've developed AI-based algorithms to help. However, additional cyberinfrastructure was needed to close the loop and turn this into a fully functional system. \n\nThe tools that were developed have been the subject of tutorials for hundreds of PhD students around the world and are impacting data analysis practices at CERN and the LHC. These efforts have also strengthened and led to new interactions with data science and machine learning researchers in industry. \n\n\n\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 01/19/2022\n\n\t\t\t\t\tSubmitted by: Kyle Cranmer"
 }
}