{
 "awd_id": "1816380",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Teachable Object Recognizers for the Blind",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 497653.0,
 "awd_amount": 497653.0,
 "awd_min_amd_letter_date": "2018-08-13",
 "awd_max_amd_letter_date": "2018-08-13",
 "awd_abstract_narration": "Identifying objects, from packages of food to items of clothing, is an everyday task that we perform predominantly using our sense of sight.  Blind persons, for whom sighted help is not available, attempt such tasks using a combination of senses, strategies, and ad hoc organizing systems.  In recent years, members of the blind community have been early adopters of mobile apps that use image recognition for identifying objects.   However, such solutions currently have limited use for many objects of interest, because image recognizers cannot provide enough granularity to distinguish among all possible objects of interest across all users.  They also tend to be trained on images taken by sighted people with different background clutter, scale, viewpoints, occlusion, and image quality than in photos taken by blind users.  The goal of this research is to empower blind users to customize the recognition task to their objects of interest, photo-taking strategies, and environment, through a new approach called teachability which holds the promise of allowing end users that are non-experts to provide the training examples for the machine learning models in these applications.  Specifically, a teachable object recognizer (TOR) app will be designed, deployed and publicly released that blind users can train by providing labels along with a few examples of their objects through a smartphone's camera.  If successful, project outcomes will have broad impact by changing the nature of smart assistive technologies by empowering people with disabilities to define the functionality of such technologies, especially for small recognition tasks.\r\n\r\nThis project addresses the data scarcity issue in accessibility through teachability, where end users teach models pretrained on more available yet less relevant data, with fewer but more pertinent data specific to their needs.  The work will examine the concept of teachability in the context of object recognition for the blind, and will investigate whether accessibility research can leverage advances in computer vision with limited data from blind users.  Questions to be explored will include: How to best explain such intelligent systems to users for higher quality training data? What is the best way to measure their efficacy? What design parameters, sensing modalities, interactions, and algorithms are most influential on their success? The project will include a variety of research methods: surveys, participatory design sessions, prototype usability testing, lab-based user studies, and longitudinal real-world evaluation with blind users.  Using a working prototype mobile application on teachable object recognition, it will investigate accessible interactions on learning-to-train and examine the underlying mechanisms by which robustness and scalability of such teachable assistive technologies can be improved.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hernisa",
   "pi_last_name": "Kacorri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hernisa Kacorri",
   "pi_email_addr": "hernisa@umd.edu",
   "nsf_id": "000760052",
   "pi_start_date": "2018-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "4105 Hornbake Library South Wing",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207424345",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "MD",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 497653.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-abbbc549-7fff-bfe7-b327-bc8f1be04e3a\"> </span></p>\n<p dir=\"ltr\"><span>This research centers on a new way to personalize technology for blind individuals, which we call 'teachability.' The idea is to help blind individuals to actively engage with AI in their assistive technologies and tailor it to their needs by teaching it with their own data. The motivation stems from a practical need for blind individuals to recognize specific objects, such as their keys or a diet soda. In computer vision, this task is called instance object recognition, aiming to identify and locate individual instances of objects within an image. The goal is not only to recognize the type of object but also to distinguish between different instances of the same object that may feel similar by touch. This type of recognition is particularly important for everyday tasks, such as finding personal items or managing dietary needs.</span></p>\n<p dir=\"ltr\"><span>Despite the early adoption of mobile apps that use image recognition among the blind community, challenges persist in recognizing or locating specific objects of interest. Existing manual methods, such as using Braille labels or arranging items in specific places, can be time-consuming and may require a lot of discipline or memorization. There are other challenges, too&mdash;some objects are too small for Braille labels, and labels might not be available when encountering similar items in new places. Recognizing these issues, we proposed the development of a personalized object recognition application called the Teachable Object Recognizer (TOR).</span></p>\n<p dir=\"ltr\"><span>The idea behind TOR is to combine manual and automated methods to help blind individuals identify and locate objects. It's an AI system that adapts to individual needs. For example, a blind person can take pictures of objects they want to recognize or locate later, add a label, and the app uses these to train its system. This way, users can customize the app to recognize and locate things important to them as their needs change.</span></p>\n<p dir=\"ltr\"><span>By the end of this project, we achieved a significant milestone by creating a functional prototype of TOR, deployed as an iOS app in the homes of blind users, and shared open-source code with the research community. In a preliminary study, we found this flexible, personalized approach promising. Blind participants could create different object recognizers based on their preferences and needs. We also observed how they captured photos with unique visual features to help the algorithm better learn and distinguish objects.</span></p>\n<p dir=\"ltr\"><span>However, many blind participants faced challenges in getting the system to recognize objects effectively. We identified two main difficulties: first, understanding how to teach a computer vision system without being an expert in machine learning, and second, determining if the training examples were adequate or if there were errors without being able to see.</span></p>\n<p dir=\"ltr\"><span>In subsequent years, we addressed these challenges by first focusing on the issue of training without expertise. In a study involving sighted individuals lacking machine learning knowledge, we explored their understanding and interaction with machine teaching. Using our mobile teachable object recognizer on Amazon Mechanical Turk, participants incorporated diversity in their examples, drawing parallels to human object recognition. Many held misconceptions regarding consistency and model capabilities, often sticking to initial strategies without adaptation. Interestingly, these misconceptions mirrored those found in our preliminary study with blind participants, indicating broader challenges beyond blind photography.</span></p>\n<p dir=\"ltr\"><span>Building on these insights, we then focused on addressing the challenge of inspecting training examples without sight. Through studies with blind individuals in both lab and home settings, we developed real-time audio-haptic feedback mechanisms integrated into our TOR app. These mechanisms, informed from our crowdsourcing study, help users identify diversity, inconsistencies, and quality issues among training examples, facilitating better photo capturing. Many blind participants found this feedback valuable, leading to improvements in their training sets and resulting in models with improved generalization capabilities.</span></p>\n<p dir=\"ltr\"><span>In the short term, this NSF-funded project has significantly improved blind people's quality of life, garnering industry interest for potential real-world applications. Beyond immediate impact, our research on teachability has broad implications for the design and evaluation of AI-infused applications. As AI becomes ubiquitous, understanding machine learning concepts is crucial. Shifting the training focus from experts to end-users, as seen in our project, democratizes AI, empowering users and fostering inclusivity. Our work not only benefits specific user groups like the blind community but also promotes wider engagement with AI models, promoting awareness of their principles and limitations. We demonstrated this with our educational and broadening participation efforts, exploring teachable machines for AI literacy with a group of children from historically underrepresented communities in computing. These efforts align well with recent government declarations and initiatives calling for an AI curriculum as early as the first five years of schooling, contributing to the broader AI4K12 initiative. In essence, our project advances assistive technologies and contributes to democratizing AI knowledge and usage.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/24/2024<br>\nModified by: Hernisa&nbsp;Kacorri</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1816380/1816380_10569534_1711300078093_Interface_of_the_TOR_app_with_descriptors--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1816380/1816380_10569534_1711300078093_Interface_of_the_TOR_app_with_descriptors--rgov-800width.png\" title=\"The user flow of our teachable object recognizer\"><img src=\"/por/images/Reports/POR/2024/1816380/1816380_10569534_1711300078093_Interface_of_the_TOR_app_with_descriptors--rgov-66x44.png\" alt=\"The user flow of our teachable object recognizer\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">It has three main parts: Recognizing an object in the camera view, reviewing and editing the information of the objects, and teaching an object to the model.</div>\n<div class=\"imageCredit\">Jonggi Hong, Jaina Gandhi, Ernest Essuah Mensah, Farnaz Zamiri Zeraati, Ebrima Jarjue, Kyungjun Lee, and Hernisa Kacorri https://doi.org/10.1145/3517428.3544824</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Hernisa&nbsp;Kacorri\n<div class=\"imageTitle\">The user flow of our teachable object recognizer</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1816380/1816380_10569534_1711301080995_evaluating_TOR_in_the_homes_of_blind_people--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1816380/1816380_10569534_1711301080995_evaluating_TOR_in_the_homes_of_blind_people--rgov-800width.png\" title=\"A blind participant in our study interacting with the teachable object recognizer app in their homes during the pandemic.\"><img src=\"/por/images/Reports/POR/2024/1816380/1816380_10569534_1711301080995_evaluating_TOR_in_the_homes_of_blind_people--rgov-66x44.png\" alt=\"A blind participant in our study interacting with the teachable object recognizer app in their homes during the pandemic.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our study setup included a dual video conferencing that captured participant?s activities via both a laptop camera and smart glasses worn by the participant. All faces in the illustration are blurred.</div>\n<div class=\"imageCredit\">onggi Hong, Jaina Gandhi, Ernest Essuah Mensah, Farnaz Zamiri Zeraati, Ebrima Jarjue, Kyungjun Lee, and Hernisa Kacorri. https://doi.org/10.1145/3517428.3544824</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Hernisa&nbsp;Kacorri\n<div class=\"imageTitle\">A blind participant in our study interacting with the teachable object recognizer app in their homes during the pandemic.</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThis research centers on a new way to personalize technology for blind individuals, which we call 'teachability.' The idea is to help blind individuals to actively engage with AI in their assistive technologies and tailor it to their needs by teaching it with their own data. The motivation stems from a practical need for blind individuals to recognize specific objects, such as their keys or a diet soda. In computer vision, this task is called instance object recognition, aiming to identify and locate individual instances of objects within an image. The goal is not only to recognize the type of object but also to distinguish between different instances of the same object that may feel similar by touch. This type of recognition is particularly important for everyday tasks, such as finding personal items or managing dietary needs.\n\n\nDespite the early adoption of mobile apps that use image recognition among the blind community, challenges persist in recognizing or locating specific objects of interest. Existing manual methods, such as using Braille labels or arranging items in specific places, can be time-consuming and may require a lot of discipline or memorization. There are other challenges, toosome objects are too small for Braille labels, and labels might not be available when encountering similar items in new places. Recognizing these issues, we proposed the development of a personalized object recognition application called the Teachable Object Recognizer (TOR).\n\n\nThe idea behind TOR is to combine manual and automated methods to help blind individuals identify and locate objects. It's an AI system that adapts to individual needs. For example, a blind person can take pictures of objects they want to recognize or locate later, add a label, and the app uses these to train its system. This way, users can customize the app to recognize and locate things important to them as their needs change.\n\n\nBy the end of this project, we achieved a significant milestone by creating a functional prototype of TOR, deployed as an iOS app in the homes of blind users, and shared open-source code with the research community. In a preliminary study, we found this flexible, personalized approach promising. Blind participants could create different object recognizers based on their preferences and needs. We also observed how they captured photos with unique visual features to help the algorithm better learn and distinguish objects.\n\n\nHowever, many blind participants faced challenges in getting the system to recognize objects effectively. We identified two main difficulties: first, understanding how to teach a computer vision system without being an expert in machine learning, and second, determining if the training examples were adequate or if there were errors without being able to see.\n\n\nIn subsequent years, we addressed these challenges by first focusing on the issue of training without expertise. In a study involving sighted individuals lacking machine learning knowledge, we explored their understanding and interaction with machine teaching. Using our mobile teachable object recognizer on Amazon Mechanical Turk, participants incorporated diversity in their examples, drawing parallels to human object recognition. Many held misconceptions regarding consistency and model capabilities, often sticking to initial strategies without adaptation. Interestingly, these misconceptions mirrored those found in our preliminary study with blind participants, indicating broader challenges beyond blind photography.\n\n\nBuilding on these insights, we then focused on addressing the challenge of inspecting training examples without sight. Through studies with blind individuals in both lab and home settings, we developed real-time audio-haptic feedback mechanisms integrated into our TOR app. These mechanisms, informed from our crowdsourcing study, help users identify diversity, inconsistencies, and quality issues among training examples, facilitating better photo capturing. Many blind participants found this feedback valuable, leading to improvements in their training sets and resulting in models with improved generalization capabilities.\n\n\nIn the short term, this NSF-funded project has significantly improved blind people's quality of life, garnering industry interest for potential real-world applications. Beyond immediate impact, our research on teachability has broad implications for the design and evaluation of AI-infused applications. As AI becomes ubiquitous, understanding machine learning concepts is crucial. Shifting the training focus from experts to end-users, as seen in our project, democratizes AI, empowering users and fostering inclusivity. Our work not only benefits specific user groups like the blind community but also promotes wider engagement with AI models, promoting awareness of their principles and limitations. We demonstrated this with our educational and broadening participation efforts, exploring teachable machines for AI literacy with a group of children from historically underrepresented communities in computing. These efforts align well with recent government declarations and initiatives calling for an AI curriculum as early as the first five years of schooling, contributing to the broader AI4K12 initiative. In essence, our project advances assistive technologies and contributes to democratizing AI knowledge and usage.\n\n\n\t\t\t\t\tLast Modified: 03/24/2024\n\n\t\t\t\t\tSubmitted by: HernisaKacorri\n"
 }
}