{
 "awd_id": "1805076",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Next Generation Screen Magnification Technology for People with Low Vision",
 "cfda_num": "47.041",
 "org_code": "07020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Grace Hwang",
 "awd_eff_date": "2018-07-15",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2018-07-12",
 "awd_max_amd_letter_date": "2018-07-12",
 "awd_abstract_narration": "People with low vision find it challenging to use currently available screen magnifiers, their \"go-to\" assistive technology for interacting with computing devices. Firstly, these magnifiers indiscriminately magnify screen content, including white space, as a blanket operation, often blocking important contextual information such as visual cues (e.g., borders), and semantic relationships between different user interface elements (e.g., a checkbox and its label) from the user's viewport, which is the visible area on a web page that the user can see at a given time.  Mentally reconstructing contextual information from exclusively narrow views dramatically increases the burden on the user. Secondly, low vision users have widely varying needs requiring a range of dynamic customizations for interface elements, which in currently available magnifiers is disruptive as it causes users to shift their focus. Thirdly, navigation aids to help users explore the application, obtain quick overviews, and easily locate elements of interest are lacking. The proposed project will research the design and development of SteeringWheel, a transformative next generation screen magnification technology to rectify these limitations. SteeringWheel will be based on several novel ideas. First, it will magnify the white space and non-white space user interface (UI) elements differently in order to keep the local context in the viewport post-magnification. Second, it will confine the cursor movement to the local context, thereby restricting panning. Third, it will interface with a physical dial, supporting simple rotate-and-press gestures with audio-haptic (sense of touch) feedback, that will enable users to quickly navigate different content sections, easily locate desired content, get a quick overview, and seamlessly customize the interface. A byproduct of the project will be the creation of standardized benchmark data sets to gauge the performance of current and future screen magnification technologies. It is anticipated that SteeringWheel will make it far easier for low vision users to perceive and consume digital information, leading to improved productivity. The project will serve as a launching board for creating project-driven graduate and undergraduate courses in Accessible Computing.\r\n\r\nThe project will research, design and engineer SteeringWheel, a transformative next generation screen magnification technology that is predicated on the fundamental idea of Semantics-based Locality-Preserving Magnification (SLM). For retaining contextual information, SLM incorporates knowledge about the semantics of different UI elements and inter-element relationships, rooted in the concept of a logical segment, which is a collection of related UI elements exhibiting consistency in presentation style and spatial locality (e.g. a form-field textbox and its associated border and label). SteeringWheel, which overcomes the limitations of extant screen magnification technologies, rests on two scientific ideas, namely, incorporating semantics into the magnification process, complemented by an interaction paradigm based on simple rotate and press gestures with haptic feedback, that serves as an \"all-in-one\" interface for all magnification-related operations.  The Research Plan is organized under six objectives.  OBJ 1: Algorithms for locality-preservation targeting different screen-sizes for Desktops and Mobiles.  Extraction algorithms will be designed to analyze the application layout, identify the semantically meaningful logical segments, and generate a semantic hierarchy by organizing these segments in an object-oriented fashion.  Extraction will be followed by the design of locality-preserving algorithms that differentially magnify different types of content of these segments in the semantic hierarchy.  Locality-preserving algorithms will keep most (if not all) of the contextual information within the users' viewport after magnification, to minimize the panning effort and hence the associated cognitive burden.  OBJ 2: Mapping and integration of SteeringWheel gestures onto input devices for Desktops.  A set of input gestures based on simple actions such as rotation and press will be designed for the SteeringWheel interface targeted at the Desktop platform.  The set of gestures will be implemented on a Microsoft's Surface Dial and a Gaming Mouse. These gestures will enable users to quickly navigate and easily explore different segments of the application as well as seamlessly make magnification adjustments as needed.  OBJ 3: Mapping and integration of SteeringWheel gestures onto input devices for Mobiles.  A set of input gestures that are variations of simple rotation and press will be designed for the SteeringWheel targeted at the Mobile platform. The set of gestures will be implemented on Apple and Android Watches. These gestures will enable the users to easily adjust the magnification settings on-the-fly and conveniently explore the mobile application content without having to go through a frustrating and time-consuming interaction process using the default touch-based gestures (e.g. triple press) available in smart phones.  OBJ 4: Infrastructure for semi-automated personalization of magnification settings.  Techniques will be designed to support semi-automated personalization that enables users to make customizations only once, and SteeringWheel will remember and automatically apply these users' preferences each time they revisit the application. Customization at the granularity of individual segments will be supported, which will further reduce the user's tedious efforts by automatically applying the customizations made for one segment to all other similar segments. OBJ 5: Spoken dialog assistant for SteeringWheel.  The Assistant will be designed to helps users easily locate and shift the navigation focus to segments or UI elements of interest using speech commands such as \"take me to menu bar\", \"move to the first form field\", etc. The Assistant will also allow users to specify commands such as \"increase brightness\", \"invert colors\", etc., for customizing the interface. Speech commands have the potential to transfer this cognitive burden from users to the magnification interface, thereby letting users focus on their tasks instead of expending needless time and effort locating and manually configuring individual segments and UI elements.  OBJ 6: Infrastructure for porting users' magnification profile across different devices.  Different methods will be designed to facilitate porting of users' profiles containing their preferred magnification settings for different applications to different devices, so that the low vision users need not make the same magnification adjustments for the same applications on different devices. Mechanisms will also be designed for letting users securely share their settings with each other, which will further reduce their interaction overload, as different users with similar eye conditions may need similar magnification settings for the same application.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CBET",
 "org_div_long_name": "Division of Chemical, Bioengineering, Environmental, and Transport Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "IV",
   "pi_last_name": "Ramakrishnan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "IV Ramakrishnan",
   "pi_email_addr": "ram@cs.stonybrook.edu",
   "nsf_id": "000365929",
   "pi_start_date": "2018-07-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vikas",
   "pi_last_name": "Ganjigunte Ashok",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vikas Ganjigunte Ashok",
   "pi_email_addr": "vganjigu@odu.edu",
   "nsf_id": "000759366",
   "pi_start_date": "2018-07-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117940001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "534200",
   "pgm_ele_name": "Disability & Rehab Engineering"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p style=\"text-align: left;\">People with vision impairments have come to rely on assistive technologies for interacting with computing devices.&nbsp; Specifically, people with low vision depend on screen magnifiers, their ?go-to? assistive technology, and people who are blind need screen readers. Despite great strides made in computing technology, assistive technologies for people with vision impairments have not kept pace with these advances.&nbsp; For example, low-vision users still find it challenging to use extant screen magnifiers, primarily because they indiscriminately magnify screen content including whitespace as a blanket operation, thereby causing occlusion of important contextual information such as visual cues (e.g., borders) and semantic relationships between different UI elements (e.g., checkbox and its label) from the user?s viewport. Consequently, low-vision users need to manually pan over these occluded portions and mentally reconstruct the contextual information necessary for understanding the content. The vision of this project was to develop methods for making screen magnifiers far more usable than was possible before. To this end, we developed a number of novel methods for enabling people with low vision to interact more efficiently and conveniently with computing devices, notably, PCs and smartphones, using their screen magnifiers.&nbsp; The overarching principle underlying all of these techniques was to identify the essential ?contextual? elements required by the low-vision user, and then bring these elements closer so that the user can view all these elements in the magnifier viewport (if possible) or at least navigate to them with very little panning and zooming effort. For example, the labels of a form constitute a form?s contextual elements. We applied this principle with great success to several important application scenarios for people with low vision, ranging from web browsing to interacting with applications to entering text in smartphones.&nbsp; Examples that we developed exemplifying the power and breadth of this principle include:</p>\n<p style=\"text-align: left; padding-left: 30px;\">1. A locality-preserving screen magnifier for desktop web browsing that identifies related elements in the locality and keeps them close to each other by suppressing the magnification of white space on the screen.</p>\n<p style=\"padding-left: 30px;\">2. A browser extension that leverages a state-of-the-art information extraction method to automatically identify data records and their attributes in a webpage, and subsequently present them to a user in a compactly arranged tabular format that needs significantly less screen space compared to that currently occupied by these items in the page. This way, we can pack more items within the magnifier focus, thereby reducing the overall content area for panning, and hence making it easy for screen-magnifier users to compare different items before making their selections.</p>\n<p style=\"padding-left: 30px;\">3. A similar browser extension to support online collaborative writing activities on Google Docs by bringing collaborative elements and the corresponding text in the document close to each other within the magnifier viewport, thereby minimizing context loss and panning effort.</p>\n<p style=\"padding-left: 30px;\">4. Interface augmentations for productivity applications to enable low-vision users to quickly and easily perform important routine tasks such as accessing the application commands and confirming the effects of applying these commands. The augmented interface pushes all the application commands within close proximity to the user?s present magnifier focus in the edit area on demand, so as to facilitate WYSIWYG (What You See Is What You Get) feedback, instead of the user having to switch context and search for these commands by panning and zooming.&nbsp;</p>\n<p style=\"padding-left: 30px;\">&nbsp;5. A saliency-based video magnifier for people with low vision that determines salient regions of interest in the video and tracks these regions of interest over time and ensures smooth transitions between these regions of interest without having to pause, pan and zoom, while the video is being watched.</p>\n<p style=\"padding-left: 30px;\">6. Augmenting the magnification gestures in a smartphone with one-handed Motion-On-Touch gesture for ad hoc panning and Arrowhead gesture for automated sequential panning, by leveraging its motion and gyroscope sensors. In the sequential panning mode, the user only needs to confine her attention to a fixed region on the screen as the screen elements pass through this region, line-by-line from top to bottom, akin to a scrolling ?ticker tape display?, resulting in smooth saccadic eye movements and an immersive reading experience.</p>\n<p style=\"padding-left: 30px;\">7. Gesture typing on smartphones for people with low vision to do efficient text entry by entering an entire word at a time, rather than one letter at a time, without the requirement of precisely selecting each letter, as is done traditionally. It supports a gradual and seamless transition to recall-based gesturing once the user gets familiar with the gesture shape and location.&nbsp;</p>\n<p>The idea of bringing contextual elements closer was also to develop novel methods to make screen reading applications far more usable. The project trained a new generation of students on the principles and practice of accessible computing. Finally, by serving as evaluators in user studies, it exposed dozens of people with visual impairments to the prototype technologies that were developed in this project.&nbsp;<br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/11/2022<br>\n\t\t\t\t\tModified by: I.&nbsp;V&nbsp;Ramakrishnan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "People with vision impairments have come to rely on assistive technologies for interacting with computing devices.  Specifically, people with low vision depend on screen magnifiers, their ?go-to? assistive technology, and people who are blind need screen readers. Despite great strides made in computing technology, assistive technologies for people with vision impairments have not kept pace with these advances.  For example, low-vision users still find it challenging to use extant screen magnifiers, primarily because they indiscriminately magnify screen content including whitespace as a blanket operation, thereby causing occlusion of important contextual information such as visual cues (e.g., borders) and semantic relationships between different UI elements (e.g., checkbox and its label) from the user?s viewport. Consequently, low-vision users need to manually pan over these occluded portions and mentally reconstruct the contextual information necessary for understanding the content. The vision of this project was to develop methods for making screen magnifiers far more usable than was possible before. To this end, we developed a number of novel methods for enabling people with low vision to interact more efficiently and conveniently with computing devices, notably, PCs and smartphones, using their screen magnifiers.  The overarching principle underlying all of these techniques was to identify the essential ?contextual? elements required by the low-vision user, and then bring these elements closer so that the user can view all these elements in the magnifier viewport (if possible) or at least navigate to them with very little panning and zooming effort. For example, the labels of a form constitute a form?s contextual elements. We applied this principle with great success to several important application scenarios for people with low vision, ranging from web browsing to interacting with applications to entering text in smartphones.  Examples that we developed exemplifying the power and breadth of this principle include:\n1. A locality-preserving screen magnifier for desktop web browsing that identifies related elements in the locality and keeps them close to each other by suppressing the magnification of white space on the screen.\n2. A browser extension that leverages a state-of-the-art information extraction method to automatically identify data records and their attributes in a webpage, and subsequently present them to a user in a compactly arranged tabular format that needs significantly less screen space compared to that currently occupied by these items in the page. This way, we can pack more items within the magnifier focus, thereby reducing the overall content area for panning, and hence making it easy for screen-magnifier users to compare different items before making their selections.\n3. A similar browser extension to support online collaborative writing activities on Google Docs by bringing collaborative elements and the corresponding text in the document close to each other within the magnifier viewport, thereby minimizing context loss and panning effort.\n4. Interface augmentations for productivity applications to enable low-vision users to quickly and easily perform important routine tasks such as accessing the application commands and confirming the effects of applying these commands. The augmented interface pushes all the application commands within close proximity to the user?s present magnifier focus in the edit area on demand, so as to facilitate WYSIWYG (What You See Is What You Get) feedback, instead of the user having to switch context and search for these commands by panning and zooming. \n 5. A saliency-based video magnifier for people with low vision that determines salient regions of interest in the video and tracks these regions of interest over time and ensures smooth transitions between these regions of interest without having to pause, pan and zoom, while the video is being watched.\n6. Augmenting the magnification gestures in a smartphone with one-handed Motion-On-Touch gesture for ad hoc panning and Arrowhead gesture for automated sequential panning, by leveraging its motion and gyroscope sensors. In the sequential panning mode, the user only needs to confine her attention to a fixed region on the screen as the screen elements pass through this region, line-by-line from top to bottom, akin to a scrolling ?ticker tape display?, resulting in smooth saccadic eye movements and an immersive reading experience.\n7. Gesture typing on smartphones for people with low vision to do efficient text entry by entering an entire word at a time, rather than one letter at a time, without the requirement of precisely selecting each letter, as is done traditionally. It supports a gradual and seamless transition to recall-based gesturing once the user gets familiar with the gesture shape and location. \n\nThe idea of bringing contextual elements closer was also to develop novel methods to make screen reading applications far more usable. The project trained a new generation of students on the principles and practice of accessible computing. Finally, by serving as evaluators in user studies, it exposed dozens of people with visual impairments to the prototype technologies that were developed in this project. \n\n\n\n\t\t\t\t\tLast Modified: 07/11/2022\n\n\t\t\t\t\tSubmitted by: I. V Ramakrishnan"
 }
}