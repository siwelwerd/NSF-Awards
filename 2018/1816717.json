{
 "awd_id": "1816717",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: SMALL: Low-Latency Model Inference Using Cellular Batching",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Erik Brunvand",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 411325.0,
 "awd_amount": 411325.0,
 "awd_min_amd_letter_date": "2018-06-13",
 "awd_max_amd_letter_date": "2018-06-13",
 "awd_abstract_narration": "Successful cloud deployment of machine learning services, such as language translation, image search and home assistants require a high performance serving system that can process hundreds of thousands requests per  second.  It is particularly crucial for the serving system to ensure low latency, as even tens of milliseconds increase in delays can annoy users when using a service like the home assistant.  Among the widely-used deep learning models, recurrent neural network (RNN) is an important class of models that incur high latency  when processed by existing serving systems.  This project aims to develop a new serving system that can handle a variety of Artificial Intelligence (AI) tasks using RNN-based deep learning models with significantly improved latency.\r\n\r\nTo achieve good throughput on modern hardware, one must perform batched computation.  This project develops a new, dynamic approach to batching, called Cellular Batching.  Cellular Batching performs batching and execution at the granularity of a \"cell\" (aka a subgraph with embedded model weights) instead of the entire dataflow graph, as is done in existing systems.  Under Cellular Batching, a new request can immediately join the execution of ongoing requests to minimize queuing delays and increase effective batching.  The project will complete research tasks that make Cellular Batching practical (by developing an efficient scheduler and supporting zero-downtime model upgrading) and generalize it to different models such as search-guided RNNs.\r\n\r\nDeep learning models based on RNNs are becoming widely used to accomplish various AI tasks ranging from speech recognition and language translation, to question answering.  As such, there is a pressing demand for a high-throughput and low-latency serving system, in order to improve end-user experience and reduce the cost of deployment.  By demonstrating significant latency and throughput benefits, there is high potential for Cellular Batching to be widely adopted.  This project will also develop a new course component on high performance machine learning systems as part of the graduate-level distributed systems course.\r\n\r\nThis project will produce data in the form of source code, various serving benchmarks, and experimental results.  The source code and all benchmarks used in the experiments will be distributed via Github.  A local copy of the source code and the publications produced by the project will also be made available at the URL (http://batchmaker.news.cs.nyu.edu) for at least three years beyond the award period.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jinyang",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jinyang Li",
   "pi_email_addr": "jinyang@cs.nyu.edu",
   "nsf_id": "000105743",
   "pi_start_date": "2018-06-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "251 Mercer Street",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100121110",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 411325.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In the past decade, deep learning methods have rapidly matured from experimental research to real world deployments.&nbsp; The life-cycle of a deep learning deployment consists of two phases: training and inference.&nbsp; While the performance goal of training is to achieve high throughput, it is equally important to achieve low latency for inference.&nbsp; The overarching theme of this proposal is to develop general and efficient techniques to improve the inference and training performance of those emerging, important deep neural models.</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>\n<p>Among existing deep learning methods, the class of models facing the biggest performance challenge is the Recurrent Neural Network (RNN).&nbsp; RNN processes variable length inputs and is a workhorse for language modeling tasks such as machine translation and sentiment analysis.&nbsp; The first contribution of the proposal is to develop a novel batching mechanism, called cellular batching, to accelerate the inference speed of RNN models.&nbsp; Cellular batching decomposes the RNN computation into a set of similar computation units, represented as common subgraphs in a dataflow graph.&nbsp; Instead of batching at the granularity of an entire dataflow graph as done in existing systems, cellular batching batches at the granularity of subgraphs (aka cells).&nbsp; As a result, it can batch a newly arrived request immediately with the ongoing execution of existing requests, thereby reducing latency.&nbsp; We have built the BatchMaker RNN inference engine based on this idea and shown that it achieves much better latency and throughput compared to existing systems.</p>\n<p>It is a trend in deep learning to increase the model size in order to improve accuracy on more complex tasks.&nbsp; The second contribution of the proposal is to support the training and inference of very large deep learning models whose weights do not fit in a single GPU's memory.&nbsp; We explored two approaches: one is to partition the model and its execution across multiple GPU devices (aka model parallelism), the other is to swap model parameters and intermediate results between CPU and GPU during execution.&nbsp; We have developed the Tofu automatic tensor partitioning system for dataflow-based deep learning frameworks.&nbsp; Tofu relies on static analysis of operator annotations to find how to partition each operator and uses a recursive search algorithm to find the best overall partitioning strategy for the entire dataflow graph.&nbsp; We have developed the SwapAdvisor to perform efficient GPU-CPU memory swapping.&nbsp; SwapAdvisor exploits the insight that the DNN computation structure can be known prior to execution. It maximizes the overlap of computation and communication by jointly optimizing operator scheduling, memory allocation and swap planning based on the underlying DNN dataflow graph.</p>\n<p>The third contribution of the proposal is to develop a framework, called DGL, for advancing research in the emerging field of deep graph learning.&nbsp; DGL provides users with a set of simple message-passing primitives to simplify the development of graph neural network (GNN) models.&nbsp; To achieve high performance on GPU, it casts such message-passing primitives during GNN forward and backward path computation to generalized sparse matrix operations. Doing so allows DGL to consolidate its implementation to two generalized sparse operations, SpMM (sparse dense matrix multiplication) and SDDMM (sparsely sampled dense dense matrix multiplication). By focusing the optimization efforts on these two operator kernels, DGL is able to drastically improve its training/inference speed while also achieving low memory overhead than alternative GNN platforms.&nbsp; More recently, we have also explored developing a simplified model called NARS to learn from large-scale heterogeneous graph data.&nbsp; NARS computes neighbor averaged features for random subsets of relation types.&nbsp; Compared to traditional GNN models, neighbor averaging is much more computationally efficient and it can be pre-computed a priori. We have shown that NARS can surpass the prediction accuracy of state-of-the-art GNN models on many node prediction tasks while achieving much higher training speed.&nbsp;&nbsp;</p>\n<p>This proposal has advanced the state-of-art of deep learning systems in several ways.&nbsp; Specifically, it introduces the novel techniques of cellular batching, automatic tensor partitioning, and the joint optimization of memory allocation, operator scheduling and swap planning.&nbsp; We have also achieved broad impacts through the development of the successful open-source DGL project (https://dgl.ai).&nbsp; DGL is now being widely used in both academia and industry beyond computer science.&nbsp; This proposal also funded three Ph.D. students as well as one undergraduate student for summer research.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2021<br>\n\t\t\t\t\tModified by: Jinyang&nbsp;Li</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn the past decade, deep learning methods have rapidly matured from experimental research to real world deployments.  The life-cycle of a deep learning deployment consists of two phases: training and inference.  While the performance goal of training is to achieve high throughput, it is equally important to achieve low latency for inference.  The overarching theme of this proposal is to develop general and efficient techniques to improve the inference and training performance of those emerging, important deep neural models.\n\n                                                                                                                                                                                                                                     \n\nAmong existing deep learning methods, the class of models facing the biggest performance challenge is the Recurrent Neural Network (RNN).  RNN processes variable length inputs and is a workhorse for language modeling tasks such as machine translation and sentiment analysis.  The first contribution of the proposal is to develop a novel batching mechanism, called cellular batching, to accelerate the inference speed of RNN models.  Cellular batching decomposes the RNN computation into a set of similar computation units, represented as common subgraphs in a dataflow graph.  Instead of batching at the granularity of an entire dataflow graph as done in existing systems, cellular batching batches at the granularity of subgraphs (aka cells).  As a result, it can batch a newly arrived request immediately with the ongoing execution of existing requests, thereby reducing latency.  We have built the BatchMaker RNN inference engine based on this idea and shown that it achieves much better latency and throughput compared to existing systems.\n\nIt is a trend in deep learning to increase the model size in order to improve accuracy on more complex tasks.  The second contribution of the proposal is to support the training and inference of very large deep learning models whose weights do not fit in a single GPU's memory.  We explored two approaches: one is to partition the model and its execution across multiple GPU devices (aka model parallelism), the other is to swap model parameters and intermediate results between CPU and GPU during execution.  We have developed the Tofu automatic tensor partitioning system for dataflow-based deep learning frameworks.  Tofu relies on static analysis of operator annotations to find how to partition each operator and uses a recursive search algorithm to find the best overall partitioning strategy for the entire dataflow graph.  We have developed the SwapAdvisor to perform efficient GPU-CPU memory swapping.  SwapAdvisor exploits the insight that the DNN computation structure can be known prior to execution. It maximizes the overlap of computation and communication by jointly optimizing operator scheduling, memory allocation and swap planning based on the underlying DNN dataflow graph.\n\nThe third contribution of the proposal is to develop a framework, called DGL, for advancing research in the emerging field of deep graph learning.  DGL provides users with a set of simple message-passing primitives to simplify the development of graph neural network (GNN) models.  To achieve high performance on GPU, it casts such message-passing primitives during GNN forward and backward path computation to generalized sparse matrix operations. Doing so allows DGL to consolidate its implementation to two generalized sparse operations, SpMM (sparse dense matrix multiplication) and SDDMM (sparsely sampled dense dense matrix multiplication). By focusing the optimization efforts on these two operator kernels, DGL is able to drastically improve its training/inference speed while also achieving low memory overhead than alternative GNN platforms.  More recently, we have also explored developing a simplified model called NARS to learn from large-scale heterogeneous graph data.  NARS computes neighbor averaged features for random subsets of relation types.  Compared to traditional GNN models, neighbor averaging is much more computationally efficient and it can be pre-computed a priori. We have shown that NARS can surpass the prediction accuracy of state-of-the-art GNN models on many node prediction tasks while achieving much higher training speed.  \n\nThis proposal has advanced the state-of-art of deep learning systems in several ways.  Specifically, it introduces the novel techniques of cellular batching, automatic tensor partitioning, and the joint optimization of memory allocation, operator scheduling and swap planning.  We have also achieved broad impacts through the development of the successful open-source DGL project (https://dgl.ai).  DGL is now being widely used in both academia and industry beyond computer science.  This proposal also funded three Ph.D. students as well as one undergraduate student for summer research.\n\n\t\t\t\t\tLast Modified: 12/30/2021\n\n\t\t\t\t\tSubmitted by: Jinyang Li"
 }
}