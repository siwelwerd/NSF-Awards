{
 "awd_id": "1848810",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Approximate Computing on Real World Data Using Representation and Coding",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Lawrence Goldberg",
 "awd_eff_date": "2018-06-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 148201.0,
 "awd_amount": 148201.0,
 "awd_min_amd_letter_date": "2018-09-20",
 "awd_max_amd_letter_date": "2018-09-20",
 "awd_abstract_narration": "The diminishing benefits from traditional transistor scaling has coincided with an overwhelming increase in the rate of data generation. Expert analyses show that in 2011, the amount of generated data surpassed 1.8 zeta bytes and will increase by a factor of 50 until 2020. To overcome these challenges, both the semiconductor industry and the research community are exploring new avenues in computing. Two of the promising approaches are acceleration and approximation. Among accelerators, Graphic Processing Units provide significant compute capabilities. Graphic Processing Units, originally designed to accelerate graphics functions, now are processing large amounts of real-world data that are collected from sensors, radar, environment, financial markets, and medical devices. As Graphic Processing Units play a major role in accelerating many classes of applications, improving their performance and energy efficiency has become imperative. This project leverages the fact that many applications that benefit from Graphic Processing Units are amenable to imprecise computation. This characteristic provides an opportunity to devise approximation techniques that trade small losses in the output quality for significant gains in performance and energy efficiency. This project aims to exploit this opportunity and develop a comprehensive framework for approximation in Graphic Processing Units along with effective quality control mechanisms based on coding theory. Energy efficiency is arguably the biggest challenge of the computing industry. To maintain the nation's economic leadership in this industry, it is vital to develop solutions, such as this project, that address the fundamental challenges of energy-efficient computing. The computing industry has reached an era in which many of the innovative techniques, such as this work, crosses the boundary of multiple disciplines, including computer architecture, information theory, and signal processing. Thus, it is imperative to educate a workforce that not only deeply understands multiple disciples, but also can innovate across their boundaries. This project provides a foundation for such education and research. This project will produce benchmarks, tools and general infrastructure. These artifacts will be made publicly available and will be integrated in the Georgia Tech and Harvard curricula. To transfer these technologies, the principle investigators have established close contacts with several companies. Besides the customary routes academics use to disseminate results, the principle investigator will continue organizing workshops on approximate computing. The principle investigator is also coauthoring a book on approximate computing, which will include results from this project. The investigators are committed to diversity and inclusion of undergraduate, underrepresented, and high school students and are currently mentoring students from all groups that will continue throughout this project. \r\n\r\nThis project will first develop an accelerated architecture for Graphic Processing Units, which leverage an approximate algorithmic transformation for faster and more energy efficient execution. The core idea is to use neural models to learn how a region of code behaves and replace the region with a hardware accelerator that is tightly integrated within the many cores of the Graphic Processing Units. Second, inspired by Shannon's work and the success of random codes in providing reliable communication over noisy channels, this work will devise quality control solutions that utilize coding techniques to reduce the imprecision. The code is implicit in a sense that whenever an approximate output must be improved, its correlation with available exact outputs is exploited for constructing and decoding the code. Third, the project will study mechanisms that leverage the inherent similarity and predictability in the real-world data to address the memory bottlenecks in Graphic Processing Units. The main idea is to predict the values of a data load operation when it misses in the local on-chip cache and continue the computation without waiting for the long-latency response from the off-chip memory. To perform effective prediction, this project will develop multi-regime adaptive nonlinear time-varying dynamical models for the input data using our new theories of model matching.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vahid",
   "pi_last_name": "Tarokh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vahid Tarokh",
   "pi_email_addr": "vahid.tarokh@duke.edu",
   "nsf_id": "000319586",
   "pi_start_date": "2018-09-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "140 Science Drive",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277089976",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "153E",
   "pgm_ref_txt": "Wireless comm & sig processing"
  },
  {
   "pgm_ref_code": "154E",
   "pgm_ref_txt": "Computat systems & security"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 148201.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Expert analyses showed that in 2011, the amount of generated data surpassed 1.8 zeta bytes. This is believed to have increased to greater than 90 zeta bytes by now. Processing this amount of data is not only a challenging task but also increasingly energy consuming. Yet another challenge is the amount of communications required for supplying the input data to the computing processors.</p>\n<p>Our goal in this project was to come up with new innovative approaches to address these issues. We have been inspired by human brain which is an amazingly energy and communications efficient processor.&nbsp; We think that this energy efficiency is due to its ability to identify, pay attention to, extract and process mostly the &ldquo;important parts&rdquo; of the input data.&nbsp; The other contributing factor may be the brain ability to &nbsp;&ldquo;interpolate&rdquo; between observations and experiences. For example a person watching pieces of a movie may still develop a good idea about the whole story.&nbsp; These properties may indicate that a subset of desired computations on &nbsp;important parts of the data may be adequate for most future computing applications.</p>\n<p>To this end, we studied mechanisms that leverage the inherent similarity and predictability in the real-world data, and came up with new feature extraction and interpolation methods that may be applied to both the input data and outcomes of computational calculations. &nbsp;&nbsp;We also came up with \"attention\" mechanisms (based on artificial neural networks) for the processors to determine when they should observe (sample) the data in order to achieve superior performance. &nbsp;</p>\n<p>The methods we employed are multi-disciplinary and build on various notions in multiple fields of science. Clearly, it is imperative to educate a workforce that not only deeply understands multiple disciples, but also can innovate across their boundaries. Our project provided a foundation for the integration of research and education. We trained a number of undergraduates, graduate students, and postdoctoral fellows who went on, and will go on to careers contributing to the United States knowledge based economy.&nbsp;&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/31/2020<br>\n\t\t\t\t\tModified by: Vahid&nbsp;Tarokh</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nExpert analyses showed that in 2011, the amount of generated data surpassed 1.8 zeta bytes. This is believed to have increased to greater than 90 zeta bytes by now. Processing this amount of data is not only a challenging task but also increasingly energy consuming. Yet another challenge is the amount of communications required for supplying the input data to the computing processors.\n\nOur goal in this project was to come up with new innovative approaches to address these issues. We have been inspired by human brain which is an amazingly energy and communications efficient processor.  We think that this energy efficiency is due to its ability to identify, pay attention to, extract and process mostly the \"important parts\" of the input data.  The other contributing factor may be the brain ability to  \"interpolate\" between observations and experiences. For example a person watching pieces of a movie may still develop a good idea about the whole story.  These properties may indicate that a subset of desired computations on  important parts of the data may be adequate for most future computing applications.\n\nTo this end, we studied mechanisms that leverage the inherent similarity and predictability in the real-world data, and came up with new feature extraction and interpolation methods that may be applied to both the input data and outcomes of computational calculations.   We also came up with \"attention\" mechanisms (based on artificial neural networks) for the processors to determine when they should observe (sample) the data in order to achieve superior performance.  \n\nThe methods we employed are multi-disciplinary and build on various notions in multiple fields of science. Clearly, it is imperative to educate a workforce that not only deeply understands multiple disciples, but also can innovate across their boundaries. Our project provided a foundation for the integration of research and education. We trained a number of undergraduates, graduate students, and postdoctoral fellows who went on, and will go on to careers contributing to the United States knowledge based economy.  \n\n \n\n\t\t\t\t\tLast Modified: 08/31/2020\n\n\t\t\t\t\tSubmitted by: Vahid Tarokh"
 }
}