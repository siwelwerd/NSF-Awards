{
 "awd_id": "1819157",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Multigrid Methods and Machine Learning",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 350000.0,
 "awd_amount": 350000.0,
 "awd_min_amd_letter_date": "2018-06-14",
 "awd_max_amd_letter_date": "2019-08-09",
 "awd_abstract_narration": "The goal of this project is to merge advanced tools from multigrid (MG) methods and machine learning (ML) towards the development of a novel class of numerical techniques targeting the data intensive applications emerging in physical, biological and social sciences.  Multigrid methods, including both geometric and algebraic multigrid (GMG and AMG) methods, are effective tools for solving linear as well as nonlinear algebraic system of equations arising from scientific and engineering computing.  On the other hand, there is a significant advancement in machine learning (ML) techniques, especially convolutional neural networks (CNN), which have successful applications in many areas such as image classification and processing.  The proposed project is to explore the resemblances and differences between these two different technologies so that more efficient multigrid methods as well more efficient deep learning models are developed.  The existing rich theory of multigrid method is expected to shed new light to the theoretical understanding of deep neural networks whereas the numerous empirical techniques used in the vast and ever-growning deep learning literature can be used to design general multigrid methods with wider range of applications.  This interdisciplinary research project is expected to have a direct impact to both the scientific computing community and the artificial intelligence industry.\r\n\r\nMore specifically, MG and CNN are similar for the use of multilevel hierarchy and the use of many technical components such as smoothers (MG) versus convolutions (CNN), restriction (MG) versus convolution with stride (CNN). But they also have some major differences: CNN has multiple channels of convolutions to be trained whereas MG often has one single smoother given a priori. Such relationships motivate the design of new multigrid methods with more general smoothers and restrictions that are subject training in different ways and, as a result, multigrid methods will become more adaptive and robust in its application to different practical problems.  The well-understood MG structure and theory can be adapted to understand and improve the existing deep learning model such as residual neural networks.  Furthermore, multilevel iterative techniques used in MG will also be investigated to speed up the stochastic gradient descent method that is now the standard training algorithm for most deep neural networks in machine learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jinchao",
   "pi_last_name": "Xu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jinchao Xu",
   "pi_email_addr": "xu@math.psu.edu",
   "nsf_id": "000184340",
   "pi_start_date": "2018-06-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ludmil",
   "pi_last_name": "Zikatanov",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Ludmil T Zikatanov",
   "pi_email_addr": "ltz1@psu.edu",
   "nsf_id": "000487917",
   "pi_start_date": "2018-06-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "314 McAllister Building",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168027000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 113936.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 236064.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>1, Further development of MgNet framework.&nbsp;</p>\n<p>2, Derivation of several new results concerning the approximation rate of neural networks.&nbsp;</p>\n<p>3, Convergence analysis on the application of neural network for solving PDEs.&nbsp;</p>\n<p>4, Development of greedy algorithms for solving nonlinear system arising from neural network discretization of PDEs.&nbsp;</p>\n<p>5. We have developed a framework that relates preconditioning with a posteriori error estimates in finite element methods and we have used such approach to construct reliable and efficient error estimators on graphs, as well as for systems of PDEs.&nbsp; &nbsp;</p>\n<p>6. We have the designed a two sided error indicator for the time dependent Biot's model providing a novel residual-based, a posteriori error estimates of mixed finite element methods for the three-field formulation of the Biot?s consolidation model.&nbsp;<br /><br />7. We have analyzed fractional Laplacian algorithm for drawing planar graphs and have shown a number of discrete trace theorems.&nbsp;</p>\n<p>8. We have analyzed&nbsp; a class of mimetic finite-difference (MFD) discretizations by showing their equivalence to a structure-preserving finite-element (FE) scheme.&nbsp;</p>\n<p>9. We have proposed&nbsp; a novel strategy to construct multilevel hierarchies on graphs using a Helmholtz decompositions.&nbsp;</p>\n<p>More than 15 papers have been published and more than 20 talks have been presented.&nbsp; One Ph.D student and several Postdocs have been supported and trained through this grant.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/11/2022<br>\n\t\t\t\t\tModified by: Ludmil&nbsp;T&nbsp;Zikatanov</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n1, Further development of MgNet framework. \n\n2, Derivation of several new results concerning the approximation rate of neural networks. \n\n3, Convergence analysis on the application of neural network for solving PDEs. \n\n4, Development of greedy algorithms for solving nonlinear system arising from neural network discretization of PDEs. \n\n5. We have developed a framework that relates preconditioning with a posteriori error estimates in finite element methods and we have used such approach to construct reliable and efficient error estimators on graphs, as well as for systems of PDEs.   \n\n6. We have the designed a two sided error indicator for the time dependent Biot's model providing a novel residual-based, a posteriori error estimates of mixed finite element methods for the three-field formulation of the Biot?s consolidation model. \n\n7. We have analyzed fractional Laplacian algorithm for drawing planar graphs and have shown a number of discrete trace theorems. \n\n8. We have analyzed  a class of mimetic finite-difference (MFD) discretizations by showing their equivalence to a structure-preserving finite-element (FE) scheme. \n\n9. We have proposed  a novel strategy to construct multilevel hierarchies on graphs using a Helmholtz decompositions. \n\nMore than 15 papers have been published and more than 20 talks have been presented.  One Ph.D student and several Postdocs have been supported and trained through this grant.\n\n\t\t\t\t\tLast Modified: 07/11/2022\n\n\t\t\t\t\tSubmitted by: Ludmil T Zikatanov"
 }
}