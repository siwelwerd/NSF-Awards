{
 "awd_id": "1811740",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Statistical and Computational Guarantees of Three Siblings: Expectation-Maximization, Mean-Field Variational Inference, and Gibbs Sampling",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "huixia wang",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 299999.0,
 "awd_amount": 299999.0,
 "awd_min_amd_letter_date": "2018-05-11",
 "awd_max_amd_letter_date": "2020-07-22",
 "awd_abstract_narration": "Three sibling algorithms, expectation-maximization (EM), mean-field variational inference, and Gibbs sampling, are among the most popular algorithms for statistical inference. These iterative algorithms are closely related: each can be seen as a variant of the others. Despite a wide range of successful applications in both statistics and machine learning, there is little theoretical analysis explaining the effectiveness of these algorithms for high-dimensional and complex models. The research presented in this project will significantly advance the theoretical understanding of those iterative algorithms by unveiling the statistical and computational guarantees as well as potential pitfalls for statistical inference. The wide range of applications of EM, mean-field variational inference, and Gibbs sampling ensure that the progress we make towards our  objectives will have a great impact in the broad scientific community which includes neuroscience and social sciences. Research results from this project will be disseminated through research articles, workshops and seminar series to researchers in other disciplines. The project will integrate research and education by teaching monograph courses and organizing workshops and seminars to help graduate students and postdocs, particularly minority, women, and domestic students and young researchers, work on this topic. In addition, the PI will work closely with the Yale Child Study Center and the Yale Institute for Network Science to explore appropriate and rigorous algorithms for neuroscience, autism spectrum disorder, social sciences, and data science education.\r\n\r\nThe PI studies these iterative algorithms by addressing the following questions: 1) what is the sharp (nearly necessary and sufficient) initialization condition for the algorithm to achieve global convergence to optimal statistical accuracy? 2) how fast does the algorithm converge? 3) what are sharp separation conditions or signal strengths to guarantee global convergence? 4) what are the estimation and clustering error rates and how do they compare to the optimal statistical accuracy? There are three stages to developing a comprehensive theory for analyzing iterative algorithms: 1) studying statistical and computational guarantees of EM for Gaussian mixtures for both global parameter estimation and latent cluster recovery, 2) extending EM to mean-field variational inference and Gibbs sampling, and considering a unified analysis for a class of iterative algorithms, 3) extending Gaussian mixtures and Stochastic Block Models to a unified framework of latent variable models.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Huibin",
   "pi_last_name": "Zhou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Huibin Zhou",
   "pi_email_addr": "huibin.zhou@yale.edu",
   "nsf_id": "000148898",
   "pi_start_date": "2018-05-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "24 Hillhouse Avenue",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065118950",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 99756.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 98815.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 101428.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Three sibling algorithms, expectation-maximization (EM), mean-field <span><span>vari</span></span><span><span>ational</span></span> inference, and Gibbs sampling, are among the most popular algorithms for statistic<span><span>al</span></span> inference. we have advanced the theoretic<span><span>al</span></span> understanding of those those celebrated&nbsp;algorithms by unveiling the statistic<span><span>al</span></span> and <span><span>comput</span></span><span><span>ational</span></span> guarantees for statistic<span><span>al</span></span> inference. We addressed the following questions: 1) what is the sharp (nearly necessary and sufficient) initialization condition for the algorithm to achieve glob<span><span>al</span></span> convergence to <span><span>optim</span></span><span><span>al</span></span> statistic<span><span>al</span></span> accuracy? 2) how fast does the algorithm converge? 3) what are sharp separation conditions or sign<span><span>al</span></span> strengths to guarantee glob<span><span>al</span></span> convergence? 4) what are the estimation and clustering error rates and how do they compare to the <span><span>optim</span></span><span><span>al</span></span> statistic<span><span>al</span></span> accuracy?</p>\n<div class=\"page\" title=\"Page 15\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><strong>Global Convergence of EM</strong><span>. Wu and <span><span>Zhou</span></span> (2021)</span><span>&nbsp;analyzes the classical EM algorithm for&nbsp;</span>parameter estimation in the symmetric two-component Gaussian mixtures.We show that, even in the absence of any separation between components, the randomly initialized EM algorithm converges to nearly minimax rate-<span><span>optim</span></span><span><span>al</span></span> estimate in small number of iterations with high probability. Both the nonparametric statistic<span><span>al</span></span> rate and the <span><span>sublinear</span></span> convergence rate are direct consequences of the zero Fisher information in the worst case. Refined <span><span>pointwise</span></span> guarantees beyond worst-case analysis and convergence to the <span><span>MLE</span></span> are also shown under mild conditions.</p>\n<p><strong>Global Convergence of Variational Inference</strong>. <span><span>Zhang</span></span> and <span><span>Zhou</span></span> (2021) studies the mean field method for community detection under the Stochastic Block Model. The mean field <span><span>vari</span></span><span><span>ational</span></span> Bayes method is becoming increasingly popular in statistics and machine learning. Its iterative Coordinate Ascent <span><span>Vari</span></span><span><span>ational</span></span> Inference algorithm has been widely applied to large scale Bayesian inference. Despite the popularity of the mean field method there exist remarkably few <span><span>fundament</span></span><span><span>al</span></span>&nbsp;theoretic<span><span>al</span></span> justifications. For an iterative Batch Coordinate Ascent <span><span>Vari</span></span><span><span>ational</span></span> Inference algorithm, we show that it has a linear convergence rate and converges to the minimax rate within logarithmic iterations. In addition, we obtain similar optimality results for Gibbs sampling and an iterative procedure to calculate maximum likelihood estimation.</p>\n<p><strong><span><span>Optimalities</span></span> of Gaussian Mixtures Estimation.</strong> <span><span>Doss</span></span> et <span><span>al</span></span>. (2022) studies the <span><span>optim</span></span><span><span>al</span></span> rate of estimation in a finite Gaussian location mixture model in high dimensions without any separation conditions. We assume that the number of components&nbsp;k&nbsp;is bounded and that the centers lie in a ball of bounded radius, while allowing the dimension&nbsp;d&nbsp;to be as large as the sample size&nbsp;n. We obtained the minimax rate of estimating the mixing distribution in <span><span>Wasserstein</span></span> distance. Furthermore, we show that the mixture density can be estimated at the <span><span>optim</span></span><span><span>al</span></span> parametric rate in <span><span>Hellinger</span></span> distance.</p>\n</div>\n</div>\n</div>\n<p>We have worked closely with the Yale Child Study Center to explore appropriate and rigorous algorithms for <span><span>neuroscience</span></span> and autism spectrum disorder.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/13/2022<br>\n\t\t\t\t\tModified by: Huibin&nbsp;Zhou</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThree sibling algorithms, expectation-maximization (EM), mean-field variational inference, and Gibbs sampling, are among the most popular algorithms for statistical inference. we have advanced the theoretical understanding of those those celebrated algorithms by unveiling the statistical and computational guarantees for statistical inference. We addressed the following questions: 1) what is the sharp (nearly necessary and sufficient) initialization condition for the algorithm to achieve global convergence to optimal statistical accuracy? 2) how fast does the algorithm converge? 3) what are sharp separation conditions or signal strengths to guarantee global convergence? 4) what are the estimation and clustering error rates and how do they compare to the optimal statistical accuracy?\n\n\n\n\nGlobal Convergence of EM. Wu and Zhou (2021) analyzes the classical EM algorithm for parameter estimation in the symmetric two-component Gaussian mixtures.We show that, even in the absence of any separation between components, the randomly initialized EM algorithm converges to nearly minimax rate-optimal estimate in small number of iterations with high probability. Both the nonparametric statistical rate and the sublinear convergence rate are direct consequences of the zero Fisher information in the worst case. Refined pointwise guarantees beyond worst-case analysis and convergence to the MLE are also shown under mild conditions.\n\nGlobal Convergence of Variational Inference. Zhang and Zhou (2021) studies the mean field method for community detection under the Stochastic Block Model. The mean field variational Bayes method is becoming increasingly popular in statistics and machine learning. Its iterative Coordinate Ascent Variational Inference algorithm has been widely applied to large scale Bayesian inference. Despite the popularity of the mean field method there exist remarkably few fundamental theoretical justifications. For an iterative Batch Coordinate Ascent Variational Inference algorithm, we show that it has a linear convergence rate and converges to the minimax rate within logarithmic iterations. In addition, we obtain similar optimality results for Gibbs sampling and an iterative procedure to calculate maximum likelihood estimation.\n\nOptimalities of Gaussian Mixtures Estimation. Doss et al. (2022) studies the optimal rate of estimation in a finite Gaussian location mixture model in high dimensions without any separation conditions. We assume that the number of components k is bounded and that the centers lie in a ball of bounded radius, while allowing the dimension d to be as large as the sample size n. We obtained the minimax rate of estimating the mixing distribution in Wasserstein distance. Furthermore, we show that the mixture density can be estimated at the optimal parametric rate in Hellinger distance.\n\n\n\n\nWe have worked closely with the Yale Child Study Center to explore appropriate and rigorous algorithms for neuroscience and autism spectrum disorder. \n\n \n\n\t\t\t\t\tLast Modified: 03/13/2022\n\n\t\t\t\t\tSubmitted by: Huibin Zhou"
 }
}