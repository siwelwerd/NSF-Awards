{
 "awd_id": "1827598",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative research: An integrated model of phonetic analysis and lexical access based on individual acoustic cues to features",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 320405.0,
 "awd_amount": 354605.0,
 "awd_min_amd_letter_date": "2018-08-28",
 "awd_max_amd_letter_date": "2020-06-23",
 "awd_abstract_narration": "One of the greatest mysteries in the cognitive and neural sciences is how humans achieve robust speech perception given extreme variation in the precise acoustics produced for any given speech sound or word. For example, people can produce different acoustics for the same vowel sound, while in other cases the acoustics for two different vowels may be nearly identical. The acoustic patterns also change depending on the rate at which the sounds are spoken.  Listeners may also perceive a sound that was not actually produced due to massive reductions in speech pronunciation (e.g., the \"t\" and \"y\" sounds in \"don't you\" are often reduced to \"doncha\"). Most theories assume that listeners recognize words in continuous speech by extracting consonants and vowels in a strictly sequential order. However, previous research has failed to find evidence for invariant cues in the acoustic signal that would allow listeners to extract the important information. This project uses a new tool for the study of language processing, LEXI (for Linguistic-Event EXtraction and Interpretation), to test the hypothesis that individual acoustic cues for consonants and vowels can in fact be extracted from the signal and can be used to determine the speaker's intended words. When some acoustic cues for speech sounds are modified or missing, LEXI can detect the remaining cues and evaluate them as evidence for the intended sounds and words. This research has potentially broad societal benefits, including optimization of human-machine interactions to accommodate atypical speech patterns seen in speech disorders or accented speech. This project supports training of 1-2 doctoral students and 8-10 undergraduate students through hands-on experience in experimental and computational research. All data, including code for computational models, the LEXI system, and speech databases labeled for acoustic cues will be publicly available through the Open Science Framework; preprints of all publications will be publicly available at PsyArxiv and NSF-PAR.\r\n\r\nThis interdisciplinary project unites signal analysis, psycholinguistic experimentation, and computational modeling to (1) survey the ways that acoustic cues vary in different contexts, (2) experimentally test how listeners use these cues through distributional learning for speech, and (3) use computational modeling to evaluate competing theories of how listeners recognize spoken words. The work will identify cue patterns in the signal that listeners use to recognize massive reductions in pronunciation and will experimentally test how listeners keep track of this systematic variation. This knowledge will be used to model how listeners \"tune in\" to the different ways speakers produce speech sounds. By using cues detected by LEXI as input to competing models of word recognition, the work provides an opportunity to examine the fine-grained time course of human speech recognition with large sets of spoken words; this is an important innovation because most cognitive models of speech do not work with speech input directly. Theoretical benefits include a strong test of the cue-based model of word recognition and the development of tools to allow virtually any model of speech recognition to work on real speech input, with practical implications for optimizing automatic speech recognition.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stefanie",
   "pi_last_name": "Shattuck-Hufnagel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefanie Shattuck-Hufnagel",
   "pi_email_addr": "sshuf@mit.edu",
   "nsf_id": "000410230",
   "pi_start_date": "2018-08-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jeung-Yoon",
   "pi_last_name": "Choi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jeung-Yoon Choi",
   "pi_email_addr": "jyechoi@mit.edu",
   "nsf_id": "000641298",
   "pi_start_date": "2018-08-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 320405.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 20400.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 13800.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In terms of Intellectual Merit, results of this project include both theoretical and practical advances.&nbsp;Theoretical developments include 1) specification of 40 basic acoustic cues to the contrastive phonological features of American English; 2) construction of the framework for segmental-cue-based analysis of spoken words; and 3) elaboration of the concept of standard/canonical/cues for each segment type, against which to characterize the systematic context-governed cue modification patterns which are pervasive (and sometimes extreme) in spoken utterances, and are challenging to study in non-cue-based frameworks.&nbsp; This approach directs analysis toward the characteristics of the speech signal that have been shown to provide critical information about the speaker's intended words and their sounds, and is transparently constructed to model the human speech perception process.&nbsp; While a very large number of acoustic-phonetic studies have, over the years, examined individual cues, this project has uniquely focussed on the entirety of the basic cue set, and has built on the large body of earlier work to develop a comprehensive approach to the analysis of both clearly-produced and highly-reduced speech, as well as dialects and the accented speech of L2 learners. Practical advances include an annotation system for phonologically-significant acoustic Landmarks, predicted from the underlying phonemes of the target words, with a separate tier to capture their modification patterns in a particular utterance.</p>\n<p>In terms of Broader Impacts, the primary outcomes of this project include 1) a set of tools for labelling acoustic cues and for training cue labellers, along with 2) several cue-labelled databases and 3) initial applications to utterances other than standard American English produced by typical healthy adult native speakers, which will enable general use of this approach by both the basic and clinical research communities.&nbsp; First, the toolbox generated during this project includes a video training course for novice labellers, and labelling assistance tools, such as a prediction algorithm (which provides a labelling tier with the set of maximal cues for each feature), an error-detection algorithm (to highlight typos for easy correction), and a self-evaluation algorithm (which identifies how and where a set of trainee labels departs from expert labels, enabling trainees to evaluate and learn from their errors). Second, labelling conventions have been specified for two types of acoustic cues: 8 Landmark cues (to sound categories defined by articulator-free features, such as [vocalic] and [consonantal] and 32 Other Acoustic Cues (to categories defined by articulator-bound features, such as [labial], [coronal] and [voiced]).&nbsp; Speech samples have been labelled with these 40 cues, creating a database of speech that ranges from VCVs (500 utterances from 3 speakers) and isolated words (4800 utterances from 4 speakers) to read sentences (400 TIMIT sentences from 10 speakers) and spontaneous task-directed speech (20 minutes of speech from 4 speakers). These labelled databases (available to researchers on request) provide the first comprehensively cue-labelled resource for the field, providing contexts with maximal cues (VCVs), some degree of context-governed modification (Isolated Words) and significant modifications (TIMIT sentences and task-directed speech).&nbsp; Finally, applications developed during the project include an alignment algorithm (to link labelled cues with predicted cues for interpretation as evidence for the speaker&rsquo;s intent; this work won Best Student Poster at the Acoustical Society of American meeting), initial steps toward expansion of the cue-labelling approach to other languages (Spanish, Korean and Italian), and application to two forms of clinical speech, produced by English-speaking children with Dyslexia, Specific Language Impairment or Autism, and adult speakers of European Spanish diagnosed with Parkinson&rsquo;s disease. &nbsp;Synergistic activities include the application of the cue-based approach to the analysis of prosody, including phrase-level prominence and boundaries of word groups.</p>\n<p>SUMMARY: Collectively, this project has yielded knowledge and tools for understanding the information that is available in the speech signal about the speaker&rsquo;s intended words, and how listeners get meaning from the acoustic speech signal, with a particular focus on how listeners &ldquo;tune in&rdquo; to the different ways speakers produce speech sounds and how these patterns vary with context. The foundational knowledge generated in this project has potentially broad societal benefits, including optimization of human-machine interactions to accommodate atypical speech patterns that are present in speech disorders or accented speech.</p>\n<!-- @font-face \t{font-family:\"Cambria Math\"; \tpanose-1:2 4 5 3 5 4 6 3 2 4; \tmso-font-charset:0; \tmso-generic-font-family:roman; \tmso-font-pitch:variable; \tmso-font-signature:3 0 0 0 1 0;}@font-face \t{font-family:Calibri; \tpanose-1:2 15 5 2 2 2 4 3 2 4; \tmso-font-charset:0; \tmso-generic-font-family:swiss; \tmso-font-pitch:variable; \tmso-font-signature:-536859905 -1073732485 9 0 511 0;}p.MsoNormal, li.MsoNormal, div.MsoNormal \t{mso-style-unhide:no; \tmso-style-qformat:yes; \tmso-style-parent:\"\"; \tmargin:0in; \tmargin-bottom:.0001pt; \tmso-pagination:widow-orphan; \tfont-size:12.0pt; \tfont-family:\"Calibri\",sans-serif; \tmso-ascii-font-family:Calibri; \tmso-ascii-theme-font:minor-latin; \tmso-fareast-font-family:Calibri; \tmso-fareast-theme-font:minor-latin; \tmso-hansi-font-family:Calibri; \tmso-hansi-theme-font:minor-latin; \tmso-bidi-font-family:\"Times New Roman\"; \tmso-bidi-theme-font:minor-bidi;}p \t{mso-style-noshow:yes; \tmso-style-priority:99; \tmso-margin-top-alt:auto; \tmargin-right:0in; \tmso-margin-bottom-alt:auto; \tmargin-left:0in; \tmso-pagination:widow-orphan; \tfont-size:12.0pt; \tfont-family:\"Times New Roman\",serif; \tmso-fareast-font-family:\"Times New Roman\";}.MsoChpDefault \t{mso-style-type:export-only; \tmso-default-props:yes; \tfont-family:\"Calibri\",sans-serif; \tmso-ascii-font-family:Calibri; \tmso-ascii-theme-font:minor-latin; \tmso-fareast-font-family:Calibri; \tmso-fareast-theme-font:minor-latin; \tmso-hansi-font-family:Calibri; \tmso-hansi-theme-font:minor-latin; \tmso-bidi-font-family:\"Times New Roman\"; \tmso-bidi-theme-font:minor-bidi;}div.WordSection1 \t{page:WordSection1;} -->\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/31/2022<br>\n\t\t\t\t\tModified by: Stefanie&nbsp;Shattuck-Hufnagel</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn terms of Intellectual Merit, results of this project include both theoretical and practical advances. Theoretical developments include 1) specification of 40 basic acoustic cues to the contrastive phonological features of American English; 2) construction of the framework for segmental-cue-based analysis of spoken words; and 3) elaboration of the concept of standard/canonical/cues for each segment type, against which to characterize the systematic context-governed cue modification patterns which are pervasive (and sometimes extreme) in spoken utterances, and are challenging to study in non-cue-based frameworks.  This approach directs analysis toward the characteristics of the speech signal that have been shown to provide critical information about the speaker's intended words and their sounds, and is transparently constructed to model the human speech perception process.  While a very large number of acoustic-phonetic studies have, over the years, examined individual cues, this project has uniquely focussed on the entirety of the basic cue set, and has built on the large body of earlier work to develop a comprehensive approach to the analysis of both clearly-produced and highly-reduced speech, as well as dialects and the accented speech of L2 learners. Practical advances include an annotation system for phonologically-significant acoustic Landmarks, predicted from the underlying phonemes of the target words, with a separate tier to capture their modification patterns in a particular utterance.\n\nIn terms of Broader Impacts, the primary outcomes of this project include 1) a set of tools for labelling acoustic cues and for training cue labellers, along with 2) several cue-labelled databases and 3) initial applications to utterances other than standard American English produced by typical healthy adult native speakers, which will enable general use of this approach by both the basic and clinical research communities.  First, the toolbox generated during this project includes a video training course for novice labellers, and labelling assistance tools, such as a prediction algorithm (which provides a labelling tier with the set of maximal cues for each feature), an error-detection algorithm (to highlight typos for easy correction), and a self-evaluation algorithm (which identifies how and where a set of trainee labels departs from expert labels, enabling trainees to evaluate and learn from their errors). Second, labelling conventions have been specified for two types of acoustic cues: 8 Landmark cues (to sound categories defined by articulator-free features, such as [vocalic] and [consonantal] and 32 Other Acoustic Cues (to categories defined by articulator-bound features, such as [labial], [coronal] and [voiced]).  Speech samples have been labelled with these 40 cues, creating a database of speech that ranges from VCVs (500 utterances from 3 speakers) and isolated words (4800 utterances from 4 speakers) to read sentences (400 TIMIT sentences from 10 speakers) and spontaneous task-directed speech (20 minutes of speech from 4 speakers). These labelled databases (available to researchers on request) provide the first comprehensively cue-labelled resource for the field, providing contexts with maximal cues (VCVs), some degree of context-governed modification (Isolated Words) and significant modifications (TIMIT sentences and task-directed speech).  Finally, applications developed during the project include an alignment algorithm (to link labelled cues with predicted cues for interpretation as evidence for the speaker\u2019s intent; this work won Best Student Poster at the Acoustical Society of American meeting), initial steps toward expansion of the cue-labelling approach to other languages (Spanish, Korean and Italian), and application to two forms of clinical speech, produced by English-speaking children with Dyslexia, Specific Language Impairment or Autism, and adult speakers of European Spanish diagnosed with Parkinson\u2019s disease.  Synergistic activities include the application of the cue-based approach to the analysis of prosody, including phrase-level prominence and boundaries of word groups.\n\nSUMMARY: Collectively, this project has yielded knowledge and tools for understanding the information that is available in the speech signal about the speaker\u2019s intended words, and how listeners get meaning from the acoustic speech signal, with a particular focus on how listeners \"tune in\" to the different ways speakers produce speech sounds and how these patterns vary with context. The foundational knowledge generated in this project has potentially broad societal benefits, including optimization of human-machine interactions to accommodate atypical speech patterns that are present in speech disorders or accented speech.\n\n\n \n\n\t\t\t\t\tLast Modified: 12/31/2022\n\n\t\t\t\t\tSubmitted by: Stefanie Shattuck-Hufnagel"
 }
}