{
 "awd_id": "1841368",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Perceptions of Fairness and Justice in AI Software for Talent Acquisition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2019-01-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 225054.0,
 "awd_amount": 225054.0,
 "awd_min_amd_letter_date": "2018-08-03",
 "awd_max_amd_letter_date": "2018-08-03",
 "awd_abstract_narration": "Perceived fairness and justice in job recruiting and hiring are influenced by several factors. Some factors are the consistency of the decision-making process across people and time, timely and informative feedback, propriety of the interview questions, and the extent to which pre-employment tests appear to relate to the job requirements. These factors come together to influence decisions about recruiting and hiring and are being made increasingly with the help of artificial intelligence (AI). In this project, a sociotechnical frame is applied to explore perceptions of fairness and justice of AI-supported talent acquisition algorithms. the investigator will elicit and analyze perceptions of human resources personnel, African American job seekers, and AI software designers. The outcomes will be used to inform the design of bias recognition and mitigation procedures and technologies for both humans and the algorithms being used.\r\n\r\nThe intellectual merit of this exploratory study is the development of qualitative instruments and metrics that can be used to measure perceptions of algorithmic fairness and justice. The research approach extends a theory of procedural rules for perceived fairness of selection systems by using a three-pronged approach comprising job seekers who are under-represented in the IT industry, human resource professionals who manage the talent acquisition process, and IT professionals who design AI software with fairness as the core value in product design and development. Perceptions using scenarios are examined as well as the actual experiences of jobseekers who are affected by these decisions. This research contributes to an assessment of algorithmic fairness at a time when there is currently little insight into how historically marginalized populations might perceive or be adversely affected by AI systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lynette",
   "pi_last_name": "Yarger",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Lynette M Yarger",
   "pi_email_addr": "lyarger@ist.psu.edu",
   "nsf_id": "000149811",
   "pi_start_date": "2018-08-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 225054.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 3\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p>Our research examines the perceptions and lived experiences of women, Black and Latinx undergraduate students completing degrees in computing majors who encounter algorithmic systems during the hiring process for entry-level technology positions. Specifically, we use Gilliland's procedural justice rules as a theoretical model for understanding applicants' reactions to Applicant Tracking System (ATS) software employers use to automate recruiting and hiring tasks. Procedural justice rules include formal characteristics like the consistency of the administration of interview questions, the opportunity to perform a pre-employment task, and the job-relatedness of the task. Also included are honest and informative explanations of hiring decisions and being treated respectfully by the hiring organization.</p>\n<p>Using surveys and workshops, we found that job seekers' perceptions about the fairness of the testing, interviewing, and other procedures applied during candidate selection process are inextricably linked with perceptions of equity in the outcomes of the selection process.&nbsp;Consistent with prior research on selection systems, women and racially minoritized job seekers perceived hiring procedures to be fairer when they are able to demonstrate their technical skills through a test that consistently and accurately evaluated performance. Fairness is also perceived when job seekers are offered two-way communication with an interviewer who does not exhibit personal bias based on the applicants' race, gender and ethnicity.&nbsp;</p>\n<p>When considering the fairness of the hiring decision, situational factors like the characteristics of the decision-maker (human or algorithm), type of task (programming vs. interviewing), explanation of the decision-making rationale (feedback on why I was/was not hired), and the possibility for human oversight (can a hiring decision be modified if a fault in the algorithm was identified?) become important. The salience of discrimination and performance expectations based on racial and gender stereotypesis critical in determining fairness in the hiring process. Thus, ATS software may reproduce human bias when algorithmic methods for detecting, classifying, and selecting human beings are built without considering the broader historical and social context of racial and gender inequality.&nbsp;</p>\n<p>The broader impacts of our work are immediately found in the demographic diversity of our research team, which reflects the under-representation found in our study participants. In addition, using a solid theoretical framework to empirically assess the reactions of job seekers from historically excluded groups in computing offers complex viewpoints highlighting both the positive and negative sides of ATS software, which provides novel insights for diversifying the technology workforce. From an ethical perspective, organizations should be concerned with the effects of automated recruiting and hiring procedures on the psychological well-being of job candidates. Moreover, organizations should be concerned about how perceived unfairness and algorithmic bias may lead applicants to pursue discrimination cases from a legal perspective.&nbsp;</p>\n</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/28/2021<br>\n\t\t\t\t\tModified by: Lynette&nbsp;M&nbsp;Yarger</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\nOur research examines the perceptions and lived experiences of women, Black and Latinx undergraduate students completing degrees in computing majors who encounter algorithmic systems during the hiring process for entry-level technology positions. Specifically, we use Gilliland's procedural justice rules as a theoretical model for understanding applicants' reactions to Applicant Tracking System (ATS) software employers use to automate recruiting and hiring tasks. Procedural justice rules include formal characteristics like the consistency of the administration of interview questions, the opportunity to perform a pre-employment task, and the job-relatedness of the task. Also included are honest and informative explanations of hiring decisions and being treated respectfully by the hiring organization.\n\nUsing surveys and workshops, we found that job seekers' perceptions about the fairness of the testing, interviewing, and other procedures applied during candidate selection process are inextricably linked with perceptions of equity in the outcomes of the selection process. Consistent with prior research on selection systems, women and racially minoritized job seekers perceived hiring procedures to be fairer when they are able to demonstrate their technical skills through a test that consistently and accurately evaluated performance. Fairness is also perceived when job seekers are offered two-way communication with an interviewer who does not exhibit personal bias based on the applicants' race, gender and ethnicity. \n\nWhen considering the fairness of the hiring decision, situational factors like the characteristics of the decision-maker (human or algorithm), type of task (programming vs. interviewing), explanation of the decision-making rationale (feedback on why I was/was not hired), and the possibility for human oversight (can a hiring decision be modified if a fault in the algorithm was identified?) become important. The salience of discrimination and performance expectations based on racial and gender stereotypesis critical in determining fairness in the hiring process. Thus, ATS software may reproduce human bias when algorithmic methods for detecting, classifying, and selecting human beings are built without considering the broader historical and social context of racial and gender inequality. \n\nThe broader impacts of our work are immediately found in the demographic diversity of our research team, which reflects the under-representation found in our study participants. In addition, using a solid theoretical framework to empirically assess the reactions of job seekers from historically excluded groups in computing offers complex viewpoints highlighting both the positive and negative sides of ATS software, which provides novel insights for diversifying the technology workforce. From an ethical perspective, organizations should be concerned with the effects of automated recruiting and hiring procedures on the psychological well-being of job candidates. Moreover, organizations should be concerned about how perceived unfairness and algorithmic bias may lead applicants to pursue discrimination cases from a legal perspective. \n\n\n\n\n\t\t\t\t\tLast Modified: 09/28/2021\n\n\t\t\t\t\tSubmitted by: Lynette M Yarger"
 }
}