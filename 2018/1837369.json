{
 "awd_id": "1837369",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Small: Multi-Human Assisted Learning for Multi-Agent Systems using Intrinsically Generated Event-Related EEG Potentials",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928950",
 "po_email": "rwachter@nsf.gov",
 "po_sign_block_name": "Ralph Wachter",
 "awd_eff_date": "2019-01-01",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2018-09-10",
 "awd_max_amd_letter_date": "2018-09-10",
 "awd_abstract_narration": "The goal of this project is to allow humans to assist machine learning algorithms in a Cyber-Physical System. The approach involves the use of electroencephalogram (EEG) based brain waves of the human-in-the-loop to generate feedback for the learning algorithms. Machine learning solutions are particularly useful for the monitoring, instrumenting, and optimization of complex cyber physical systems (CPS). However, several important problems spanning domains such as natural language processing, automated language translation, and understanding what is in a scene or image remain beyond the scope of true machine learning algorithms, and require human participation. The project involves collaboration with Georgia Tech's Create-X and Venture Lab programs to commercialize the outcomes of the proposed research.\r\n \r\nThis project considers recognizing the erroneous behavior of the machine intrinsically by the error related negativity (ERN) in the human EEG signals, which are then used as the reward function for the reinforcement learning (RL) algorithm of the machine to improve its intelligence. Another novel aspect of the research is the consideration of multiple humans-in-the-loop providing feedback that is used as a reward function by the learning algorithms embedded within the CPS. Finally, the project also is exploring the benefits of convergence time acceleration and deep learning techniques to extract features from EEG that are independent of canonical ERN component peaks. The research is evaluated using both a game proxy based analysis and experimental analysis within the context of real-world CPS.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Raghupathy",
   "pi_last_name": "Sivakumar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Raghupathy Sivakumar",
   "pi_email_addr": "siva@ece.gatech.edu",
   "nsf_id": "000310027",
   "pi_start_date": "2018-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Faramarz",
   "pi_last_name": "Fekri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Faramarz Fekri",
   "pi_email_addr": "fekri@ece.gatech.edu",
   "nsf_id": "000261381",
   "pi_start_date": "2018-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Ave, NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span class=\"ContentPasted0\">We investigated using brain signals obtained using EEG (electroencephalogram) as implicit human feedback to accelerate reinforcement learning agents. Providing Reinforcement Learning (RL) agents with human feedback can dramatically improve various aspects of learning. However, previous methods require the human observers to give inputs explicitly (e.g., press buttons, voice interface), burdening the human in the loop of the RL agent&rsquo;s learning process. In this work, we investigated capturing human intrinsic reactions as implicit (and natural) feedback through EEG in the form of error-related potentials (ErrP), providing a natural and direct way for humans to improve RL agent learning. We developed three relatively complex 2D discrete navigational games to experimentally evaluate the overall performance of the proposed work. Major contributions of our work are as follows, (i) We proposed and experimentally validate the zero-shot learning of ErrPs, where the ErrPs can be learned for one game, and transferred to other unseen games, (ii) we proposed a novel RL framework for integrating implicit human feedbacks via ErrPs with RL agent, improving the label efficiency and robustness to human mistakes, and (iii) compared to prior works, we extended the application of ErrPs to complex environments and demonstrated the significance of our approach for accelerated learning through real user experiments.</span></p>\n<p><span><br class=\"ContentPasted0\" /></span></p>\n<p><span class=\"ContentPasted0\">In this context, we also investigated enhancing the generalizability of detecting brain signals across different individuals which considerably reduces calibration time and effort. Brain signals like the ErrP show a lot of variation across different users, tasks, environments, etc. which makes it challenging for algorithms to reliably detect them across the board. Specifically, we proposed transfer learning and few-shot learning techniques to improve the generalization accuracy of ErrP detection models. Also, we investigated using the input of multiple simultaneous human observers to enhance the detection accuracy, thereby making the input to RL agents more efficient. P</span><span class=\"ContentPasted0\">oor generalization and robustness of brain-signal models is&nbsp;</span><span class=\"ContentPasted0\">one of the fundamental challenges present in brain-computer interfaces research and the research performed under this project helped address this.&nbsp;</span></p>\n<p><span><br class=\"ContentPasted0\" /></span></p>\n<p><span class=\"ContentPasted0\">In addition, we investigated using Inductive Logic Programming ILP to better teach RL agents to follow human instructions. Teaching a deep reinforcement learning (RL) agent to follow instructions in multi-task environments is a challenging problem. Specifically, we proposed a hierarchical reinforcement learning (HRL) framework in which a symbolic transition model is learned via ILP to efficiently produce high-level plans that can guide the agent efficiently solve different tasks. We evaluated the proposed framework on three environments in both discrete and continuous domains, showing advantages over previous representative methods. Moreover, the application of ILP into RL agents is noteworthy because it is a step in the right direction to incorporate cognition and logical reasoning into RL.</span></p>\n<p><span class=\"ContentPasted0\">&nbsp;</span></p>\n<p><span class=\"ContentPasted0\">Some other noteworthy outcomes include (I) Application of Hamiltonian Monte Carlo Sampling method to improve the sample efficiency of RL agent, and (II) Employing future dependent options in RL for solving temporal logic tasks.</span></p>\n<p><span><br class=\"ContentPasted0\" /></span></p>\n<p class=\"elementToProof\"><span style=\"font-family: UICTFontTextStyleBody;\"><span class=\"ContentPasted0\">One student defended his Ph.D. thesis on the interplay of machine learning with brain-computer interfaces and o</span></span><span class=\"ContentPasted0\">ne more student will be finishing up his Ph.D. thesis on reinforcement learning with human in the loop based on the research performed under this project. Additionally, another student is working on his Ph.D. thesis on addressing the fundamental challenges in brain-computer interfaces based on the research performed under this project.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/10/2023<br>\n\t\t\t\t\tModified by: Raghupathy&nbsp;Sivakumar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWe investigated using brain signals obtained using EEG (electroencephalogram) as implicit human feedback to accelerate reinforcement learning agents. Providing Reinforcement Learning (RL) agents with human feedback can dramatically improve various aspects of learning. However, previous methods require the human observers to give inputs explicitly (e.g., press buttons, voice interface), burdening the human in the loop of the RL agent\u2019s learning process. In this work, we investigated capturing human intrinsic reactions as implicit (and natural) feedback through EEG in the form of error-related potentials (ErrP), providing a natural and direct way for humans to improve RL agent learning. We developed three relatively complex 2D discrete navigational games to experimentally evaluate the overall performance of the proposed work. Major contributions of our work are as follows, (i) We proposed and experimentally validate the zero-shot learning of ErrPs, where the ErrPs can be learned for one game, and transferred to other unseen games, (ii) we proposed a novel RL framework for integrating implicit human feedbacks via ErrPs with RL agent, improving the label efficiency and robustness to human mistakes, and (iii) compared to prior works, we extended the application of ErrPs to complex environments and demonstrated the significance of our approach for accelerated learning through real user experiments.\n\n\n\nIn this context, we also investigated enhancing the generalizability of detecting brain signals across different individuals which considerably reduces calibration time and effort. Brain signals like the ErrP show a lot of variation across different users, tasks, environments, etc. which makes it challenging for algorithms to reliably detect them across the board. Specifically, we proposed transfer learning and few-shot learning techniques to improve the generalization accuracy of ErrP detection models. Also, we investigated using the input of multiple simultaneous human observers to enhance the detection accuracy, thereby making the input to RL agents more efficient. Poor generalization and robustness of brain-signal models is one of the fundamental challenges present in brain-computer interfaces research and the research performed under this project helped address this. \n\n\n\nIn addition, we investigated using Inductive Logic Programming ILP to better teach RL agents to follow human instructions. Teaching a deep reinforcement learning (RL) agent to follow instructions in multi-task environments is a challenging problem. Specifically, we proposed a hierarchical reinforcement learning (HRL) framework in which a symbolic transition model is learned via ILP to efficiently produce high-level plans that can guide the agent efficiently solve different tasks. We evaluated the proposed framework on three environments in both discrete and continuous domains, showing advantages over previous representative methods. Moreover, the application of ILP into RL agents is noteworthy because it is a step in the right direction to incorporate cognition and logical reasoning into RL.\n\n \n\nSome other noteworthy outcomes include (I) Application of Hamiltonian Monte Carlo Sampling method to improve the sample efficiency of RL agent, and (II) Employing future dependent options in RL for solving temporal logic tasks.\n\n\nOne student defended his Ph.D. thesis on the interplay of machine learning with brain-computer interfaces and one more student will be finishing up his Ph.D. thesis on reinforcement learning with human in the loop based on the research performed under this project. Additionally, another student is working on his Ph.D. thesis on addressing the fundamental challenges in brain-computer interfaces based on the research performed under this project.\n\n \n\n\t\t\t\t\tLast Modified: 04/10/2023\n\n\t\t\t\t\tSubmitted by: Raghupathy Sivakumar"
 }
}