{
 "awd_id": "1827550",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "The intertwined roles of vision and sensorimotor adaptation on reach-to-grasp movements",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 523550.0,
 "awd_amount": 523550.0,
 "awd_min_amd_letter_date": "2018-09-11",
 "awd_max_amd_letter_date": "2018-09-11",
 "awd_abstract_narration": "When we perform mundane daily actions like picking up a cup of coffee, our brain needs to figure out the location of the cup, its shape and weight (which changes depending on how full it is). We tend to think that our perceptual experience of the cup is what determines our interaction with it. However, several studies over the past two decades have repeatedly shown that a perceptual task, like judging the size of an object or its weight, is processed by a different part of the brain than an action task, like reaching to lift the object. This project takes an alternate view, in which the brain processes the visual scene, but this process may be subject to errors (like overestimating the size of a cup or its weight). These errors are immediately detected while an action is unfolding and the subsequent movements toward the object are quickly corrected. This research plan will explore the nature of these complex corrections. It will also examine certain circumstances in which our perception is faulty while our actions are accurate. This knowledge could help people rapidly learn new visuomotor skills, such as interacting in virtual-reality environments and teleoperation. Indeed, a more comprehensive understanding of visually guided action could inform the development of these emerging technologies. Finally, discoveries from this proposal could help improve the lives of individuals with neurological disorders, which often lead to a profound loss of motor ability that significantly impairs activities of daily living. \r\n\r\nThis research project uses state-of-the-art virtual reality environments to test 1) to what extent humans can adjust their motor actions \"on the fly\" to compensate for inaccuracies in visual perception and 2) whether visual perception changes when smooth movement coordination cannot be achieved through motor adjustments. Three mechanistic hypotheses will be tested in which sensory-prediction errors - signals produced when sensory feedback does not match one's expectations - enhance the accuracy of action during repeated visuomotor interactions. At the core of each hypothesis is the idea that when biases in perception lead to inaccurate movements, sensory-prediction errors will drive adaptive changes across the sensorimotor system. Three non-mutually exclusive mechanisms of adaptive change will be tested: (1) Rapid re-alignment of the motor output with the physical world, (2) Changes in calibration of visual perception, and/or (3) Selective changes in the contribution of specific aspects of visual information to action. To test these hypotheses, an integrated set of behavioral experiments will be conducted, in tandem with the development of computational models to mechanistically explain the closely intertwined roles of perception and action.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Fulvio",
   "pi_last_name": "Domini",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fulvio Domini",
   "pi_email_addr": "Fulvio_Domini@Brown.edu",
   "nsf_id": "000487044",
   "pi_start_date": "2018-09-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "Office of Sponsored Projects",
  "perf_city_name": "Providence",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129093",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "139700",
   "pgm_ele_name": "Cross-Directorate  Activities"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 523550.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of perception is to interact with the physical world. Simple actions you perform daily, such as picking up the coffee mug sitting on the table in front of you, require coordinated neural mechanisms that interpret the retinal images to establish where the object of your action is, what its shape is, and what kind of commands should be imparted to your muscles for the intended action to succeed. Despite the inextricable connection between perception and action, understanding how the brain organizes our perceptual world, on the one hand, and how it guides our actions, on the other hand, have been traditionally explored by separate scientific communities. A highly influential theory has even postulated that the organization of visual information shaping our perceptual experience of the world is distinct from that guiding our actions. In this research project we adopted an entirely different view that considers perception and action as two aspects of the same process.&nbsp;</p>\n<p>Visual perception is how our conscious experience of the visual world presents us with a multitude of possible actions we can take on objects in our environment. A particular motor action is the implementation of one of these possibilities. Given that our goal-directed actions seldom fail, one would intuitively expect that that our perception of the world around us is accurate. Paradoxically, however, vision scientists have frequently discovered that perception is in fact not accurate. To solve this paradox, in this project we brought together two fields, 3D vision and motor learning, to test a new, more integrated theory of visually guided action.</p>\n<p>In a series of studies, using a complex virtual reality environment where participants can see and experience virtual objects as if they were touching and manipulating real objects, we found empirical evidence that supports this theory. First, we found that faulty perceptions indeed lead to faulty actions. Second, we show that unconscious and automatic mechanisms quickly correct these faulty actions so that, when repeated, they become more accurate. Most remarkably, we also discovered that when these automatic mechanisms fail, the brain re-shapes the very perception of the objects that are targets of our actions. As a result, this project has successfully bridged two research fields - depth perception and error-based motor learning - to develop an integrated understanding of how vision and action are intertwined in visually guided action.</p>\n<p>Developing a more comprehensive framework of how visual perception is transformed into action does not only provide a window into general principles of the brain, but could also be applied to facilitate the rapid acquisition of visuomotor skills demanded by our society, such as interacting in a virtual reality (VR) environment and the use of tele-operated tools. The enthusiasm for this nascent technology has inspired a profusion of engineering solutions intended to make VR as realistic as possible, while the human experience has been widely overlooked. As a result, we still have little idea of what it means for humans to learn to interact and navigate in fully virtual worlds. The fields of both VR and teleoperation have yet to fully consider how our brain transforms visual percepts into successful motor outputs. This lacuna in our knowledge leads to clumsy interactions and frustrations within these environments. The findings from this research project could provide insight into the development of these emerging technologies.</p>\n<p>Moreover, there are obvious benefits to neurorehabilitation from understanding how visual perception is translated into motor output to interact with the physical world. Stroke and neurodegenerative disease affect hundreds of thousands of individuals each year, leading to profound losses of motor function and severe impairments in daily living. In order to restore lost motor function in afflicted individuals, it is necessary to understand and leverage the brain's remarkable ability to transform vision into action. Results from this project may lead to further research aiming to revolutionize how we think this transformation occurs, which could prove to be critical in creating bespoke neurorehabilitation training protocols that help people transfer gains made in the clinic to the real world.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/25/2022<br>\n\t\t\t\t\tModified by: Fulvio&nbsp;Domini</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1827550/1827550_10582130_1671891132640_Figure1_NSF_Final_Rep--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1827550/1827550_10582130_1671891132640_Figure1_NSF_Final_Rep--rgov-800width.jpg\" title=\"Experimental set-up\"><img src=\"/por/images/Reports/POR/2022/1827550/1827550_10582130_1671891132640_Figure1_NSF_Final_Rep--rgov-66x44.jpg\" alt=\"Experimental set-up\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">(A) Subjects view stereoscopic renderings of 3D objects in a tabletop virtual reality setup.  (B) Infrared light-emitting diodes (IREDs) attached to the fingernails provide precise location information about the fingertips. (C) Motorized devices aligned with VR objects providehaptic feedback.</div>\n<div class=\"imageCredit\">Evan Cesanek</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Fulvio&nbsp;Domini</div>\n<div class=\"imageTitle\">Experimental set-up</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe goal of perception is to interact with the physical world. Simple actions you perform daily, such as picking up the coffee mug sitting on the table in front of you, require coordinated neural mechanisms that interpret the retinal images to establish where the object of your action is, what its shape is, and what kind of commands should be imparted to your muscles for the intended action to succeed. Despite the inextricable connection between perception and action, understanding how the brain organizes our perceptual world, on the one hand, and how it guides our actions, on the other hand, have been traditionally explored by separate scientific communities. A highly influential theory has even postulated that the organization of visual information shaping our perceptual experience of the world is distinct from that guiding our actions. In this research project we adopted an entirely different view that considers perception and action as two aspects of the same process. \n\nVisual perception is how our conscious experience of the visual world presents us with a multitude of possible actions we can take on objects in our environment. A particular motor action is the implementation of one of these possibilities. Given that our goal-directed actions seldom fail, one would intuitively expect that that our perception of the world around us is accurate. Paradoxically, however, vision scientists have frequently discovered that perception is in fact not accurate. To solve this paradox, in this project we brought together two fields, 3D vision and motor learning, to test a new, more integrated theory of visually guided action.\n\nIn a series of studies, using a complex virtual reality environment where participants can see and experience virtual objects as if they were touching and manipulating real objects, we found empirical evidence that supports this theory. First, we found that faulty perceptions indeed lead to faulty actions. Second, we show that unconscious and automatic mechanisms quickly correct these faulty actions so that, when repeated, they become more accurate. Most remarkably, we also discovered that when these automatic mechanisms fail, the brain re-shapes the very perception of the objects that are targets of our actions. As a result, this project has successfully bridged two research fields - depth perception and error-based motor learning - to develop an integrated understanding of how vision and action are intertwined in visually guided action.\n\nDeveloping a more comprehensive framework of how visual perception is transformed into action does not only provide a window into general principles of the brain, but could also be applied to facilitate the rapid acquisition of visuomotor skills demanded by our society, such as interacting in a virtual reality (VR) environment and the use of tele-operated tools. The enthusiasm for this nascent technology has inspired a profusion of engineering solutions intended to make VR as realistic as possible, while the human experience has been widely overlooked. As a result, we still have little idea of what it means for humans to learn to interact and navigate in fully virtual worlds. The fields of both VR and teleoperation have yet to fully consider how our brain transforms visual percepts into successful motor outputs. This lacuna in our knowledge leads to clumsy interactions and frustrations within these environments. The findings from this research project could provide insight into the development of these emerging technologies.\n\nMoreover, there are obvious benefits to neurorehabilitation from understanding how visual perception is translated into motor output to interact with the physical world. Stroke and neurodegenerative disease affect hundreds of thousands of individuals each year, leading to profound losses of motor function and severe impairments in daily living. In order to restore lost motor function in afflicted individuals, it is necessary to understand and leverage the brain's remarkable ability to transform vision into action. Results from this project may lead to further research aiming to revolutionize how we think this transformation occurs, which could prove to be critical in creating bespoke neurorehabilitation training protocols that help people transfer gains made in the clinic to the real world.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/25/2022\n\n\t\t\t\t\tSubmitted by: Fulvio Domini"
 }
}