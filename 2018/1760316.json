{
 "awd_id": "1760316",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FRG: Collaborative Research: Randomization as a Resource for Rapid Prototyping",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922948",
 "po_email": "slevine@nsf.gov",
 "po_sign_block_name": "Stacey Levine",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 790668.0,
 "awd_amount": 790668.0,
 "awd_min_amd_letter_date": "2018-08-02",
 "awd_max_amd_letter_date": "2018-08-02",
 "awd_abstract_narration": "A principled foundation for fast prototyping data analysis methods will be developed.  The main approach will be to use fast randomized matrix algorithms, as developed within the research area known as Randomized Numerical Linear Algebra (RandNLA).  Prior work has shown that these RandNLA algorithms come with strong theory and that they perform well for many practical data science and machine learning problems.  The foundation will develop novel uses of randomization to combine complementary algorithmic and statistical perspectives. The statistical viewpoint attributes randomness to an inherent and desirable property of the data, while the algorithmic viewpoint claims randomness as a computational resource to be exploited.  The coupling of these complementary approaches poses challenging mathematical problems to be investigated in the proposed work.\r\n\r\nThe proposed work will establish the foundations for fast prototyping in two directions: A Multi-Pronged Direction to bring RandNLA to the next level and explore what is technically feasible; and an overarching Synergy Direction that fuses the results for prototyping. The Multi-Pronged Direction includes the following topics: (i) Matrix perturbation theory, to bridge the gap between traditional worst-case bounds for asymptotically small perturbations on the one hand; and perturbations caused by stochastic noise, and missing or highly corrupted matrix entries on the other hand. (ii) Implicit versus explicit regularization, where randomness as a computational resource for speeding up algorithms additionally contributes to implicit statistical regularization, thereby improving statistical and numerical robustness. (iii) Krylov space methods for fast computation of good warm-starts and computation of surrogate models in the form of low-rank approximations, and specifically a better understanding of these methods in an algorithm-independent setting. (iv) Randomized basis construction methods that use matrix factorizations to compute low-rank approximations at low to moderate levels of accuracy. The Synergy Direction will explore topics like ultra-low accuracy matrix computations in machine learning applications, where merely a correct sign or exponent is sufficient. As a group, the PIs possess unrivaled and complementary expertise in applying fundamental mathematical tools to numerical applications in machine learning, data mining and scientific computing.  Importantly, the proposed methods will have significant impact in big data analysis, scientific computing, data mining and machine learning, where matrix computations are of paramount importance. The proposed work is fundamentally interdisciplinary and will enable fast, yet user-friendly extraction of insight from large-scale data these societally-important scientific domains.  Specifically, the proposed work will (i) create a numerically reliable and robust footing for fast prototyping; (ii) advance mathematics at the interface of computer science and statistics, one of the objectives being a synergy of numerical and statistical robustness; and (iii) advance the development of an interdisciplinary community with RandNLA as a pillar for the mathematics of data. The award will allow the investigators to increase their active engagement in reaching out to undergraduate and graduate students, and research communities in numerical linear algebra, theoretical computer science, machine learning, and scientific domains such as astronomy, materials science, and genetics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Mahoney",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Mahoney",
   "pi_email_addr": "mmahoney@icsi.berkeley.edu",
   "nsf_id": "000661349",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ming",
   "pi_last_name": "Gu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ming Gu",
   "pi_email_addr": "mgu@math.berkeley.edu",
   "nsf_id": "000205573",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "Department of Statistics",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947045940",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  },
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1616",
   "pgm_ref_txt": "FOCUSED RESEARCH GROUPS IN MATH SCIENCES"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 790668.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Randomized Numerical Linear Algebra (RandNLA) has been used to establish a principled foundation for fast prototyping via randomization, where fast randomized matrix algorithms act as non-invasive and robust black-box accelerators for existing methods and software, and implementations take the form of warm starts; preconditioners; and surrogate or reduced-order models.&nbsp; To accomplish this, we have obtained a qualitatively more refined understanding of using randomized preconditioners for RandNLA.&nbsp; This is based on the sketch-and-iterate framework (that is distinct from sketch-and-solve or sketch-and-project) that permits us not only to solve core linear algebra problems more quickly, but also relate to more general optimization algorithms, in particular those widely used in machine learning.&nbsp; An example result is that we have randomized preconditioners that explicitly depend on approximate eigenvalue information, meaning that we have opened the door to using them to precondition all at once a family of problems that are parameterized by a regularization parameter.&nbsp; In terms of broader impact, this permits us to use them for a wide range of scientific machine learning problems, where the preconditioning depends on physical processes and parameters.&nbsp; Graduate students and postdoctoral fellows have been trained in state of the art numerical and algorithmic methods, at the interdisciplinary interface with scientific computation and scientific machine learning.</p><br>\n<p>\n Last Modified: 12/27/2023<br>\nModified by: Michael&nbsp;Mahoney</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nRandomized Numerical Linear Algebra (RandNLA) has been used to establish a principled foundation for fast prototyping via randomization, where fast randomized matrix algorithms act as non-invasive and robust black-box accelerators for existing methods and software, and implementations take the form of warm starts; preconditioners; and surrogate or reduced-order models. To accomplish this, we have obtained a qualitatively more refined understanding of using randomized preconditioners for RandNLA. This is based on the sketch-and-iterate framework (that is distinct from sketch-and-solve or sketch-and-project) that permits us not only to solve core linear algebra problems more quickly, but also relate to more general optimization algorithms, in particular those widely used in machine learning. An example result is that we have randomized preconditioners that explicitly depend on approximate eigenvalue information, meaning that we have opened the door to using them to precondition all at once a family of problems that are parameterized by a regularization parameter. In terms of broader impact, this permits us to use them for a wide range of scientific machine learning problems, where the preconditioning depends on physical processes and parameters. Graduate students and postdoctoral fellows have been trained in state of the art numerical and algorithmic methods, at the interdisciplinary interface with scientific computation and scientific machine learning.\t\t\t\t\tLast Modified: 12/27/2023\n\n\t\t\t\t\tSubmitted by: MichaelMahoney\n"
 }
}