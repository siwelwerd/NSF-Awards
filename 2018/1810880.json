{
 "awd_id": "1810880",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Consistent Risk Estimation under High-Dimensional Asymptotics",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 69999.0,
 "awd_amount": 69999.0,
 "awd_min_amd_letter_date": "2018-08-01",
 "awd_max_amd_letter_date": "2018-08-01",
 "awd_abstract_narration": "Learning from large datasets has been the cornerstone of modern innovations and discoveries in science, medicine, and technology. Fast prediction of unseen events is a canonical goal in statistical learning. A classic approach to this end is leave-one-out cross-validation, a time-consuming routine of leaving a datum out,  fitting the model on the rest, and testing it on the left out datum, repeatedly. The recent emergence of massive data has exacerbated the computational infeasibility of such approaches. Moreover, in many recent instances, the number of features per observation can be extremely large, adding another challenging facet to the fast estimation of prediction error. To overcome these problems a new set of scalable and consistent risk estimators will be developed in this project. \r\n\r\nThe importance of risk estimation has motivated this project of different schemes, such as cross-validation, Stein's unbiased risk estimation (SURE), Generalized cross-validation, Akaike Information Criterion (AIC), and Bootstrap. The emergence of high-dimensional datasets has challenged most classical approaches to risk estimation. For instance, the large discrepancy between in-sample and out-of-sample prediction error, in applications involving predictions based on previously unseen features, makes it hard to rely on popular estimators, such as SURE or AIC, in high-dimensional regimes where the number of predictors is smaller than or at the same order as the number of observations. On the other hand, the information value of a datum in these regimes (as opposed to the information value of a datum in low-dimensional settings) casts doubt on the reliability of other techniques, such as 5-fold cross-validation. The project offers a novel theoretical framework to find the middle ground between scalability and reliability, and specifically, to obtain theoretically consistent and computationally efficient risk-estimation schemes under high-dimensional settings. Since risk estimation is at the core of areas including but not limited to machine learning, signal processing, medical imaging, neuroscience, and social and environmental sciences, any success in this project will lead to reliable and immediate scientific discoveries and better learning systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kamiar",
   "pi_last_name": "Rahnama Rad",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kamiar Rahnama Rad",
   "pi_email_addr": "Kamiar.RahnamaRad@baruch.cuny.edu",
   "nsf_id": "000761730",
   "pi_start_date": "2018-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "CUNY Baruch College",
  "inst_street_address": "1 BERNARD BARUCH WAY",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6463122211",
  "inst_zip_code": "100105585",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NY12",
  "org_lgl_bus_name": "RESEARCH FOUNDATION OF THE CITY UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "CBKYNSMGNDD5"
 },
 "perf_inst": {
  "perf_inst_name": "Baruch College",
  "perf_str_addr": "1 Bernard Baruch Way",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100105585",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 69999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-c59a126f-7fff-dfe9-a79d-0605933d66f3\"> </span></p>\n<p dir=\"ltr\"><span>Risk estimation is at the core of many learning systems. This project explored a new asymptotic framework for studying the accuracy of risk estimation techniques in high-dimensional settings. Under this framework the accuracy of the existing risk estimates was evaluated, and several new computationally efficient and statistically accurate estimates were introduced. Our statistical analyses in the big data regime lead to the observation of deep new and counterintuitive phenomena important for applications. The new results of this project are not only improving our knowledge of high-dimensional settings, which can be used for other high-dimensional testing and estimation problems, but also offer a better understanding of risk estimation methodologies and their limitations.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Specifically, our project lead to the following outcomes and findings:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Traditional methods such as K-fold cross-validation, Stein?s unbiased risk estimation, Akaike Information Criterion are unreliable in the high-dimensional settings we studied.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Contrary to common belief, leave-one-out cross-validation (LO) does not suffer from high variance for a large class of models in the generalized linear model family. Specifically we showed that LO is a consistent estimator of the out-of-sample risk.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>A computationally efficient and statistically accurate approach, titled approximate leave-one-out cross-validation (ALO), was innovated to estimate the out-of-sample risk in the high dimensional setting.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>A unifying methodology accompanied with a rigorous theory was proposed to study the problem of risk estimation in the high dimensional setting. Specifically, a rigorous proof for the consistency of the LO, ALO, and the approximate message passing risk estimate was obtained.</span></p>\n</li>\n</ul>\n<p><br /><br /></p>\n<p dir=\"ltr\"><span>The estimators innovated in this project can be used in many application areas including but not limited to machine learning, signal processing, medical imaging, seismic data analysis, biology, neuroscience, and political, social, and environmental sciences. Hence, the success of this project will lead to more reliable scientific discoveries and better learning systems. In particular, the computationally efficient and statistically consistent risk estimators innovated in this project are currently employed to analyse spatio-temporal data obtained from neuroscience experiments aimed to elucidate and enhance our understanding of the neural representation of space in the brain.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/19/2021<br>\n\t\t\t\t\tModified by: Kamiar&nbsp;Rahnama Rad</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nRisk estimation is at the core of many learning systems. This project explored a new asymptotic framework for studying the accuracy of risk estimation techniques in high-dimensional settings. Under this framework the accuracy of the existing risk estimates was evaluated, and several new computationally efficient and statistically accurate estimates were introduced. Our statistical analyses in the big data regime lead to the observation of deep new and counterintuitive phenomena important for applications. The new results of this project are not only improving our knowledge of high-dimensional settings, which can be used for other high-dimensional testing and estimation problems, but also offer a better understanding of risk estimation methodologies and their limitations. \n\n \nSpecifically, our project lead to the following outcomes and findings:\n\n\nTraditional methods such as K-fold cross-validation, Stein?s unbiased risk estimation, Akaike Information Criterion are unreliable in the high-dimensional settings we studied.\n\n\nContrary to common belief, leave-one-out cross-validation (LO) does not suffer from high variance for a large class of models in the generalized linear model family. Specifically we showed that LO is a consistent estimator of the out-of-sample risk.\n\n\nA computationally efficient and statistically accurate approach, titled approximate leave-one-out cross-validation (ALO), was innovated to estimate the out-of-sample risk in the high dimensional setting.\n\n\nA unifying methodology accompanied with a rigorous theory was proposed to study the problem of risk estimation in the high dimensional setting. Specifically, a rigorous proof for the consistency of the LO, ALO, and the approximate message passing risk estimate was obtained.\n\n\n\n\n\n\nThe estimators innovated in this project can be used in many application areas including but not limited to machine learning, signal processing, medical imaging, seismic data analysis, biology, neuroscience, and political, social, and environmental sciences. Hence, the success of this project will lead to more reliable scientific discoveries and better learning systems. In particular, the computationally efficient and statistically consistent risk estimators innovated in this project are currently employed to analyse spatio-temporal data obtained from neuroscience experiments aimed to elucidate and enhance our understanding of the neural representation of space in the brain.\n\n \n\n\t\t\t\t\tLast Modified: 11/19/2021\n\n\t\t\t\t\tSubmitted by: Kamiar Rahnama Rad"
 }
}