{
 "awd_id": "1815656",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR:Small:System Support for Petabyte Memories",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2018-06-15",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 499813.0,
 "awd_amount": 499813.0,
 "awd_min_amd_letter_date": "2018-06-14",
 "awd_max_amd_letter_date": "2021-11-01",
 "awd_abstract_narration": "Computers have long separated working memory from permanent storage. Memory has been managed via fixed-sized pages (like a page in a book) while storage gets managed with large, variable-sized files (like a whole book). Especially for computers that provide online services, memory is growing to mammoth size. Equally important, new technologies will allow memory to be permanent like storage.  This project embraces these changes and will develop techniques to manage memory more like files to speed access to huge data sets in a mammoth amount of memory. The new techniques will be driven with new tools, data, and analysis, all made publicly available. \r\n\r\nThis project has five thrusts. First, the project will build and distribute new tools for emulating computers with mammoth memories via virtualization and application-specific compression. Second, the project will develop \"file-only memory\" to support efficient management of the arbitrarily large regions that compose a mammoth memory. Third, the project will analyze and mitigate memory fragmentation that plagues old and new solutions to memory management. Fourth, the project will development a full software-hardware stack for address translation that scales efficiently to mammoth memory sizes. Fifth, the project will engage undergraduates in both teaching of computer science generally, and in the proposed research as assistants to graduate student researchers. \r\n\r\nThis project promises more efficient computers with mammoth memories, leading to higher performance and fewer wasted resources. Through the development and release of emulation tools, this work will allow rapid efficient studies of mammoth memories not currently feasible. The study of fragmentation will spur further research on new memory management techniques to specifically address fragmentation of mammoth memories. The proposed system enhancements will enable mammoth-memory systems at reduced cost and increased efficiency and simplify management of large data sets. Finally, the investigators will release all workloads, tools, and emulation platforms as open source.\r\n \r\nThe project repository will be available from the research group home page (http://research.cs.wisc.edu/multifacet/) as well as the investigators' home web pages. Emulation tools will be published to source code repositories allowing collaborative development.  Data sets will be published in full on a web site hosted at the University of Wisconsin at Madison or be provided via a query interface to large data sets. Papers and articles will also be available on the research group web page.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Swift",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Michael M Swift",
   "pi_email_addr": "swift@cs.wisc.edu",
   "nsf_id": "000103907",
   "pi_start_date": "2018-06-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Hill",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Mark D Hill",
   "pi_email_addr": "markhill@cs.wisc.edu",
   "nsf_id": "000328470",
   "pi_start_date": "2018-06-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "1210 West Dayton Street",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061613",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499813.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-151ff679-7fff-a057-4609-e7e82d9e8da2\"> </span></p>\n<p dir=\"ltr\"><span>Memory capacities have been growing continuously, advanced by new hardware technologies such as Optane persistent memory and interconnects such as Compute Express Link (CXL) that enable large pools of remote memory. Current operating system designs, formulated when memories were small and precious, can incur substantial overhead to manage large amounts of memory, such as when allocating and assigning it to a running program. To enable more efficient management of large memories, this project investigated measurement and design approaches to adapt systems to modern memory sizes.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>This project produced an emulation environment, &empty;sim, that allows developers to test system software on applications with 30-40 times more emulated memory than is available on their platform. It does this by replacing application data in memory with constant values when application behavior does not depend on the data contents. WIth this tool, the project produced several studies of Linux kernel behavior, demonstrated tail latency problems with Linux memory management mechanisms, and studied scalability of memory initialization functions.&nbsp;&nbsp;&nbsp;</span></p>\n<p dir=\"ltr\"><span><br /></span><span>Building on memory management issues identified with &empty;sim, the project studied the performance and latency of Linux memory management mechanisms, and found that it often makes bad choices when allocating memory. For example, it may spend hundreds of milliseconds trying to optimize memory use, when the benefit of the optimization is at most a few milliseconds of performance gain. This project developed a cost-based approach to memory management for Linux, where optimizations are applied only if their benefit exceeds the cost, and showed this both improved performance and made workload behavior more stable.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>In addition, this work looked at how to make accessing data in large persistent memory faster. While current systems map it into a program a few KB at a time, this project developed a technique to store mapping information with the data, so that data could be mapped and available for use instantaneously, which allows fast access to small files as well as fast startup of programs using large files.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Separately, this project investigated the use of special-purpose accelerators and developed a model, called Gables. This is a common problem in mobile devices, where manufacturers include special purpose circuits for common operations such as video processing. The gables model can predict when a set of accelerators could improve performance based on the memory and computational intensity of the computation.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/15/2022<br>\n\t\t\t\t\tModified by: Michael&nbsp;M&nbsp;Swift</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nMemory capacities have been growing continuously, advanced by new hardware technologies such as Optane persistent memory and interconnects such as Compute Express Link (CXL) that enable large pools of remote memory. Current operating system designs, formulated when memories were small and precious, can incur substantial overhead to manage large amounts of memory, such as when allocating and assigning it to a running program. To enable more efficient management of large memories, this project investigated measurement and design approaches to adapt systems to modern memory sizes.\n\n \nThis project produced an emulation environment, &empty;sim, that allows developers to test system software on applications with 30-40 times more emulated memory than is available on their platform. It does this by replacing application data in memory with constant values when application behavior does not depend on the data contents. WIth this tool, the project produced several studies of Linux kernel behavior, demonstrated tail latency problems with Linux memory management mechanisms, and studied scalability of memory initialization functions.   \n\nBuilding on memory management issues identified with &empty;sim, the project studied the performance and latency of Linux memory management mechanisms, and found that it often makes bad choices when allocating memory. For example, it may spend hundreds of milliseconds trying to optimize memory use, when the benefit of the optimization is at most a few milliseconds of performance gain. This project developed a cost-based approach to memory management for Linux, where optimizations are applied only if their benefit exceeds the cost, and showed this both improved performance and made workload behavior more stable.\n\n \nIn addition, this work looked at how to make accessing data in large persistent memory faster. While current systems map it into a program a few KB at a time, this project developed a technique to store mapping information with the data, so that data could be mapped and available for use instantaneously, which allows fast access to small files as well as fast startup of programs using large files.\n\n \nSeparately, this project investigated the use of special-purpose accelerators and developed a model, called Gables. This is a common problem in mobile devices, where manufacturers include special purpose circuits for common operations such as video processing. The gables model can predict when a set of accelerators could improve performance based on the memory and computational intensity of the computation.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/15/2022\n\n\t\t\t\t\tSubmitted by: Michael M Swift"
 }
}