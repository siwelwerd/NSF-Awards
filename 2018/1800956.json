{
 "awd_id": "1800956",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Optimizing Scientific Peer Review",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Mary Feeney",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 531339.0,
 "awd_amount": 531339.0,
 "awd_min_amd_letter_date": "2018-06-22",
 "awd_max_amd_letter_date": "2018-06-22",
 "awd_abstract_narration": "Scientific peer review is a central process when deciding who gets published, promoted, or awarded a prize or grant. Consequently, it may have tremendous impact on the career of scientists and the direction of science. Several researchers, however, have shown that scientific peer review can be slow and low-quality. Moreover, some studies have quantified peer review biases - e.g., prejudices against certain ideas - and inconsistencies - e.g., the same work receiving widely different opinions from different groups of peers. These problems delay or sometimes truncate the dissemination of important research, affecting technological development and ultimately the economy. This project analyzes factors that affect the outcomes of peer review, uses these to improve reviewer selection, develops software that optimizes reviewer assignments, and evaluates the resulting models in the real-world context of a scientific journal, major scientific conferences, and massive open, online courses (MOOCs). By the end of this project, the scientific community will have a better understanding of the factors that affect peer review and actionable insights to make peer review better.\r\n\r\nThe first component of this project quantifies problems in bias, variance, timing, and quality of reviews. This includes direct effects (e.g., do they collaborate or cite one another) and indirect effects (e.g., do they contribute to and hopefully self-identify with the same community). The project also identifies bias as a function of personal characteristics of author and reviewer. These aspects include age, gender, and minority status, and their visibility and centrality within the field. The same general approach is used to predict the timing of reviews, including the choice to accept the review task. Lastly, the research uses this feature set to predict the quality of reviews. The result, for a given manuscript, includes prediction for each possible reviewer's biases and decision variance, likelihood and timing to participate in the review process, and ultimate review quality. The second component of this project researches and develops techniques to estimate the characteristics of potential reviewers and uses those inferred characteristics to propose, for any given manuscript, a review panel. The techniques optimize the expected value for a cost function that balances the three objectives of reviewer choice variance (bias and covariance), review timing, and review quality.  Presumably, this involves suggesting panels comprised of reviewers with complementary expertise and potentially career stage, who understand the topic and are interested in the manuscripts contents. The project allows the option of making these recommendations conditional on the background, characteristics and position of the editor under consideration. Lastly, the project tests the techniques that automatically assign reviewers and analyzes the output of the process in real world applications. In particular, the project collaborates with a large journal, scientific conferences, and massive open, online course (MOOC) organizations. Through random assignments (current methods versus the project's algorithm), the project evaluates the degree to which the assignment approach produces less reviewer choice variance, faster reviews, and reviews of higher quality. The project creates software and results that can be used by other venues.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Acuna",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel E Acuna",
   "pi_email_addr": "daniel.acuna@colorado.edu",
   "nsf_id": "000722750",
   "pi_start_date": "2018-06-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Konrad",
   "pi_last_name": "Kording",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Konrad Kording",
   "pi_email_addr": "koerding@gmail.com",
   "nsf_id": "000096260",
   "pi_start_date": "2018-06-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Evans",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "James A Evans",
   "pi_email_addr": "jevans@uchicago.edu",
   "nsf_id": "000496995",
   "pi_start_date": "2018-06-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Syracuse University",
  "inst_street_address": "900 S CROUSE AVE",
  "inst_street_address_2": "",
  "inst_city_name": "SYRACUSE",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "3154432807",
  "inst_zip_code": "13244",
  "inst_country_name": "United States",
  "cong_dist_code": "22",
  "st_cong_dist_code": "NY22",
  "org_lgl_bus_name": "SYRACUSE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "C4BXLBC11LC6"
 },
 "perf_inst": {
  "perf_inst_name": "Syracuse University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "132441190",
  "perf_ctry_code": "US",
  "perf_cong_dist": "22",
  "perf_st_cong_dist": "NY22",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "762600",
   "pgm_ele_name": "SciSIP-Sci of Sci Innov Policy"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7626",
   "pgm_ref_txt": "SCIENCE OF SCIENCE POLICY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 531339.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Scientific peer review is an essential step in the research process. It is a form of quality control for scientific publications, allowing experts to vet any claims made within them. Peer review helps ensure that published scientific findings are accurate and authoritative, avoiding potential errors or bias from individual researchers. This process helps ensure that research results can be trusted and relied upon. However, there are some problems with the peer review system. First, bias can arise from favoring specific ideas or preconceived notions about a particular topic. Reviewers can introduce noise if a reviewer gives different opinions about fundamentally similar work. Finally, delays in the process can occur when papers remain unpublished for long periods, waiting for peer review. Issues in scientific peer review have rarely been systematically investigated using data. By examining peer review with rigorous analysis and statistical evidence, we can start thinking about problems in the system and potential ways to address them.&nbsp;</p>\n<p><br />In this grant, we have systematically studied peer review processes to identify areas for improvement. Additionally, we have developed new methods for automating certain aspects of the process, enabling faster and more efficient peer reviews. Finally, we have applied our methods to real-world scenarios to demonstrate their effectiveness.</p>\n<p><br />Our findings indicate a strong relationship between authors and their sources for review. Specifically, research revealed that the source of peer reviewer names directly impacted the evaluation of the work in question. Examining a large dataset of reviews, we discovered that author-recommended reviewers offered higher ratings to the given work but also produced overall reviews that were slower and of lower quality than those obtained through other, perhaps more objective methods. These results suggest that authors considerably influence the peer review process, potentially creating bias and skewing its objective nature. While such subjectivity can initially result in more favorable evaluations, it ultimately deters from providing quality feedback needed to improve scientific writing. Therefore, relying heavily upon author recommendations when selecting peer reviewers should be considered carefully.&nbsp;</p>\n<p><br />One focus of this grant is to concentrate on establishing new methods for modeling authors' and reviewers' expertise. To accomplish this, we explored methods from Natural Language Processing to numerically represent the publications of scientists. These representations are then used in machine learning tasks such as search, recommendation, and, most importantly, reviewer-manuscript matching. In particular, the obtained representations effectively capture an individual's works' intricate details and nuances for reliable matching. Moreover, these numerical measures allow for content-based comparison between individuals. In other words, our methods can accurately capture someone's topic without directly considering their seniority, field, and other factors.</p>\n<p><br />Novelty is essential in advancing scientific research, but it is often misunderstood and undervalued. Grant funding agencies tend to be risk-averse, making it difficult for researchers to acquire funds for innovative projects outside the traditional framework. Our grant produced essential works to understand novelty's role and its importance in the larger context of science. Our research has explored how scientific progress is driven by surprise&mdash;the more creative, unpredictable, and non-linear a project's approach to problem-solving, the more potential for revolutionizing an established field or filling gaps in understanding. As such, we think peer review could reward impulse-based innovation just as much as replicable methods; both have an essential role in pushing forward new lines of inquiry. The novelty can drive groundbreaking discoveries and challenge existing paradigms when adequately funded and supported by academic institutions. We firmly believe that scientific advancement will remain stifled without a culture that encourages pushing boundaries and embracing unconventional thinking.</p>\n<p><br />In 2020 and 2021, we successfully hosted the NeuroMatch global conference despite the COVID-19 pandemic due to methods and understandings developed within our grant. It had far more than 1,000 attendees worldwide who were chosen according to methods we created using Natural Language Processing. This recognition of our methods was critical for further testing and applying our research, enabling us to go on confidently. We also applied peer review methods for work selection, which have been used for conferences like ours since the late 2000s. This increased trust and transparency among conference attendees, incorporating values such as rigor, reproducibility, and ethical considerations into all of our methods used at NeuroMatch. By sharing these methods with collaborators worldwide through NeuroMatch's platform, we can continue exploring cutting-edge science while ensuring high-quality results.</p>\n<p><br />Thanks to the grant, we developed several pieces of software related to peer review. One such program is PubMed Parser, which makes it easy to parse Open Access Papers published by PubMed (https://github.com/titipata/pubmed_parser). Thanks to this tool, users can access structured data quickly and take advantage of features like article extraction and saving articles in a directory. The second software is Paper Reviewer Matcher (https://github.com/titipata/paper-reviewer-matcher), which aims to streamline the process of finding appropriate reviewers for research conferences.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/21/2022<br>\n\t\t\t\t\tModified by: Daniel&nbsp;E&nbsp;Acuna</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nScientific peer review is an essential step in the research process. It is a form of quality control for scientific publications, allowing experts to vet any claims made within them. Peer review helps ensure that published scientific findings are accurate and authoritative, avoiding potential errors or bias from individual researchers. This process helps ensure that research results can be trusted and relied upon. However, there are some problems with the peer review system. First, bias can arise from favoring specific ideas or preconceived notions about a particular topic. Reviewers can introduce noise if a reviewer gives different opinions about fundamentally similar work. Finally, delays in the process can occur when papers remain unpublished for long periods, waiting for peer review. Issues in scientific peer review have rarely been systematically investigated using data. By examining peer review with rigorous analysis and statistical evidence, we can start thinking about problems in the system and potential ways to address them. \n\n\nIn this grant, we have systematically studied peer review processes to identify areas for improvement. Additionally, we have developed new methods for automating certain aspects of the process, enabling faster and more efficient peer reviews. Finally, we have applied our methods to real-world scenarios to demonstrate their effectiveness.\n\n\nOur findings indicate a strong relationship between authors and their sources for review. Specifically, research revealed that the source of peer reviewer names directly impacted the evaluation of the work in question. Examining a large dataset of reviews, we discovered that author-recommended reviewers offered higher ratings to the given work but also produced overall reviews that were slower and of lower quality than those obtained through other, perhaps more objective methods. These results suggest that authors considerably influence the peer review process, potentially creating bias and skewing its objective nature. While such subjectivity can initially result in more favorable evaluations, it ultimately deters from providing quality feedback needed to improve scientific writing. Therefore, relying heavily upon author recommendations when selecting peer reviewers should be considered carefully. \n\n\nOne focus of this grant is to concentrate on establishing new methods for modeling authors' and reviewers' expertise. To accomplish this, we explored methods from Natural Language Processing to numerically represent the publications of scientists. These representations are then used in machine learning tasks such as search, recommendation, and, most importantly, reviewer-manuscript matching. In particular, the obtained representations effectively capture an individual's works' intricate details and nuances for reliable matching. Moreover, these numerical measures allow for content-based comparison between individuals. In other words, our methods can accurately capture someone's topic without directly considering their seniority, field, and other factors.\n\n\nNovelty is essential in advancing scientific research, but it is often misunderstood and undervalued. Grant funding agencies tend to be risk-averse, making it difficult for researchers to acquire funds for innovative projects outside the traditional framework. Our grant produced essential works to understand novelty's role and its importance in the larger context of science. Our research has explored how scientific progress is driven by surprise&mdash;the more creative, unpredictable, and non-linear a project's approach to problem-solving, the more potential for revolutionizing an established field or filling gaps in understanding. As such, we think peer review could reward impulse-based innovation just as much as replicable methods; both have an essential role in pushing forward new lines of inquiry. The novelty can drive groundbreaking discoveries and challenge existing paradigms when adequately funded and supported by academic institutions. We firmly believe that scientific advancement will remain stifled without a culture that encourages pushing boundaries and embracing unconventional thinking.\n\n\nIn 2020 and 2021, we successfully hosted the NeuroMatch global conference despite the COVID-19 pandemic due to methods and understandings developed within our grant. It had far more than 1,000 attendees worldwide who were chosen according to methods we created using Natural Language Processing. This recognition of our methods was critical for further testing and applying our research, enabling us to go on confidently. We also applied peer review methods for work selection, which have been used for conferences like ours since the late 2000s. This increased trust and transparency among conference attendees, incorporating values such as rigor, reproducibility, and ethical considerations into all of our methods used at NeuroMatch. By sharing these methods with collaborators worldwide through NeuroMatch's platform, we can continue exploring cutting-edge science while ensuring high-quality results.\n\n\nThanks to the grant, we developed several pieces of software related to peer review. One such program is PubMed Parser, which makes it easy to parse Open Access Papers published by PubMed (https://github.com/titipata/pubmed_parser). Thanks to this tool, users can access structured data quickly and take advantage of features like article extraction and saving articles in a directory. The second software is Paper Reviewer Matcher (https://github.com/titipata/paper-reviewer-matcher), which aims to streamline the process of finding appropriate reviewers for research conferences.\n\n\t\t\t\t\tLast Modified: 11/21/2022\n\n\t\t\t\t\tSubmitted by: Daniel E Acuna"
 }
}