{
 "awd_id": "1747483",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Building the Community for the Open Storage Network",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927092",
 "po_email": "alsuarez@nsf.gov",
 "po_sign_block_name": "Alejandro Suarez",
 "awd_eff_date": "2018-06-15",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 431786.0,
 "awd_amount": 431786.0,
 "awd_min_amd_letter_date": "2018-06-07",
 "awd_max_amd_letter_date": "2019-03-21",
 "awd_abstract_narration": "The scientific community is facing a major challenge dealing with the increasing amount of open scientific data emerging from research projects on all scales-- from large facilities to small research labs. Over the last five years the NSF has funded more than 200 high-speed connections to the Internet-2 backbone operating at 10-100Gbps speeds. The goal of this project is to develop a prototype module for a high performance distributed storage system that extends the usability of the existing high-speed interconnects. This project is a pilot for a potential national-scale storage infrastructure for open scientific data, which at full scale could serve hundred sites and many hundreds of Petabytes.  Many of the technologies associated with such a distributed system already exist; the key challenge in this project is social engineering: how can one design a simple enough yet robust storage node that can be easily replicated, is attractive for universities and research projects to adopt, is easy to manage and can support the various patterns for large scale scientific analyses?\r\n\r\nMany universities have several of the necessary pieces for Data Intensive Science in place-- reasonably sized computing clusters, a few PB of storage and even a high-speed connection-- yet performing the analyses of data intensive science is very painful and slow. Data is never there when needed, large storage systems often fail despite having massive RAID configurations, and moving data from disk-to-disk at the full network speed still requires complex skills. The project offers a broad community buy-in through the Big Data Hubs, a unique combination of skills, facilities and science challenges to test, evaluate and deploy different hardware and software combinations that can be used in the design of a much larger, national-scale system. The goal is to design and run detailed benchmarks for various test science projects requiring different combinations of data transfer, data processing and massive compute, and use the results to design and build a low-cost, scalable petascale appliance including inexpensive hardware nodes and a simple software stack that can be replicated across many universities, supercomputer centers and large NSF facilities. The proposed system could become an enormous multiplier on the existing NSF investments in high end computing and fast networks. It could also accelerate the pace of standardization of data storage across the nation. The public, open data products, often discussed in the Data Management Plans at the end of NSF proposals could find an easy-to-use home. Various educational projects could simply rely upon a robust storage infrastructure with a simple API, and build a variety of delivery services for the educational community.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stanley",
   "pi_last_name": "Ahalt",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stanley Ahalt",
   "pi_email_addr": "ahalt@renci.org",
   "nsf_id": "000241232",
   "pi_start_date": "2018-06-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jay",
   "pi_last_name": "Aikat",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jay Aikat",
   "pi_email_addr": "ja@unc.edu",
   "nsf_id": "000561232",
   "pi_start_date": "2019-03-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Lea",
   "pi_last_name": "Shanley",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lea Shanley",
   "pi_email_addr": "lshanley@icsi.berkeley.edu",
   "nsf_id": "000729995",
   "pi_start_date": "2018-06-07",
   "pi_end_date": "2019-03-21"
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "The University of North Carolina at Chapel Hill",
  "perf_str_addr": "104 Airport Drive",
  "perf_city_name": "Chapel Hill",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275991350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "024Y00",
   "pgm_ele_name": "BD Spokes -Big Data Regional I"
  },
  {
   "pgm_ele_code": "807400",
   "pgm_ele_name": "EarthCube"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 431786.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-4d9adb62-7fff-4d27-44b4-7e8047bef377\">&nbsp;</span></p>\n<p dir=\"ltr\"><span>The Open Storage Network (OSN) created a distributed platform for data sharing and data reuse.&nbsp; The pilot arrived at a time when commercial cloud storage was cost prohibitive to medium and large research projects. The OSN platform attracted usage from research communities addressing natural disasters, dark matter, Earth Sciences, the Humanities, Computer Science, Materials Science, and more. Researchers reported satisfaction using the platform, and many of the teams integrated OSN into their research plan and utilized increased quotas. The OSN team represents a partnership between the nation?s Advanced Computing centers and leading innovators. Rallying around the common goal of low-cost, high performance, distributed storage, the team devised a technical methodology, processes, and approach for the most efficient operations possible, concentrated in the OSN Command Center. The five-site OSN deployment that went into production in December 2019 is operating and improving, and capacity is expanding through engagement with sites investing in deploying new OSN pods. The initial OSN deployment has matured into a production service with robust and scalable hardware, software, policy, and practices. OSN is a boon to computing on the network, particularly for the growing needs of mid-scale research, which focuses on collaboration and open data, but also needs access to storage cyberinfrastructure within limited resources. OSN provides a solution that helps to fill a gap in high performance and big data applications, building in performance and cost effectiveness that is competitive with or exceeds current solutions.</span></p>\n<p dir=\"ltr\"><span>Researchers received allocations directly from the OSN, and later also through XSEDE following the platform's inclusion as a Level 2 XSEDE service provider. The project provided a roadmap of how other storage resources may be included alongside computation in the NSF XSEDE and future ACCESS cyberinfrastructures. The testing and selection of a reliable hardware configuration proved useful to the broader community, and multiple institutions are now acquiring pods to be connected to the OSN. OSN Pods feature low cost per terabyte and high bandwidth capability in a single, well-integrated rack, supported with automated provisioning.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>In the last year of the award, the team conducted an assessment of related and companion projects. Results and lessons learned have been communicated through a series of webinars that covered the research drivers that motivated initial use of the OSN, national and international trends in large scale research storage, OSN outcomes to date, and visions for the future of data sharing and distributed storage in research. Accompanying the webinar series were three concept papers (see Attachments), highlighting research drivers, state of the art,&nbsp; synergistic projects, and OSN capabilities and features. Community input and engagement was high, and collaborators and colleagues agreed there is a need for greater investment in distributed storage infrastructure. It is inefficient for every medium to large research project to reinvent research cyberinfrastructure. Platforms like the OSN allow the project teams to focus on their domain research and to trust in a well-architected and maintained resource for storage and data sharing.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>OSN is poised for continued engagement with the scientific computing community.&nbsp; It is currently cooperatively managed by a distributed team that is bound by shared governance, with representation from the community. The project</span><span> </span><span>has used input from stakeholders to ensure uniform global behavior and continues to build out and implement flexible governance structures and functions that will allow the OSN to scale and operate efficiently as a distributed, readily accessible infrastructure. The OSN has established policies (see Attachments) that form a foundation for governance and operation. </span><span>This work has informed the policy work of other projects and organizations through team member participation in project working groups.</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span>The OSN is uniquely positioned to solve challenges encountered in mid-scale research. It is geographically distributed in national supercomputer and advanced computing centers near compute resources; it is connected to high-speed,low latency networks and it can support horizontal scaling by allowing simultaneous reads and writes to multiple OSN pods. Mid-scale researchers are already using the OSN to exploit the high-speed research network on which the OSN pods are strategically distributed to stream OSN data to their compute resources.&nbsp;</p>\n<p dir=\"ltr\"><span>Links to webinar recordings, concept papers, specifications, and other information can be found at http://www.openstoragenetwork.org</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/22/2021<br>\n\t\t\t\t\tModified by: Jay&nbsp;Aikat</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe Open Storage Network (OSN) created a distributed platform for data sharing and data reuse.  The pilot arrived at a time when commercial cloud storage was cost prohibitive to medium and large research projects. The OSN platform attracted usage from research communities addressing natural disasters, dark matter, Earth Sciences, the Humanities, Computer Science, Materials Science, and more. Researchers reported satisfaction using the platform, and many of the teams integrated OSN into their research plan and utilized increased quotas. The OSN team represents a partnership between the nation?s Advanced Computing centers and leading innovators. Rallying around the common goal of low-cost, high performance, distributed storage, the team devised a technical methodology, processes, and approach for the most efficient operations possible, concentrated in the OSN Command Center. The five-site OSN deployment that went into production in December 2019 is operating and improving, and capacity is expanding through engagement with sites investing in deploying new OSN pods. The initial OSN deployment has matured into a production service with robust and scalable hardware, software, policy, and practices. OSN is a boon to computing on the network, particularly for the growing needs of mid-scale research, which focuses on collaboration and open data, but also needs access to storage cyberinfrastructure within limited resources. OSN provides a solution that helps to fill a gap in high performance and big data applications, building in performance and cost effectiveness that is competitive with or exceeds current solutions.\nResearchers received allocations directly from the OSN, and later also through XSEDE following the platform's inclusion as a Level 2 XSEDE service provider. The project provided a roadmap of how other storage resources may be included alongside computation in the NSF XSEDE and future ACCESS cyberinfrastructures. The testing and selection of a reliable hardware configuration proved useful to the broader community, and multiple institutions are now acquiring pods to be connected to the OSN. OSN Pods feature low cost per terabyte and high bandwidth capability in a single, well-integrated rack, supported with automated provisioning. \n\n \nIn the last year of the award, the team conducted an assessment of related and companion projects. Results and lessons learned have been communicated through a series of webinars that covered the research drivers that motivated initial use of the OSN, national and international trends in large scale research storage, OSN outcomes to date, and visions for the future of data sharing and distributed storage in research. Accompanying the webinar series were three concept papers (see Attachments), highlighting research drivers, state of the art,  synergistic projects, and OSN capabilities and features. Community input and engagement was high, and collaborators and colleagues agreed there is a need for greater investment in distributed storage infrastructure. It is inefficient for every medium to large research project to reinvent research cyberinfrastructure. Platforms like the OSN allow the project teams to focus on their domain research and to trust in a well-architected and maintained resource for storage and data sharing.\n\n \nOSN is poised for continued engagement with the scientific computing community.  It is currently cooperatively managed by a distributed team that is bound by shared governance, with representation from the community. The project has used input from stakeholders to ensure uniform global behavior and continues to build out and implement flexible governance structures and functions that will allow the OSN to scale and operate efficiently as a distributed, readily accessible infrastructure. The OSN has established policies (see Attachments) that form a foundation for governance and operation. This work has informed the policy work of other projects and organizations through team member participation in project working groups.\n The OSN is uniquely positioned to solve challenges encountered in mid-scale research. It is geographically distributed in national supercomputer and advanced computing centers near compute resources; it is connected to high-speed,low latency networks and it can support horizontal scaling by allowing simultaneous reads and writes to multiple OSN pods. Mid-scale researchers are already using the OSN to exploit the high-speed research network on which the OSN pods are strategically distributed to stream OSN data to their compute resources. \nLinks to webinar recordings, concept papers, specifications, and other information can be found at http://www.openstoragenetwork.org\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 09/22/2021\n\n\t\t\t\t\tSubmitted by: Jay Aikat"
 }
}