{
 "awd_id": "1764033",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: RI: Medium: Collaborative Research: Understanding and Improving Optimization in Deep and Recurrent Networks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 328204.0,
 "awd_amount": 328204.0,
 "awd_min_amd_letter_date": "2018-06-18",
 "awd_max_amd_letter_date": "2018-06-18",
 "awd_abstract_narration": "Machine learning using deep neural networks has recently demonstrated broad empirical success. Despite this success, the optimization procedures that fit deep neural networks to data are still poorly understood. Besides playing a crucial role in fitting deep neural networks to data, optimization also strongly affects the model's ability to generalize from training examples to unseen data. This project will establish a working theory for why and when large artificial neural networks train and generalize well, and use this theory to develop new optimization methods. The utility of the new methods will be demonstrated in applications involving language, speech, biological sequences and other sequence data. The project will involve training of graduate and undergraduate students, and the project leaders will offer tutorials aimed at both the machine learning community, and other researchers and engineers using machine learning tools. \r\n\r\nIn order to establish a theory of why and when non-convex optimization works well when training deep networks, both empirical top-down and analytic bottom-up approaches will be pursued. The top-down approach will involve phenomenological analysis of large scale deep models used in practice, both when presented with real data, and when presented with data specifically crafted to test the behavior of the network. The bottom-up approach will involve precise analytic investigation from increasingly more complex models, starting with linear models, and non-convex matrix factorization, progressing through linear neural networks, models with a small number of hidden layers, and eventually reaching deeper and more complex networks. The theory developed aims to be both explanatory and actionable, and will be used to derive new optimization methods and modifications to architectures that aid in optimization and generalization. A particularly important testbed is the case of recurrent neural networks. Recurrent neural networks are powerful sequence models that maintain state as they process an input sequence and are used for sequence data. Particularly challenging to optimize, recurrent neural networks still leave much room for a stronger principled understanding, which the project aims to provide.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Moritz",
   "pi_last_name": "Hardt",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Moritz Hardt",
   "pi_email_addr": "hardt@eecs.berkeley.edu",
   "nsf_id": "000747429",
   "pi_start_date": "2018-06-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "722 Sutardja Dai Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947201776",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 328204.0
  }
 ],
 "por": null
}