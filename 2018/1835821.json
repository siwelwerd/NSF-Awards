{
 "awd_id": "1835821",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Elements: Software: NSCI: A high performance suite of SVD related solvers for machine learning",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rob Beverly",
 "awd_eff_date": "2019-01-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2018-09-06",
 "awd_max_amd_letter_date": "2022-02-23",
 "awd_abstract_narration": "The accrual of vast amounts of data is one of the defining characteristics of our century. With the help of computers, scientists use this data to make and test hypotheses, draw inferences, predict complex phenomena, and make educated policy decisions. Machine learning (ML) is an area in computer science that uses statistical methods to allow computers to \"learn\" from data, with and without human supervision. Central to the application of machine learning methods is the numerical computation of the Singular Value Decomposition (SVD) of matrices of very large dimension, often larger than a million or even a billion. Since \"off-the-shelf\" algorithms and SVD software, however, cannot handle matrices of very large dimension, iterative methods used in scientific computing are more appropriate. Yet their stringent approximation quality requirements are often excessive for downstream applications, and result in slow execution times. Recently, methods based on randomization have improved execution times, but their implementations relax the approximation quality, often to detrimental levels. This project proposes to develop a software package that unifies randomized and iterative methods with a particular focus on the specific requirements of various ML applications and with high performance optimizations for modern computing platforms. This will allow scientists to analyze significantly larger datasets, ML researchers to study large models that could not be tackled before, and ML service providers to use the new solvers to reduce their operational cost. \r\n\r\nThis project proposes to develop a software package that unifies randomized and iterative methods with a particular focus on the specific requirements of various ML applications and with high performance optimizations for modern computing platforms. This will allow scientists to analyze significantly larger datasets, ML researchers to study large models that could not be tackled before, and ML service providers to use the new solvers to reduce their operational cost. Specifically, the software package builds upon the state-of-the-art eigenvalue/singular value software package PRIMME that integrates cutting-edge iterative methods and high-performance implementations. The development of the package consists of two thrusts: (T1) Unifying state-of-the-art algorithmic techniques including randomized, streaming, and iterative methods, to deliver consistent experience for a diverse range of matrices with different quality requirements, hardware platforms and precisions, and programming environments. (T2) Developing software devices that enable downstream systems and SVD solvers to interoperate so that users can tune and customize solvers without being experts in numeric linear algebra.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andreas",
   "pi_last_name": "Stathopoulos",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andreas Stathopoulos",
   "pi_email_addr": "andreas@cs.wm.edu",
   "nsf_id": "000346145",
   "pi_start_date": "2018-09-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Zhenming",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhenming Liu",
   "pi_email_addr": "zliu20@wm.edu",
   "nsf_id": "000730307",
   "pi_start_date": "2018-09-06",
   "pi_end_date": "2022-02-23"
  }
 ],
 "inst": {
  "inst_name": "College of William and Mary",
  "inst_street_address": "1314 S MOUNT VERNON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "WILLIAMSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "7572213965",
  "inst_zip_code": "23185",
  "inst_country_name": "United States",
  "cong_dist_code": "08",
  "st_cong_dist_code": "VA08",
  "org_lgl_bus_name": "COLLEGE OF WILLIAM AND MARY",
  "org_prnt_uei_num": "EVWJPCY6AD97",
  "org_uei_num": "EVWJPCY6AD97"
 },
 "perf_inst": {
  "perf_inst_name": "College of William and Mary",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "231878795",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "VA01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "772600",
   "pgm_ele_name": "Data Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "077Z",
   "pgm_ref_txt": "CSSI-1: Cyberinfr for Sustained Scientif"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 600000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We live in the age of \"big data\", where enormous amounts of data are accumulated and need to be processed to guide intelligent decisions in fields as diverse as law, business, science, or engineering. This data is often represented in the form of a matrix from whch Machine Learning (ML) methods are used to extract useful information.&nbsp;</p>\n<p>The Singular Value Decomposition (SVD) is an important method in numerical mathematics that decomposes a matrix to each principle directions. Therefore, it provides a natural way to compress this data or to infer information about the most important parts of it. Computing the entire SVD for very large matrices, however, is not computationally feasible.&nbsp;</p>\n<p>The problem we address in this project is the solution of the partial SVD for applications in ML. We restrict our focus on iterative and randomized techniques which can scale to the large matrix dimensions needed in ML.</p>\n<p><span>Contrary to many scientific computing areas, ML applications have a wide variety of sophisticated requirements on the singular spaces used and thus iterative methods cannot often be used as black box. Tuning these SVD solvers requires understanding of three different dimensions:<br /> &nbsp; &nbsp; Dim.1: The algorithms behind these methods.<br /> &nbsp; &nbsp; Dim.2: The application requirements and their interaction with the methods.<br /> &nbsp; &nbsp; Dim.3: The implementations of all these in modern software libraries on HPC and distributed systems.</span></p>\n<div><span>With an interdisciplinary team of experts in numerical analysis, high performance computing, and ML, we have made significant contributions along and across all three dimensions.&nbsp;<br /><br /></span>We have identified and analyzed a set of stopping criteria for SVD iterative methods that not only stop early without wasting unnecessary iterations but also that check the metrics that users need rarther than generic numerical criteria. These are implemented on a variety of solvers, most prominently in PRIMME, a state-of-the-art SVD and eigenvalue package. Based on this support, PRIMME itself has undergone major improvements with new methods and software capability resulting in four releases and a significant increase of its user base.&nbsp;</div>\n<div></div>\n<div>A high performance computing, streaming SVD solver with adaptive criteria has also been developed on top of state-of-the-art numerical libraries. Streaming becomes crucial as the ability to store all the data wanes, but our need to extract trends and information increases. In addition, a new algorithm and software nears completion for the classical Ridge Regression method (a regularization method). &nbsp;This new algorithm automatically finds the optimal regularization parameter.&nbsp;</div>\n<div><br />We performed an extensive comparison against an important new class of methods based on randomization. We found that for both streaming and the regular SVD, randomization can give rough estimates very quickly, but classical iterative methods give consistently higher quality results.</div>\n<div></div>\n<div>We have developed a high performance computing software for an end-to-end hyperparameter optimization for Kernel Learning, both as an experimentation and as a production engine. It combines ML optimization techniques and numerical linear algebra software that solves the numerical problems. The implementation allows the use of distributed computing and massively parallel platforms. As a demonstration of capability we have solved one of the largest eigenvalue problems reported in the literature. Moreover, our hyperparameter optimization using boosting and more than 8000 compute cores of Stampede 2&nbsp;was able to improve on the accuracy of state-of-the-art ML solutions of huge problems.&nbsp;</div>\n<div></div>\n<div><span>\n<p>In the other direction, our work on kernel methods has also provided new intuition for theoretical ML. We introduced a framework, named additive influence model, that allows us to decouple the learning of high-dimensional interactions from the learning of non-linear feature interactions. Kernel-based techniques can then be used to learn the high-dimensional interactions with provable guarantees. Similarly, our adaptive criteria have shown improvements on various ML problems, most notably on matrix completion which is used in recommender systems (e.g., for recommendations on Amazon, Netflix, etc).</p>\n<p>This support has also spawned new directions and lines of research, both mathetical and high performance computing, which we will continue to explore.</p>\n<div><span><br /></span></div>\n<br /></span></div>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/30/2024<br>\nModified by: Andreas&nbsp;Stathopoulos</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nWe live in the age of \"big data\", where enormous amounts of data are accumulated and need to be processed to guide intelligent decisions in fields as diverse as law, business, science, or engineering. This data is often represented in the form of a matrix from whch Machine Learning (ML) methods are used to extract useful information.\n\n\nThe Singular Value Decomposition (SVD) is an important method in numerical mathematics that decomposes a matrix to each principle directions. Therefore, it provides a natural way to compress this data or to infer information about the most important parts of it. Computing the entire SVD for very large matrices, however, is not computationally feasible.\n\n\nThe problem we address in this project is the solution of the partial SVD for applications in ML. We restrict our focus on iterative and randomized techniques which can scale to the large matrix dimensions needed in ML.\n\n\nContrary to many scientific computing areas, ML applications have a wide variety of sophisticated requirements on the singular spaces used and thus iterative methods cannot often be used as black box. Tuning these SVD solvers requires understanding of three different dimensions:\n   Dim.1: The algorithms behind these methods.\n   Dim.2: The application requirements and their interaction with the methods.\n   Dim.3: The implementations of all these in modern software libraries on HPC and distributed systems.\nWith an interdisciplinary team of experts in numerical analysis, high performance computing, and ML, we have made significant contributions along and across all three dimensions.\n\nWe have identified and analyzed a set of stopping criteria for SVD iterative methods that not only stop early without wasting unnecessary iterations but also that check the metrics that users need rarther than generic numerical criteria. These are implemented on a variety of solvers, most prominently in PRIMME, a state-of-the-art SVD and eigenvalue package. Based on this support, PRIMME itself has undergone major improvements with new methods and software capability resulting in four releases and a significant increase of its user base.\n\nA high performance computing, streaming SVD solver with adaptive criteria has also been developed on top of state-of-the-art numerical libraries. Streaming becomes crucial as the ability to store all the data wanes, but our need to extract trends and information increases. In addition, a new algorithm and software nears completion for the classical Ridge Regression method (a regularization method). This new algorithm automatically finds the optimal regularization parameter.\n\nWe performed an extensive comparison against an important new class of methods based on randomization. We found that for both streaming and the regular SVD, randomization can give rough estimates very quickly, but classical iterative methods give consistently higher quality results.\n\nWe have developed a high performance computing software for an end-to-end hyperparameter optimization for Kernel Learning, both as an experimentation and as a production engine. It combines ML optimization techniques and numerical linear algebra software that solves the numerical problems. The implementation allows the use of distributed computing and massively parallel platforms. As a demonstration of capability we have solved one of the largest eigenvalue problems reported in the literature. Moreover, our hyperparameter optimization using boosting and more than 8000 compute cores of Stampede 2was able to improve on the accuracy of state-of-the-art ML solutions of huge problems.\n\n\n\n\nIn the other direction, our work on kernel methods has also provided new intuition for theoretical ML. We introduced a framework, named additive influence model, that allows us to decouple the learning of high-dimensional interactions from the learning of non-linear feature interactions. Kernel-based techniques can then be used to learn the high-dimensional interactions with provable guarantees. Similarly, our adaptive criteria have shown improvements on various ML problems, most notably on matrix completion which is used in recommender systems (e.g., for recommendations on Amazon, Netflix, etc).\n\n\nThis support has also spawned new directions and lines of research, both mathetical and high performance computing, which we will continue to explore.\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 04/30/2024\n\n\t\t\t\t\tSubmitted by: AndreasStathopoulos\n"
 }
}