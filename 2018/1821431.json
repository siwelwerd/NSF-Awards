{
 "awd_id": "1821431",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Collaborative Research: Coupling Computation and Communication in FPGA-Enhanced Clouds and Clusters",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2017-10-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 178192.0,
 "awd_amount": 218192.0,
 "awd_min_amd_letter_date": "2017-12-20",
 "awd_max_amd_letter_date": "2020-05-22",
 "awd_abstract_narration": "The introduction of Field Programmable Gate Arrays (FPGAs) to accelerate clusters of servers in datacenters and clouds provides a great, immediate opportunity to leverage a new technology in high-end computing. With their flexible logic and native massive communication capability, FPGAs are ideal for high-performance computing in the post-Moore?s Law world. Since the hardware adapts to the application higher efficiency can be achieved, and since FPGAs are hybrid communication/computation processors, they can be interconnected directly chip-to-chip. Large-scale communication can consequently proceed with both higher bandwidth, lower latency, and less processor impact. These features are crucial to enhancing performance beyond current levels. The proposed design allows for useful processing while data is in flight in the network resulting in reduced software overhead in parallel middleware and reduced network congestion. The key tenets of the research are to achieve programmable, intelligent acceleration of applications while emphasizing overlap of communication and computation at low latency, while also cutting substantially software overhead. \r\nThe research project, FC5 (an FPGA framework for coupling communication and computation in clouds and clusters) has several thrusts. First, hardware support for FC5 and investigation of methods of configurability in FC5 to reduce communication latency and support computing in the network are studied. A second outcome is a prototype version of the Open MPI open source version of MPI-3.1 parallel middleware that utilizes FC5 to deliver the features and performance enhancements involving data movement between and within servers, mathematical data reductions, and bulk data reorganizations. Third, proof-of-concept versions of multiple FC5 software models, including direct hardware access, a transparent MPI-in-OpenCL, and an API-based mechanism that exposes essential functionality. Finally, because FC5 is evolving rapidly with major new announcements expected imminently, continued refinement is essential. At least two model applications, Molecular Dynamics and Map-Reduce, will be used as test cases. \r\nWith the continued consolidation of computing services into the cloud, the potential broader impact is to increase both the scale and availability of parallel applications. The broad range of uses of cloud and cluster computing for commercial, government, and academic applications means that acceleration offered will have a widespread impact applicable across many sectors. The growing acceptance of high performance computing in industry (e.g., fast machine learning) is one particular potential commercial sector that will be enhanced by this project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anthony",
   "pi_last_name": "Skjellum",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anthony Skjellum",
   "pi_email_addr": "askjellum@tntech.edu",
   "nsf_id": "000446874",
   "pi_start_date": "2017-12-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Chattanooga",
  "inst_street_address": "615 MCCALLIE AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHATTANOOGA",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "4234254431",
  "inst_zip_code": "374032504",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "TN03",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "RZ1YV5AUBN39",
  "org_uei_num": "JNZFHMGJN7M3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Chattanooga",
  "perf_str_addr": "615 McCallie Ave.",
  "perf_city_name": "Chattanooga",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "374032504",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "TN03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 178192.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has introduced Field Programmable Gate arrays (FPGAs) as a first-class accelerator technology for multicore CPU server nodes in clusters and supercomputers, utilized for peer-based parallel programs based on the message passing parallel programming model, as defined by the Message Passing Interface ad hoc standard, which is ubiqutously used worldwide.&nbsp; Generally, the program model is now called MPI+X (where X may multicore concurrency on node, a GPU accelerator, or an FPGA, among others).</p>\n<p>Understanding of how to manage heterogeneity of CPUs+FPGAs for offload of operations that are better implemented on FPGAs has been shown to accelerate I) basic operations in parallel programs, II) enhance the behavior of MPI middleware itself, such as dealing with non-continguous data to be sent or received, and III) the potential for offload operations into an FPGA-enabled network that otherwise would be implemented in software in a the multicore CPUs.</p>\n<p>This work provides a complement to ubiquitous studies and production utilization of CPUs+GPUs for parallel programming on small clusters through the largest supercomputers of today.&nbsp; It also complements the pure network offload activities of proprietary networks such as Mellanox' InfiniBand SHARP.</p>\n<p>Broader impacts of this work include technical and societal:</p>\n<p>1) learning, publications, prototypes, and insights into MPI+X programming;</p>\n<p>2) support for, training, and advancement of several graduate students, at Boston University, University of Tennessee, and at Auburn University.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/29/2021<br>\n\t\t\t\t\tModified by: Anthony&nbsp;Skjellum</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project has introduced Field Programmable Gate arrays (FPGAs) as a first-class accelerator technology for multicore CPU server nodes in clusters and supercomputers, utilized for peer-based parallel programs based on the message passing parallel programming model, as defined by the Message Passing Interface ad hoc standard, which is ubiqutously used worldwide.  Generally, the program model is now called MPI+X (where X may multicore concurrency on node, a GPU accelerator, or an FPGA, among others).\n\nUnderstanding of how to manage heterogeneity of CPUs+FPGAs for offload of operations that are better implemented on FPGAs has been shown to accelerate I) basic operations in parallel programs, II) enhance the behavior of MPI middleware itself, such as dealing with non-continguous data to be sent or received, and III) the potential for offload operations into an FPGA-enabled network that otherwise would be implemented in software in a the multicore CPUs.\n\nThis work provides a complement to ubiquitous studies and production utilization of CPUs+GPUs for parallel programming on small clusters through the largest supercomputers of today.  It also complements the pure network offload activities of proprietary networks such as Mellanox' InfiniBand SHARP.\n\nBroader impacts of this work include technical and societal:\n\n1) learning, publications, prototypes, and insights into MPI+X programming;\n\n2) support for, training, and advancement of several graduate students, at Boston University, University of Tennessee, and at Auburn University.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 09/29/2021\n\n\t\t\t\t\tSubmitted by: Anthony Skjellum"
 }
}