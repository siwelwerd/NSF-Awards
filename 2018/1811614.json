{
 "awd_id": "1811614",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Properties of Approximate Inference for Complex High-Dimensional Models",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2018-05-07",
 "awd_max_amd_letter_date": "2021-07-24",
 "awd_abstract_narration": "Modern scientific data acquisition often creates datasets with many measurements on a large sample size of individuals.  The structure of interest in the data may be low dimensional or sparse. Examples arise in scientific domains from econometrics to genomics and image and signal processing and beyond.  The project explores approximate methods of statistical inference for such structure in a representative set of contemporary settings: high dimensional estimation, generalized linear mixed models and low rank multivariate models. It aims to develop approximate methods backed by an optimality theory and/or performance guarantees.  The work is expected to provide new theoretical insights into current problems in specific application domains such as Magnetic Resonance Imaging and quantitative genetics.\r\n\r\nThe project will bring ideas from classical decision theory to compressed sensing and robust linear modeling, rigorously solving nonconvex optimization problems and obtaining reconstruction performance rigorously better than traditional convex optimization methods.  It will exploit decision theoretic ideas in the proposer's previous work to provide new theoretical insights into a pressing practical problem in compressed sensing -- deriving optimal variable-density sampling schedules applicable to Magnetic Resonance Imaging and NMR spectroscopy.  The project will also study theoretically the statistical performance of some methods for deterministic approximate inference popular in machine learning, but for which little attention has been given to properties such as consistency, asymptotic normality and efficiency.  The study will begin with concrete examples in the realm of generalized linear mixed models, from a frequentist perspective, and seek to establish first-of-kind results for asymptotic efficiency of Expectation Propagation.  Finally, the project will study approximate inference for the eigenstructure of highly multivariate models with low dimensional structure.  It will adapt James' classical framework for multivariate analysis to a broad class of multispike models.  Through collaboration with quantitative geneticists, it will develop methods for inference for low dimensional structure in high dimensional genetic covariance matrices.  In both cases, methods from random matrix theory will be essential.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Iain",
   "pi_last_name": "Johnstone",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Iain M Johnstone",
   "pi_email_addr": "imj@stanford.edu",
   "nsf_id": "000155839",
   "pi_start_date": "2018-05-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Donoho",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "David L Donoho",
   "pi_email_addr": "donoho@stat.stanford.edu",
   "nsf_id": "000367778",
   "pi_start_date": "2018-05-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "390 Serra Mall",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943054020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 148390.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 149202.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 150816.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 151592.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project made a number of significant contributions to the study of approximate inference for complex high-dimensional models. Among these we highlight the following four:<em> <br /></em></p>\r\n<p><em>Expectation propagation and maximum likelihood in generalized linear mixed models. &nbsp;&nbsp;</em>We studied a class of generalized linear mixed models in which both the number of groups and the number of observations within each group are large, and in which usual likelihood analysis encounters both computational and technical challenges. Matt Wand and colleagues have adapted the machine learning technique of expectation propagation (EP)&nbsp; to yield state-of-the-art estimation of parameters in such models.&nbsp; Here we ask: are the EP estimators asymptotically efficient?&nbsp; A main challenge is to define an appropriate objective function that captures the EP iteration and approximates maximum likelihood well enough to inherit its efficiency.&nbsp;&nbsp; A second issue is to show that maximum likelihood actually is<em> </em>efficient, due to integrals over random effects in the likelihood. For this we propose a novel method based on classical complex analysis. This was joint work of the PI with a group including the late Peter Hall, Matt Wand, Song Mei and Apratim Dey.<em>&nbsp;</em></p>\r\n<p><em>Prevalence of neural collapse during the terminal phase of deep learning training.&nbsp; </em>Modern deep neural networks for image classification have achieved superhuman performance. Yet, the complex details of trained networks have forced most practitioners and researchers to regard them as black boxes with little that could be understood. This paper considers in detail a now-standard training methodology: driving the cross-entropy loss to zero, continuing long after the classification error is already zero. Applying this methodology to an authoritative collection of standard deepnets and datasets, we observe the emergence of a simple and highly symmetric geometry of the deepnet features and of the deepnet classifier, and we document important benefits that the geometry conveys -- thereby helping us understand an important component of the modern deep learning training paradigm. This was joint work of the co-PI with Vardan Papyan and X.Y. Han. <em>&nbsp;</em></p>\r\n<p><em>ScreeNOT: Exact MSE-optimal singular value thresholding In correlated noise.&nbsp; </em>The paper derives a formula for optimal hard thresholding of the singular value<em> </em>decomposition in the presence of correlated additive noise; although it nominally<em> </em>involves unobservables, we show how to apply it even where the noise covariance structure is not a priori known or is not independently estimable. The proposed method, which we call ScreeNOT, is a mathematically solid alternative to Cattell's ever-popular but vague scree plot heuristic from 1966. ScreeNOT has a surprising oracle property: it typically achieves exactly, in large finite samples, the lowest possible MSE for matrix recovery, on each given problem instance, that is, the specific threshold it selects gives exactly the smallest achievable MSE loss among all possible threshold choices for<em> </em>that noisy data set and that unknown underlying true low rank model. The method is computationally efficient and robust against perturbations of the underlying covariance structure. Our results depend on the assumption that the singular values of the noise have a limiting empirical distribution of compact support; this property, which is standard in random matrix theory, is satisfied by many models exhibiting either cross-row correlation structure or cross-column correlation structure, and also by many situations with more<em> </em>general, interelement correlation structure. Simulations demonstrate the effectiveness<em> </em>of the method even at moderate matrix sizes. The paper is supplemented by ready-to-use software packages implementing the proposed algorithm in Python and R. The paper was joint work of the CoPI with Matan Gavish and Elad Romanov.<em>&nbsp;</em></p>\r\n<p><em>Spin Glass to Paramagnetic Transition and Triple Point in Spherical SK Model.&nbsp; </em>Motivated by hypothesis testing problems in complex high-dimensional statistical models, this paper studies spin glass to paramagnetic transition in the Spherical Sherrington-Kirkpatrick model with ferromagnetic Curie-Weiss interaction with coupling constant and inverse temperature parameters. &nbsp;The disorder of the system is represented by a general Wigner matrix. We confirm a conjecture of Baik and Lee concerning the critical window of temperatures for this transition. The limiting distribution of the scaled free energy is Gaussian for negative parameter regime<em> </em>and a weighted linear combination of independent Gaussian and Tracy-Widom components for the positive regime. In the special case where the Wigner matrix is from the Gaussian Orthogonal or Unitary Ensemble, we describe the triple point transition between spin glass, paramagnetic, and ferromagnetic regimes in a critical window around the triple point : the Tracy-Widom component is replaced by the one parameter family of deformations previously described by Bloemendal and Virag. This was joint work of the PI with Yegor Klochkov, Alexei Onatski, and Damian Pavlyshyn.<em>&nbsp;</em></p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/14/2025<br>\nModified by: Iain&nbsp;M&nbsp;Johnstone</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project made a number of significant contributions to the study of approximate inference for complex high-dimensional models. Among these we highlight the following four: \n\r\n\n\nExpectation propagation and maximum likelihood in generalized linear mixed models. We studied a class of generalized linear mixed models in which both the number of groups and the number of observations within each group are large, and in which usual likelihood analysis encounters both computational and technical challenges. Matt Wand and colleagues have adapted the machine learning technique of expectation propagation (EP) to yield state-of-the-art estimation of parameters in such models. Here we ask: are the EP estimators asymptotically efficient? A main challenge is to define an appropriate objective function that captures the EP iteration and approximates maximum likelihood well enough to inherit its efficiency. A second issue is to show that maximum likelihood actually is efficient, due to integrals over random effects in the likelihood. For this we propose a novel method based on classical complex analysis. This was joint work of the PI with a group including the late Peter Hall, Matt Wand, Song Mei and Apratim Dey.\r\n\n\nPrevalence of neural collapse during the terminal phase of deep learning training. Modern deep neural networks for image classification have achieved superhuman performance. Yet, the complex details of trained networks have forced most practitioners and researchers to regard them as black boxes with little that could be understood. This paper considers in detail a now-standard training methodology: driving the cross-entropy loss to zero, continuing long after the classification error is already zero. Applying this methodology to an authoritative collection of standard deepnets and datasets, we observe the emergence of a simple and highly symmetric geometry of the deepnet features and of the deepnet classifier, and we document important benefits that the geometry conveys -- thereby helping us understand an important component of the modern deep learning training paradigm. This was joint work of the co-PI with Vardan Papyan and X.Y. Han. \r\n\n\nScreeNOT: Exact MSE-optimal singular value thresholding In correlated noise. The paper derives a formula for optimal hard thresholding of the singular value decomposition in the presence of correlated additive noise; although it nominally involves unobservables, we show how to apply it even where the noise covariance structure is not a priori known or is not independently estimable. The proposed method, which we call ScreeNOT, is a mathematically solid alternative to Cattell's ever-popular but vague scree plot heuristic from 1966. ScreeNOT has a surprising oracle property: it typically achieves exactly, in large finite samples, the lowest possible MSE for matrix recovery, on each given problem instance, that is, the specific threshold it selects gives exactly the smallest achievable MSE loss among all possible threshold choices for that noisy data set and that unknown underlying true low rank model. The method is computationally efficient and robust against perturbations of the underlying covariance structure. Our results depend on the assumption that the singular values of the noise have a limiting empirical distribution of compact support; this property, which is standard in random matrix theory, is satisfied by many models exhibiting either cross-row correlation structure or cross-column correlation structure, and also by many situations with more general, interelement correlation structure. Simulations demonstrate the effectiveness of the method even at moderate matrix sizes. The paper is supplemented by ready-to-use software packages implementing the proposed algorithm in Python and R. The paper was joint work of the CoPI with Matan Gavish and Elad Romanov.\r\n\n\nSpin Glass to Paramagnetic Transition and Triple Point in Spherical SK Model. Motivated by hypothesis testing problems in complex high-dimensional statistical models, this paper studies spin glass to paramagnetic transition in the Spherical Sherrington-Kirkpatrick model with ferromagnetic Curie-Weiss interaction with coupling constant and inverse temperature parameters. The disorder of the system is represented by a general Wigner matrix. We confirm a conjecture of Baik and Lee concerning the critical window of temperatures for this transition. The limiting distribution of the scaled free energy is Gaussian for negative parameter regime and a weighted linear combination of independent Gaussian and Tracy-Widom components for the positive regime. In the special case where the Wigner matrix is from the Gaussian Orthogonal or Unitary Ensemble, we describe the triple point transition between spin glass, paramagnetic, and ferromagnetic regimes in a critical window around the triple point : the Tracy-Widom component is replaced by the one parameter family of deformations previously described by Bloemendal and Virag. This was joint work of the PI with Yegor Klochkov, Alexei Onatski, and Damian Pavlyshyn.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 02/14/2025\n\n\t\t\t\t\tSubmitted by: IainMJohnstone\n"
 }
}