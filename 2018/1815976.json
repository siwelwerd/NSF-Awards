{
 "awd_id": "1815976",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Enhancing Educational Virtual Reality with Headset-based Eye Tracking",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 499813.0,
 "awd_amount": 531813.0,
 "awd_min_amd_letter_date": "2018-07-31",
 "awd_max_amd_letter_date": "2021-04-22",
 "awd_abstract_narration": "Virtual reality (VR) can bring lab or field-trip-like experiences to students who are unable to visit physical sites because of location, budget, or schedule.  Potential advantages of these experiences over traditional teaching tools include increased student engagement and motivation, more direct viewing of size and spatial relationships of modeled objects, and stronger memories of the content.  Emerging consumer VR devices are starting to provide sufficient quality and affordability for home and school use, and this will eventually make educational VR experiences broadly available.  Future consumer VR headsets are expected to include increased sensing, such as eye tracking cameras to determine where users are looking and strain gauges to detect facial expressions. The sensor data can be analyzed for insight into users' attention and emotional affect.  The project will investigate how such insight into student attention can be used to improve educational VR through the design of personalized educational environments that respond to individual students' attention. The project will also develop techniques for using sensor data to give teachers enhanced real-time insight into student activities and behavior patterns to help them provide better teacher-guided VR experiences.  This will involve development of new approaches for educational VR technology and experiments that generate fundamental knowledge and guidelines for applying such approaches.  In addition to the potential long-term benefit of improving education, the project will provide a number of more immediate, direct educational benefits.  The team will incorporate the work into courses and undergraduate research experiences on human-computer interaction and VR, as well as outreach activities and summer programs aimed at high school students across Louisiana. \r\n\r\nThe team will design and assess methods including the following: 1) educational content that responds to student eye gaze for more responsive and engaging presentation; 2) visual effects or indicators, based on detected eye behaviors, to encourage student attention to particular content in a VR environment; and 3) visualizations of student eye gaze that use both raw and processed gaze data to help teachers understand and guide students. To understand the tradeoffs between approaches and to develop guidelines for wider development and use of these techniques, effects will be studied in terms of behavior, subjective experience, and learning. The most promising methods will be applied to a case study of a networked VR interface that allows teachers to monitor and guide students through an immersive educational VR environment. To do this, the team will build on their existing educational VR framework that has previously been deployed at regional high schools and to thousands of students at outreach events. The project is expected to improve the effectiveness of such VR systems and of teachers' ability to supervise and assist students.  Resulting methods and principles will provide a foundation for headset-based eye tracking in educational VR and in other related applications such as simulation-based training and accessibility.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christoph",
   "pi_last_name": "Borst",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Christoph W Borst",
   "pi_email_addr": "cwborst@gmail.com",
   "nsf_id": "000305310",
   "pi_start_date": "2018-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Arun",
   "pi_last_name": "Kulshreshth",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Arun K Kulshreshth",
   "pi_email_addr": "arunkul@louisiana.edu",
   "nsf_id": "000738217",
   "pi_start_date": "2018-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Louisiana at Lafayette",
  "inst_street_address": "104 E UNIVERSITY AVE",
  "inst_street_address_2": "",
  "inst_city_name": "LAFAYETTE",
  "inst_state_code": "LA",
  "inst_state_name": "Louisiana",
  "inst_phone_num": "3374825811",
  "inst_zip_code": "705032014",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "LA03",
  "org_lgl_bus_name": "UNIVERSITY OF LOUISIANA AT LAFAYETTE",
  "org_prnt_uei_num": "C169K7T4QZ96",
  "org_uei_num": "C169K7T4QZ96"
 },
 "perf_inst": {
  "perf_inst_name": "University of Louisiana at Lafayette",
  "perf_str_addr": "104 University Circle",
  "perf_city_name": "Lafayette",
  "perf_st_code": "LA",
  "perf_st_name": "Louisiana",
  "perf_zip_code": "705032701",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "LA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499813.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project produced techniques to improve VR-based education and training by using data from head-mounted sensors to present additional information to teachers, to automated presentation systems, or to students in VR. The techniques primarily used eye trackers that sense eye movements in recent headset designs, and sometimes considered other sensing such as heart rate and EEG signals. Experiments investigated the benefits and tradeoffs of techniques, to discover principles and guidelines for effective integration into future educational VR systems.</p>\n<p><em>Visual cues to students</em>: We studied 9 visual cues in VR to assess their ability to guide attention and to restore attention after a short distraction. For example, when a student looks away from an object being pointed out by a teacher, an arrow may appear, moving based on eye gaze, to suggest attention to the object. We studied a broader range of cues and tasks than prior studies. Results showed substantially different performance patterns than prior studies, hinging on the task type (guidance vs. restoration). Later, the three most promising cues were deployed in a VR-based \"field trip\" at high schools to test results in a real-world setting. This provided insight into student responses to such cues and into differences between lab results and in-school results.</p>\n<p><em>Gaze-responsive content for students</em>: We developed a system that adjusts the presentation of educational clips (VR teacher animations) when a student&rsquo;s eye gaze indicates a distraction or misunderstanding about what is being pointed out. For example, playback can pause and provide additional spoken guidance until a student looks at a critical object. A study showed that users consider adjusted teacher behaviors that have increased interactivity to be more appropriate, more natural, and less strange than default behaviors.</p>\n<p><em>Visuals and interfaces for teachers</em>: We studied different ways to present student gaze and other information about students to help a teacher understand and guide students who are in VR. For example, the teacher may see a glowing dot at any location viewed by a student, and a central dashboard may combine gaze-related information with notifications about student actions like hand raising. An experiment on several direct gaze indicators showed their performance and discussed tradeoffs. Evaluations of different information displays validated a central dashboard and showed preferences for certain presentation styles over others. Elements of these information displays were also incorporated into novel \"cross-reality\" interfaces designed for a teacher to guide VR-immersed students without requiring the teacher to use a headset. This may be important for teacher comfort across multiple VR sessions or to maintain oversight of a real room. Desktop-based eye and motion sensors were used to capture teacher behaviors and reflect them on an avatar in VR. Studies of four interface styles revealed their tradeoffs for students and teachers. &nbsp;</p>\n<p><em>AI-Based approaches to detect distracted students</em>: We developed AI-based approaches to estimate distraction level from student eye-gaze data in VR. Techniques for detecting distraction can help an educational system automatically adjust to student difficulties or can simplify the information presented to a teacher by prioritizing visual cues for selective display. This could make it easier for a teacher to understand and adjust to audience attention. Experiments assessed the AI models by adding controlled distractions into virtual field trips and gathering data about eye gaze to train and test AI systems. EEG sensor data was also considered in the most recent study. The AI systems were able to detect distraction within the studied student group and VR environment, and further research is needed to improve the generalizability across different educational sessions and students.</p>\n<p><em>Other research</em>: Alongside the main results above, the research produced other results including: 1) a study of a class that met in VR for several weeks, for insight into areas of improvement for teachers and students, 2) a validated set of emotion-inducing tasks relevant to training, for gathering data about physiological responses, 3) a related interface for rating emotions, with an experiment comparing alternatives, 4) studies of identifiability of users from collected eye tracking data (by anonymized ID), for understanding capabilities and privacy risks, and 5) development and evaluation of two specialized interaction techniques needed in the educational VR environments: one for a teacher to point out objects to students, and one for students to rapidly retrieve VR objects that move out-of-range during manipulation tasks.</p>\n<p><em>Education and Outreach</em>: This project supported and produced several graduates at the Ph.D., M.S., and B.S. levels. They gained expertise on VR systems, sensors, and experimental research. Additionally, high school students and teachers were exposed to educational VR and experimental techniques. Techniques and applications were demonstrated at events including international conferences, regional annual career fairs, and college recruiting events to inform and motivate young people about VR and applications with societal benefit.</p>\n<p>Project results provide a foundation for using headset-based eye tracking in VR for education, training, and other applications.</p><br>\n<p>\n Last Modified: 11/29/2023<br>\nModified by: Christoph&nbsp;W&nbsp;Borst</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project produced techniques to improve VR-based education and training by using data from head-mounted sensors to present additional information to teachers, to automated presentation systems, or to students in VR. The techniques primarily used eye trackers that sense eye movements in recent headset designs, and sometimes considered other sensing such as heart rate and EEG signals. Experiments investigated the benefits and tradeoffs of techniques, to discover principles and guidelines for effective integration into future educational VR systems.\n\n\nVisual cues to students: We studied 9 visual cues in VR to assess their ability to guide attention and to restore attention after a short distraction. For example, when a student looks away from an object being pointed out by a teacher, an arrow may appear, moving based on eye gaze, to suggest attention to the object. We studied a broader range of cues and tasks than prior studies. Results showed substantially different performance patterns than prior studies, hinging on the task type (guidance vs. restoration). Later, the three most promising cues were deployed in a VR-based \"field trip\" at high schools to test results in a real-world setting. This provided insight into student responses to such cues and into differences between lab results and in-school results.\n\n\nGaze-responsive content for students: We developed a system that adjusts the presentation of educational clips (VR teacher animations) when a students eye gaze indicates a distraction or misunderstanding about what is being pointed out. For example, playback can pause and provide additional spoken guidance until a student looks at a critical object. A study showed that users consider adjusted teacher behaviors that have increased interactivity to be more appropriate, more natural, and less strange than default behaviors.\n\n\nVisuals and interfaces for teachers: We studied different ways to present student gaze and other information about students to help a teacher understand and guide students who are in VR. For example, the teacher may see a glowing dot at any location viewed by a student, and a central dashboard may combine gaze-related information with notifications about student actions like hand raising. An experiment on several direct gaze indicators showed their performance and discussed tradeoffs. Evaluations of different information displays validated a central dashboard and showed preferences for certain presentation styles over others. Elements of these information displays were also incorporated into novel \"cross-reality\" interfaces designed for a teacher to guide VR-immersed students without requiring the teacher to use a headset. This may be important for teacher comfort across multiple VR sessions or to maintain oversight of a real room. Desktop-based eye and motion sensors were used to capture teacher behaviors and reflect them on an avatar in VR. Studies of four interface styles revealed their tradeoffs for students and teachers. \n\n\nAI-Based approaches to detect distracted students: We developed AI-based approaches to estimate distraction level from student eye-gaze data in VR. Techniques for detecting distraction can help an educational system automatically adjust to student difficulties or can simplify the information presented to a teacher by prioritizing visual cues for selective display. This could make it easier for a teacher to understand and adjust to audience attention. Experiments assessed the AI models by adding controlled distractions into virtual field trips and gathering data about eye gaze to train and test AI systems. EEG sensor data was also considered in the most recent study. The AI systems were able to detect distraction within the studied student group and VR environment, and further research is needed to improve the generalizability across different educational sessions and students.\n\n\nOther research: Alongside the main results above, the research produced other results including: 1) a study of a class that met in VR for several weeks, for insight into areas of improvement for teachers and students, 2) a validated set of emotion-inducing tasks relevant to training, for gathering data about physiological responses, 3) a related interface for rating emotions, with an experiment comparing alternatives, 4) studies of identifiability of users from collected eye tracking data (by anonymized ID), for understanding capabilities and privacy risks, and 5) development and evaluation of two specialized interaction techniques needed in the educational VR environments: one for a teacher to point out objects to students, and one for students to rapidly retrieve VR objects that move out-of-range during manipulation tasks.\n\n\nEducation and Outreach: This project supported and produced several graduates at the Ph.D., M.S., and B.S. levels. They gained expertise on VR systems, sensors, and experimental research. Additionally, high school students and teachers were exposed to educational VR and experimental techniques. Techniques and applications were demonstrated at events including international conferences, regional annual career fairs, and college recruiting events to inform and motivate young people about VR and applications with societal benefit.\n\n\nProject results provide a foundation for using headset-based eye tracking in VR for education, training, and other applications.\t\t\t\t\tLast Modified: 11/29/2023\n\n\t\t\t\t\tSubmitted by: ChristophWBorst\n"
 }
}