{
 "awd_id": "1749397",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Multimethod Investigation of Articulatory and Perceptual Constraints on Natural Language Evolution",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927920",
 "po_email": "jvaldesk@nsf.gov",
 "po_sign_block_name": "Jorge Valdes Kroff",
 "awd_eff_date": "2018-05-15",
 "awd_exp_date": "2022-10-31",
 "tot_intn_awd_amt": 148025.0,
 "awd_amount": 160025.0,
 "awd_min_amd_letter_date": "2018-05-17",
 "awd_max_amd_letter_date": "2021-07-15",
 "awd_abstract_narration": "Languages change over time, such that the way we speak English now is very different than the speech patterns of elder generations and our distant ancestors. This project will exploit the visual nature of sign languages--where the body parts producing language are highly visible--to determine whether languages change so that they are easier to produce or so that they are easier to understand. In doing so, the project will address fundamental theoretical questions about language change that cannot be addressed by analyzing historical samples of spoken languages. To this end, the researchers will develop computational tools that allow 3D human body poses to be automatically extracted from 2D video. Such tools will be useful for the development of automated sign language recognition, promoting accessibility for deaf and hard-of-hearing people, and for developing automated systems for recognizing and classifying human gestures. The research will involve deaf and hard-of-hearing students, helping to increase diversity in the nation's scientific workforce.\r\n\r\nIt is well documented that sign languages change over time, and it is a commonly held belief that those changes have resulted from successive generations making language easier to perceive. However, most of this evidence has been anecdotal and descriptive and has not quantified changes in the ease of perception and production of ASL over time. The research team will take advantage of the fully visible articulators of sign languages to develop novel pose estimation algorithms that are able to automatically extract information contained in 2D video to create accurate 3D models of articulator movement during language production. The recent birth and rapid evolution of Nicaraguan Sign Language (NSL) has allowed researchers to study language change, from the beginning, on a compressed time-scale. By leveraging an existing NSL database - comprised of 2D videos from four generations of Nicaraguan signers - and utilizing these novel pose estimation algorithms, the researchers will be able to empirically assess the extent to which linguistic changes are driven by perceptual constraints imposed by the human visual system and/or articulatory constraints imposed by the musculoskeletal system. The researchers will also query lexical databases of American Sign Language to test predictions about the perceptual form of modern day ASL, and conduct behavioral studies with deaf and hearing users of ASL to test hypotheses regarding the allocation of visual attention as a result of both deafness and acquisition of a sign language. In doing so, the research will provide valuable information about how the human brain changes the tools we use (in this case, language) and the way that those tools in turn shape the function of the human brain. This will provide a more complex understanding of language change that illuminates the complex interaction between languages and the human beings that use them.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Norman",
   "pi_last_name": "Badler",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Norman I Badler",
   "pi_email_addr": "badler@central.cis.upenn.edu",
   "nsf_id": "000452500",
   "pi_start_date": "2018-05-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": "3451 Walnut Street",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046205",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "102Z",
   "pgm_ref_txt": "COVID-Disproportionate Impcts Inst-Indiv"
  },
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "1V21",
   "app_name": "R&RA ARP Act DEFC V",
   "app_symb_id": "040100",
   "fund_code": "010V2122DB",
   "fund_name": "R&RA ARP Act DEFC V",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 148025.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 6000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 6000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p style=\"text-indent: 0px; line-height: 100%; margin: 0px;\">This work is part of an interdisciplinary effort -- between linguists, psychologists, computer vision researchers, and virtual human researchers -- to understand the extent to which perceptual and articulatory factors affect the evolution of sign languages over time. A major challenge for studying this question is that most sign language datasets are video-based. Thus, they do not directly provide the head, arm, and finger positions needed to estimate locations, distances, velocities, and energy. To address this challenge, we have built a system for computing three-dimensional biomechanical skeletal motion from point poses extracted from standard videos. The point poses are estimated using a computer vision approach based on deep learning, developed by the computer vision team. The biomechanical skeleton models the upper body with realistic proportions. Using the skeleton, we can relate the video poses to real-world units, enforce constraints on the limb sizes and their rotations, and directly compute quantifiable metrics on the motions of the arms and head. Specifically, we can compare the effort required to make different signs, estimated in terms of rotational forces and kinetic energy, as well as the rotational velocities, accelerations, proximity of the hands to the face, chest, or hips, and symmetry of the hand movements.</p>\n<p style=\"text-indent: 0px; line-height: 100%; margin: 0px;\">&nbsp;</p>\n<p style=\"text-indent: 0px; line-height: 100%; margin: 0px;\">We have demonstrated this pipeline on two datasets. The first consists of RGB-D images recorded with the Kinect. The second is an archive of RBG videos of Nicaraguan Sign Language that were recorded decades ago on analog tape. We have presented the details of our open-source pipeline at the Theoretical Issues in Sign Language Research (TISLR) conference, in the talk entitled Open-Source Pipeline for Skeletal Modeling of Sign Language Utterances from 2D Video Sources. These automated tools allow us to quantitatively analyze large video datasets in ways that were previously infeasible.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/02/2023<br>\n\t\t\t\t\tModified by: Norman&nbsp;I&nbsp;Badler</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1749397/1749397_10544822_1680456781776_OutcomeImage--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1749397/1749397_10544822_1680456781776_OutcomeImage--rgov-800width.jpg\" title=\"Pipeline for the numerical analysis of signs\"><img src=\"/por/images/Reports/POR/2023/1749397/1749397_10544822_1680456781776_OutcomeImage--rgov-66x44.jpg\" alt=\"Pipeline for the numerical analysis of signs\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Top, our pipeline fits a biomechanical skeleton to a video. Bottom, with this skeleton, we can automatically compare the characteristics of different signs. Here, we compare two signs where the first sign requires less effort than the second, estimated based on the forces of the arms.</div>\n<div class=\"imageCredit\">Aline Normoyle, Norman I. Badler, Matthew W. G. Dye</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Norman&nbsp;I&nbsp;Badler</div>\n<div class=\"imageTitle\">Pipeline for the numerical analysis of signs</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "This work is part of an interdisciplinary effort -- between linguists, psychologists, computer vision researchers, and virtual human researchers -- to understand the extent to which perceptual and articulatory factors affect the evolution of sign languages over time. A major challenge for studying this question is that most sign language datasets are video-based. Thus, they do not directly provide the head, arm, and finger positions needed to estimate locations, distances, velocities, and energy. To address this challenge, we have built a system for computing three-dimensional biomechanical skeletal motion from point poses extracted from standard videos. The point poses are estimated using a computer vision approach based on deep learning, developed by the computer vision team. The biomechanical skeleton models the upper body with realistic proportions. Using the skeleton, we can relate the video poses to real-world units, enforce constraints on the limb sizes and their rotations, and directly compute quantifiable metrics on the motions of the arms and head. Specifically, we can compare the effort required to make different signs, estimated in terms of rotational forces and kinetic energy, as well as the rotational velocities, accelerations, proximity of the hands to the face, chest, or hips, and symmetry of the hand movements.\n \nWe have demonstrated this pipeline on two datasets. The first consists of RGB-D images recorded with the Kinect. The second is an archive of RBG videos of Nicaraguan Sign Language that were recorded decades ago on analog tape. We have presented the details of our open-source pipeline at the Theoretical Issues in Sign Language Research (TISLR) conference, in the talk entitled Open-Source Pipeline for Skeletal Modeling of Sign Language Utterances from 2D Video Sources. These automated tools allow us to quantitatively analyze large video datasets in ways that were previously infeasible.\n\n\t\t\t\t\tLast Modified: 04/02/2023\n\n\t\t\t\t\tSubmitted by: Norman I Badler"
 }
}