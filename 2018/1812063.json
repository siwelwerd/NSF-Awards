{
 "awd_id": "1812063",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Probabilistic Underpinning of Imprecise Probability and Statistical Learning with Low-Resolution Information",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 199928.0,
 "awd_amount": 199928.0,
 "awd_min_amd_letter_date": "2018-07-03",
 "awd_max_amd_letter_date": "2018-07-03",
 "awd_abstract_narration": "All fruitful scientific and statistical analyses require assumptions. Some assumptions rightfully reflect past experience, present consensus, or future speculations. Others are imposed solely due to limitations of the investigation methods.  Useful information in practice often comes in a vague, \"low-resolution\" form, like a blurred picture, both literally and figuratively. Currently, statistical models have largely relied on overly precise model structures, built upon a mix of sound scientific knowledge and some less verifiable assumptions. As models grow larger to accommodate the ever-growing volume and variety of data, statistical inference is faced with the pressing need to accurately and honestly express all types of low-resolution knowledge. Without adequate tools to deal with vague information, investigators are forced to concoct high-resolution assumptions that can neither be trusted nor invalidated in meaningful ways, the culprit in the ongoing crisis of irreplicable research. This project aims to provide scientists and statisticians both a theoretical framework and practical methods to tackle this challenge without having to abandon familiar probabilistic rules and tools, thereby strengthening the effort in reducing irreplicable scientific findings. \r\n\r\nThe need to reduce unwanted assumptions in scientific and statistical studies has led to an extensive literature on imprecise probability (IP), or more broadly, soft methods in probability and statistics (SMPS). As of today, both have received little attention from the statistics community, which generally equivocates on anything that does not obey precise probabilistic rules. This project demonstrates that both the precise probability and hard statistical principles have much to offer for studying IP and SMPS, with the fundamental realization that once going beyond precise probabilities, the learning rules by which we update the imprecise model must become the vehicle for implicit assumptions, explaining some paradoxes and puzzles that arise in IP and SMPS.  With a clearer understanding of what IP/SMPS can and cannot do, the proposed research contributes in theoretical and practical ways to ensure and enhance replicability of scientific studies that rely on probabilistic reasoning and statistical analysis.\r\nThe initial idea of this project stemmed from the PI's realization that in handling low-resolution information, the well-accepted Heitjan-Rubin framework for data coarsening in the literature of missing data induces essentially the same mathematical structure as does the Dempster-Shafer theory of belief function. Consequently, belief function can be understood and studied using ordinary probability. The proposed research explores this link and extensions to its variations, and aims to provide (1) a precise probabilistic formulation of belief function, which offers both insights and questions for the Dempster-Shafer theory, especially Dempster's Rule of Combination; (2) a detailed comparison and contrast of three learning rules for updating and propagating low-resolution information, especially with respect to the phenomena of dilation, contraction, and sure loss; and (3) an exploration of the design and implementation of efficient, MCMC-type algorithms for learning rules of low-resolution inference, in parallel to MCMC for Bayesian inference. The overarching goal of the proposed research is to enhance the scientists' and statisticians' toolkit for conducting more objective inference and data analysis.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiao-Li",
   "pi_last_name": "Meng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiao-Li Meng",
   "pi_email_addr": "meng@stat.harvard.edu",
   "nsf_id": "000107265",
   "pi_start_date": "2018-07-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "One Oxford Street",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382901",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 199928.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>All fruitful scientific and statistical analyses require assumptions. Some assumptions rightfully reflect past experience, present consensus, or future speculations, while others are imposed solely due to limitations of the investigation methods. Without clear high-resolution information, investigators are forced to make up assumptions that can be neither trusted nor invalidated in meaningful ways, a practice that contributes to the ongoing crisis of irreplicable research. Imprecise probabilities alleviate the need for high-resolution and unwarranted assumptions in statistical modeling and risk assessment. They present an alternative strategy to reduce irreplicable findings. However, making inference with imprecise probabilities requires the user to choose among alternative updating rules, that is, modifying existing imprecise probabilities for a learning object in the presence of new data or information, regardless of their resolution levels. This research project has led to a featured article (with comments by experts from multiple disciplines) in <em>Statistical Science</em> that studies three major updating rules, which are revealed to represent respectively optimistic, pessimistic, and opportunistic strategies for updating with low-resolution information. It also reveals some unexpected contradictions among the rules, as well as a host of their behaviors, ranging from highly desirable and seriously questionable.&nbsp; These findings highlight the invaluable role of judicious judgment in handling vague low-resolution information, and the care that needs to be taken.&nbsp; A vivid case is about dealing with self-selection bias in response to questionary surveys, a process typically is not fully understood but it can have devastating consequences when it is not handled properly.&nbsp; The selection biases in the responses from very large surveys (e.g., those from social media) can destroy the vast majority of the data information. In fact, such biased &ldquo;big data&rdquo; are worse than being useless because their sheer volumes often provide a false sense of confidence, leading to the big-data paradox: the bigger the data, the surer we fool ourselves. This is because the bigger the biased data, the surer they lead us to the wrong target, with misleadingly small margin of error.</p>\n<p>Another major research outcome is the establishment of a multi-resolution framework for quantifying, assessing, and communicating statistical evidence for personalized treatments.&nbsp;&nbsp; Since one cannot collect direct information about the effectiveness of a treatment on an individual before treating the person, all statistical evidence for assessing the effectiveness at the individual levels must be indirect, relying on transitional inference, an empiricism notion rooted and practiced in clinical medicine since ancient Greece. Knowledge and experiences gained from treating an entity (e.g., one disease or a group of patients) are applied to treat a related but distinctively different one (e.g., a similar disease or a new patient). This notion of &ldquo;transition to the similar'' renders personalized treatments an operational meaning, yet its theoretical foundation defies the familiar inductive inference framework. The multi-resolution framework from the engineering literature (e.g., for dealing with photo images with different pixel levels) is generalized to address this major gap, where one uses the resolution level to index the degree of approximation to ultimate individuality. &nbsp;One then seeks an appropriate level of resolution based on the indirect data to see most clearly the potential treatment effectiveness or lack thereof.&nbsp; Intuitively, this is much like when one takes a photo, one needs to seek an appropriate zooming level in order to make the image clearest; either too much or too little zooming will lead to the loss of the big picture. &nbsp;Unexpectedly, a key formular for understanding how the resolution level affects the clarity of the evidence also suggests a newer interpretation of the effectiveness of some machine learning algorithms, when seemingly high-resolution complex patterns and relationships can be well approximated by lower-resolution deterministic systems. These findings resulted in an article in the special issue of <em>The Journal of the American Statistical Association </em>on &ldquo;Precision medicine and individualized policy discovery\", which has inspired further work.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/08/2023<br>\n\t\t\t\t\tModified by: Xiao-Li&nbsp;Meng</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAll fruitful scientific and statistical analyses require assumptions. Some assumptions rightfully reflect past experience, present consensus, or future speculations, while others are imposed solely due to limitations of the investigation methods. Without clear high-resolution information, investigators are forced to make up assumptions that can be neither trusted nor invalidated in meaningful ways, a practice that contributes to the ongoing crisis of irreplicable research. Imprecise probabilities alleviate the need for high-resolution and unwarranted assumptions in statistical modeling and risk assessment. They present an alternative strategy to reduce irreplicable findings. However, making inference with imprecise probabilities requires the user to choose among alternative updating rules, that is, modifying existing imprecise probabilities for a learning object in the presence of new data or information, regardless of their resolution levels. This research project has led to a featured article (with comments by experts from multiple disciplines) in Statistical Science that studies three major updating rules, which are revealed to represent respectively optimistic, pessimistic, and opportunistic strategies for updating with low-resolution information. It also reveals some unexpected contradictions among the rules, as well as a host of their behaviors, ranging from highly desirable and seriously questionable.  These findings highlight the invaluable role of judicious judgment in handling vague low-resolution information, and the care that needs to be taken.  A vivid case is about dealing with self-selection bias in response to questionary surveys, a process typically is not fully understood but it can have devastating consequences when it is not handled properly.  The selection biases in the responses from very large surveys (e.g., those from social media) can destroy the vast majority of the data information. In fact, such biased \"big data\" are worse than being useless because their sheer volumes often provide a false sense of confidence, leading to the big-data paradox: the bigger the data, the surer we fool ourselves. This is because the bigger the biased data, the surer they lead us to the wrong target, with misleadingly small margin of error.\n\nAnother major research outcome is the establishment of a multi-resolution framework for quantifying, assessing, and communicating statistical evidence for personalized treatments.   Since one cannot collect direct information about the effectiveness of a treatment on an individual before treating the person, all statistical evidence for assessing the effectiveness at the individual levels must be indirect, relying on transitional inference, an empiricism notion rooted and practiced in clinical medicine since ancient Greece. Knowledge and experiences gained from treating an entity (e.g., one disease or a group of patients) are applied to treat a related but distinctively different one (e.g., a similar disease or a new patient). This notion of \"transition to the similar'' renders personalized treatments an operational meaning, yet its theoretical foundation defies the familiar inductive inference framework. The multi-resolution framework from the engineering literature (e.g., for dealing with photo images with different pixel levels) is generalized to address this major gap, where one uses the resolution level to index the degree of approximation to ultimate individuality.  One then seeks an appropriate level of resolution based on the indirect data to see most clearly the potential treatment effectiveness or lack thereof.  Intuitively, this is much like when one takes a photo, one needs to seek an appropriate zooming level in order to make the image clearest; either too much or too little zooming will lead to the loss of the big picture.  Unexpectedly, a key formular for understanding how the resolution level affects the clarity of the evidence also suggests a newer interpretation of the effectiveness of some machine learning algorithms, when seemingly high-resolution complex patterns and relationships can be well approximated by lower-resolution deterministic systems. These findings resulted in an article in the special issue of The Journal of the American Statistical Association on \"Precision medicine and individualized policy discovery\", which has inspired further work. \n\n \n\n\t\t\t\t\tLast Modified: 01/08/2023\n\n\t\t\t\t\tSubmitted by: Xiao-Li Meng"
 }
}