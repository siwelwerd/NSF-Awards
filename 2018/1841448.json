{
 "awd_id": "1841448",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032928235",
 "po_email": "bmihaila@nsf.gov",
 "po_sign_block_name": "Bogdan Mihaila",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 422981.0,
 "awd_amount": 422981.0,
 "awd_min_amd_letter_date": "2018-09-07",
 "awd_max_amd_letter_date": "2018-09-07",
 "awd_abstract_narration": "The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of \"real\" data collected from the experiments with \"synthetic\" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.\r\n\r\nThe main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the \"lingua franca\" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.\r\n\r\nThis project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Hildreth",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Hildreth",
   "pi_email_addr": "hildreth.2@nd.edu",
   "nsf_id": "000090746",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Notre Dame",
  "inst_street_address": "940 GRACE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "NOTRE DAME",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "5746317432",
  "inst_zip_code": "465565708",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "IN02",
  "org_lgl_bus_name": "UNIVERSITY OF NOTRE DAME DU LAC",
  "org_prnt_uei_num": "FPU6XGFXMBE9",
  "org_uei_num": "FPU6XGFXMBE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Notre Dame",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "465565708",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "IN02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "768400",
   "pgm_ele_name": "CESER-Cyberinfrastructure for"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "020Z",
   "pgm_ref_txt": "OAC Facility Cyberinfrastructure"
  },
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 422981.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our portion of the SCAILFIN (Scalable Cyberinfrastructure for Aritificial Intelligence and Likelihood Free INference) project was to build infrastructure that would allow the orchestration of very complicated computational workflows on large HPC systems.&nbsp;&nbsp; The intellectual merit of this work is that it enables the rapid training of advanced machine learning (ML) and artificial intelligence (AI) models that will advance our capabiliities to study physics processes at the Large Hadron Collider (and beyond). These models are currently \"too big\" to be constructed on a typical desktop server or a small cluster of computers, and the workflows required to construct them are very complicated (see Figure 1).&nbsp; Workflows such as this can be executed by the REANA computational framework. Before this project, however, the REANA framework was limited to running on local clusters and hence was not capable of exploiting large-scale HPC-like installations.&nbsp;</p>\n<p>To make accessing these larger systems possible, we extended the functionality of the VC3 (Virtual Clusters for Community Computation; https://www.virtualclusters.org) project that developed flexible infrastructure for resource provisioning and job orchestration. This new infrastrcture is able to run a REANA instance on essentially any platform.&nbsp; We then created a back-end to REANA that can submit jobs to HTCondor, a job execution and scheduling engine that is an industry standard.&nbsp; This interface allows our new version of REANA to pass computational jobs to the local infrastructure on the HPC sites through an intermediary such as BOSCO. &nbsp; The new VC3/REANA infrastructure is shown in Figure 2 in the configuration used to compute on the CORI cluster at NERSC.</p>\n<p>We have tested this infrastructure at a wide variety of sites hosting large computing clusters, including NERSC, TACC, and PSC.&nbsp; We have demonstrated the ability to orchestrate up to 1000 simultaneous jobs in a given workflow step. We have been able to genereate 1M simulated collision events in a single run; such large statistics are necessary to train the complicated machine learning models for which the system was designed. Demonstrating this ability has been possible only after finding and solving several different types of bottlenecks in the existing infrastructure.</p>\n<p>The broader impacts of this project stem from the ability to run complex workflows at scale.&nbsp; Under the hood, REANA works by orchestrating linux containers encapsulating specific pieces of a computational workflow. Any workfow in any discipline that can be captured in this way and represetned by any number of different computational graphs can be executed by the SCAILFIN infastructure we have built. &nbsp; Our code and instructions to run it are all publicly available.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/29/2023<br>\n\t\t\t\t\tModified by: Michael&nbsp;Hildreth</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1841448/1841448_10580696_1675024513544_SCAILFIN_NERSC_diagram--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1841448/1841448_10580696_1675024513544_SCAILFIN_NERSC_diagram--rgov-800width.jpg\" title=\"Figure 2. REANA/VC3 configuration of SCAILFIN components\"><img src=\"/por/images/Reports/POR/2023/1841448/1841448_10580696_1675024513544_SCAILFIN_NERSC_diagram--rgov-66x44.jpg\" alt=\"Figure 2. REANA/VC3 configuration of SCAILFIN components\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This diagram shows the components of a VC3-based REANA installation used to compute machine learning workflows at scale on the CORI cluster at NERSC</div>\n<div class=\"imageCredit\">Kenyi Hurtado Anampa, Notre Dame</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Michael&nbsp;Hildreth</div>\n<div class=\"imageTitle\">Figure 2. REANA/VC3 configuration of SCAILFIN components</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1841448/1841448_10580696_1675023446502_yadage_workflow--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1841448/1841448_10580696_1675023446502_yadage_workflow--rgov-800width.jpg\" title=\"Figure 1. Example machine learning training workflow\"><img src=\"/por/images/Reports/POR/2023/1841448/1841448_10580696_1675023446502_yadage_workflow--rgov-66x44.jpg\" alt=\"Figure 1. Example machine learning training workflow\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An example of a workflow specifying the different elements of a physics analysis specified in the yadage scripting/scheduling language. Ovals are data objects (input or output), rectangles are computational operations</div>\n<div class=\"imageCredit\">Lucas Heinreich, CERN</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Michael&nbsp;Hildreth</div>\n<div class=\"imageTitle\">Figure 1. Example machine learning training workflow</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nOur portion of the SCAILFIN (Scalable Cyberinfrastructure for Aritificial Intelligence and Likelihood Free INference) project was to build infrastructure that would allow the orchestration of very complicated computational workflows on large HPC systems.   The intellectual merit of this work is that it enables the rapid training of advanced machine learning (ML) and artificial intelligence (AI) models that will advance our capabiliities to study physics processes at the Large Hadron Collider (and beyond). These models are currently \"too big\" to be constructed on a typical desktop server or a small cluster of computers, and the workflows required to construct them are very complicated (see Figure 1).  Workflows such as this can be executed by the REANA computational framework. Before this project, however, the REANA framework was limited to running on local clusters and hence was not capable of exploiting large-scale HPC-like installations. \n\nTo make accessing these larger systems possible, we extended the functionality of the VC3 (Virtual Clusters for Community Computation; https://www.virtualclusters.org) project that developed flexible infrastructure for resource provisioning and job orchestration. This new infrastrcture is able to run a REANA instance on essentially any platform.  We then created a back-end to REANA that can submit jobs to HTCondor, a job execution and scheduling engine that is an industry standard.  This interface allows our new version of REANA to pass computational jobs to the local infrastructure on the HPC sites through an intermediary such as BOSCO.   The new VC3/REANA infrastructure is shown in Figure 2 in the configuration used to compute on the CORI cluster at NERSC.\n\nWe have tested this infrastructure at a wide variety of sites hosting large computing clusters, including NERSC, TACC, and PSC.  We have demonstrated the ability to orchestrate up to 1000 simultaneous jobs in a given workflow step. We have been able to genereate 1M simulated collision events in a single run; such large statistics are necessary to train the complicated machine learning models for which the system was designed. Demonstrating this ability has been possible only after finding and solving several different types of bottlenecks in the existing infrastructure.\n\nThe broader impacts of this project stem from the ability to run complex workflows at scale.  Under the hood, REANA works by orchestrating linux containers encapsulating specific pieces of a computational workflow. Any workfow in any discipline that can be captured in this way and represetned by any number of different computational graphs can be executed by the SCAILFIN infastructure we have built.   Our code and instructions to run it are all publicly available. \n\n\t\t\t\t\tLast Modified: 01/29/2023\n\n\t\t\t\t\tSubmitted by: Michael Hildreth"
 }
}