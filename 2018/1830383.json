{
 "awd_id": "1830383",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: INT: COLLAB: Manufacturing USA: Intelligent Human-Robot Collaboration for Smart Factory",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Bruce Kramer",
 "awd_eff_date": "2018-09-15",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 377930.0,
 "awd_amount": 432930.0,
 "awd_min_amd_letter_date": "2018-09-11",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "This National Robotics Initiative (NRI) collaborative research project addresses the NSF Big Idea of Work at the Human-Technology Frontier by targeting human-robot collaboration in manufacturing.  Recent advances in sensing, computational intelligence, and big data analytics have been rapidly transforming and revolutionizing the manufacturing industry towards robot-rich and digitally connected factories. However, effective, efficient and safe coordination between humans and robots on the factory floor has remained a significant challenge. To meet the need for safe and effective human-robot collaboration in manufacturing, the investigators will research an integrated set of algorithms and robotic test beds to sense, understand, predict and control the interaction of human workers and robots in collaborative manufacturing cells.  It is expected that these methods will  significantly improve the safety and productivity of hybrid human-robot production systems, thereby promoting their deployment in future \"smart factories\".  To broaden the impact of this project, a partnership with Manufacturing USA Institute(s) and professional societies will be established to provide human-robot collaboration learning modules for inclusion in robotics and smart manufacturing-related curricula.  These learning modules, together with annual events aimed at community college and pre-college students, and workshops for the dissemination of research results will raise public awareness and attract new entrants into the manufacturing and robotics industries, creating truly synergetic education opportunities in science, technology, engineering and mathematics, as well as accelerating the adoption of smart factory-enabling technologies.  \r\n\r\nThe project will address fundamental challenges in human-robot collaboration in the manufacturing environment, such as the limitation of one-to-one sensing between humans and robots, the lack of adaptive and stochastic modeling methods for reliable recognition and prediction of human actions and motions in different manufacturing scenarios, and multi-scale human-robot coordination. To address these challenges, multi-disciplinary research involving sensing, machine learning, stochastic modeling, robot path planning, and advanced manufacturing will be performed.  Specific tasks include algorithm development and deployment on lab-scale and real-world test beds to: (1) sense and recognize where objects (e.g., robots, humans, parts or tools) are located and what each worker is doing; (2) predict what the next human action will be; and (3) plan and control safe and optimal robot trajectories for individualized on-the-job assistance for humans, proactively avoiding worker injury. The outcomes from the project will be evaluated on the shop-floor at the collaborating company COsorizio MAcchine Uensili (COMAU) in Michigan, and the Institute of Intelligent Industrial Technologies and Systems for Advanced Manufacturing of the National Research Council of Italy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gloria",
   "pi_last_name": "Wiens",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Gloria J Wiens",
   "pi_email_addr": "gwiens@ufl.edu",
   "nsf_id": "000410503",
   "pi_start_date": "2018-09-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Florida",
  "inst_street_address": "1523 UNION RD RM 207",
  "inst_street_address_2": "",
  "inst_city_name": "GAINESVILLE",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "3523923516",
  "inst_zip_code": "326111941",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "FL03",
  "org_lgl_bus_name": "UNIVERSITY OF FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NNFQH1JAPEP3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Florida",
  "perf_str_addr": "1 University of Florida",
  "perf_city_name": "Gainesville",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "326112002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "FL03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "150400",
   "pgm_ele_name": "GOALI-Grnt Opp Acad Lia wIndus"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "019Z",
   "pgm_ref_txt": "Grad Prep APG:Enhan. Experience"
  },
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "1504",
   "pgm_ref_txt": "GRANT OPP FOR ACAD LIA W/INDUS"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "MANU",
   "pgm_ref_txt": "MANUFACTURING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 377930.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 55000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to enable safe and effective human-robot collaboration (HRC) in manufacturing through research on algorithms and to evaluate those algorithms on appropriate testbeds in order to: (1) sense and cognize where objects are located and what each worker is doing; (2) predict what the next human action will be and the trajectory of the human worker motion as part of the operational sequence for assembling a certain object/system; and (3) plan and control the safe and optimal robot trajectories to provide individualized on-the-job coordination and assistance for humans and avoid worker injury proactively.&nbsp; This project is a USA-Italy collaboration involving four U.S. universities, STIIMA-National Research Council of Italy, and industry.</p>\n<p>The work of the University of Florida (UF) team focused on algorithms to transform sensing, cognition, and prediction data into robot behaviors, collectively known as the Proactive Adaptive Collaborative Intelligence (PACI) controller. Robot behaviors include adaptations to robot motion and robot sequencing. As a foundation, UF research began by developing a framework for segmenting robot motion and determining allowable robot behavior modifications per segment. In this framework, robot motion for a task is broken down into motion segments. If a person or other obstacle interferes with robot motion, the robot path for the affected motion segment can be re-planned to promote task completion. The framework also included a cost function to switch behaviors, possibly to a re-planning strategy, if interference persists for enough time.</p>\n<p>To intelligently adapt robot behaviors, UF work also included algorithms to localize humans and predict human motion. The Skeleton Fusion method was developed to more accurately track a person with a multi-camera sensor suite. From a single camera point of view, parts of a human could be occluded by objects such as robots or parts. Therefore, Skeleton Fusion stitches together a complete human model by taking location data of each limb from the most accurate camera. The Human Prediction - Generative Network (HP-GN) was developed to predict human motion while reaching for parts or workpieces on or above a table. HP-GN simultaneously estimates a final human pose at which his/her hand is at the target and predicts a sequence of poses a person will pass through between a starting pose and the final pose. The predicted sequence of human motion can then be avoided in planning robot paths or used to estimate a level of human-robot interaction for tasks.</p>\n<p>Next, the UF team used the human location and prediction data for proactive collision detection and robot path planning. An algorithm was developed to anticipate collision between predicted robot motion and predicted human motion by sweeping robot and human volumes along their anticipated paths. Anticipated robot/human collisions occurs at the spatio-temporal intersection of the robot and human sweeps. The UF team also developed the Spatio-Temporal Avoidance of Predictions - Prediction and Planning Framework (STAP-PPF) to proactively plan robot paths to avoid anticipated human motion for robot and human tasks. The STAP-PPF framework incorporated the HP-GN method to predict individual human motions for tasks. STAP-PPF developed a time-based variation of the Rapidly Exploring Random Trees planner to estimate a path and robot timing that avoids time-varying human occupancy volumes within the shared workspace between robot and human.&nbsp; STAP-PPF generated robot paths of shorter duration while keeping the robot farther from the human relative to benchmarking methods, improving productivity and safety. The UF team also developed a Reinforcement Learning (RL) algorithm to generate a robot path in real-time that avoids anticipated human occupancy.</p>\n<p>Finally, the UF team developed a Decision Switch Module (DSM) framework for determining when to re-plan robot motion and when to switch robot subtasks. The foundation for the DSM is the segmentation framework. The DSM permits continuous replanning of the robot path to avoid obstacles.&nbsp; It limits the number of replanning attempts before considering switching the robot to a different subtask to improve productivity. When the DSM switches the subtask, an RL contextual bandit-based algorithm considers factors from outputs of HP-GN, STAP-PPF, and other factors like human awareness to select the subtask that can be completed the fastest and that enables other subtasks.</p>\n<p>Key outcomes, achievements and activities also include: 1) internships providing immersive R&amp;D experience and access to industry testbeds for verification and validation (V&amp;V) testing of the project algorithms and methodology, 2) engagement of Florida Advanced Technology Education Programs for broader impact of the research, 3) DSM enhanced switching logic that balances robot motion planning, 4) HRC algorithms that incorporate industry required ISO protocols and standards, 5) PACI robot behaviors/reactions maintaining safety and production efficiency.&nbsp; To engage stakeholders from academe, industry, and government, a one-day Human-Robot Collaboration &amp; AI Integration workshop was organized as a special event at ASME International Mechanical Engineering Congress &amp; Exposition (IMECE 2023), on November 2 in New Orleans, LA.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/16/2024<br>\nModified by: Gloria&nbsp;J&nbsp;Wiens</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1830383/1830383_10582159_1704140307186_brescia--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1830383/1830383_10582159_1704140307186_brescia--rgov-800width.png\" title=\"Validation of STAP-PPF\"><img src=\"/por/images/Reports/POR/2024/1830383/1830383_10582159_1704140307186_brescia--rgov-66x44.png\" alt=\"Validation of STAP-PPF\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">STIIMA/CNR/University of Brescia, IT joint laboratory of an Industrial UR10 6-dof robotic cell used for validation of STAP-PPF with 19 participants. Tests: Robot picks from parts storage bin and moves to pallet C; while, human picks parts from table in front of pallet B and moves to pallets A and B.</div>\n<div class=\"imageCredit\">G. Wiens / J. Flowers</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Gloria&nbsp;J&nbsp;Wiens\n<div class=\"imageTitle\">Validation of STAP-PPF</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1830383/1830383_10582159_1704143036381_rope--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1830383/1830383_10582159_1704143036381_rope--rgov-800width.png\" title=\"Determination of Robot Risk of Passage (ROPE) - Entrapment\"><img src=\"/por/images/Reports/POR/2024/1830383/1830383_10582159_1704143036381_rope--rgov-66x44.png\" alt=\"Determination of Robot Risk of Passage (ROPE) - Entrapment\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">ROPE estimated level of risk is based on factors such as robot proximity to the human along the current robot path and speed of human wrists and elbows relative to the robot. Darker red workcell voxel - higher risk for robot to be able to pass between the human arm and the table in this image.</div>\n<div class=\"imageCredit\">G. Wiens / J. Flowers</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Gloria&nbsp;J&nbsp;Wiens\n<div class=\"imageTitle\">Determination of Robot Risk of Passage (ROPE) - Entrapment</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1830383/1830383_10582159_1704140606121_skeleton_fusion--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/1830383/1830383_10582159_1704140606121_skeleton_fusion--rgov-800width.jpg\" title=\"Skeleton Fusion\"><img src=\"/por/images/Reports/POR/2024/1830383/1830383_10582159_1704140606121_skeleton_fusion--rgov-66x44.jpg\" alt=\"Skeleton Fusion\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Robot blocking a camera from seeing part of the human during manufacturing tasks, yielding incorrect skeleton of human left arm in the image. Skeleton Fusion method considers skeleton models from multiple camera perspectives, selecting human link data from the camera providing most accurate data.</div>\n<div class=\"imageCredit\">G. Wiens / J. Flowers</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Gloria&nbsp;J&nbsp;Wiens\n<div class=\"imageTitle\">Skeleton Fusion</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of this project was to enable safe and effective human-robot collaboration (HRC) in manufacturing through research on algorithms and to evaluate those algorithms on appropriate testbeds in order to: (1) sense and cognize where objects are located and what each worker is doing; (2) predict what the next human action will be and the trajectory of the human worker motion as part of the operational sequence for assembling a certain object/system; and (3) plan and control the safe and optimal robot trajectories to provide individualized on-the-job coordination and assistance for humans and avoid worker injury proactively. This project is a USA-Italy collaboration involving four U.S. universities, STIIMA-National Research Council of Italy, and industry.\n\n\nThe work of the University of Florida (UF) team focused on algorithms to transform sensing, cognition, and prediction data into robot behaviors, collectively known as the Proactive Adaptive Collaborative Intelligence (PACI) controller. Robot behaviors include adaptations to robot motion and robot sequencing. As a foundation, UF research began by developing a framework for segmenting robot motion and determining allowable robot behavior modifications per segment. In this framework, robot motion for a task is broken down into motion segments. If a person or other obstacle interferes with robot motion, the robot path for the affected motion segment can be re-planned to promote task completion. The framework also included a cost function to switch behaviors, possibly to a re-planning strategy, if interference persists for enough time.\n\n\nTo intelligently adapt robot behaviors, UF work also included algorithms to localize humans and predict human motion. The Skeleton Fusion method was developed to more accurately track a person with a multi-camera sensor suite. From a single camera point of view, parts of a human could be occluded by objects such as robots or parts. Therefore, Skeleton Fusion stitches together a complete human model by taking location data of each limb from the most accurate camera. The Human Prediction - Generative Network (HP-GN) was developed to predict human motion while reaching for parts or workpieces on or above a table. HP-GN simultaneously estimates a final human pose at which his/her hand is at the target and predicts a sequence of poses a person will pass through between a starting pose and the final pose. The predicted sequence of human motion can then be avoided in planning robot paths or used to estimate a level of human-robot interaction for tasks.\n\n\nNext, the UF team used the human location and prediction data for proactive collision detection and robot path planning. An algorithm was developed to anticipate collision between predicted robot motion and predicted human motion by sweeping robot and human volumes along their anticipated paths. Anticipated robot/human collisions occurs at the spatio-temporal intersection of the robot and human sweeps. The UF team also developed the Spatio-Temporal Avoidance of Predictions - Prediction and Planning Framework (STAP-PPF) to proactively plan robot paths to avoid anticipated human motion for robot and human tasks. The STAP-PPF framework incorporated the HP-GN method to predict individual human motions for tasks. STAP-PPF developed a time-based variation of the Rapidly Exploring Random Trees planner to estimate a path and robot timing that avoids time-varying human occupancy volumes within the shared workspace between robot and human. STAP-PPF generated robot paths of shorter duration while keeping the robot farther from the human relative to benchmarking methods, improving productivity and safety. The UF team also developed a Reinforcement Learning (RL) algorithm to generate a robot path in real-time that avoids anticipated human occupancy.\n\n\nFinally, the UF team developed a Decision Switch Module (DSM) framework for determining when to re-plan robot motion and when to switch robot subtasks. The foundation for the DSM is the segmentation framework. The DSM permits continuous replanning of the robot path to avoid obstacles. It limits the number of replanning attempts before considering switching the robot to a different subtask to improve productivity. When the DSM switches the subtask, an RL contextual bandit-based algorithm considers factors from outputs of HP-GN, STAP-PPF, and other factors like human awareness to select the subtask that can be completed the fastest and that enables other subtasks.\n\n\nKey outcomes, achievements and activities also include: 1) internships providing immersive R&D experience and access to industry testbeds for verification and validation (V&V) testing of the project algorithms and methodology, 2) engagement of Florida Advanced Technology Education Programs for broader impact of the research, 3) DSM enhanced switching logic that balances robot motion planning, 4) HRC algorithms that incorporate industry required ISO protocols and standards, 5) PACI robot behaviors/reactions maintaining safety and production efficiency. To engage stakeholders from academe, industry, and government, a one-day Human-Robot Collaboration & AI Integration workshop was organized as a special event at ASME International Mechanical Engineering Congress & Exposition (IMECE 2023), on November 2 in New Orleans, LA.\n\n\n\t\t\t\t\tLast Modified: 03/16/2024\n\n\t\t\t\t\tSubmitted by: GloriaJWiens\n"
 }
}