{
 "awd_id": "1837244",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CPS: TTP Option: Medium: Collaborative Research: Smoothing Traffic via Energy-efficient Autonomous Driving (STEAD)",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Corman",
 "awd_eff_date": "2019-01-01",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 610000.0,
 "awd_amount": 610000.0,
 "awd_min_amd_letter_date": "2018-09-13",
 "awd_max_amd_letter_date": "2019-08-26",
 "awd_abstract_narration": "Studies show five of the top 10 most-gridlocked cities in the world are in the United States. Traffic congestion puts undue burden on transportation systems across the United States, raising transportation costs and the energy footprint. Vehicle automation creates an opportunity to reduce traffic and improve efficiency of the transportation infrastructure. In particular, this project aims to reduce the energy footprint of phantom traffic jams, where dense traffic comes to a halt for no apparent reason, and also stop-and-go-waves in congestion. The research team aims to reduce the overall energy footprint of stop-and-go congestion by up to 40% via a small portion of connected and autonomous vehicles (CAVs) inserted into normal traffic with drivers, also known as manned traffic. The work will build models of mixed autonomy (a combination of CAVs and manned traffic), and test the ability for this portion of CAVs to smooth the flow of traffic in a controlled manner, and thus reduce the energy footprint. The research combines mathematics, control theory, machine learning, and transportation engineering. The project includes four universities and engages industry and government partners. The project will also engage students and community stakeholders, including State and Federal transportation agencies and CAV manufacturers.\r\n\r\nSpecifically, the technical contributions enabling traffic smoothing and reduction in the environmental footprint include new mean-field optimal control formulations for sparse control settings where only a subset of vehicles are CAVs and can be controlled. Investigators will develop data-driven control algorithms based on deep reinforcement learning designed to enable control in settings where analytical approaches to derive explicit controllers are too complex (e.g., due to multi-lane, ramps, and high variation of human driving styles). They will also develop tools based on Satisfiability Modulo Convex optimization to enable safety and robustness of these controllers. The approach will first be validated using microsimulation tools to assess their efficiency and their validity. Once validated in simulation, the project will then field test the algorithm with manned vehicles following real-time control commands of the system, executed by 100 human drivers following control signals communicated via a phone app with target speeds and lanes. After which, the system will be tested with up to 20 CAVs inserted onto a freeway stretch in the Transition to Practice component of the project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexandre",
   "pi_last_name": "Bayen",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Alexandre M Bayen",
   "pi_email_addr": "bayen@ce.berkeley.edu",
   "nsf_id": "000062644",
   "pi_start_date": "2018-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Shladover",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Steven Shladover",
   "pi_email_addr": "steve@path.berkeley.edu",
   "nsf_id": "000779388",
   "pi_start_date": "2018-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Xiao-Yun",
   "pi_last_name": "Lu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiao-Yun Lu",
   "pi_email_addr": "xiaoyun.lu@berkeley.edu",
   "nsf_id": "000779362",
   "pi_start_date": "2018-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947045940",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "033Y00",
   "pgm_ele_name": "S&CC: Smart & Connected Commun"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "042Z",
   "pgm_ref_txt": "S&CC: Smart and Connected Communities"
  },
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 226417.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 383583.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-488c9826-7fff-4f37-6539-46359392ec17\"> </span></p>\n<p><span id=\"docs-internal-guid-3da5d75f-7fff-0051-29db-9f5b53431204\"> </span></p>\n<p dir=\"ltr\"><span>Traffic congestion puts undue burden on transportation systems across the United States, raising transportation costs. In this project, we set out to leverage vehicle automation technologies to design automated driving behaviors that minimize the energy footprint of the entire system of vehicles on a highway, with only a small penetration rate of automated vehicles. The efforts on this project span multiple academic institutions and research disciplines, and most of the research thrusts were collaborative and cross-functional.</span></p>\n<p dir=\"ltr\"><span>Specifically, the primary focus at UC Berkeley was to design vehicle control behaviors via machine learning/artificial intelligence. The approach that we specialize in is reinforcement learning (RL), wherein an agent learns a behavior (aka \"policy\") via trial-and-error and collecting rewards and penalties along the way, much in the way a child may learn a new task. In order to use RL, we require a simulation environment that is sufficiently similar to real traffic flow, and we need to design a reward system that produces a desirable outcome (e.g., rewarding for maximizing speed and/or penalizing for energy consumption).</span></p>\n<p dir=\"ltr\"><span>A \"typical\" approach to building a realistic simulation environment would be to replicate the road geometry, find models in the literature for car-following behaviors and lane-changing behaviors, set some rules of the road and boundary conditions, and then calibrate the whole system to known data. However, through the course of our work, we found a vastly simpler and more effective approach:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>reduce the simulation to a straight single-lane geometry;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>initialize the simulation with a lead vehicle that replays a realistic trajectory (nominally collected beforehand by driving down the highway and recording the speed);</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>add following vehicles where</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>car-following behavior is from some well-known model in the literature, and</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>lane-changing behavior is realized by randomly inserting and removing vehicles, based on simple probabilities extracted from real data.</span></p>\n</li>\n</ul>\n</li>\n</ul>\n<p dir=\"ltr\"><span>The advantages of this approach are numerous, e.g.,</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>the replayed trajectory ensures that the bulk flow of traffic is realistic, without the need of complicated boundary conditions and model calibration;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>the software architecture is simpler, does not rely on complex third-party simulators, and is computationally much less expensive (allowing for faster and more RL training).</span></p>\n</li>\n</ul>\n<p dir=\"ltr\"><span>With this approach, each drive down the freeway adds to our set of trajectories that we can train or test controllers with, exposing our agents to a variety of diverse experiences.</span></p>\n<p dir=\"ltr\"><span>Utilizing this new simulation approach, we designed a controller by placing an AV agent into the simulation, positioned directly behind the replayed leader trajectory. The agent takes as input its own current speed, its leader's current speed, and the bumper-to-bumper gap between the two. It then outputs a desired acceleration to be applied to the AV. The agent is generally rewarded for some combination of high fuel economy and speed, and it is penalized for some combination of jerky movements and gaps that are too large or too small. This allows us to design a range of AV policies that can be hand-picked for the most desirable qualities. In simulation, we have achieved fuel economy improvements up to 20%. This work continues through other grants, as we aim to deploy our policy in a large-scale real world test later in 2022.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/14/2022<br>\n\t\t\t\t\tModified by: Alexandre&nbsp;M&nbsp;Bayen</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1837244/1837244_10582653_1649960262855_ScreenShot2022-04-14at11.12.10AM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1837244/1837244_10582653_1649960262855_ScreenShot2022-04-14at11.12.10AM--rgov-800width.jpg\" title=\"Time-Space Diagram of Smoothed Flow\"><img src=\"/por/images/Reports/POR/2022/1837244/1837244_10582653_1649960262855_ScreenShot2022-04-14at11.12.10AM--rgov-66x44.jpg\" alt=\"Time-Space Diagram of Smoothed Flow\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A benchmark example of a simulation result. The time-space diagram illustrates the trajectories of all vehicles through time (x-axis) and space (y-axis), colored by their speed. Here, eight equally-spaced automated vehicles  (among 200 human-driven vehicles) smooth out stop-and-go waves in the flow.</div>\n<div class=\"imageCredit\">Arwa AlAnqary</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Alexandre&nbsp;M&nbsp;Bayen</div>\n<div class=\"imageTitle\">Time-Space Diagram of Smoothed Flow</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \n\n \nTraffic congestion puts undue burden on transportation systems across the United States, raising transportation costs. In this project, we set out to leverage vehicle automation technologies to design automated driving behaviors that minimize the energy footprint of the entire system of vehicles on a highway, with only a small penetration rate of automated vehicles. The efforts on this project span multiple academic institutions and research disciplines, and most of the research thrusts were collaborative and cross-functional.\nSpecifically, the primary focus at UC Berkeley was to design vehicle control behaviors via machine learning/artificial intelligence. The approach that we specialize in is reinforcement learning (RL), wherein an agent learns a behavior (aka \"policy\") via trial-and-error and collecting rewards and penalties along the way, much in the way a child may learn a new task. In order to use RL, we require a simulation environment that is sufficiently similar to real traffic flow, and we need to design a reward system that produces a desirable outcome (e.g., rewarding for maximizing speed and/or penalizing for energy consumption).\nA \"typical\" approach to building a realistic simulation environment would be to replicate the road geometry, find models in the literature for car-following behaviors and lane-changing behaviors, set some rules of the road and boundary conditions, and then calibrate the whole system to known data. However, through the course of our work, we found a vastly simpler and more effective approach:\n\n\nreduce the simulation to a straight single-lane geometry;\n\n\ninitialize the simulation with a lead vehicle that replays a realistic trajectory (nominally collected beforehand by driving down the highway and recording the speed);\n\n\nadd following vehicles where\n\n\ncar-following behavior is from some well-known model in the literature, and\n\n\nlane-changing behavior is realized by randomly inserting and removing vehicles, based on simple probabilities extracted from real data.\n\n\n\n\nThe advantages of this approach are numerous, e.g.,\n\n\nthe replayed trajectory ensures that the bulk flow of traffic is realistic, without the need of complicated boundary conditions and model calibration;\n\n\nthe software architecture is simpler, does not rely on complex third-party simulators, and is computationally much less expensive (allowing for faster and more RL training).\n\n\nWith this approach, each drive down the freeway adds to our set of trajectories that we can train or test controllers with, exposing our agents to a variety of diverse experiences.\nUtilizing this new simulation approach, we designed a controller by placing an AV agent into the simulation, positioned directly behind the replayed leader trajectory. The agent takes as input its own current speed, its leader's current speed, and the bumper-to-bumper gap between the two. It then outputs a desired acceleration to be applied to the AV. The agent is generally rewarded for some combination of high fuel economy and speed, and it is penalized for some combination of jerky movements and gaps that are too large or too small. This allows us to design a range of AV policies that can be hand-picked for the most desirable qualities. In simulation, we have achieved fuel economy improvements up to 20%. This work continues through other grants, as we aim to deploy our policy in a large-scale real world test later in 2022.\n\n \n\n\t\t\t\t\tLast Modified: 04/14/2022\n\n\t\t\t\t\tSubmitted by: Alexandre M Bayen"
 }
}