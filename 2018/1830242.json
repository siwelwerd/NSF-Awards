{
 "awd_id": "1830242",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Communicating Physical Interactions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Juan Wachs",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 749986.0,
 "awd_amount": 797986.0,
 "awd_min_amd_letter_date": "2018-08-17",
 "awd_max_amd_letter_date": "2022-05-23",
 "awd_abstract_narration": "For robots to become ubiquitous collaborators, they must interact physically with objects in unstructured human environments. Robots must adeptly grasp, push, squeeze, snap, balance, stabilize and hand-off objects, and must effectively communicate about these actions with their human collaborators. People must not only be able to specify to a robot what to do and how to do it, they must also be able to interpret what a robot intends to do (and how).  Effective communication about physical interactions will be essential if robots are going to perform such tasks as delivering care to older adults, responsively helping technicians in performing repairs, or being trained by non-experts to perform repetitive assembly tasks.  This project will enable a new generation of robot applications, and advance the vision of ubiquitous, collaborative robots by developing better methods for robots to more effectively communicate with people. \r\n\r\nThe project will address key challenges in communication about physical interactions: conveying invisible and unfamiliar quantities (e.g., forces and compliances), communicating plans and contingencies, and communicating about what did not (or should not) happen.  The project will involve three key challenges: specification, interpretation, and monitoring.  To address these challenges, the project will 1) perform formative studies to gain insight into how people communicate about physical interactions and interpret displays; 2) develop methods for specifying physical actions based on the idea of augmented demonstrations, methods for interpreting physical actions based on the idea of interpretable representations, and methods for monitoring physical actions based on multimodal communication; and 3) deploy these ideas in prototype systems for contextualized scenarios for evaluation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Gleicher",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Gleicher",
   "pi_email_addr": "gleicher@cs.wisc.edu",
   "nsf_id": "000192407",
   "pi_start_date": "2018-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Zinn",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Michael R Zinn",
   "pi_email_addr": "mzinn@wisc.edu",
   "nsf_id": "000169632",
   "pi_start_date": "2018-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Bilge",
   "pi_last_name": "Mutlu",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Bilge D Mutlu",
   "pi_email_addr": "bilge@cs.wisc.edu",
   "nsf_id": "000546805",
   "pi_start_date": "2018-08-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "1210 West Dayton St",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061613",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 749986.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has developed a better understanding of and methods for communicating with robots about physical interactions in order to enable more effective robotic collaborators across all human-robot interaction paradigms. The efforts of this project have provided designs for devices, algorithms and systems that allow people to communicate desired actions to robots and to have awareness of robot behavior. <br /><br />Broader Impact: The results of this project will improve systems where people need to interact with robots performing physical tasks. These advances contribute to the potential use of robots in tasks in human environments, and will contribute to the efforts to bring more widespread use of robots in applications such as light manufacturing, personal assistants, and healthcare.<br /><br />Intellectual Merit: This project has developed algorithmic approaches to creating robot motions that meet the needs of interactive applications. The project has provided experimental evidence for how people sense and specify physical behaviors and how people respond to different types of interactive robot controls. The project has provided methods that use dynamic cameras to help people monitor behavior. The project has shown how new sensor types can be incorporated into interactive robotics systems.&nbsp; The project has demonstrated how physical aspects of manipulation can be inferred from observations.</p><br>\n<p>\n Last Modified: 11/29/2023<br>\nModified by: Michael&nbsp;L&nbsp;Gleicher</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280275855_periscope--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280275855_periscope--rgov-800width.jpg\" title=\"Behaviors in the Periscope System\"><img src=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280275855_periscope--rgov-66x44.jpg\" alt=\"Behaviors in the Periscope System\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Periscope System allows a remote expert to help a user with manual tasks by providing a robot-controlled dynamic camera.</div>\n<div class=\"imageCredit\">Paper Authors</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Gleicher\n<div class=\"imageTitle\">Behaviors in the Periscope System</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280542122_teaser_espresso_demo--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280542122_teaser_espresso_demo--rgov-800width.jpg\" title=\"Inferring Constraints from Demonstrations\"><img src=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280542122_teaser_espresso_demo--rgov-66x44.jpg\" alt=\"Inferring Constraints from Demonstrations\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of our approach to inferring physical constraints from a human demonstration.</div>\n<div class=\"imageCredit\">Paper Authors</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Gleicher\n<div class=\"imageTitle\">Inferring Constraints from Demonstrations</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280852010_camera_teleop_figures_05--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280852010_camera_teleop_figures_05--rgov-800width.jpg\" title=\"Dynamic Camera Telemanipulation\"><img src=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280852010_camera_teleop_figures_05--rgov-66x44.jpg\" alt=\"Dynamic Camera Telemanipulation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of a telemanipulation system with a dynamic camera robot. The user controls one robot while a second robot autonomously directs a camera at it.</div>\n<div class=\"imageCredit\">Paper Authors</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Gleicher\n<div class=\"imageTitle\">Dynamic Camera Telemanipulation</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280775319_Knobs_Capture--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280775319_Knobs_Capture--rgov-800width.jpg\" title=\"Knob Simulation Setup\"><img src=\"/por/images/Reports/POR/2023/1830242/1830242_10572594_1701280775319_Knobs_Capture--rgov-66x44.jpg\" alt=\"Knob Simulation Setup\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our experimental aparatus for experimenting with the perception of forces. We have a real knob as well as motors that can simulate the behavior of different knobs by responsively applying forces.</div>\n<div class=\"imageCredit\">Paper Authors</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Gleicher\n<div class=\"imageTitle\">Knob Simulation Setup</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project has developed a better understanding of and methods for communicating with robots about physical interactions in order to enable more effective robotic collaborators across all human-robot interaction paradigms. The efforts of this project have provided designs for devices, algorithms and systems that allow people to communicate desired actions to robots and to have awareness of robot behavior. \n\nBroader Impact: The results of this project will improve systems where people need to interact with robots performing physical tasks. These advances contribute to the potential use of robots in tasks in human environments, and will contribute to the efforts to bring more widespread use of robots in applications such as light manufacturing, personal assistants, and healthcare.\n\nIntellectual Merit: This project has developed algorithmic approaches to creating robot motions that meet the needs of interactive applications. The project has provided experimental evidence for how people sense and specify physical behaviors and how people respond to different types of interactive robot controls. The project has provided methods that use dynamic cameras to help people monitor behavior. The project has shown how new sensor types can be incorporated into interactive robotics systems. The project has demonstrated how physical aspects of manipulation can be inferred from observations.\t\t\t\t\tLast Modified: 11/29/2023\n\n\t\t\t\t\tSubmitted by: MichaelLGleicher\n"
 }
}