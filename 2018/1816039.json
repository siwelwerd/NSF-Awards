{
 "awd_id": "1816039",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: A Cognitive Framework for Technical, Hard and Explainable Question Answering (THE-QA) with respect to Combined Textual and Visual Inputs",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 499999.0,
 "awd_amount": 515999.0,
 "awd_min_amd_letter_date": "2018-06-25",
 "awd_max_amd_letter_date": "2023-06-05",
 "awd_abstract_narration": "Understanding of visual and textual inputs are important aspects of Artificial Intelligence systems. Often such inputs are presented together to instruct and explain.  As examples, an intelligent robot might learn about its tasks and environment by observing both language and gesture; and an intelligent system addressing scientific questions must interpret figures and diagrams along with text. While there has been a lot of research concerning visual understanding and textual understanding in isolation, there has been very little research that addresses them jointly. This project is developing a framework for answering hard questions about combined visual and textual inputs, and providing supporting explanations. By developing a system that integrates visual and linguistic information for this task, the project could provide the basis for automated tutoring systems in K-12 education, and interpretable interfaces for the workers operating intelligent machines. \r\n\r\nThe project will employ an integrated approach of deep model-based visual recognition and natural language processing, and knowledge representation and reasoning to develop a question answering engine and its components. It will create a challenge corpus that has visual and textual inputs and questions about those inputs given in natural language. It will provide a baseline for semantic image and text parsing and reasoning-based question answering systems. It will develop semantic parsing of non-continuous text items, such as figures, diagrams, and graphs. It will enhance semantic parsing to various formats of natural language text and questions. It will develop methods to acquire knowledge and reasoning with them for answering questions and providing explanations to the answers. Together these contributions of the project will advance Artificial General Intelligence and allow future service robots and personal mobile applications to understand combined visual and textual inputs. The findings from this project will advance the development of knowledge-driven, reasoning-based question answering by filling the current gap on how to efficiently conduct explainable probabilistic reasoning over deep models.  This helps to overcome the fragility of the trained visual and textual understanding models. It will also uncover the intrinsic connections between deep model-based vision and language understanding algorithms and probabilistic knowledge representation and reasoning by exploring a joint solution for answering the hard questions. In general, this project may result in advances in multiple sub-fields of Artificial Intelligence; namely, computer vision, natural language processing, and question answering; and may impact others such as robotics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chitta",
   "pi_last_name": "Baral",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Chitta R Baral",
   "pi_email_addr": "chitta@asu.edu",
   "nsf_id": "000231928",
   "pi_start_date": "2018-06-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yezhou",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yezhou Yang",
   "pi_email_addr": "yz.yang@asu.edu",
   "nsf_id": "000733585",
   "pi_start_date": "2018-06-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "699 S. Mill Avenue",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852813673",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 499999.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>This project that started in 2018 was about joint understanding of text and images together. Until then most datasets treated vision and language modalities separately. Inspired by the \"comprehension\" part of the international PISA test administered to high school students, whose evaluation of students' comprehension included understanding text and images together, the Pis proposed and then developed the VLQA (visuo-linguistic question answering) dataset and published about it in 2020 EMNLP Findings. A key aspect of the data items in this dataset was that to answer a question one had to understand both an image and the associated text; Understanding just one or the other was snot sufficient to answer the associated question. With progress in vision-language models, this aspect has finally caught on and is now referred to as multi-modal understanding or multi-modal question answering and many other datasets have been developed by others; some referring to our initial work. After the initial VLQA dataset the PIs envisioned the importance of interaction between objects in an image and actions that may happen and their impact on those objects and created the CLEVR_HYP dataset that had images from CLEVR and the text included information about hypothetical action sequences that may take places and expected joint understanding of both. This work, first published in NAACL 2021, could be considered a precursor of \"world models\" that is now becoming a popular research trend. Thus, our research results in this project plays an important role on two major future research trends with implications to AI, Computer Science and the broader research community:&nbsp; multi-modal understanding and world models. This project involved more than six PhD students (two female), and four undergraduate (two female) students, at different times. One of the PhD students is now a tenure-track faculty. This project enriched graduate courses in NLP, and Vision at ASU; and it led to other projects through which the PIs were able to build a GPU server infrastructure that is playing a big role in the overall research of the two PIs.</span></p><br>\n<p>\n Last Modified: 12/15/2024<br>\nModified by: Chitta&nbsp;R&nbsp;Baral</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project that started in 2018 was about joint understanding of text and images together. Until then most datasets treated vision and language modalities separately. Inspired by the \"comprehension\" part of the international PISA test administered to high school students, whose evaluation of students' comprehension included understanding text and images together, the Pis proposed and then developed the VLQA (visuo-linguistic question answering) dataset and published about it in 2020 EMNLP Findings. A key aspect of the data items in this dataset was that to answer a question one had to understand both an image and the associated text; Understanding just one or the other was snot sufficient to answer the associated question. With progress in vision-language models, this aspect has finally caught on and is now referred to as multi-modal understanding or multi-modal question answering and many other datasets have been developed by others; some referring to our initial work. After the initial VLQA dataset the PIs envisioned the importance of interaction between objects in an image and actions that may happen and their impact on those objects and created the CLEVR_HYP dataset that had images from CLEVR and the text included information about hypothetical action sequences that may take places and expected joint understanding of both. This work, first published in NAACL 2021, could be considered a precursor of \"world models\" that is now becoming a popular research trend. Thus, our research results in this project plays an important role on two major future research trends with implications to AI, Computer Science and the broader research community: multi-modal understanding and world models. This project involved more than six PhD students (two female), and four undergraduate (two female) students, at different times. One of the PhD students is now a tenure-track faculty. This project enriched graduate courses in NLP, and Vision at ASU; and it led to other projects through which the PIs were able to build a GPU server infrastructure that is playing a big role in the overall research of the two PIs.\t\t\t\t\tLast Modified: 12/15/2024\n\n\t\t\t\t\tSubmitted by: ChittaRBaral\n"
 }
}