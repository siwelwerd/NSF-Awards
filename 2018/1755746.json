{
 "awd_id": "1755746",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CHS: Bridging the Age-Related Performance Gap: Multimodal Interfaces to Support Older Adults in Transitioning to Manual Control in Autonomous Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2018-03-26",
 "awd_max_amd_letter_date": "2018-03-26",
 "awd_abstract_narration": "Advanced autonomous systems have the potential to significantly reduce workload and extend human capabilities in a number of safety-critical transportation and work environments, such as driving, medicine, and manufacturing.  However, even the most sophisticated systems are often constrained by design limits and/or may experience occasional malfunctions requiring human-in-the-loop manual interventions.  To date, there is no consensus on how best to assist operators of varying age and ability levels in noticing, diagnosing, and recovering manual control across a wide range of autonomous systems.  Adults 65 years and older are now the fastest growing age group, and are expected to encounter systems with increasing levels of automation throughout later stages of life, yet perceptual and cognitive challenges often prevent their effective use of such technology.  The goals of this project are to better understand age-related differences in human-automation interactions, and to begin the development of methods and tools that support the manual recovery of older adults for various automated technologies.  Combined sensory feedback will be explored as a potential technique to these ends, as it has been shown to improve attention management and benefit older individuals.  Project outcomes will contribute to a more in-depth understanding of the capabilities and limitations of different operator demographics, and will help guide the development of next generation human-machine interfaces.  The work has broader implications for enhancing safety in many complex operations, such as autonomous driving and automated process assembly.  Public educational activities will include community and study population (senior) focused workshops, pre-college and summer outreach, and undergraduate research programs for underrepresented students.\r\n\r\nMultimodal interfaces present information to the visual, auditory, and tactile sensory channels.  By manipulating signal parameters, these interfaces are able to capture attention, inform operators of system status, and provide decision aids to perform needed actions.  However, the extent to which this approach can effectively communicate to a range of operators with considerable variability in sensory and cognitive abilities in the context of transfer-of-authority has not been quantified.  Given the rapid development of advanced autonomous technology and the projected population changes expected within the next decade, it will be critical to fill these research gaps.  This project will generate age-related empirical data on complex interactions within autonomous systems.  A series of experiments will be conducted using semi-autonomous driving simulations and involving subjects from different age groups.  The research will quantify age-related time differences in noticing multimodal transition (takeover) requests, determine age-specific transition times as a function of lead time and sensory modality notification, and investigate the effectiveness of various tactile signals to support situation awareness and reduce transition times.  Results are expected to inform quantitative and qualitative models of human perception, information processing, and performance.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brandon",
   "pi_last_name": "Pitts",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brandon Pitts",
   "pi_email_addr": "bjpitts@purdue.edu",
   "nsf_id": "000752324",
   "pi_start_date": "2018-03-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "315 N. Grant Street",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072023",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Autonomous systems and intelligent technologies have penetrated nearly every area of human life, especially complex work, transportation, and healthcare environments. At the same time, adults aged 65 years and older have become the fastest-growing age group worldwide. Many technologies are not designed to consider characteristics of older adults, such as perceptual, cognitive, and physical challenges experienced in older age. This lack of consideration can prevent aging populations from being able to effectively and safely use emerging technologies.</p>\n<p>The goals of this project were to characterize differences in how older and younger adults interact with automated systems and explore display designs that could improve older adults&rsquo; collaborations with automation. The specific research objectives were to 1) quantify age differences in noticing requests to take (manual) control from an automated system, 2) determine machine-to-human takeover performance as a function of age and takeover notification format, and 3) investigate whether vibrotactile displays could assist in the takeover process. A series of human-subject experiments were conducted using automated driving as the testbed.</p>\n<p>Overall, we found that older adults took longer to notice system takeover notifications compared to younger adults. However, takeover requests presented in combined visual, auditory, and tactile formats were associated with faster response times, especially for older adults. When given the opportunity to engage with a non-driving-related task as the automation controlled the vehicle, older adults chose to instead pay more attention to the driving environment compared to younger adults. With respect to takeover performance, older adults who frequently exercised in their daily lives were better able to control the vehicle once they took over from the automation. Finally, vibrotactile displays integrated into the vehicle's seat back and seat pan showed promise for assisting drivers in taking over. Findings suggest that informative vibrotactile signals (which displayed information about the driving environment) may better help drivers to take over compared to instructional vibrotactile signals (which only showed which maneuver to make). &nbsp;</p>\n<p>This work has significant broader implications for enhancing the usability of technologies in many complex environments and, thus, promoting safety in operations such as autonomous driving and automated manufacturing. Project results have been disseminated via several conference presentations, workshops, and peer-reviewed publications. Five undergraduate students were supported by this grant, two of whom were underrepresented minority (URM) students. Finally, this research engaged older adult communities both as participants in lab studies and as beneficiaries of technological support through a community service program focused on bridging the age-related digital divide.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/28/2022<br>\n\t\t\t\t\tModified by: Brandon&nbsp;Pitts</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAutonomous systems and intelligent technologies have penetrated nearly every area of human life, especially complex work, transportation, and healthcare environments. At the same time, adults aged 65 years and older have become the fastest-growing age group worldwide. Many technologies are not designed to consider characteristics of older adults, such as perceptual, cognitive, and physical challenges experienced in older age. This lack of consideration can prevent aging populations from being able to effectively and safely use emerging technologies.\n\nThe goals of this project were to characterize differences in how older and younger adults interact with automated systems and explore display designs that could improve older adults\u2019 collaborations with automation. The specific research objectives were to 1) quantify age differences in noticing requests to take (manual) control from an automated system, 2) determine machine-to-human takeover performance as a function of age and takeover notification format, and 3) investigate whether vibrotactile displays could assist in the takeover process. A series of human-subject experiments were conducted using automated driving as the testbed.\n\nOverall, we found that older adults took longer to notice system takeover notifications compared to younger adults. However, takeover requests presented in combined visual, auditory, and tactile formats were associated with faster response times, especially for older adults. When given the opportunity to engage with a non-driving-related task as the automation controlled the vehicle, older adults chose to instead pay more attention to the driving environment compared to younger adults. With respect to takeover performance, older adults who frequently exercised in their daily lives were better able to control the vehicle once they took over from the automation. Finally, vibrotactile displays integrated into the vehicle's seat back and seat pan showed promise for assisting drivers in taking over. Findings suggest that informative vibrotactile signals (which displayed information about the driving environment) may better help drivers to take over compared to instructional vibrotactile signals (which only showed which maneuver to make).  \n\nThis work has significant broader implications for enhancing the usability of technologies in many complex environments and, thus, promoting safety in operations such as autonomous driving and automated manufacturing. Project results have been disseminated via several conference presentations, workshops, and peer-reviewed publications. Five undergraduate students were supported by this grant, two of whom were underrepresented minority (URM) students. Finally, this research engaged older adult communities both as participants in lab studies and as beneficiaries of technological support through a community service program focused on bridging the age-related digital divide. \n\n \n\n\t\t\t\t\tLast Modified: 11/28/2022\n\n\t\t\t\t\tSubmitted by: Brandon Pitts"
 }
}