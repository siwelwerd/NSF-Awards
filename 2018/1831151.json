{
 "awd_id": "1831151",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase II:  In-Memory Artificial Neural Network",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928323",
 "po_email": "bschrag@nsf.gov",
 "po_sign_block_name": "Benaiah Schrag",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 760000.0,
 "awd_min_amd_letter_date": "2018-09-04",
 "awd_max_amd_letter_date": "2019-11-19",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is provided by a novel data processing architecture, utilizing high-parallel in-memory computing for certain recurring and data intensive functions. Traditional computer architecture funnels all data through the central processing unit (CPU). Multiple CPU cores and very high clock frequencies are used to address the issue of ever increasing demands on data processing capability. However, the transportation capacity of data between memory and CPU cores has become a limiting factor creating a 'memory bottleneck'. This limitation is most noticeable in the recent and rapid development of artificial intelligence applications which deploy so called neuromorphic computing techniques, which in turn require a very high parallelism in computation and proportional demands on memory bandwidth. This project performs key repetitive operations within the memory itself, leveraging the inherent parallelism of the memory architecture, thereby avoiding a large percentage of the data transport otherwise required. The resulting elimination of the memory bottleneck provides a path forward for high complexity neuromorphic computing applications such as autonomous navigation used for self-driving cars. Reduced demands on data transport and CPU also significantly reduce power consumption, enabling a wide variety of mobile artificial intelligence applications.\r\n\r\nThe proposed project investigates the system level integration challenges of a memory-centric neuromorphic computing approach, and aims to demonstrate a seamless integration with existing software platforms currently using traditional neuromorphic computing processors. It is important for a novel hardware platform to be compatible with existing software in order to lower barriers to market entry. This Phase II project also develops the actual semiconductor product which has been investigated in Phase I as a feasibility demonstrator. The Phase II product is based on a non-volatile high density memory architecture, and as such is expected to provide the full capability in terms of both power and operations per second. Once the hardware is available in the second half of the project, these key parameters will be thoroughly characterized and benchmarked against the current state of the art technology. A projection will be made outlining the future scaling potential using ultra high density volatile and non-volatile memory geared towards high complexity neuromorphic computing beyond what is currently possible using existing approaches.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Wolfgang",
   "pi_last_name": "Hokenmaier",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wolfgang Hokenmaier",
   "pi_email_addr": "whokenmaier@greenmountainsemi.com",
   "nsf_id": "000710820",
   "pi_start_date": "2018-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Green Mountain Semiconductor, Inc.",
  "inst_street_address": "28 HOWARD ST",
  "inst_street_address_2": "STE 301",
  "inst_city_name": "BURLINGTON",
  "inst_state_code": "VT",
  "inst_state_name": "Vermont",
  "inst_phone_num": "8023438175",
  "inst_zip_code": "054015986",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "VT00",
  "org_lgl_bus_name": "GREEN MOUNTAIN SEMICONDUCTOR, INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "JRJTZGPZ16M3"
 },
 "perf_inst": {
  "perf_inst_name": "Green Mountain Semiconductor Inc.",
  "perf_str_addr": "182 Main Street Suite 304",
  "perf_city_name": "Burlington",
  "perf_st_code": "VT",
  "perf_st_name": "Vermont",
  "perf_zip_code": "054018349",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "VT00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "097E",
   "pgm_ref_txt": "High Freq Devices & Circuits"
  },
  {
   "pgm_ref_code": "5373",
   "pgm_ref_txt": "SMALL BUSINESS PHASE II"
  },
  {
   "pgm_ref_code": "8035",
   "pgm_ref_txt": "Hardware Devices"
  },
  {
   "pgm_ref_code": "8240",
   "pgm_ref_txt": "SBIR/STTR CAP"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 750000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 10000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-de08b3cd-7fff-8fda-2bbe-5e8f113bdb2b\"> </span></p>\n<p dir=\"ltr\"><span>Green Mountain Semiconductor, Inc. has developed intellectual property for general in-memory acceleration of artificial intelligence algorithms. By embedding this circuitry on the same die as the memory technology, we can reduce chip-to-chip transactions required for the fundamental multiply-accumulate operation, the basic mathematical operation needed to evaluate artificial neural networks. The amount of power needed to transfer data from one chip to another is becoming an increasing problem. New interface standards, packaging techniques and other unique designs all try to reduce the amount of picojoules per bit to the smallest number possible.&nbsp; The trends for the latest artificial neural network algorithms also suggest memory needs for these networks are growing significantly. This will increase the amount of data that must be transferred to and from traditional memory chips, and thus returns from in-memory computation will continue to scale in ways that conventional ASICs cannot replicate.</span></p>\n<p dir=\"ltr\"><span>While there are many parameters to evaluate the performance of artificial neural network hardware (operations per second, operations per second per watt, latency, weight size, batch size, training vs inference, form factor and absolute power are but a few) the in-memory approach allows a new set of tradeoffs that are not available to a chip architect who is limited by a logic process, and in many ways these tradeoffs can lead to a superior design based on the needs of the end application.&nbsp;</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span>Initially we targeted our effort towards highly parallel in-memory search capabilities, leveraging the very high bandwidth inherent in the memory array archiecture, but normally limited by input and output channel limitations. By processing the data without moving it, we garner not only the maximum bandwidth possible, but also the power savings inherent in the avoidance of data transport. Target applications include the indexing of very large amounts of sensor or communication data (Big Data) and genome sequencing algorithms.</p>\n<p dir=\"ltr\">Having concluded the characterization, optimization and architecture for the high parallel in-memory search IP and implemented in tested hardware funded by the Phase I project, we concluded that we were only one step away from also integrating key AI algorithms along the same premise. We applied for and received Phase II funding to move to this next goal. The initial Phase II funded design was a smaller edge-computing chip using non-volatile phase change memory, which we then ported to a large scale, high density DRAM technology.</p>\n<p dir=\"ltr\">When deviating from the classic von-Neumann computer architecture, as we are doing, it is important to also enable the usage of this new hardware so that a software engineer can make use of it as seemlessly as possible. To this end we developed software to compile trained models for platforms into a hardware-compatible data set, which can then be uploaded to our in-memory AI inferencing computers.</p>\n<p dir=\"ltr\">To further enhance the capabilities for very large AI applications, we developed a control algorithm allowing the network to be distributed across multiple memory chips.</p>\n<p dir=\"ltr\">Finally, we performed detailed power consumption comparisons and projections, benchmarking our technology against existing solutions trying to mitigate the AI memory bottleneck with CMOS logic solutions. We found that while some applications are achieving sufficient performance via various workarounds such as \"batching\", the in-memory approach will increasingly outperform the von-Neumann memory usage as overall memory demands continue to scale.&nbsp;</p>\n<p dir=\"ltr\">Our architecture will benefit further from software development to integrate in-memory computing into high-level software platforms, and from advanced packaging technologies such as 2.5D and 3D, which will further enhance capabilities. Artificial intelligence has many beneficial applications, and our IP will contribute to making this a sustainable reality and future.</p>\n<p><br /><br /></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/05/2022<br>\n\t\t\t\t\tModified by: Wolfgang&nbsp;Hokenmaier</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640747427414_IMG_6018--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640747427414_IMG_6018--rgov-800width.jpg\" title=\"Bach version 1\"><img src=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640747427414_IMG_6018--rgov-66x44.jpg\" alt=\"Bach version 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Phase I test chip for high parallel in-memory computing</div>\n<div class=\"imageCredit\">Wolfgang Hokenmaier</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wolfgang&nbsp;Hokenmaier</div>\n<div class=\"imageTitle\">Bach version 1</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640746261988_Figure5_new--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640746261988_Figure5_new--rgov-800width.jpg\" title=\"Tiersen II advanced packaging\"><img src=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640746261988_Figure5_new--rgov-66x44.jpg\" alt=\"Tiersen II advanced packaging\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">In-memory AI high density memory, combined with advanced packaging technology for minimum power consumption, maximum bandwidth.</div>\n<div class=\"imageCredit\">Wolfgang Hokenmaier</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wolfgang&nbsp;Hokenmaier</div>\n<div class=\"imageTitle\">Tiersen II advanced packaging</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640745109959_Tiersen_Image--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640745109959_Tiersen_Image--rgov-800width.jpg\" title=\"Tiersen I\"><img src=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640745109959_Tiersen_Image--rgov-66x44.jpg\" alt=\"Tiersen I\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Tiersen I in-memory AI accelerator based on 2Mb non-volatile phase change memory (PCM)</div>\n<div class=\"imageCredit\">Wolfgang Hokenmaier</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wolfgang&nbsp;Hokenmaier</div>\n<div class=\"imageTitle\">Tiersen I</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640747650456_IMG_5779--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640747650456_IMG_5779--rgov-800width.jpg\" title=\"Device under test\"><img src=\"/por/images/Reports/POR/2021/1831151/1831151_10579539_1640747650456_IMG_5779--rgov-66x44.jpg\" alt=\"Device under test\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our NSF-funded in-memory computation device under test in our lab</div>\n<div class=\"imageCredit\">Wolfgang Hokenmaier</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wolfgang&nbsp;Hokenmaier</div>\n<div class=\"imageTitle\">Device under test</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1831151/1831151_10579539_1641411131600_Multi-chipnetwork--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1831151/1831151_10579539_1641411131600_Multi-chipnetwork--rgov-800width.jpg\" title=\"Multi-chip netwok\"><img src=\"/por/images/Reports/POR/2022/1831151/1831151_10579539_1641411131600_Multi-chipnetwork--rgov-66x44.jpg\" alt=\"Multi-chip netwok\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Expansion of complex neural networks across multiple in-memory inferencing machines</div>\n<div class=\"imageCredit\">Angela Johnson</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wolfgang&nbsp;Hokenmaier</div>\n<div class=\"imageTitle\">Multi-chip netwok</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nGreen Mountain Semiconductor, Inc. has developed intellectual property for general in-memory acceleration of artificial intelligence algorithms. By embedding this circuitry on the same die as the memory technology, we can reduce chip-to-chip transactions required for the fundamental multiply-accumulate operation, the basic mathematical operation needed to evaluate artificial neural networks. The amount of power needed to transfer data from one chip to another is becoming an increasing problem. New interface standards, packaging techniques and other unique designs all try to reduce the amount of picojoules per bit to the smallest number possible.  The trends for the latest artificial neural network algorithms also suggest memory needs for these networks are growing significantly. This will increase the amount of data that must be transferred to and from traditional memory chips, and thus returns from in-memory computation will continue to scale in ways that conventional ASICs cannot replicate.\nWhile there are many parameters to evaluate the performance of artificial neural network hardware (operations per second, operations per second per watt, latency, weight size, batch size, training vs inference, form factor and absolute power are but a few) the in-memory approach allows a new set of tradeoffs that are not available to a chip architect who is limited by a logic process, and in many ways these tradeoffs can lead to a superior design based on the needs of the end application. \n Initially we targeted our effort towards highly parallel in-memory search capabilities, leveraging the very high bandwidth inherent in the memory array archiecture, but normally limited by input and output channel limitations. By processing the data without moving it, we garner not only the maximum bandwidth possible, but also the power savings inherent in the avoidance of data transport. Target applications include the indexing of very large amounts of sensor or communication data (Big Data) and genome sequencing algorithms.\nHaving concluded the characterization, optimization and architecture for the high parallel in-memory search IP and implemented in tested hardware funded by the Phase I project, we concluded that we were only one step away from also integrating key AI algorithms along the same premise. We applied for and received Phase II funding to move to this next goal. The initial Phase II funded design was a smaller edge-computing chip using non-volatile phase change memory, which we then ported to a large scale, high density DRAM technology.\nWhen deviating from the classic von-Neumann computer architecture, as we are doing, it is important to also enable the usage of this new hardware so that a software engineer can make use of it as seemlessly as possible. To this end we developed software to compile trained models for platforms into a hardware-compatible data set, which can then be uploaded to our in-memory AI inferencing computers.\nTo further enhance the capabilities for very large AI applications, we developed a control algorithm allowing the network to be distributed across multiple memory chips.\nFinally, we performed detailed power consumption comparisons and projections, benchmarking our technology against existing solutions trying to mitigate the AI memory bottleneck with CMOS logic solutions. We found that while some applications are achieving sufficient performance via various workarounds such as \"batching\", the in-memory approach will increasingly outperform the von-Neumann memory usage as overall memory demands continue to scale. \nOur architecture will benefit further from software development to integrate in-memory computing into high-level software platforms, and from advanced packaging technologies such as 2.5D and 3D, which will further enhance capabilities. Artificial intelligence has many beneficial applications, and our IP will contribute to making this a sustainable reality and future.\n\n\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/05/2022\n\n\t\t\t\t\tSubmitted by: Wolfgang Hokenmaier"
 }
}