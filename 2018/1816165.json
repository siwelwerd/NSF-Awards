{
 "awd_id": "1816165",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI:Small: Nonlinear signal representations for speech applications",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 336814.0,
 "awd_amount": 336814.0,
 "awd_min_amd_letter_date": "2018-08-02",
 "awd_max_amd_letter_date": "2018-08-02",
 "awd_abstract_narration": "Human speech is a very rich signal. In addition to words, it contains several kinds of important information about the speaker such as identity, gender, age, native language, dialect, and emotion. It also provides information about the transmission channel and environment; for example, whether the speech came from a phone call or a high-fidelity recording, and whether or not there was background noise.   This project aims to create a powerful uniform representation that reflects all the information carried by speech. Such representation would enable important speech applications in multiple sectors of society: commercial (security, healthcare, user interfaces), government (security, information filtering), and law enforcement (speaker identification, forensics).\r\n\r\nIn this project, Johns Hopkins University researchers, who invented the original i-vector framework, intend to progress beyond the linear i-vector approach by investigating non-linear models with the expectation to better explain the complex structure of speech. To achieve this goal, two different models are investigated. First, a non-linear i-vector version is explored. In this method, the speech signal distribution is modeled by a Gaussian mixture model (GMM). The super-vector formed by the GMM means is a non-linear function (neural network) of a latent variable (speech representation). The parameters of the neural network and the latent representation can be jointly estimated by stochastic gradient descent iterations. Secondly, the team intends to investigate different types of auto-encoder networks (AE, VAE, RBM, DBM) to obtain representations from their hidden layers. Preliminary research shows that it is feasible to obtain good representations by combining activations from several hidden layers. Visualization tools are used to understand how the speech data have been represented and structured. By understanding the non-linear relationships created via the auto-encoder network modeling and using the visualization tools, there is potential to produce valuable insights into speech modeling. These insights can help cognitive science and neuroscience researchers to understand how the brain represents speech signals. The proposed methods are developed as software that takes a speech segment as input and generates a single vector that may be used to characterize the segment for the important applications mentioned in the previous paragraph.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Najim",
   "pi_last_name": "Dehak",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Najim Dehak",
   "pi_email_addr": "ndehak3@jhu.edu",
   "nsf_id": "000724141",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jesus",
   "pi_last_name": "Villalba Lopez",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Jesus A Villalba Lopez",
   "pi_email_addr": "jesus.antonio.villalba@gmail.com",
   "nsf_id": "000764206",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N. Charles St.",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182686",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 336814.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Lately, speech applications have been using speech embeddings and representations to encode information for various tasks such as speech, speaker, language, and emotion recognition. These embeddings can be trained in a supervised or unsupervised manner to encode specific information or the majority of speech variability. Our project aimed to learn nonlinear representations that are useful for speech processing applications, especially for tasks with limited data. We explored using supervised and unsupervised speech embeddings and tested them on different tasks.</p>\n<p>One key outcome of our project is developing a new method similar to the factor analysis-based i-vector representation. However, instead of training it with an expectation-maximization algorithm, we trained it with stochastic gradient descent. This new approach allows any differentiable assumptions in our framework to be used, which can extend the linear factor analysis representation to model the non-linear behavior of the data. Our experiments found that the proposed new method outperforms the classical i-vector representation method on a few speaker recognition tasks.</p>\n<p>The second outcome is the investigation into discriminatively learned representations for speaker and language recognition, known as \"x-vectors.\" These deep neural networks consist of an encoder network, global temporal pooling, and classification layer and are trained using cross-entropy objectives. The study tested various components, including encoder networks, pooling methods, and objective functions, to achieve optimal performance on challenging benchmarks. We also investigated if the supervised nonlinear speaker embedding trained on a large labeled dataset can be used as a good prior or initialization to train an embedding for other speech tasks such as speech emotion recognition and Parkinson detection. These two tasks have the characteristic of suffering from limited data. We found that using supervised speaker embedding such as x-vector and transfer learning approach are a very suitable solution to avoid overfitting for emotion recognition and Parkinson's detection. In addition, to use the x-vector as the prior model, we also answered the following question: Does the change of emotion affect speaker verification performance? Our experiments revealed that speaker verification performance is sensitive to speakers' emotional mismatch.</p>\n<p>The last outcome consists of unsupervised deep neural networks learning for speech representation. Unsupervised learning methods can help improve speech systems because labeling speech data is expensive. One successful method is contrastive self-supervised learning, which requires finding negative examples to contrast with the current example. However, this can be difficult without labels. We explore the use of DIstillation with NO Labels (DINO), initially proposed for computer vision, in the speech domain. DINO does not require negative sampling and was used to create embeddings for several speech tasks, including speaker verification and emotion recognition. The DINO representation outperforms other self-supervised learning methods in speaker verification and emotion recognition, showing its generalizability to different speech applications.</p>\n<p>This project facilitated&nbsp;the creation of the open-source Hyperion toolkit (<a href=\"https://github.com/hyperion-ml/hyperion\">https://github.com/hyperion-ml/hyperion</a>). This toolkit contains a library of tools and neural network architectures for a wide variety of non-linear representations. Among them, it includes supervised x-vector variants like Time Delay Neural Network (TDNN), Emphasized Channel Attention, Propagation and Aggregation in TDNN,<strong>&nbsp;</strong>Residual Neural Network&nbsp;(ResNet)&nbsp;and Res2Net x-vectors. Also, unsupervised methods like DINO, which was investigated in the last part of the project. The toolkit also contains step-by-step recipes to reproduce&nbsp;results in several speaker verification tasks, like VoxCeleb, NIST SRE19, SRE21, VOICES; language recognition tasks like LRE22, and diarization like CallHome.</p>\n<p>This project involved several Ph.D. students, most of whom have already graduated. An undergraduate student who worked in our lab is now pursuing a Ph.D. at another institution. Additionally, we had the privilege of hosting and mentoring several female students from Baltimore City College who participated in our lab through the Women in Science and Engineering program. During their stay with us, they gained knowledge about speech processing and learned how to represent and extract information from speech signals.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/18/2023<br>\n\t\t\t\t\tModified by: Najim&nbsp;Dehak</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nLately, speech applications have been using speech embeddings and representations to encode information for various tasks such as speech, speaker, language, and emotion recognition. These embeddings can be trained in a supervised or unsupervised manner to encode specific information or the majority of speech variability. Our project aimed to learn nonlinear representations that are useful for speech processing applications, especially for tasks with limited data. We explored using supervised and unsupervised speech embeddings and tested them on different tasks.\n\nOne key outcome of our project is developing a new method similar to the factor analysis-based i-vector representation. However, instead of training it with an expectation-maximization algorithm, we trained it with stochastic gradient descent. This new approach allows any differentiable assumptions in our framework to be used, which can extend the linear factor analysis representation to model the non-linear behavior of the data. Our experiments found that the proposed new method outperforms the classical i-vector representation method on a few speaker recognition tasks.\n\nThe second outcome is the investigation into discriminatively learned representations for speaker and language recognition, known as \"x-vectors.\" These deep neural networks consist of an encoder network, global temporal pooling, and classification layer and are trained using cross-entropy objectives. The study tested various components, including encoder networks, pooling methods, and objective functions, to achieve optimal performance on challenging benchmarks. We also investigated if the supervised nonlinear speaker embedding trained on a large labeled dataset can be used as a good prior or initialization to train an embedding for other speech tasks such as speech emotion recognition and Parkinson detection. These two tasks have the characteristic of suffering from limited data. We found that using supervised speaker embedding such as x-vector and transfer learning approach are a very suitable solution to avoid overfitting for emotion recognition and Parkinson's detection. In addition, to use the x-vector as the prior model, we also answered the following question: Does the change of emotion affect speaker verification performance? Our experiments revealed that speaker verification performance is sensitive to speakers' emotional mismatch.\n\nThe last outcome consists of unsupervised deep neural networks learning for speech representation. Unsupervised learning methods can help improve speech systems because labeling speech data is expensive. One successful method is contrastive self-supervised learning, which requires finding negative examples to contrast with the current example. However, this can be difficult without labels. We explore the use of DIstillation with NO Labels (DINO), initially proposed for computer vision, in the speech domain. DINO does not require negative sampling and was used to create embeddings for several speech tasks, including speaker verification and emotion recognition. The DINO representation outperforms other self-supervised learning methods in speaker verification and emotion recognition, showing its generalizability to different speech applications.\n\nThis project facilitated the creation of the open-source Hyperion toolkit (https://github.com/hyperion-ml/hyperion). This toolkit contains a library of tools and neural network architectures for a wide variety of non-linear representations. Among them, it includes supervised x-vector variants like Time Delay Neural Network (TDNN), Emphasized Channel Attention, Propagation and Aggregation in TDNN, Residual Neural Network (ResNet) and Res2Net x-vectors. Also, unsupervised methods like DINO, which was investigated in the last part of the project. The toolkit also contains step-by-step recipes to reproduce results in several speaker verification tasks, like VoxCeleb, NIST SRE19, SRE21, VOICES; language recognition tasks like LRE22, and diarization like CallHome.\n\nThis project involved several Ph.D. students, most of whom have already graduated. An undergraduate student who worked in our lab is now pursuing a Ph.D. at another institution. Additionally, we had the privilege of hosting and mentoring several female students from Baltimore City College who participated in our lab through the Women in Science and Engineering program. During their stay with us, they gained knowledge about speech processing and learned how to represent and extract information from speech signals.\n\n\t\t\t\t\tLast Modified: 04/18/2023\n\n\t\t\t\t\tSubmitted by: Najim Dehak"
 }
}