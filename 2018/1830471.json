{
 "awd_id": "1830471",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI:INT: Ad-Hoc Collaborative Human-Robot Swarms",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 1490568.0,
 "awd_amount": 1490568.0,
 "awd_min_amd_letter_date": "2018-08-18",
 "awd_max_amd_letter_date": "2018-08-18",
 "awd_abstract_narration": "This project addresses the challenge of creating ad-hoc collaborative teams (swarms) of robots and people, where the people are not robotics experts and are interacting with the robots to accomplish an ad-hoc task such as emergency evacuation. The project investigates what the robots should look like, how they should behave, and how they should interact with people without prior experience with robots, to make sure the overall task is accomplished. The scientific insights of this project can apply to different tasks such as crowd control and large scale search missions. Here the research is grounded in a series of room and building evacuation scenarios where robots work autonomously, provide information and guidance, and respond to people's requests and behaviors. This project will impact three graduate classes taught by the researchers, will support undergraduate and masters projects and the results will be incorporated into activities targeted at increasing the participation of groups traditionally underrepresented in engineering.    \r\n\r\nThis project advances knowledge in three subfields of robotics and especially at their intersection; swarm design, autonomy, and human-swarm interaction. The challenge includes development of hardware platforms that can traverse and modify their environment and permit safe human-robot interaction, while remaining inexpensive, robust, and low maintenance. Specifically, this will involve blimps to facilitate and provide broad situational awareness by projecting images on the ground, and ground robots with inflatable, interactive bladders. For autonomy, this project considers a top-down approach where individual robot control is synthesized from a high-level task. In addition, algorithms will be developed that enable robots to automatically modify their behavior and provide intuitive feedback based on interactions with humans, the humans' behaviors, and the observed environments. This project will contribute to the field of Human-Robot Interaction by designing and evaluating distributed interactions where the role of the human fluidly changes between being a leader providing instructions to the robots, a cooperative swarm member guided by the robots, an uncooperative member who acts in opposition to the swarm guidance, and a passive member who needs to be physically assisted. Evaluating the human-swarm behavior using established metrics in a variety of increasingly complex environments will provide unique insights into the opportunities and challenges of creating ad-hoc human-robot swarms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hadas",
   "pi_last_name": "Kress Gazit",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hadas Kress Gazit",
   "pi_email_addr": "hadaskg@cornell.edu",
   "nsf_id": "000521463",
   "pi_start_date": "2018-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Guy",
   "pi_last_name": "Hoffman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Guy Hoffman",
   "pi_email_addr": "hoffman@cornell.edu",
   "nsf_id": "000718306",
   "pi_start_date": "2018-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kirstin",
   "pi_last_name": "Petersen",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Kirstin H Petersen",
   "pi_email_addr": "kirstin@cornell.edu",
   "nsf_id": "000727830",
   "pi_start_date": "2018-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "124 Hoy Road",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148537501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 1490568.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project innovated in three aspects related to robots interacting with people for the first time under stressful conditions, for example, robots assisting during a building evacuation scenario. The three directions are: novel hardware for interaction, interaction interfaces, and robot autonomy.</p>\r\n<p><strong>Hardware</strong>: We designed a lightweight wheeled robot that has a fabric bladder on top of it that can be inflated and deflated autonomously. Inside the bladder, we mounted a projector and a camera that allow the robot to display images on the bladder and detect shadows created by the people interacting with the robots.</p>\r\n<p><strong>Interaction</strong>: We created ShadowSense, a shadow-based interaction system where people interact with the robot&rsquo;s inflatable bladder. Specifically, the camera detects the shadow the person makes, and the system classifies the shadow as one of a set of different gestures that can be assigned meaning by the system designer. This novel type of interaction allows a user to instruct a system (in this project&rsquo;s case, interact with a robot) while preserving the privacy of the person, as opposed to other visual of auditory interfaces. &nbsp;This work was featured in several media articles, including in a National Geographic story \"The Science of Touch\", and New Atlas &ldquo;ShadowSense tech tracks shadows to give robots a sense of touch&rdquo;.</p>\r\n<p><strong>Robot autonomy</strong>: We created a mathematically precise language, based on temporal logic, that allows a person to describe at a high-level, desired robot behavior. Given such a description, we created methods to automatically transform the requirements into robot behavior without needing an engineer to program the robot. In addition to automatically generating robot control, we are able to provide feedback when the required behavior cannot be achieved and we can incorporate &nbsp;on-the-fly changes in the requirements.</p>\r\n<p>We tested the autonomous robot in user studies, both in an online simulation and in a physical environment, simulating a building evacuation. The users studies resulted in insights regarding the type of behavior the robot should exhibit to ensure faster evacuation, and how people react to robot failures.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/01/2025<br>\nModified by: Hadas&nbsp;Kress Gazit</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project innovated in three aspects related to robots interacting with people for the first time under stressful conditions, for example, robots assisting during a building evacuation scenario. The three directions are: novel hardware for interaction, interaction interfaces, and robot autonomy.\r\n\n\nHardware: We designed a lightweight wheeled robot that has a fabric bladder on top of it that can be inflated and deflated autonomously. Inside the bladder, we mounted a projector and a camera that allow the robot to display images on the bladder and detect shadows created by the people interacting with the robots.\r\n\n\nInteraction: We created ShadowSense, a shadow-based interaction system where people interact with the robots inflatable bladder. Specifically, the camera detects the shadow the person makes, and the system classifies the shadow as one of a set of different gestures that can be assigned meaning by the system designer. This novel type of interaction allows a user to instruct a system (in this projects case, interact with a robot) while preserving the privacy of the person, as opposed to other visual of auditory interfaces. This work was featured in several media articles, including in a National Geographic story \"The Science of Touch\", and New Atlas ShadowSense tech tracks shadows to give robots a sense of touch.\r\n\n\nRobot autonomy: We created a mathematically precise language, based on temporal logic, that allows a person to describe at a high-level, desired robot behavior. Given such a description, we created methods to automatically transform the requirements into robot behavior without needing an engineer to program the robot. In addition to automatically generating robot control, we are able to provide feedback when the required behavior cannot be achieved and we can incorporate on-the-fly changes in the requirements.\r\n\n\nWe tested the autonomous robot in user studies, both in an online simulation and in a physical environment, simulating a building evacuation. The users studies resulted in insights regarding the type of behavior the robot should exhibit to ensure faster evacuation, and how people react to robot failures.\r\n\n\n\t\t\t\t\tLast Modified: 01/01/2025\n\n\t\t\t\t\tSubmitted by: HadasKress Gazit\n"
 }
}