{
 "awd_id": "1816627",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: From acoustics to semantics:  Embedding speech for a hierarchy of tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 449984.0,
 "awd_amount": 449984.0,
 "awd_min_amd_letter_date": "2018-08-02",
 "awd_max_amd_letter_date": "2018-09-13",
 "awd_abstract_narration": "There is an increasingly large array of spoken language interfaces available, such as virtual assistants and telephone customer service interfaces.  These technologies both (1) recognize the words spoken by a user and (2) extract actionable information, such as the topic of the user's query and the degree of match between the query and documents in a database.  Such applications are typically treated as a pipeline of automatic speech transcription followed by text processing to extract the meaning.  This project aims to develop technology that directly extracts meaning from speech, while using a variety of linguistic information along the way.  This approach is intended to mitigate the effects of speech recognition errors, as well as to use all of the meaning-bearing information in speech, such as intonation.  This work is expected to have long-term broad impact through technological advances, as well as immediate broad impact through the PI's involvement in local schools and mentoring for a diverse set of visiting students.\r\n\r\nThe technical goals of this work are (1) to do high-quality natural language processing directly on speech; (2) to seamlessly integrate domain knowledge into end-to-end speech models; (3) improve the performance-vs.-resources tradeoff; and (4) develop models for embedding arbitrary speech signals into meaning-bearing representations.  The process of mapping from speech to meaning can be viewed as a hierarchy of tasks, from the most basic acoustic-phonetic tasks to the deepest semantic tasks.  The experimental work will focus on two task hierarchies:  a \"retrieval\" hierarchy including query-by-example search, keyword spotting, semantic speech search; and a \"recognition\" hierarchy including phonetic recognition, word recognition, parsing, and topic identification.  The main technical approaches to be developed include hierarchical multitask learning methods for incorporating domain knowledge and mitigating low-data settings, as well as new models for acoustic-semantic speech embedding.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Karen",
   "pi_last_name": "Livescu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Karen Livescu",
   "pi_email_addr": "klivescu@ttic.edu",
   "nsf_id": "000512036",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Toyota Technological Institute at Chicago",
  "inst_street_address": "6045 S KENWOOD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7738340409",
  "inst_zip_code": "606372803",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO",
  "org_prnt_uei_num": "ERBJF4DMW6G4",
  "org_uei_num": "ERBJF4DMW6G4"
 },
 "perf_inst": {
  "perf_inst_name": "Toyota Technological Institute at Chicago",
  "perf_str_addr": "6045 S. Kenwood Ave.",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606372803",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 449984.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>There is an increasingly large array of spoken language interfaces that are commercially available, such as virtual assistants and telephone customer service interfaces. These technologies both recognize the words spoken by a user and extract actionable information, such as the topic of a user's query or the degree of match between a query and documents in a database. Such applications are typically treated as a pipeline of automatic speech transcription followed by text processing to extract the meaning. This typical pipeline approach can't recover from speech recognition errors, and also can't use additional meaningful information in the speech signal beyond the words. This project aims to develop machine learning tools for directly extracting meaning from speech. In addition, typical machine learning approaches require large amounts of labeled training data; another goal of this project is to enable speech understanding with fewer training resources, specifically by learning \"pre-trained\" representation models using either unlabeled speech data or small amounts of labeled data.</p>\n<p><br />The main outcomes of this project can be divided into (1) improved techniques for learning speech representation models at the frame level, at the word level, and beyond; (2) application of such models for improved speech recognition and speech search; (3) improved models for speech understanding tasks when training data are limited; (4) creation of new benchmark speech understanding tasks and corresponding resources; (5) development of analysis techniques to better understand how and to what extent speech representation models encode linguistic information; and (6) broader impact on beyond speech processing research.&nbsp; Each of these is described further below.</p>\n<p><br />(1) This project has produced a variety of approaches for learning speech representations, including self-supervised frame-level speech representation models learned via time-frequency masked reconstruction, multilingual learning of acoustic word and larger span embeddings, and unsupervised acoustic word embeddings.&nbsp;&nbsp;</p>\n<p><br />(2) Each of the representation learning approaches described above has improved performance and/or efficiency on standard speech tasks.&nbsp; For example, our multilingual acoustic span embeddings have been used to obtain state-of-the-art performance on a benchmark query-by-example search task, improving both performance and computation requirements over previous methods.&nbsp; Our jointly trained acoustic and written word embeddings have been used to develop high-performing whole-word speech recognition models, and to enable recognition of out-of-vocabulary words.</p>\n<p><br />(3) In situations where the amount of training data is very limited, we have studied the use of visual grounding and transfer learning.&nbsp; We have used visual grounding by learning models of speech meaning from images with corresponding spoken captions, and we have shown that visual grounding is helpful even in the presence of a small amount of textual labels. These models have been used for searching for spoken content that relates semantically to a user query.&nbsp; We have also studied the use of transfer learning for improving speech translation models, finding that we can improve speech translation in a low-resource language by using a model that is pre-trained for a different task (speech recognition) on a different but high-resource language.</p>\n<p><br />(4) To address the need for benchmark spoken language understanding tasks, we have collected and annotated data, and developed baselines and a leaderboard, for new spoken language understanding tasks.&nbsp; This suite of benchmark tasks will help track the research community's progress in automatic understanding of spoken language.</p>\n<p><br />(5) During the span of this project, there have been exciting developments throughout the field of speech processing, in large part due to multi-layer neural speech representation models trained on large amounts of unlabeled speech.&nbsp; One outcome of this project is a set of analyses of pre-trained representation models, measuring the information content of different model layers to locate which parts of the models contain more acoustic, phonetic, word identity, and word meaning information.</p>\n<p><br />(6) The project has produced software used in the PI's courses on speech technologies and unsupervised learning, where it has been used by students in their term projects, in some cases leading to published research.&nbsp; Some of the work in the project is also included in a survey paper on self-supervised learning of speech representations, which will help disseminate the work to a broader audience and hopefully facilitate cross-field impact.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/02/2022<br>\n\t\t\t\t\tModified by: Karen&nbsp;Livescu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThere is an increasingly large array of spoken language interfaces that are commercially available, such as virtual assistants and telephone customer service interfaces. These technologies both recognize the words spoken by a user and extract actionable information, such as the topic of a user's query or the degree of match between a query and documents in a database. Such applications are typically treated as a pipeline of automatic speech transcription followed by text processing to extract the meaning. This typical pipeline approach can't recover from speech recognition errors, and also can't use additional meaningful information in the speech signal beyond the words. This project aims to develop machine learning tools for directly extracting meaning from speech. In addition, typical machine learning approaches require large amounts of labeled training data; another goal of this project is to enable speech understanding with fewer training resources, specifically by learning \"pre-trained\" representation models using either unlabeled speech data or small amounts of labeled data.\n\n\nThe main outcomes of this project can be divided into (1) improved techniques for learning speech representation models at the frame level, at the word level, and beyond; (2) application of such models for improved speech recognition and speech search; (3) improved models for speech understanding tasks when training data are limited; (4) creation of new benchmark speech understanding tasks and corresponding resources; (5) development of analysis techniques to better understand how and to what extent speech representation models encode linguistic information; and (6) broader impact on beyond speech processing research.  Each of these is described further below.\n\n\n(1) This project has produced a variety of approaches for learning speech representations, including self-supervised frame-level speech representation models learned via time-frequency masked reconstruction, multilingual learning of acoustic word and larger span embeddings, and unsupervised acoustic word embeddings.  \n\n\n(2) Each of the representation learning approaches described above has improved performance and/or efficiency on standard speech tasks.  For example, our multilingual acoustic span embeddings have been used to obtain state-of-the-art performance on a benchmark query-by-example search task, improving both performance and computation requirements over previous methods.  Our jointly trained acoustic and written word embeddings have been used to develop high-performing whole-word speech recognition models, and to enable recognition of out-of-vocabulary words.\n\n\n(3) In situations where the amount of training data is very limited, we have studied the use of visual grounding and transfer learning.  We have used visual grounding by learning models of speech meaning from images with corresponding spoken captions, and we have shown that visual grounding is helpful even in the presence of a small amount of textual labels. These models have been used for searching for spoken content that relates semantically to a user query.  We have also studied the use of transfer learning for improving speech translation models, finding that we can improve speech translation in a low-resource language by using a model that is pre-trained for a different task (speech recognition) on a different but high-resource language.\n\n\n(4) To address the need for benchmark spoken language understanding tasks, we have collected and annotated data, and developed baselines and a leaderboard, for new spoken language understanding tasks.  This suite of benchmark tasks will help track the research community's progress in automatic understanding of spoken language.\n\n\n(5) During the span of this project, there have been exciting developments throughout the field of speech processing, in large part due to multi-layer neural speech representation models trained on large amounts of unlabeled speech.  One outcome of this project is a set of analyses of pre-trained representation models, measuring the information content of different model layers to locate which parts of the models contain more acoustic, phonetic, word identity, and word meaning information.\n\n\n(6) The project has produced software used in the PI's courses on speech technologies and unsupervised learning, where it has been used by students in their term projects, in some cases leading to published research.  Some of the work in the project is also included in a survey paper on self-supervised learning of speech representations, which will help disseminate the work to a broader audience and hopefully facilitate cross-field impact.\n\n \n\n\t\t\t\t\tLast Modified: 08/02/2022\n\n\t\t\t\t\tSubmitted by: Karen Livescu"
 }
}