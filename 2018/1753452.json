{
 "awd_id": "1753452",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Advancing Personal Informatics through Semi-Automated and Collaborative Tracking",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2017-06-20",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 546348.0,
 "awd_amount": 562348.0,
 "awd_min_amd_letter_date": "2017-11-03",
 "awd_max_amd_letter_date": "2021-03-08",
 "awd_abstract_narration": "This research examines a novel self-tracking approach called semi-automated tracking to help people easily engage with a rich set of personal data, such as weight, activities, sleep pattern, and medication use. In principle, being aware of self-tracking data can help people reflect on their health condition and understand how their behavior affects their progress toward goals, potentially improving health and well-being. In practice, however, self-tracking is hard. Manual tracking approaches such as diaries require much effort, while automated tracking approaches such as wearable sensing tools significantly reduce the tracker's awareness, accountability, and involvement compared to manual tracking. To address these problems, this research proposes to design and develop a semi-automated tracking platform, combining both manual and automated data collection methods. The platform will enable people to design and customize their own tracking tools and capture many kinds of personal data depending on individuals' diverse tracking needs. As the customization itself can be hard, the platform will also support collaborative tracking by incorporating templates created based on experts' input. The proposal will test these ideas while working with two user groups who can benefit from practicing self-tracking: (1) older adults living in retirement communities, and (2) clinicians and surgical patients in prehabilitation programs (that is, dietary and exercise plans for enhancing patients' health prior to surgery). The differences in individuals, their motivations, and the demands of tracking between the older adult and prehabilitation groups will provide insight on how to design semi-automated and collaborative tracking tools. The principal investigator (PI) will use both the resulting case studies and research platform in her courses on human-computer interaction and personal informatics, and make the curricula openly available to other educators and researchers. The PI will also work closely with programs at her institution to involve people from under-represented groups in the research, including undergraduates, women, and minorities.\r\n\r\nThe first phase of the research will focus on learning people's concerns, needs, and challenges regarding the use of tracking technologies via formative studies. The research team will conduct a technology probing study using a platform that supports customizable manual tracking, along with interviews and observations of older adults, clinicians, and surgical patients. During this phase, the research team will continue developing the research platform, which will be used as the technical basis for the proposed studies and interventions. These formative studies will generate insights into how these populations currently approach self-tracking: what do they do and what would they like to do, and what makes it hard and what are they afraid of. These insights will provide general design guidelines for the research platform, which will include the semi-automated tracking elements designed to balance people's information needs and data capture burden while enhancing their engagement. In the second phase, the research team will test the feasibility of the semi-automated tracking approach with a short-term deployment study followed by design iterations. Then the revised platform will be deployed longitudinally to test its efficacy with both older adults and surgical patients. In the third phase, the research team will focus on the collaborative tracking approach, aiming to help patients configure self-tracking settings and collect high quality data that are useful for clinicians. To support these goals, the research team will conduct design workshops with clinicians to generate templates for common prehabilitation regimens, which will later be incorporated in the research platform. The collaborative tracking elements of the platform will be evaluated in a longitudinal study in hospitals. This research will contribute to the growing bodies of knowledge in personal informatics and health informatics. It will inform us of elders' and surgical patients' self-tracking practices and ways in which self-tracking tools should be designed to support the needs of various stakeholders, including partners, caretakers, and clinicians. As a way to expand the impact of the work, the research team will disseminate the semi-automated tracking platform to academic communities (e.g., behavioral scientists, personal informatics researchers), medical communities (e.g., clinicians and patients), Quantified Self communities (i.e., dedicated self-trackers), and individuals who wish to engage in self-tracking practices.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eun Kyoung",
   "pi_last_name": "Choe",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eun Kyoung Choe",
   "pi_email_addr": "choe@umd.edu",
   "nsf_id": "000678206",
   "pi_start_date": "2017-11-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425103",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 91468.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 132573.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 110435.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 118512.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 109360.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>This research centers on a novel self-tracking approach called semi-automated tracking, aiming to help individuals effectively engage with a rich set of personal data. The motivation stems from the potential benefits of individuals interacting with diverse personal data, such as food consumption, weight, activities, sleep patterns, productivity, and more. Collecting and reflecting on such data can empower individuals to better understand how their behaviors influence progress toward health and wellness goals. Despite the theoretical positive impact on health, practical challenges associated with self-tracking and data reflection were substantial, leading us to address these issues.</span></p>\n<p><span>Traditional manual tracking methods, like diaries, demand significant effort and may impede widespread adoption, while automated tracking tools, like wearable sensors, may inadvertently reduce user awareness, accountability, and involvement. Recognizing these issues, we proposed the development of a semi-automated tracking platform called OmniTrack. This platform integrates both manual and automated data collection methods, providing users with a dynamic and customizable tracking experience. The central idea was to empower people to design and customize their tracking tools based on their diverse and evolving needs.</span></p>\n<p><span>We accomplished a major milestone by creating a prototype of OmniTrack, offering extensive flexibility through end-user configuration of personal data trackers on their phones. A deployment study demonstrated the value of this flexible, semi-automated tracking approach, with participants creating different&nbsp;trackers based on personal preferences and needs. We also observed shifts in their tracking practice and how they modified their trackers over time.</span></p>\n<p><span>Nevertheless, customizing the platform presented challenges for novice users. To overcome these customization hurdles, the platform supported collaborative tracking by integrating templates derived from expert input. This collaborative aspect aimed to facilitate user engagement and make the process of self-tracking more accessible. We explored the potential of flexible and customizable semi-automated tracking in two different contexts: food and productivity. Co-design workshops with dietitians identified opportunities for designing customizable food trackers, while a diary study with knowledge workers using OmniTrack revealed how they conceptualize personal productivity.</span></p>\n<p><span>In subsequent years, we expanded OmniTrack by integrating multimodal interactions, combining speech and touch interaction to enhance data capture and exploration capabilities. The increasing use of speech input on mobile phones is transforming how researchers and individuals collect data, potentially reducing the burden of data capture while allowing for more detailed information collection. To investigate the complementarity of voice interaction within a mobile app, we designed and developed TandemTrack, combining a mobile app and an Alexa skill to support exercise regimens, data capture, feedback, and reminder.</span></p>\n<p><span>We also extended OmniTrack to include audio recording capabilities, resulting in FoodScrap. We showed that participants using speech input provided more extensive responses, detailing meal components and preparation methods. Enhancing speech input capability, we designed NoteWordy, which enabled users to choose specific input modalities or combine different ones for diverse data capture context. </span><span>Through these explorations, we provided implications for leveraging touch and speech input strengths in self-tracking contexts. Going beyond multimodal interaction&rsquo;s benefits in data capture, we explored the potential of speech in visual data exploration. In designing Data@Hand, we aimed to empower people to navigate and compare personal health data on smartphones through flexible time manipulation with speech. This work contributes the first mobile app that leverages the synergy of speech and touch input modalities for personal data exploration.</span></p>\n<p><span>Through the design and development of multimodal data capture and exploration tools&mdash;TandemTrack, FoodScrap, NoteWordy, and Data@Hand&mdash;we uncovered the benefits and challenges of using voice interaction for personal data capture and reflection. Our results generated insights regarding how participants' personal preferences, proximity to the device, living environment, social context, and voice recognition errors influence their use of speech and touch in personal data capturing.</span><span>&nbsp;</span></p>\n<p><span>Finally, we examined accessibility challenges faced by blind and low-vision individuals (BLVIs) in the realm of self-tracking technologies. Recognizing the growing importance of self-tracking technologies in health management, we aimed to understand and mitigate challenges faced by BLVIs. Surveys and interviews conducted with BLVIs revealed </span><span>unintended consequences contributing to a widening health disparity gap, particularly for individuals with disabilities. The inherent design bias of self-tracking tools posed significant barriers for people with disabilities, hampering their accessibility to personal health data and compromising equitable access to healthcare services. Our research extends beyond conventional considerations of mobile app accessibility, exploring the unique challenges posed by mainstream health technologies and seeking to foster inclusivity and equal access to personal health technologies for individuals of all abilities, contributing to a more equitable and just society.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/01/2024<br>\nModified by: Eun Kyoung&nbsp;Choe</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis research centers on a novel self-tracking approach called semi-automated tracking, aiming to help individuals effectively engage with a rich set of personal data. The motivation stems from the potential benefits of individuals interacting with diverse personal data, such as food consumption, weight, activities, sleep patterns, productivity, and more. Collecting and reflecting on such data can empower individuals to better understand how their behaviors influence progress toward health and wellness goals. Despite the theoretical positive impact on health, practical challenges associated with self-tracking and data reflection were substantial, leading us to address these issues.\n\n\nTraditional manual tracking methods, like diaries, demand significant effort and may impede widespread adoption, while automated tracking tools, like wearable sensors, may inadvertently reduce user awareness, accountability, and involvement. Recognizing these issues, we proposed the development of a semi-automated tracking platform called OmniTrack. This platform integrates both manual and automated data collection methods, providing users with a dynamic and customizable tracking experience. The central idea was to empower people to design and customize their tracking tools based on their diverse and evolving needs.\n\n\nWe accomplished a major milestone by creating a prototype of OmniTrack, offering extensive flexibility through end-user configuration of personal data trackers on their phones. A deployment study demonstrated the value of this flexible, semi-automated tracking approach, with participants creating differenttrackers based on personal preferences and needs. We also observed shifts in their tracking practice and how they modified their trackers over time.\n\n\nNevertheless, customizing the platform presented challenges for novice users. To overcome these customization hurdles, the platform supported collaborative tracking by integrating templates derived from expert input. This collaborative aspect aimed to facilitate user engagement and make the process of self-tracking more accessible. We explored the potential of flexible and customizable semi-automated tracking in two different contexts: food and productivity. Co-design workshops with dietitians identified opportunities for designing customizable food trackers, while a diary study with knowledge workers using OmniTrack revealed how they conceptualize personal productivity.\n\n\nIn subsequent years, we expanded OmniTrack by integrating multimodal interactions, combining speech and touch interaction to enhance data capture and exploration capabilities. The increasing use of speech input on mobile phones is transforming how researchers and individuals collect data, potentially reducing the burden of data capture while allowing for more detailed information collection. To investigate the complementarity of voice interaction within a mobile app, we designed and developed TandemTrack, combining a mobile app and an Alexa skill to support exercise regimens, data capture, feedback, and reminder.\n\n\nWe also extended OmniTrack to include audio recording capabilities, resulting in FoodScrap. We showed that participants using speech input provided more extensive responses, detailing meal components and preparation methods. Enhancing speech input capability, we designed NoteWordy, which enabled users to choose specific input modalities or combine different ones for diverse data capture context. Through these explorations, we provided implications for leveraging touch and speech input strengths in self-tracking contexts. Going beyond multimodal interactions benefits in data capture, we explored the potential of speech in visual data exploration. In designing Data@Hand, we aimed to empower people to navigate and compare personal health data on smartphones through flexible time manipulation with speech. This work contributes the first mobile app that leverages the synergy of speech and touch input modalities for personal data exploration.\n\n\nThrough the design and development of multimodal data capture and exploration toolsTandemTrack, FoodScrap, NoteWordy, and Data@Handwe uncovered the benefits and challenges of using voice interaction for personal data capture and reflection. Our results generated insights regarding how participants' personal preferences, proximity to the device, living environment, social context, and voice recognition errors influence their use of speech and touch in personal data capturing.\n\n\nFinally, we examined accessibility challenges faced by blind and low-vision individuals (BLVIs) in the realm of self-tracking technologies. Recognizing the growing importance of self-tracking technologies in health management, we aimed to understand and mitigate challenges faced by BLVIs. Surveys and interviews conducted with BLVIs revealed unintended consequences contributing to a widening health disparity gap, particularly for individuals with disabilities. The inherent design bias of self-tracking tools posed significant barriers for people with disabilities, hampering their accessibility to personal health data and compromising equitable access to healthcare services. Our research extends beyond conventional considerations of mobile app accessibility, exploring the unique challenges posed by mainstream health technologies and seeking to foster inclusivity and equal access to personal health technologies for individuals of all abilities, contributing to a more equitable and just society.\n\n\n\t\t\t\t\tLast Modified: 01/01/2024\n\n\t\t\t\t\tSubmitted by: Eun KyoungChoe\n"
 }
}