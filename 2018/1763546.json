{
 "awd_id": "1763546",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CSR: Medium: Rethinking Distributed SSD Storage Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Alexander Jones",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 1000000.0,
 "awd_amount": 1000000.0,
 "awd_min_amd_letter_date": "2018-08-13",
 "awd_max_amd_letter_date": "2021-09-15",
 "awd_abstract_narration": "Modern storage technologies, such as solid-state disks (SSDs), have increased their capacity by increasing density: representing more digital bits in the same physical media.  To do so, their margin of error for representing data has shrunk, and the resulting bit error rate has risen.  While disks employ error correcting codes to mask such bit errors, these error rates rise as disk blocks age, by being (over)written, and eventually the disk fails.  Thus, as density has risen, disks' expected lifetimes have plunged.  This research rethinks how to build scalable and cost-effective storage through a cross-layer design to the storage stack.\r\n\r\nFor most datacenter applications, objects are already replicated between multiple devices, and this application-layer redundancy can be leveraged for increased reliability.  This research thus explores a new approach for end-to-end reliability for SSD storage systems: datacenter flash devices should be far less reliable and push error visibility and correction up to the distributed application layer.  The design challenges and implications of providing such a holistic, cross-layer approach are explored through three research thrusts: at the individual disk layer, at the local file system, and at the distributed storage system layer.\r\n\r\nBy extending the lifecycle of disks by several orders of magnitude, this research has the potential for the adoption of new SSD storage technologies, significant cost savings, and enabling new applications to use SSDs.  Due to the plunging lifecycles of newer generations of SSD technologies, large datacenter applications have been unable to integrate these new generations in a number of scenarios (e.g., workloads with heavy write rates, including databases and caches).  Given this research's potential for orders of magnitude of reliability improvements, it provides a potential path out from this reliability cliff of the hardware.\r\n\r\nCode, data, and results relating to this research will be made available via a git repository, hosted publicly at https://github.com/princeton-sns/crosslayer-storage/.  Materials will be available throughout the Award term and at least five years following it.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Freedman",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Michael J Freedman",
   "pi_email_addr": "mfreed@cs.princeton.edu",
   "nsf_id": "000500977",
   "pi_start_date": "2018-08-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kyle",
   "pi_last_name": "Jamieson",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Kyle A Jamieson",
   "pi_email_addr": "kylej@princeton.edu",
   "nsf_id": "000709482",
   "pi_start_date": "2018-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "87 Prospect Avenue",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 280965.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 500845.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 218190.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span class=\"s1\">This research rethinks how we can build scalable and cost-effective storage systems through a holistic, cross-layer consideration of the storage stack. We reconsider the interfaces for each layer (e.g., allowing tunable reliability for disk writes, or exposing faulty reads to higher-level applications), and how the needs of different applications lead them to leverage these interfaces in differing ways.<span>&nbsp; </span>It also includes considering the use of hybrid storage technologies that individually optimize for different parts of the performance/reliability/cost design space. Exploring such research is important for next-generation storage needs, given that many large datacenter applications have not integrated SSD-based storage in a number of scenarios (typically with heavy write rates, including caching) due to lifecycle concerns, and the latest-generation SSD storage technologies have only made these problems worse.</span></p>\n<p class=\"p1\"><span class=\"s1\">To address these challenges, we advocated for an end-to-end reconsideration of how to leverage datacenter flash storage reliability. Each layer of the storage stack---from individual disks, to local file systems, and to distributed storage systems---has been designed rather independently, with each building in fault tolerance for its next level above. For example, disk manufacturers employ robust mechanisms to reduce hardware failures, as file systems (and their users) react rather poorly to lost or corrupted data.<span>&nbsp; </span>This research argues that addressing this at higher levels in the datacenter has the potential for order of magnitude improvements for such systems, including how to leverage emerging SSD storage technologies, which illustrate the competing goals of improving performance and reducing storage costs.<span>&nbsp; </span>On one side, high performance non-volatile memory (NVM) technologies, such as Optane SSD and Z-NAND, provide single-digit us latencies. On the other end of the spectrum, cheap and dense storage such as QLC NAND enables applications to store vast amounts of data on flash at a low cost-per-bit. Yet with this lower cost, QLC has a higher latency and is significantly less reliable than less dense flash technology; this reliability challenge is somewhat fundamental given that each subsequent flash generation has exponentially increased the flash storage density by increasing the number of bits that are represented by each flash cell (and thus making each cell more error prone due to tighter voltage charge levels representing each bit).</span></p>\n<p class=\"p1\"><span class=\"s1\">It is hard for system architects to reason about these unique performance, cost, and endurance characteristics when developing software data systems. Therefore, significant recent effort has sought to build new software storage systems that are architected specifically for these new technologies. The focus of this research has been to explore the idea that these new architectures do not go far enough in rethinking the use of emerging hardware technology, as they continue to take the legacy perspective of the storage substrate as a homogeneous and monolithic layer.<span>&nbsp; </span>Our results have been two-fold.</span></p>\n<p class=\"p1\"><span class=\"s1\">First, we worked on addressing the flash lifetime problem by allowing devices to expose higher bit error rates to applications, and then enabling these applications to better handle, mask, and/or recover from such errors.<span>&nbsp; </span>In particular, we observed that flash endurance can be extended by allowing devices to go beyond their advertised uncorrectable bit error rate (UBER) and embracing the use of flash disks that exhibit much higher error rates.<span>&nbsp; </span>To do so, we introduced a set of three simple general-purpose techniques, called Distributed error Isolation and RECovery Techniques (DIRECT), which enable distributed storage systems to achieve high availability and correctness in the face of uncorrectable bit errors.<span>&nbsp; </span>We implemented DIRECT on two real-world storage systems: ZippyDB, a distributed key-value store in production at Facebook that is backed by and supports transactions on top of RocksDB, and HDFS, a distributed file system. When tested on production traces at Facebook, DIRECT reduces application-visible error rates in ZippyDB by more than 100x and recovery time by more than 10,000x. DIRECT also allows HDFS to tolerate a 10,000-100,000x higher bit error rate without experiencing application-visible errors.</span></p>\n<p class=\"p2\">Second, we explored how to better leverage and incorporate heterogeneous storage into software systems. We investigated how storage systems can simultaneously leverage multiple, heterogeneous storage technologies to achieve \"the best of both worlds\" -- fast, cheap, and enduring -- largely by ensuring the reads and most writes go to technologies like NVM or SLC, while the bulk of long-term storage is maintained on QLC for cost effectiveness. Compared to the standard use of RocksDB on flash in datacenters today, our average throughput on heterogeneous is 3.3x faster and its read tail latency is 2x better, using equivalently-priced hardware.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/26/2022<br>\n\t\t\t\t\tModified by: Michael&nbsp;J&nbsp;Freedman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "This research rethinks how we can build scalable and cost-effective storage systems through a holistic, cross-layer consideration of the storage stack. We reconsider the interfaces for each layer (e.g., allowing tunable reliability for disk writes, or exposing faulty reads to higher-level applications), and how the needs of different applications lead them to leverage these interfaces in differing ways.  It also includes considering the use of hybrid storage technologies that individually optimize for different parts of the performance/reliability/cost design space. Exploring such research is important for next-generation storage needs, given that many large datacenter applications have not integrated SSD-based storage in a number of scenarios (typically with heavy write rates, including caching) due to lifecycle concerns, and the latest-generation SSD storage technologies have only made these problems worse.\nTo address these challenges, we advocated for an end-to-end reconsideration of how to leverage datacenter flash storage reliability. Each layer of the storage stack---from individual disks, to local file systems, and to distributed storage systems---has been designed rather independently, with each building in fault tolerance for its next level above. For example, disk manufacturers employ robust mechanisms to reduce hardware failures, as file systems (and their users) react rather poorly to lost or corrupted data.  This research argues that addressing this at higher levels in the datacenter has the potential for order of magnitude improvements for such systems, including how to leverage emerging SSD storage technologies, which illustrate the competing goals of improving performance and reducing storage costs.  On one side, high performance non-volatile memory (NVM) technologies, such as Optane SSD and Z-NAND, provide single-digit us latencies. On the other end of the spectrum, cheap and dense storage such as QLC NAND enables applications to store vast amounts of data on flash at a low cost-per-bit. Yet with this lower cost, QLC has a higher latency and is significantly less reliable than less dense flash technology; this reliability challenge is somewhat fundamental given that each subsequent flash generation has exponentially increased the flash storage density by increasing the number of bits that are represented by each flash cell (and thus making each cell more error prone due to tighter voltage charge levels representing each bit).\nIt is hard for system architects to reason about these unique performance, cost, and endurance characteristics when developing software data systems. Therefore, significant recent effort has sought to build new software storage systems that are architected specifically for these new technologies. The focus of this research has been to explore the idea that these new architectures do not go far enough in rethinking the use of emerging hardware technology, as they continue to take the legacy perspective of the storage substrate as a homogeneous and monolithic layer.  Our results have been two-fold.\nFirst, we worked on addressing the flash lifetime problem by allowing devices to expose higher bit error rates to applications, and then enabling these applications to better handle, mask, and/or recover from such errors.  In particular, we observed that flash endurance can be extended by allowing devices to go beyond their advertised uncorrectable bit error rate (UBER) and embracing the use of flash disks that exhibit much higher error rates.  To do so, we introduced a set of three simple general-purpose techniques, called Distributed error Isolation and RECovery Techniques (DIRECT), which enable distributed storage systems to achieve high availability and correctness in the face of uncorrectable bit errors.  We implemented DIRECT on two real-world storage systems: ZippyDB, a distributed key-value store in production at Facebook that is backed by and supports transactions on top of RocksDB, and HDFS, a distributed file system. When tested on production traces at Facebook, DIRECT reduces application-visible error rates in ZippyDB by more than 100x and recovery time by more than 10,000x. DIRECT also allows HDFS to tolerate a 10,000-100,000x higher bit error rate without experiencing application-visible errors.\nSecond, we explored how to better leverage and incorporate heterogeneous storage into software systems. We investigated how storage systems can simultaneously leverage multiple, heterogeneous storage technologies to achieve \"the best of both worlds\" -- fast, cheap, and enduring -- largely by ensuring the reads and most writes go to technologies like NVM or SLC, while the bulk of long-term storage is maintained on QLC for cost effectiveness. Compared to the standard use of RocksDB on flash in datacenters today, our average throughput on heterogeneous is 3.3x faster and its read tail latency is 2x better, using equivalently-priced hardware.\n\n\t\t\t\t\tLast Modified: 11/26/2022\n\n\t\t\t\t\tSubmitted by: Michael J Freedman"
 }
}