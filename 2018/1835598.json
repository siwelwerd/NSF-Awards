{
 "awd_id": "1835598",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rob Beverly",
 "awd_eff_date": "2018-11-01",
 "awd_exp_date": "2024-03-31",
 "tot_intn_awd_amt": 540000.0,
 "awd_amount": 540000.0,
 "awd_min_amd_letter_date": "2018-08-28",
 "awd_max_amd_letter_date": "2018-08-28",
 "awd_abstract_narration": "Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term \"networks\" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.\r\n\r\n\r\nResources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jurij",
   "pi_last_name": "Leskovec",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jurij Leskovec",
   "pi_email_addr": "jure@cs.stanford.edu",
   "nsf_id": "000514495",
   "pi_start_date": "2018-08-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943055008",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "077Z",
   "pgm_ref_txt": "CSSI-1: Cyberinfr for Sustained Scientif"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 540000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-bf7bff27-7fff-1b51-86ea-e193a200ac1f\"> </span></p>\n<p dir=\"ltr\"><span>Graph Neural Networks (GNN) </span><span>have emerged as a dominant paradigm for analyzing complex data. They provide a novel, foundational approach for performing machine learning over graphs. Additionally, </span><span>representation learning on networks using GNNs has revolutionized state-of-the-art and has shown broad utility for applications in many areas, such as biomedicine and other sciences, as well as in industry and government. Our work in improving our understanding of fundamentals and methods for graph neural networks allows us to develop radically new approaches to analyze large-scale datasets. It advances state of the art in using network science for study of complex systems in a broad set of domains, such as social, communication and information networks and knowledge graphs. We highlight some of our contributions next.</span></p>\n<p dir=\"ltr\"><span>We developed strategies for GNN pretraining. Many applications of machine learning require a model to make accurate predictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it was an open question how to effectively use pre-training on graph datasets. Our proposed strategy and self-supervised methods for pre-training GNNs work at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. As opposed to naive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, our strategy avoids negative transfer and improves generalization significantly across downstream tasks.</span></p>\n<p dir=\"ltr\"><span>The behavior of GNN-based models is opaque and hard to interpret. To address this issue, we developed GNNExplainer, which was the first general, model-agnostic approach that provides interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction.</span></p>\n<p dir=\"ltr\"><span>In addition to foundational, methodological contributions, we demonstrated the utility of graph-based machine learning approaches in numerous domains, including goal-directed molecule generation, modeling of polypharmacy side effects, simulation of complex physics phenomena, generation of SAT formulas, question answering, multi-hop reasoning over knowledge graphs, and fusion of language models and knowledge graphs.</span></p>\n<p dir=\"ltr\"><span>To promote the field of machine learning on graphs, we developed Open Graph Benchmark (OGB), ogb.stanford.edu. OGB is a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provided a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. Finally, we provided an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. We organized workshops to present the best performing approaches, which received submissions from over 500 teams.</span></p>\n<p dir=\"ltr\"><span>We made significant contributions to PyG (pyg.org), one of the most popular open source libraries for machine learning on graphs, with over 200,000 downloads per month and over 400 contributors. The library provides powerful and easy-to-use primitives for writing and training Graph Neural Networks for a wide range of applications.</span></p>\n<p dir=\"ltr\"><span>Our research has had a major impact on the course design at Stanford University. The course CS224W, taught at Stanford by PI Leskovec, has been renamed from Social &amp; Information Network Analysis to Machine Learning with Graphs and the course content has been revised significantly with completely new material. Most of the course now covers topics related to the research under this project. Complete course lectures have been made available to the public via YouTube, where they received over 1.5 </span><span>million views.</span></p>\n<p dir=\"ltr\"><span>To disseminate our work, we are organizing annual Stanford Graph Learning Workshops with over 200 in-person and 7,000 online participants.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 06/03/2024<br>\nModified by: Jurij&nbsp;Leskovec</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nGraph Neural Networks (GNN) have emerged as a dominant paradigm for analyzing complex data. They provide a novel, foundational approach for performing machine learning over graphs. Additionally, representation learning on networks using GNNs has revolutionized state-of-the-art and has shown broad utility for applications in many areas, such as biomedicine and other sciences, as well as in industry and government. Our work in improving our understanding of fundamentals and methods for graph neural networks allows us to develop radically new approaches to analyze large-scale datasets. It advances state of the art in using network science for study of complex systems in a broad set of domains, such as social, communication and information networks and knowledge graphs. We highlight some of our contributions next.\n\n\nWe developed strategies for GNN pretraining. Many applications of machine learning require a model to make accurate predictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it was an open question how to effectively use pre-training on graph datasets. Our proposed strategy and self-supervised methods for pre-training GNNs work at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. As opposed to naive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, our strategy avoids negative transfer and improves generalization significantly across downstream tasks.\n\n\nThe behavior of GNN-based models is opaque and hard to interpret. To address this issue, we developed GNNExplainer, which was the first general, model-agnostic approach that provides interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction.\n\n\nIn addition to foundational, methodological contributions, we demonstrated the utility of graph-based machine learning approaches in numerous domains, including goal-directed molecule generation, modeling of polypharmacy side effects, simulation of complex physics phenomena, generation of SAT formulas, question answering, multi-hop reasoning over knowledge graphs, and fusion of language models and knowledge graphs.\n\n\nTo promote the field of machine learning on graphs, we developed Open Graph Benchmark (OGB), ogb.stanford.edu. OGB is a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provided a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. Finally, we provided an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. We organized workshops to present the best performing approaches, which received submissions from over 500 teams.\n\n\nWe made significant contributions to PyG (pyg.org), one of the most popular open source libraries for machine learning on graphs, with over 200,000 downloads per month and over 400 contributors. The library provides powerful and easy-to-use primitives for writing and training Graph Neural Networks for a wide range of applications.\n\n\nOur research has had a major impact on the course design at Stanford University. The course CS224W, taught at Stanford by PI Leskovec, has been renamed from Social & Information Network Analysis to Machine Learning with Graphs and the course content has been revised significantly with completely new material. Most of the course now covers topics related to the research under this project. Complete course lectures have been made available to the public via YouTube, where they received over 1.5 million views.\n\n\nTo disseminate our work, we are organizing annual Stanford Graph Learning Workshops with over 200 in-person and 7,000 online participants.\n\n\n\t\t\t\t\tLast Modified: 06/03/2024\n\n\t\t\t\t\tSubmitted by: JurijLeskovec\n"
 }
}