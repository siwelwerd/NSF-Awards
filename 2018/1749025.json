{
 "awd_id": "1749025",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:  The MegaAttitude Project: Investigating selection and polysemy at the scale of the lexicon",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032924770",
 "po_email": "rtheodor@nsf.gov",
 "po_sign_block_name": "Rachel M. Theodore",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-02-28",
 "tot_intn_awd_amt": 123650.0,
 "awd_amount": 123650.0,
 "awd_min_amd_letter_date": "2018-08-02",
 "awd_max_amd_letter_date": "2021-04-06",
 "awd_abstract_narration": "This project addresses how humans draw complex inferences from the thousands of English predicates that combine with subordinate clauses -- \"think\", \"know\", \"say\", \"tell\", \"remember\", \"forget\", etc. -- when the structural characteristics of the clauses they combine with vary. For example, the sentence \"John forgot that he bought milk\" is similar to the sentence \"John forgot to buy milk\"; but from the first sentence, a listener infers that John bought milk, while from the second, a listener infers that he didn't. This inference pattern is only one among many such patterns in English; yet, in spite of this variety, there appears to be substantial regularities across predicates and subordinate clause structures that prior work has only scratched the surface of. Investigating the systematicities in how humans compute these inference patterns sheds light on how the human cognitive system constructs complex meanings from simpler parts and supports the development of intelligent computational systems for comprehending and reasoning about natural language in human-like ways.\r\n\r\nThe current project approaches this investigation in two parts. First, it develops and deploys multiple scalable, crowd-sourced annotation protocols, based on experimental methodologies from psycholinguistics, in order to collect data about a wide variety of inference patterns triggered by all of the thousands of English predicates that combine with subordinate clauses. Second, it leverages recent advances in multi-task machine learning to build a unified computational model of the relationship between such predicates, the structure of their subordinate clauses, and the inferences that they trigger, which is trained on these data. This model not only helps to reveal systematicities in how humans compute the inference patterns of interest; it can also be straightforwardly incorporated into applied technologies for natural language understanding.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Van Durme",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin Van Durme",
   "pi_email_addr": "vandurme@cs.jhu.edu",
   "nsf_id": "000611034",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kyle",
   "pi_last_name": "Rawlins",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kyle Rawlins",
   "pi_email_addr": "rawlins@cogsci.jhu.edu",
   "nsf_id": "000586455",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N. Charles St.",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182686",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "139700",
   "pgm_ele_name": "Cross-Directorate  Activities"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7298",
   "pgm_ref_txt": "COLLABORATIVE RESEARCH"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 58024.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 65626.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 0.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The data and analyses from this project have been used to constrain the space of possible theories of natural language meaning and in some cases falsify existing widely accepted theories.</span></p>\r\n<p><span>Some of the most well-developed theories of clause selection aim to explain the distribution of embedded interrogative and declarative clauses&mdash;specifically, which lexical properties condition whether a predicate takes only interrogative complements, only declarative complements, or both. A canonical contrast in this literature is that between predicates like think, believe, hope, and fear&mdash;which are often judged worse with interrogative complements than with declaratives (1) out of context&mdash;and know&mdash;which is often judged fine with both (2).</span></p>\r\n<ol>\r\n<li><span>&nbsp; 1. Jo {thinks, believes, hopes, fears} (*whether) Bo left.</span></li>\r\n<li><span>&nbsp; 2. Jo knows (whether) Bo left.</span></li>\r\n</ol>\r\n<p><span>This out-of-context contrast, which is corroborated by the acceptability judgments in the MegaAcceptability dataset, forms the basis for various generalizations that authors then attempt to derive. Using MegaAcceptability, MegaVeridicality, and MegaNegRaising, we show in a paper recently published in&nbsp;</span><span>Semantics &amp; Pragmatics&nbsp;</span><span>(\"On believing and hoping whether\") that these generalizations, and thus their derivations, are not viable, even if heavily constrained. We have provided fully documented code&nbsp;</span><span>for replicating these analyses. We hope that these findings force a broad-scale reevaluation of theories of clause selection away from inference-based explanations. The datasets collected under this project (all publicly released) can help with this reevaluation by allowing linguists to test lexicon-scale generalizations against quantitative data.</span></p>\r\n<p><span>One piece we believe will be crucial in this effort is theoretically motivated computational techniques. We cover a broad array of such techniques in a paper recently published in&nbsp;</span><span>Glossa&nbsp;</span><span>(\"Frequency, acceptability, and selection: A case study of clause-embedding\"), and we have provided fully documented code&nbsp;</span><span>for implementing those techniques. This release serves to broaden the accessibility of these modeling techniques to linguists.</span></p>\r\n<p><span>In this same vein, we present novel approaches to synthesizing the data collected in this project using advanced statistical techniques, and we have similarly provided fully documenteded</span><span>for implementing those techniques.</span></p>\r\n<p><span>To tie these findings together within a single modeling framework, we are working on packaging the newest generation of our modeling for synthesizing acceptability and inference judgments as a Python package with extensive documentation, making it easy for linguists to implement and extend our computational models.</span></p>\r\n<p><span>Finally, to introduce linguists to these modeling techniques, the lead PI at Rochester (Aaron White) taught a course using the MegaAcceptability data at the LSA Summer Institute in 2023. Like previous summer school and academic year courses he has taught, this course would be focused on introducing core technical proficiencies for implementing models to linguists using an interactive code notebook format.</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/08/2025<br>\nModified by: Kyle&nbsp;Rawlins</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe data and analyses from this project have been used to constrain the space of possible theories of natural language meaning and in some cases falsify existing widely accepted theories.\r\n\n\nSome of the most well-developed theories of clause selection aim to explain the distribution of embedded interrogative and declarative clausesspecifically, which lexical properties condition whether a predicate takes only interrogative complements, only declarative complements, or both. A canonical contrast in this literature is that between predicates like think, believe, hope, and fearwhich are often judged worse with interrogative complements than with declaratives (1) out of contextand knowwhich is often judged fine with both (2).\r\n\r\n 1. Jo {thinks, believes, hopes, fears} (*whether) Bo left.\r\n 2. Jo knows (whether) Bo left.\r\n\r\n\n\nThis out-of-context contrast, which is corroborated by the acceptability judgments in the MegaAcceptability dataset, forms the basis for various generalizations that authors then attempt to derive. Using MegaAcceptability, MegaVeridicality, and MegaNegRaising, we show in a paper recently published inSemantics & Pragmatics(\"On believing and hoping whether\") that these generalizations, and thus their derivations, are not viable, even if heavily constrained. We have provided fully documented codefor replicating these analyses. We hope that these findings force a broad-scale reevaluation of theories of clause selection away from inference-based explanations. The datasets collected under this project (all publicly released) can help with this reevaluation by allowing linguists to test lexicon-scale generalizations against quantitative data.\r\n\n\nOne piece we believe will be crucial in this effort is theoretically motivated computational techniques. We cover a broad array of such techniques in a paper recently published inGlossa(\"Frequency, acceptability, and selection: A case study of clause-embedding\"), and we have provided fully documented codefor implementing those techniques. This release serves to broaden the accessibility of these modeling techniques to linguists.\r\n\n\nIn this same vein, we present novel approaches to synthesizing the data collected in this project using advanced statistical techniques, and we have similarly provided fully documentededfor implementing those techniques.\r\n\n\nTo tie these findings together within a single modeling framework, we are working on packaging the newest generation of our modeling for synthesizing acceptability and inference judgments as a Python package with extensive documentation, making it easy for linguists to implement and extend our computational models.\r\n\n\nFinally, to introduce linguists to these modeling techniques, the lead PI at Rochester (Aaron White) taught a course using the MegaAcceptability data at the LSA Summer Institute in 2023. Like previous summer school and academic year courses he has taught, this course would be focused on introducing core technical proficiencies for implementing models to linguists using an interactive code notebook format.\r\n\n\n\t\t\t\t\tLast Modified: 01/08/2025\n\n\t\t\t\t\tSubmitted by: KyleRawlins\n"
 }
}