{
 "awd_id": "1834597",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: SaTC: Tracking Semantic Change in Medical Information",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2018-05-15",
 "awd_exp_date": "2022-03-31",
 "tot_intn_awd_amt": 298823.0,
 "awd_amount": 298823.0,
 "awd_min_amd_letter_date": "2018-05-22",
 "awd_max_amd_letter_date": "2018-05-22",
 "awd_abstract_narration": "Changes in the meaning of information as it passes through cyberspace can mislead those who access the information. This project will develop a new dataset and algorithms to identify and categorize medical information that remains true to the original meaning or undergoes distortion. Instead of imposing an external true/false label on this information, this project looks into a series of changes within the news coverage itself that gradually lead to a deviation from the original medical claims. Identifying important differences between original medical articles and news stories is a challenging, high risk-high reward venture. Broader impacts of this work include benefits to the research community by making novel contributions to understanding temporal changes in natural language information, as well as social benefits in the form of improved informational tools like question-answering. For the medical domain in particular, understanding temporal distortions and deviations from actual medical findings can reduce occurrences of harmful health choices, for instance, by embedding the research outcomes in news, social media, or search engines. \r\n\r\nThis project will develop a large dataset of medical scientific publications, and record their characteristics as they change over time across news by designing and developing discrete time-series representations of entities and their attributes and relations. This task will provide the basis for designing and implementing machine learning tasks that exploit stylometric features in natural language in conjunction with temporal distributions to identify and categorize such changes. This research will go beyond current approaches limited to true/false classification of individual articles, and hence be able to identify and analyze information change in narratives, including semantic changes and nuances, or selective emphasis of related information. The research entails an unsupervised and a semi-supervised machine learning approach with bootstrapping, and exploring a binary labeling task to distinguish distorted pieces of information from those that are faithful to the scientific finding, and a multi-label categorization to learn the type of semantic change occurring through time. The dataset will be disseminated via an archival location for natural language processing resources such as the Linguistic Data Consortium (https://www.ldc.upenn.edu/) to facilitate long-term availability to other researchers, and BitBucket or GitHub will be used to ensure the development, maintenance, sharing, and archiving of code.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ritwik",
   "pi_last_name": "Banerjee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ritwik Banerjee",
   "pi_email_addr": "rbanerjee@cs.stonybrook.edu",
   "nsf_id": "000736602",
   "pi_start_date": "2018-05-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "",
  "perf_city_name": "Stony Brook",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117944400",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "065Z",
   "pgm_ref_txt": "Human factors for security research"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 298823.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div>\n<p>Changes in the meaning of information as it passes through cyberspace can mislead those who access the information. The goal of this project was to identify changes in medical information. The project was driven by the idea that by studying semantic changes in information, it is possible to understand misinformation without introducing any external adjudication or bias. In quantitative AI-based misinformation analysis, this was the first project to preclude human judgment of truth and falsehood for any specific piece of information.</p>\n<p>The project began by developing a supervised machine learning (ML) model to identify information worth verifying. It then used information retrieval (IR) algorithms to identify how information was presented to readers in original peer-reviewed publications, and how the same information was presented using vastly different language in newswire articles. The key observation was that changes in medical information can largely be attributed to two phenomena:</p>\n<ol>\n<li>Information propagates across multiple distinct genres ? from research literature to news articles to social media, where each genre has its own linguistic properties and pragmatic hurdles to overcome.</li>\n<li>Information is often distorted due to subtle modifications (e.g., oversimplification) instead of outright false claims.</li>\n</ol></div>\n<div>This project pursued scientific investigations in both directions.</div>\n<div>The research team investigated linguistic transformations occurring when <strong>information transitions from specialized research articles into news</strong> intended for wider readership. This transition makes the information vulnerable to misinterpretation and misrepresentation, which are difficult to identify without specialized medical knowledge, and may exist even in the presence of explicit citations. Moreover, news articles seldom provide a precise correspondence between a claim and its origin, making it harder to identify which claims, if any, reflect the original findings. For instance, ?Flagellin shows therapeutic potential with H3N2, known as Aussie Flu.? contains two claims: Flagellin show therapeutic potential with H3N2, and H3N2 is known as Aussie Flu. These claims may be true or false independent of each other. It is <em>prima facie</em> unclear which claims, if any, are supported by the cited research. This difficult problem was overcome by developing a corpus of medical news along with the sources from medical research journals these news articles cite. It was then used to study what a reader perceives to be true, and how to verify the scientific source of claims. This was the first dataset to allow a study of information distortion across two genres with disparate readership and different vocabularies. This project presented the very first AI-based study of health-related fact-checking across these genres. The researchers also investigated how trust is used to create belief in misleading claims. Social media posts often leverage the trust readers have in prestigious news agencies and cite news articles for credibility. It is not, however, always the case that the cited article supports the claim being upheld in the social media post. In other words, the post only makes it ?look? like the claim originates from a credible source! The research team designed and developed a cross-genre IR model to identify whether the information in a Twitter post is, indeed, supported by the cited article. For this, a large collection of nearly 47 million posts about COVID was used to develop AI models to detect Tweets containing claim worth checking and verify whether the claims made in a Tweet are supported by the newswire article it cites. Unlike previous studies that detect unsubstantiated information by analysis of propagation patterns, this approach can identify deceptive support <em>before</em> misinformation begins to spread. An important discovery was that among the posts that contain a seemingly factual claim while citing a news article as supporting evidence, at least 1% include a citation intended to deceive the reader.</div>\n<div><strong>Misleading information can also involve subtle semantic changes</strong> (e.g., selective reporting, disease-mongering). These attributes make the automatic detection of semantic changes a daunting challenge, and had previously only been explored by journalists and healthcare professionals in purely qualitative studies. The research team built another novel corpus, and conducted experiments using cutting edge language models to test whether medical news articles mislead readers in these ways. This was found to be an extremely challenging task with direct ramifications in misinformation, but the team demonstrated that AI-based software has the potential to identify such subtle flaws in reportage.</div>\n<div>The project has made significant and novel contributions in our understanding of how and why medical information in news and social media may deviate from original scientific findings. It has produced three unique datasets and novel cross-genre IR models. The results have been disseminated through five peer-reviewed publications in workshops and conference proceedings, and two peer-reviewed publications in reputable journals. The data and the AI software packages are made publicly available on GitHub, an Internet hosting platform. Two Ph.D., three M.S., and two undergraduate students were trained in theoretical and experimental work in IR, ML, and natural language processing.</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/28/2022<br>\n\t\t\t\t\tModified by: Ritwik&nbsp;Banerjee</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nChanges in the meaning of information as it passes through cyberspace can mislead those who access the information. The goal of this project was to identify changes in medical information. The project was driven by the idea that by studying semantic changes in information, it is possible to understand misinformation without introducing any external adjudication or bias. In quantitative AI-based misinformation analysis, this was the first project to preclude human judgment of truth and falsehood for any specific piece of information.\n\nThe project began by developing a supervised machine learning (ML) model to identify information worth verifying. It then used information retrieval (IR) algorithms to identify how information was presented to readers in original peer-reviewed publications, and how the same information was presented using vastly different language in newswire articles. The key observation was that changes in medical information can largely be attributed to two phenomena:\n\nInformation propagates across multiple distinct genres ? from research literature to news articles to social media, where each genre has its own linguistic properties and pragmatic hurdles to overcome.\nInformation is often distorted due to subtle modifications (e.g., oversimplification) instead of outright false claims.\n\nThis project pursued scientific investigations in both directions.\nThe research team investigated linguistic transformations occurring when information transitions from specialized research articles into news intended for wider readership. This transition makes the information vulnerable to misinterpretation and misrepresentation, which are difficult to identify without specialized medical knowledge, and may exist even in the presence of explicit citations. Moreover, news articles seldom provide a precise correspondence between a claim and its origin, making it harder to identify which claims, if any, reflect the original findings. For instance, ?Flagellin shows therapeutic potential with H3N2, known as Aussie Flu.? contains two claims: Flagellin show therapeutic potential with H3N2, and H3N2 is known as Aussie Flu. These claims may be true or false independent of each other. It is prima facie unclear which claims, if any, are supported by the cited research. This difficult problem was overcome by developing a corpus of medical news along with the sources from medical research journals these news articles cite. It was then used to study what a reader perceives to be true, and how to verify the scientific source of claims. This was the first dataset to allow a study of information distortion across two genres with disparate readership and different vocabularies. This project presented the very first AI-based study of health-related fact-checking across these genres. The researchers also investigated how trust is used to create belief in misleading claims. Social media posts often leverage the trust readers have in prestigious news agencies and cite news articles for credibility. It is not, however, always the case that the cited article supports the claim being upheld in the social media post. In other words, the post only makes it ?look? like the claim originates from a credible source! The research team designed and developed a cross-genre IR model to identify whether the information in a Twitter post is, indeed, supported by the cited article. For this, a large collection of nearly 47 million posts about COVID was used to develop AI models to detect Tweets containing claim worth checking and verify whether the claims made in a Tweet are supported by the newswire article it cites. Unlike previous studies that detect unsubstantiated information by analysis of propagation patterns, this approach can identify deceptive support before misinformation begins to spread. An important discovery was that among the posts that contain a seemingly factual claim while citing a news article as supporting evidence, at least 1% include a citation intended to deceive the reader.\nMisleading information can also involve subtle semantic changes (e.g., selective reporting, disease-mongering). These attributes make the automatic detection of semantic changes a daunting challenge, and had previously only been explored by journalists and healthcare professionals in purely qualitative studies. The research team built another novel corpus, and conducted experiments using cutting edge language models to test whether medical news articles mislead readers in these ways. This was found to be an extremely challenging task with direct ramifications in misinformation, but the team demonstrated that AI-based software has the potential to identify such subtle flaws in reportage.\nThe project has made significant and novel contributions in our understanding of how and why medical information in news and social media may deviate from original scientific findings. It has produced three unique datasets and novel cross-genre IR models. The results have been disseminated through five peer-reviewed publications in workshops and conference proceedings, and two peer-reviewed publications in reputable journals. The data and the AI software packages are made publicly available on GitHub, an Internet hosting platform. Two Ph.D., three M.S., and two undergraduate students were trained in theoretical and experimental work in IR, ML, and natural language processing.\n\n\t\t\t\t\tLast Modified: 08/28/2022\n\n\t\t\t\t\tSubmitted by: Ritwik Banerjee"
 }
}