{
 "awd_id": "1814522",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Applying discrete reasoning steps in solving natural language processing tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 447614.0,
 "awd_amount": 457212.0,
 "awd_min_amd_letter_date": "2018-07-25",
 "awd_max_amd_letter_date": "2020-06-29",
 "awd_abstract_narration": "Modern natural language processing systems are effective at shallow analysis of unstructured text data, performing tasks such as discovering events, identifying the actors of those events, and grouping events with the same actors. Neural networks help make these systems robust to effects like paraphrasing, but still capture mostly superficial text patterns. To answer deeper questions about things like causal relationships between the events in a text, a system might need to combine several pieces of information, abstract away irrelevant details, and incorporate prior world knowledge to arrive at an answer. This project aims to develop systems that can address these challenges: these systems explicitly model reasoning over text and draw on the power of neural networks to do this reasoning in a nuanced way. Such reasoning is explicitly taught to the systems via \"handholding\" supervision, which encourages the systems to mimic how humans solve a problem and helps them generalize better to new problem instances. This alignment with what humans do also serves to expose the systems' decision-making processes; it provides a form of explanation of their behavior so that one may evaluate them against desired criteria such as equitability.\r\n\r\nThis proposal's technical innovation is focused on two fronts: designing latent variable models and exploiting new types of handholding supervision during model training. These techniques are explored in the context of three challenging problems requiring complex reasoning: (1) solving mathematical word problems; (2) resolving coreference using world knowledge; (3) answering questions from documents. For each problem, new models are proposed centering around discrete derivations of answers, which draw on state-of-the-art tools like attention-based recurrent neural networks to capture the larger context of the reasoning process. The discreteness of the models' decisions provides an anchor to incorporate auxiliary supervision, which is hard to do in fully end-to-end neural models. The nature of the handholding supervision depends on the task and is a combination of incidental supervision, heuristically identified derivations, and targeted human annotation. Each of the addressed problems tests different aspects of the approach, such as handling complex derivations and incorporating world knowledge, and these problems yield concrete evaluation frameworks to understand the efficacy of the proposed techniques.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Durrett",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory C Durrett",
   "pi_email_addr": "gdurrett@cs.utexas.edu",
   "nsf_id": "000737219",
   "pi_start_date": "2018-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "2317 Speedway",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121757",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 447614.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 9598.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explored the development of methods for supervising discrete reasoning processes in neural network models for natural language processing (NLP). Large neural networks have been successfully applied to many disciplines involving machine learning, including NLP. Typically these models are applied as black boxes optimized in an end-to-end fashion. As a result, they are not interpretable or controllable, and when they fail on complex problems like answering a sophisticated question (\"who was vice president the year the Vietnam war ended?\"), their failures are hard to diagnose.</p>\n<p>This project studied three different problems: resolving entities, answering questions, and constructing programs given a user's intent. Across all three domains, the project resulted in new methods for reasoning with intermediate structures. For example, resolving an entity might entail recognizing that \"Michael Jordan\" in a given context actually refers to a machine learning professor and not the basketball player. If one can recognize that the description of Michael Jordan in this context is probably talking about a scientist, one can correctly resolve this reference to a Wikipedia article (Michael I Jordan). The methods in this project pioneered systems for doing better fine-grained typing and then showed how judgments about these types (scientist) can help build systems to resolve entities to Wikipedia articles. Crucially, these systems transfer better to new domains of text than existing end-to-end deep learning systems. Finally, a user can intervene on these representations directly to \"debug\" a system on-the-fly, a key feature of the approach presented here.</p>\n<p>In question answering, this project similarly showed that intermediate representation of the reasoning needed to answer a question can improve the robustness of models: that is, how well they can work in new settings beyond those they were trained on. An intriguing new application of these methods that the project recently pioneered was fact-checking. By validating intermediate parts of a complex political claim, automatic systems can both do a better job of fact-checking claims in general and also provide a user a more detailed picture of why a claim is true or false and what parts of it hold water.</p>\n<p>Since the project started, large pre-trained neural network language models like GPT-3 have dramatically advanced the capabilities of NLP systems. The dominant paradigm in 2022 is to solve problems by generating the answer from these models. Results from this project show a template for how to do this: what intermediate representations are useful, how to supervise them, and what impact they may have on a model's \"reasoning\" process.&nbsp; However, the results also showed that these representations still have a way to go, and large language models can produce explanations that may be unreliable: not faithful to their inputs and not explaining the decisions they give. Optimism about the potential of these approaches must therefore be tempered with the recognition of their limitations.</p>\n<p>The project also supported educational initiatives and teaching at both the high school and undergraduate levels, as well as training of PhD students. Support to the primary PI aided the development of NLP courses at UT Austin that have so far served over 1000 students, as well as outreach to high school student at 12 high schools across the state of Texas.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/27/2022<br>\n\t\t\t\t\tModified by: Gregory&nbsp;C&nbsp;Durrett</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596486817_thrust3-fig1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596486817_thrust3-fig1--rgov-800width.jpg\" title=\"Fact-checking with decomposed questions\"><img src=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596486817_thrust3-fig1--rgov-66x44.jpg\" alt=\"Fact-checking with decomposed questions\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A prototype fact-checking system that decomposes a complex claim into pieces and then checks each one individually. Compared to a neural network model that only returns a final judgment, this project's approach leverages these subquestions to find evidence for the user and return an explanation.</div>\n<div class=\"imageCredit\">Chen, Sriram, Choi, Durrett, EMNLP 2022</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Gregory&nbsp;C&nbsp;Durrett</div>\n<div class=\"imageTitle\">Fact-checking with decomposed questions</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596319837_thrust2-nocaption--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596319837_thrust2-nocaption--rgov-800width.jpg\" title=\"Box embeddings for entity typing\"><img src=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596319837_thrust2-nocaption--rgov-66x44.jpg\" alt=\"Box embeddings for entity typing\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An explicitly-structured embedding space to determine the entity types associated with a mention of a real-world entity in context. These can be used as intermediate products for making additional judgments about that entity or reasoning about it in downstream systems.</div>\n<div class=\"imageCredit\">Onoe, Boratko, McCallum, and Durrett, ACL 2021</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Gregory&nbsp;C&nbsp;Durrett</div>\n<div class=\"imageTitle\">Box embeddings for entity typing</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596251496_thrust1-nocaption--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596251496_thrust1-nocaption--rgov-800width.jpg\" title=\"Intermediate reasoning for textual question answering with GPT-3\"><img src=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596251496_thrust1-nocaption--rgov-66x44.jpg\" alt=\"Intermediate reasoning for textual question answering with GPT-3\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">When GPT-3 is prompted with explanations, it can produce explanations of its reasoning which are sometimes wrong. However, this can allow a system designer to improve the reliability of the system by detecting when such wrong explanations are produced, a key feature of this project's approach.</div>\n<div class=\"imageCredit\">Xi Ye and Greg Durrett, NeurIPS 2022</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Gregory&nbsp;C&nbsp;Durrett</div>\n<div class=\"imageTitle\">Intermediate reasoning for textual question answering with GPT-3</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596596920_thrust3-fig2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596596920_thrust3-fig2--rgov-800width.jpg\" title=\"Question answering through graph alignment\"><img src=\"/por/images/Reports/POR/2022/1814522/1814522_10561285_1669596596920_thrust3-fig2--rgov-66x44.jpg\" alt=\"Question answering through graph alignment\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Example of answering a question by aligning a graph over the question with a graph over the \"context\" that that question is to be answered based on.  Using such structures, as this project explored, makes systems more robust to answering questions different than those in its training data.</div>\n<div class=\"imageCredit\">Chen and Durrett, NAACL 2021</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Gregory&nbsp;C&nbsp;Durrett</div>\n<div class=\"imageTitle\">Question answering through graph alignment</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project explored the development of methods for supervising discrete reasoning processes in neural network models for natural language processing (NLP). Large neural networks have been successfully applied to many disciplines involving machine learning, including NLP. Typically these models are applied as black boxes optimized in an end-to-end fashion. As a result, they are not interpretable or controllable, and when they fail on complex problems like answering a sophisticated question (\"who was vice president the year the Vietnam war ended?\"), their failures are hard to diagnose.\n\nThis project studied three different problems: resolving entities, answering questions, and constructing programs given a user's intent. Across all three domains, the project resulted in new methods for reasoning with intermediate structures. For example, resolving an entity might entail recognizing that \"Michael Jordan\" in a given context actually refers to a machine learning professor and not the basketball player. If one can recognize that the description of Michael Jordan in this context is probably talking about a scientist, one can correctly resolve this reference to a Wikipedia article (Michael I Jordan). The methods in this project pioneered systems for doing better fine-grained typing and then showed how judgments about these types (scientist) can help build systems to resolve entities to Wikipedia articles. Crucially, these systems transfer better to new domains of text than existing end-to-end deep learning systems. Finally, a user can intervene on these representations directly to \"debug\" a system on-the-fly, a key feature of the approach presented here.\n\nIn question answering, this project similarly showed that intermediate representation of the reasoning needed to answer a question can improve the robustness of models: that is, how well they can work in new settings beyond those they were trained on. An intriguing new application of these methods that the project recently pioneered was fact-checking. By validating intermediate parts of a complex political claim, automatic systems can both do a better job of fact-checking claims in general and also provide a user a more detailed picture of why a claim is true or false and what parts of it hold water.\n\nSince the project started, large pre-trained neural network language models like GPT-3 have dramatically advanced the capabilities of NLP systems. The dominant paradigm in 2022 is to solve problems by generating the answer from these models. Results from this project show a template for how to do this: what intermediate representations are useful, how to supervise them, and what impact they may have on a model's \"reasoning\" process.  However, the results also showed that these representations still have a way to go, and large language models can produce explanations that may be unreliable: not faithful to their inputs and not explaining the decisions they give. Optimism about the potential of these approaches must therefore be tempered with the recognition of their limitations.\n\nThe project also supported educational initiatives and teaching at both the high school and undergraduate levels, as well as training of PhD students. Support to the primary PI aided the development of NLP courses at UT Austin that have so far served over 1000 students, as well as outreach to high school student at 12 high schools across the state of Texas.\n\n\t\t\t\t\tLast Modified: 11/27/2022\n\n\t\t\t\t\tSubmitted by: Gregory C Durrett"
 }
}