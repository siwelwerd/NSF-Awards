{
 "awd_id": "1829403",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Parallel Methods for Large-Scale Probabilistic Inference",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2017-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 433499.0,
 "awd_amount": 433499.0,
 "awd_min_amd_letter_date": "2018-04-17",
 "awd_max_amd_letter_date": "2018-04-17",
 "awd_abstract_narration": "We are undergoing a revolution in data.  We have grown accustomed to constant upheaval in computing -- quicker processors, bigger storage and faster networks -- but this century presents the new challenge of almost unlimited access to raw data.  Whether from sensor networks, social computing, or high-throughput cell biology, we face a deluge of data about our world.  Scientists, engineers, policymakers, and industrialists need to use these enormous floods of data to make better decisions.  This research project is about providing foundations for tools to achieve these goals. Simple models give only coarse understanding.  The world is sophisticated and dynamic, providing rich information.  Furthermore, representation of uncertainty is critical to discovering patterns in complex data.  Not only are many natural processes intrinsically random, but our knowledge is always limited.  The calculus of probability allows us to represent this uncertainty and design algorithms to act effectively in an unpredictable world. The gold standard for probabilistic analysis is Markov chain Monte Carlo (MCMC), a way to identify hypotheses about the unobserved structure of the world that are consistent with observed data.  It is a powerful and principled way to perform data analysis, but traditional MCMC methods do not map well onto modern computing environments.  MCMC is a sequential procedure that cannot generally take advantage of the parallelism offered by multi-core desktops and laptops, cloud computing, and graphical processing units.  This research will develop new methods for MCMC that are provably correct, but that take advantage of large-scale parallel computing. There are a variety of broader impacts of this work.  In addition to the core technical contributions, the project engages in deep scientific collaborations.  New photovoltaic materials will lead to better solar cells and more sustainable energy production.  New techniques for uncovering genetic regulatory mechanisms will lead to better understanding of disease. Quantitative models of mouse activity will give insight into the neural basis of behavior and provide a deeper understanding of brain disorders.  \r\n\r\nFrom a technical point of view, this work pursues two complementary approaches to large-scale Bayesian data analysis with MCMC: 1) a novel general-purpose framework for sharing of information between parallel Markov chains for faster mixing, and 2) a new computational concept for speculative parallelization of individual Markov chains. These theoretical and practical explorations, combined with the release of associated open source software, will yield more robust and scalable probabilistic modeling. It will develop provably-correct foundations and efficient new algorithms for parallelization of Markov transition operators for posterior simulation.  These operators will be used in three collaborations that are representative of the methodological demands for large-scale statistical inference: 1) predicting the efficiencies of novel organic photovoltaic materials, 2) discovering new genetic regulatory mechanisms, and 3) quantitative neuroscientific models for mouse behavior. While this proposal focuses on the generalizable technical challenges of these problems, these collaborations provide compelling examples of how machine learning can be broadly transformative.\r\n\r\nFinally, the project includes a significant outreach component, engaging with local middle schoolers, and involving underrepresented minorities in summer research.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ryan",
   "pi_last_name": "Adams",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Ryan P Adams",
   "pi_email_addr": "rpa@princeton.edu",
   "nsf_id": "000623159",
   "pi_start_date": "2018-04-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "87 Prospect Avenue - 2nd floor",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 433499.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this work was to develop new ways to make inferences about the structure of the world.&nbsp; There are many scientific phenomena that we can only observe indirectly; we get noisy information about these \"latent\" properties, but we'd like to be able to understand and reason about the underlying complexity.&nbsp; By understanding this latent structure, we can make better decisions and predictions as well as develop more accurate scientific hypotheses.</p>\n<p>It is a daunting computational task, however, to perform such inference.&nbsp; It often involve merging many different kinds of observations, and perhaps many examples of each.&nbsp; The resulting \"posterior distribution\" may be high-dimensional and complex.&nbsp; Generally, we have two ways to manipulate such distributions in support of inference: Markov chain Monte Carlo, and variational inference.&nbsp; These complementary techniques use random sampling and optimization, respectively.&nbsp; &nbsp;This project's goal was to develop new methods that enable these methods to scale to larger and more difficult problems.&nbsp;</p>\n<p>The project was a success in terms of performing fundamental methodogical work to develop new ways to perform inference, particularly variational techniques.&nbsp; It also developed new approaches to amortized inference (using a deep neural network to accelerate optimization) and several completely new ideas on how to approximate quantities of interest with randomized techniques.</p>\n<p>One of the exciting aspects of this work was that it also enabled collaboration and exploration across multiple scientific disciplines, including chemistry, genetics, continuum mechanics, and neuroscience.&nbsp; The two figures below show examples from some of the resulting collaborative work in chemistry and neuroscience.</p>\n<p>The outcomes of this work have been very impactful, appearing in top venues such as NeurIPS, ICML, Neuron, and Nature Genetics.&nbsp; Multiple of the papers resulting from this project have garnered hundreds of citations.&nbsp; The trainees involved in the work have benefited significantly, going on to faculty positions at Stanford, Cambridge, and other top universities, as well as top industry research positions at places such as Google and Microsoft.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/24/2020<br>\n\t\t\t\t\tModified by: Ryan&nbsp;P&nbsp;Adams</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1829403/1829403_10334013_1606236477685_ScreenShot2020-11-24at11.32.43AM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1829403/1829403_10334013_1606236477685_ScreenShot2020-11-24at11.32.43AM--rgov-800width.jpg\" title=\"Illustration of approximate inference\"><img src=\"/por/images/Reports/POR/2020/1829403/1829403_10334013_1606236477685_ScreenShot2020-11-24at11.32.43AM--rgov-66x44.jpg\" alt=\"Illustration of approximate inference\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The goal of Bayesian inference is to approximate a complicated posterior distribution arising from data.  This is a simple example showing how optimization can be understood as inference.</div>\n<div class=\"imageCredit\">Duvenaud et al. Early Stopping as Nonparametric Variational Inference</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ryan&nbsp;P&nbsp;Adams</div>\n<div class=\"imageTitle\">Illustration of approximate inference</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1829403/1829403_10334013_1606236372637_ScreenShot2020-11-24at11.32.01AM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1829403/1829403_10334013_1606236372637_ScreenShot2020-11-24at11.32.01AM--rgov-800width.jpg\" title=\"Probabilistic model for natural mouse behavior\"><img src=\"/por/images/Reports/POR/2020/1829403/1829403_10334013_1606236372637_ScreenShot2020-11-24at11.32.01AM--rgov-66x44.jpg\" alt=\"Probabilistic model for natural mouse behavior\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This is a figure from our Neuron paper on developing probabilistic modeling and inference techniques for mouse behavior in support of neuroscience problems.</div>\n<div class=\"imageCredit\">Wiltschko et al. Mapping Sub-Second Structure in Mouse Behavior</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ryan&nbsp;P&nbsp;Adams</div>\n<div class=\"imageTitle\">Probabilistic model for natural mouse behavior</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1829403/1829403_10334013_1606236222152_ScreenShot2020-11-24at11.29.52AM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1829403/1829403_10334013_1606236222152_ScreenShot2020-11-24at11.29.52AM--rgov-800width.jpg\" title=\"Graph convolutional neural network\"><img src=\"/por/images/Reports/POR/2020/1829403/1829403_10334013_1606236222152_ScreenShot2020-11-24at11.29.52AM--rgov-66x44.jpg\" alt=\"Graph convolutional neural network\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This is an illustration of a new architecture for learning and inference with molecules, a graph neural network.  This architecture is now one of the dominant ways that these systems are solved.</div>\n<div class=\"imageCredit\">Duvenaud et al. Convolutional Networks on Graphs for Learning Molecular Fingerprints</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ryan&nbsp;P&nbsp;Adams</div>\n<div class=\"imageTitle\">Graph convolutional neural network</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe goal of this work was to develop new ways to make inferences about the structure of the world.  There are many scientific phenomena that we can only observe indirectly; we get noisy information about these \"latent\" properties, but we'd like to be able to understand and reason about the underlying complexity.  By understanding this latent structure, we can make better decisions and predictions as well as develop more accurate scientific hypotheses.\n\nIt is a daunting computational task, however, to perform such inference.  It often involve merging many different kinds of observations, and perhaps many examples of each.  The resulting \"posterior distribution\" may be high-dimensional and complex.  Generally, we have two ways to manipulate such distributions in support of inference: Markov chain Monte Carlo, and variational inference.  These complementary techniques use random sampling and optimization, respectively.   This project's goal was to develop new methods that enable these methods to scale to larger and more difficult problems. \n\nThe project was a success in terms of performing fundamental methodogical work to develop new ways to perform inference, particularly variational techniques.  It also developed new approaches to amortized inference (using a deep neural network to accelerate optimization) and several completely new ideas on how to approximate quantities of interest with randomized techniques.\n\nOne of the exciting aspects of this work was that it also enabled collaboration and exploration across multiple scientific disciplines, including chemistry, genetics, continuum mechanics, and neuroscience.  The two figures below show examples from some of the resulting collaborative work in chemistry and neuroscience.\n\nThe outcomes of this work have been very impactful, appearing in top venues such as NeurIPS, ICML, Neuron, and Nature Genetics.  Multiple of the papers resulting from this project have garnered hundreds of citations.  The trainees involved in the work have benefited significantly, going on to faculty positions at Stanford, Cambridge, and other top universities, as well as top industry research positions at places such as Google and Microsoft.\n\n\t\t\t\t\tLast Modified: 11/24/2020\n\n\t\t\t\t\tSubmitted by: Ryan P Adams"
 }
}