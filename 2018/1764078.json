{
 "awd_id": "1764078",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI:Medium:Collaborative Research: Object-Centric Inference of Actionable Information from Visual Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 425000.0,
 "awd_amount": 425000.0,
 "awd_min_amd_letter_date": "2018-08-14",
 "awd_max_amd_letter_date": "2018-08-14",
 "awd_abstract_narration": "This project will create novel algorithms and learning architectures suitable for understanding how to plan and execute actions in an environment for purposeful object manipulation. Such understanding is indispensable for autonomous agents operating in unstructured environments, and it is also valuable in providing automated assistance to humans during the execution of various physical tasks. The project will computationally \"imagine\" changes that actors with human-like manipulation capabilities can make on that environment and generate plans that can accomplish the desired manipulations. Such tools facilitate the creation of smart environments, where for example a perception system watching an elderly person can infer the task the person is trying to accomplish and offer advice/assistance. They also allow the creation of automated instructional videos customized to a particular environment that can be used for efficient training of unskilled workers. The project will provide mentoring and research opportunities for a diverse set of students, including members of groups typically under-represented in computer science.\r\n\r\nThis research will study environments formed by objects, some of which can be manipulated, while others define obstacles to be avoided or support surfaces to be used. Manipulating an object typically means interacting with small parts of the object, referred to as its active sites: handles, buttons, levers, graspable or pushable regions, etc. A deep challenge is to develop tools for identifying and classifying these active sites on objects at large scale, and to codify the types of interactions they partake of based on dynamic 2D/3D imagery, building a vocabulary of elementary actions. This requires novel machine learning methods and deep architectures for processing large-scale dynamic visual and geometric data. It also requires characterizing manipulations at a more abstract level so that they can be used by a variety of effectors, robotic or human, on different object geometries and physical characteristics. A further challenge is the accumulation and update of actionable information as more visual data is received in online object model repositories, such as ShapeNet. A final but key step of the approach will be the development of tools for transporting such action knowledge to new settings that are similar but not identical to the capture settings, using a variety of mathematical tools including functional maps.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hao",
   "pi_last_name": "Su",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hao Su",
   "pi_email_addr": "haosu@ucsd.edu",
   "nsf_id": "000758031",
   "pi_start_date": "2018-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Drive MC0934",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 425000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of environment perception is often the planning and execution of actions -- for both humans and autonomous robotic agents. In this effort we develop algorithms and data sets that support object-centric actionable information extraction -- that is low-dimensional information that is sufficient for planning and action. Our research is motivated by the need to develop algorithms that can extract actionable information from visual data, and to develop data sets that can be used to train and evaluate such algorithms. The major research outcome includes the following:</p>\n<ul>\n<li>\n<p>We developed PartNet, a large-scale 3D shape dataset with fine-grained object parts organized hierarchically based on the AND/OR graph framework. PartNet is nowadays the most widely used 3D object part dataset by academia and industry. To further push the study of manipulation, we developed a widely used robot simulation platform, SAPIEN, that can support the simulation of articulated objects in PartNet using the physical engine. Finally, we developed a rich space of object manipulation tasks, called ManiSkill, using 3D models in PartNet and the SAPIEN robot simulator.</p>\n</li>\n<li>\n<p>We developed algorithms to discover how objects can potentially be used using the PartNet dataset and SAPIEN platform. Representative work in this line includes the grasp pose analysis algorithm, S4G, and object-object interaction affordance analysis algorithm, O2O Afford;</p>\n</li>\n<li>\n<p>We studied algorithms that can understand the structure of objects based on the analysis-by-synthesis paradigm. Representative work in this line includes the structure-aware 3D generative model, StructureNet and StructEdit;</p>\n</li>\n<li>\n<p>We investigated algorithms that can infer the structure and usability of objects from the real world. Toward this goal, we proposed high-quality 3D capture algorithms such as PointMVSNet and MVSNeRF, and we introduced structure-aware 3D pose tracking algorithm for articulated objects, CAPTRA;</p>\n</li>\n<li>\n<p>To collect data on object manipulation, we studied several algorithms in the areas of physical simulation and low-cost interaction trajectory collection. Representative work in this line includes our latest work on collision shape analysis (CoACD), teleoperation, and model-based reinforcement learning algorithms (TD-MPC).</p>\n</li>\n</ul>\n<p>To sum up, the outcomes of the NSF project have laid the foundation of datasets and algorithms for the study of manipulation. Papers published with the support of the NSF project have been cited over 1000 times, and datasets/softwares have been widely used by many researchers and companies in the world.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/23/2023<br>\n\t\t\t\t\tModified by: Hao&nbsp;Su</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1764078/1764078_10570855_1674479204266_teaser_full2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1764078/1764078_10570855_1674479204266_teaser_full2--rgov-800width.jpg\" title=\"Affordance Learning\"><img src=\"/por/images/Reports/POR/2023/1764078/1764078_10570855_1674479204266_teaser_full2--rgov-66x44.jpg\" alt=\"Affordance Learning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">For a given object-object interaction task, our method takes as inputs a 3D acting object point cloud (a) and a partial 3D scan of the scene/object (b), and outputs an affordance prediction heatmap (c) that estimates the likelihood of the acting object successfully accomplishing the task.</div>\n<div class=\"imageCredit\">Hao Su</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Hao&nbsp;Su</div>\n<div class=\"imageTitle\">Affordance Learning</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1764078/1764078_10570855_1674478929649_teaser--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1764078/1764078_10570855_1674478929649_teaser--rgov-800width.jpg\" title=\"ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills\"><img src=\"/por/images/Reports/POR/2023/1764078/1764078_10570855_1674478929649_teaser--rgov-66x44.jpg\" alt=\"ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">ManiSkill2 provides a unified, fast, and accessible system that encompasses well-curated manipulation tasks (e.g., stationary/mobile-base, single/dual-arm, rigid/soft-body).</div>\n<div class=\"imageCredit\">Hao Su</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Hao&nbsp;Su</div>\n<div class=\"imageTitle\">ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe goal of environment perception is often the planning and execution of actions -- for both humans and autonomous robotic agents. In this effort we develop algorithms and data sets that support object-centric actionable information extraction -- that is low-dimensional information that is sufficient for planning and action. Our research is motivated by the need to develop algorithms that can extract actionable information from visual data, and to develop data sets that can be used to train and evaluate such algorithms. The major research outcome includes the following:\n\n\n\nWe developed PartNet, a large-scale 3D shape dataset with fine-grained object parts organized hierarchically based on the AND/OR graph framework. PartNet is nowadays the most widely used 3D object part dataset by academia and industry. To further push the study of manipulation, we developed a widely used robot simulation platform, SAPIEN, that can support the simulation of articulated objects in PartNet using the physical engine. Finally, we developed a rich space of object manipulation tasks, called ManiSkill, using 3D models in PartNet and the SAPIEN robot simulator.\n\n\n\nWe developed algorithms to discover how objects can potentially be used using the PartNet dataset and SAPIEN platform. Representative work in this line includes the grasp pose analysis algorithm, S4G, and object-object interaction affordance analysis algorithm, O2O Afford;\n\n\n\nWe studied algorithms that can understand the structure of objects based on the analysis-by-synthesis paradigm. Representative work in this line includes the structure-aware 3D generative model, StructureNet and StructEdit;\n\n\n\nWe investigated algorithms that can infer the structure and usability of objects from the real world. Toward this goal, we proposed high-quality 3D capture algorithms such as PointMVSNet and MVSNeRF, and we introduced structure-aware 3D pose tracking algorithm for articulated objects, CAPTRA;\n\n\n\nTo collect data on object manipulation, we studied several algorithms in the areas of physical simulation and low-cost interaction trajectory collection. Representative work in this line includes our latest work on collision shape analysis (CoACD), teleoperation, and model-based reinforcement learning algorithms (TD-MPC).\n\n\n\nTo sum up, the outcomes of the NSF project have laid the foundation of datasets and algorithms for the study of manipulation. Papers published with the support of the NSF project have been cited over 1000 times, and datasets/softwares have been widely used by many researchers and companies in the world.\n\n \n\n\t\t\t\t\tLast Modified: 01/23/2023\n\n\t\t\t\t\tSubmitted by: Hao Su"
 }
}