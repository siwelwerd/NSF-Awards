{
 "awd_id": "1763981",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Inverse Reinforcement Learning for Human Attention Modeling",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2018-06-15",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 1198950.0,
 "awd_amount": 1198950.0,
 "awd_min_amd_letter_date": "2018-06-08",
 "awd_max_amd_letter_date": "2018-09-14",
 "awd_abstract_narration": "The process by which people shift their attention from one thing to another touches upon everything that we think and do, and as such has widespread importance in fields ranging from basic research and education to applications in industry and national defense. This research develops a computational model for predicting these human shifts in visual attention. Prediction is understanding, and with this model we will achieve a greater understanding of this core human cognitive process. More tangibly, prediction enables applications to anticipate where attention will shift in response to seeing specific imagery. This in turn would usher in 1) a new generation of human-computer interactive systems, ones capable of interacting with users at the level of their attention movements, and 2) novel ways to annotate and index visual content based on attentional importance or interest. This project combines computational work with cognitive science and digital media, providing an entry to computer science through valuable learning experiences for women and underrepresented minorities who might otherwise be intimidated by traditional computational work. The project broadens the exposure of underrepresented minorities to STEM through partnerships with several ongoing efforts at Stony Brook University, including the Women in Science and Engineering (WISE) and Louis Stokes Alliance for Minority Participation (LSAMP) initiatives.\r\n\r\nThis project investigates a synergistic computational and behavioral approach for modeling the movements of human attention. This approach is based on an assumption that attentional engagement on an image (or video frame) depends on both the pixels that are being viewed and the viewer's previous state. Based on this assumption, visual attention is posed as a Markov decision process, and inverse reinforcement learning is used to learn a reward function to associate specific spatio-temporal regions in an image, corresponding to the pixels at a viewer's momentary locus of attention, with a reward. Under this novel approach, the attention mechanism is treated as an agent whose action is to select a location in an image or image frame that will maximize its total reward. This model is being evaluated against a behavioral ground truth consisting of the eye movements that people make as they view images and video in the context of free viewing and visual search tasks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Minh Hoai",
   "pi_last_name": "Nguyen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Minh Hoai Nguyen",
   "pi_email_addr": "minhhoai@cs.stonybrook.edu",
   "nsf_id": "000678789",
   "pi_start_date": "2018-06-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Zelinsky",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory J Zelinsky",
   "pi_email_addr": "gregory.zelinsky@stonybrook.edu",
   "nsf_id": "000209271",
   "pi_start_date": "2018-06-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Dimitrios",
   "pi_last_name": "Samaras",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dimitrios Samaras",
   "pi_email_addr": "samaras@cs.sunysb.edu",
   "nsf_id": "000096125",
   "pi_start_date": "2018-06-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "Stony Brook University",
  "perf_city_name": "Stony Brook",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117902424",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 1198950.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The process by which people allocate and shift their attention touches upon everything that we think and do, and as such has widespread importance in fields ranging from basic research and education to applications in industry and national defense. This NSF award provided funding for a team of researchers and graduate students at Stony Brook University to develop computational models for predicting human visual attention and for understanding how visual attention was shifted from one thing to another.&nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>The outcomes of the project encompass a diverse range of achievements, including advanced models and algorithms for visual attention prediction, datasets capturing gaze behavior for training and evaluating the models, research papers contributing to a deeper understanding of human visual attention, and the training of graduate students.</p>\n<p>&nbsp;</p>\n<p>In terms of computational models and algorithms, the specific outcomes include: (i) baseline methods for fixation prediction were created, focusing on comprehending and predicting the points where visual fixation occurs during the visual search process; (ii) an innovative model based on inverse reinforcement learning was developed to predict scan paths, thereby enhancing the understanding of the rewards associated with attentional shifts; (iii) a model for target-absent scenarios, enabling the prediction of search behavior when the target object was absent in the image; (iv) an algorithm to predict stopping criteria, determining when individuals would cease searching for a specific target; (v) a gaze prediction model for the zero-gaze setting, enabling the generalization of predictions to situations where the target category had not been encountered during training.</p>\n<p>&nbsp;</p>\n<p>Regarding dataset collection, the project yielded the following outcomes: (i) multiple visual search datasets, namely Microwave-Clock, COCO-Search18, and COCO-Freeview. These datasets encompassed various object categories and included gaze fixations, free-viewing eye movements, and mouse movements recorded during search behavior; (ii) a multimodal behavioral dataset called RefCOCO-Gaze, capturing search behavior while subjects listened to audio descriptions of objects they needed to find in images. This dataset aimed to capture the natural interaction between audio cues and visual attention.</p>\n<p>&nbsp;</p>\n<p>The project significantly advanced the understanding of human visual attention, and the findings were disseminated through the publication of a dozen research papers at prestigious conferences and workshops, such as CVPR, ECCV, and Scientific Reports. These publications showcased the research team's substantial contributions to the field, sharing valuable insights and advancements.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Moreover, the project provided funding to train graduate students and created opportunities for self-funded graduate students to work on cutting-edge research problems. Over the project's duration, at least 10 students worked on it, either full time or part-time. The project's Principal Investigator (PI) and Co-PIs utilized it to educate and train these graduate students, honing their research, problem-solving, writing, and presentation skills. Notably, three PhD students developed their theses based on this project, and another student is on the verge of deriving their thesis from the work done in this project.</p>\n<p>In short, the project's outcomes encompass advanced visual attention prediction models, diverse datasets, and a deeper understanding of visual attention documented in numerous publications. Additionally, it has significantly contributed to the education and training of graduate students in the field. These achievements collectively enhance our understanding of human visual attention and open new avenues for research and applications in this dynamic domain.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/29/2023<br>\n\t\t\t\t\tModified by: Minh Hoai&nbsp;Nguyen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe process by which people allocate and shift their attention touches upon everything that we think and do, and as such has widespread importance in fields ranging from basic research and education to applications in industry and national defense. This NSF award provided funding for a team of researchers and graduate students at Stony Brook University to develop computational models for predicting human visual attention and for understanding how visual attention was shifted from one thing to another.  \n\n \n\nThe outcomes of the project encompass a diverse range of achievements, including advanced models and algorithms for visual attention prediction, datasets capturing gaze behavior for training and evaluating the models, research papers contributing to a deeper understanding of human visual attention, and the training of graduate students.\n\n \n\nIn terms of computational models and algorithms, the specific outcomes include: (i) baseline methods for fixation prediction were created, focusing on comprehending and predicting the points where visual fixation occurs during the visual search process; (ii) an innovative model based on inverse reinforcement learning was developed to predict scan paths, thereby enhancing the understanding of the rewards associated with attentional shifts; (iii) a model for target-absent scenarios, enabling the prediction of search behavior when the target object was absent in the image; (iv) an algorithm to predict stopping criteria, determining when individuals would cease searching for a specific target; (v) a gaze prediction model for the zero-gaze setting, enabling the generalization of predictions to situations where the target category had not been encountered during training.\n\n \n\nRegarding dataset collection, the project yielded the following outcomes: (i) multiple visual search datasets, namely Microwave-Clock, COCO-Search18, and COCO-Freeview. These datasets encompassed various object categories and included gaze fixations, free-viewing eye movements, and mouse movements recorded during search behavior; (ii) a multimodal behavioral dataset called RefCOCO-Gaze, capturing search behavior while subjects listened to audio descriptions of objects they needed to find in images. This dataset aimed to capture the natural interaction between audio cues and visual attention.\n\n \n\nThe project significantly advanced the understanding of human visual attention, and the findings were disseminated through the publication of a dozen research papers at prestigious conferences and workshops, such as CVPR, ECCV, and Scientific Reports. These publications showcased the research team's substantial contributions to the field, sharing valuable insights and advancements. \n\n \n\nMoreover, the project provided funding to train graduate students and created opportunities for self-funded graduate students to work on cutting-edge research problems. Over the project's duration, at least 10 students worked on it, either full time or part-time. The project's Principal Investigator (PI) and Co-PIs utilized it to educate and train these graduate students, honing their research, problem-solving, writing, and presentation skills. Notably, three PhD students developed their theses based on this project, and another student is on the verge of deriving their thesis from the work done in this project.\n\nIn short, the project's outcomes encompass advanced visual attention prediction models, diverse datasets, and a deeper understanding of visual attention documented in numerous publications. Additionally, it has significantly contributed to the education and training of graduate students in the field. These achievements collectively enhance our understanding of human visual attention and open new avenues for research and applications in this dynamic domain.\n\n \n\n\t\t\t\t\tLast Modified: 07/29/2023\n\n\t\t\t\t\tSubmitted by: Minh Hoai Nguyen"
 }
}