{
 "awd_id": "1822650",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRCNS Research Proposal:  Collaborative Research: New Dimensions of Visual Cortical Organization",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 625115.0,
 "awd_amount": 625115.0,
 "awd_min_amd_letter_date": "2018-09-07",
 "awd_max_amd_letter_date": "2018-09-07",
 "awd_abstract_narration": "The visual system of the mouse is now widely studied as a model for developmental neurobiology, as well as for the understanding of human disease, because it can be studied with the most powerful modern genetic and optical tools.  This project aims to discover how neurons in the visual cortex of the mouse allow it to see well by measuring how the cortex represents ecologically-relevant properties of the visual world.  Quantitative studies of neurons in the mouse's primary visual cortex to date reveal only very poor vision, but their behavior indicates that mice can see much better than that -- they avoid predators and catch crickets in the wild. To understand mouse vision, the investigators will study responses to novel, mathematically tractable stimuli resembling the flow of images across the retina as the mouse moves through a field of grass.  Studies based on these new stimuli indicate that most V1 neurons respond reliably to fine details of the visual scene.  A mathematical understanding of how the brain takes in the visual world should have real implications for how we see, and should have great benefits for artificial vision by computers and robots.  Bringing these ideas into the classroom will provide the foundation for new technologies, and will expose students to both real and artificial vision systems.\r\n\r\nAnalyses of the brain's visual function are limited by the stimuli used to probe them. Conventional quantitative approaches to understanding biological vision have been based on models with linear kernels in which only the output might be subject to a nonlinearity, all derived from responses of neurons in the brain to gratings of a range of spatial frequencies.  This analysis fails to capture relevant features of natural images, which can not be constrained to linearity. The goal of this project is to probe the mouse visual system beyond the linear range but below the barrier posed by the complexity of arbitrary natural images. The investigators have identified an intermediate stimulus class--visual flow patterns--that formally approximate important features of natural visual scenes, resembling what an animal would see when running through grass. Flow patterns have a rich geometry that is mathematically tractable.  This project will develop such stimuli and test them on awake-behaving mice, while recording the resultant neural activity in the visual cortex.  Studying the mouse opens up the possibility of applying the entire range of powerful modern neuroscience tools-- genetic, optical, and electrophysiological. Visual responses will be analyzed using a novel variety of machine learning algorithms, which will allow the investigators to model the possible neural circuits and then test predictions from those model circuits.  Such an understanding of the brain will inform both primate vision and the next generation of artificially-intelligent algorithms which, as a result, should benefit from being more \"brain-like.\"\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Zucker",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Steven Zucker",
   "pi_email_addr": "steven.zucker@yale.edu",
   "nsf_id": "000218453",
   "pi_start_date": "2018-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "AKWatson Hall",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208285",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "732700",
   "pgm_ele_name": "CRCNS-Computation Neuroscience"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7327",
   "pgm_ref_txt": "CRCNS"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 625115.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>A challenge in neuroscience is understanding how populations of neurons operate in concert to represent<br />diverse stimuli. To meet this challenge, we have created ''encoding manifolds'' that reveal the overall<br />responses of brain areas to diverse stimuli with the resolution of individual neurons and their response<br />dynamics. The distances between neurons on the encoding manifold reflect the similarity of their responses<br />in time to the stimulus ensemble.<br /><br />We applied the encoding manifold approach to the visual system of the mouse. The mouse became a<br />popular species for vision research after one of our laboratories had characterized highly selective responses<br />of neurons in its visual cortex. While mice had advantages, such as powerful molecular tools and genetics, concern<br />arose over the seemingly poor vision implied by traditional vision science measurements of visual cortical<br />responses. We showed that this concern was incorrect by using a more natural stimulus for mice--an<br />abstract version of the 'optic flow' that the mouse would see as it ran through&nbsp; grass. Our recordings and<br />analysis showed that the mouse visual system could respond vigorously to these stimuli, and to quite fine detail. Supported by this grant and a<br />subsequent NIH grant,we expanded our experimental studies beyond the visual cortex to include the retina,<br />where vision starts.</p>\r\n<p>We then had to confront data analysis. How should these different parts of visual cortex be compared with retina? How should a neural circuit be compared<br />with an artificial neural network? Individual neural responses must be considered, of course, but neurons act together to<br />produce the sensation of vision: how is their collective action to be understood? . Since thousands of neurons are involved, global population relationships (beyond pairwise) must be considered. Our encoding<br />manifold achieved this.<br /><br />We used encoding manifolds to compare the population-level encoding of primary visual cortex of<br />mouse with that of the retina, those of five higher visual areas (VISam, VISal, VISpm, VISlm, and VISrl), and<br />those of convolutional neural networks (CNNs) used for object recognition in Artificial Intelligence. For the higher<br />cortical visual areas we used data from the Allen Institute Visual Coding--Neuropixels dataset. Using both the Stryker laboratory's visual cortex recordings and the Allen Institute data, we showed that the encoding manifold topology is topologically continuous for V1, and it is<br />similarly continuous for all the higher visual areas, with smooth coordinates spanning it that include<br />orientation selectivity and firing-rate magnitude. This confirmed our methodology, because the Allen Institute data comprised gratings only. Surprisingly, while responses to natural scenes and<br />distinctions among cortical layers and cell types were not used for its computation, all of these features were<br />organized on the encoding manifold.&nbsp; The encoding manifold also revealed a novel relationship between the encoding of<br />natural scenes relative to static gratings.</p>\r\n<p>Encoding manifolds from the retina were qualitatively different.<br />Instead of continuity as for cortex, the retinal manifold consisted of clusters of neurons corresponding to<br />known retinal ganglion cell types. Again, inference of known physiological properties confirmed our algorithmic approach.</p>\r\n<p>Finally, encoding manifolds computed from the most popular convolutional neural<br />networks (AlexNet(R1), VGG16, ResNet50, and MouseNet) all resembled that of mouse retina, suggesting<br />that CNNs are more like a big retinas than small mouse brains. These results reveal how a machine<br />learning approach can organize and make visible the structure of sensory coding, thereby revealing novel<br />relationships within and across brain areas and sensory stimuli. We believe this final result will influence the interpretation of AI systems.<br /><br />All of these results appeared in our four primary publications, below. In addition, we gave more than 40<br />presentations on these results at meetings such as CoSyne, the Society for Neuroscience and the<br />International Conference on Learning Representations, and workshops and educational outreach, such as<br />the NSF sponsored Telluride Neuromorphic Cognition Engineering Workshop.</p>\r\n<p>In closing, we expect that our novel mathematical approach of computing encoding manifolds will be useful for other areas of data analysis, such as<br />understanding sensorimotor behavior and genomic networks and development. We have begun preliminary<br />studies on these additional problems.<br /><br />Dyballa, L., Hoseini, M., Dadarlat, M., Zucker, S.W., Stryker, M. (2018), ``Flow stimuli reveal ecologically<br />appropriate responses in mouse visual cortex'', Proceedings of the National Academy of Sciences<br />(USA), 115(44), 11304--11309.<br />Dyballa, L., Zucker, S. (2023), ``IAN kernel: Iterated Adaptive Neighborhoods for manifold learning and<br />dimensionality estimation'', Neural Computation, 35 (3): 453-524.<br />Dyballa, L., Rudzite, M., Hoseini, M., Thapa, M., Stryker, M., Field, G., Zucker, S. (2024), ``Population<br />coding of stimulus features along the visual hierarchy'', Proceedings of the National Academy of<br />Sciences (USA), 121(4): e2317773121.<br />Dyballa, L., Field, G., Stryker, M., Zucker, S., (2024) Encoding manifolds constructed from grating<br />responses organize responses to natural scenes across mouse cortical visual areas, bioRxiv<br />2024.10.24.620089.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/10/2024<br>\nModified by: Steven&nbsp;Zucker</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nA challenge in neuroscience is understanding how populations of neurons operate in concert to represent\ndiverse stimuli. To meet this challenge, we have created ''encoding manifolds'' that reveal the overall\nresponses of brain areas to diverse stimuli with the resolution of individual neurons and their response\ndynamics. The distances between neurons on the encoding manifold reflect the similarity of their responses\nin time to the stimulus ensemble.\n\nWe applied the encoding manifold approach to the visual system of the mouse. The mouse became a\npopular species for vision research after one of our laboratories had characterized highly selective responses\nof neurons in its visual cortex. While mice had advantages, such as powerful molecular tools and genetics, concern\narose over the seemingly poor vision implied by traditional vision science measurements of visual cortical\nresponses. We showed that this concern was incorrect by using a more natural stimulus for mice--an\nabstract version of the 'optic flow' that the mouse would see as it ran through grass. Our recordings and\nanalysis showed that the mouse visual system could respond vigorously to these stimuli, and to quite fine detail. Supported by this grant and a\nsubsequent NIH grant,we expanded our experimental studies beyond the visual cortex to include the retina,\nwhere vision starts.\r\n\n\nWe then had to confront data analysis. How should these different parts of visual cortex be compared with retina? How should a neural circuit be compared\nwith an artificial neural network? Individual neural responses must be considered, of course, but neurons act together to\nproduce the sensation of vision: how is their collective action to be understood? . Since thousands of neurons are involved, global population relationships (beyond pairwise) must be considered. Our encoding\nmanifold achieved this.\n\nWe used encoding manifolds to compare the population-level encoding of primary visual cortex of\nmouse with that of the retina, those of five higher visual areas (VISam, VISal, VISpm, VISlm, and VISrl), and\nthose of convolutional neural networks (CNNs) used for object recognition in Artificial Intelligence. For the higher\ncortical visual areas we used data from the Allen Institute Visual Coding--Neuropixels dataset. Using both the Stryker laboratory's visual cortex recordings and the Allen Institute data, we showed that the encoding manifold topology is topologically continuous for V1, and it is\nsimilarly continuous for all the higher visual areas, with smooth coordinates spanning it that include\norientation selectivity and firing-rate magnitude. This confirmed our methodology, because the Allen Institute data comprised gratings only. Surprisingly, while responses to natural scenes and\ndistinctions among cortical layers and cell types were not used for its computation, all of these features were\norganized on the encoding manifold. The encoding manifold also revealed a novel relationship between the encoding of\nnatural scenes relative to static gratings.\r\n\n\nEncoding manifolds from the retina were qualitatively different.\nInstead of continuity as for cortex, the retinal manifold consisted of clusters of neurons corresponding to\nknown retinal ganglion cell types. Again, inference of known physiological properties confirmed our algorithmic approach.\r\n\n\nFinally, encoding manifolds computed from the most popular convolutional neural\nnetworks (AlexNet(R1), VGG16, ResNet50, and MouseNet) all resembled that of mouse retina, suggesting\nthat CNNs are more like a big retinas than small mouse brains. These results reveal how a machine\nlearning approach can organize and make visible the structure of sensory coding, thereby revealing novel\nrelationships within and across brain areas and sensory stimuli. We believe this final result will influence the interpretation of AI systems.\n\nAll of these results appeared in our four primary publications, below. In addition, we gave more than 40\npresentations on these results at meetings such as CoSyne, the Society for Neuroscience and the\nInternational Conference on Learning Representations, and workshops and educational outreach, such as\nthe NSF sponsored Telluride Neuromorphic Cognition Engineering Workshop.\r\n\n\nIn closing, we expect that our novel mathematical approach of computing encoding manifolds will be useful for other areas of data analysis, such as\nunderstanding sensorimotor behavior and genomic networks and development. We have begun preliminary\nstudies on these additional problems.\n\nDyballa, L., Hoseini, M., Dadarlat, M., Zucker, S.W., Stryker, M. (2018), ``Flow stimuli reveal ecologically\nappropriate responses in mouse visual cortex'', Proceedings of the National Academy of Sciences\n(USA), 115(44), 11304--11309.\nDyballa, L., Zucker, S. (2023), ``IAN kernel: Iterated Adaptive Neighborhoods for manifold learning and\ndimensionality estimation'', Neural Computation, 35 (3): 453-524.\nDyballa, L., Rudzite, M., Hoseini, M., Thapa, M., Stryker, M., Field, G., Zucker, S. (2024), ``Population\ncoding of stimulus features along the visual hierarchy'', Proceedings of the National Academy of\nSciences (USA), 121(4): e2317773121.\nDyballa, L., Field, G., Stryker, M., Zucker, S., (2024) Encoding manifolds constructed from grating\nresponses organize responses to natural scenes across mouse cortical visual areas, bioRxiv\n2024.10.24.620089.\r\n\n\n\t\t\t\t\tLast Modified: 12/10/2024\n\n\t\t\t\t\tSubmitted by: StevenZucker\n"
 }
}