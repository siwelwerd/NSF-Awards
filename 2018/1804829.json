{
 "awd_id": "1804829",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SaTC: CORE: Frontier: Collaborative: End-to-End Trustworthiness of Machine-Learning Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 700095.0,
 "awd_amount": 700095.0,
 "awd_min_amd_letter_date": "2018-08-07",
 "awd_max_amd_letter_date": "2022-07-15",
 "awd_abstract_narration": "This frontier project establishes the Center for Trustworthy Machine Learning (CTML), a large-scale, multi-institution, multi-disciplinary effort whose goal is to develop scientific understanding of the risks inherent to machine learning, and to develop the tools, metrics, and methods to manage and mitigate them.  The center is led by a cross-disciplinary team developing unified theory, algorithms and empirical methods within complex and ever-evolving ML approaches, application domains, and environments. The science and arsenal of defensive techniques emerging within the center will provide the basis for building future systems in a more trustworthy and secure manner, as well as fostering a long term community of research within this essential domain of technology. The center has a number of outreach efforts, including a massive open online course (MOOC) on this topic, an annual conference, and broad-based educational initiatives. The investigators continue their ongoing efforts at broadening participation in computing via a joint summer school on trustworthy ML aimed at underrepresented groups, and by engaging in activities for high school students across the country via a sequence of webinars advertised through the She++ network and other organizations.\r\n\r\nThe center focuses on three interconnected and parallel investigative directions that represent the different classes of attacks attacking ML systems: inference attacks, training attacks, and abuses of ML.   The first direction explores inference time security, namely methods to defend a trained model from adversarial inputs. This effort emphasizes developing formally grounded measurements of robustness against adversarial examples (defenses), as well as understanding the limits and costs of attacks. The second research direction aims to develop rigorously grounded measures of robustness to attacks that corrupt the training data and new training methods that are robust to adversarial manipulation.   The final direction tackles the general security implications of sophisticated ML algorithms including the potential abuses of generative ML models, such as models that generate (fake) content, as well as data mechanisms to prevent the theft of a machine learning model by an adversary who interacts with the model.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kamalika",
   "pi_last_name": "Chaudhuri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kamalika Chaudhuri",
   "pi_email_addr": "kamalika@cs.ucsd.edu",
   "nsf_id": "000573596",
   "pi_start_date": "2018-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "UCSD",
  "perf_str_addr": "9500 Gilman Drive",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930404",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "8087",
   "pgm_ref_txt": "Frontiers in SaTC"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 130442.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 134976.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 139755.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 144800.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 150122.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"accomplishments bottomBorder\">\r\n<p>The goal of this frontier effort is to support the Center of Trustworthy Machine Learning (CTML), which brought together a broad team of investigators from computer science, statistics, machine learning and computational learning theory to develop a new cross-disciplinary science of machine learning in adversarial settings that will provide a basis for future trustworthy ML systems. The team&rsquo;s approach was to systematically explore and deeply understand the space of adversaries that attack ML systems, and design algorithms that provide provable robustness guarantees against classes of adversaries.</p>\r\n<p>The project led to a number of advances in robust ML. At UCSD, the project led to a body of work on the theoretical foundations of robustness. We characterized what robust classifiers should look like in the limit of infinite data, and the tradeoffs between robustness and generalization in deep learning. In collaboration with Stanford, we also looked at how to verify or audit properties of models that are themselves confidential. This is done by an application of zero-knowledge proofs (ZKP), and we showed that this enables verification of fairness properties and correctness of explanations.</p>\r\n<p>In terms of broader impacts, at UCSD, the project partially supported three students and one postdoctoral researcher. Two of the students are now graduated, with one in AI industry, and the other a postdoc. The postdoc who was partially supported is now an assistant professor at University of Michigan. The project also supported participation in the AI4All program in Berkeley.&nbsp;</p>\r\n<p>&nbsp;</p>\r\n</div>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/21/2025<br>\nModified by: Kamalika&nbsp;Chaudhuri</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\r\n\n\nThe goal of this frontier effort is to support the Center of Trustworthy Machine Learning (CTML), which brought together a broad team of investigators from computer science, statistics, machine learning and computational learning theory to develop a new cross-disciplinary science of machine learning in adversarial settings that will provide a basis for future trustworthy ML systems. The teams approach was to systematically explore and deeply understand the space of adversaries that attack ML systems, and design algorithms that provide provable robustness guarantees against classes of adversaries.\r\n\n\nThe project led to a number of advances in robust ML. At UCSD, the project led to a body of work on the theoretical foundations of robustness. We characterized what robust classifiers should look like in the limit of infinite data, and the tradeoffs between robustness and generalization in deep learning. In collaboration with Stanford, we also looked at how to verify or audit properties of models that are themselves confidential. This is done by an application of zero-knowledge proofs (ZKP), and we showed that this enables verification of fairness properties and correctness of explanations.\r\n\n\nIn terms of broader impacts, at UCSD, the project partially supported three students and one postdoctoral researcher. Two of the students are now graduated, with one in AI industry, and the other a postdoc. The postdoc who was partially supported is now an assistant professor at University of Michigan. The project also supported participation in the AI4All program in Berkeley.\r\n\n\n\r\n\r\n\n\n\t\t\t\t\tLast Modified: 02/21/2025\n\n\t\t\t\t\tSubmitted by: KamalikaChaudhuri\n"
 }
}