{
 "awd_id": "1816382",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NSF-BSF: RI: Small: Decentralized Active Goal Recognition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2018-07-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 469987.0,
 "awd_amount": 469987.0,
 "awd_min_amd_letter_date": "2018-06-25",
 "awd_max_amd_letter_date": "2023-03-21",
 "awd_abstract_narration": "Autonomous systems often need to coordinate with other sensors, robots, autonomous cars, and people.  This results in multi-agent systems, in which agents must be able to determine what others are currently doing and predict what they will be doing in the future. This task of plan and goal recognition, typically relies upon a passive observer that continually observes the multi-agent system. In many real-world systems, such as assistive robotics in the home, this is not practical. Real-world systems will require active goal recognition, where information has a cost, and other tasks are pursued and completed continuously during goal recognition. For example, consider a team of robots assisting a disabled or an elderly person. The robots must fetch items and clean areas, while also opening doors or otherwise escorting the person. The agents will have to balance completion of their own tasks with information gathering about the target person's behavior. Current goal recognition methods cannot solve this active goal recognition problem. Furthermore, in realistic multi-agent domains including agricultural applications, disaster assistance, or military settings, communication will be limited or noisy. This will require decentralized active goal recognition methods where agents make choices based on their own limited viewpoints. Developing such active goal recognition methods will be the focus of this research. \r\n\r\nMore specifically, the research will develop new methods for active goal recognition to allow teams of agents to coordinate with other systems. The project will  develop methods for: active goal recognition, combining the observer's planning problem with goal recognition to balance information gathering with task completion for a single agent (observer) and single target, decentralized active goal recognition, combining multi-agent planning for the observers with goal recognition to balance information gathering with task completion and coordination for multiple observer agents and a single target agent, and decentralized active goal recognition of multiple targets, combining multi-agent planning for the observers with goal recognition to balance information gathering with task completion and coordination for multiple observer agents and target agents. The research will develop a range of methods that are based on classical, information-theoretic and decision- theoretic planning that exploit the special structure in our problem. The work will be tested on a range of common benchmarks, against current methods and in multi-robot domains to ensure realistic experiments. This research will consider active goal recognition (combining an observer's planning problem with goal recognition of a target) in single-agent and decentralized multi-agent environments. The resulting work will greatly extend the usefulness of goal recognition, making it realistic to use in scenarios when information gathering has a cost and other tasks may need to be completed by the observer(s).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Amato",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher Amato",
   "pi_email_addr": "c.amato@northeastern.edu",
   "nsf_id": "000677651",
   "pi_start_date": "2018-06-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "014Z",
   "pgm_ref_txt": "NSF and US-Israel Binational Science Fou"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 469987.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We developed a new way of modeling the active goal recognition problems described above (for both the single and multi-agent case) as well as new methods for solving them. Many of these methods were general reinforcement learning methods that could also be applied to other problems.&nbsp;</p>\n<p><br />In the partially observable case, reinforcement learning is challenging due to the lack of observable state information. Thankfully, learning offline in a simulator with such state information is often possible. We developed a number of methods that are able to use this state information in a theoretically sound way while being scalable to large domains. In particular, we showed that several methods that use state information in partially observable settings do so incorrectly, resulting in poor solutions when the problem has significant partial observability. We developed a theoretically-grounded fix for this problem and incorporate it into common value (i.e., Q-learning) and actor-critic methods. We also developed methods that further improve performance by using auxiliary tasks based on state or belief prediction. Furthermore, for the case where we have access to an expert policy that depends on the state information, we developed a method that balances performing actions similar to the state expert and getting high returns under partial observability. Lastly, we developed methods that are able to improve learning by exploiting other domain structure in the form of mixed observability (i.e., some aspects are fully observable and others are partially observable) and symmetry (e.g., the solution would be the same if we rotated the robot).</p>\n<p><br />In the multi-agent reinforcement learning (MARL) case, we developed methods for the case when each agent is learning separately (i.e., decentralized training and execution) as well as when agents can coordinate first before executing in a decentralized fashion (i.e., centralized training for decentralized execution). The decentralized case is difficult due to the lack of shared information and other agents constantly changing their policies during learning but we developed an approach that is robust to these changes. In particular, we developed a distributional reinforcement learning approach that doesn't just learn a single value estimate, but a distribution of estimates to help distinguish between environment stochasticity and changes in other agent policies. The resulting approach significantly improves performance over previous methods. In the centralized training for decentralized execution case, it has become popular to develop actor-critic methods that train decentralized actors with a centralized critic where the centralized critic is allowed access global information of the entire system, including the true system state. Such centralized critics are possible given offline information and are not used for online execution. While these methods perform well in a number of domains and have become a de facto standard in MARL, we show theoretically and empirically that 1) centralized critics are not better than decentralized critics, having the same convergence properties (in expectation), 2) centralized critics are typically higher variance than decentralized critics, and 3) state-based critics are theoretically unsound in partially observable domains. The experiments show practical issues such as the difficulty of representation learning with partial observability, which highlights why the theoretical problems are often overlooked in the literature.&nbsp;</p>\n<p><br />Aspects of this research were integrated into courses at Northeastern and a diverse set of undergraduate and graduate students assisted on the work.</p><br>\n<p>\n Last Modified: 02/28/2024<br>\nModified by: Christopher&nbsp;Amato</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nWe developed a new way of modeling the active goal recognition problems described above (for both the single and multi-agent case) as well as new methods for solving them. Many of these methods were general reinforcement learning methods that could also be applied to other problems.\n\n\n\nIn the partially observable case, reinforcement learning is challenging due to the lack of observable state information. Thankfully, learning offline in a simulator with such state information is often possible. We developed a number of methods that are able to use this state information in a theoretically sound way while being scalable to large domains. In particular, we showed that several methods that use state information in partially observable settings do so incorrectly, resulting in poor solutions when the problem has significant partial observability. We developed a theoretically-grounded fix for this problem and incorporate it into common value (i.e., Q-learning) and actor-critic methods. We also developed methods that further improve performance by using auxiliary tasks based on state or belief prediction. Furthermore, for the case where we have access to an expert policy that depends on the state information, we developed a method that balances performing actions similar to the state expert and getting high returns under partial observability. Lastly, we developed methods that are able to improve learning by exploiting other domain structure in the form of mixed observability (i.e., some aspects are fully observable and others are partially observable) and symmetry (e.g., the solution would be the same if we rotated the robot).\n\n\n\nIn the multi-agent reinforcement learning (MARL) case, we developed methods for the case when each agent is learning separately (i.e., decentralized training and execution) as well as when agents can coordinate first before executing in a decentralized fashion (i.e., centralized training for decentralized execution). The decentralized case is difficult due to the lack of shared information and other agents constantly changing their policies during learning but we developed an approach that is robust to these changes. In particular, we developed a distributional reinforcement learning approach that doesn't just learn a single value estimate, but a distribution of estimates to help distinguish between environment stochasticity and changes in other agent policies. The resulting approach significantly improves performance over previous methods. In the centralized training for decentralized execution case, it has become popular to develop actor-critic methods that train decentralized actors with a centralized critic where the centralized critic is allowed access global information of the entire system, including the true system state. Such centralized critics are possible given offline information and are not used for online execution. While these methods perform well in a number of domains and have become a de facto standard in MARL, we show theoretically and empirically that 1) centralized critics are not better than decentralized critics, having the same convergence properties (in expectation), 2) centralized critics are typically higher variance than decentralized critics, and 3) state-based critics are theoretically unsound in partially observable domains. The experiments show practical issues such as the difficulty of representation learning with partial observability, which highlights why the theoretical problems are often overlooked in the literature.\n\n\n\nAspects of this research were integrated into courses at Northeastern and a diverse set of undergraduate and graduate students assisted on the work.\t\t\t\t\tLast Modified: 02/28/2024\n\n\t\t\t\t\tSubmitted by: ChristopherAmato\n"
 }
}