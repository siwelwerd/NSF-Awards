{
 "awd_id": "1761582",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Smooth National Measurement of Public Opinion Across Boundaries and Levels: A View From the Bayesian Spatial Approach",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032927269",
 "po_email": "ceavey@nsf.gov",
 "po_sign_block_name": "Cheryl Eavey",
 "awd_eff_date": "2017-07-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 122802.0,
 "awd_amount": 122802.0,
 "awd_min_amd_letter_date": "2018-03-28",
 "awd_max_amd_letter_date": "2018-03-28",
 "awd_abstract_narration": "This research project will measure public opinion in voting constituencies around the United States. The project will provide estimates of opinion in districts with little or no survey data, such as state legislative districts. The project's intellectual merit comes from establishing a new means for measuring public opinion that not only uses survey respondents' answers to polling questions but also incorporates important information about where respondents are located and what that implies about geographic patterns in public opinion. Coupled with population information from the U.S. Census, the project will produce stronger estimates of public sentiment when survey data are sparsely distributed.  The investigators will release free software that includes user-friendly functions allowing any citizen to determine public opinion in his or her own district or in districts that have not yet cast any votes, such as proposed congressional districts in a redistricting cycle.  The software will allow more sophisticated users to obtain measures for any variable (even if unrelated to public opinion) in relationship to geographic boundaries, which will have extensions to research in public health, epidemiology, economics, sociology, business, and law. The broader impact to society will be that the data and software from this project will provide more information for the news media, the public, and elected officials regarding the outlook of the nation by constituency and locale, thereby providing a better understanding of the American representation process. The project will recruit a diverse group of research assistants that will be trained in this kind of statistical analysis.\r\n\r\nStudies relating to public opinion often settle for less-than-ideal data. Frequently, researchers will measure public opinion in the 50 states or the 435 congressional districts by pooling together several surveys taken over time (losing a sense of change over time), using old measures of public opinion (which may not be consistent with current public views), or using presidential vote share to approximate public sentiment (which is prone to error because factors besides ideology affect vote choices). With smaller districts than these, such as state legislative districts, the problem is magnified considerably, because it is rare to have many survey respondents in such a small area. In this project, the investigators ask: How can survey responses and the geographic location of the respondents be used to reliably forecast constituency public opinion? To answer this question, the investigators will use the method of Bayesian universal kriging. This technique fits a training model over survey data to determine how demographic factors shape public opinion and how the portion of survey responses that cannot be explained by demographics can be explained by a geographically smoothed process. With a model like this, public opinion in constituencies can be predicted with known population demographics and the values of the geographically smoothed error process over that district.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeff",
   "pi_last_name": "Gill",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jeff Gill",
   "pi_email_addr": "jgill@american.edu",
   "nsf_id": "000441493",
   "pi_start_date": "2018-03-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "American University",
  "inst_street_address": "4400 MASSACHUSETTS AVE NW",
  "inst_street_address_2": "",
  "inst_city_name": "WASHINGTON",
  "inst_state_code": "DC",
  "inst_state_name": "District of Columbia",
  "inst_phone_num": "2028853440",
  "inst_zip_code": "200168003",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DC00",
  "org_lgl_bus_name": "AMERICAN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "H4VNDUN2VWU5"
 },
 "perf_inst": {
  "perf_inst_name": "American University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "DC",
  "perf_st_name": "District of Columbia",
  "perf_zip_code": "200168003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DC00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "133300",
   "pgm_ele_name": "Methodology, Measuremt & Stats"
  },
  {
   "pgm_ele_code": "137100",
   "pgm_ele_name": "Political Science"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 122802.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>So how would one estimate an unobserved outcome that is geographically<br />between two observed outcomes. Obviously averaging or interpolating<br />would provide a solution, but these simple approaches do not account<br />for variation in this measurement that are a function of other<br />explanatory variables. A goal of spatial statistics is to relate a set<br />of these explanatory variables to the outcome in a way that relates<br />case-specific information as well as geolocation. So for instance, a<br />particular fracking site is likely to be as productive as sites<br />nearby but particular geological characteristics of that location<br />may matter as much or more.<br /><br />In this grant work, we first extended the geospatial method of kriging<br />(spatial smoothing) and focus on how to estimate a Bayesian multilevel<br />kriging model. This allows us to treat the data in more flexible ways,<br />to include prior information before collecting the data, to produce a<br />smooth, continuous ``density blanket'' of the outcomes over a<br />geographical, reason, and how to deal with very large datasets. This<br />approach led to some serious computational challenges, which we<br />overcame by working on developing new mathematical and computational<br />algorithms to make analysis possible and in realistic times. These are<br />new treatments of large data matrices that contribute a related and<br />growing area in spatial ``big data.'' Geospatial statistical models<br />are clearly important in the big data world.&nbsp; There is little doubt<br />that marketeers, political campaigns, government agencies,<br />epidemiologists, and a wide variety of social science researchers<br />will continue to gather and use finely-grained geographical data. But<br />such work is uniquely challenging since they provide not only<br />conventional covariate analysis but also relational specifications<br />between cases, thus exploding the impact of data size. Here we<br />address a key tradeoff that is imposed by a combination of big data<br />and spatial model specification: the choice between realistic<br />computational time and the ability to use the full dataset.<br /><br />Our specific intellectual merit contributions are centered on<br />improvements in Bayesian spatial modeling, numerical handling of large<br />datasets, algorithmic developments implemented in publicly distributed<br />software, graphical summarization tools, and a new computational<br />estimation and prediction process: Bootstrap Random Spatial Sampling<br />(BRSS). Using the BRSS method, researchers will be able to examine<br />research questions with spatial correlation with less computation time<br />and power. Utilizing the BRSS method, researchers can take a random<br />sample of the data as well as a random sample of the cases to predict.<br />With each round of the bootstrap, we therefore create an unbiased<br />approximation of both parameter estimates from the model and forecasts<br />of the selected predictive observations.&nbsp; At the end, we pool the<br />sample values of parameter estimates in order to summarize the results.<br /><br />Measurable results from this work include two referee journal articles<br />at high impact outlets, two packages of freely distributed software,<br />and the training of five graduate students plus two undergraduates in<br />a technical area. Furthermore, the broader impact of these activities<br />means that the tools are disseminated to a wide audience of empirical<br />researchers, and these students are trained in advanced statistical<br />estimation and computational algorithmic advancing their academic and<br />professional careers.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/04/2020<br>\n\t\t\t\t\tModified by: Jeff&nbsp;Gill</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSo how would one estimate an unobserved outcome that is geographically\nbetween two observed outcomes. Obviously averaging or interpolating\nwould provide a solution, but these simple approaches do not account\nfor variation in this measurement that are a function of other\nexplanatory variables. A goal of spatial statistics is to relate a set\nof these explanatory variables to the outcome in a way that relates\ncase-specific information as well as geolocation. So for instance, a\nparticular fracking site is likely to be as productive as sites\nnearby but particular geological characteristics of that location\nmay matter as much or more.\n\nIn this grant work, we first extended the geospatial method of kriging\n(spatial smoothing) and focus on how to estimate a Bayesian multilevel\nkriging model. This allows us to treat the data in more flexible ways,\nto include prior information before collecting the data, to produce a\nsmooth, continuous ``density blanket'' of the outcomes over a\ngeographical, reason, and how to deal with very large datasets. This\napproach led to some serious computational challenges, which we\novercame by working on developing new mathematical and computational\nalgorithms to make analysis possible and in realistic times. These are\nnew treatments of large data matrices that contribute a related and\ngrowing area in spatial ``big data.'' Geospatial statistical models\nare clearly important in the big data world.  There is little doubt\nthat marketeers, political campaigns, government agencies,\nepidemiologists, and a wide variety of social science researchers\nwill continue to gather and use finely-grained geographical data. But\nsuch work is uniquely challenging since they provide not only\nconventional covariate analysis but also relational specifications\nbetween cases, thus exploding the impact of data size. Here we\naddress a key tradeoff that is imposed by a combination of big data\nand spatial model specification: the choice between realistic\ncomputational time and the ability to use the full dataset.\n\nOur specific intellectual merit contributions are centered on\nimprovements in Bayesian spatial modeling, numerical handling of large\ndatasets, algorithmic developments implemented in publicly distributed\nsoftware, graphical summarization tools, and a new computational\nestimation and prediction process: Bootstrap Random Spatial Sampling\n(BRSS). Using the BRSS method, researchers will be able to examine\nresearch questions with spatial correlation with less computation time\nand power. Utilizing the BRSS method, researchers can take a random\nsample of the data as well as a random sample of the cases to predict.\nWith each round of the bootstrap, we therefore create an unbiased\napproximation of both parameter estimates from the model and forecasts\nof the selected predictive observations.  At the end, we pool the\nsample values of parameter estimates in order to summarize the results.\n\nMeasurable results from this work include two referee journal articles\nat high impact outlets, two packages of freely distributed software,\nand the training of five graduate students plus two undergraduates in\na technical area. Furthermore, the broader impact of these activities\nmeans that the tools are disseminated to a wide audience of empirical\nresearchers, and these students are trained in advanced statistical\nestimation and computational algorithmic advancing their academic and\nprofessional careers.\n\n\t\t\t\t\tLast Modified: 11/04/2020\n\n\t\t\t\t\tSubmitted by: Jeff Gill"
 }
}