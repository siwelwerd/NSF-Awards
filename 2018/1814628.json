{
 "awd_id": "1814628",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: RUI: UbiqOmics: HCI for augmenting our world with pervasive personal and environmental omic data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 249781.0,
 "awd_amount": 257781.0,
 "awd_min_amd_letter_date": "2018-08-02",
 "awd_max_amd_letter_date": "2019-06-14",
 "awd_abstract_narration": "Recent years are seeing a sharp increase in the availability of personal and environmental 'omic' data about genomes or microbiomes to non-experts. Popular omic testing services produce data about people's personal genome, about their microbiome, and about plants and organisms in their living environment. As a result, people with no formal education in the life sciences can get access to their omic data by sending a self-collected sample to a direct-to-consumer testing provider, and results are delivered online. These people then need to interpret large amounts of complex data involving sensitive issues such as disease risk, carrier status, and potentially meaningful correlations with health and physical traits. While personal omics promise advances in health, for example through precision medicine, the slew of potential findings and the rapidly evolving interpretations these approaches produce are difficult to communicate to most non-experts. This project aims to empower people to explore, share, curate and better understand such data, which in turn can make substantial impact on their wellbeing. The project will identify user needs and develop novel human-computer interfaces to help people make sense of personal, social, and environmental omic data. These tools will be evaluated in a longitudinal study in real households, both validating the work and providing direct impact on participants' understanding of the data and their wellbeing. The tools will also be available through Open Humans, an open platform that brings together researchers, citizen scientists, and members of the public who share their personal omic data. In addition, the project will increase omic literacy among non-experts and contribute to increasing the participation of women and other underrepresented minorities in STEM research.\r\n\r\nThe project will conduct research on human-computer interaction for UbiqOmic environments: living spaces and social interactions where omic data is available about people, plants, animals, and surfaces. In particular, the team will identify user needs and develop web-based visual tools that integrate omic data sets from heterogeneous resources and multiple samples. These tools will allow users to aggregate, explore, relate, and connect pervasive omic information, and facilitate collaborative sense making of omic information within various social contexts including families and cohabiting communities. In addition, the project will harness the power of augmented reality (AR) to visualize the invisible, designing, developing, and evaluating an AR interface which overlays timely and actionable omic data in the environment and on the user's own body (oral, gut, skin). The team will evaluate these tools with both general audiences and early adopters through a series of usability and longitudinal studies. The project will advance the fields of human-computer interaction, computer-supported cooperative work, and personal informatics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Orit",
   "pi_last_name": "Shaer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Orit Shaer",
   "pi_email_addr": "oshaer@wellesley.edu",
   "nsf_id": "000547518",
   "pi_start_date": "2018-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Wellesley College",
  "inst_street_address": "106 CENTRAL ST",
  "inst_street_address_2": "",
  "inst_city_name": "WELLESLEY HILLS",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "7812832079",
  "inst_zip_code": "024818203",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "WELLESLEY COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "Z17DSLNJ1DX1"
 },
 "perf_inst": {
  "perf_inst_name": "Wellesley College",
  "perf_str_addr": "106 Central St",
  "perf_city_name": "Wellesley",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "024818203",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 249781.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-eba112f9-7fff-ec08-bb2b-b2480eabe561\"> </span></p>\n<p dir=\"ltr\"><span>The project's overall goal was to identify user needs and develop novel human computer interfaces to help people make sense of personal, social, and environmental omic data. Using multiple research methods we investigated whether augmented reality (AR) that directs one's attention to the self or external environment impacts vaccine intentions by changing one's belief of who or what is the cause of their risk of infection; we developed a novel mobile AR application which provides information about indoor air pollution; we designed an online study which investigates the impact of self-focus attention and vicarious reinforcement design strategies in online health communication environments on behavioral intentions, self-efficacy, threat severity, outcome expectancy, and other perceptual factors; we identified users information needs and collaborative sensemaking of microbiome data by analyzing users' discussions on Reddit's r/HumanMicrobiome; finally, we designed a novel AR application, called AR-OmiX, which&nbsp; overlays nutritional and environmental omic-data in a shared kitchen environment. We designed the AR app to help non-expert users make everyday decisions that impact their wellbeing as well&nbsp;</span>as to foster curiosity and understanding of omic data so that non-experts could contribute to shaping future products and policies related to such sensitive data. More than 2000 people participated in user studies and learned about personal omic data and risk mitigating behaviors within the context of pathogen transmission, indoor air pollution, and food safety.</p>\n<p dir=\"ltr\"><span>Several graduate and undergraduate students worked on this project in an interdisciplinary environment. They received training in software and hardware design and development, experimental and user study research methods, statistical analysis, and paper writing. All findings were disseminated through publications in journals and presentations at technical conferences. The research was further promoted to diverse audiences through formal and informal activities, including talks and presentations.&nbsp;</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/21/2023<br>\n\t\t\t\t\tModified by: Orit&nbsp;Shaer</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1814628/1814628_10565116_1681777026980_ScreenShot2023-03-30at4.42.28PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1814628/1814628_10565116_1681777026980_ScreenShot2023-03-30at4.42.28PM--rgov-800width.jpg\" title=\"Self Focus AR study 1\"><img src=\"/por/images/Reports/POR/2023/1814628/1814628_10565116_1681777026980_ScreenShot2023-03-30at4.42.28PM--rgov-66x44.jpg\" alt=\"Self Focus AR study 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Experimental conditions for studying the impact of self-focused augmented reality on risk perceptions & handhygiene intentions</div>\n<div class=\"imageCredit\">Ayanna Seals</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Orit&nbsp;Shaer</div>\n<div class=\"imageTitle\">Self Focus AR study 1</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1814628/1814628_10565116_1681777422422_ScreenShot2023-03-30at4.42.38PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1814628/1814628_10565116_1681777422422_ScreenShot2023-03-30at4.42.38PM--rgov-800width.jpg\" title=\"Self Focus AR study 2\"><img src=\"/por/images/Reports/POR/2023/1814628/1814628_10565116_1681777422422_ScreenShot2023-03-30at4.42.38PM--rgov-66x44.jpg\" alt=\"Self Focus AR study 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Experimental conditions for studying the impact of self-focused augmented reality on causal attributions of risk</div>\n<div class=\"imageCredit\">Ayanna Seals</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Orit&nbsp;Shaer</div>\n<div class=\"imageTitle\">Self Focus AR study 2</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1814628/1814628_10565116_1681778016358_ScreenShot2023-03-30at4.46.08PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1814628/1814628_10565116_1681778016358_ScreenShot2023-03-30at4.46.08PM--rgov-800width.jpg\" title=\"The DiCRAs Framework\"><img src=\"/por/images/Reports/POR/2023/1814628/1814628_10565116_1681778016358_ScreenShot2023-03-30at4.46.08PM--rgov-66x44.jpg\" alt=\"The DiCRAs Framework\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of a framework design space configurations for remote augmented reality interventions. The framework allows researchers to consider what strategies might be best for their specific intervention.</div>\n<div class=\"imageCredit\">Orit Shaer</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Orit&nbsp;Shaer</div>\n<div class=\"imageTitle\">The DiCRAs Framework</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nThe project's overall goal was to identify user needs and develop novel human computer interfaces to help people make sense of personal, social, and environmental omic data. Using multiple research methods we investigated whether augmented reality (AR) that directs one's attention to the self or external environment impacts vaccine intentions by changing one's belief of who or what is the cause of their risk of infection; we developed a novel mobile AR application which provides information about indoor air pollution; we designed an online study which investigates the impact of self-focus attention and vicarious reinforcement design strategies in online health communication environments on behavioral intentions, self-efficacy, threat severity, outcome expectancy, and other perceptual factors; we identified users information needs and collaborative sensemaking of microbiome data by analyzing users' discussions on Reddit's r/HumanMicrobiome; finally, we designed a novel AR application, called AR-OmiX, which  overlays nutritional and environmental omic-data in a shared kitchen environment. We designed the AR app to help non-expert users make everyday decisions that impact their wellbeing as well as to foster curiosity and understanding of omic data so that non-experts could contribute to shaping future products and policies related to such sensitive data. More than 2000 people participated in user studies and learned about personal omic data and risk mitigating behaviors within the context of pathogen transmission, indoor air pollution, and food safety.\nSeveral graduate and undergraduate students worked on this project in an interdisciplinary environment. They received training in software and hardware design and development, experimental and user study research methods, statistical analysis, and paper writing. All findings were disseminated through publications in journals and presentations at technical conferences. The research was further promoted to diverse audiences through formal and informal activities, including talks and presentations. \n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 04/21/2023\n\n\t\t\t\t\tSubmitted by: Orit Shaer"
 }
}