{
 "awd_id": "1810888",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Consistent Risk Estimation under High-Dimensional Asymptotics",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 119880.0,
 "awd_amount": 119880.0,
 "awd_min_amd_letter_date": "2018-08-01",
 "awd_max_amd_letter_date": "2018-08-01",
 "awd_abstract_narration": "Learning from large datasets has been the cornerstone of modern innovations and discoveries in science, medicine, and technology. Fast prediction of unseen events is a canonical goal in statistical learning. A classic approach to this end is leave-one-out cross-validation, a time-consuming routine of leaving a datum out,  fitting the model on the rest, and testing it on the left out datum, repeatedly. The recent emergence of massive data has exacerbated the computational infeasibility of such approaches. Moreover, in many recent instances, the number of features per observation can be extremely large, adding another challenging facet to the fast estimation of prediction error. To overcome these problems a new set of scalable and consistent risk estimators will be developed in this project. \r\n\r\nThe importance of risk estimation has motivated this project of different schemes, such as cross-validation, Stein's unbiased risk estimation (SURE), Generalized cross-validation, Akaike Information Criterion (AIC), and Bootstrap. The emergence of high-dimensional datasets has challenged most classical approaches to risk estimation. For instance, the large discrepancy between in-sample and out-of-sample prediction error, in applications involving predictions based on previously unseen features, makes it hard to rely on popular estimators, such as SURE or AIC, in high-dimensional regimes where the number of predictors is smaller than or at the same order as the number of observations. On the other hand, the information value of a datum in these regimes (as opposed to the information value of a datum in low-dimensional settings) casts doubt on the reliability of other techniques, such as 5-fold cross-validation. The project offers a novel theoretical framework to find the middle ground between scalability and reliability, and specifically, to obtain theoretically consistent and computationally efficient risk-estimation schemes under high-dimensional settings. Since risk estimation is at the core of areas including but not limited to machine learning, signal processing, medical imaging, neuroscience, and social and environmental sciences, any success in this project will lead to reliable and immediate scientific discoveries and better learning systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mohammad Ali",
   "pi_last_name": "Maleki",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mohammad Ali Maleki",
   "pi_email_addr": "mm4338@columbia.edu",
   "nsf_id": "000649654",
   "pi_start_date": "2018-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277922",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 119880.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Since its development, statistical analysis has been a cornerstone of many scientific, medical, social, environmental, and technological discoveries. In the past fifty years, the emergence of social-media and smart-phones, major technological advances in data acquisition techniques, and the awareness of private and public sectors of the importance of data has led to many massive datasets, in which the number of features per observation is relatively large. For instance, in medical studies, it is costumery to detect the cause of a disease based on measuring many observable traits (each trait being a feature) of a handful of individuals. These high-dimensional datasets violate the assumptions of the classical statistics, make the tools developed by those assumptions useless, and hence call for new tools and analyses. The main objective of this project was to study one of the basic tools that arise in the data-analysis, i.e. the problem of risk estimation or the closely related problem of parameter tuning, under the high-dimensional settings.&nbsp;</p>\n<p>&nbsp;</p>\n<p>More specifically, our goal was to study one of the most basic properties of the risk estimators, known as consistency. Consistent estimators are expected to provide accurate results when the number of features and observations are large in a dataset. During the course of this project, we showed that many well-known risk estimators, such as five-fold cross validation, are not consistent in high-dimensional settings. We then proved that another well-known risk estimator, leave-one-out cross validation (LO) offers a consistent risk estimate. Despite this appealing mathematical property, due to its high computation complexity, LO is not useful for real-world applications. In order to address this challenge, we obtained a computationally-efficient and mathematically-accurate approximation of LO for a wide range of estimation problems. We called the new estimate approximate leave-one-out (ALO). We studied the consistency of ALO and evaluated its performance on several real-world and simulated datasets. In addition to ALO, during the course of the project, we also obtained another consistent risk estimate based on a different framework, known as approximate message passing (AMP). We also provided a detailed comparison between the estimates of ALO and AMP.&nbsp;</p>\n<p>&nbsp;</p>\n<p>The final contribution of this project was to provide a better understanding of parameter tuning for the variable selection (VS) problem. VS arises when for instance one uses the results of a medical study to detect the cause of a disease. In this project, we showed that two-stage variable selection techniques outperform the single-stage ones for linear models. Furthermore, we proved that the tuning of the first stage can be done using consistent risk estimation techniques such as ALO or AMP.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/01/2022<br>\n\t\t\t\t\tModified by: Mohammad Ali&nbsp;Maleki</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSince its development, statistical analysis has been a cornerstone of many scientific, medical, social, environmental, and technological discoveries. In the past fifty years, the emergence of social-media and smart-phones, major technological advances in data acquisition techniques, and the awareness of private and public sectors of the importance of data has led to many massive datasets, in which the number of features per observation is relatively large. For instance, in medical studies, it is costumery to detect the cause of a disease based on measuring many observable traits (each trait being a feature) of a handful of individuals. These high-dimensional datasets violate the assumptions of the classical statistics, make the tools developed by those assumptions useless, and hence call for new tools and analyses. The main objective of this project was to study one of the basic tools that arise in the data-analysis, i.e. the problem of risk estimation or the closely related problem of parameter tuning, under the high-dimensional settings. \n\n \n\nMore specifically, our goal was to study one of the most basic properties of the risk estimators, known as consistency. Consistent estimators are expected to provide accurate results when the number of features and observations are large in a dataset. During the course of this project, we showed that many well-known risk estimators, such as five-fold cross validation, are not consistent in high-dimensional settings. We then proved that another well-known risk estimator, leave-one-out cross validation (LO) offers a consistent risk estimate. Despite this appealing mathematical property, due to its high computation complexity, LO is not useful for real-world applications. In order to address this challenge, we obtained a computationally-efficient and mathematically-accurate approximation of LO for a wide range of estimation problems. We called the new estimate approximate leave-one-out (ALO). We studied the consistency of ALO and evaluated its performance on several real-world and simulated datasets. In addition to ALO, during the course of the project, we also obtained another consistent risk estimate based on a different framework, known as approximate message passing (AMP). We also provided a detailed comparison between the estimates of ALO and AMP. \n\n \n\nThe final contribution of this project was to provide a better understanding of parameter tuning for the variable selection (VS) problem. VS arises when for instance one uses the results of a medical study to detect the cause of a disease. In this project, we showed that two-stage variable selection techniques outperform the single-stage ones for linear models. Furthermore, we proved that the tuning of the first stage can be done using consistent risk estimation techniques such as ALO or AMP. \n\n \n\n\t\t\t\t\tLast Modified: 12/01/2022\n\n\t\t\t\t\tSubmitted by: Mohammad Ali Maleki"
 }
}