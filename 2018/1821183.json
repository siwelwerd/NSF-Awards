{
 "awd_id": "1821183",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CDS&E: Collaborative Research: Scalable Nonparametric Learning for Massive Data with Statistical Guarantees",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Christopher Stark",
 "awd_eff_date": "2018-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 190000.0,
 "awd_amount": 190000.0,
 "awd_min_amd_letter_date": "2018-08-15",
 "awd_max_amd_letter_date": "2020-08-04",
 "awd_abstract_narration": "We now live in the era of data deluge. The sheer volume of the data to be processed, together with the growing complexity of statistical models and the increasingly distributed nature of the data sources, creates new challenges to modern statistics theory. Standard machine learning methods are no longer able to accommodate the computational requirements. They need to be re-designed or adapted, which calls for a new generation of design and theory of scalable learning algorithms for massive data. This project aims to provide a collection of state-of-the-art nonparametric learning tools for big data analysis, which can be directly used by scientists and practitioners and have beneficial impacts on various fields such as biomedicine, health-care, defense and security, and information technology. The deliverables of this project include easy-to-use software packages that will be thoroughly evaluated using a range of application examples. They will directly help scientists to explore and analyze complex data sets.\r\n \r\nDue to storage and computational bottlenecks, traditional statistical inferential procedures originally designed for a single machine are no longer applicable to modern large datasets. This project aims to design new scalable learning algorithms of wide-ranging nonparametric models for data that are distributed across a large number of multi-core computational nodes, or in a fashion of random sketching if only a single machine is available. The computational limits of these new algorithms will be examined from a statistical perspective. For example, in the divide-and-conquer setup, the number of deployed machines can be viewed as a simple proxy for computing cost. The project aims to establish a sharp upper bound for this number: when the number is below this bound, statistical optimality (in terms of nonparametric estimation or testing) is achievable; otherwise, statistical optimality becomes impossible. Related questions will also be addressed in the randomized sketching method in terms of the minimal number of random projections.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Guang",
   "pi_last_name": "Cheng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Guang Cheng",
   "pi_email_addr": "guangcheng@ucla.edu",
   "nsf_id": "000512109",
   "pi_start_date": "2018-08-15",
   "pi_end_date": "2020-08-04"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Qifan",
   "pi_last_name": "Song",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qifan Song",
   "pi_email_addr": "qfsong@purdue.edu",
   "nsf_id": "000710181",
   "pi_start_date": "2020-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "250 N. University Street",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072066",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  },
  {
   "pgm_ele_code": "808400",
   "pgm_ele_name": "CDS&E"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 190000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>In the era of big data, the growing complexity of the data models and the increasingly distributed nature of the data sources, creates new challenges to the modern statistical theory. This project developed a new generation of theoretical principles needed to scale classic&nbsp;</span><em>nonparametric inference procedures and learning algorithms</em><span>&nbsp;to massive data settings. Computationally efficient algorithms/methodologies are designed to address real-world massive data applications such as those in neuroscience.</span></p>\n<p><span>This project proposed novel algorithms for scenarios in which data are either distributed across a large number of multi-core computational nodes, or in a fashion of random sketching if only a single machine is available. The computational limit of the proposed algorithm is examined from a statistical perspective, which justifies the usefulness of the algorithms. To accommodate heterogeneity, robustness and online features of big data which are commonly encountered in the modern era, the project further developed inference procedures,  such as nonparametric (adaptive) testing and quantile regression process.&nbsp;</span></p>\n<p><span><br />Theoretical insights gained from analyzing the proposed algorithmic framework are beneficial for a wide range of real-world problems arising from large scale brain imaging data and high frequency stock pricing data.The research results (including theoretical insights, scalable algorithms and their implementations) was distributed through archival publications, course offerings, undergraduate research opportunities, tutorials and conference presentations. . On the educational level, the funded work was incorporated into training of STEM undergraduate and graduate students.&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/29/2021<br>\n\t\t\t\t\tModified by: Qifan&nbsp;Song</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn the era of big data, the growing complexity of the data models and the increasingly distributed nature of the data sources, creates new challenges to the modern statistical theory. This project developed a new generation of theoretical principles needed to scale classic nonparametric inference procedures and learning algorithms to massive data settings. Computationally efficient algorithms/methodologies are designed to address real-world massive data applications such as those in neuroscience.\n\nThis project proposed novel algorithms for scenarios in which data are either distributed across a large number of multi-core computational nodes, or in a fashion of random sketching if only a single machine is available. The computational limit of the proposed algorithm is examined from a statistical perspective, which justifies the usefulness of the algorithms. To accommodate heterogeneity, robustness and online features of big data which are commonly encountered in the modern era, the project further developed inference procedures,  such as nonparametric (adaptive) testing and quantile regression process. \n\n\nTheoretical insights gained from analyzing the proposed algorithmic framework are beneficial for a wide range of real-world problems arising from large scale brain imaging data and high frequency stock pricing data.The research results (including theoretical insights, scalable algorithms and their implementations) was distributed through archival publications, course offerings, undergraduate research opportunities, tutorials and conference presentations. . On the educational level, the funded work was incorporated into training of STEM undergraduate and graduate students. \n\n \n\n\t\t\t\t\tLast Modified: 11/29/2021\n\n\t\t\t\t\tSubmitted by: Qifan Song"
 }
}