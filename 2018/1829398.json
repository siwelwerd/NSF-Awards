{
 "awd_id": "1829398",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Functional Organization of Navigational Coding in the Human Brain",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032924502",
 "po_email": "dkravitz@nsf.gov",
 "po_sign_block_name": "Dwight Kravitz",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 669803.0,
 "awd_amount": 669803.0,
 "awd_min_amd_letter_date": "2018-08-21",
 "awd_max_amd_letter_date": "2023-07-26",
 "awd_abstract_narration": "With funding from NSF, the researchers will study how the human brain represents dimensions that are needed for finding our way around in the world. The interdisciplinary approach combines cognitive and visual neuroscience methods, aimed towards understanding how the human brain processes spatial navigation information. The studies incorporate behavioral, cognitive, and neuroimaging techniques to examine how the human brain codes for distance, heading direction, speed, and time, which may contribute to higher-level navigation mechanisms, such as planning a route to a known destination or finding one's way home. The results have the potential to impact other fields, including robotics and spatial sciences.  In robotics, autonomous systems have difficulty determining whether they have successfully returned back to their origin after an outbound journey, which robotics researchers call the loop closure problem.  In contrast, humans and animals can readily solve this problem.  Understanding how visual information is used to localize and orient will provide knowledge that could potentially facilitate innovation in mobile robots and self-driving cars or training for more efficient navigation in humans.  Greater knowledge of the basic properties of navigation in humans could also lead to improved electronic navigation systems, emergency response training, and more effective transportation signage.\r\n\r\nThe scientific goals harness the strengths of cognitive neuroscience, visual neuroscience, and spatial sciences to examine navigation in humans.  While much is known about the navigation system in rodents, the rat and primate have fundamentally different visual systems. Contributions from the visual system provide critical information necessary for self-motion guided navigation, and the theoretical basis for this proposal stems from computational models that posit that perceptual information, including optic flow, speed, and direction signals, are necessary for successful navigation. The researchers propose a framework in which spatial representations transform from a retinotopic to a spatiotopic organization. This framework posits testable hypotheses about the nature of self-motion guided navigational representations in the brain. A series of experiments will examine how the human brain codes lower-level representations, such as distance, heading direction, speed, and time, which may serve as basis functions for generating higher-level level navigational representations. The studies will examine how these selective properties are spatially organized in the brain, as well as the higher-level computations that bring this information together to compute path integration. To do so, the proposed studies employ innovative functional MRI paradigms adapted from visual neuroscience, including population receptive-field mapping, phase-encoded analyses, and model-based time-series analyses. The proposed work is critical for extending computational models of navigation to the systems level in humans.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chantal",
   "pi_last_name": "Stern",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Chantal E Stern",
   "pi_email_addr": "chantal@bu.edu",
   "nsf_id": "000714708",
   "pi_start_date": "2018-08-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sam",
   "pi_last_name": "Ling",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sam Ling",
   "pi_email_addr": "samling@bu.edu",
   "nsf_id": "000666336",
   "pi_start_date": "2018-08-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Elizabeth",
   "pi_last_name": "Chrastil",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Elizabeth Chrastil",
   "pi_email_addr": "chrastil@uci.edu",
   "nsf_id": "000727197",
   "pi_start_date": "2018-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "881 Commonwealth Avenue",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151300",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "135200",
   "pgm_ele_name": "Geography and Spatial Sciences"
  },
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1352",
   "pgm_ref_txt": "GEOGRAPHY AND SPATIAL SCIENCES"
  },
  {
   "pgm_ref_code": "1699",
   "pgm_ref_txt": "COGNEURO"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 669803.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Imagine that you and your friends are out hiking in the woods or exploring a new mountain bike trail.&nbsp; Why is it that some people are excellent at finding their way in a new environment while others get lost easily? Successful navigation relies on the ability to compute one's spatial location within an environment. One of the cortical computations likely carried out to support this ability is the estimation of one's perceived self-motion, a process considered paramount to successful navigation. Numerous selective properties of self-motion, including distance, location, degrees of arc, and time, likely work in concert to support path integration, the constant updating of the navigator's representation of position and orientation based on movement. Computations of self-motion likely rely heavily on input from the visual system, particularly in humans. The goal of the research supported by this grant was to understand the mechanisms by which visual information supports navigation during self-motion in landmark-free environments. We set out to study this in humans, using a combination of behavioral and non-invasive neuroimaging methods.&nbsp; Current models of spatial navigation in novel environments center on the medial temporal lobe (MTL), particularly the functional contributions of the entorhinal cortex and hippocampus. However, contributions from the visual system provide critical information about self-motion, among other properties important for navigation. Neurophysiological research in rodents has left open questions about the neural coding in visual cortical regions providing input to the entorhinal cortex and hippocampus, supporting visual navigation. In the studies supported by this NSF award, we examined the integration between visual areas and navigational areas in the human brain, suggesting that these areas work together as a contiguous unified system. Our starting hypothesis was that visual input propagates self-motion information, which in turn supports higher-level navigational capabilities, including egocentric and allocentric frames of reference. Our framework, which assumes that regions throughout this pathway share a set of canonical computations, introduced several testable hypotheses about the nature of navigational representations in the brain.&nbsp; By using functional MRI and behavioral studies in humans, we sought to examine the formation of the basis functions that act in concert to support position coding, how these selective properties are spatially organized in the brain, and finally the higher-level computations that bring this information together to compute path integration. One of our key results is that we found evidence for a travel direction signal in humans that is independent of head direction. A second key result is the retrosplenial cortex and parahippocampampal cortex are critical regions demonstrating a sensitivity to change in egocentric distance and bearing. This second result is consistent with what has been found in animal studies, and extends this knowledge to the human brain. &nbsp;The neuroimaging studies further suggested that the magnitude of these egocentric signals was correlated with a behavioral measure of navigation that requires transformation between reference frames. &nbsp;Overall, the studies supported by this award provide us with a detailed systems level understanding of how the human brain codes selective properties that support self-motion perception and navigation.&nbsp; The broader impact of this work extends beyond increasing our understanding of the human brain, in that understanding human navigation, and the variability in human spatial navigation abilities, impacts how we design systems for assisting people with real-world navigational challenges, be it through signs, maps, navigational apps, or AI self-driving systems.&nbsp; This award has also provided training and professional development opportunities for undergraduate and graduate students at the University of California, Irvine and at Boston University.&nbsp;</span></p><br>\n<p>\n Last Modified: 05/02/2024<br>\nModified by: Chantal&nbsp;E&nbsp;Stern</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nImagine that you and your friends are out hiking in the woods or exploring a new mountain bike trail. Why is it that some people are excellent at finding their way in a new environment while others get lost easily? Successful navigation relies on the ability to compute one's spatial location within an environment. One of the cortical computations likely carried out to support this ability is the estimation of one's perceived self-motion, a process considered paramount to successful navigation. Numerous selective properties of self-motion, including distance, location, degrees of arc, and time, likely work in concert to support path integration, the constant updating of the navigator's representation of position and orientation based on movement. Computations of self-motion likely rely heavily on input from the visual system, particularly in humans. The goal of the research supported by this grant was to understand the mechanisms by which visual information supports navigation during self-motion in landmark-free environments. We set out to study this in humans, using a combination of behavioral and non-invasive neuroimaging methods. Current models of spatial navigation in novel environments center on the medial temporal lobe (MTL), particularly the functional contributions of the entorhinal cortex and hippocampus. However, contributions from the visual system provide critical information about self-motion, among other properties important for navigation. Neurophysiological research in rodents has left open questions about the neural coding in visual cortical regions providing input to the entorhinal cortex and hippocampus, supporting visual navigation. In the studies supported by this NSF award, we examined the integration between visual areas and navigational areas in the human brain, suggesting that these areas work together as a contiguous unified system. Our starting hypothesis was that visual input propagates self-motion information, which in turn supports higher-level navigational capabilities, including egocentric and allocentric frames of reference. Our framework, which assumes that regions throughout this pathway share a set of canonical computations, introduced several testable hypotheses about the nature of navigational representations in the brain. By using functional MRI and behavioral studies in humans, we sought to examine the formation of the basis functions that act in concert to support position coding, how these selective properties are spatially organized in the brain, and finally the higher-level computations that bring this information together to compute path integration. One of our key results is that we found evidence for a travel direction signal in humans that is independent of head direction. A second key result is the retrosplenial cortex and parahippocampampal cortex are critical regions demonstrating a sensitivity to change in egocentric distance and bearing. This second result is consistent with what has been found in animal studies, and extends this knowledge to the human brain. The neuroimaging studies further suggested that the magnitude of these egocentric signals was correlated with a behavioral measure of navigation that requires transformation between reference frames. Overall, the studies supported by this award provide us with a detailed systems level understanding of how the human brain codes selective properties that support self-motion perception and navigation. The broader impact of this work extends beyond increasing our understanding of the human brain, in that understanding human navigation, and the variability in human spatial navigation abilities, impacts how we design systems for assisting people with real-world navigational challenges, be it through signs, maps, navigational apps, or AI self-driving systems. This award has also provided training and professional development opportunities for undergraduate and graduate students at the University of California, Irvine and at Boston University.\t\t\t\t\tLast Modified: 05/02/2024\n\n\t\t\t\t\tSubmitted by: ChantalEStern\n"
 }
}