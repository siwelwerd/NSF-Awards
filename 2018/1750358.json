{
 "awd_id": "1750358",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Generative Models for Targeted Domain Interpretability with Applications to Healthcare",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2018-02-15",
 "awd_exp_date": "2025-01-31",
 "tot_intn_awd_amt": 548034.0,
 "awd_amount": 548034.0,
 "awd_min_amd_letter_date": "2018-02-12",
 "awd_max_amd_letter_date": "2024-01-02",
 "awd_abstract_narration": "The imminent deployment of AI and machine learning in poorly-characterized settings such as autonomous driving, personalized news feeds, and treatment recommendation systems has created an urgent need for machine learning systems that explain their decisions. Interpretability helps human experts ascertain whether machine learning systems, trained on technical objective functions, have sensible outputs despite unmodeled unknowns. For example, a clinical decision support system will never know all of a patient's history, nor may it know which of many side effects a specific patient is willing to tolerate. An important challenge, then, is how to design machine learning systems that both predict well and provide explanation.  Within this broad challenge, this work develops techniques for domain-targeted interpretability, finding summaries of high-dimensional data that are relevant for making decisions.  The proposed work focuses on healthcare applications, where interpretable models are essential to safety.  However, the project aims to produce foundational learning algorithms applicable to a range of scientific and social domains.  The developed methods will be tested on real problems in personalizing treatment recommendations and prognoses for sepsis, depression, and autism spectrum disorder. Thus, the successful completion of the work will impact both interpretable machine learning and clinical science.  All software developed in the course of the project will be freely shared.  The educational component of the proposed work will educate early elementary students about the impact of statistics in medicine and educate policy-makers and legal scholars on how a right to explanation might be regulated in the context of machine learning, such as clinical decision support systems. PI Doshi-Velez also engages high school students, undergraduates, women, and researchers from underserved areas in her lab.\r\n\r\n\r\nThe proposed work addresses a specific challenge common in scientific settings: domain-targeted interpretability. In many scientific domains, unsupervised generative models are used by domain experts to understand patterns in the data, but as the dimensionality of data grow, the most salient patterns in the data may not be relevant for the specific investigation. For example, a psychiatrist may find the strongest signals in the data from his patient cohort come from diabetes and heart disease, which may not be relevant for choosing therapies for depression.  The proposed work leverages synergies in explaining domain-relevant patterns in the data and performing well on domain-relevant tasks to achieve domain-targeted interpretability. It defines a task-constrained approach to domain-targeted interpretability and develops essential inference techniques, develops extensions to sequential decision making, and defines extensions to improve downstream task performance while retaining interpretabilty.  While there is a large body of work on making unsupervised learning models also useful for downstream tasks, none of these approaches truly manage the trade-offs between providing an interpretation of data and task performance. The proposed work addresses these shortcomings to make domain-targeted interpretability and task performance synergistic goals, and proposes a number of innovations to solve the proposed objective. Innovations include combining an existing rich literature on inference traditional unsupervised models with modern inference techniques and directly searching for dimensions or patterns relevant to the downstream task.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Finale",
   "pi_last_name": "Doshi-Velez",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Finale Doshi-Velez",
   "pi_email_addr": "finale@seas.harvard.edu",
   "nsf_id": "000599894",
   "pi_start_date": "2018-02-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "33 Oxford St.",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382933",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 108068.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 328696.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 111270.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In many high-stakes settings, we want people to be able to inspect a model for errors and insights.&nbsp; For example, when using a model to make decisions about a patient's care, especially knowing the many ways in which models can be biased, we may want a clinician to be able to inspect the model first. But how is this even remotely possible, given the exploding size of our data streams?&nbsp;&nbsp;</p>\r\n<p>Our core insight is that there are many settings in which a complex data stream is a combination of many signals, not all of which are relevant to a task. For example, the variation in the measurements of a patient in the ICU may have to do with their different organ systems, time of day, and whether they currently have visitors. Only some of these patterns may be relevant if the task at hand is to determine whether the patient's blood pressure medications need to be changed.&nbsp; This insight allows us to build small, inspectable models from large, complex data.</p>\r\n<p><br />Intellectual Merit: The main contribution of this work was to understand and extend the settings in which decision-focused methods can be applied.&nbsp; For example, we made precise the statistical regimes in which very simple models, such as Gaussian mixtures, can be cleverly trained -- with our decision-focused methods -- to provide the insights necessary for decision-making.&nbsp; Of course, the trade-off when training models for just one purpose -- like choosing the blood pressure medications -- is that while we get a model that is smal and inspectable, it may not be usable for a different purpose -- like choosing kidney medications. The second core technical component of our work was to develop methods that create small models that not only do the original task well, but also can be used for as many other tasks as well.</p>\r\n<p>Broader Impact: In addition to training postdocs, graduate students, and masters students, as well as teaching a seminar based on this material, we applied our methods to several real contexts in psychiatry, critical care, and HIV management.&nbsp; In ongoing work, we are applying these ideas to cardiology and mental wellness.&nbsp; While our demonstrations have been relatively small proofs-of-concept, they provide evidence that it is possible to build small, inspectable, well-performing models if one uses the right training methods.&nbsp; We have disseminated this message not only within the machine learning community but also to the clinical community through publications and talks.</p><br>\n<p>\n Last Modified: 03/24/2025<br>\nModified by: Finale&nbsp; &nbsp;Doshi-Velez</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn many high-stakes settings, we want people to be able to inspect a model for errors and insights. For example, when using a model to make decisions about a patient's care, especially knowing the many ways in which models can be biased, we may want a clinician to be able to inspect the model first. But how is this even remotely possible, given the exploding size of our data streams?\r\n\n\nOur core insight is that there are many settings in which a complex data stream is a combination of many signals, not all of which are relevant to a task. For example, the variation in the measurements of a patient in the ICU may have to do with their different organ systems, time of day, and whether they currently have visitors. Only some of these patterns may be relevant if the task at hand is to determine whether the patient's blood pressure medications need to be changed. This insight allows us to build small, inspectable models from large, complex data.\r\n\n\n\nIntellectual Merit: The main contribution of this work was to understand and extend the settings in which decision-focused methods can be applied. For example, we made precise the statistical regimes in which very simple models, such as Gaussian mixtures, can be cleverly trained -- with our decision-focused methods -- to provide the insights necessary for decision-making. Of course, the trade-off when training models for just one purpose -- like choosing the blood pressure medications -- is that while we get a model that is smal and inspectable, it may not be usable for a different purpose -- like choosing kidney medications. The second core technical component of our work was to develop methods that create small models that not only do the original task well, but also can be used for as many other tasks as well.\r\n\n\nBroader Impact: In addition to training postdocs, graduate students, and masters students, as well as teaching a seminar based on this material, we applied our methods to several real contexts in psychiatry, critical care, and HIV management. In ongoing work, we are applying these ideas to cardiology and mental wellness. While our demonstrations have been relatively small proofs-of-concept, they provide evidence that it is possible to build small, inspectable, well-performing models if one uses the right training methods. We have disseminated this message not only within the machine learning community but also to the clinical community through publications and talks.\t\t\t\t\tLast Modified: 03/24/2025\n\n\t\t\t\t\tSubmitted by: Finale Doshi-Velez\n"
 }
}