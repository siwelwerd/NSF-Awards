{
 "awd_id": "1763657",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2018-06-28",
 "awd_max_amd_letter_date": "2020-08-06",
 "awd_abstract_narration": "Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. \r\n\r\nThe specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes \"coded computing\", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Viveck",
   "pi_last_name": "Cadambe",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Viveck R Cadambe",
   "pi_email_addr": "vcadambe6@gatech.edu",
   "nsf_id": "000677546",
   "pi_start_date": "2018-06-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "110 Technology Center Building",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 75777.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 75165.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 149058.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Modern machine learning (ML) is typically performed over large-scale parallel and distributed computing systems. The fundamental bottlenecks that limit the performance of such systems are (i) a few excessively slow and unreliable computing nodes (also known as&nbsp;</span><em>straggler</em><span>&nbsp;nodes) and (ii) communication performed during computation. In fact, due to the scale of the data and model complexity, obtaining reliable ML models for applications with large datasets can take several hours to several days in practice. Data privacy constraints can induce additional overheads, limiting the scalability of ML training and inference tasks. The project proposed, studied, and developed techniques in a sub-area of information theory called</span><em>&nbsp;Coded Computing</em><span>. Coded computing uses information-theoretic ideas to add a controlled amount of redundancy to the data/computation to handle delays, unreliability, privacy constraints, and communication overheads with the purpose of improving performance.</span></p>\n<p>&nbsp;The project developed and studied new coding techniques called&nbsp;<em>ShortDot</em>&nbsp;and&nbsp;<em>MatDot</em>&nbsp;codes. These codes respectively add redundancy for distributed matrix-vector and matrix-matrix products - operations at the core of several machine learning algorithms. Compared to state-of-the-art techniques, ShortDot and MatDot codes are shown to achieve a superior trade-off between the amount of redundancy required and the extent of tolerance to stragglers. The project incorporated MatDot codes into an existing parallel matrix multiplication called SUMMA and demonstrated the potential benefits in practice through implementation. The project generalized the initially developed MatDot codes by creating numerically stable variants, incorporating techniques from numerical approximation theory. The project also developed novel variants of MatDot codes for approximate computation (instead of exact computation), leading to a significant reduction in the amount of redundancy required for a fixed number of stragglers.</p>\n<p><span>The project also studied and developed coding techniques to efficiently incorporate redundancy in certain non-linear polynomial computations. The project also contributed new coding-theoretic formulations and solutions to reduce overheads in distributed computation frameworks with data privacy constraints. These coding theoretic formulations study the trade-offs between the accuracy of computation and the amount of leaked privacy (measured via a well-known metric called differential privacy) for a fixed number of colluding adversarial computing nodes aiming to violate privacy.</span></p>\n<p><strong>Broader Impacts</strong></p>\n<p>&nbsp;The project developed several novel techniques that promise to mitigate straggler bottlenecks in distributed machine learning. The developed techniques received significant attention from the information theory and machine learning research communities, as demonstrated by several citations, workshops, tutorials, and conference panels. The project contributed to training at least three graduate students in information theory and machine learning.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/21/2023<br>\nModified by: Viveck&nbsp;R&nbsp;Cadambe</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nModern machine learning (ML) is typically performed over large-scale parallel and distributed computing systems. The fundamental bottlenecks that limit the performance of such systems are (i) a few excessively slow and unreliable computing nodes (also known asstragglernodes) and (ii) communication performed during computation. In fact, due to the scale of the data and model complexity, obtaining reliable ML models for applications with large datasets can take several hours to several days in practice. Data privacy constraints can induce additional overheads, limiting the scalability of ML training and inference tasks. The project proposed, studied, and developed techniques in a sub-area of information theory calledCoded Computing. Coded computing uses information-theoretic ideas to add a controlled amount of redundancy to the data/computation to handle delays, unreliability, privacy constraints, and communication overheads with the purpose of improving performance.\n\n\nThe project developed and studied new coding techniques calledShortDotandMatDotcodes. These codes respectively add redundancy for distributed matrix-vector and matrix-matrix products - operations at the core of several machine learning algorithms. Compared to state-of-the-art techniques, ShortDot and MatDot codes are shown to achieve a superior trade-off between the amount of redundancy required and the extent of tolerance to stragglers. The project incorporated MatDot codes into an existing parallel matrix multiplication called SUMMA and demonstrated the potential benefits in practice through implementation. The project generalized the initially developed MatDot codes by creating numerically stable variants, incorporating techniques from numerical approximation theory. The project also developed novel variants of MatDot codes for approximate computation (instead of exact computation), leading to a significant reduction in the amount of redundancy required for a fixed number of stragglers.\n\n\nThe project also studied and developed coding techniques to efficiently incorporate redundancy in certain non-linear polynomial computations. The project also contributed new coding-theoretic formulations and solutions to reduce overheads in distributed computation frameworks with data privacy constraints. These coding theoretic formulations study the trade-offs between the accuracy of computation and the amount of leaked privacy (measured via a well-known metric called differential privacy) for a fixed number of colluding adversarial computing nodes aiming to violate privacy.\n\n\nBroader Impacts\n\n\nThe project developed several novel techniques that promise to mitigate straggler bottlenecks in distributed machine learning. The developed techniques received significant attention from the information theory and machine learning research communities, as demonstrated by several citations, workshops, tutorials, and conference panels. The project contributed to training at least three graduate students in information theory and machine learning.\n\n\n\t\t\t\t\tLast Modified: 12/21/2023\n\n\t\t\t\t\tSubmitted by: ViveckRCadambe\n"
 }
}