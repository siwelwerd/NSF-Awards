{
 "awd_id": "1833137",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Learning to Su",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2017-11-08",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 501505.0,
 "awd_amount": 501505.0,
 "awd_min_amd_letter_date": "2018-04-25",
 "awd_max_amd_letter_date": "2022-06-22",
 "awd_abstract_narration": "Today there is far more video being captured - by consumers, scientists, defense analysts, and others - than can ever be watched.  With this explosion of video data comes a pressing need to develop automatic video summarization algorithms.  Video summarization takes a long video as input and produces a short video as output, while preserving its information content as much as possible.   As such, summarization techniques have great potential to make large video collections substantially more efficient to browse, search, disseminate, and facilitate communication.  Such increased efficiency will play a vital role in many important application areas.  For example, with reliable summarization systems, a primatologist gathering long videos of her animal subjects could quickly browse a week's worth of their activity before deciding where to inspect the data most closely.  A young student searching YouTube to learn about Yellowstone National Park could see at a glance what content exists, much better than today's simple thumbnail images can depict.  An intelligence agent could rapidly sift through reams of aerial video, reducing the resources required to analyze surveillance data to identify suspicious activities.\r\n\r\nThis project develops new machine learning and computer vision algorithms for video summarization.  Unsupervised methods, which are the cornerstone of nearly all existing approaches, have become increasingly limiting due to their reliance on hand-crafted heuristics.  By instead posing video summarization as a supervised learning problem, this project investigates a markedly different formulation of the task.  The research team is investigating four key new ideas: powerful probabilistic models for learning to select the optimal subset of video frames for summarization, semi-supervised learning models and co-summarization algorithms for leveraging the abundance of multiple related videos, algorithms for exploiting photos on the Web to improve summarization, and evaluation protocols that assess summaries in a way that aligns well with human comprehension.  The broader impact of the proposed research includes practical tools for video summarization, scientific advances that appeal broadly to several communities, publicly disseminated research results, inter-disciplinarily trained graduate students, and outreach activities to engage young students in STEM education and career paths.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Leana",
   "pi_last_name": "Golubchik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Leana Golubchik",
   "pi_email_addr": "leana@cs.usc.edu",
   "nsf_id": "000445297",
   "pi_start_date": "2021-11-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Fei",
   "pi_last_name": "Sha",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fei Sha",
   "pi_email_addr": "feisha@usc.edu",
   "nsf_id": "000510744",
   "pi_start_date": "2018-04-25",
   "pi_end_date": "2021-11-26"
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S. Flower St.",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900890001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "CA37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 79573.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 135828.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 286104.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span class=\"s1\">This project's primary goal was to consider development of new approaches to video summarization.&nbsp;</span>Our efforts focused on an urgent need to develop methods for summarizing user-generated videos (e.g., in the context of mobile phones or wearable cameras) as that has become a common video genre on public sharing sites and in private exchanges.</p>\n<p class=\"p1\">Specifically, we focused on yielding practical tools for video summarization, to facilitate (video) information browsing, search, dissemination, and communication. Applications of reliable summarization systems with societal impact are abundant; examples include: a primatologist gathering long videos of her animal subjects, who could quickly browse a week's worth of their activity before deciding where to inspect the data more closely, or a young student searching YouTube to learn about Yellowstone National Park, seeing at a glance what content exists, (leading to much better outcomes than what simple thumbnail images can provide).</p>\n<p class=\"p1\"><span class=\"s1\">To address these important problems, in this project we focused on the following. We proposed a novel sequence-to-sequence learning model.<span>&nbsp;&nbsp;</span></span>We also investigated modeling techniques for hierarchical sequential data such as video and texts where there are correspondences across multiple modalities. We also focused on important research problems in vision+language, e.g., visual question and answering (VQA), where we showed that the current design of VQA suffered from significant flaws. We also investigated semi-supervised methods for learning&nbsp; representation for video summarization, where our keyinsight was that, videos of the same genre share structured visual concepts along time, with such visual concepts (visual motifs) having the potential to provide crucial hints for video summarization. Our work also included few-shot learning (FSL) for object recognition, overcoming the challenge of limited data annotation. Efforts in this project also included semantic understanding in text-only and multimodal, grounded spaces, with a focus on compositional generalization. Overall, we believe our research results advanced the state of the art in machine learning and computer vision.</p>\n<p class=\"p1\"><span class=\"s1\">Beyond the research outcomes of the project, we have also focused on training of PhD students,&nbsp;</span>while also using the project to promote interdisciplinary research between vision and language.</p><br>\n<p>\n Last Modified: 12/22/2023<br>\nModified by: Leana&nbsp;Golubchik</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project's primary goal was to consider development of new approaches to video summarization.Our efforts focused on an urgent need to develop methods for summarizing user-generated videos (e.g., in the context of mobile phones or wearable cameras) as that has become a common video genre on public sharing sites and in private exchanges.\n\n\nSpecifically, we focused on yielding practical tools for video summarization, to facilitate (video) information browsing, search, dissemination, and communication. Applications of reliable summarization systems with societal impact are abundant; examples include: a primatologist gathering long videos of her animal subjects, who could quickly browse a week's worth of their activity before deciding where to inspect the data more closely, or a young student searching YouTube to learn about Yellowstone National Park, seeing at a glance what content exists, (leading to much better outcomes than what simple thumbnail images can provide).\n\n\nTo address these important problems, in this project we focused on the following. We proposed a novel sequence-to-sequence learning model.We also investigated modeling techniques for hierarchical sequential data such as video and texts where there are correspondences across multiple modalities. We also focused on important research problems in vision+language, e.g., visual question and answering (VQA), where we showed that the current design of VQA suffered from significant flaws. We also investigated semi-supervised methods for learning representation for video summarization, where our keyinsight was that, videos of the same genre share structured visual concepts along time, with such visual concepts (visual motifs) having the potential to provide crucial hints for video summarization. Our work also included few-shot learning (FSL) for object recognition, overcoming the challenge of limited data annotation. Efforts in this project also included semantic understanding in text-only and multimodal, grounded spaces, with a focus on compositional generalization. Overall, we believe our research results advanced the state of the art in machine learning and computer vision.\n\n\nBeyond the research outcomes of the project, we have also focused on training of PhD students,while also using the project to promote interdisciplinary research between vision and language.\t\t\t\t\tLast Modified: 12/22/2023\n\n\t\t\t\t\tSubmitted by: LeanaGolubchik\n"
 }
}