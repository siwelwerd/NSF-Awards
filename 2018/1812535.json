{
 "awd_id": "1812535",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER - Integrating machine learning on autonomous platforms for target-tracking operations using stereo imagery",
 "cfda_num": "47.050",
 "org_code": "06040100",
 "po_phone": "7032927577",
 "po_email": "kbinkley@nsf.gov",
 "po_sign_block_name": "Kandace Binkley",
 "awd_eff_date": "2018-01-01",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 269173.0,
 "awd_amount": 323007.0,
 "awd_min_amd_letter_date": "2017-12-13",
 "awd_max_amd_letter_date": "2021-11-23",
 "awd_abstract_narration": "The ocean's midwaters (depths from 200 to 1000 meters where sunlight is dim) are increasingly becoming an area of interest for scientific discovery and study. Efforts to further explore this vast and incredibly important region in the ocean involves the development of small, nimble, autonomous underwater vehicles (AUVs) that can be used for a variety of missions. This proposal will use a large database in video images collected over 25 years to train the vehicle to identify and track targets in real-time using a pair of stereo cameras. This project will involve a Postdoctoral Researcher who will be mentored by collaborators at MBARI and Stanford, who are pioneers in applying machine learning algorithms to underwater imagery. Results of this effort will be disseminated via conferences, publications, and outreach through industry and media partners. Media programs at MBARI and National Geographic Society will produce YouTube videos and social media posts detailing the efforts, the project's personnel, methods, and discoveries.\r\n\r\nThe ocean's midwaters represent the largest ecosystem on earth with unique inhabitants and processes that link the surface waters to the seafloor. Efforts to further explore this vast and incredibly important region in the ocean involves development of AUVs that can be used for a variety of missions (e.g., transecting, tracking, fluid sampling). One of the key vehicle missions for these autonomous vehicles is to track targets in real-time. The tracking missions can be used for science questions as diverse as rates of marine snow sinking and its impact on biogeochemical cycling, the fate of rising methane from the benthos, and direct observations of organismal behavior to address their ecology and biomechanics. In order to conduct these tracking missions, robust algorithms are needed to identify and track targets as they change shape and state in realtime.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "GEO",
 "org_dir_long_name": "Directorate for Geosciences",
 "div_abbr": "OCE",
 "org_div_long_name": "Division Of Ocean Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kakani",
   "pi_last_name": "Young",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Kakani K Young",
   "pi_email_addr": "kakani@mbari.org",
   "nsf_id": "000597948",
   "pi_start_date": "2017-12-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Monterey Bay Aquarium Research Institute",
  "inst_street_address": "7700 SANDHOLDT RD",
  "inst_street_address_2": "",
  "inst_city_name": "MOSS LANDING",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8317751803",
  "inst_zip_code": "950399644",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "CA19",
  "org_lgl_bus_name": "MONTEREY BAY AQUARIUM RESEARCH INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GM6EL1UH2L83"
 },
 "perf_inst": {
  "perf_inst_name": "Monterey Bay Aquarium Research Institute",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "950399644",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "CA19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "168000",
   "pgm_ele_name": "OCEAN TECH & INTERDISC COORDIN"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 269173.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 53834.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">The ocean is our final frontier: despite being the largest habitable ecosystem on our planet, it is one of the least explored. Less than 1% of the ocean volume has been observed at any particular time with human eyes or imaging, and anywhere from 30-60% of life in the ocean is completely unknown to science. This life may harbor bioinspired innovations that could change the way we generate energy, discover new medical drugs, or rethink approaches to transportation. However, our lack of biological observational capacity in the ocean makes filling these knowledge gaps a challenge. Autonomous robots, particularly robots with the capability of targeted sampling and persistent observations, are needed to address this need. Here we set out to determine whether smarter algorithms that use artificial intelligence can enable autonomous underwater vehicles to study life in the ocean by enabling more robust, automated visual tracking.</p>\n<p class=\"p1\">In order to achieve this, several technological and organizational milestones needed to be overcome and included: (1) developing an image training set for machine learning to automate detection and classification of biological targets, (2) training machine learning algorithms on the image training set, (3) demonstrating automated detection and classification of biological targets on underwater vehicle-collected in situ video data, (4) integrating machine learning and computer vision tracking algorithms on an underwater vehicle, (5) demonstrating tracking algorithms on a platform in the laboratory, (6) test tank, and (7) in situ using an underwater vehicle (either autonomous or remote), and (8) dissemination of our efforts and results. All of these milestones were reached and included:</p>\n<ul class=\"ul1\">\n<li class=\"li2\"><span class=\"s1\">&nbsp;</span>Developing a machine learning image training set, called FathomNet, from a subset of MBARI?s 30-year, manually annotated video/image database. FathomNet is a global repository for labeled image data, and can be accessed at www.fathomnet.org.<span>&nbsp;</span></li>\n<li class=\"li2\">Training neural networks using a variety of architectures and making them publicly available through FathomNet?s Model Zoo at www.github.com/fathomnet/models. <span>&nbsp;</span></li>\n<li class=\"li2\">Using ROV MiniROV, we were able to demonstrate tracking algorithms using upgraded camera hardware. We also modified the tracking software to enable higher throughput of images using NVIDIA?s deepstream system.</li>\n<li class=\"li2\">Using ROV MiniROV during multiple sea days on the RVs Rachel Carson and Fulmar, we were able to demonstrate machine learning-integrated tracking algorithms as well as automated acquisition.</li>\n<li class=\"li2\">Disseminating our work through several invited lectures at both public and academic forums, numerous websites and news articles, conferences (Ocean Sciences, Society of Integrative and Comparative Biology, the National Academy of Science/Engineering UN Decade of the Ocean Ocean Shot event), and publications (Workshop on Applications of Computer Vision, Scientific Reports, arrive, and in review at Science Robotics).</li>\n</ul>\n<p class=\"p2\">Our efforts showed that smarter underwater vehicles can be deployed to tackle challenging biological observational tasks, including tracking individual animals in the deep sea for several hours at a time. The algorithms we developed are a significant stride toward expanding the capabilities of underwater robots by actively adjusting behavior in response to real-time visual observations. Our experiments demonstrated the approach's efficacy on an ROV, allowing a human operator to completely step away from the controls. Deploying these algorithms on AUVs would fundamentally alter our approach to studying organisms in the deep sea, speeding the discovery of ocean life and processes unknown to the research community. The future of species discovery must someday leverage algorithms like ours to autonomously run exploration missions to continuously monitor an ocean region or explore a new one. As algorithms and embedded hardware continue to improve on autonomous vehicles, data collected during these missions may someday lead to onboard learning of features of animals and objects without loss of performance on existing classes, identification of unknown classes, and verification by human observers via Supervised Autonomy. These advances, enabled by our efforts here, are critical to scale our ability to discover, study, and monitor the diverse animals that inhabit our ocean.<span>&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/16/2023<br>\n\t\t\t\t\tModified by: Kakani&nbsp;K&nbsp;Young</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1812535/1812535_10525791_1683003310237_mission-overview--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1812535/1812535_10525791_1683003310237_mission-overview--rgov-800width.jpg\" title=\"Underwater mission overview\"><img src=\"/por/images/Reports/POR/2023/1812535/1812535_10525791_1683003310237_mission-overview--rgov-66x44.jpg\" alt=\"Underwater mission overview\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Integrating machine learning (ML) algorithms into vehicle controllers enables a suite of underwater observational missions. By varying the duration of various modes (search, acquire, track), an autonomous underwater vehicle can conduct Discovery, Transect, and Follow missions.</div>\n<div class=\"imageCredit\">MBARI</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Kakani&nbsp;K&nbsp;Young</div>\n<div class=\"imageTitle\">Underwater mission overview</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1812535/1812535_10525791_1683003419648_ML-tracking_classes--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1812535/1812535_10525791_1683003419648_ML-tracking_classes--rgov-800width.jpg\" title=\"Animals studied\"><img src=\"/por/images/Reports/POR/2023/1812535/1812535_10525791_1683003419648_ML-tracking_classes--rgov-66x44.jpg\" alt=\"Animals studied\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Highlight images of midwater animals that served as target objects during field trials. Each image represents one of the 15 taxonomic groups that formed 17separate classes in the model used in this work.</div>\n<div class=\"imageCredit\">MBARI</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Kakani&nbsp;K&nbsp;Young</div>\n<div class=\"imageTitle\">Animals studied</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "The ocean is our final frontier: despite being the largest habitable ecosystem on our planet, it is one of the least explored. Less than 1% of the ocean volume has been observed at any particular time with human eyes or imaging, and anywhere from 30-60% of life in the ocean is completely unknown to science. This life may harbor bioinspired innovations that could change the way we generate energy, discover new medical drugs, or rethink approaches to transportation. However, our lack of biological observational capacity in the ocean makes filling these knowledge gaps a challenge. Autonomous robots, particularly robots with the capability of targeted sampling and persistent observations, are needed to address this need. Here we set out to determine whether smarter algorithms that use artificial intelligence can enable autonomous underwater vehicles to study life in the ocean by enabling more robust, automated visual tracking.\nIn order to achieve this, several technological and organizational milestones needed to be overcome and included: (1) developing an image training set for machine learning to automate detection and classification of biological targets, (2) training machine learning algorithms on the image training set, (3) demonstrating automated detection and classification of biological targets on underwater vehicle-collected in situ video data, (4) integrating machine learning and computer vision tracking algorithms on an underwater vehicle, (5) demonstrating tracking algorithms on a platform in the laboratory, (6) test tank, and (7) in situ using an underwater vehicle (either autonomous or remote), and (8) dissemination of our efforts and results. All of these milestones were reached and included:\n\n Developing a machine learning image training set, called FathomNet, from a subset of MBARI?s 30-year, manually annotated video/image database. FathomNet is a global repository for labeled image data, and can be accessed at www.fathomnet.org. \nTraining neural networks using a variety of architectures and making them publicly available through FathomNet?s Model Zoo at www.github.com/fathomnet/models.  \nUsing ROV MiniROV, we were able to demonstrate tracking algorithms using upgraded camera hardware. We also modified the tracking software to enable higher throughput of images using NVIDIA?s deepstream system.\nUsing ROV MiniROV during multiple sea days on the RVs Rachel Carson and Fulmar, we were able to demonstrate machine learning-integrated tracking algorithms as well as automated acquisition.\nDisseminating our work through several invited lectures at both public and academic forums, numerous websites and news articles, conferences (Ocean Sciences, Society of Integrative and Comparative Biology, the National Academy of Science/Engineering UN Decade of the Ocean Ocean Shot event), and publications (Workshop on Applications of Computer Vision, Scientific Reports, arrive, and in review at Science Robotics).\n\nOur efforts showed that smarter underwater vehicles can be deployed to tackle challenging biological observational tasks, including tracking individual animals in the deep sea for several hours at a time. The algorithms we developed are a significant stride toward expanding the capabilities of underwater robots by actively adjusting behavior in response to real-time visual observations. Our experiments demonstrated the approach's efficacy on an ROV, allowing a human operator to completely step away from the controls. Deploying these algorithms on AUVs would fundamentally alter our approach to studying organisms in the deep sea, speeding the discovery of ocean life and processes unknown to the research community. The future of species discovery must someday leverage algorithms like ours to autonomously run exploration missions to continuously monitor an ocean region or explore a new one. As algorithms and embedded hardware continue to improve on autonomous vehicles, data collected during these missions may someday lead to onboard learning of features of animals and objects without loss of performance on existing classes, identification of unknown classes, and verification by human observers via Supervised Autonomy. These advances, enabled by our efforts here, are critical to scale our ability to discover, study, and monitor the diverse animals that inhabit our ocean. \n\n \n\n\t\t\t\t\tLast Modified: 05/16/2023\n\n\t\t\t\t\tSubmitted by: Kakani K Young"
 }
}