{
 "awd_id": "2147064",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "How does visual long-term memory implement control over visual attention?",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032924502",
 "po_email": "dkravitz@nsf.gov",
 "po_sign_block_name": "Dwight Kravitz",
 "awd_eff_date": "2022-09-01",
 "awd_exp_date": "2025-08-31",
 "tot_intn_awd_amt": 475715.0,
 "awd_amount": 475715.0,
 "awd_min_amd_letter_date": "2022-08-11",
 "awd_max_amd_letter_date": "2022-08-11",
 "awd_abstract_narration": "In our everyday lives we often apply our prior knowledge while simultaneously interacting with the natural world in all its complexity. Consider a simple trip to the grocery store. First, one might have to find your car keys on a messy desk. Your desk may have items like a holiday photo, pencil sharpener or a computer that naturally draw attention but are of no use in going to the store, so they must be ignored. Then, having found the keys and starting to drive, one must forget the keys and instead remember to attend to children at play, pedestrians, cyclists, other drivers, traffic signals, street signs and landmarks to safely follow the right route to the store. Despite all the challenges apparent in even this seemingly simple task, people accomplish it, and far more complex tasks, with apparent ease. This project seeks to gain a better understanding of how prior knowledge guides our current actions and perceptions of the environment. A better handle on this process can help us understand a fundamental element of human ability that still eludes even our most advanced artificial intelligence systems. A substantial benefit to society is inherent in the research: the potential to understand and thereby improve attentional control for drivers or air traffic controllers. In addition to the broader impact in psychology and neuroscience, scientists will engage in public outreach such as the Annual Brain Blast events, the Brain Bios podcast, and the K-12 Teacher-Neuroscientist partnership to improve understanding and appreciation of neuroscience.\r\n\r\nTo understand how prior knowledge guides perception and current behavior, this proposal involves non-invasively recording electrical brain activity (with electroencephalography or EEG) from people searching for particular objects in cluttered scenes. These scenes are picture of objects arranged randomly on a computer screen. The researchers will use these recordings to investigate whether prior knowledge (templates in visual long-term memory) needs to be intentionally held in mind to guide visual attention and visuospatial behavior such as visual target detection. Using this approach, researchers can measure brain activity that tracks what you store in memory, as well as what you pay attention to in the scenes. Analysis of these data will reveal whether brain activity related to object memory is all you need to pay attention to that object. A key question these experiments will resolve is the relative contribution of long-term object memory and short-term working memory to attentional object search. Competing hypotheses will be tested using EEG combined with noninvasive transcranial brain stimulation. Scientists will also study whether if you have previously found an object in a scene whether it then becomes particularly difficult to ignore in the future. The hypothesis is that previously having searched for an object will lead to more attention being drawn to that object in a future search. Experiments will test whether this effect can be observed in the response of the brain. Preliminary results suggest that memory shapes attention for specific objects and moreover, that people can simultaneously hold multiple objects in memory, and use these search templates to look for multiple objects at the same time, a feat that is currently impossible for artificial intelligence systems. These findings will be used to make scientific progress by testing models of human attention and memory, as well as helping us design better artificial intelligence systems that can match human ability and potentially better partner with humans as well.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Geoffrey",
   "pi_last_name": "Woodman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Geoffrey Woodman",
   "pi_email_addr": "geoffrey.f.woodman@vanderbilt.edu",
   "nsf_id": "000084360",
   "pi_start_date": "2022-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Vanderbilt University",
  "inst_street_address": "110 21ST AVE S",
  "inst_street_address_2": "",
  "inst_city_name": "NASHVILLE",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "6153222631",
  "inst_zip_code": "372032416",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "TN05",
  "org_lgl_bus_name": "VANDERBILT UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "GTNBNWXJ12D5"
 },
 "perf_inst": {
  "perf_inst_name": "Vanderbilt University",
  "perf_str_addr": "110 21 St. Ave. South",
  "perf_city_name": "Nashville",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "372350002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "TN07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1699",
   "pgm_ref_txt": "COGNEURO"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 475715.0
  }
 ],
 "por": null
}