{
 "awd_id": "2231796",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: EAGER: RI: Causal Decision-Making",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2022-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2022-08-01",
 "awd_max_amd_letter_date": "2022-08-01",
 "awd_abstract_narration": "Artificial intelligence (AI) plays an increasingly prominent role in society since decisions that were once made by humans are now being delegated to automated systems. These systems are expected to be efficient, robust, explainable, generalizable, and lead to outcomes agreed upon by society. There is a growing understanding that robust decision-making relies on some knowledge of the causal mechanisms underlying the environment. For instance, an intelligent robot has to know the cause and effect relationships in its environment to plan its course of action more robustly; a physician needs to understand the effects of available drugs to design an effective strategy for her patients. The current generation of AI systems responsible for decision-making does not explicitly represent the underlying causal model. This project will build the foundations toward a general framework \u2014 i.e., a set of principles, algorithms, and tools \u2014 for decision-making systems by enriching the traditional AI formalism with causal ingredients for more efficient, robust, and explainable decision-making. The research will plant the seed for a transformation in the decision-making field and have consequences for developing the next generation of AI systems. The research results are expected to have significant impacts on AI foundations and may potentially have broad implications for society as more and more decisions are being delegated to AI systems. The researchers will develop new educational materials and course curricula in causal inference. The researchers will provide research training for graduate students and are committed to continuing to recruit from underrepresented groups. The research team will continue supporting the \u201cCausality in Statistics Education Award\u201d to improve the teaching and learning of modern causal inference tools in statistics and the data sciences.\r\n\r\nThis project is the first step toward the integration of causal inference (CI) and reinforcement learning (RL) into the discipline of causal reinforcement learning (CRL). The idea is to endow an RL agent with an explicit causal model of the environment and new capabilities for interventional and counterfactual reasoning. CRL will open a new family of learning opportunities and challenges that were neither acknowledged nor understood before.  The tasks included in this research include integrating offline and online methods when the agents have different perceptual and actuation capabilities and developing general machinery for counterfactual decision-making, which is more powerful than its standard, interventional counterpart.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Elias",
   "pi_last_name": "Bareinboim",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Elias Bareinboim",
   "pi_email_addr": "eb@cs.columbia.edu",
   "nsf_id": "000717905",
   "pi_start_date": "2022-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "202 LOW LIBRARY 535 W 116 ST MC 4309,",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "10027",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7484",
   "pgm_ref_txt": "IIS SPECIAL PROJECTS"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project significantly advanced the field of Artificial Intelligence (AI) decision-making by developing Causal Reinforcement Learning (CRL) techniques, which aim to address statistical limitations and improve the robustness, explainability, and adaptability of automated decision systems. Unlike traditional AI, which relies on fixed rules or deep learning without explicit causation, CRL leverages cause-effect reasoning to enhance decision quality, especially in complex environments where decisions must adapt to new, dynamic data.</p>\r\n<p>The project made significant progress across three main areas:</p>\r\n<p>First, in Integrated Online-Offline Policy Learning, we developed techniques for learning from both real-time interactions and historical data while addressing varied agent perspectives and intervention capabilities. This approach minimizes confounding biases, helping to accurately evaluate and apply policies across diverse environments, benefiting applications like healthcare, finance, and autonomous systems.</p>\r\n<p>Second, the project introduced a Counterfactual Decision Criterion&mdash;a novel framework using counterfactual reasoning to estimate how different choices might impact outcomes. This framework, especially beneficial for systems with unobserved influences (e.g., social or economic factors), demonstrated more reliable decision-making than traditional methods and has applications in policy analysis and medical treatment planning.</p>\r\n<p>Finally, in Causal Imitation Learning, we expanded traditional imitation learning to enable agents to learn policies even when their data perceptions differ from expert perspectives. This innovation supports safe decision-making in fields such as robotics and autonomous driving, where agents operate in human-designed environments with different \"senses\" from humans.</p>\r\n<p>The project&rsquo;s intellectual merit lies in starting to&nbsp;create a bridge between causal inference and reinforcement learning towards a new generation of more adaptable and safe decision-making systems. By addressing issues of bias, counterfactual decision-making, and cross-agent learning disparities, this research paves the way for robust and interpretable AI systems capable of functioning effectively in diverse, real-world conditions. Broadly, these advancements could improve AI-driven decisions in healthcare, autonomous vehicles, and economic modeling, fostering safer, transparent, and effective solutions across multiple disciplines.</p>\r\n<p>Through these advancements, this project not only contributed to AI research but also laid a foundation for developing decision-making systems that could benefit society by ensuring AI is fairer, more transparent, and better aligned with human ethical standards.</p><br>\n<p>\n Last Modified: 01/21/2025<br>\nModified by: Elias&nbsp;Bareinboim</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project significantly advanced the field of Artificial Intelligence (AI) decision-making by developing Causal Reinforcement Learning (CRL) techniques, which aim to address statistical limitations and improve the robustness, explainability, and adaptability of automated decision systems. Unlike traditional AI, which relies on fixed rules or deep learning without explicit causation, CRL leverages cause-effect reasoning to enhance decision quality, especially in complex environments where decisions must adapt to new, dynamic data.\r\n\n\nThe project made significant progress across three main areas:\r\n\n\nFirst, in Integrated Online-Offline Policy Learning, we developed techniques for learning from both real-time interactions and historical data while addressing varied agent perspectives and intervention capabilities. This approach minimizes confounding biases, helping to accurately evaluate and apply policies across diverse environments, benefiting applications like healthcare, finance, and autonomous systems.\r\n\n\nSecond, the project introduced a Counterfactual Decision Criteriona novel framework using counterfactual reasoning to estimate how different choices might impact outcomes. This framework, especially beneficial for systems with unobserved influences (e.g., social or economic factors), demonstrated more reliable decision-making than traditional methods and has applications in policy analysis and medical treatment planning.\r\n\n\nFinally, in Causal Imitation Learning, we expanded traditional imitation learning to enable agents to learn policies even when their data perceptions differ from expert perspectives. This innovation supports safe decision-making in fields such as robotics and autonomous driving, where agents operate in human-designed environments with different \"senses\" from humans.\r\n\n\nThe projects intellectual merit lies in starting tocreate a bridge between causal inference and reinforcement learning towards a new generation of more adaptable and safe decision-making systems. By addressing issues of bias, counterfactual decision-making, and cross-agent learning disparities, this research paves the way for robust and interpretable AI systems capable of functioning effectively in diverse, real-world conditions. Broadly, these advancements could improve AI-driven decisions in healthcare, autonomous vehicles, and economic modeling, fostering safer, transparent, and effective solutions across multiple disciplines.\r\n\n\nThrough these advancements, this project not only contributed to AI research but also laid a foundation for developing decision-making systems that could benefit society by ensuring AI is fairer, more transparent, and better aligned with human ethical standards.\t\t\t\t\tLast Modified: 01/21/2025\n\n\t\t\t\t\tSubmitted by: EliasBareinboim\n"
 }
}