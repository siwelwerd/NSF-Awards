{
 "awd_id": "2150135",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "REU Site: Undergraduate Research Experiences in Machine Learning, Analytics, and Augmented Reality for Smart and Connected Health",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032928104",
 "po_email": "shabagch@nsf.gov",
 "po_sign_block_name": "Sharmistha Bagchi-Sen",
 "awd_eff_date": "2022-01-15",
 "awd_exp_date": "2024-12-31",
 "tot_intn_awd_amt": 344029.0,
 "awd_amount": 350429.0,
 "awd_min_amd_letter_date": "2022-01-11",
 "awd_max_amd_letter_date": "2024-01-31",
 "awd_abstract_narration": "This project aims to facilitate the inclusion of undergraduates in research targeting the emerging areas of smart and connected health and computer engineering. Undergraduate students will design and develop concepts and applications that integrate artificial intelligence (AI) and data-driven techniques to advance the national health, prosperity, and welfare. The REU site provides a unique opportunity for undergraduates to engage with a diverse research program that includes working with persons with disabilities such as autism spectrum disorder (ASD) and multiple sclerosis (MS) and planning for smart city fire-related responses. Some of the research projects address the behavior and hygiene of children with ASD using new engineering data and cyber visual techniques. Integrating technology such as biosensors, virtual reality, and machine learning are crucial for developing reliable and sensitive assessment tools for people with MS and increasing confidence in the results. Smart city firefighting provides a unique learning experience for undergraduates in monitoring temperatures while developing autonomous data collection methods to highlight areas of concern for safety officials. Moreover, many research topics are not covered under traditional undergraduate curricula and will help prepare a new generation of professionals.\r\n\r\nThis REU site seeks to expose undergraduate students to research in the emerging fields of smart and connected communities and smart and connected health. The undergraduate students will design and develop concepts, systems, and applications that target smart-health and smart-community engineering problems. The intellectual theme of this project ranges from designing an ecological assessment to applying machine learning (ML), data analytics, and augmented reality (AR) as new paradigms to analyze and discover new aspects of health-related applications. Each summer, the REU site will provide a diverse group of undergraduate students, many from underrepresented groups, opportunities to directly participate in research activities to develop disciplinary knowledge, research abilities, and team skills. The focus of the REU site is to enable students to perform research in three main areas: 1) intelligent emotion recognition and cyber training for children with autism spectrum disorder (ASD); 2) cognitive and body posture control for persons with disabilities; and 3) smart-city fire response. The REU research projects will advance the state of the art in ecological assessment utilizing ML computational models, AR visualization, and data analysis and will be performed by the undergraduate students to serve their communities. Eight weeks of in-depth summer research will increase undergraduate students\u2019 retention, improve their career perspectives, engage them to participate in the next frontier of data-driven research, and motivate them to enter graduate STEM-Health programs.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Damian",
   "pi_last_name": "Valles Molina",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Damian Valles Molina",
   "pi_email_addr": "d_v173@txstate.edu",
   "nsf_id": "000769807",
   "pi_start_date": "2022-01-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ting",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ting Liu",
   "pi_email_addr": "ting.liu@tamusa.edu",
   "nsf_id": "000851093",
   "pi_start_date": "2022-01-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas State University - San Marcos",
  "inst_street_address": "601 UNIVERSITY DR",
  "inst_street_address_2": "",
  "inst_city_name": "SAN MARCOS",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5122452314",
  "inst_zip_code": "786664684",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "TX15",
  "org_lgl_bus_name": "TEXAS STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HS5HWWK1AAU5"
 },
 "perf_inst": {
  "perf_inst_name": "Texas State University - San Marcos",
  "perf_str_addr": "601 University Drive",
  "perf_city_name": "San Marcos",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "786664684",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "TX15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "113900",
   "pgm_ele_name": "RSCH EXPER FOR UNDERGRAD SITES"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9250",
   "pgm_ref_txt": "REU SITE-Res Exp for Ugrd Site"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 344029.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 6400.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span>The REU site successfully engaged undergraduate researchers in interdisciplinary projects integrating AI, machine learning, AR/VR, and sensor technologies to address challenges in real-world healthcare and smart city systems. Projects demonstrated substantial intellectual merit by advancing diagnostic tools for Autism Spectrum Disorder (ASD) and Multiple Sclerosis (MS) through novel applications of Convolutional Neural Network (CNN), postural biometrics, gait analysis, and immersive simulations. Several models achieved over 90% accuracy, with impactful innovations such as real-time emotion recognition, VR-based motor skill assessments, and AI-augmented autonomous rovers. The broader impact is evident through inclusive datasets, attention to gender representation, real-world app development, and accessibility of diagnostic tools&mdash;empowering diverse communities and preparing students for careers in data-driven, socially responsive STEM fields.</span></p>\r\n<p class=\"p1\"><span>Project 1:&nbsp;<em><span>Measuring the effects of machine learning app on emotion recognition ability in autism spectrum disorder</span></em>. Undergraduate researchers developed and tested a real-time facial emotion recognition system using CNN-based models trained on a newly curated, diverse emotion portrayal database. Over 208 participants were interviewed via Zoom and prompted to demonstrate seven universal emotions, generating over 225,000 labeled facial images. Na&iuml;ve-CNN and VGG16 achieved the highest performance, with over 95% test accuracy, while EfficientNetV2 and MobileNetV2 showed potential but struggled with class imbalance and contextual misclassification. A gender analysis revealed discrepancies in how male and female expressions were labeled, with neutral expressions most reliably identified. A mobile iOS app was successfully developed for real-world emotion recognition.</span></p>\r\n<p class=\"p1\"><span>Project 2:&nbsp;<em><span>Smart city firefighting autonomous data collection</span></em>. Undergraduate researchers developed an autonomous rover for firefighter rescue operations, integrating ROS2 with MobileNetV2 and YAMNet models for real-time person and scream detection. Built on a rocker-bogie chassis for stair-climbing and debris navigation, the rover uses three Jetson Nanos for parallel computing. Hardware includes environmental sensors, an infrared camera, and a directional microphone. Post-optimization, AI inference times dropped from 215.7s to 3.1s (scream) and 30.9s to 0.00008s (person), enabling real-time responsiveness. A master ROS2 node synchronized data streams for efficient communication. The system demonstrates scalable, low-latency, AI-enhanced capabilities for emergency scenarios and future multi-agent coordination.</span></p>\r\n<p class=\"p1\"><span>Project 3:&nbsp;<em><span>Augmented reality hand washing tool for children with ASD</span></em>. The students introduced and evaluated an Augmented Reality (AR) handwashing training tool for children with Autism Spectrum Disorder (ASD) using Microsoft HoloLens 2. The tool simulated realistic bathroom settings and guided users through CDC-compliant handwashing steps with visual, audio, and gamified cues. Experimental trials with children and neurotypical participants showed improved handwashing scores and reduced anxiety due to minimized sensory stimuli. Users favored the interactive experience, mainly the &ldquo;scrub both hands&rdquo; phase with confetti and music. Enhanced generalization across different fixtures and scenes indicated strong potential for skill transfer, paving the way for scalable AR interventions in life skills education.</span></p>\r\n<p class=\"p1\"><span>Project 4:&nbsp;<em><span>Virtual reality and machine learning on movement assessment in autism spectrum disorder.</span></em>&nbsp;The students evaluated using Virtual Reality (VR) and machine learning to assess and support motor skill development in children with Autism Spectrum Disorder (ASD). Researchers recreated the full MABC-2 motor assessment in VR using Unity and Blender, allowing children to observe avatar-led demonstrations and interact in immersive tasks. Using the Random Forest machine learning model, we analyzed a dataset of MABC-2 scores from 250 children with ASD across all age bands. A statistically significant improvement was observed in balance scores post-VR (p &lt; 0.05), while a Random Forest classifier achieved 91% accuracy in identifying motor delays based solely on balance data.</span></p>\r\n<p class=\"p1\"><span>Project 5:&nbsp;<em><span>The Development of a Disability Assessment Tool using Biosensors, Virtual Reality, and Machine Learning for Persons with Multiple Sclerosis</span></em>. The students explored machine learning and data augmentation for classifying multiple sclerosis (MS) severity based on gait data from inertial measurement units (IMUs). Researchers used simulated impairments and the 6-Minute Walk Test to collect gait parameters, then generated synthetic data using a variational autoencoder (VAE). The generated dataset closely matched 18 of 22 original gait parameters. Classification models&mdash;Random Forest, Logistic Regression, Gradient Boosting&mdash;achieved balanced accuracies of around 85%, with Logistic Regression performing best. The system successfully predicted EDSS scores within 0.5 of the actual value, showing strong potential for augmenting clinical MS diagnosis with AI-driven gait assessment.</span></p>\r\n<p class=\"p1\"><span>Project 6:&nbsp;<em><span>Enhancing early diagnosis of autism with machine learning algorithms using postural control features</span></em>. The students investigated postural control data as a diagnostic tool for Autism Spectrum Disorder (ASD) using machine learning. Researchers collected center of pressure (COP) features from 38 children aged 5&ndash;16 using a force plate under eyes-open and eyes-closed conditions. Linear and nonlinear features were extracted, including displacement, sway area, and multiscale entropy. Seven classification models were trained and evaluated, with Random Forest achieving 97% accuracy and F1-score. AP complexity, path distance, and ML complexity emerged as the most influential features. Results support the viability of COP-based machine learning models for early ASD diagnosis and highlight key postural biomarkers.</span></p>\r\n<p class=\"p1\">&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/18/2025<br>\nModified by: Damian&nbsp;Valles Molina</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735748726952_nxp_9--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735748726952_nxp_9--rgov-800width.jpg\" title=\"NXP Visit\"><img src=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735748726952_nxp_9--rgov-66x44.jpg\" alt=\"NXP Visit\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">TXST REU students toured an industry site</div>\n<div class=\"imageCredit\">TXST REU</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ting&nbsp;Liu\n<div class=\"imageTitle\">NXP Visit</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735748391624_reu_txst_presentations_1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735748391624_reu_txst_presentations_1--rgov-800width.jpg\" title=\"TXST REU Student Reserach Poster Presentation\"><img src=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735748391624_reu_txst_presentations_1--rgov-66x44.jpg\" alt=\"TXST REU Student Reserach Poster Presentation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Student Reserach Poster Presentation</div>\n<div class=\"imageCredit\">TXST REU</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ting&nbsp;Liu\n<div class=\"imageTitle\">TXST REU Student Reserach Poster Presentation</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735748293475_1691618423690--rgov-214x142.jpeg\" original=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735748293475_1691618423690--rgov-800width.jpeg\" title=\"Development of an Augmented Reality Handwashing Tool for Children with Autism Spectrum Disorder\"><img src=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735748293475_1691618423690--rgov-66x44.jpeg\" alt=\"Development of an Augmented Reality Handwashing Tool for Children with Autism Spectrum Disorder\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">AR Handwashing\r\nTool for ASD</div>\n<div class=\"imageCredit\">TXST REU</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ting&nbsp;Liu\n<div class=\"imageTitle\">Development of an Augmented Reality Handwashing Tool for Children with Autism Spectrum Disorder</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735749155159_thumbnail_IMG_1831--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735749155159_thumbnail_IMG_1831--rgov-800width.jpg\" title=\"Implementation of REU Student-Designed Virtual Reality to Enhance Motor-Skill Performance in Children with Autism Spectrum Disorder\"><img src=\"/por/images/Reports/POR/2025/2150135/2150135_10779953_1735749155159_thumbnail_IMG_1831--rgov-66x44.jpg\" alt=\"Implementation of REU Student-Designed Virtual Reality to Enhance Motor-Skill Performance in Children with Autism Spectrum Disorder\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">REU Student-Designed Virtual Reality on Children with ASD Motor Skill Assessment</div>\n<div class=\"imageCredit\">TXST REU</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ting&nbsp;Liu\n<div class=\"imageTitle\">Implementation of REU Student-Designed Virtual Reality to Enhance Motor-Skill Performance in Children with Autism Spectrum Disorder</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe REU site successfully engaged undergraduate researchers in interdisciplinary projects integrating AI, machine learning, AR/VR, and sensor technologies to address challenges in real-world healthcare and smart city systems. Projects demonstrated substantial intellectual merit by advancing diagnostic tools for Autism Spectrum Disorder (ASD) and Multiple Sclerosis (MS) through novel applications of Convolutional Neural Network (CNN), postural biometrics, gait analysis, and immersive simulations. Several models achieved over 90% accuracy, with impactful innovations such as real-time emotion recognition, VR-based motor skill assessments, and AI-augmented autonomous rovers. The broader impact is evident through inclusive datasets, attention to gender representation, real-world app development, and accessibility of diagnostic toolsempowering diverse communities and preparing students for careers in data-driven, socially responsive STEM fields.\r\n\n\nProject 1:Measuring the effects of machine learning app on emotion recognition ability in autism spectrum disorder. Undergraduate researchers developed and tested a real-time facial emotion recognition system using CNN-based models trained on a newly curated, diverse emotion portrayal database. Over 208 participants were interviewed via Zoom and prompted to demonstrate seven universal emotions, generating over 225,000 labeled facial images. Nave-CNN and VGG16 achieved the highest performance, with over 95% test accuracy, while EfficientNetV2 and MobileNetV2 showed potential but struggled with class imbalance and contextual misclassification. A gender analysis revealed discrepancies in how male and female expressions were labeled, with neutral expressions most reliably identified. A mobile iOS app was successfully developed for real-world emotion recognition.\r\n\n\nProject 2:Smart city firefighting autonomous data collection. Undergraduate researchers developed an autonomous rover for firefighter rescue operations, integrating ROS2 with MobileNetV2 and YAMNet models for real-time person and scream detection. Built on a rocker-bogie chassis for stair-climbing and debris navigation, the rover uses three Jetson Nanos for parallel computing. Hardware includes environmental sensors, an infrared camera, and a directional microphone. Post-optimization, AI inference times dropped from 215.7s to 3.1s (scream) and 30.9s to 0.00008s (person), enabling real-time responsiveness. A master ROS2 node synchronized data streams for efficient communication. The system demonstrates scalable, low-latency, AI-enhanced capabilities for emergency scenarios and future multi-agent coordination.\r\n\n\nProject 3:Augmented reality hand washing tool for children with ASD. The students introduced and evaluated an Augmented Reality (AR) handwashing training tool for children with Autism Spectrum Disorder (ASD) using Microsoft HoloLens 2. The tool simulated realistic bathroom settings and guided users through CDC-compliant handwashing steps with visual, audio, and gamified cues. Experimental trials with children and neurotypical participants showed improved handwashing scores and reduced anxiety due to minimized sensory stimuli. Users favored the interactive experience, mainly the scrub both hands phase with confetti and music. Enhanced generalization across different fixtures and scenes indicated strong potential for skill transfer, paving the way for scalable AR interventions in life skills education.\r\n\n\nProject 4:Virtual reality and machine learning on movement assessment in autism spectrum disorder.The students evaluated using Virtual Reality (VR) and machine learning to assess and support motor skill development in children with Autism Spectrum Disorder (ASD). Researchers recreated the full MABC-2 motor assessment in VR using Unity and Blender, allowing children to observe avatar-led demonstrations and interact in immersive tasks. Using the Random Forest machine learning model, we analyzed a dataset of MABC-2 scores from 250 children with ASD across all age bands. A statistically significant improvement was observed in balance scores post-VR (p \r\n\n\nProject 5:The Development of a Disability Assessment Tool using Biosensors, Virtual Reality, and Machine Learning for Persons with Multiple Sclerosis. The students explored machine learning and data augmentation for classifying multiple sclerosis (MS) severity based on gait data from inertial measurement units (IMUs). Researchers used simulated impairments and the 6-Minute Walk Test to collect gait parameters, then generated synthetic data using a variational autoencoder (VAE). The generated dataset closely matched 18 of 22 original gait parameters. Classification modelsRandom Forest, Logistic Regression, Gradient Boostingachieved balanced accuracies of around 85%, with Logistic Regression performing best. The system successfully predicted EDSS scores within 0.5 of the actual value, showing strong potential for augmenting clinical MS diagnosis with AI-driven gait assessment.\r\n\n\nProject 6:Enhancing early diagnosis of autism with machine learning algorithms using postural control features. The students investigated postural control data as a diagnostic tool for Autism Spectrum Disorder (ASD) using machine learning. Researchers collected center of pressure (COP) features from 38 children aged 516 using a force plate under eyes-open and eyes-closed conditions. Linear and nonlinear features were extracted, including displacement, sway area, and multiscale entropy. Seven classification models were trained and evaluated, with Random Forest achieving 97% accuracy and F1-score. AP complexity, path distance, and ML complexity emerged as the most influential features. Results support the viability of COP-based machine learning models for early ASD diagnosis and highlight key postural biomarkers.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 04/18/2025\n\n\t\t\t\t\tSubmitted by: DamianValles Molina\n"
 }
}