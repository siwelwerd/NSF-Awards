{
 "awd_id": "2217086",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: PPoSS: Planning: Cross-layer Coordination and Optimization for Scalable and Sparse Tensor Networks (CROSS)",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Damian Dechev",
 "awd_eff_date": "2022-10-01",
 "awd_exp_date": "2024-02-29",
 "tot_intn_awd_amt": 62500.0,
 "awd_amount": 62500.0,
 "awd_min_amd_letter_date": "2022-06-29",
 "awd_max_amd_letter_date": "2022-06-29",
 "awd_abstract_narration": "High-dimensional data computation or analytics are gaining importance in many domains, such as quantum chemistry/physics, quantum circuit simulation, brain processing, social networks, healthcare and machine/deep learning, to name a few. Tensors, a representation of high-dimensional data, are playing an increasingly critical role, and so are tensor methods. Tensor decompositions or factorizations of low-dimensional data (three to five dimensions) have been extensively studied over the past years from a high-performance computing and also compiler and computer architecture angles for their computational core operations, while tensor networks targeting very high-dimensional data (over ten dimensions) and extracting physically meaningful latent variables are underdeveloped because of their complicated mathematical nature, extremely high computational complexity, and more domain-dependent challenges. The project\u2019s novelties are manifold: 1) memory heterogeneity-aware representations with algorithm and system optimizations, which could be adopted to solve other problems such as irregular applications and sparse numerical methods; 2) hardware-software co-design of specialized, sparse-tensor network-accelerator architectures, that are among the first hardware implementations of sparse-tensor networks. The project\u2019s impacts are 1) advancing state-of-the-art tensor decomposition studies to model true higher-order and sparse data; 2) triggering a closer long-term collaboration ranging from academia to research labs to industry by studying solicitous applications; 3) bringing appropriate educational opportunities.\r\n\r\nThis project proposes Cross-layer cooRdination and Optimization for Scalable and Sparse-Tensor Networks (CROSS) for heterogeneous systems that are equipped with various types of accelerators, such as GPUs, TPUs and FPGAs, as well as heterogeneous memories with dynamic and non-volatile random-access memories (DRAM+NVRAM). This research aims to study the sparsity in widely used tensor networks by introducing constraints, regularization, dictionaries, and/or domain knowledge for better data compression, faster computation, lower memory usage and better interpretability. Besides the sparsity challenges, sparse-tensor networks also suffer from the curse of dimensionality, aggravated data randomness and irregular program and memory access behaviors. This planning project conducts preliminary research that aims to address these challenges from four perspectives: (1) memory heterogeneity-aware representations and data (re-)arrangement, (2) balanced sparse tensor contraction (SpTC) algorithms with smart page arrangement, (3) memoization and intelligent allocation to reduce computational cost, and (4) specialized accelerator architectures for sparse-tensor networks. The optimized sparse tensor networks will encompass efforts from high-performance computing, algorithms, compilers, computer architecture and performance modeling and will be tested under multiple application scenarios.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dong",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dong Li",
   "pi_email_addr": "dli35@ucmerced.edu",
   "nsf_id": "000690232",
   "pi_start_date": "2022-06-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California - Merced",
  "inst_street_address": "5200 N LAKE RD",
  "inst_street_address_2": "",
  "inst_city_name": "MERCED",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2092012039",
  "inst_zip_code": "953435001",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "CA13",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, MERCED",
  "org_prnt_uei_num": "",
  "org_uei_num": "FFM7VPAG8P92"
 },
 "perf_inst": {
  "perf_inst_name": "University of California, Merced",
  "perf_str_addr": "1346 Luke Dr.",
  "perf_city_name": "Merced",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "953435001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "CA13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 62500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The tensor network targets very high-dimensional data (over ten dimensions) and extracts physically meaningful latent variables. The study on the tensor network is limited because of its complicated mathematical nature, extremely high computational complexity, and more domain-dependent challenges. In this project, we use an approach of cross-layer coordination and optimization for scalable and sparse-tensor networks for heterogeneous systems equipped with various types of accelerators, such as GPUs, TPUs and FPGAs, as well as heterogeneous memories with various memory components.&nbsp;This project is a planning work for the next step of a large PPoSS proposal.</p>\n<p>&nbsp;</p>\n<p>The outcome of this project is two folds. (1) We performed preliminary study on using memoization and intelligent allocation to reduce computational cost for tensor network. (2) We performed preliminary study on memory heterogeneity-aware representations and data (re-)arrangement. Our study used large transformer-based models as an example, and revealed the opportunities of applying memoization to tensor networks. Such opportunities are quantified by looking at the similarity of attention probability matrix across input sequences to the large transformer models. Our study also identified the limitations of current operating systems to support memory-consuming tensor networks on big heterogeneous memory systems. Such limitations root in the inability of the current systems to support accurate and low-overhead memory profiling and lightweight page migration.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/01/2024<br>\nModified by: Dong&nbsp;Li</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe tensor network targets very high-dimensional data (over ten dimensions) and extracts physically meaningful latent variables. The study on the tensor network is limited because of its complicated mathematical nature, extremely high computational complexity, and more domain-dependent challenges. In this project, we use an approach of cross-layer coordination and optimization for scalable and sparse-tensor networks for heterogeneous systems equipped with various types of accelerators, such as GPUs, TPUs and FPGAs, as well as heterogeneous memories with various memory components.This project is a planning work for the next step of a large PPoSS proposal.\n\n\n\n\n\nThe outcome of this project is two folds. (1) We performed preliminary study on using memoization and intelligent allocation to reduce computational cost for tensor network. (2) We performed preliminary study on memory heterogeneity-aware representations and data (re-)arrangement. Our study used large transformer-based models as an example, and revealed the opportunities of applying memoization to tensor networks. Such opportunities are quantified by looking at the similarity of attention probability matrix across input sequences to the large transformer models. Our study also identified the limitations of current operating systems to support memory-consuming tensor networks on big heterogeneous memory systems. Such limitations root in the inability of the current systems to support accurate and low-overhead memory profiling and lightweight page migration.\n\n\n\t\t\t\t\tLast Modified: 07/01/2024\n\n\t\t\t\t\tSubmitted by: DongLi\n"
 }
}