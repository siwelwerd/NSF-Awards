{
 "awd_id": "2151022",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: EAGER: Real-time Strategies and Synchronized Time Distribution Mechanisms for Enhanced Exascale Performance-Portability and Predictability",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2022-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 75000.0,
 "awd_amount": 75000.0,
 "awd_min_amd_letter_date": "2022-05-20",
 "awd_max_amd_letter_date": "2022-05-20",
 "awd_abstract_narration": "Advances throughout science and engineering have for several decades been driven by High Performance Computing (HPC), with the pace of discovery accelerating in concert with continued innovation in computing capabilities. But as semiconductor technology now faces fundamental physical limits, even while large-scale systems are reaching warehouse scales, new approaches are becoming essential to achieving efficient use of computing resources. In particular, given this divergence of scales, HPC systems have necessarily become more distributed and  asynchronous (in the sense that system clocks are asynchronous), resulting in increasingly variable and unpredictable execution. While these effects are recognized as critical hindrances to HPC performance, the mechanisms are not yet fully understood. What is known, however, is that much HPC infrastructure is tasked with dealing with inefficiency derived from asynchrony, variability, and unpredictability, leading to a deep and complex hardware/software support stack. The project team's hypothesis is that while each stack element provides a local solution, it may also exacerbate the global problem: that complexity has resulted in more variability, not less, and made determining its causes more difficult. This project explores the possibility of reversing the trend of ever-increasing complexity by removing and simplifying support layers. This strategy\u2019s achievable gains remain limited, however, while the underlying cause, execution asynchrony, remains unaddressed. The approach begins by leveraging recently developed technology that enables clocks to remain extremely accurate even when distributed on a planetary scale. Such accurate, distributed clocks serve to underpin a virtuous cycle where synchrony establishes baseline predictability, which, in turn, reduces variability, and at each stage of the cycle enables reduction in the complexity of the support stack. A benefit of this approach is that the individual steps are largely simple and can be applied directly to existing software systems. \r\n\r\nThis one-year project aims to obtain early findings and practical demonstrations for the importance of synchrony and predictability to increase HPC compute efficiency and thereby improve large-scale program execution. Five tasks are conducted. The first is to demonstrate the feasibility of accurate clock distribution by augmenting existing HPC network infrastructure. The second is to demonstrate the application of synchrony in the establishing a virtuous cycle enabling  simplifications to the software/system support stack. The third is to devise mechanisms to model, measure, and validate systems using the proposed methods. The fourth is to investigate the relative benefits of applying the synchrony-based virtuous cycle with respect to various application classes. The fifth is to demonstrate the overall efficacy of the proposed approach through a case study involving a production application. Overall, the project works to  determine whether added synchronization through accurate clocks enables significant improvements to HPC computations in terms of how efficiently they use computational resources.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Amanda",
   "pi_last_name": "Bienz",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Amanda Bienz",
   "pi_email_addr": "bienz@unm.edu",
   "nsf_id": "000848609",
   "pi_start_date": "2022-05-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of New Mexico",
  "inst_street_address": "1 UNIVERSITY OF NEW MEXICO",
  "inst_street_address_2": "",
  "inst_city_name": "ALBUQUERQUE",
  "inst_state_code": "NM",
  "inst_state_name": "New Mexico",
  "inst_phone_num": "5052774186",
  "inst_zip_code": "871310001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NM01",
  "org_lgl_bus_name": "UNIVERSITY OF NEW MEXICO",
  "org_prnt_uei_num": "",
  "org_uei_num": "F6XLTRUQJEN4"
 },
 "perf_inst": {
  "perf_inst_name": "University of New Mexico",
  "perf_str_addr": "1700 Lomas Blvd. NE, Suite 2200",
  "perf_city_name": "Albuquerque",
  "perf_st_code": "NM",
  "perf_st_name": "New Mexico",
  "perf_zip_code": "871310001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NM01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 75000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The findings of this project show that parallel performance varies with computer, MPI implementation, and acquired computer partition. &nbsp;Further, for iterative solvers, performance varies with thespecific problem being solved. &nbsp;One significant cause of performance degradation and variation is the queue search cost. &nbsp;Irregular communication, widely used through numerical methods and sparse matrix operations, requires calling MPI_Isend and MPI_Irecv, or some variation thereof, for each message to be communicated. &nbsp;At scale, solvers and sparse matrix operations often require hundreds to thousands of messages per process. &nbsp;Each process calls MPI_Irecv for each of the hundreds or thousands of messages to be received. &nbsp;When a message arrives, the queue of posted receives is searched for a match. &nbsp;However, with large message counts, this queue search overhead is signficant. &nbsp;Further, the order in which messages arrive to a given process is dependent on many factors, including the specific partition acquired from a supercomputer and the given problem being solved. &nbsp;Processes that have a larger amount of work will communicate messages later. &nbsp;Further, messages from processes that are further away are likely to arrive later than those from neighboring processes.<br /><br />This project investigated methods for reordering receive queues for persistent to better match the order in which messages arrive. &nbsp;During the first iteration of communication, MPI_Probe is used to obtain the order in which messages arrive. &nbsp;The posted receives are then reordered to match the order in which they arrived during the first iteration. &nbsp;Supercomputer partitions and load balances are typically unchanging across iterations, and as a result, the order in which messages arrive during the first iteration is a postiive predictor of the ordering for future iterations. &nbsp;This research has been added to the open-source codebase MPI Advance behind the neighborhood collective API. &nbsp;The broader community can take advantage of the queue reorderings simply by linking with this library and replacing hand-rolled point to point communication with neighborhood collectives.</p><br>\n<p>\n Last Modified: 09/27/2024<br>\nModified by: Amanda&nbsp;Bienz</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe findings of this project show that parallel performance varies with computer, MPI implementation, and acquired computer partition. Further, for iterative solvers, performance varies with thespecific problem being solved. One significant cause of performance degradation and variation is the queue search cost. Irregular communication, widely used through numerical methods and sparse matrix operations, requires calling MPI_Isend and MPI_Irecv, or some variation thereof, for each message to be communicated. At scale, solvers and sparse matrix operations often require hundreds to thousands of messages per process. Each process calls MPI_Irecv for each of the hundreds or thousands of messages to be received. When a message arrives, the queue of posted receives is searched for a match. However, with large message counts, this queue search overhead is signficant. Further, the order in which messages arrive to a given process is dependent on many factors, including the specific partition acquired from a supercomputer and the given problem being solved. Processes that have a larger amount of work will communicate messages later. Further, messages from processes that are further away are likely to arrive later than those from neighboring processes.\n\nThis project investigated methods for reordering receive queues for persistent to better match the order in which messages arrive. During the first iteration of communication, MPI_Probe is used to obtain the order in which messages arrive. The posted receives are then reordered to match the order in which they arrived during the first iteration. Supercomputer partitions and load balances are typically unchanging across iterations, and as a result, the order in which messages arrive during the first iteration is a postiive predictor of the ordering for future iterations. This research has been added to the open-source codebase MPI Advance behind the neighborhood collective API. The broader community can take advantage of the queue reorderings simply by linking with this library and replacing hand-rolled point to point communication with neighborhood collectives.\t\t\t\t\tLast Modified: 09/27/2024\n\n\t\t\t\t\tSubmitted by: AmandaBienz\n"
 }
}