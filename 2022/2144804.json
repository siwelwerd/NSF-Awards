{
 "awd_id": "2144804",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Inferring Minimal but Sufficient Environment Models from Natural Language and Semantic Perception for Collaborative Robots in Dynamic Environments",
 "cfda_num": "47.041, 47.070",
 "org_code": "05010000",
 "po_phone": "7032922095",
 "po_email": "kwimmer@nsf.gov",
 "po_sign_block_name": "Karl Wimmer",
 "awd_eff_date": "2022-06-01",
 "awd_exp_date": "2027-05-31",
 "tot_intn_awd_amt": 513081.0,
 "awd_amount": 296965.0,
 "awd_min_amd_letter_date": "2022-05-20",
 "awd_max_amd_letter_date": "2024-06-25",
 "awd_abstract_narration": "Collaborative robots are poised to have a transformative impact on society. Significant advances in perception, planning, mapping, localization, and actuation over the past decade have accelerated the migration of robots from controlled laboratory environments to diverse applications in a complex and dynamic human world. The ability to effectively communicate has become increasingly important as robots transition from specialized applications performed in isolation to significant roles in human-robot teams.  While smart voice assistants have become prevalent in recent years due to advances in automatic speech recognition, those interactions are not conditioned on a model of the environment inferred from sensor observations and do not lead to the physical manipulation of their surroundings. Recent advances in grounded language communication for human-robot interaction, where speech and text both inform and guide a robot's understanding of and actions in the world, have transformed the limited space of concepts that robots and humans could exchange into rich representations that could be composed of many different objects, spatial relationships, planning constraints, and actions.  Point solutions with bespoke environment representations or models tailored to specific applications may exist, but no current approach is able to efficiently interpret the meaning of human instructions across many scenarios over non-trivial timescales.  Fundamentally new approaches to grounded language communication that fluidly reason about the past dynamics, present configuration and/or future state of objects from select observations are needed for collaborative robots to interpret linguistic interactions quickly and accurately in human-robot teams. This Faculty Early Career Development (CAREER) project seeks to revisit algorithms, models, and intelligence architectures for collaborative robots and reformulate how probabilistic graphical models are constructed and solved for efficient grounded language communication in environments with complex dynamics, non-trivial affordances, and rich semantic representations.\r\n\r\nThis research will investigate this topic by developing algorithms that address two critical technical gaps.  First, this research will develop algorithms that infer which past and/or present observations and/or future projections in time are needed to construct minimal but sufficient environment models for interpreting natural language.  Second, this research will develop algorithms that iteratively update their solutions based on minor modifications of the environment model and augmentation of symbolic representations from incrementally updating observations and environment projections.  These algorithms will enable a novel intelligence architecture for natural language symbol grounding that reasons about which observations and/or environment projections and what kinds of information are needed from these dynamic environment representations to enable scalable and capable models for grounded language communication with collaborative robots.  Experimental evaluation of these ideas will be performed on a variety of robot manipulators, field robots, and mobile manipulators in addition to purposefully built collaborative mobile robots that will be developed for teaching and outreach activities.\r\n\r\nThis project is supported by the cross-directorate Foundational Research in Robotics program, jointly managed and funded by the Directorates for Engineering (ENG) and Computer and Information Science and Engineering (CISE).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Howard",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas Howard",
   "pi_email_addr": "thomas.howard@rochester.edu",
   "nsf_id": "000681387",
   "pi_start_date": "2022-05-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "518 HYLAN, RC BOX 270140",
  "perf_city_name": "Rochester",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146270140",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "144Y00",
   "pgm_ele_name": "FRR-Foundationl Rsrch Robotics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "6840",
   "pgm_ref_txt": "ROBOTICS"
  },
  {
   "pgm_ref_code": "6856",
   "pgm_ref_txt": "ARTIFICIAL INTELL & COGNIT SCI"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002627DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 193812.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 103153.0
  }
 ],
 "por": null
}