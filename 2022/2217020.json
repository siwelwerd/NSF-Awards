{
 "awd_id": "2217020",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: PPoSS: Planning: Cross-layer Coordination and Optimization for Scalable and Sparse Tensor Networks (CROSS)",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Damian Dechev",
 "awd_eff_date": "2022-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 62500.0,
 "awd_amount": 62500.0,
 "awd_min_amd_letter_date": "2022-06-29",
 "awd_max_amd_letter_date": "2022-06-29",
 "awd_abstract_narration": "High-dimensional data computation or analytics are gaining importance in many domains, such as quantum chemistry/physics, quantum circuit simulation, brain processing, social networks, healthcare and machine/deep learning, to name a few. Tensors, a representation of high-dimensional data, are playing an increasingly critical role, and so are tensor methods. Tensor decompositions or factorizations of low-dimensional data (three to five dimensions) have been extensively studied over the past years from a high-performance computing and also compiler and computer architecture angles for their computational core operations, while tensor networks targeting very high-dimensional data (over ten dimensions) and extracting physically meaningful latent variables are underdeveloped because of their complicated mathematical nature, extremely high computational complexity, and more domain-dependent challenges. The project\u2019s novelties are manifold: 1) memory heterogeneity-aware representations with algorithm and system optimizations, which could be adopted to solve other problems such as irregular applications and sparse numerical methods; 2) hardware-software co-design of specialized, sparse-tensor network-accelerator architectures, that are among the first hardware implementations of sparse-tensor networks. The project\u2019s impacts are 1) advancing state-of-the-art tensor decomposition studies to model true higher-order and sparse data; 2) triggering a closer long-term collaboration ranging from academia to research labs to industry by studying solicitous applications; 3) bringing appropriate educational opportunities.\r\n\r\nThis project proposes Cross-layer cooRdination and Optimization for Scalable and Sparse-Tensor Networks (CROSS) for heterogeneous systems that are equipped with various types of accelerators, such as GPUs, TPUs and FPGAs, as well as heterogeneous memories with dynamic and non-volatile random-access memories (DRAM+NVRAM). This research aims to study the sparsity in widely used tensor networks by introducing constraints, regularization, dictionaries, and/or domain knowledge for better data compression, faster computation, lower memory usage and better interpretability. Besides the sparsity challenges, sparse-tensor networks also suffer from the curse of dimensionality, aggravated data randomness and irregular program and memory access behaviors. This planning project conducts preliminary research that aims to address these challenges from four perspectives: (1) memory heterogeneity-aware representations and data (re-)arrangement, (2) balanced sparse tensor contraction (SpTC) algorithms with smart page arrangement, (3) memoization and intelligent allocation to reduce computational cost, and (4) specialized accelerator architectures for sparse-tensor networks. The optimized sparse tensor networks will encompass efforts from high-performance computing, algorithms, compilers, computer architecture and performance modeling and will be tested under multiple application scenarios.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Mueller",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Frank Mueller",
   "pi_email_addr": "fmuelle@ncsu.edu",
   "nsf_id": "000484031",
   "pi_start_date": "2022-06-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "890 Oval Dr.",
  "perf_city_name": "Raleigh",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276958206",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 62500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The work carried out under this planning grant contributed to the<br />advancement of computational techniques addressing sparse tensor<br />network problems, including irregular applications and sparse<br />numerical methods. Tensor computations are the backbone of machine<br />learning, and contemporary methods are increasingly investigating<br />sparse data representations instead of the dominant dense ones in<br />order to reduce computational cost. To this end, the project provided<br />an initial study of sparsity in widely-used tensor networks suitable<br />for introducing constraints, regularization, dictionary, and domain<br />knowledge for better data compression, faster computation, lower<br />memory storage, along with better interpretability.<br /><br />This to end, we developed a new sparse data structure that exploits<br />the found sparsity pattern to improve memory requirements and compute<br />requirements for operations like contraction/decomposition. This data<br />representation, called DiaQ, is specifically designed to speed up<br />quantum simulations on classical hardware. DiaQ stores all diagonals<br />as a hash map and allows the simulation of very wide quantum circuits<br />with memory demands that grow at a significantly slower rate than dense<br />ones. We developed a numerical library for DiaQ with support for<br />fundamental BLAS (matrix/vector and matrix/matrix) operations,<br />including support for higher order tensors.<br />We tested our software artifact with the SupermarQ benchmark suite and<br />found that multi-threading helped reduce the time taken to format convert by<br />~65%. We further implemented state-vector simulation enabling kernels for DiaQ<br />matrices and integrated it into NWQ-Sim's state vector simulation<br />framework. By adding Open-MP multi-threading and AVX vectorization, we<br />observe ~50% speedups for the GHZ benchmark using 28 qubits. The work<br />has resulted in an M.S. thesis and a paper draft prepared for<br />submission. A software release will follow as part of the full<br />project following the initial planning project reported on here.</p><br>\n<p>\n Last Modified: 11/15/2024<br>\nModified by: Frank&nbsp;Mueller</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe work carried out under this planning grant contributed to the\nadvancement of computational techniques addressing sparse tensor\nnetwork problems, including irregular applications and sparse\nnumerical methods. Tensor computations are the backbone of machine\nlearning, and contemporary methods are increasingly investigating\nsparse data representations instead of the dominant dense ones in\norder to reduce computational cost. To this end, the project provided\nan initial study of sparsity in widely-used tensor networks suitable\nfor introducing constraints, regularization, dictionary, and domain\nknowledge for better data compression, faster computation, lower\nmemory storage, along with better interpretability.\n\nThis to end, we developed a new sparse data structure that exploits\nthe found sparsity pattern to improve memory requirements and compute\nrequirements for operations like contraction/decomposition. This data\nrepresentation, called DiaQ, is specifically designed to speed up\nquantum simulations on classical hardware. DiaQ stores all diagonals\nas a hash map and allows the simulation of very wide quantum circuits\nwith memory demands that grow at a significantly slower rate than dense\nones. We developed a numerical library for DiaQ with support for\nfundamental BLAS (matrix/vector and matrix/matrix) operations,\nincluding support for higher order tensors.\nWe tested our software artifact with the SupermarQ benchmark suite and\nfound that multi-threading helped reduce the time taken to format convert by\n~65%. We further implemented state-vector simulation enabling kernels for DiaQ\nmatrices and integrated it into NWQ-Sim's state vector simulation\nframework. By adding Open-MP multi-threading and AVX vectorization, we\nobserve ~50% speedups for the GHZ benchmark using 28 qubits. The work\nhas resulted in an M.S. thesis and a paper draft prepared for\nsubmission. A software release will follow as part of the full\nproject following the initial planning project reported on here.\t\t\t\t\tLast Modified: 11/15/2024\n\n\t\t\t\t\tSubmitted by: FrankMueller\n"
 }
}