{
 "awd_id": "2132294",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Post-training deep neural networks certification against backdoor data poisoning attacks",
 "cfda_num": "47.041, 47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2022-04-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 255391.0,
 "awd_amount": 255391.0,
 "awd_min_amd_letter_date": "2022-02-09",
 "awd_max_amd_letter_date": "2022-06-21",
 "awd_abstract_narration": "The broader impact of this Small Business Innovation Research (SBIR) Phase I project will be to secure and certify deep learning models that are becoming ubiquitous in many safety and security-sensitive applications, such as finance, health, military/intelligence, cyber security, critical infrastructure, and personal/consumer use.  Strong growth in deep neural network (DNN) deployments is forecast in the near term in several of these domains, some of which are subject to regulatory requirements that artificial intelligence (AI) models be certified to perform as advertised.  This project proposes a new method to confidently certify against backdoor attacks. \r\n\r\nThis Small Business Innovation Research Phase I project will provide the first commercial prototype of a mathematically principled certification service for DNNs against evasive backdoor attacks (BAs). The proposed method is wholly unsupervised, requiring no known examples of poisoned DNNs nor any samples from the (possibly poisoned) training set. This project will advance a broadly applicable and computationally efficient approach through parallel computation leveraging cost-effective cloud-computing services, to address challenges such as very large input feature space dimensions and number of classes, as well as very large DNNs. Another challenge is to make the detector insensitive to the mechanism (e.g., additive, multiplicative) by which the backdoor pattern (BP) is incorporated into a sample across different application domains. In addition to \"static\" DNNs and image domains, the prototype will be able to: process recurrent DNNs; handle time series, point cloud, and document data domains; and defend AIs used for time-series prediction and regression. APIs will be developed to expand the prototype to defend against related attacks, e.g., backdoor patterns that are perceptible but \"scene plausible\" or test-time evasion attacks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Miller",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "David J Miller",
   "pi_email_addr": "djmiller@engr.psu.edu",
   "nsf_id": "000205937",
   "pi_start_date": "2022-02-09",
   "pi_end_date": "2022-03-30"
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "George",
   "pi_last_name": "Kesidis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "George Kesidis",
   "pi_email_addr": "gik2@psu.edu",
   "nsf_id": "000490250",
   "pi_start_date": "2022-03-30",
   "pi_end_date": "2022-06-21"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xi",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xi Li",
   "pi_email_addr": "lee.xi.1994@gmail.com",
   "nsf_id": "000882048",
   "pi_start_date": "2022-06-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Anomalee Incorporated",
  "inst_street_address": "692 TANAGER DR",
  "inst_street_address_2": "",
  "inst_city_name": "STATE COLLEGE",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8142381516",
  "inst_zip_code": "168032503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "ANOMALEE INC",
  "org_prnt_uei_num": "",
  "org_uei_num": "FG47SQJG18Q7"
 },
 "perf_inst": {
  "perf_inst_name": "Anomalee Incorporated",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168032503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8027",
   "pgm_ref_txt": "Cyber Secur - Cyberinfrastruc"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 255391.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Artificial Intelligences (AIs), also known as Deep Neural Networks (DNNs), are increasingly being used for applications which may have safety and security concerns, e.g., for purposes of cyber (including network) security, or for quality control and monitoring of manufacturing systems. AIs have an enormous number of parameters that need to be (deeply) learned from a commensurately enormous training dataset. The aim of this project was to secure AIs against backdoor poisoning to their training dataset (Trojans) and to make them generally more robust. Such threats are possible because of insecure outsourcing or inside attacks when formulating the training dataset or conducting deep learning.</p>\n<p><br />A main outcome of this project was the development of two proprietary, state-of-the-art methods for detection of subtle backdoors in AI classifiers: one is classification-margin based and clips neural activations (see the figure), and the other is reverse-engineering (RE) based and modifies the \"normalization\" parameters. The intellectual merit of these outcomes are multifold: they address a wide variety of backdoor attack patterns and methods of incorporation; they are based on mathematically verified assumptions (e.g., that the backdoor pattern is overfit to the attack's target class); they computationally scale to very large AIs; they make minimal assumptions regarding the attacks and do not have hyper parameters to set (they are genuinely unsupervised); their superior performance was confirmed through extensive experimentation (including 2nd place at IEEE TRC'22 competition by a preliminary version of the activation clipping method); they can be embodied in easy to use software packages. These developed methods were extensively tested against different threat models for different application domains (e.g., image classification, speech-to-text) and different AI architectures.</p>\n<p><br />The RE method produces an estimate of the backdoor pattern itself (if one is present) which can be used by the defender to help determine whether the backdoor is natural or maliciously planted. The current version of the activation clipping method has significantly improved performance with no additional computational cost. Considering how the backdoor is overfit to the target class, a motivation behind the activation clipping method is general principle that security of a system is improved when unnecessary functionality is removed (so as to minimize its attack surface).&nbsp;</p>\n<p>Motivated by the I-Corps bootcamp industrial-outreach experience, we \"partially pivoted\" to also explore the usefulness of such adversarial AI techniques to make AIs more robust to \"natural\" failings in the training dataset or deep learning process. Also, we explored out-of-distribution detection (OODD) at test/operation time and how best to adapt the AI to test-samples in response to detected operational conditions that are not well-represented in (anomalous with respect to) the training dataset. As these methods rely on internal (embedded) activations, they can be applied (have broader impact) to a wide variety of domains which employ similar AI architectures.</p>\n<p><br />After filing provisional patents, we disseminate our developed methods including through a new text book, \"Adversarial Learning and Secure AI\", to be published by Cambridge University Press in Fall 2023, with accompanying slideware and software for some of the projects. &nbsp;We've also distributed the software of an earlier version of our classification-margin based backdoor mitigation method on GitHub for non-profit, educational use.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/28/2023<br>\n\t\t\t\t\tModified by: Xi&nbsp;Li</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2132294/2132294_10784473_1687693438280_UnivBM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2132294/2132294_10784473_1687693438280_UnivBM--rgov-800width.jpg\" title=\"Backdoor Mitigation by Activation Clipping\"><img src=\"/por/images/Reports/POR/2023/2132294/2132294_10784473_1687693438280_UnivBM--rgov-66x44.jpg\" alt=\"Backdoor Mitigation by Activation Clipping\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Activation Clipping is a post-training (without access to the training dataset), backdoor mitigation method that uses a small clean dataset and access to the potentially backdoor poisoned AI.  Here, the activations of ReLU neurons are bounded (clipped) based on the clean data activations.</div>\n<div class=\"imageCredit\">Anomalee Inc.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Xi&nbsp;Li</div>\n<div class=\"imageTitle\">Backdoor Mitigation by Activation Clipping</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nArtificial Intelligences (AIs), also known as Deep Neural Networks (DNNs), are increasingly being used for applications which may have safety and security concerns, e.g., for purposes of cyber (including network) security, or for quality control and monitoring of manufacturing systems. AIs have an enormous number of parameters that need to be (deeply) learned from a commensurately enormous training dataset. The aim of this project was to secure AIs against backdoor poisoning to their training dataset (Trojans) and to make them generally more robust. Such threats are possible because of insecure outsourcing or inside attacks when formulating the training dataset or conducting deep learning.\n\n\nA main outcome of this project was the development of two proprietary, state-of-the-art methods for detection of subtle backdoors in AI classifiers: one is classification-margin based and clips neural activations (see the figure), and the other is reverse-engineering (RE) based and modifies the \"normalization\" parameters. The intellectual merit of these outcomes are multifold: they address a wide variety of backdoor attack patterns and methods of incorporation; they are based on mathematically verified assumptions (e.g., that the backdoor pattern is overfit to the attack's target class); they computationally scale to very large AIs; they make minimal assumptions regarding the attacks and do not have hyper parameters to set (they are genuinely unsupervised); their superior performance was confirmed through extensive experimentation (including 2nd place at IEEE TRC'22 competition by a preliminary version of the activation clipping method); they can be embodied in easy to use software packages. These developed methods were extensively tested against different threat models for different application domains (e.g., image classification, speech-to-text) and different AI architectures.\n\n\nThe RE method produces an estimate of the backdoor pattern itself (if one is present) which can be used by the defender to help determine whether the backdoor is natural or maliciously planted. The current version of the activation clipping method has significantly improved performance with no additional computational cost. Considering how the backdoor is overfit to the target class, a motivation behind the activation clipping method is general principle that security of a system is improved when unnecessary functionality is removed (so as to minimize its attack surface). \n\nMotivated by the I-Corps bootcamp industrial-outreach experience, we \"partially pivoted\" to also explore the usefulness of such adversarial AI techniques to make AIs more robust to \"natural\" failings in the training dataset or deep learning process. Also, we explored out-of-distribution detection (OODD) at test/operation time and how best to adapt the AI to test-samples in response to detected operational conditions that are not well-represented in (anomalous with respect to) the training dataset. As these methods rely on internal (embedded) activations, they can be applied (have broader impact) to a wide variety of domains which employ similar AI architectures.\n\n\nAfter filing provisional patents, we disseminate our developed methods including through a new text book, \"Adversarial Learning and Secure AI\", to be published by Cambridge University Press in Fall 2023, with accompanying slideware and software for some of the projects.  We've also distributed the software of an earlier version of our classification-margin based backdoor mitigation method on GitHub for non-profit, educational use. \n\n\t\t\t\t\tLast Modified: 06/28/2023\n\n\t\t\t\t\tSubmitted by: Xi Li"
 }
}