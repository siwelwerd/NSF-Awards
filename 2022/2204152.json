{
 "awd_id": "2204152",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Fellowship Award",
 "awd_titl_txt": "Investigating inductive biases for language acquisition with meta-learning",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": "7032927376",
 "po_email": "jwmirand@nsf.gov",
 "po_sign_block_name": "Josie Welkom Miranda",
 "awd_eff_date": "2022-08-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 138000.0,
 "awd_amount": 97750.0,
 "awd_min_amd_letter_date": "2022-08-04",
 "awd_max_amd_letter_date": "2024-07-26",
 "awd_abstract_narration": "This award was provided as part of NSF's Social, Behavioral and Economic Sciences (SBE) Postdoctoral Research Fellowships (SPRF) program and SBE's Linguistics program. The goal of the SPRF program is to prepare promising, early career doctoral-level scientists for scientific careers in academia, industry or private sector, and government. SPRF awards involve two years of training under the sponsorship of established scientists and encourage Postdoctoral Fellows to perform independent research. NSF seeks to promote the participation of scientists from all segments of the scientific community, including those from underrepresented groups, in its research programs and activities; the postdoctoral period is considered to be an important level of professional development in attaining this goal. Each Postdoctoral Fellow must address important scientific questions that advance their respective disciplinary fields. Under the sponsorship of Dr. Thomas L. Griffiths at Princeton University, this postdoctoral fellowship award supports an early career scientist investigating how children can acquire language so rapidly and how we can create artificial intelligence systems with similarly impressive learning abilities. Language acquisition involves a complex interplay between the data and the learner. Suppose a learner has been told that a green triangle is an example of a \u201cdax.\u201d A learner preferring shape-based generalizations would conclude that \u201cdax\u201d means \u201ctriangle,\u201d while a learner preferring color-based generalizations would conclude that \u201cdax\u201d means \u201cgreen object.\u201d The properties of the learner that guide generalization are called inductive biases. It is not clear which inductive biases enable people to acquire languages so rapidly. The proposed research will investigate this topic using computational modeling. By creating different computational systems that have different inductive biases, we will test which inductive biases yield the most human-like learning, allowing us to determine what factors guide language acquisition in humans. In addition to improving our understanding of how humans learn, this research will also have implications for artificial intelligence (AI). AI plays an increasingly large role in public and private life, but current AI systems replicate harmful prejudices from their training data. Our approach provides a way to control the inductive biases of AI systems, which could be used to discourage harmful generalization patterns that current models display.\r\n\r\nThis project presents a novel computational approach for modeling inductive biases and how they interact with data, and it then uses this approach in classic settings where inductive biases are debated. Our framework is based on meta-learning, a machine learning technique in which a model is shown a variety of tasks from which it automatically finds an inductive bias that enables it to learn new tasks more easily. In our application of meta-learning, we control the space of tasks, thereby controlling the inductive bias that is imparted via meta-learning. We use this approach to create models which instantiate a range of hypothesized inductive biases so that we can analyze which biases best explain human learning behavior. We evaluate this approach in a series of case studies in language acquisition, focusing on the acquisition of question formation in English and subject-verb agreement across languages. For each case study, we analyze inductive biases ranging in specificity from a general-purpose preference for simplicity to detailed innate knowledge of the structure of language, motivated by longstanding questions about whether children need extensive innate knowledge to acquire language. We define the inductive biases of interest using probabilistic models and then use meta-learning to instantiate those biases in neural networks. Our approach therefore combines the complementary strengths of two major approaches to computational cognitive science: the controllability of probabilistic models and the learning power of neural networks. All of our experiments involve the creation of useful resources (code, datasets, and trained models) that we will make publicly available to facilitate further research into language learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "McCoy",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Richard T McCoy",
   "pi_email_addr": "",
   "nsf_id": "000867888",
   "pi_start_date": "2022-08-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Griffiths",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas L Griffiths",
   "pi_email_addr": "",
   "nsf_id": "000488584",
   "pi_start_date": "2022-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "McCoy, Richard T",
  "inst_street_address": "",
  "inst_street_address_2": "",
  "inst_city_name": "Baltimore",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "",
  "inst_zip_code": "212182680",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "",
  "org_prnt_uei_num": "",
  "org_uei_num": ""
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": null,
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "040Y00",
   "pgm_ele_name": "(SPRF-FR) SBE Postdoctoral Res"
  },
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "102Z",
   "pgm_ref_txt": "COVID-Disproportionate Impcts Inst-Indiv"
  },
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7137",
   "pgm_ref_txt": "POSTDOCTORAL FELLOWSHIPS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 97749.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-b73a0279-7fff-79a2-3f08-f075c18c51af\"> </span></p>\n<p dir=\"ltr\"><span>One remarkable aspect of human cognition is the way that people make rich generalizations from limited experience. For example, a person can learn the meaning of a new word based on just one or two examples of that word used in context. A central challenge in cognitive science is explaining how people can learn so rapidly.&nbsp;</span></p>\n<p dir=\"ltr\"><span>In this project, we developed a new computational modeling approach for enabling machines to learn as rapidly and flexibly as humans do. This approach combines two existing schools of thought, called Bayesian models and neural networks. Bayesian models have previously been shown to be effective at capturing rapid learning, but practical difficulties make it challenging to apply these models to large-scale, naturalistic data. In contrast, neural networks, such as ChatGPT, are effective at handling naturalistic data, but they do not perform well at learning rapidly - they often consume thousands of times more data than humans encounter. The approach that we developed involves distilling the learning behaviors of a Bayesian model into a neural network to create a single system that has the strengths of both approaches: it can learn rapidly, like a Bayesian model, and it can also flexibly handle naturalistic data, like a neural network.</span></p>\n<p dir=\"ltr\"><span>As an example, we tested the ability of this system to learn formal linguistic patterns. An example of such a pattern is sequences of letters copied twice; for instance, ADBADB would fit this pattern (because it contains the sequence ADB repeated twice), but ADBADC would not fit the pattern (because its two halves are different). We found that a standard neural network required about one thousand examples to learn this pattern, but the new system that we created could learn it from just one hundred examples. This system's abilities are not just limited to synthetic patterns: it is also effective at learning aspects of English grammar after being trained on sentences of the sort that parents say to their young children, and it outperforms a standard neural network at learning some linguistic phenomena such as recursion.&nbsp;</span></p>\n<p dir=\"ltr\"><span>What do these results mean for how we understand the human mind? They suggest that human language learning involves the combination of two powerful types of cognitive mechanisms. On one hand, we use probabilistic inference to quickly determine the abstract rules that define the data we have observed (this ability is what the Bayesian component of our model contributes). At the same time, we also make use of flexible, graded learning that allows us to handle noise and complexity (this flexibility is what the neural network component of our model contributes). Overall, the results of this project provide a powerful new framework for understanding how humans and machines can learn from limited data.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/23/2024<br>\nModified by: Richard&nbsp;T&nbsp;Mccoy</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nOne remarkable aspect of human cognition is the way that people make rich generalizations from limited experience. For example, a person can learn the meaning of a new word based on just one or two examples of that word used in context. A central challenge in cognitive science is explaining how people can learn so rapidly.\n\n\nIn this project, we developed a new computational modeling approach for enabling machines to learn as rapidly and flexibly as humans do. This approach combines two existing schools of thought, called Bayesian models and neural networks. Bayesian models have previously been shown to be effective at capturing rapid learning, but practical difficulties make it challenging to apply these models to large-scale, naturalistic data. In contrast, neural networks, such as ChatGPT, are effective at handling naturalistic data, but they do not perform well at learning rapidly - they often consume thousands of times more data than humans encounter. The approach that we developed involves distilling the learning behaviors of a Bayesian model into a neural network to create a single system that has the strengths of both approaches: it can learn rapidly, like a Bayesian model, and it can also flexibly handle naturalistic data, like a neural network.\n\n\nAs an example, we tested the ability of this system to learn formal linguistic patterns. An example of such a pattern is sequences of letters copied twice; for instance, ADBADB would fit this pattern (because it contains the sequence ADB repeated twice), but ADBADC would not fit the pattern (because its two halves are different). We found that a standard neural network required about one thousand examples to learn this pattern, but the new system that we created could learn it from just one hundred examples. This system's abilities are not just limited to synthetic patterns: it is also effective at learning aspects of English grammar after being trained on sentences of the sort that parents say to their young children, and it outperforms a standard neural network at learning some linguistic phenomena such as recursion.\n\n\nWhat do these results mean for how we understand the human mind? They suggest that human language learning involves the combination of two powerful types of cognitive mechanisms. On one hand, we use probabilistic inference to quickly determine the abstract rules that define the data we have observed (this ability is what the Bayesian component of our model contributes). At the same time, we also make use of flexible, graded learning that allows us to handle noise and complexity (this flexibility is what the neural network component of our model contributes). Overall, the results of this project provide a powerful new framework for understanding how humans and machines can learn from limited data.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 04/23/2024\n\n\t\t\t\t\tSubmitted by: RichardTMccoy\n"
 }
}