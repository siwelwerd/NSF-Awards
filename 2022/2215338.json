{
 "awd_id": "2215338",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Doctoral Dissertation Research: Intonational Cues in Emotional Contexts",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927920",
 "po_email": "jvaldesk@nsf.gov",
 "po_sign_block_name": "Jorge Valdes Kroff",
 "awd_eff_date": "2022-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 10091.0,
 "awd_amount": 10091.0,
 "awd_min_amd_letter_date": "2022-07-07",
 "awd_max_amd_letter_date": "2022-07-07",
 "awd_abstract_narration": "The meaning conveyed through language depends not only on what is said--sounds, words, phrases--but also on how it is said--pitch, loudness, tempo, and other dimensions of intonation. In American English, distinct intonation patterns are used to convey meaning related to the discourse context of an utterance, distinguishing questions from assertions, the continuation of a speaker's turn in a dialogue, or marking emphasis on words that contribute new information. Decoding linguistic meaning from phonetic cues related to intonation is complicated by the fact that the same phonetic parameters are also integral to the speaker\u2019s expression of emotion. How do these two factors\u2014discourse meaning and speaker emotion\u2014interact in determining the intonational form of an utterance, and how do listeners disentangle intonational cues to linguistic meaning from cues to the speaker's emotional state? To answer this question, this project investigates variation in the phonetic encoding of discourse meaning based on speaker emotion, and how listeners use information about a speaker\u2019s emotional state to guide their interpretation of discourse meaning. Phase I of this project examines evidence from speech production experiments that elicit specific intonation patterns in sentences produced under different conditions of enacted emotion. Phase II tests how listeners' perception of speaker emotion affects how they process linguistic intonational categories and the interpretation of discourse meaning. The findings from this study will shed light on the interaction between linguistic context and speaker emotion in the production and perception of intonation. \r\n\r\nThis work adopts the Autosegmental-Metrical model of American English intonation, in which the phonetic implementation of intonation derives from phonological structures that group words into prosodic phrases, and tone features that mark prosodic phrase edges (boundary tones) and phrasal prominence (pitch accents). The investigation focuses on how enacted emotion affects the production of eight phrase-final intonation \u201ctunes\u201d (e.g., falling pitch or rising pitch) that have distinct pragmatic meaning. Tunes are elicited using an imitation paradigm. Participants hear a tune on a model sentence, and then produce the same tune on a new sentence presented in a discourse context congruent with the tune meaning, under four conditions of enacted emotion. The effect of emotion on acoustic correlates of intonation is modeled using Bayesian mixed-effects regression. A subset of recorded utterances from the production experiment are used as stimuli in a series of perception and comprehension experiments. Perceptual discrimination among tunes is tested as a function of enacted emotion using AXB discrimination, and a free classification task where listeners group utterances according to the speaker's inferred communicative goal (e.g., to express surprise, make an assertion). Comprehension of tune meaning is tested through a structured set of questions probing listeners\u2019 pragmatic judgments for each tune as a function of the enacted emotion. This project contributes to linguistic theories of intonational form and meaning, with new insights into speaker emotion as a source of variation in intonation production and perception, and contributes a new corpus of speech recordings, the English IntEX Corpus, for future research on intonation and emotion.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jennifer",
   "pi_last_name": "Cole",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Jennifer S Cole",
   "pi_email_addr": "jennifer.cole1@northwestern.edu",
   "nsf_id": "000383964",
   "pi_start_date": "2022-07-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Turner",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel R Turner",
   "pi_email_addr": "dturner@u.northwestern.edu",
   "nsf_id": "000823166",
   "pi_start_date": "2022-07-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "2016 Sheridan Road",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602084090",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "837400",
   "pgm_ele_name": "DDRI Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 10091.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Intonation is one of the main ways linguistic information is conveyed in speech, yet linking our understanding of the linguistic features that encode intonation to the physical expression of intonation in the speech signal has been a longstanding challenge. The challenge facing intonation researchers is consequential because it hampers our ability to evaluate, and thereby improve, our understanding of intonation in its cognitive and physical form and its function in linguistic communication. The root of the problem is the pervasive acoustic variation that emerges in the key correlates of intonation: tempo, intensity, and especially pitch. This project explored a novel method to account for acoustic variation by jointly modeling the contributions from the tonal specification of intonation (which conveys meaning distinctions between, for example, questions and statement) and the vocal expression of speaker emotion. Specifically, we tested the predictions from the current linguistic model of English intonation, the Autosegmental-Metrical (AM) model, which recognizes a system of distinct pitch trajectories, called intonational tunes, that convey distinctions in discourse-related meaning. Although not the scientific focus of the current project, emotion was also formalized using a validated model in psychology, allowing us to manipulate emotion along dimensions that are independent of a particular language or culture. The experiments for this project were organized into three phases corresponding to intonation in the production, perception, and interpretation of spoken American English.</p>\n<p>In production, we had participants (including a cohort of trained voice actors) give their version of a particular intonational tune while portraying a given emotion, and then we modeled the resulting pitch trajectories as a function of their tonal components (the tune label), emotion, and speaker. Results from modeling using k-means clustering and regression models for time-series data (GAMMs) confirm our hypothesis that the speaker&rsquo;s (enacted) emotion amplifies distinctions among the pitch patterns of the linguistic tunes they produce. The modeling results showed distinct pitch trajectories for nearly each of the eight tonally distinct tunes predicted by the AM model despite substantial effects of emotional portrayal on the realization of those tunes.</p>\n<p>In addition to being the basis of our acoustic analysis, select tune-emotion recordings were used as audio stimuli for subsequent perception and interpretation experiments. In perception, participants judged pairs of stimuli based on their perceived linguistic function while ignoring emotion, to test the perceptual distinctiveness of intonational tunes. Results from the perception experiments largely confirmed what we observed in production; listeners appear to be sensitive to the tune distinctions predicted by the linguistic model despite having to account for vocal cues of emotion that were also present in the speech signal. In the interpretation phase of this project, one experiment involved participants grouping recordings based on perceived linguistic function, ignoring emotion, without providing suggested meanings. The other experiment asked participants to rate how well a given interpretation fit a particular recording (a tune-emotion combination presented as an auditory stimulus), for interpretations drawn from published work on intonational meaning. Despite striking methodological differences, both experiments provided generally converging results, pointing to a highly robust intonational system that adapts to maintain informativity by preserving tonally specified, meaningful contrasts.</p>\n<p>This work has major implications for the development of theory and research methodology in linguistics and for the development of speech technology. Before this project it was assumed, but not confirmed, that intonational contrasts would be maintained in speech production across diverse emotional contexts that involve other, sometimes conflicting, acoustic cues. Given our results showing intonational tunes to be robust under different conditoins of speaker emotion, further development of the AM model seems justified.</p>\n<p>Research in this area has long been hampered by challenges in separating the linguistically informative features of the speech signal from other factors, but our project demonstrates a new way forward. Using an approach informed by linguistics and psychology, our research points to a separation of linguistic and emotional information in speech processing. According to our findings, acoustic variation is complex but structured, which opens the door to analyzing more naturalistic speech data in the future. As intonation researchers learn to decode the linguistic features underlying pitch patterns in the speech signal, despite variation due to psycho-social factors like emotion, we should be better able to identify a speaker&rsquo;s communicative intention from their expressive intonation. In addition, the findings from this project present new opportunities to exploit information from intonation in the development of interactive speech technologies, a likely next frontier for human-computer communication.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 08/02/2024<br>\nModified by: Jennifer&nbsp;S&nbsp;Cole</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIntonation is one of the main ways linguistic information is conveyed in speech, yet linking our understanding of the linguistic features that encode intonation to the physical expression of intonation in the speech signal has been a longstanding challenge. The challenge facing intonation researchers is consequential because it hampers our ability to evaluate, and thereby improve, our understanding of intonation in its cognitive and physical form and its function in linguistic communication. The root of the problem is the pervasive acoustic variation that emerges in the key correlates of intonation: tempo, intensity, and especially pitch. This project explored a novel method to account for acoustic variation by jointly modeling the contributions from the tonal specification of intonation (which conveys meaning distinctions between, for example, questions and statement) and the vocal expression of speaker emotion. Specifically, we tested the predictions from the current linguistic model of English intonation, the Autosegmental-Metrical (AM) model, which recognizes a system of distinct pitch trajectories, called intonational tunes, that convey distinctions in discourse-related meaning. Although not the scientific focus of the current project, emotion was also formalized using a validated model in psychology, allowing us to manipulate emotion along dimensions that are independent of a particular language or culture. The experiments for this project were organized into three phases corresponding to intonation in the production, perception, and interpretation of spoken American English.\n\n\nIn production, we had participants (including a cohort of trained voice actors) give their version of a particular intonational tune while portraying a given emotion, and then we modeled the resulting pitch trajectories as a function of their tonal components (the tune label), emotion, and speaker. Results from modeling using k-means clustering and regression models for time-series data (GAMMs) confirm our hypothesis that the speakers (enacted) emotion amplifies distinctions among the pitch patterns of the linguistic tunes they produce. The modeling results showed distinct pitch trajectories for nearly each of the eight tonally distinct tunes predicted by the AM model despite substantial effects of emotional portrayal on the realization of those tunes.\n\n\nIn addition to being the basis of our acoustic analysis, select tune-emotion recordings were used as audio stimuli for subsequent perception and interpretation experiments. In perception, participants judged pairs of stimuli based on their perceived linguistic function while ignoring emotion, to test the perceptual distinctiveness of intonational tunes. Results from the perception experiments largely confirmed what we observed in production; listeners appear to be sensitive to the tune distinctions predicted by the linguistic model despite having to account for vocal cues of emotion that were also present in the speech signal. In the interpretation phase of this project, one experiment involved participants grouping recordings based on perceived linguistic function, ignoring emotion, without providing suggested meanings. The other experiment asked participants to rate how well a given interpretation fit a particular recording (a tune-emotion combination presented as an auditory stimulus), for interpretations drawn from published work on intonational meaning. Despite striking methodological differences, both experiments provided generally converging results, pointing to a highly robust intonational system that adapts to maintain informativity by preserving tonally specified, meaningful contrasts.\n\n\nThis work has major implications for the development of theory and research methodology in linguistics and for the development of speech technology. Before this project it was assumed, but not confirmed, that intonational contrasts would be maintained in speech production across diverse emotional contexts that involve other, sometimes conflicting, acoustic cues. Given our results showing intonational tunes to be robust under different conditoins of speaker emotion, further development of the AM model seems justified.\n\n\nResearch in this area has long been hampered by challenges in separating the linguistically informative features of the speech signal from other factors, but our project demonstrates a new way forward. Using an approach informed by linguistics and psychology, our research points to a separation of linguistic and emotional information in speech processing. According to our findings, acoustic variation is complex but structured, which opens the door to analyzing more naturalistic speech data in the future. As intonation researchers learn to decode the linguistic features underlying pitch patterns in the speech signal, despite variation due to psycho-social factors like emotion, we should be better able to identify a speakers communicative intention from their expressive intonation. In addition, the findings from this project present new opportunities to exploit information from intonation in the development of interactive speech technologies, a likely next frontier for human-computer communication.\n\n\n\t\t\t\t\tLast Modified: 08/02/2024\n\n\t\t\t\t\tSubmitted by: JenniferSCole\n"
 }
}