{
 "awd_id": "2203399",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CCF: SHF: Small: Transformer synthesis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2022-02-01",
 "awd_exp_date": "2025-01-31",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2022-01-28",
 "awd_max_amd_letter_date": "2024-05-10",
 "awd_abstract_narration": "Just within four years of being first proposed, transformers have had a dramatic impact on the natural language processing (NLP) field and are also beginning to have an impact on other fields, such as computer vision. This success has largely been driven by large-scale pre-training datasets, increasing computational power, and robust training techniques. However, a major challenge that remains is efficient optimal transformer model synthesis for a specific task and set of user requirements. However, this is not easy to do since the design space of transformer models is vast.  This project addresses this challenge through the development of transformer-synthesis methodologies and tools.  Given the importance of transformers, such tools are likely to have a transformative impact on many application areas. The research will be disseminated to industry via tech transfer e.g., via open-source online distribution of source codes, summer internships, and by leveraging PIs involvement with local companies. Outreach and curriculum development plans will also be undertaken within the context of the proposed research.\r\n\r\nThere is currently no universal framework that can navigate the vast transformer hyperparameter design space. Previously proposed transformer models are homogeneous in terms of data flow through the network. Unfortunately, this leads to very suboptimal transformer architectures. This project expands the transformer design space to incorporate heterogeneous architectures that venture beyond self-attention by employing other operations like convolutions and linear transforms. It will also explore novel projection layers and positional encodings to make hidden sizes flexible across various transformer layers. It will use a dense embedding to capture model similarity to significantly enhance search efficiency. It will develop a heteroscedastic surrogate model to further speed up search. It will include operations that optimize long-range interactions for long input sequences. It will also explore skipped connections and block-level grow-and-prune synthesis to improve architectural search efficiency.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Niraj",
   "pi_last_name": "Jha",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Niraj K Jha",
   "pi_email_addr": "jha@princeton.edu",
   "nsf_id": "000123477",
   "pi_start_date": "2022-01-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "Off. of Research & Proj. Admin.",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 194635.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 199697.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 205668.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The invention of transformers in 2017 was a seminal event that changed the course of research and development in many important areas: natural language processing, computer vision, text-to-image/video generation, decision-making, analogies, etc. Thus, synthesis of efficient and accurate transformer architectures became key to making major headway on these applications.</p>\r\n<p>We have developed several frameworks for making transformers efficient. Accelerating the transformer is challenging due to its quadratic computational complexity. AccelTran, a novel accelerator architecture for transformers, substantially reduces the number of ineffectual operations. This improves the throughput of transformer inference. It also improves data reuse, thus enabling higher energy efficiency. Our DynaMo framework reduces language model inference times significantly by predicting multiple word tokens simultaneously.</p>\r\n<p>One of the side benefits of making transformers efficient is that they can be placed on edge devices, like smartphones, thus enabling brand new applications. One way to accomplish this goal is through token pruning.&nbsp; However, prior methods employ fine-tuning of the transformers in this context. This is computationally intensive. In our Zero-TPrune framework, we propose a very efficient fine-tuning-free method. It can prune large models at negligible computational cost. This work was extended to diffusion models in our AT-EDM framework. Diffusion models exhibit superior performance in generating high-quality and diverse images. However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models. Existing works mainly adopt a retraining process to enhance diffusion model efficiency. This is computationally expensive and not very scalable. AT-EDM leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining.</p>\r\n<p>Text-to-video generation has attracted a lot of interest in recent years, spurred by the introduction of Sora by OpenAI. The holy grail in this area is generation of a movie, given a movie script. Text-to-video generation enhances content creation but is highly computationally intensive since the computational cost of diffusion transformers employed in this context scales quadratically in the number of pixels. This makes minute-length video generation extremely expensive, limiting most existing models to generating videos of only 10-20 seconds length. Our LinGen framework makes a linear-complexity text-to-video generation possible. Its computational cost only scales linearly in the number of pixels. For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. Current video generation methods break down long videos into sequential generation of short video segments. We introduce a new paradigm called Video Interface Networks (VINs) that enables parallel inference of video chunks. This makes it possible for VINs to scale to long videos and learn their essential semantics.</p>\r\n<p>Large language models can solve diverse tasks from a handful of demonstrations. This implicit understanding of tasks suggests that their attention mechanisms over word tokens may play a role in analogical reasoning. In our Im-Promptu framework, we investigate whether analogical reasoning can enable in-context composition over composable elements of visual stimuli. Analogies are easy for humans, however, not so easy for artificial intelligence models. This framework takes a step in this direction.</p>\r\n<p>We humans are also good at counterfactual thinking &ndash; asking &ldquo;what if&rdquo; questions and imagining solutions to them. The Synthetic Control method has pioneered a class of powerful data-driven techniques to estimate the counterfactual reality of a unit from donor units. It has had wide applications ranging from medicine to sociology, policy, and economics. At its core, the technique involves a linear model fitted on the pre-intervention period that combines donor outcomes to yield the counterfactual. However, linearly combining spatial information at each time instance using time-agnostic weights fails to capture important inter-unit and intra-unit temporal contexts and complex nonlinear dynamics of real data. In our SCouT framework, we instead use local spatiotemporal information before the onset of the intervention as a promising way to estimate the counterfactual sequence using a transformer, obtaining state-of-the-art results. We demonstrate its use in personalized medical decision-making.</p>\r\n<p>Seven PhD students and several undergraduates were trained in this exciting area of research.&nbsp; Their work resulted in several journal and conference articles.&nbsp; The results were disseminated to the wider public through various invited talks. The students also did technology transfer through their summer internships at various companies.&nbsp; The frameworks were used in various course projects.&nbsp; Several tools/frameworks developed in this project have been deployed in various companies (Adobe, Meta, Google, Samsung).</p><br>\n<p>\n Last Modified: 02/01/2025<br>\nModified by: Niraj&nbsp;K&nbsp;Jha</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe invention of transformers in 2017 was a seminal event that changed the course of research and development in many important areas: natural language processing, computer vision, text-to-image/video generation, decision-making, analogies, etc. Thus, synthesis of efficient and accurate transformer architectures became key to making major headway on these applications.\r\n\n\nWe have developed several frameworks for making transformers efficient. Accelerating the transformer is challenging due to its quadratic computational complexity. AccelTran, a novel accelerator architecture for transformers, substantially reduces the number of ineffectual operations. This improves the throughput of transformer inference. It also improves data reuse, thus enabling higher energy efficiency. Our DynaMo framework reduces language model inference times significantly by predicting multiple word tokens simultaneously.\r\n\n\nOne of the side benefits of making transformers efficient is that they can be placed on edge devices, like smartphones, thus enabling brand new applications. One way to accomplish this goal is through token pruning. However, prior methods employ fine-tuning of the transformers in this context. This is computationally intensive. In our Zero-TPrune framework, we propose a very efficient fine-tuning-free method. It can prune large models at negligible computational cost. This work was extended to diffusion models in our AT-EDM framework. Diffusion models exhibit superior performance in generating high-quality and diverse images. However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models. Existing works mainly adopt a retraining process to enhance diffusion model efficiency. This is computationally expensive and not very scalable. AT-EDM leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining.\r\n\n\nText-to-video generation has attracted a lot of interest in recent years, spurred by the introduction of Sora by OpenAI. The holy grail in this area is generation of a movie, given a movie script. Text-to-video generation enhances content creation but is highly computationally intensive since the computational cost of diffusion transformers employed in this context scales quadratically in the number of pixels. This makes minute-length video generation extremely expensive, limiting most existing models to generating videos of only 10-20 seconds length. Our LinGen framework makes a linear-complexity text-to-video generation possible. Its computational cost only scales linearly in the number of pixels. For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. Current video generation methods break down long videos into sequential generation of short video segments. We introduce a new paradigm called Video Interface Networks (VINs) that enables parallel inference of video chunks. This makes it possible for VINs to scale to long videos and learn their essential semantics.\r\n\n\nLarge language models can solve diverse tasks from a handful of demonstrations. This implicit understanding of tasks suggests that their attention mechanisms over word tokens may play a role in analogical reasoning. In our Im-Promptu framework, we investigate whether analogical reasoning can enable in-context composition over composable elements of visual stimuli. Analogies are easy for humans, however, not so easy for artificial intelligence models. This framework takes a step in this direction.\r\n\n\nWe humans are also good at counterfactual thinking  asking what if questions and imagining solutions to them. The Synthetic Control method has pioneered a class of powerful data-driven techniques to estimate the counterfactual reality of a unit from donor units. It has had wide applications ranging from medicine to sociology, policy, and economics. At its core, the technique involves a linear model fitted on the pre-intervention period that combines donor outcomes to yield the counterfactual. However, linearly combining spatial information at each time instance using time-agnostic weights fails to capture important inter-unit and intra-unit temporal contexts and complex nonlinear dynamics of real data. In our SCouT framework, we instead use local spatiotemporal information before the onset of the intervention as a promising way to estimate the counterfactual sequence using a transformer, obtaining state-of-the-art results. We demonstrate its use in personalized medical decision-making.\r\n\n\nSeven PhD students and several undergraduates were trained in this exciting area of research. Their work resulted in several journal and conference articles. The results were disseminated to the wider public through various invited talks. The students also did technology transfer through their summer internships at various companies. The frameworks were used in various course projects. Several tools/frameworks developed in this project have been deployed in various companies (Adobe, Meta, Google, Samsung).\t\t\t\t\tLast Modified: 02/01/2025\n\n\t\t\t\t\tSubmitted by: NirajKJha\n"
 }
}