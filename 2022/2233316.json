{
 "awd_id": "2233316",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Medium: Data-Mediated Communication with Proximal Robots for Emergency Response",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 1194056.0,
 "awd_amount": 391993.0,
 "awd_min_amd_letter_date": "2022-07-27",
 "awd_max_amd_letter_date": "2022-07-27",
 "awd_abstract_narration": "Robots may augment emergency response teams by collecting information in environments that may be dangerous or inaccessible for human responders, such as in wildfire fighting, search and rescue, or hurricane response. For example, robots might collect critical visual, mapping, and environmental data to inform responders of conditions ahead that could improve their awareness of the operational environment. These data would assist in planning and re-planning courses of action and enhance in-the-field decision making. However, response teams currently have little ability to directly access robot-collected information in the field, despite its value for rapidly responding to local conditions, because current systems typically route the data through a central command post. This project's goal is to design systems that support more direct access and analysis for first responders while not imposing additional distractions or operational risks through using faulty data. Through collaboration with several local response groups, the project team will develop better understandings of responders' needs and concerns around robot-collected data, algorithms and visualizations that meet those needs using augmented reality technologies, and systems that integrate well with responders' actual work practices. The project will also develop a series of demonstrations, outreach activities, and technology challenges based on the project goals aimed at increasing public interest in science, including among high school students and underrepresented groups in computer science. \r\n\r\nOverall, this research will develop fundamental knowledge in robotics and visualization, leading to new methods and tools that enable responders to take advantage of robot-collected data while in the field. In particular, this project will explore how see-through augmented reality head-mounted displays (ARHMDs) might offer an intuitive and powerful medium for in situ analysis of robot-collected data through developing an ARHMD system that allows emergency responders to interact with robot-collected information in the contexts of where, when, and how that data was obtained. The team will conduct empirical studies to guide the design of system components that allow responders to actively analyze available data through interactive visualization, passively view digital traces and \"data drops\" left by robots as they collect information about the environment, and query specific information such as camera feeds on-demand. The team will also develop novel algorithms for 3D scene reconstruction and simultaneous location and mapping that will be useful for a broad variety of applications. Overall, the project will contribute empirical knowledge of how different factors of ARHMD visualizations influence data interpretation, novel algorithms for estimating, correcting, and sharing maps between intermittently-networked agents in the field, and information regarding how data from collocated robots can mediate human-robot interactions, particularly within the context of emergency response.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Szafir",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Szafir",
   "pi_email_addr": "daniel.szafir@cs.unc.edu",
   "nsf_id": "000705038",
   "pi_start_date": "2022-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275991350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 367993.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 24000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-2255bb97-7fff-2e87-a1e0-8ed950c6300c\"> </span></p>\n<p dir=\"ltr\"><span>Robots hold the potential to make broad and significant benefits to society. In this project, we investigated how robots might assist human emergency response teams, such as by helping collect information in environments that are dangerous or inaccessible for human responders or working alongside human responders in the field to collaboratively map areas and conduct search operations. Our main focus has been </span><span>developing fundamental knowledge in both robotics and visualization, leading to new methods and tools that enable responders to take advantage of robot-collected data. In particular, we explored how mixed reality technologies offer an intuitive and powerful medium for analysis and interaction with robot-collected data in the contexts of where, when, and how that data was obtained, </span><span>enabling users to leverage robot-collected data to enhance their awareness and decision making. Over the course of the funding period of this award, our team made advances that impact several disciplines, including autonomous perception, human-robot interaction, human-computer interaction, visualization, virtual, augmented, and mixed reality, and field robotics, which we summarize below.</span></p>\n<p dir=\"ltr\"><span>In the first year of our project, we developed new knowledge regarding the impact of avatar size for human-human communication in augmented reality and provided a systematic, historical review of past work integrating mixed reality and robotics that culminated in an online tool (</span><a href=\"https://vamhri.com/\"><span>https://vamhri.com/</span></a><span>) which enables researchers to better understand the landscape of existing research and identify areas for new development. We also developed new insights into how visual analytics might operate in the field and empirically-grounded guidelines for how visualization tools might employ mixed reality technologies to support rapid and accurate analyses. In the area of robotic perception, we developed a method for improving robot sensor fusion and map building as well as a novel method for estimating the noise properties of various sensors.</span></p>\n<p dir=\"ltr\"><span>In our second and third years, we began a process of prototyping and evaluating new mixed reality robot interfaces. As part of this effort, we continued to develop new algorithms to improve robot perception and localization capabilities and conducted several experiments to explore the design space of immersive analytics (IA), generating new knowledge of the unique affordances of IA beyond traditional desktop environments. We also began integrating research across the various components of this project. For example, one of our key outcomes during these years was the development of a framework</span><span> that enables robot interface designers to consider best-practices in data visualization techniques, linking the human-robot interaction and data visualization fields. Similarly, we began leveraging our advances in robot perception in developing immersive, real-time robot teleoperation and supervisory systems that facilitate data visualization and geospatial data analysis.</span></p>\n<p dir=\"ltr\"><span>In the final two years of our project, we explored elements that may affect end-user interactions with robots, such as how visualization techniques might improve user abilities to debug robotic systems and recover from errors. In addition, we completed the design, implementation, and evaluation of an end-to-end mixed reality system for robot-assisted emergency response operations that integrated all of the knowledge developed throughout the course of this research award. Our final system, termed a &ldquo;Cyber-Physical Control Room,&rdquo; showcases novel methods for visualizing robot mapping data and demonstrates a new robot interface design paradigm for combining the strengths of both ego- and exocentric robot perspectives rendered within a unified virtual environment. We conducted a large-scale field trial of our system in the context of a robot-assisted emergency response scenario and found that our Cyber-Physical Control Room interface improved completion time about 25% over a baseline interface that was representative of how such systems are currently designed, thus demonstrating an achievement of our major project goal of investigating how to effectively develop interfaces that enhance robot-assisted emergency response.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 05/29/2024<br>\nModified by: Daniel&nbsp;Szafir</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nRobots hold the potential to make broad and significant benefits to society. In this project, we investigated how robots might assist human emergency response teams, such as by helping collect information in environments that are dangerous or inaccessible for human responders or working alongside human responders in the field to collaboratively map areas and conduct search operations. Our main focus has been developing fundamental knowledge in both robotics and visualization, leading to new methods and tools that enable responders to take advantage of robot-collected data. In particular, we explored how mixed reality technologies offer an intuitive and powerful medium for analysis and interaction with robot-collected data in the contexts of where, when, and how that data was obtained, enabling users to leverage robot-collected data to enhance their awareness and decision making. Over the course of the funding period of this award, our team made advances that impact several disciplines, including autonomous perception, human-robot interaction, human-computer interaction, visualization, virtual, augmented, and mixed reality, and field robotics, which we summarize below.\n\n\nIn the first year of our project, we developed new knowledge regarding the impact of avatar size for human-human communication in augmented reality and provided a systematic, historical review of past work integrating mixed reality and robotics that culminated in an online tool (https://vamhri.com/) which enables researchers to better understand the landscape of existing research and identify areas for new development. We also developed new insights into how visual analytics might operate in the field and empirically-grounded guidelines for how visualization tools might employ mixed reality technologies to support rapid and accurate analyses. In the area of robotic perception, we developed a method for improving robot sensor fusion and map building as well as a novel method for estimating the noise properties of various sensors.\n\n\nIn our second and third years, we began a process of prototyping and evaluating new mixed reality robot interfaces. As part of this effort, we continued to develop new algorithms to improve robot perception and localization capabilities and conducted several experiments to explore the design space of immersive analytics (IA), generating new knowledge of the unique affordances of IA beyond traditional desktop environments. We also began integrating research across the various components of this project. For example, one of our key outcomes during these years was the development of a framework that enables robot interface designers to consider best-practices in data visualization techniques, linking the human-robot interaction and data visualization fields. Similarly, we began leveraging our advances in robot perception in developing immersive, real-time robot teleoperation and supervisory systems that facilitate data visualization and geospatial data analysis.\n\n\nIn the final two years of our project, we explored elements that may affect end-user interactions with robots, such as how visualization techniques might improve user abilities to debug robotic systems and recover from errors. In addition, we completed the design, implementation, and evaluation of an end-to-end mixed reality system for robot-assisted emergency response operations that integrated all of the knowledge developed throughout the course of this research award. Our final system, termed a Cyber-Physical Control Room, showcases novel methods for visualizing robot mapping data and demonstrates a new robot interface design paradigm for combining the strengths of both ego- and exocentric robot perspectives rendered within a unified virtual environment. We conducted a large-scale field trial of our system in the context of a robot-assisted emergency response scenario and found that our Cyber-Physical Control Room interface improved completion time about 25% over a baseline interface that was representative of how such systems are currently designed, thus demonstrating an achievement of our major project goal of investigating how to effectively develop interfaces that enhance robot-assisted emergency response.\n\n\n\t\t\t\t\tLast Modified: 05/29/2024\n\n\t\t\t\t\tSubmitted by: DanielSzafir\n"
 }
}