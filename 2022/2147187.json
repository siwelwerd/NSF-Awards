{
 "awd_id": "2147187",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FAI: A Normative Economic Approach to Fairness in AI",
 "cfda_num": "47.070, 47.075",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2022-03-01",
 "awd_exp_date": "2025-08-31",
 "tot_intn_awd_amt": 560345.0,
 "awd_amount": 560345.0,
 "awd_min_amd_letter_date": "2022-02-23",
 "awd_max_amd_letter_date": "2022-02-23",
 "awd_abstract_narration": "A vast body of work in algorithmic fairness is devoted to preventing artificial intelligence (AI) from exacerbating societal biases. The predominant viewpoints in this literature equates fairness with lack of bias or seeks to achieve some form of statistical parity between demographic groups. By contrast, this project pursues alternative approaches rooted in normative economics, the field that evaluates policies and programs by asking \"what should be\". The work is driven by two observations. First, fairness to individuals and groups can be realized according to people\u2019s preferences represented in the form of utility functions. Second, traditional notions of algorithmic fairness may be at odds with welfare (the overall utility of groups), including the welfare of those groups the fairness criteria intend to protect.  The goal of this project is to establish normative economic approaches as a central tool in the study of fairness in AI. Towards this end the team pursues two research questions.  First, can the perspective of normative economics be reconciled with existing approaches to fairness in AI? Second, how can normative economics be drawn upon to rethink what fairness in AI should be? The project will integrate theoretical and algorithmic advances into real systems used to inform refugee resettlement decisions. The system will be examined from a fairness viewpoint, with the goal of ultimately ensuring fairness guarantees and welfare.\r\n\r\nThe research plan includes two main directions based on previous work has shown that classifiers incorporating parity-based fairness criteria can be Pareto inefficient.  That is, the welfare of all groups\u2014including the protected group\u2014would be higher under a classifier that is less fair. In the first direction, the project extends this observation to non-convex problems and then from in-processing to post-processing bias mitigation. The planned research will also study the interaction between multiple policy makers and its impact on social goals such as fairness and welfare.  In the second direction, the project develops a new conceptualization in which classifiers are viewed as public resources or goods.  This work then draws on ideas from fair division, a long-established branch of normative economics that defines and applies rigorous notions of fairness, and on the specific notion of the core. To put this idea into practice, there are several challenges that must be addressed: conditions for the existence of classifiers in the core, algorithms for their computation, and generalization from a training set.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ariel",
   "pi_last_name": "Procaccia",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Ariel D Procaccia",
   "pi_email_addr": "arielpro@seas.harvard.edu",
   "nsf_id": "000596429",
   "pi_start_date": "2022-02-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yiling",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yiling Chen",
   "pi_email_addr": "yiling@seas.harvard.edu",
   "nsf_id": "000508042",
   "pi_start_date": "2022-02-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021380001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 560345.0
  }
 ],
 "por": null
}