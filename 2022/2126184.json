{
 "awd_id": "2126184",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Automated processing of images and video for wildlife conservation",
 "cfda_num": "47.041, 47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2022-02-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 255993.0,
 "awd_amount": 255993.0,
 "awd_min_amd_letter_date": "2022-02-09",
 "awd_max_amd_letter_date": "2022-02-09",
 "awd_abstract_narration": "The broader impact of this Small Business Innovation Research (SBIR) Phase I project is to revolutionize wildlife field work and research. Many wildlife professionals already understand the potential for using video as a sensor, but without commercial software to turn video into data, its vast benefits remain untapped. It is important to monitor environmental effects and the impact of urban growth; billions of dollars are spent annually understanding how construction and infrastructure projects are impacting species and habitats. The state of practice is to send technicians into the field for observation. In many situations using a camera system to monitoring for wildlife is not only safer and more effective, but also much less expensive that using human labor. This project will reduce the cost, decrease the timeline, and yield better outcomes for these important conservation efforts, allowing the existing biodiversity to be preserved.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project will develop a web-based software-as-a-service to detect, track, and classify wildlife from video and images captured in the field. There is substantial potential for video to provide better data to help wildlife biologists perform their work, but the main barrier is the complexity. Using video systems to study wildlife requires advanced multidisciplinary knowledge; appropriate camera technology, recording methods, advanced video processing techniques and machine learning algorithms.  There are currently no commercial-off-the-shelf hardware or software packages available for biologists. This project will reduce the complexity of obtaining quantitative data from video by developing commercial-off-the-shelf equipment packages, automated video processing software, and the advisory services needed to make this transformational technology more accessible to biologists in the field.  The video processing needed is the focus of the research; the goal is the create algorithms that automate object detection and classification from the video and allow non-experts to create an accurate machine learning classifier with minimal labeling requirements.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brogan",
   "pi_last_name": "Morton",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brogan Morton",
   "pi_email_addr": "brogan@wildlifeimagingsystems.com",
   "nsf_id": "000806365",
   "pi_start_date": "2022-02-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "WILDLIFE IMAGING SYSTEMS LLC",
  "inst_street_address": "328 MECHANICSVILLE RD",
  "inst_street_address_2": "",
  "inst_city_name": "HINESBURG",
  "inst_state_code": "VT",
  "inst_state_name": "Vermont",
  "inst_phone_num": "8023439889",
  "inst_zip_code": "054619152",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "VT00",
  "org_lgl_bus_name": "WILDLIFE IMAGING SYSTEMS LLC",
  "org_prnt_uei_num": "",
  "org_uei_num": "HG37VNH2YKN8"
 },
 "perf_inst": {
  "perf_inst_name": "WILDLIFE IMAGING SYSTEMS LLC",
  "perf_str_addr": "328 MECHANICSVILLE RD",
  "perf_city_name": "HINESBURG",
  "perf_st_code": "VT",
  "perf_st_name": "Vermont",
  "perf_zip_code": "054619152",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "VT00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7453",
   "pgm_ref_txt": "GRAPHICS & VISUALIZATION"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 255993.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>During this NSF Phase 1 Wildlife Imaging Systems explored ways to automate the analysis of the wildlife images &amp; videos using various techniques.&nbsp; The goal was to reduce the amount of manual review of the data (including labeling for machine learning models) and to increase the value of the information gleaned from the various media.</p>\n<p>We participated in the NSF-sponsored Beat-the-Odds Boot Camp for Phase 1 SBIR recipients. We had an incredibly productive time during the Boot Camp and got to know the market much better. We gathered some very important insights that made us focus our development efforts.</p>\n<p>The first boot camp insight was that there did not seem to be a large customer concern about the current situation with wildlife image sorting and classification. While people wanted to hear about new technology and were happy to share images, when the cost saving were estimated based on reduction in labor, they did not justify a great deal of development to solve the problem. Our big conclusion was that we should focus on processing video where there did not appear to be widespread use of the technology.</p>\n<p>The second boot camp insight is that while wildlife professionals wanted more automated video analysis, it became evident that they were not interested in using software to develop a machine learning model on their own. They simply wanted to outsource the video processing and get the resulting data back for their review. For them computer vision and machine learning are only a means to an end, getting better data. If they could outsource that part to 'experts', they wouldn't have to take their time to do it and they could focus on the work they were more suited for. They don't want to use a tool to get the data, they want to work with someone to give them the data.</p>\n<p>The first technical objective that we tackled was creating new features to characterize the detections in our video processing pipeline. Originally, we had proposed to research and implement promising detection features based on things like color, texture and shape features. We realized that for a new dataset in a new context, the general features that we created may not be the optimal way to discriminate object classes. Instead, we implemented an unsupervised deep learning technique to generate novel features. To do this we used an unsupervised machine learning model called an Autoencoder. Autoencoders are trained to compress and then reproduce an input image as faithfully as possible. They have two parts: the encoder and decoder. The encoder converts the input image into a compressed code. The decoder converts the compressed code back into the reconstructed image.</p>\n<p>We trained the full autoencoder on our detection images and would then deploy only the encoder portion of the model to convert detection images to a code. This allowed us a way to train an unsupervised model while also being able to use the code to perform machine learning classification. Unfortunately, autoencoders are not the right tool to apply to our current area of focus. Our experiments found that on datasets with small objects of interest, and therefore large backgrounds, the codes that the encoder produces emphasize the background at the expense of the objects of interest.</p>\n<p>The second technical objective we accomplished was developing active learning software. The goal of active learning is to reduce the effort required to achieve a high accuracy classification model. In traditional machine learning scenarios, a large, labeled dataset is required to train a model. Labeling the data is time-consuming and expensive, especially when expert knowledge is necessary. Active learning aims to mitigate this issue by actively selecting the most informative instances from the unlabeled data for annotation.</p>\n<p>We implemented an active learning architecture and compared it to a na?ve random selection methodology and tested it on a preexisting data set that had already been labeled. The comparison between the active learning approach and the naive random selection method demonstrated the benefits of active learning in terms of model accuracy improvement and labeling efficiency. Active learning found all classes at early iterations when compared with random model and converged on a final model much more quickly. The active learning approach's advantage lies in its ability to select informative points from the unlabeled data, leading to a more comprehensive and accurate model with reduced labeling effort compared to random selection.</p>\n<p>The final technical objective that we met was creating an open-source video acquisition system designs that will make the assembly of a capable system achievable without any technical expertise. The goal of the design documents is to provide an overview and in-depth work instructions for purchasing, assembling, and deploying a solar powered thermal camera system to the field. All the guides we created were put onto a publicly available Google Drive.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/27/2023<br>\n\t\t\t\t\tModified by: Brogan&nbsp;Morton</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDuring this NSF Phase 1 Wildlife Imaging Systems explored ways to automate the analysis of the wildlife images &amp; videos using various techniques.  The goal was to reduce the amount of manual review of the data (including labeling for machine learning models) and to increase the value of the information gleaned from the various media.\n\nWe participated in the NSF-sponsored Beat-the-Odds Boot Camp for Phase 1 SBIR recipients. We had an incredibly productive time during the Boot Camp and got to know the market much better. We gathered some very important insights that made us focus our development efforts.\n\nThe first boot camp insight was that there did not seem to be a large customer concern about the current situation with wildlife image sorting and classification. While people wanted to hear about new technology and were happy to share images, when the cost saving were estimated based on reduction in labor, they did not justify a great deal of development to solve the problem. Our big conclusion was that we should focus on processing video where there did not appear to be widespread use of the technology.\n\nThe second boot camp insight is that while wildlife professionals wanted more automated video analysis, it became evident that they were not interested in using software to develop a machine learning model on their own. They simply wanted to outsource the video processing and get the resulting data back for their review. For them computer vision and machine learning are only a means to an end, getting better data. If they could outsource that part to 'experts', they wouldn't have to take their time to do it and they could focus on the work they were more suited for. They don't want to use a tool to get the data, they want to work with someone to give them the data.\n\nThe first technical objective that we tackled was creating new features to characterize the detections in our video processing pipeline. Originally, we had proposed to research and implement promising detection features based on things like color, texture and shape features. We realized that for a new dataset in a new context, the general features that we created may not be the optimal way to discriminate object classes. Instead, we implemented an unsupervised deep learning technique to generate novel features. To do this we used an unsupervised machine learning model called an Autoencoder. Autoencoders are trained to compress and then reproduce an input image as faithfully as possible. They have two parts: the encoder and decoder. The encoder converts the input image into a compressed code. The decoder converts the compressed code back into the reconstructed image.\n\nWe trained the full autoencoder on our detection images and would then deploy only the encoder portion of the model to convert detection images to a code. This allowed us a way to train an unsupervised model while also being able to use the code to perform machine learning classification. Unfortunately, autoencoders are not the right tool to apply to our current area of focus. Our experiments found that on datasets with small objects of interest, and therefore large backgrounds, the codes that the encoder produces emphasize the background at the expense of the objects of interest.\n\nThe second technical objective we accomplished was developing active learning software. The goal of active learning is to reduce the effort required to achieve a high accuracy classification model. In traditional machine learning scenarios, a large, labeled dataset is required to train a model. Labeling the data is time-consuming and expensive, especially when expert knowledge is necessary. Active learning aims to mitigate this issue by actively selecting the most informative instances from the unlabeled data for annotation.\n\nWe implemented an active learning architecture and compared it to a na?ve random selection methodology and tested it on a preexisting data set that had already been labeled. The comparison between the active learning approach and the naive random selection method demonstrated the benefits of active learning in terms of model accuracy improvement and labeling efficiency. Active learning found all classes at early iterations when compared with random model and converged on a final model much more quickly. The active learning approach's advantage lies in its ability to select informative points from the unlabeled data, leading to a more comprehensive and accurate model with reduced labeling effort compared to random selection.\n\nThe final technical objective that we met was creating an open-source video acquisition system designs that will make the assembly of a capable system achievable without any technical expertise. The goal of the design documents is to provide an overview and in-depth work instructions for purchasing, assembling, and deploying a solar powered thermal camera system to the field. All the guides we created were put onto a publicly available Google Drive.\n\n \n\n\t\t\t\t\tLast Modified: 09/27/2023\n\n\t\t\t\t\tSubmitted by: Brogan Morton"
 }
}