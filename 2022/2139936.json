{
 "awd_id": "2139936",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: An Algorithmic Theory of Brain Behavior:  Concept Representation and Learning in Spiking Neural Networks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922095",
 "po_email": "kwimmer@nsf.gov",
 "po_sign_block_name": "Karl Wimmer",
 "awd_eff_date": "2022-06-01",
 "awd_exp_date": "2026-05-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2022-01-06",
 "awd_max_amd_letter_date": "2022-01-06",
 "awd_abstract_narration": "This project aims at understanding computation in the brain, in terms of abstract, interacting, distributed algorithms.  It assumes a mathematical model of computation based on directed graphs (nodes and connecting edges), where the nodes correspond to neurons and the edges correspond to nerve fibers by which neurons may influence each other.  The project studies problems that are typical of those solved by actual brains, such as problems of focusing attention, making decisions, detecting similarity between sensed odors or visual scenes, and recognizing and learning concepts with interesting structure.  It studies these problems using techniques from theoretical computer science. This work has both biological and computer-science motivations.  The algorithmic perspective is expected to help in understanding the computational mechanisms employed by biological neural networks.  On the other hand, many of the problems that are solved by these networks are also fundamental in computer science and artificial intelligence; studying them in the setting of biological neural networks is expected to offer a new perspective and yield new results.  Biological algorithms are naturally flexible, robust, and adaptive---properties that are also desirable for modern computer systems.\r\n\r\nIn more detail, this work is based on a synchronous, stochastic Spiking Neural Network (SNN) model.  Previously, the investigator and collaborators used this type of model to study several problems including Winner-Take-All decision-making, data compression and clustering, and learning of simple hierarchically-structured concepts.  They produced new algorithms (networks) and analyzed them in terms of costs such as network size and convergence time.  They also discovered some cost tradeoffs and proved related lower bound results. This project continues this research program, but now focusing on the central issues of how concepts are represented in the brain, how those representations are used, and how they may be learned.  \"Concepts\" here encompass both logical concepts, such as hierarchical structures and linguistic constructs, and physical concepts, such as moving objects.  A main thesis is:  \"Structure that is naturally present in real-world concepts gets mirrored in their neural representations, in a way that facilitates both learning and recognition.\"  This project uses approaches from theoretical computer science, notably, distributed and probabilistic algorithms, as well as linear algebra and complexity theory, to investigate this hypothesis. Another emphasis of the project is on how noise and uncertainty affect the costs of solving problems in brain networks, as well as the choice of representations.  Still another is on how the brain may combine networks that solve simpler problems into larger networks that solve more complex problems. Specifically, the project is studying (1) fundamental theoretical questions about SNN models and their computing power, (2) common neural primitives (such as Winner-Take-All) that may be used to solve more complex brain problems, (3) questions about efficient static representations of structured concepts in brain networks, and (4) questions about how such representations can be learned.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nancy",
   "pi_last_name": "Lynch",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Nancy A Lynch",
   "pi_email_addr": "lynch@csail.mit.edu",
   "nsf_id": "000169435",
   "pi_start_date": "2022-01-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7934",
   "pgm_ref_txt": "PARAL/DISTRIBUTED ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": null
}