{
 "awd_id": "2151235",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Computational and Mathematical Studies of Compression and Distillation Methods for Deep Neural Networks and Applications",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922948",
 "po_email": "slevine@nsf.gov",
 "po_sign_block_name": "Stacey Levine",
 "awd_eff_date": "2022-09-01",
 "awd_exp_date": "2025-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2022-03-11",
 "awd_max_amd_letter_date": "2024-08-26",
 "awd_abstract_narration": "This project will develop efficient deep learning architectures for the deployment of artificial intelligence algorithms on resource limited platforms, such as mobile computing and the internet of things. High performance deep neural networks consume hundreds of billions of flops in computation and store hundreds of millions of parameters in memory.  However, devices with limited resources with respect to both power and memory call for constructions of lightweight deep neural networks to maintain the performance level of their heavyweight counterparts. This project aims to develop an efficient search-based architecture compression method and a novel teacher-tutor-student (knowledge distillation) framework to extract a smart lightweight network (student) from a state-of-the-art heavyweight network (teacher) with the help of an intermediate network (tutor). Real world applications benefitting from the project include visual computing on mobile phone and autonomous driving, the delivery, monitor and rescue missions by the drone, and disease detection and diagnosis in mobile health. The project will train graduate students and enrich data science curriculum for a diverse body of undergraduate students in science and engineering at minority serving institutions. \r\n\r\nThe project will study a dual-network cooperation method for the search-based architecture compression so that the low level network weights and high level network structures are both optimized efficiently. A key element is a relaxation of bilevel optimization to a single level optimization task together with non-differentiable decision-making in the search approximated by a differentiable proxy function. The project will advance knowledge distillation methods, expanding distillation to intermediate layers of teacher networks by leveraging similarity measures on tensors of different shapes, and multi-resolution path learning techniques arising in image segmentations. The investigator will also formulate simplified classification problems for mathematical analysis and the understanding of distillation learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jack",
   "pi_last_name": "Xin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jack Xin",
   "pi_email_addr": "jxin@math.uci.edu",
   "nsf_id": "000181369",
   "pi_start_date": "2022-03-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "160 Aldrich Hall",
  "perf_city_name": "Irvine",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926973875",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 103269.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 96657.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 100074.0
  }
 ],
 "por": null
}