{
 "awd_id": "2111631",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Multi-depth-camera volumetric video recording and streaming",
 "cfda_num": "47.041, 47.084",
 "org_code": "15030000",
 "po_phone": "7032922174",
 "po_email": "rmehta@nsf.gov",
 "po_sign_block_name": "Rajesh Mehta",
 "awd_eff_date": "2022-03-15",
 "awd_exp_date": "2023-03-31",
 "tot_intn_awd_amt": 256000.0,
 "awd_amount": 256000.0,
 "awd_min_amd_letter_date": "2022-03-08",
 "awd_max_amd_letter_date": "2022-03-08",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to improve digital content delivery of volumetric content, or representation of 3D subjects. Current volumetric content is commonly limited to very short clips (typically < 10 seconds) due to huge file sizes. This research advances a low-cost, live-streaming, 360-degree volumetric content platform with no duration limits, even on 4G networks. An individual will be able to take low-cost, readily available components and create what would commonly be referred to as a hologram of themselves, for live broadcast and/or recording. The content will be compatible with existing mobile devices, and most commercially available wearable extended reality (XR) devices. both mobile and tethered. Due to the entry-level off-the-shelf hardware that this project will leverage, volumetric content creation costs will be on par with online video production costs, ensuring broad adoption across a wide number of applications, including medical, education, training, safety, entertainment and telecommunications.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project seeks to enable the capture of a human subject using two or more depth-sensors, quickly combine those in a single stream of compressed video, and broadcast them to mobile and wearable devices using existing cloud services. This content can then be viewed on a standard two-dimensional device like a mobile phone, or on a 3D device like an XR wearable. The content will be fully realized, and viewable from any angle at any time. There will be no duration limits, and file sizes will be comparable to standard HD video content.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Teresa",
   "pi_last_name": "Jones",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Teresa G Jones",
   "pi_email_addr": "DGSJones@gmail.com",
   "nsf_id": "000844786",
   "pi_start_date": "2022-03-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "BLINXEL LLC",
  "inst_street_address": "355 S GRAND AVE",
  "inst_street_address_2": "STE 2450",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5183047248",
  "inst_zip_code": "900719500",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "BLINXEL LLC",
  "org_prnt_uei_num": "",
  "org_uei_num": "Y3X3DBJU8MK8"
 },
 "perf_inst": {
  "perf_inst_name": "BLINXEL LLC",
  "perf_str_addr": "355 S Grand Ave",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900711560",
  "perf_ctry_code": "US",
  "perf_cong_dist": "34",
  "perf_st_cong_dist": "CA34",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079E",
   "pgm_ref_txt": "VISUALIZATION & VIRTUAL DESIGN"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 256000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Augmented Reality (AR) has been predicted as the future of mobile marketing and entertainment ever since the first iPhones established the modern mobile landscape. This prediction has remained for the past fifteen years to this day, but has yet to see widespread success outside of Niantic's Pokemon Go. Our team, having worked in mobile augmented reality throughout this whole period, sought and received NSF funding for addressing one of the major issues in addressing the AR market - content creation for augmented reality is too expensive and inaccessible.</p>\n<p><br />&nbsp; &nbsp; Traditional AR offerings are limited to either hand-made 3D content or photogrammetric and similar scanning techniques, all of which impose a high cost of entry that is rarely met with equivalent return. Prior work with our team has focused on capturing live subjects using a portable depth cameras, performing a volumetric capture of a subject with minimal processing, bringing the effort of content creation closer to traditional video. Our NSF SBIR Phase 1 project, Multi-depth-camera volumetric video recording and streaming, is an expansion on our prior volumetric video work to handle the inclusion of additional capture angles, allowing for content closer to the capability of a traditional volumetric video capture stage. Existing capture stages are tailored for very high-end production, and lack the portability, processing, and recording cost requirements required by most clients. In order to meet this demand, new methods needed to be developed, predominately to allow for the live-streaming of volumetric content to mobile hand held and wearable devices over existing mobile networks.</p>\n<p><br />&nbsp; &nbsp; For portability, we determined that the Kinect Azure sensors to be the only viable option as a depth-camera due to both their sensor capability and sourcing the hardware. Market research showed that to make effective use of live broadcast volumetric content, the portability and impact of our setup must be minimal enough to to be unobtrusive over existing live stage performances and allow for flexibility in the capture setup. For these size and weight requirements, we had hoped to use Nvidia Jetson systems to drive each sensor, yet supply chain and manufacture issues prevented us from sourcing the hardware and our team went on to find an alternative, Intel provided solution.</p>\n<p><br />&nbsp; &nbsp;Processing of these capture nodes is coordinated and run through another portable computer, for which we had to find and develop a solution for calibrating the alignments from each capture node. We found that a combination of visual marker \"Apriltags\" which we had extended to use the depth-sensing capability of the Kinect Azure provided our required flexibility. Processing overhead for the multitude of depth sensors required careful consideration as to how much could be processed by the nodes themselves without missing frame timings, in addition to managing the high volume of data being managed and saved by the computer coordinating the capture itself.</p>\n<p><br />&nbsp; &nbsp; Storage and distribution of data also required new developments; our prior work from a single sensor did not require the same degree of data consistency that would be required for alignment of multiple cameras. Direct handling of the amount of depth data recovered during a capture is infeasible, and currently impossible on mobile devices. To allow for content be transmissible and displayable the use of video containers as an intermediary is required, and our team had previously obtained patents for the use of video containers to address this issue.</p>\n<p><br />&nbsp; &nbsp; Although required for their compression and speedy playback, video containers contain a number of legacy issues that had to be addressed. Video data is often \"damaged\" during transformations, with video tooling having longstanding issues with data loss and degradation. Data formats accommodate legacy broadcast standards inconsistently, and internal colorspaces being inconsistently implemented and inherently untranslatable resulted in our team having to thoroughly inspect the entire video processing pipeline to develop a process for minimizing any unpreventable data loss, and reversing data transformations. Because of these developments, our capture sizes can be measured alongside traditional videos in the thousands of kilobits per second, instead of the hundreds of megabits per second of traditional capture stage outputs.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/21/2023<br>\n\t\t\t\t\tModified by: Teresa&nbsp;G&nbsp;Jones</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2111631/2111631_10788569_1679435330903_image(9)--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2111631/2111631_10788569_1679435330903_image(9)--rgov-800width.jpg\" title=\"Assembled point cloud\"><img src=\"/por/images/Reports/POR/2023/2111631/2111631_10788569_1679435330903_image(9)--rgov-66x44.jpg\" alt=\"Assembled point cloud\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An early approximation of the final assembly of 6 recorded angles on a single subject for alignment validation.</div>\n<div class=\"imageCredit\">Blinxel LLC</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Teresa&nbsp;G&nbsp;Jones</div>\n<div class=\"imageTitle\">Assembled point cloud</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/2111631/2111631_10788569_1679435215662_IMG_2878--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2111631/2111631_10788569_1679435215662_IMG_2878--rgov-800width.jpg\" title=\"Node Computer\"><img src=\"/por/images/Reports/POR/2023/2111631/2111631_10788569_1679435215662_IMG_2878--rgov-66x44.jpg\" alt=\"Node Computer\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A small form factor Intel node PC for each distributed sensor.</div>\n<div class=\"imageCredit\">Blinxel LLC</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Teresa&nbsp;G&nbsp;Jones</div>\n<div class=\"imageTitle\">Node Computer</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nAugmented Reality (AR) has been predicted as the future of mobile marketing and entertainment ever since the first iPhones established the modern mobile landscape. This prediction has remained for the past fifteen years to this day, but has yet to see widespread success outside of Niantic's Pokemon Go. Our team, having worked in mobile augmented reality throughout this whole period, sought and received NSF funding for addressing one of the major issues in addressing the AR market - content creation for augmented reality is too expensive and inaccessible.\n\n\n    Traditional AR offerings are limited to either hand-made 3D content or photogrammetric and similar scanning techniques, all of which impose a high cost of entry that is rarely met with equivalent return. Prior work with our team has focused on capturing live subjects using a portable depth cameras, performing a volumetric capture of a subject with minimal processing, bringing the effort of content creation closer to traditional video. Our NSF SBIR Phase 1 project, Multi-depth-camera volumetric video recording and streaming, is an expansion on our prior volumetric video work to handle the inclusion of additional capture angles, allowing for content closer to the capability of a traditional volumetric video capture stage. Existing capture stages are tailored for very high-end production, and lack the portability, processing, and recording cost requirements required by most clients. In order to meet this demand, new methods needed to be developed, predominately to allow for the live-streaming of volumetric content to mobile hand held and wearable devices over existing mobile networks.\n\n\n    For portability, we determined that the Kinect Azure sensors to be the only viable option as a depth-camera due to both their sensor capability and sourcing the hardware. Market research showed that to make effective use of live broadcast volumetric content, the portability and impact of our setup must be minimal enough to to be unobtrusive over existing live stage performances and allow for flexibility in the capture setup. For these size and weight requirements, we had hoped to use Nvidia Jetson systems to drive each sensor, yet supply chain and manufacture issues prevented us from sourcing the hardware and our team went on to find an alternative, Intel provided solution.\n\n\n   Processing of these capture nodes is coordinated and run through another portable computer, for which we had to find and develop a solution for calibrating the alignments from each capture node. We found that a combination of visual marker \"Apriltags\" which we had extended to use the depth-sensing capability of the Kinect Azure provided our required flexibility. Processing overhead for the multitude of depth sensors required careful consideration as to how much could be processed by the nodes themselves without missing frame timings, in addition to managing the high volume of data being managed and saved by the computer coordinating the capture itself.\n\n\n    Storage and distribution of data also required new developments; our prior work from a single sensor did not require the same degree of data consistency that would be required for alignment of multiple cameras. Direct handling of the amount of depth data recovered during a capture is infeasible, and currently impossible on mobile devices. To allow for content be transmissible and displayable the use of video containers as an intermediary is required, and our team had previously obtained patents for the use of video containers to address this issue.\n\n\n    Although required for their compression and speedy playback, video containers contain a number of legacy issues that had to be addressed. Video data is often \"damaged\" during transformations, with video tooling having longstanding issues with data loss and degradation. Data formats accommodate legacy broadcast standards inconsistently, and internal colorspaces being inconsistently implemented and inherently untranslatable resulted in our team having to thoroughly inspect the entire video processing pipeline to develop a process for minimizing any unpreventable data loss, and reversing data transformations. Because of these developments, our capture sizes can be measured alongside traditional videos in the thousands of kilobits per second, instead of the hundreds of megabits per second of traditional capture stage outputs.\n\n\t\t\t\t\tLast Modified: 03/21/2023\n\n\t\t\t\t\tSubmitted by: Teresa G Jones"
 }
}