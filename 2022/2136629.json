{
 "awd_id": "2136629",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I: Video-to-speech software application to provide real-time, noninvasive, natural voice restoration for voiceless individuals",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032922174",
 "po_email": "rmehta@nsf.gov",
 "po_sign_block_name": "Rajesh Mehta",
 "awd_eff_date": "2022-09-01",
 "awd_exp_date": "2023-11-30",
 "tot_intn_awd_amt": 255997.0,
 "awd_amount": 255997.0,
 "awd_min_amd_letter_date": "2022-08-25",
 "awd_max_amd_letter_date": "2023-10-31",
 "awd_abstract_narration": "The broader impact of this Small Business Innovation Research (SBIR) Phase I project seeks to enable one million Americans that suffer with the loss of ability to speak through disease of or damage to the larynx or mouth (aphonia). The inability to fluently communicate with other people has severe consequences. Voiceless individuals are three times more likely to suffer a preventable adverse event in medical settings than speaking patients, and this can lead to health problems and even life-threatening situations. Up to 50% of these adverse events could be avoided with adequate communication between patients and clinicians. The proposed solution is a video-to-speech software application that provides voiceless people with real-time communication assistance, especially geared towards medical settings. The technology could help prevent hundreds of thousands of adverse health events each year (costing $6.8 billion annually), with benefits for the voiceless population and the healthcare system in general. The innovation may improve voice restoration by providing real-time translation with no training needed and allowing complex messages to be expressed while looking eye-to-eye (an important part of human communication). Moreover, the technology does not require invasive installations nor complex equipment, is readily accessible, and has maintenance requirements that are marginal.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project aims to address the intellectual challenge of overcoming the ambiguity of visemes when trying to automate lip-reading. Visemes (the gestures made when talking) and phonemes (the sounds produced with these gestures) do not share a one-to-one correspondence. This makes accurately predicting the intended speech based on visual information challenging. Previous researchers have failed to reach acceptable accuracy levels in the interpretation of visemes, while other tools only work with a few dozen words that must be structured according to pre-defined, fixed rules that are impractical. The main goal of this effort is to develop a combination of convolutional neural networks and recurrent neural network transducers that is capable of accurately differentiating visemes and permits real-time, reliable voice assistance for voiceless people. Project objectives include: (1) pre-training an algorithm to detect phonemes using publicly available speech video, (2) optimizing the phoneme-trained algorithm against healthcare relevant vocabulary, and (3) alpha-testing of the lip-reading algorithm against real-time speech.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Subhash",
   "pi_last_name": "Jaini",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Subhash Jaini",
   "pi_email_addr": "Lira_grants_SJ@outlook.com",
   "nsf_id": "000887778",
   "pi_start_date": "2022-08-25",
   "pi_end_date": "2022-10-20"
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Dina",
   "pi_last_name": "Yamaleyeva",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dina Yamaleyeva",
   "pi_email_addr": "dina.yamaleyeva.lira@outlook.com",
   "nsf_id": "000901927",
   "pi_start_date": "2022-10-20",
   "pi_end_date": "2023-10-31"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yi",
   "pi_last_name": "Han",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yi Han",
   "pi_email_addr": "yi.han@liraglobal.com",
   "nsf_id": "000920304",
   "pi_start_date": "2023-10-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "LIRA, INC.",
  "inst_street_address": "3341 HELMSLEY CT",
  "inst_street_address_2": "",
  "inst_city_name": "CONCORD",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9855022367",
  "inst_zip_code": "280277980",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "NC06",
  "org_lgl_bus_name": "LIRA INC",
  "org_prnt_uei_num": "",
  "org_uei_num": "E48FMBAD2VX5"
 },
 "perf_inst": {
  "perf_inst_name": "LIRA, INC.",
  "perf_str_addr": "119 HOLLOW OAK DR",
  "perf_city_name": "DURHAM",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277138643",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1654",
   "pgm_ref_txt": "HUMAN COMPUTER INTERFACE"
  },
  {
   "pgm_ref_code": "1707",
   "pgm_ref_txt": "ADVANCED LEARNING TECHNOLOGIES"
  },
  {
   "pgm_ref_code": "8031",
   "pgm_ref_txt": "Education Products"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 255997.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>For individuals who have lost their ability to generate voice, communication can be a daily struggle. This need is particularly challenging for individuals critically ill and in the hospital. This project set out to develop a groundbreaking lip-reading application, offering a new pathway for expression and a bridge for the current communication gap.</p>\n<p>Three key areas drove our success:</p>\n<p>1. Building a Diverse Training Dataset:</p>\n<ul>\n<li>Enhanced      UX:&nbsp; LipTrain is how we get training data; <a href=\"http://www.liraglobal.com/liptrain\">www.liraglobal.com/liptrain</a>. We enhanced      the platform&rsquo;s user-friendliness,&nbsp;streamlining the recording process      and integrating storytelling elements to convey the gravity of      voicelessness.</li>\n<li>Crowdsourcing      Power:&nbsp;Our LipTrain campaign rallied some 1,118 microvolunteers,      primarily through social media, collecting 7,269 videos showcasing a      variety of accents,&nbsp;ages,&nbsp;and ethnicities.</li>\n<li>Quality      Control:&nbsp;Clinicians and patients/family members verified the sentences      we ask LipTrain microvolunteers to read,&nbsp;ensuring our video dataset's      relevance to address the critical challenge of being voiceless while      hospitalized.</li>\n</ul>\n<p>2. Refining the Lip-reading Predictive Algorithm:</p>\n<ul>\n<li>Shifting      Gears:&nbsp;Our previous efforts focused on sentence-level prediction      requiring users to stick to a fixed set of phrases. We transitioned to a word-based      prediction,&nbsp;allowing for the flexibility we are accustomed to using. </li>\n<li>First in class      prediction: We completed the project with a word-level prediction model      that functions at a 74-78% accuracy on a 195-word list inclusive of a      basic English and healthcare-centric vocabulary.</li>\n<li>Large Language      Models:&nbsp;Integrating LLMs significantly boosted the algorithm's      sophistication and understanding of language context. This enabled key      accuracy gains and offers promise in expanding the prediction vocabulary      to fluent English. </li>\n</ul>\n<p>3. Creating a User-Friendly Interface:</p>\n<ul>\n<li>User-Driven      Design:&nbsp;Initial layouts and wireframes were tested and refined based      on user feedback, ensuring a seamless experience.</li>\n<li>Word-Level      Integration:&nbsp;We designed the interface to seamlessly integrate the      word-level predictive model,&nbsp;providing real-time lip-reading      assistance.</li>\n</ul>\n<p>This project is not just a technological feat; it's a beacon of hope for those who yearn to be heard. The positive user feedback fuels our commitment to refining this technology, expanding its reach, and ultimately delivering a product that transforms lives.</p>\n<p>Our journey is far from over, but this breakthrough is a pivotal step towards revolutionizing communication for individuals facing voicelessness. We envision a future where our application empowers them to express themselves freely, participate in conversations, and reconnect with the world around them.</p>\n<p>This is more than a project; it's a promise towards a world where everyone has a voice.</p><br>\n<p>\n Last Modified: 04/25/2024<br>\nModified by: Yi&nbsp;Han</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nFor individuals who have lost their ability to generate voice, communication can be a daily struggle. This need is particularly challenging for individuals critically ill and in the hospital. This project set out to develop a groundbreaking lip-reading application, offering a new pathway for expression and a bridge for the current communication gap.\n\n\nThree key areas drove our success:\n\n\n1. Building a Diverse Training Dataset:\n\nEnhanced      UX: LipTrain is how we get training data; www.liraglobal.com/liptrain. We enhanced      the platforms user-friendliness,streamlining the recording process      and integrating storytelling elements to convey the gravity of      voicelessness.\nCrowdsourcing      Power:Our LipTrain campaign rallied some 1,118 microvolunteers,      primarily through social media, collecting 7,269 videos showcasing a      variety of accents,ages,and ethnicities.\nQuality      Control:Clinicians and patients/family members verified the sentences      we ask LipTrain microvolunteers to read,ensuring our video dataset's      relevance to address the critical challenge of being voiceless while      hospitalized.\n\n\n\n2. Refining the Lip-reading Predictive Algorithm:\n\nShifting      Gears:Our previous efforts focused on sentence-level prediction      requiring users to stick to a fixed set of phrases. We transitioned to a word-based      prediction,allowing for the flexibility we are accustomed to using. \nFirst in class      prediction: We completed the project with a word-level prediction model      that functions at a 74-78% accuracy on a 195-word list inclusive of a      basic English and healthcare-centric vocabulary.\nLarge Language      Models:Integrating LLMs significantly boosted the algorithm's      sophistication and understanding of language context. This enabled key      accuracy gains and offers promise in expanding the prediction vocabulary      to fluent English. \n\n\n\n3. Creating a User-Friendly Interface:\n\nUser-Driven      Design:Initial layouts and wireframes were tested and refined based      on user feedback, ensuring a seamless experience.\nWord-Level      Integration:We designed the interface to seamlessly integrate the      word-level predictive model,providing real-time lip-reading      assistance.\n\n\n\nThis project is not just a technological feat; it's a beacon of hope for those who yearn to be heard. The positive user feedback fuels our commitment to refining this technology, expanding its reach, and ultimately delivering a product that transforms lives.\n\n\nOur journey is far from over, but this breakthrough is a pivotal step towards revolutionizing communication for individuals facing voicelessness. We envision a future where our application empowers them to express themselves freely, participate in conversations, and reconnect with the world around them.\n\n\nThis is more than a project; it's a promise towards a world where everyone has a voice.\t\t\t\t\tLast Modified: 04/25/2024\n\n\t\t\t\t\tSubmitted by: YiHan\n"
 }
}