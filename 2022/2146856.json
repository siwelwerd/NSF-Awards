{
 "awd_id": "2146856",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "DDRIG: The Algorithmic Translation of Expertise: Credible Knowledge and Machine Learning in Medicine",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Frederick Kronz",
 "awd_eff_date": "2022-06-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 15706.0,
 "awd_amount": 15706.0,
 "awd_min_amd_letter_date": "2022-01-20",
 "awd_max_amd_letter_date": "2022-01-20",
 "awd_abstract_narration": "The use of artificial intelligence (AI) and machine learning (ML) to assist experts in making sophisticated professional decisions is well under way in such areas as medical diagnosis, drug development, and public health. This project focuses on an especially promising application of ML systems: their use to support medical diagnoses by analyzing images, such as CT scans and digitized pathology slides. This research will study the development of ML-based medical image analysis systems, tracing their production, application, and regulation. It will pay special attention to how medical experts and policymakers assess the credibility of the diagnostic suggestions that ML systems make. The research aims to contribute to the use of ML tools to improve the quality and accessibility of healthcare and to inform policymaking about the introduction of these technologies. \r\n\r\nDeveloping an ML system involves translating human expertise into a new algorithmic form. This study will investigate novel questions raised by this process about the credibility of diagnosis. How can medical experts evaluate the credibility of ML systems, given that the internal workings of these systems are complex and, to some extent, inscrutable? How might the rise of ML systems affect the credibility of human experts? How will understanding of expertise change when well-trained experts, historically the most credible judges of complex professional questions, find their judgments implicitly challenged by AI systems? To explore these questions, the investigators will conduct ethnography at two AI startups, conduct semi-structured interviews with engineers and clinicians, and analyze written materials. By analyzing negotiations over credible knowledge in this context, the project will provide insights about how the credibility of the human and the machine are assessed. Beyond its immediate implications for understanding the credibility of ML systems, the study aims to enrich scholarship in the sociology of expertise, medical sociology, data studies, and the governance of emerging technologies.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Hilgartner",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen Hilgartner",
   "pi_email_addr": "shh6@cornell.edu",
   "nsf_id": "000172468",
   "pi_start_date": "2022-01-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Wanheng",
   "pi_last_name": "Hu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wanheng Hu",
   "pi_email_addr": "wh429@cornell.edu",
   "nsf_id": "000833872",
   "pi_start_date": "2022-01-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148502820",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "124Y00",
   "pgm_ele_name": "Science & Technology Studies"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 15706.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This award supported twelve months of research on how machine learning (ML), a type of artificial intelligence (AI) technology, is being developed, applied, and evaluated in expert practices. As a case study, we examined image-based medical diagnosis, an area where ML is being widely applied. During the award period, we conducted hands-on fieldwork, including in-depth interviews with radiologists and pathologists, and clinical shadowing at three general hospitals. We also analyzed the empirical materials previously gathered from participant observations at two medical AI startups, interviews with medical image annotators and engineers, as well as various documents including government reports and news media articles. We found that while there&rsquo;s a general perception that ML systems can act like digital versions of top doctors and provide credible diagnostic decisions, the reality is a bit more complex. For example, in the AI industry, the ML systems do not always &ldquo;learn&rdquo; from top doctors because the training data is often annotated by medically uncredentialed personnel who are much cheaper and available. Regardless, engineers take these annotations as baseline truths and use statistical metrics to indicate the trustworthiness of AI. For clinicians, however, the credibility of AI output is primarily based on its alignment with their own judgments, rather than those quantitative metrics. Our findings shed light on the challenges and considerations of integrating ML into healthcare. Such insights will be of interest to clinicians, engineers, tech companies, policymakers, as well as citizens who might one day benefit from the application of ML in expert practices.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/25/2023<br>\n\t\t\t\t\tModified by: Wanheng&nbsp;Hu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis award supported twelve months of research on how machine learning (ML), a type of artificial intelligence (AI) technology, is being developed, applied, and evaluated in expert practices. As a case study, we examined image-based medical diagnosis, an area where ML is being widely applied. During the award period, we conducted hands-on fieldwork, including in-depth interviews with radiologists and pathologists, and clinical shadowing at three general hospitals. We also analyzed the empirical materials previously gathered from participant observations at two medical AI startups, interviews with medical image annotators and engineers, as well as various documents including government reports and news media articles. We found that while there\u2019s a general perception that ML systems can act like digital versions of top doctors and provide credible diagnostic decisions, the reality is a bit more complex. For example, in the AI industry, the ML systems do not always \"learn\" from top doctors because the training data is often annotated by medically uncredentialed personnel who are much cheaper and available. Regardless, engineers take these annotations as baseline truths and use statistical metrics to indicate the trustworthiness of AI. For clinicians, however, the credibility of AI output is primarily based on its alignment with their own judgments, rather than those quantitative metrics. Our findings shed light on the challenges and considerations of integrating ML into healthcare. Such insights will be of interest to clinicians, engineers, tech companies, policymakers, as well as citizens who might one day benefit from the application of ML in expert practices.\n\n \n\n\t\t\t\t\tLast Modified: 09/25/2023\n\n\t\t\t\t\tSubmitted by: Wanheng Hu"
 }
}