{
 "awd_id": "2224263",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Towards robust, interpretable deep learning via communication theory and neuro-inspiration",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2022-07-01",
 "awd_exp_date": "2025-06-30",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2022-05-20",
 "awd_max_amd_letter_date": "2022-05-20",
 "awd_abstract_narration": "Deep neural networks (DNNs) are attaining great success in an increasing array of applications, yet there remain persistent concerns regarding their lack of interpretability and robustness. The standard approach to training DNNs is to optimize an end-to-end cost function based on variants of gradient descent. This simple approach is flexible, allowing innovation in architectures and applications, and scaling to neural networks with a large number of parameters, given enough data and computational power. Such end-to-end, or top-down training, however, does not provide control over, or understanding of, the features being extracted by the layers of the neural networks. The vulnerability of DNNs to adversarial attacks, for example, is a symptom of this phenomenon. The proposed research seeks to address these drawbacks using ideas from communication theory and neuroscience: the goal is to actively shape the features being extracted by individual layers of the neural network, in addition to training the overall network to attain an end-to-end goal. This research will contribute to curricular enhancements in signal processing and machine learning explored via courses, REU projects and senior capstone projects.\r\n\r\nThe proposed technical approach leverages the existing computational infrastructure for training, while imposing layer-by-layer constraints aimed at producing sparse, strong activations. Drawing on ideas from communication theory, the goal is to learn \u201cmatched filters\u201d which enhance the \u201csignal-to-noise ratio (SNR)\u201d at neuron outputs at each layer. One may show that this approach is consistent with Hebbian and anti-Hebbian (HAH) learning as posited in neuroscience, in which neurons that are strongly activated for an input are promoted, with less active neurons being demoted. This work posits enhanced robustness via such an SNR-maximizing strategy, together with additional nonlinear transformations such as divisive normalization borrowed from neuroscience. While preliminary visualizations indicate more interpretable neurons, there is reason to expect sparse, strong activations to be more amenable to quantitative interpretation via statistical and information-theoretic analysis. The goal of the proposed research is two-fold: to gain theoretical insight into HAH-based learning via toy models, and to demonstrate practical gains in robustness and interpretability relative to state of the art DNNs. Experimental evaluations will initially be conducted on image datasets which provide standard performance benchmarks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Upamanyu",
   "pi_last_name": "Madhow",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Upamanyu Madhow",
   "pi_email_addr": "madhow@ece.ucsb.edu",
   "nsf_id": "000296458",
   "pi_start_date": "2022-05-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "3227 CHEADLE HALL",
  "perf_city_name": "SANTA BARBARA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931060001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": null
}