{
 "awd_id": "2230693",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Visual Representation Learning Using Mixed Labeled and Unlabeled Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 167041.0,
 "awd_amount": 84970.0,
 "awd_min_amd_letter_date": "2022-06-02",
 "awd_max_amd_letter_date": "2022-06-02",
 "awd_abstract_narration": "Recent advances in deep learning has led to great results in visual recognition and object detection. These deep learning models have various applications from self-driving cars to early disease diagnosis and household robots. However, most such models are supervised, meaning that they need large scale manually annotated datasets to tune the parameters, and obtaining the annotation may be expensive in many applications. This project explores a family of self-supervised learning algorithms where the learning is based on unlabeled data only. The new models can learn visual features that can be used for various visual recognition tasks including object detection and action recognition. This project provides research opportunities for under-represented groups and integrates research outcomes into the course curriculum.\r\n\r\nThis project studies a family of self-supervised learning algorithms that can learn rich features from unlabeled images and videos. Self-supervised learning algorithms harvest the knowledge from unlabeled data by modeling some regularity in the space of natural images or videos. This project studies novel self-supervised learning algorithms based on constraining the learning by relating transformations of images to transformations of their representations. Moreover, this project studies a novel multi-task learning framework for aggregating the knowledge learned from multiple supervised and self-supervised learning algorithms. This algorithm uses quantization methods to ignore the task specific details of the representation in transferring the knowledge. This algorithm results in a rich set of representations that generalize well across various visual recognition tasks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hamed",
   "pi_last_name": "Pirsiavash",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hamed Pirsiavash",
   "pi_email_addr": "hpirsiav@ucdavis.edu",
   "nsf_id": "000704409",
   "pi_start_date": "2022-06-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956186134",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 84969.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Broader impacts:</strong>&nbsp;</p>\r\n<p>This project involved training a group of graduate students, as well as a couple of undergraduate students, with a focus on equipping them with highly relevant skills that will make them competitive in the job market. Six PhD students who worked on this project have graduated and transitioned into full-time research and development roles in industry. In addition, several students gained valuable real-world experience through summer internships.</p>\r\n<p>Moreover, the Principal Investigator (PI) has participated in the COSMOS program (California State Summer School for Mathematics and Science) in the summers of 2023 and 2024. During this program, 22+24 high school students (11th grade) from across California were hosted at UC Davis and introduced to machine learning and deep learning concepts. The program aims to inspire these students to pursue further education and careers in STEM fields.</p>\r\n<p>&nbsp;</p>\r\n<p><strong>Outcomes:</strong></p>\r\n<p>This project has resulted in several peer-reviewer papers published in top-tier conferences along with open-source code.</p>\r\n<p>We have been working on developing self-supervised learning (SSL) methods that can learn from unlabeled images and videos, eliminating the need for costly, time-consuming manual labeling. These advancements aim to improve the accuracy and efficiency of machine learning models while also addressing issues such as bias and privacy concerns. Here are some key contributions from our research:</p>\r\n<ol>\r\n<li>\r\n<p><strong>Compressing SSL Representations</strong>: One area of focus is improving the efficiency of SSL models, particularly by reducing the computational complexity of deep learning systems. Building on earlier work, we have explored techniques for compressing large, powerful SSL models into smaller, more efficient ones. This involves distilling knowledge from a larger \"teacher\" model to a smaller \"student\" model, even applying this approach to different data types (like video or flow data) and various precision levels (such as reducing model weights from double precision to binary).</p>\r\n</li>\r\n<li>\r\n<p><strong>Iterative Similarity Distillation (ISD)</strong>: We introduced a new method that refines the SSL process by softening the distinction between \"positive\" and \"negative\" image pairs. Traditional SSL methods rely on a binary approach, where similar images are pulled together and dissimilar images are pushed apart. Our approach uses soft similarity, allowing the model to handle data more effectively, especially when the data is unbalanced or includes semantically similar negative samples. This iterative learning process, where the teacher model evolves over time, provides better representations with fewer assumptions.</p>\r\n</li>\r\n<li>\r\n<p><strong>Mean Shift for SSL</strong>: We proposed a novel method inspired by the mean shift algorithm, which helps group similar images together without enforcing strict clustering. Unlike other methods that rely on predefined cluster shapes, our approach can more flexibly learn from unlabeled data and produce better features for tasks like image recognition. This method contrasts the similarities between different image augmentations and their nearest neighbors, making it a simple yet powerful addition to SSL.</p>\r\n</li>\r\n<li>\r\n<p><strong>Simplifying Knowledge Distillation</strong>: In the realm of model compression, we showed that a simpler approach&mdash;regression&mdash;can be just as effective for transferring knowledge from larger models to smaller ones. This method uses a multi-layer perceptron (MLP) head to improve the learning of features in smaller networks. Surprisingly, this simpler technique outperformed more complex methods in certain settings, such as when transferring knowledge from a large SimCLR model to a smaller ResNet-50 model.</p>\r\n</li>\r\n<li>\r\n<p><strong>Defending Against Attacks and Enhancing Model Robustness</strong>: We explored vulnerabilities in self-supervised learning models, particularly to <strong>backdoor attacks</strong> and <strong>adversarial patch attacks</strong>. In backdoor attacks, an attacker can poison a small portion of the unlabeled data with a trigger, causing the model to behave incorrectly at test time. We developed a defense mechanism using knowledge distillation to identify and remove poisoned data. Additionally, we found that adaptive models, which adjust their computational resources based on input, are vulnerable to attacks that manipulate their resource usage. Our research highlights the need for better defenses to ensure SSL models remain robust, even in the face of such adversarial threats.</p>\r\n</li>\r\n<li>\r\n<p><strong>Generative Augmentation for Better Generalization</strong>: In an effort to enhance model generalization, we introduced <strong>GeNIe</strong>, a novel data augmentation technique that combines images and text descriptions through a diffusion model. This approach generates challenging examples (hard negatives) by fusing source images with target categories, which forces the model to learn more discriminative features. A variant, <strong>GeNIe-Ada</strong>, adapts the noise level during generation to further improve performance, especially in few-shot learning scenarios.</p>\r\n</li>\r\n<li>\r\n<p><strong>Efficient Model Fine-Tuning with NOLA</strong>: We also introduced <strong>NOLA</strong>, a method that makes fine-tuning large models, like GPT-3, much more efficient. Traditional techniques reduce the number of trainable parameters but are limited in flexibility. NOLA improves on this by making it possible to shrink models <strong>up to 20 times</strong>&nbsp;more than state-of-the-art methods without losing accuracy. This allows large models to be adapted to new tasks with less computational power, making them more accessible and efficient for a wider range of applications.</p>\r\n</li>\r\n</ol>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/30/2024<br>\nModified by: Hamed&nbsp;Pirsiavash</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nBroader impacts:\r\n\n\nThis project involved training a group of graduate students, as well as a couple of undergraduate students, with a focus on equipping them with highly relevant skills that will make them competitive in the job market. Six PhD students who worked on this project have graduated and transitioned into full-time research and development roles in industry. In addition, several students gained valuable real-world experience through summer internships.\r\n\n\nMoreover, the Principal Investigator (PI) has participated in the COSMOS program (California State Summer School for Mathematics and Science) in the summers of 2023 and 2024. During this program, 22+24 high school students (11th grade) from across California were hosted at UC Davis and introduced to machine learning and deep learning concepts. The program aims to inspire these students to pursue further education and careers in STEM fields.\r\n\n\n\r\n\n\nOutcomes:\r\n\n\nThis project has resulted in several peer-reviewer papers published in top-tier conferences along with open-source code.\r\n\n\nWe have been working on developing self-supervised learning (SSL) methods that can learn from unlabeled images and videos, eliminating the need for costly, time-consuming manual labeling. These advancements aim to improve the accuracy and efficiency of machine learning models while also addressing issues such as bias and privacy concerns. Here are some key contributions from our research:\r\n\r\n\r\n\n\nCompressing SSL Representations: One area of focus is improving the efficiency of SSL models, particularly by reducing the computational complexity of deep learning systems. Building on earlier work, we have explored techniques for compressing large, powerful SSL models into smaller, more efficient ones. This involves distilling knowledge from a larger \"teacher\" model to a smaller \"student\" model, even applying this approach to different data types (like video or flow data) and various precision levels (such as reducing model weights from double precision to binary).\r\n\r\n\r\n\n\nIterative Similarity Distillation (ISD): We introduced a new method that refines the SSL process by softening the distinction between \"positive\" and \"negative\" image pairs. Traditional SSL methods rely on a binary approach, where similar images are pulled together and dissimilar images are pushed apart. Our approach uses soft similarity, allowing the model to handle data more effectively, especially when the data is unbalanced or includes semantically similar negative samples. This iterative learning process, where the teacher model evolves over time, provides better representations with fewer assumptions.\r\n\r\n\r\n\n\nMean Shift for SSL: We proposed a novel method inspired by the mean shift algorithm, which helps group similar images together without enforcing strict clustering. Unlike other methods that rely on predefined cluster shapes, our approach can more flexibly learn from unlabeled data and produce better features for tasks like image recognition. This method contrasts the similarities between different image augmentations and their nearest neighbors, making it a simple yet powerful addition to SSL.\r\n\r\n\r\n\n\nSimplifying Knowledge Distillation: In the realm of model compression, we showed that a simpler approachregressioncan be just as effective for transferring knowledge from larger models to smaller ones. This method uses a multi-layer perceptron (MLP) head to improve the learning of features in smaller networks. Surprisingly, this simpler technique outperformed more complex methods in certain settings, such as when transferring knowledge from a large SimCLR model to a smaller ResNet-50 model.\r\n\r\n\r\n\n\nDefending Against Attacks and Enhancing Model Robustness: We explored vulnerabilities in self-supervised learning models, particularly to backdoor attacks and adversarial patch attacks. In backdoor attacks, an attacker can poison a small portion of the unlabeled data with a trigger, causing the model to behave incorrectly at test time. We developed a defense mechanism using knowledge distillation to identify and remove poisoned data. Additionally, we found that adaptive models, which adjust their computational resources based on input, are vulnerable to attacks that manipulate their resource usage. Our research highlights the need for better defenses to ensure SSL models remain robust, even in the face of such adversarial threats.\r\n\r\n\r\n\n\nGenerative Augmentation for Better Generalization: In an effort to enhance model generalization, we introduced GeNIe, a novel data augmentation technique that combines images and text descriptions through a diffusion model. This approach generates challenging examples (hard negatives) by fusing source images with target categories, which forces the model to learn more discriminative features. A variant, GeNIe-Ada, adapts the noise level during generation to further improve performance, especially in few-shot learning scenarios.\r\n\r\n\r\n\n\nEfficient Model Fine-Tuning with NOLA: We also introduced NOLA, a method that makes fine-tuning large models, like GPT-3, much more efficient. Traditional techniques reduce the number of trainable parameters but are limited in flexibility. NOLA improves on this by making it possible to shrink models up to 20 timesmore than state-of-the-art methods without losing accuracy. This allows large models to be adapted to new tasks with less computational power, making them more accessible and efficient for a wider range of applications.\r\n\r\n\r\n\n\n\t\t\t\t\tLast Modified: 12/30/2024\n\n\t\t\t\t\tSubmitted by: HamedPirsiavash\n"
 }
}