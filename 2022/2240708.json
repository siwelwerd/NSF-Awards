{
 "awd_id": "2240708",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: CIF: Small: Deep Sparse Models: Analysis and Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032920000",
 "po_email": "ahero@nsf.gov",
 "po_sign_block_name": "Alfred Hero",
 "awd_eff_date": "2022-08-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 205479.0,
 "awd_amount": 110335.0,
 "awd_min_amd_letter_date": "2022-08-29",
 "awd_max_amd_letter_date": "2022-08-29",
 "awd_abstract_narration": "Deep convolutional neural networks are a class of mathematical models that provide a variety of machine learning tools with impressive success, often obtaining state-of-the-art results across different fields. Yet, their theoretical understanding and the fundamental ideas behind these algorithms have remained elusive. These questions are essential to recognize and characterize their limitations, to provide guarantees for their performance, and even to develop and engineer improved practical models. A promising approach to obtain this understanding is to make assumptions about the class of samples on which these models are deployed (e.g., so that these are \"simple enough\") with the intention of providing theoretical insights about them. Further understanding of this 'multi-layered convolutional sparse model' is what this project seeks accomplish, broadening the understanding of its related optimization and learning problems, and shedding light on deep learning methodologies.\r\n\r\nThis project proposes to advance the state of the art in generalized sparse models of different numbers of layers, focusing on both inference and learning problems. Provable and efficient optimization methods will be derived for the inverse problems associated with multilayer sparse models by relying on new results in proximal gradient and subgradient descent methods. This proposal will further extend the formulation of the pursuit to other settings, increasing stability and robustness to the choice of parameters and to outliers. Furthermore, efficient algorithms for the corresponding unsupervised learning problem will be proposed and analyzed. Questions of sample complexity and generalization bounds will in turn be studied in supervised learning settings. Throughout this project, the resulting algorithms will be studied in terms of their relation to specific convolutional network architectures. The project brings together combined expertise in signal processing, dictionary learning, machine learning, and the design, analysis and implementation of optimization methods for large-scale problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhihui",
   "pi_last_name": "Zhu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhihui Zhu",
   "pi_email_addr": "zhu.3440@osu.edu",
   "nsf_id": "000807941",
   "pi_start_date": "2022-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "Address: 281 W Lane Ave",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101132",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 110335.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Deep convolutional neural networks are a class of mathematical models that provide a variety of machine learning tools with impressive success, often obtaining state-of-the-art results across different fields. Yet, their theoretical understanding and the fundamental ideas behind these algorithms have remained elusive. These questions are essential to recognize and characterize their limitations, to provide guarantees for their performance, and even to develop and engineer improved practical models. This project aimed to address several basic research challenges&nbsp;in understanding deep neural networks by exploiting ideas of parsimonious and sparse representations.</p>\n<p>This project considered parsimonious representations in both supervised and unsupervised settings. In the unsupervised setting, we showed that learning over-realized dictionaries can be beneficial not only for reducing training and population risk, but also for improving recovery of the underlying model. In the supervised setting, where deep convolutional neural networks were trained for image classification, this project studied the emergence of low-dimensional representations from shallow to deep layers. These representations progressively compressed within-class data while discriminating between-class data. We provided a geometric analysis of the neural collapse phenomenon, which persists across a range of neural network architectures and datasets, and leveraged this understanding to improve neural networks in aspects such as architecture design, training, and transferability.</p>\n<p>The project also explored the use of sparse models to improve the robustness and efficiency of deep neural networks. By leveraging the stable recovery property of sparse modeling, we introduced a sparse convolution model to enhance the robustness of convolutional neural networks against input corruptions and adversarial perturbations during testing. Additionally, we developed efficient proximal gradient methods to solve structured-sparsity optimization problems, which can be applied to compress deep neural networks into slimmer architectures with competitive performance and significant FLOPs reductions.</p>\n<p>This project supported the training and education of graduate research assistants and provided research internship opportunities for both undergraduate and high school students. It also enabled team members to attend scientific meetings, conferences, and seminars to present the project&rsquo;s findings, and contributed material for two courses taught at OSU at both the graduate and undergraduate levels.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 10/14/2024<br>\nModified by: Zhihui&nbsp;Zhu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nDeep convolutional neural networks are a class of mathematical models that provide a variety of machine learning tools with impressive success, often obtaining state-of-the-art results across different fields. Yet, their theoretical understanding and the fundamental ideas behind these algorithms have remained elusive. These questions are essential to recognize and characterize their limitations, to provide guarantees for their performance, and even to develop and engineer improved practical models. This project aimed to address several basic research challengesin understanding deep neural networks by exploiting ideas of parsimonious and sparse representations.\n\n\nThis project considered parsimonious representations in both supervised and unsupervised settings. In the unsupervised setting, we showed that learning over-realized dictionaries can be beneficial not only for reducing training and population risk, but also for improving recovery of the underlying model. In the supervised setting, where deep convolutional neural networks were trained for image classification, this project studied the emergence of low-dimensional representations from shallow to deep layers. These representations progressively compressed within-class data while discriminating between-class data. We provided a geometric analysis of the neural collapse phenomenon, which persists across a range of neural network architectures and datasets, and leveraged this understanding to improve neural networks in aspects such as architecture design, training, and transferability.\n\n\nThe project also explored the use of sparse models to improve the robustness and efficiency of deep neural networks. By leveraging the stable recovery property of sparse modeling, we introduced a sparse convolution model to enhance the robustness of convolutional neural networks against input corruptions and adversarial perturbations during testing. Additionally, we developed efficient proximal gradient methods to solve structured-sparsity optimization problems, which can be applied to compress deep neural networks into slimmer architectures with competitive performance and significant FLOPs reductions.\n\n\nThis project supported the training and education of graduate research assistants and provided research internship opportunities for both undergraduate and high school students. It also enabled team members to attend scientific meetings, conferences, and seminars to present the projects findings, and contributed material for two courses taught at OSU at both the graduate and undergraduate levels.\n\n\n\t\t\t\t\tLast Modified: 10/14/2024\n\n\t\t\t\t\tSubmitted by: ZhihuiZhu\n"
 }
}