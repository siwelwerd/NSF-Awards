{
 "awd_id": "2204115",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: SciDatBench: Principles and Prototypes of Science Data Benchmarks",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922656",
 "po_email": "vchandol@nsf.gov",
 "po_sign_block_name": "Varun Chandola",
 "awd_eff_date": "2022-01-01",
 "awd_exp_date": "2023-10-31",
 "tot_intn_awd_amt": 296877.0,
 "awd_amount": 260242.0,
 "awd_min_amd_letter_date": "2021-11-26",
 "awd_max_amd_letter_date": "2021-11-26",
 "awd_abstract_narration": "Analysis of large scientific data sets requires new research in both the data analysis methods and the information technology hardware and software to use in the analysis. This project is investigating and prototyping a new set of science data benchmarks, dubbed SciDatBench. It establishes a new collection of important and representative big scientific datasets together with typical software implementations of the machine learning algorithms that are needed for best practice analysis. The SciDatBench collection is accompanied by documentation allowing it to be used in the training of researchers in the rapidly evolving Big Data analysis techniques. The project has a potential to impact a broad range of scientific disciplines including eventually material sciences, environmental sciences, life sciences including epidemiology, fusion, particle physics, astronomy, earthquake, and earth sciences, with more than one representative problem from each of these domains.\r\n\r\nSciDatBench generates particular instances of big data analysis benchmarks and establishes a sustainable process for maintaining and enhancing them. This collection includes both standalone examples and end-to-end examples needing multiple components that are seen in the analysis of many science experiments. SciDatBench is affiliated as an approved Science Data working group with the very successful MLPerf activity with 80 organizational members looking at Industry machine learning benchmarks. The state-of-the-art examples in SciDatBench are contributing to progress in scientific discovery that advances the national health, prosperity, and welfare, as stated by NSF's mission. The project is proactively involving under-represented communities in its activities. SciDatBench supports comparative studies and identifies requirements for future cyberinfrastructure to support scientific data analysis.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Geoffrey",
   "pi_last_name": "Fox",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Geoffrey C Fox",
   "pi_email_addr": "vxj6mb@virginia.edu",
   "nsf_id": "000231257",
   "pi_start_date": "2021-11-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia Main Campus",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229044195",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7231",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 260242.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-d58738c8-7fff-82ab-d046-220fca19df04\"><br /> </span></p>\n<p dir=\"ltr\"><span>This project identifies and prototypes a new set of Science Data benchmarks. These are set up with Science discovery metrics as well as those measuring computer performance. This research exploits three major interacting approaches. The first comes from our existing research; the second comes from working with the major industry activity MLCommons (hosting MLPerf), where we have interacted with their working groups for the last five years, gaining a good understanding of the goals and process of this organization plus a rapport with the leadership. MLCommons is a non-profit consortium comprised of 62 corporations, University and Government entities, and over 2000 members.&nbsp; It promotes non-proprietary activities based on three pillars: Machine Learning (ML) benchmarks and models, ML datasets, and ML best practices. Our early work led to the approval of the MLCommons Science Research Data benchmarking working group led by Fox and Hey four years ago. Our third approach comes from a collaboration established with the SciML (Scientific Machine Learning Benchmark) group at the Rutherford-Appleton Laboratory in the UK. Other useful collaborations have been established through the regular MLCommons working group meetings, especially with the Department of Energy Laboratories and NSF supercomputer centers. The project sets up a Science data benchmarking activity, with both a standalone existence outside MLCommons and SciML and the ability to develop MLCommons benchmarks. This allows us to pursue a Science Data benchmarking project that can benefit from MLCommons as much as possible but not be handicapped by possible differences in the preferred process and goals. We pursue performance, quality of scientific discovery, and pedagogical goals, and already we have identified many benchmark datasets that seed our activities. We foster broad community involvement in identifying possible benchmarks and their use in education and research. The heart of the project is a set of virtual working group meetings held approximately every two weeks (70 so far) associated with Science Data and other MLPerf activities. These meetings continue to attract new members and are very active. We document the current five science benchmarks in Figures 1 and 2.</span></p>\n<p dir=\"ltr\"><span><br /></span><span>In addition to the actual benchmarks, we study and develop software and metadata to support benchmarking. We also exploit the tutorial value of the benchmarks with an &ldquo;AI for Science Gymnasium.&rdquo; We support understanding specific benchmarks and the development of AI approaches to other datasets with related analysis processes. Benchmarks are supported by tutorial pages attached to each benchmark entry, and every benchmark has a reference solution. Our work enables enhanced insights into architectures, application design, new algorithms, and emerging AI-specialized hardware. This helps identify requirements for the future cyberinfrastructure needed to support scientific data analysis. The benchmarks will not only record the time to a solution but also possibly multiple measures of the scientific quality of the solution. Finally, the link with MLCommons allows fruitful exchanges between Industry and Research best practices.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The success of this project has encouraged us to generalize this work to collaborate with MLCommons Research (see figure 3) with a focus on Science Foundation models. MLCommons Research is comprised of more than 125 machine learning researchers from the community, who are self-organized into working groups: Algorithms, DataPerf, DynaBench, Medical, Science, and Storage. MLCommons Research comprises both people and resources for facilitating access to, experimentation with, and modification of benchmarks, datasets, and models for machine learning research. Courseware, training, and documentation built with the current project will be continued and extended. MLCommons Research products are curated, reviewed, organized, and presented as an open-source resource connecting industry, government, and academia. We will continue to pursue MLCommon's general goal to accelerate machine learning innovation for the benefit of everyone in the field of scientific advancement. Our process, involving the different parts of MLCommons, is shown in the primary image (figure 4).</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/24/2024<br>\nModified by: Geoffrey&nbsp;C&nbsp;Fox</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314696694_5benchmarks--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314696694_5benchmarks--rgov-800width.jpg\" title=\"List of LCommons Science Benchmarks\"><img src=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314696694_5benchmarks--rgov-66x44.jpg\" alt=\"List of LCommons Science Benchmarks\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 2:A Table of 5 MLCommons Science Benchmarks</div>\n<div class=\"imageCredit\">Geoffrey Fox</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Geoffrey&nbsp;C&nbsp;Fox\n<div class=\"imageTitle\">List of LCommons Science Benchmarks</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314561921_MLC--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314561921_MLC--rgov-800width.jpg\" title=\"MLCommons Research\"><img src=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314561921_MLC--rgov-66x44.jpg\" alt=\"MLCommons Research\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 3: Structure of MLCommons Research including the Science Working Group</div>\n<div class=\"imageCredit\">Geoffrey Fox</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Geoffrey&nbsp;C&nbsp;Fox\n<div class=\"imageTitle\">MLCommons Research</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314824659_Benchmarks2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314824659_Benchmarks2--rgov-800width.jpg\" title=\"First 4 MLCommons Science Benchmarks\"><img src=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314824659_Benchmarks2--rgov-66x44.jpg\" alt=\"First 4 MLCommons Science Benchmarks\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 1: Description of first four MLCommons Science Benchmarks</div>\n<div class=\"imageCredit\">Geoffrey Fox</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Geoffrey&nbsp;C&nbsp;Fox\n<div class=\"imageTitle\">First 4 MLCommons Science Benchmarks</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314982548_ScienceFoundation_Models--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314982548_ScienceFoundation_Models--rgov-800width.jpg\" title=\"MLCommons Science Foundation Model Processes\"><img src=\"/por/images/Reports/POR/2024/2204115/2204115_10693067_1711314982548_ScienceFoundation_Models--rgov-66x44.jpg\" alt=\"MLCommons Science Foundation Model Processes\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 4: The Process by which MLCommons Research examines Foundation Models</div>\n<div class=\"imageCredit\">Geoffrey Fox</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Geoffrey&nbsp;C&nbsp;Fox\n<div class=\"imageTitle\">MLCommons Science Foundation Model Processes</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n \n\n\nThis project identifies and prototypes a new set of Science Data benchmarks. These are set up with Science discovery metrics as well as those measuring computer performance. This research exploits three major interacting approaches. The first comes from our existing research; the second comes from working with the major industry activity MLCommons (hosting MLPerf), where we have interacted with their working groups for the last five years, gaining a good understanding of the goals and process of this organization plus a rapport with the leadership. MLCommons is a non-profit consortium comprised of 62 corporations, University and Government entities, and over 2000 members. It promotes non-proprietary activities based on three pillars: Machine Learning (ML) benchmarks and models, ML datasets, and ML best practices. Our early work led to the approval of the MLCommons Science Research Data benchmarking working group led by Fox and Hey four years ago. Our third approach comes from a collaboration established with the SciML (Scientific Machine Learning Benchmark) group at the Rutherford-Appleton Laboratory in the UK. Other useful collaborations have been established through the regular MLCommons working group meetings, especially with the Department of Energy Laboratories and NSF supercomputer centers. The project sets up a Science data benchmarking activity, with both a standalone existence outside MLCommons and SciML and the ability to develop MLCommons benchmarks. This allows us to pursue a Science Data benchmarking project that can benefit from MLCommons as much as possible but not be handicapped by possible differences in the preferred process and goals. We pursue performance, quality of scientific discovery, and pedagogical goals, and already we have identified many benchmark datasets that seed our activities. We foster broad community involvement in identifying possible benchmarks and their use in education and research. The heart of the project is a set of virtual working group meetings held approximately every two weeks (70 so far) associated with Science Data and other MLPerf activities. These meetings continue to attract new members and are very active. We document the current five science benchmarks in Figures 1 and 2.\n\n\n\nIn addition to the actual benchmarks, we study and develop software and metadata to support benchmarking. We also exploit the tutorial value of the benchmarks with an AI for Science Gymnasium. We support understanding specific benchmarks and the development of AI approaches to other datasets with related analysis processes. Benchmarks are supported by tutorial pages attached to each benchmark entry, and every benchmark has a reference solution. Our work enables enhanced insights into architectures, application design, new algorithms, and emerging AI-specialized hardware. This helps identify requirements for the future cyberinfrastructure needed to support scientific data analysis. The benchmarks will not only record the time to a solution but also possibly multiple measures of the scientific quality of the solution. Finally, the link with MLCommons allows fruitful exchanges between Industry and Research best practices.\n\n\n\n\n\nThe success of this project has encouraged us to generalize this work to collaborate with MLCommons Research (see figure 3) with a focus on Science Foundation models. MLCommons Research is comprised of more than 125 machine learning researchers from the community, who are self-organized into working groups: Algorithms, DataPerf, DynaBench, Medical, Science, and Storage. MLCommons Research comprises both people and resources for facilitating access to, experimentation with, and modification of benchmarks, datasets, and models for machine learning research. Courseware, training, and documentation built with the current project will be continued and extended. MLCommons Research products are curated, reviewed, organized, and presented as an open-source resource connecting industry, government, and academia. We will continue to pursue MLCommon's general goal to accelerate machine learning innovation for the benefit of everyone in the field of scientific advancement. Our process, involving the different parts of MLCommons, is shown in the primary image (figure 4).\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 03/24/2024\n\n\t\t\t\t\tSubmitted by: GeoffreyCFox\n"
 }
}