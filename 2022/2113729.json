{
 "awd_id": "2113729",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Augmented Reality (AR) Web Commerce for Footware Retailers",
 "cfda_num": "47.041, 47.084",
 "org_code": "15030000",
 "po_phone": "7032922174",
 "po_email": "rmehta@nsf.gov",
 "po_sign_block_name": "Rajesh Mehta",
 "awd_eff_date": "2022-03-01",
 "awd_exp_date": "2023-02-28",
 "tot_intn_awd_amt": 256000.0,
 "awd_amount": 276000.0,
 "awd_min_amd_letter_date": "2022-02-18",
 "awd_max_amd_letter_date": "2022-11-03",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to enable footwear retailers to allow consumers to virtually try-on products on their eCommerce websites. Research suggests product fit and look are important parameters for over 66% of eCommerce shoppers. This research and development effort provides a convenicence for consumers  that is readily implemented by existing eCommerce site.  The technology allow customers to measure their foot size using a smartphone camera without a need for an app. The project develops and commercializes a novel Augmented Reality (AR) eCommerce platform, enabling retailers to offer customers unique and immersive online product-try-on  shopping experiences instantly, reduces inventory instory, provides seamless conversion of existing e-Commerce websites to AR-enabled commerce platforms contributing to satisfied customers, higher sales, reduced returns, and decreased cost per sale.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project will employ artificial intelligence, machine learning, augmented realiity (AR), and 3D modelling solutions to create eCommerce platforms that convert physical products into 3D models that work on all smart devices seamlessly.  The successful product will provide high levels of performance and precision in the superimposition of images. Solution must ensure that the AR model  ensures split second inference overlays and tracking streams with lightweight 3D visualizations and > 95% accuracy to fit into the real world. If successful, the proposed AR Commerce platform may be applicable to a variety of applications including retail and medical industries.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Suresh",
   "pi_last_name": "Thankavel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Suresh Thankavel",
   "pi_email_addr": "suresht@wisdominfotech.com",
   "nsf_id": "000845051",
   "pi_start_date": "2022-02-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "AUGRAY LLC",
  "inst_street_address": "18650 W CORPORATE DR STE 120",
  "inst_street_address_2": "",
  "inst_city_name": "BROOKFIELD",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "2622998086",
  "inst_zip_code": "530456344",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "WI05",
  "org_lgl_bus_name": "AUGRAY LLC",
  "org_prnt_uei_num": "",
  "org_uei_num": "KPU6ACD9M5W4"
 },
 "perf_inst": {
  "perf_inst_name": "Augray",
  "perf_str_addr": "Brookfield",
  "perf_city_name": "Brookfield",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "530455840",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "WI05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  },
  {
   "pgm_ele_code": "809100",
   "pgm_ele_name": "SBIR Outreach & Tech. Assist"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079E",
   "pgm_ref_txt": "VISUALIZATION & VIRTUAL DESIGN"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 256000.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 20000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 2\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p>DEVELOP A NOVEL AUGMENTED REALITY (AR) COMMERCE SAS PLATFORM KNOWN AS ?AR WEB TRYON? FOR FOOTWEAR INDUSTRY</p>\n</div>\n</div>\n<div class=\"layoutArea\"></div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 3\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>Key goals: (Technology) </span></p>\n<p><span>USING ARTIFICIAL INTELLIGENCE (AI), MACHINE LEARNING (ML), DATA SCIENCE, 3D MODELLING AND ANIMATION TO CREATE A NOVEL AR COMMERCE PLATFORM. </span></p>\n</div>\n</div>\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>WEB POSSIBILITY </span></p>\n<p><span>A web platform that works with a very minimal memory footprint to allow for seamlessly integration on mobile web browsers. Memory usage is extremely low as 30 MB. </span></p>\n</div>\n<div class=\"column\">\n<p><span>FOR ALL MOBILE DEVICES </span></p>\n<p><span>A lightweight proprietary trained computer vision machine learning model with fast inference times and a pose estimation algorithm makes this suitable for all mobile devices with almost real-time feedback. </span></p>\n</div>\n<div class=\"column\">\n<p><span>3D FOOTWEAR VIZ - LiDAR </span></p>\n<p><span>A scanner app to create 3D models from actual product without a need for expensive hardware scanners, producing smaller file sizes with the details needed for AR experience </span></p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 4\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>R&amp;D Efforts </span></p>\n<p><span>The major goals for phase 1 proof of concept is the following:<br /> a. Collect high quality images of the left and right foot with assortment of backgrounds and socks. </span></p>\n<p><span>b. Process the data to ensure only qualified images are used for annotation.<br /> </span><span>c. Develop a trained FINET model to perform segmentation of the left and right foot, resulting in a set of coordinates that for the polygon (mask). </span></p>\n<p><span>d. Determine pose orientation from the<br /> predicted mask for each foot, through a proprietary pose estimator algorithm.<br /> e.Deploy the model to production and<br /> evaluate the result of real time inference </span><span>04 </span><span>from a live camera feed. </span></p>\n<p><span>f. Deploy the custom trained model into an app (web based and app based) and superimpose the shoe onto each foot from a real-time camera feed. </span></p>\n</div>\n</div>\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><strong>01 </strong><span><strong>Data collection</strong><br /> </span><span>Collect data (foot) with various&nbsp;</span>different foot sizes and backgrounds from the field.</p>\n</div>\n<div class=\"column\">\n<div class=\"page\" title=\"Page 4\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><strong>02 Data Preprocessing<br /> </strong><span>Validate the data against&nbsp;</span>acceptable dataset attributes through semi-supervised analysis to create a clean dataset</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 4\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><strong>03 Annotation</strong></p>\n<p><strong>&nbsp;</strong>Label the left and right foot in each image for training custom UNET ML model&nbsp;</p>\n<p><strong>04 Model Design</strong></p>\n<p><span>Proprietary network FINET(Fast inference Network) derived from UNET with high speed inference (30/s) capabilities</span></p>\n</div>\n</div>\n</div>\n</div>\n<p><strong>05&nbsp;Model Training &amp; Inference</strong></p>\n</div>\n<div class=\"column\">\n<p><span>Train and validate the FINET model from the labeled data set. Current accuracy of the proof of concept model is 75% </span></p>\n<p><strong>06 Pose Estimation </strong></p>\n<p><span>Build a pose estimation algorithm to predict the orientation of the foot from an image without depth information so as to superimpose the model of the shoe precisely </span></p>\n<p><strong>07 Real time superimposition </strong></p>\n<p><span>Build a real time application to test the super impose on a live camera feed and position the foot wear right over the foot inline with its orientation. </span></p>\n<p><strong>08 Web based solution </strong></p>\n<p><span>Port the real time superimposition to work through java script ML libraries </span></p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 5\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>he Architecture </span></p>\n<p><span>The end goal is to superimpose a model of a shoe on top&nbsp;</span>of a person's foot from a real time camera feed. The system works with the help of a rendering framework linked to various system level modules such as the sensor API(s), media devices, computer vision, machine learning model and finally rendering frameworks such as Three.js and A-Frame for a web application and use OpenGL/Metal for app based solution.</p>\n<p><span>The key operating parameter of the system is the inference speed which is targeted at 30 fps of inference and a minimal threshold of 20 renderings per second. This lead to a architecture design of using a very lightweight java script based inference framework compatible to handle the designed FINET model. </span></p>\n<p><span>The FINET model invented and built by AugRay with higher performance in inference time and handles minimalistic parameters as less as 2,158,451 total trainable parameters </span></p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 6\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>THE RESEARCH AND IMPLEMENTATION SO FAR HAS BEEN PROMISING. </span></p>\n<p><span>\"A high quality annotation tool was built with options to annotate using polygons to mark the exact shape of the foot in the image.\" </span></p>\n<p><span>\"FINET model was invented and built to predict the segmentations of any random foot image, after training about 1500 plus annotated images in the dataset\" </span></p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 7\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>CHALLENGES FACED </span></p>\n</div>\n</div>\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>A pose estimator was successfully designed to </span><span>predict the orientation </span><span>of the foot. The was an </span><span>accomplishment </span><span>towards our goal. It was equally tough as the data was just a raw RGB image with no depth data or sensor information which could give us some orientation meta information</span></p>\n</div>\n</div>\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>\"Finally the </span><span>superimposition algorithm was also solved </span><span>and a PC application was built to test the algorithm as superimpose shoes on top of the foot!\" </span></p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 8\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>Achivements as planned</span></p>\n</div>\n</div>\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>FINET TO WORK ON WEB AND APP SOLUTIONS </span></p>\n<p><span>We were able to port it to a lite model using tensorflow_js and TFLiteCovertor for mobile app. The result on the side is a real time superimposition using a web application with WebGL rendering. </span></p>\n</div>\n</div>\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>On the right side is the mobile application that </span><span>superimposes 3D models real time on a camera feed </span><span>through OpenGL and Metal Graphics API </span></p>\n</div>\n</div>\n</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/24/2023<br>\n\t\t\t\t\tModified by: Suresh&nbsp;Thankavel</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2113729/2113729_10785871_1682373353525_NSF_Phase_1_Presentation_11024_5--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2113729/2113729_10785871_1682373353525_NSF_Phase_1_Presentation_11024_5--rgov-800width.jpg\" title=\"File with images of Architecture\"><img src=\"/por/images/Reports/POR/2023/2113729/2113729_10785871_1682373353525_NSF_Phase_1_Presentation_11024_5--rgov-66x44.jpg\" alt=\"File with images of Architecture\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Phase I report with images of the Architecture</div>\n<div class=\"imageCredit\">Suresh Thankavel</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Suresh&nbsp;Thankavel</div>\n<div class=\"imageTitle\">File with images of Architecture</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n\n\n\n\nDEVELOP A NOVEL AUGMENTED REALITY (AR) COMMERCE SAS PLATFORM KNOWN AS ?AR WEB TRYON? FOR FOOTWEAR INDUSTRY\n\n\n\n\n\n\n\n\n\n\nKey goals: (Technology) \n\nUSING ARTIFICIAL INTELLIGENCE (AI), MACHINE LEARNING (ML), DATA SCIENCE, 3D MODELLING AND ANIMATION TO CREATE A NOVEL AR COMMERCE PLATFORM. \n\n\n\n\n\nWEB POSSIBILITY \n\nA web platform that works with a very minimal memory footprint to allow for seamlessly integration on mobile web browsers. Memory usage is extremely low as 30 MB. \n\n\n\nFOR ALL MOBILE DEVICES \n\nA lightweight proprietary trained computer vision machine learning model with fast inference times and a pose estimation algorithm makes this suitable for all mobile devices with almost real-time feedback. \n\n\n\n3D FOOTWEAR VIZ - LiDAR \n\nA scanner app to create 3D models from actual product without a need for expensive hardware scanners, producing smaller file sizes with the details needed for AR experience \n\n\n\n\n\n\n\n\n\nR&amp;D Efforts \n\nThe major goals for phase 1 proof of concept is the following:\n a. Collect high quality images of the left and right foot with assortment of backgrounds and socks. \n\nb. Process the data to ensure only qualified images are used for annotation.\n c. Develop a trained FINET model to perform segmentation of the left and right foot, resulting in a set of coordinates that for the polygon (mask). \n\nd. Determine pose orientation from the\n predicted mask for each foot, through a proprietary pose estimator algorithm.\n e.Deploy the model to production and\n evaluate the result of real time inference 04 from a live camera feed. \n\nf. Deploy the custom trained model into an app (web based and app based) and superimpose the shoe onto each foot from a real-time camera feed. \n\n\n\n\n\n01 Data collection\n Collect data (foot) with various different foot sizes and backgrounds from the field.\n\n\n\n\n\n\n\n02 Data Preprocessing\n Validate the data against acceptable dataset attributes through semi-supervised analysis to create a clean dataset\n\n\n\n\n\n\n\n\n\n03 Annotation\n\n Label the left and right foot in each image for training custom UNET ML model \n\n04 Model Design\n\nProprietary network FINET(Fast inference Network) derived from UNET with high speed inference (30/s) capabilities\n\n\n\n\n\n05 Model Training &amp; Inference\n\n\n\nTrain and validate the FINET model from the labeled data set. Current accuracy of the proof of concept model is 75% \n\n06 Pose Estimation \n\nBuild a pose estimation algorithm to predict the orientation of the foot from an image without depth information so as to superimpose the model of the shoe precisely \n\n07 Real time superimposition \n\nBuild a real time application to test the super impose on a live camera feed and position the foot wear right over the foot inline with its orientation. \n\n08 Web based solution \n\nPort the real time superimposition to work through java script ML libraries \n\n\n\n\n\n\n\n\n\nhe Architecture \n\nThe end goal is to superimpose a model of a shoe on top of a person's foot from a real time camera feed. The system works with the help of a rendering framework linked to various system level modules such as the sensor API(s), media devices, computer vision, machine learning model and finally rendering frameworks such as Three.js and A-Frame for a web application and use OpenGL/Metal for app based solution.\n\nThe key operating parameter of the system is the inference speed which is targeted at 30 fps of inference and a minimal threshold of 20 renderings per second. This lead to a architecture design of using a very lightweight java script based inference framework compatible to handle the designed FINET model. \n\nThe FINET model invented and built by AugRay with higher performance in inference time and handles minimalistic parameters as less as 2,158,451 total trainable parameters \n\n\n\n\n\n\n\n\n\nTHE RESEARCH AND IMPLEMENTATION SO FAR HAS BEEN PROMISING. \n\n\"A high quality annotation tool was built with options to annotate using polygons to mark the exact shape of the foot in the image.\" \n\n\"FINET model was invented and built to predict the segmentations of any random foot image, after training about 1500 plus annotated images in the dataset\" \n\n\n\n\n\n\n\n\n\nCHALLENGES FACED \n\n\n\n\n\nA pose estimator was successfully designed to predict the orientation of the foot. The was an accomplishment towards our goal. It was equally tough as the data was just a raw RGB image with no depth data or sensor information which could give us some orientation meta information\n\n\n\n\n\n\"Finally the superimposition algorithm was also solved and a PC application was built to test the algorithm as superimpose shoes on top of the foot!\" \n\n\n\n\n\n\n\n\n\nAchivements as planned\n\n\n\n\n\nFINET TO WORK ON WEB AND APP SOLUTIONS \n\nWe were able to port it to a lite model using tensorflow_js and TFLiteCovertor for mobile app. The result on the side is a real time superimposition using a web application with WebGL rendering. \n\n\n\n\n\nOn the right side is the mobile application that superimposes 3D models real time on a camera feed through OpenGL and Metal Graphics API \n\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 04/24/2023\n\n\t\t\t\t\tSubmitted by: Suresh Thankavel"
 }
}