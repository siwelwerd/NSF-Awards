{
 "awd_id": "2140826",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Representing and learning stress: Grammatical constraints and neural networks",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032924770",
 "po_email": "rtheodor@nsf.gov",
 "po_sign_block_name": "Rachel M. Theodore",
 "awd_eff_date": "2022-04-15",
 "awd_exp_date": "2026-09-30",
 "tot_intn_awd_amt": 386226.0,
 "awd_amount": 386226.0,
 "awd_min_amd_letter_date": "2022-04-06",
 "awd_max_amd_letter_date": "2022-04-06",
 "awd_abstract_narration": "Languages are systems of remarkable complexity, and linguists and computer scientists have devoted considerable effort to the development of methods for representing those complex systems, as well as computational methods for learning the system of a given language. This effort is driven by the desires to better understand human cognition, and to build better language technologies. This project draws on the theories and methods of both linguistics and computer science to study the learning of word stress, the pattern of relative prominence of the syllables in a word. The stress systems of the world's languages are relatively well described, and there are competing linguistic theories of how they are represented. This project applies learning methods from computer science to find new evidence to distinguish the competing linguistic theories. It also examines systems of language representation that have been developed in computer science and have received relatively little attention by linguists (neural networks). The research will engage undergraduate and graduate linguistics students at a public university. Linguistics has a much higher proportion of female students than computer science, and this project aims to address gender imbalance in STEM.  \r\n\r\nFrom a linguistic perspective, learning stress involves learning hidden structure, parts of the representation that are not present in the observed data and that must be inferred by the learner. A given pattern of prominence over syllables is often consistent with multiple prosodic representations. The approach to hidden structure learning used in this project applies the general technique of Expectation Maximization, which in pilot work achieved good results on a standard test set. Intriguingly, many of the languages that this learner failed on in the test set are ones that are in fact cross-linguistically unattested. This project expands the set of tested languages to include more of the range of systems found cross-linguistically, and further explores the possibility that typological gaps have learning explanations. It compares hypotheses about the constraints responsible for stress placement by comparing how well they support the learning of attested systems, and whether they can help explain typological gaps. Pilot work also found indications that a neural network could learn generalizable representations of the data; the project is further testing this method. All of the software developed in this project is being made freely available, as is a database of the stress systems of the world\u2019s languages.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Joseph",
   "pi_last_name": "Pater",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Joseph Pater",
   "pi_email_addr": "pater@linguist.umass.edu",
   "nsf_id": "000168563",
   "pi_start_date": "2022-04-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gaja",
   "pi_last_name": "Jarosz",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gaja Jarosz",
   "pi_email_addr": "jarosz@linguist.umass.edu",
   "nsf_id": "000721525",
   "pi_start_date": "2022-04-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "100 Venture Way Suite 201",
  "perf_city_name": "Hadley",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010359450",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "147Y00",
   "pgm_ele_name": "Human Networks & Data Sci Res"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  },
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "104Z",
   "pgm_ref_txt": "HNDS-R: Human Networks & Data Sci Resrch"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 386226.0
  }
 ],
 "por": null
}