{
 "awd_id": "2240916",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: MLWiNS:Physical Layer Communication revisited via Deep Learning",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2022-10-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 222334.0,
 "awd_amount": 196176.0,
 "awd_min_amd_letter_date": "2022-09-17",
 "awd_max_amd_letter_date": "2022-09-17",
 "awd_abstract_narration": "Reliable communication is a workhorse of the modern information age. The disciplines of communication, coding, and information theory drive the innovation by designing efficient codes that allow transmissions to be robustly decoded. Progress in near optimal codes is made by individual human ingenuity and breakthroughs have been, befittingly, sporadic, spread over several decades. Deep learning has recently shown strong promise in problems where the space of algorithmic choices is enormous (e.g., Go). This scenario likewise characterizes communication theory. Deep learning methods can play a crucial role in achieving the aforementioned goals. All resulting algorithms will be maintained on an online repository with full source code and documentation. The practical applications of the new codes will be explored in the context of wireless deployments. The research outcomes will be used to develop new undergraduate and graduate curricula. \r\n \r\nThe fundamental nature of the research spans two areas of independent scientific and technical interest: finite block length information theory and the mathematics of deep learning. This project aims to bring the tools of deep learning to design a new family of encoding and decoding methods for canonical communication models; the codes so generated are naturally built for finite block lengths. In parallel, the viewpoint of neural network architectures as encoding and decoding procedures provides a unique vantage point to study their mathematical properties.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Pramod",
   "pi_last_name": "Viswanath",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Pramod Viswanath",
   "pi_email_addr": "pramodv@princeton.edu",
   "nsf_id": "000486611",
   "pi_start_date": "2022-09-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "1 NASSAU HALL",
  "perf_city_name": "PRINCETON",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "021Z",
   "pgm_ref_txt": "Industry Partnerships"
  },
  {
   "pgm_ref_code": "8585",
   "pgm_ref_txt": "NSF/Intel Partnership Projects"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 196176.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Motivated by the successes of deep learning in mathematically well defined and extremely challenging tasks (e.g., protein folding), we posit that deep learning methods will play a crucial role in solving&nbsp;the critical goals of wireless coding theory (development of new and efficient decoders and the more ambitious goals of inventing new coding families altogether); this research project is based on this hypothesis. In this project we thoroughly test this hypothesis and demonstrate both practical (new decoders and coding families) and scientific (systematic methods to bring deep learning and coding theory together) outcomes.&nbsp;</p>\r\n<div class=\"page\" title=\"Page 2\">\r\n<div class=\"section\">\r\n<div class=\"layoutArea\">\r\n<div class=\"column\">\r\n<p><span>The key scientific question we ask is how can we automate the search for good codes? One straightforward approach is by parameterizing and learning both the encoder and decoder using neural networks. However, constructing effective non-linear codes using this approach is highly challenging: we demonstrate that naively parameterizing with off-the-shelf neural architectures often results in performance worse than even repetition codes</span><span>. Rather, a more promising approach is to design neural architectures that enable structured redundancy. Specifically, our work delves into the innovative intersection between algebraic coding theory and machine learning by exploring non-linear generalizations of polarization-driven and broader Reed-Solomon code family structures. This can be achieved by parameterizing each kernel by a neural network, combining the information-theoretic properties of polarization-driven and algebraic structures of the broader family of Reed-Solomon code structures with the adaptability and learning capabilities of deep learning. This interplay between algebraic coding structures and deep learning is an unchartered territory which we systematically map in this project. This is done through our invention of KO codes (nonlinear generalization of both Reed Solomon and Polar codes) and DeepPolar codes (nonlinear generalization of Polar codes).&nbsp;</span></p>\r\n<p><span>We also constructed new decoder families that are state of the art for existing families of codes, including Turbo codes, Reed-Solomon codes and Polar codes. Through this research, we demystify the deep learning procedure of curriculum learning by studying it in a concrete and mathematical context of decoding. The practical outcomes of this project are being transitioned into practice: one of the students who worked on this project joined as a founding engineer at a startup building 5G models and other students have interned at top wireless companies in the country.&nbsp;</span></p>\r\n</div>\r\n</div>\r\n</div>\r\n</div>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/10/2025<br>\nModified by: Pramod&nbsp;Viswanath</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nMotivated by the successes of deep learning in mathematically well defined and extremely challenging tasks (e.g., protein folding), we posit that deep learning methods will play a crucial role in solvingthe critical goals of wireless coding theory (development of new and efficient decoders and the more ambitious goals of inventing new coding families altogether); this research project is based on this hypothesis. In this project we thoroughly test this hypothesis and demonstrate both practical (new decoders and coding families) and scientific (systematic methods to bring deep learning and coding theory together) outcomes.\r\n\r\n\r\n\r\n\r\n\n\nThe key scientific question we ask is how can we automate the search for good codes? One straightforward approach is by parameterizing and learning both the encoder and decoder using neural networks. However, constructing effective non-linear codes using this approach is highly challenging: we demonstrate that naively parameterizing with off-the-shelf neural architectures often results in performance worse than even repetition codes. Rather, a more promising approach is to design neural architectures that enable structured redundancy. Specifically, our work delves into the innovative intersection between algebraic coding theory and machine learning by exploring non-linear generalizations of polarization-driven and broader Reed-Solomon code family structures. This can be achieved by parameterizing each kernel by a neural network, combining the information-theoretic properties of polarization-driven and algebraic structures of the broader family of Reed-Solomon code structures with the adaptability and learning capabilities of deep learning. This interplay between algebraic coding structures and deep learning is an unchartered territory which we systematically map in this project. This is done through our invention of KO codes (nonlinear generalization of both Reed Solomon and Polar codes) and DeepPolar codes (nonlinear generalization of Polar codes).\r\n\n\nWe also constructed new decoder families that are state of the art for existing families of codes, including Turbo codes, Reed-Solomon codes and Polar codes. Through this research, we demystify the deep learning procedure of curriculum learning by studying it in a concrete and mathematical context of decoding. The practical outcomes of this project are being transitioned into practice: one of the students who worked on this project joined as a founding engineer at a startup building 5G models and other students have interned at top wireless companies in the country.\r\n\r\n\r\n\r\n\r\n\n\n\t\t\t\t\tLast Modified: 02/10/2025\n\n\t\t\t\t\tSubmitted by: PramodViswanath\n"
 }
}