{
 "awd_id": "2151021",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: EAGER: Real-time Strategies and Synchronized Time Distribution Mechanisms for Enhanced Exascale Performance-Portability and Predictability",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2022-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2022-05-20",
 "awd_max_amd_letter_date": "2022-05-20",
 "awd_abstract_narration": "Advances throughout science and engineering have for several decades been driven by High Performance Computing (HPC), with the pace of discovery accelerating in concert with continued innovation in computing capabilities. But as semiconductor technology now faces fundamental physical limits, even while large-scale systems are reaching warehouse scales, new approaches are becoming essential to achieving efficient use of computing resources. In particular, given this divergence of scales, HPC systems have necessarily become more distributed and  asynchronous (in the sense that system clocks are asynchronous), resulting in increasingly variable and unpredictable execution. While these effects are recognized as critical hindrances to HPC performance, the mechanisms are not yet fully understood. What is known, however, is that much HPC infrastructure is tasked with dealing with inefficiency derived from asynchrony, variability, and unpredictability, leading to a deep and complex hardware/software support stack. The project team's hypothesis is that while each stack element provides a local solution, it may also exacerbate the global problem: that complexity has resulted in more variability, not less, and made determining its causes more difficult. This project explores the possibility of reversing the trend of ever-increasing complexity by removing and simplifying support layers. This strategy\u2019s achievable gains remain limited, however, while the underlying cause, execution asynchrony, remains unaddressed. The approach begins by leveraging recently developed technology that enables clocks to remain extremely accurate even when distributed on a planetary scale. Such accurate, distributed clocks serve to underpin a virtuous cycle where synchrony establishes baseline predictability, which, in turn, reduces variability, and at each stage of the cycle enables reduction in the complexity of the support stack. A benefit of this approach is that the individual steps are largely simple and can be applied directly to existing software systems. \r\n\r\nThis one-year project aims to obtain early findings and practical demonstrations for the importance of synchrony and predictability to increase HPC compute efficiency and thereby improve large-scale program execution. Five tasks are conducted. The first is to demonstrate the feasibility of accurate clock distribution by augmenting existing HPC network infrastructure. The second is to demonstrate the application of synchrony in the establishing a virtuous cycle enabling  simplifications to the software/system support stack. The third is to devise mechanisms to model, measure, and validate systems using the proposed methods. The fourth is to investigate the relative benefits of applying the synchrony-based virtuous cycle with respect to various application classes. The fifth is to demonstrate the overall efficacy of the proposed approach through a case study involving a production application. Overall, the project works to  determine whether added synchronization through accurate clocks enables significant improvements to HPC computations in terms of how efficiently they use computational resources.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Martin",
   "pi_last_name": "Herbordt",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Martin C Herbordt",
   "pi_email_addr": "herbordt@bu.edu",
   "nsf_id": "000112548",
   "pi_start_date": "2022-05-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "West",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Richard West",
   "pi_email_addr": "richwest@cs.bu.edu",
   "nsf_id": "000211081",
   "pi_start_date": "2022-05-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "881 COMMONWEALTH AVE",
  "perf_city_name": "BOSTON",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151300",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Advances throughout science and engineering have for several decades been driven by High Performance Computing (HPC), now accelerated by the use of Machine Learning (ML), with the pace of discovery correlated with continued innovation in computing technology. But as semiconductor technology now faces fundamental physical limits, even as computing resources are consolidating into ever larger systems reaching warehouse scales, new approaches are becoming essential to achieving efficient use of those computing resources. In particular, given this divergence of scales, HPC systems have necessarily become distributed and asynchronous, in the sense of independent clocks, resulting in increasingly variable and unpredictable execution. While these effects -- often apparent as load imbalance or processing skew -- are recognized as critical hindrances to HPC performance, the mechanisms are not yet fully understood. What is known, however, is that much HPC infrastructure is tasked with dealing with inefficiency derived from asynchrony, variability, and unpredictability, leading to a deep and complex hardware/software support stack. Our hypothesis is that while each stack element provides a local solution, collectively they may exacerbate the global problem. The complexity has resulted in more variability, not less, and made determining its causes more difficult.</p>\n<p>In this project we explore the possibility of reversing this trend of ever increasing complexity by removing and simplifying layers. Our hypothesis is that it is possible to create a virtuous cycle: that creating the ability to predict fine-grained execution times can be used to reduce variability, which, in turn, can be used to simplify the support stack and improve scheduling, thus enabling more yet accurate prediction, and so on. Underpinning each iteration of this cycle is addressing execution asynchrony, which we propose to do by leveraging recently developed technologies that enable clocks to remain extremely accurate even when distributed on a planetary scale. Each iteration results in more efficient computing with no apparent limit until skews are reduced to microseconds.</p>\n<p>We have obtained a number of findings that support this hypothesis, and taken initial steps in creating a framework, called TimeLord, wherein variability is characterized, predicted, and exploited.</p>\n<p>The first thrust was to confirm substantial inefficiency in HPC program execution. Our initial study was with respect to a set of popular programs with substantial use of MPI collectives and synchronization and found that an average of 40% of processor execution time was unproductive.</p>\n<p>The second thrust was to characterize the skew and thus determine how it should be predicted and exploited. The first finding was that there was no systematic lag, e.g., as would be caused by having a slow node, which would perhaps make static load balancing preferable to TimeLord. The second finding was the scale of exploitable skew. We found that this increased as the granularity decreased and points to the type of solution required to reduce or exploit this skew. The third finding was that some skew was due to inefficiencies in the MPI progress engine. All three findings support the hypothesis as to effective methods for addressing variability.</p>\n<p>The third thrust was to create TimeLord, a runtime library that can be embedded transparently into MPI runtimes. TimeLord serves three purposes: (i) to serve as a measurement tool, (ii) to be an experiment framework to explore the major ideas in this project (e.g., prediction and mechanisms for pre-emption), and (iii) to serve as a mechanism to exploit process skew and load imbalance through the execution of &ldquo;extra work&rdquo; (in place of polling and other idling). Notably, the extra work was executed with near-zero overhead, in user space, and completely transparently to the application and system software (OS, middleware, scheduler, etc.).</p>\n<p>The fourth thrust was to, within TimeLord, devise and implement methods for exploiting idle time with extra work. These were, respectively, based on prediction, pre-emption, and a hybrid of the two. A surprising result was that pre-emption yielded the best performance. To be determined is whether this extends to larger clusters, other workloads, and more complex extra work. For prediction we created a new empirically-based algorithm, Adaptive Deadline Estimation, which monitors skew from previous iterations.</p>\n<p>The fifth thrust investigaged system-wide mechanisms for predictable task and dataflow scheduling, in particular, the use of time-aware bus and networking technologies to enforce predictable end-to-end processing and the transfer of data between host nodes. This approach was shown to greatly outperform transfer predictability using existing Linux-based approaches for end-to-end data exchange. Further work combined network, task, and interrupt scheduling to meet end-to-end service requirements in terms of throughput and delay requirements.</p>\n<p>Besides the technical broader impacts already described, other broader impacts included training and outreach. Five students were supported directly by this project with four more contributing through Master&rsquo;s or undergraduate research projects.</p><br>\n<p>\n Last Modified: 09/28/2024<br>\nModified by: Martin&nbsp;C&nbsp;Herbordt</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAdvances throughout science and engineering have for several decades been driven by High Performance Computing (HPC), now accelerated by the use of Machine Learning (ML), with the pace of discovery correlated with continued innovation in computing technology. But as semiconductor technology now faces fundamental physical limits, even as computing resources are consolidating into ever larger systems reaching warehouse scales, new approaches are becoming essential to achieving efficient use of those computing resources. In particular, given this divergence of scales, HPC systems have necessarily become distributed and asynchronous, in the sense of independent clocks, resulting in increasingly variable and unpredictable execution. While these effects -- often apparent as load imbalance or processing skew -- are recognized as critical hindrances to HPC performance, the mechanisms are not yet fully understood. What is known, however, is that much HPC infrastructure is tasked with dealing with inefficiency derived from asynchrony, variability, and unpredictability, leading to a deep and complex hardware/software support stack. Our hypothesis is that while each stack element provides a local solution, collectively they may exacerbate the global problem. The complexity has resulted in more variability, not less, and made determining its causes more difficult.\n\n\nIn this project we explore the possibility of reversing this trend of ever increasing complexity by removing and simplifying layers. Our hypothesis is that it is possible to create a virtuous cycle: that creating the ability to predict fine-grained execution times can be used to reduce variability, which, in turn, can be used to simplify the support stack and improve scheduling, thus enabling more yet accurate prediction, and so on. Underpinning each iteration of this cycle is addressing execution asynchrony, which we propose to do by leveraging recently developed technologies that enable clocks to remain extremely accurate even when distributed on a planetary scale. Each iteration results in more efficient computing with no apparent limit until skews are reduced to microseconds.\n\n\nWe have obtained a number of findings that support this hypothesis, and taken initial steps in creating a framework, called TimeLord, wherein variability is characterized, predicted, and exploited.\n\n\nThe first thrust was to confirm substantial inefficiency in HPC program execution. Our initial study was with respect to a set of popular programs with substantial use of MPI collectives and synchronization and found that an average of 40% of processor execution time was unproductive.\n\n\nThe second thrust was to characterize the skew and thus determine how it should be predicted and exploited. The first finding was that there was no systematic lag, e.g., as would be caused by having a slow node, which would perhaps make static load balancing preferable to TimeLord. The second finding was the scale of exploitable skew. We found that this increased as the granularity decreased and points to the type of solution required to reduce or exploit this skew. The third finding was that some skew was due to inefficiencies in the MPI progress engine. All three findings support the hypothesis as to effective methods for addressing variability.\n\n\nThe third thrust was to create TimeLord, a runtime library that can be embedded transparently into MPI runtimes. TimeLord serves three purposes: (i) to serve as a measurement tool, (ii) to be an experiment framework to explore the major ideas in this project (e.g., prediction and mechanisms for pre-emption), and (iii) to serve as a mechanism to exploit process skew and load imbalance through the execution of extra work (in place of polling and other idling). Notably, the extra work was executed with near-zero overhead, in user space, and completely transparently to the application and system software (OS, middleware, scheduler, etc.).\n\n\nThe fourth thrust was to, within TimeLord, devise and implement methods for exploiting idle time with extra work. These were, respectively, based on prediction, pre-emption, and a hybrid of the two. A surprising result was that pre-emption yielded the best performance. To be determined is whether this extends to larger clusters, other workloads, and more complex extra work. For prediction we created a new empirically-based algorithm, Adaptive Deadline Estimation, which monitors skew from previous iterations.\n\n\nThe fifth thrust investigaged system-wide mechanisms for predictable task and dataflow scheduling, in particular, the use of time-aware bus and networking technologies to enforce predictable end-to-end processing and the transfer of data between host nodes. This approach was shown to greatly outperform transfer predictability using existing Linux-based approaches for end-to-end data exchange. Further work combined network, task, and interrupt scheduling to meet end-to-end service requirements in terms of throughput and delay requirements.\n\n\nBesides the technical broader impacts already described, other broader impacts included training and outreach. Five students were supported directly by this project with four more contributing through Masters or undergraduate research projects.\t\t\t\t\tLast Modified: 09/28/2024\n\n\t\t\t\t\tSubmitted by: MartinCHerbordt\n"
 }
}