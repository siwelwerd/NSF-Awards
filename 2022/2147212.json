{
 "awd_id": "2147212",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FAI: Breaking the Tradeoff Barrier in Algorithmic Fairness",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2022-06-01",
 "awd_exp_date": "2025-05-31",
 "tot_intn_awd_amt": 392992.0,
 "awd_amount": 392992.0,
 "awd_min_amd_letter_date": "2022-02-11",
 "awd_max_amd_letter_date": "2022-02-11",
 "awd_abstract_narration": "In order to be robust and trustworthy, algorithmic systems need to usefully serve diverse populations of users. Standard machine learning methods can easily fail in this regard, e.g. by optimizing for majority populations represented within their training data at the expense of worse performance on minority populations. A large literature on \"algorithmic fairness\" has arisen to address this widespread problem. However, at a technical level, this literature has viewed various technical notions of \"fairness\" as constraints, and has therefore viewed \"fair learning\" through the lens of constrained optimization. Although this has been a productive viewpoint from the perspective of algorithm design, it has led to tradeoffs being centered as the central object of study in \"fair machine learning\". In the standard framing, adding new protected populations, or quantitatively strengthening fairness constraints, necessarily leads to decreased accuracy overall and within each group. This has the effect of pitting the interests of different stakeholders against one another, and making it difficult to build consensus around \"fair machine learning\" techniques. The over-arching goal of this project is to break through this \"fairness/accuracy tradeoff\" paradigm. \r\n\r\nSpecifically, we will draw on ideas from learning theory and uncertainty estimation to introduce notions of fairness that can be satisfied in ways that are monotonically error improving. For example, if it is discovered that a deployed model has error that is unacceptably high on some population, our aim will be to find ways to decrease the error on that population without increasing the error on any other population. We also aim to find methods that do not require identifying which groups might be disadvantaged by a particular application of machine learning ahead of time, since this can be very hard to predict. Instead, we will develop methods to dynamically update models as it is discovered that they are performing poorly on populations of interest. Finally, rather than talking about \"fairness\" of predictive models in the abstract, we will aim to formulate and implement notions of fairness that have meaning in the context of particular downstream applications, and find methods of training upstream predictive methods that will guarantee these kinds of fairness when the predictive models are deployed in these downstream use case. In addition to research papers and software, this project will develop human capital by training PhD students to be leading researchers in trustworthy machine learning. It will also develop educational materials aimed at researchers, students, and the general public.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "AARON",
   "pi_last_name": "ROTH",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "AARON ROTH",
   "pi_email_addr": "aaroth@cis.upenn.edu",
   "nsf_id": "000624673",
   "pi_start_date": "2022-02-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Kearns",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Kearns",
   "pi_email_addr": "mkearns@cis.upenn.edu",
   "nsf_id": "000440175",
   "pi_start_date": "2022-02-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": "200 S. 33rd Street, Moore",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046314",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 392992.0
  }
 ],
 "por": null
}