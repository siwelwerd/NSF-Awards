{
 "awd_id": "2132060",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RII Track-4:NSF: Safety Validation of Autonomous Systems from Multiple Sources of Information",
 "cfda_num": "47.083",
 "org_code": "01060100",
 "po_phone": "7032928246",
 "po_email": "pbentzvi@nsf.gov",
 "po_sign_block_name": "Pinhas Ben-Tzvi",
 "awd_eff_date": "2022-02-01",
 "awd_exp_date": "2024-01-31",
 "tot_intn_awd_amt": 199822.0,
 "awd_amount": 199822.0,
 "awd_min_amd_letter_date": "2022-01-27",
 "awd_max_amd_letter_date": "2022-01-27",
 "awd_abstract_narration": "With the rapid growth of interest in the use of artificial intelligence (AI) in autonomy, it is critical to revolutionize safety validation approaches that reason about the safety behaviors of a complex AI-enabled autonomous system. The goal of this project is to build trust in AI-enabled complex systems for safety-critical applications. This trust could be built by means of some offline or online validation processes. The drawback of online verification methods is that they require some form of real-world deployments which could be unsafe and risky. Typically, it is of great interest to reveal possible failure scenarios in a simulated environment before deploying an AI-based decision-making system into the real world. Since the space of failure events and corner cases is extensive in complex systems, the validation process might be very time-consuming as a huge number of experiments are required for safety validation. This project aims to develop approaches that capture information from multiple sources to significantly speed up the validation process and reduce the overall computational cost. Through the collaboration with the Stanford Center for AI Safety, the PI and graduate trainee will gain invaluable training opportunities that will help to build a strong STEM research and education partnership between West Virginia University (WVU) and Stanford.\r\n\r\nThe overarching objective of this project is to develop algorithms for safety validation of autonomous systems that reason about the safety behaviors of autonomous systems from multiple sources of information. The central philosophy behind this work is that a cyber-physical system (CPS) or a robot can query data from multiple sources, including different levels of granularity in simulation, offline or online real-world data, and/or human expert inputs. Currently, there is no rigorous mechanism to reason about the safety behaviors of a learning-enabled decision-making system that optimally considers data from different sources of information. This research will combine the decision making under uncertainty and formal methods expertise at the Stanford Center for AI Safety and the PI's expertise in machine learning and data-driven optimization techniques to arrive at safety validation frameworks that leverage data from multiple sources. The PI and his students will develop tools from data-driven optimization and reinforcement learning algorithms to identify failure events from multiple sources of information. The proposed algorithms will be applied to a suite of simulated environments for autonomous driving. This research will significantly extend the tools and open-source software for the safety validation of autonomous systems. Through collaboration with the Stanford Center for AI Safety, the PI will maintain ties between academia and industry, open new avenues for joint proposal writing, joint journal publications, and student exchange programs between Stanford and WVU. The proposal will be integrated into an educational plan that involves undergraduate and graduate students in research and enriches the curriculum of robotics and engineering at West Virginia University (WVU).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "O/D",
 "org_dir_long_name": "Office Of The Director",
 "div_abbr": "OIA",
 "org_div_long_name": "OIA-Office of Integrative Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ali",
   "pi_last_name": "Baheri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ali Baheri",
   "pi_email_addr": "akbeme@rit.edu",
   "nsf_id": "000855167",
   "pi_start_date": "2022-01-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "West Virginia University Research Corporation",
  "inst_street_address": "886 CHESTNUT RIDGE ROAD",
  "inst_street_address_2": "",
  "inst_city_name": "MORGANTOWN",
  "inst_state_code": "WV",
  "inst_state_name": "West Virginia",
  "inst_phone_num": "3042933998",
  "inst_zip_code": "265052742",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WV02",
  "org_lgl_bus_name": "WEST VIRGINIA UNIVERSITY RESEARCH CORPORATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "M7PNRH24BBM8"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University, The Center for AI Safety",
  "perf_str_addr": "496 Lomita Mall",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943054008",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "196Y00",
   "pgm_ele_name": "EPSCoR RII: EPSCoR Research Fe"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 199822.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aimed to enhance the safety and reliability of AI systems critical in applications such as autonomous vehicles and robotics. The focal point of our efforts was the innovative use of multi-fidelity simulations for the efficient and accurate verification of these systems' safety protocols.</p>\n<p>The principal achievement of this research was the development of a methodology that utilizes simulations across different fidelity levels for the safety evaluation of learning-based systems. This method significantly reduced the computational resources typically required for exhaustive verification processes. Collaborations with the Stanford Center for AI Safety were instrumental in advancing our understanding and application of multi-fidelity verification techniques.</p>\n<p>Our empirical investigations, which ranged from simplified simulated environments to complex real-world application scenarios like autonomous driving, validated the efficacy and applicability of our approach across diverse domains reliant on AI technologies.</p>\n<p>The broader impact of our work is multifaceted: By making it more feasible to thoroughly test AI systems, our project contributes to enhancing the overall safety of technologies that are becoming increasingly integral to our daily lives, such as self-driving cars and automated medical diagnostics. Furthermore, reducing the computational cost of safety testing not only accelerates the development cycle of AI technologies but also makes it economically more viable. This efficiency can lead to quicker innovations and more rapid deployment of safe AI applications in the market.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/19/2024<br>\nModified by: Ali&nbsp;Baheri</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project aimed to enhance the safety and reliability of AI systems critical in applications such as autonomous vehicles and robotics. The focal point of our efforts was the innovative use of multi-fidelity simulations for the efficient and accurate verification of these systems' safety protocols.\n\n\nThe principal achievement of this research was the development of a methodology that utilizes simulations across different fidelity levels for the safety evaluation of learning-based systems. This method significantly reduced the computational resources typically required for exhaustive verification processes. Collaborations with the Stanford Center for AI Safety were instrumental in advancing our understanding and application of multi-fidelity verification techniques.\n\n\nOur empirical investigations, which ranged from simplified simulated environments to complex real-world application scenarios like autonomous driving, validated the efficacy and applicability of our approach across diverse domains reliant on AI technologies.\n\n\nThe broader impact of our work is multifaceted: By making it more feasible to thoroughly test AI systems, our project contributes to enhancing the overall safety of technologies that are becoming increasingly integral to our daily lives, such as self-driving cars and automated medical diagnostics. Furthermore, reducing the computational cost of safety testing not only accelerates the development cycle of AI technologies but also makes it economically more viable. This efficiency can lead to quicker innovations and more rapid deployment of safe AI applications in the market.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 02/19/2024\n\n\t\t\t\t\tSubmitted by: AliBaheri\n"
 }
}