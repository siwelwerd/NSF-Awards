{
 "awd_id": "2217028",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: PPoSS: Planning: Cross-layer Coordination and Optimization for Scalable and Sparse Tensor Networks (CROSS)",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Damian Dechev",
 "awd_eff_date": "2022-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 62500.0,
 "awd_amount": 62500.0,
 "awd_min_amd_letter_date": "2022-06-29",
 "awd_max_amd_letter_date": "2022-06-29",
 "awd_abstract_narration": "High-dimensional data computation or analytics are gaining importance in many domains, such as quantum chemistry/physics, quantum circuit simulation, brain processing, social networks, healthcare and machine/deep learning, to name a few. Tensors, a representation of high-dimensional data, are playing an increasingly critical role, and so are tensor methods. Tensor decompositions or factorizations of low-dimensional data (three to five dimensions) have been extensively studied over the past years from a high-performance computing and also compiler and computer architecture angles for their computational core operations, while tensor networks targeting very high-dimensional data (over ten dimensions) and extracting physically meaningful latent variables are underdeveloped because of their complicated mathematical nature, extremely high computational complexity, and more domain-dependent challenges. The project\u2019s novelties are manifold: 1) memory heterogeneity-aware representations with algorithm and system optimizations, which could be adopted to solve other problems such as irregular applications and sparse numerical methods; 2) hardware-software co-design of specialized, sparse-tensor network-accelerator architectures, that are among the first hardware implementations of sparse-tensor networks. The project\u2019s impacts are 1) advancing state-of-the-art tensor decomposition studies to model true higher-order and sparse data; 2) triggering a closer long-term collaboration ranging from academia to research labs to industry by studying solicitous applications; 3) bringing appropriate educational opportunities.\r\n\r\nThis project proposes Cross-layer cooRdination and Optimization for Scalable and Sparse-Tensor Networks (CROSS) for heterogeneous systems that are equipped with various types of accelerators, such as GPUs, TPUs and FPGAs, as well as heterogeneous memories with dynamic and non-volatile random-access memories (DRAM+NVRAM). This research aims to study the sparsity in widely used tensor networks by introducing constraints, regularization, dictionaries, and/or domain knowledge for better data compression, faster computation, lower memory usage and better interpretability. Besides the sparsity challenges, sparse-tensor networks also suffer from the curse of dimensionality, aggravated data randomness and irregular program and memory access behaviors. This planning project conducts preliminary research that aims to address these challenges from four perspectives: (1) memory heterogeneity-aware representations and data (re-)arrangement, (2) balanced sparse tensor contraction (SpTC) algorithms with smart page arrangement, (3) memoization and intelligent allocation to reduce computational cost, and (4) specialized accelerator architectures for sparse-tensor networks. The optimized sparse tensor networks will encompass efforts from high-performance computing, algorithms, compilers, computer architecture and performance modeling and will be tested under multiple application scenarios.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lizhong",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lizhong Chen",
   "pi_email_addr": "chenliz@oregonstate.edu",
   "nsf_id": "000689417",
   "pi_start_date": "2022-06-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon State University",
  "inst_street_address": "1500 SW JEFFERSON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CORVALLIS",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5417374933",
  "inst_zip_code": "973318655",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OR04",
  "org_lgl_bus_name": "OREGON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MZ4DYXE1SL98"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon State University",
  "perf_str_addr": "",
  "perf_city_name": "Corvallis",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "973312140",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OR04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 62500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Tensors are multidimensional arrays used to represent data in many scientific fields, such as machine learning, quantum chemistry, and quantum physics. These tensors, particularly high-dimensional (high-order) ones, often contain a small percentage of nonzero elements and are thus referred to as sparse tensors. This multi-university collaborative project explores cross-layer coordination and optimization for scalable and sparse tensor networks on heterogeneous systems, which are prevalently equipped with various types of accelerators and processors, as well as heterogeneous memory, including dynamic and non-volatile random-access memory.</p>\r\n<p>As a planning project to support the development of a larger initiative, this effort has conducted preliminary research addressing challenges from four aspects: (1) memory heterogeneity-aware representations and data arrangement, (2) balanced sparse tensor contraction algorithms with smart page arrangement, (3) memoization and intelligent allocation to reduce computational cost, and (4) specialized accelerator architectures for sparse tensor networks. Six application domains&mdash;quantum chemistry, quantum physics, quantum circuit simulation, brain processing, social networks, healthcare, and machine/deep learning&mdash;were targeted for acceleration on distributed heterogeneous systems. Our team at Oregon State University primarily focuses on aspect (4) while also assisting collaborative teams at NCSU and UC Merced with aspects (1) through (3).</p>\r\n<p>Specifically, we have designed, implemented, and evaluated a hardware accelerator for high-order sparse tensor contraction. This work includes proposing a novel architecture for computing tensor contractions with modular flexibility in both the arithmetic decomposition and the data formats; developing a prototype hardware implementation to decompose an arbitrary-order tensor contraction into a set of dot products; and demonstrating the efficacy of the proposed architecture through experiments comparing its performance to that of a common data science software package for sparse tensor operations. These outcomes have established a solid foundation for the upcoming full five-year project.</p>\r\n<p>This project also provided training and professional development to a Ph.D. student, who acquired valuable skills and experience in hardware design, software tool development, performance evaluation, analysis, and presentation. Findings from this research have been disseminated on publicly available platforms. To further enhance the project&rsquo;s impact, a dedicated workshop was organized, featuring a keynote address, an invited talk, three technical talks by students, and a panel/open discussion on the computational and optimization challenges of sparse tensors in various application domains.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/26/2024<br>\nModified by: Lizhong&nbsp;Chen</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nTensors are multidimensional arrays used to represent data in many scientific fields, such as machine learning, quantum chemistry, and quantum physics. These tensors, particularly high-dimensional (high-order) ones, often contain a small percentage of nonzero elements and are thus referred to as sparse tensors. This multi-university collaborative project explores cross-layer coordination and optimization for scalable and sparse tensor networks on heterogeneous systems, which are prevalently equipped with various types of accelerators and processors, as well as heterogeneous memory, including dynamic and non-volatile random-access memory.\r\n\n\nAs a planning project to support the development of a larger initiative, this effort has conducted preliminary research addressing challenges from four aspects: (1) memory heterogeneity-aware representations and data arrangement, (2) balanced sparse tensor contraction algorithms with smart page arrangement, (3) memoization and intelligent allocation to reduce computational cost, and (4) specialized accelerator architectures for sparse tensor networks. Six application domainsquantum chemistry, quantum physics, quantum circuit simulation, brain processing, social networks, healthcare, and machine/deep learningwere targeted for acceleration on distributed heterogeneous systems. Our team at Oregon State University primarily focuses on aspect (4) while also assisting collaborative teams at NCSU and UC Merced with aspects (1) through (3).\r\n\n\nSpecifically, we have designed, implemented, and evaluated a hardware accelerator for high-order sparse tensor contraction. This work includes proposing a novel architecture for computing tensor contractions with modular flexibility in both the arithmetic decomposition and the data formats; developing a prototype hardware implementation to decompose an arbitrary-order tensor contraction into a set of dot products; and demonstrating the efficacy of the proposed architecture through experiments comparing its performance to that of a common data science software package for sparse tensor operations. These outcomes have established a solid foundation for the upcoming full five-year project.\r\n\n\nThis project also provided training and professional development to a Ph.D. student, who acquired valuable skills and experience in hardware design, software tool development, performance evaluation, analysis, and presentation. Findings from this research have been disseminated on publicly available platforms. To further enhance the projects impact, a dedicated workshop was organized, featuring a keynote address, an invited talk, three technical talks by students, and a panel/open discussion on the computational and optimization challenges of sparse tensors in various application domains.\r\n\n\n\t\t\t\t\tLast Modified: 12/26/2024\n\n\t\t\t\t\tSubmitted by: LizhongChen\n"
 }
}