{
 "awd_id": "2245634",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "I-Corps: Developing an eye tracking algorithm that will assess the changes in depression, anxiety and other neurological insults",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032922061",
 "po_email": "jcamelio@nsf.gov",
 "po_sign_block_name": "Jaime A. Camelio",
 "awd_eff_date": "2022-12-01",
 "awd_exp_date": "2023-11-30",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2022-11-21",
 "awd_max_amd_letter_date": "2022-11-21",
 "awd_abstract_narration": "The broader impact/commercial potential of this I-Corps project is the development of a comprehensive communication-detection solution using eye tracking for patients with communication loss due to neurological insults like stroke. Communication loss also can be accompanied by loss of motor control, followed by decrease in social interaction and depression. Such patients are at high risk for another stroke or other neurological insults. There is no single solution available to the speech pathologists who use paper-pencil tests as standard practice for diagnosis and rehabilitation. There is also no monitoring or mental health help provided at follow-up despite the fact that these communication disorders in older adults are a chronic condition and require any solution to be continuously adapted to their needs. The intended primary customers are speech pathologists who can remotely access the content and upload and monitor progress continuously. The goal of the proposed technology is to use eye tracking to provide a communication platform between caregiver and patient, increasing social interaction and reducing depression. In addition, the proposed eye-tracking algorithm may provide mental and physical progress to the provider as well as detect the next stroke. Future iterations may reduce the $4 billion healthcare costs per year in this population.\r\n\r\nThis I-Corps project is based on the development of an eye tracking algorithm to assess the changes in depression, anxiety, and other neurological insults.  The proposed technology utilizes existing eye tracking technology available in the Apple\u00ae AR kit to optimize the capture and delivery of standardized stimuli (pictures and words) digitally to another device, and use the metrics for the detection algorithm. This proposed method may be optimized for two environmental scenarios: acute in-patient settings and outpatient/in-home settings. A prototype is under development to test this method for detection and tracking of user gaze vectors for object recognition for clinical stimuli generated individually for each patient.  Previously, eye tracking technology has been tested for detection of traumatic brain injury.  The goal is to fuse gaze information with all other available sources of data (stimuli presented, information from previous visits including baseline results, self-report and guided assessments, and expert clinical feedback).  Fused features will then be used to recognize a required \u201cevent/state of interest\" defined as any event or condition that is used for prediction in the decision system. Pilot and patient data using this technology may provide a comprehensive method for speech pathologists that allows communication, monitoring and detection.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Maarten",
   "pi_last_name": "Lansberg",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Maarten G Lansberg",
   "pi_email_addr": "lansberg@stanford.edu",
   "nsf_id": "000887484",
   "pi_start_date": "2022-11-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "455 Broadway Street",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802300",
   "pgm_ele_name": "I-Corps"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "066E",
   "pgm_ref_txt": "INSTRUMENTATION & DIAGNOSTICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Project Outcomes Report</strong></p>\n<p>The combined impact of communication loss, depression and risk of the next neurological impact is deleterious to the quality of life of older adults, their caregivers, and the healthcare system. Currently, there are no comprehensive solutions that target older adults with communication disorders resulting from adult injuries involving their caregivers and providers with monitoring, assessments, and alerts for increase in depression or indications for other neurological events.</p>\n<h3>Soof Solutions, Inc. proposes to use vision neuroscience and artificial intelligence to develop a comprehensive communication-detection method to give voice to older adults who cannot speak due to various neurological insults with the following innovations: 1) safe, efficient, accurate, and improved communication with the care team, 2) reduced depression in the patient, and 3) increased power for detecting early neural biomarkers for depression, anxiety, stress, fatigue, cognitive decline and other neurological insults (such as another stroke).</h3>\n<p>During NSF-ICorps, the Soof Solutions Team interviewed many speech pathologists, patients with stroke, caregivers, neurologists, and hospital nurses who worked in inpatient and outpatient rehabilitation services. We also spoke with experts in FDA, Medicare/Medicaid and Blue Cross Blue Shields executive in Rhode Island. A survey was conducted with speech pathologists (n = 55) to ask about what solutions they use and what they would like to see in the market. These interviews helped us validate our target market, the need for a comprehensive solution that can benefit the patient in the hospital, home and throughout the rehabilitation phase. We also confirmed that the market size is large, and our solution can help patients, caregivers, and providers (e.g., speech pathologists) in a substantial way. We identified the need for Soof Solutions Inc. because there is no comprehensive solution currently that can communicate and detect simultaneously or adapt to patient needs (Figure 1).</p>\n<p>&nbsp;</p>\n<p>Based on this research, we were able to focus on our aims to create our three Apple AR-Kit based apps that are technically feasible, will reduce technical risk and bring our solutions closer to being commercially viable:</p>\n<p>&nbsp;</p>\n<p><strong>Specific Aim 1</strong>: Patients with aphasia and/or ALS will consistently utilize the communication app on an iPad over a period of two months to communicate with their caregiver. <span style=\"text-decoration: underline;\">Measure</span>: number of tasks completed per day.</p>\n<p>In order to accomplish this, we i) developed stimuli for basic and personal needs, and ii) adapted standardized assessments in the software for weekly monitoring of depression (e.g., mood emojis. Figures 2 &amp; 3 illustrate the patient, caregiver, and provider app respectively.</p>\n<p>&nbsp;</p>\n<p><span style=\"text-decoration: underline;\">Challenge</span>: Our challenge was to create an optimal layout of the screen with various personalized stimuli so the patient, provider and the caregiver use it consistently every day instead of switching to another app. We were able to create the layout, and our apps will be hosted on Apple Apps as soon as regulatory approval is obtained.</p>\n<p><strong>Specific Aim 3</strong>: The eye tracking metrics collected for the algorithm in patients with aphasia or ALS will be significantly different from healthy older adults during the use of the communication app. <span style=\"text-decoration: underline;\">Measures</span>:</p>\n<p>1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Time For Selection (TFS) Time from when eye contact is made on the stimuli screen to reasonable eye lock on chosen stimuli.</p>\n<p>2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Accuracy in Selection (AIS) Accuracy of eye selection of stimuli compared to actual stimuli wanted.</p>\n<p>3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Caregiver Response Time (CRT) Amount of time between sending of stimulus request to the acknowledgment of request on Caregiver device.</p>\n<p>4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Degradation in Selection Time (DST) Difference in (TFS) over a given time.</p>\n<p>5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Head Position Vector (HPV) Nodding off indication.</p>\n<p>6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fixation Rate (FR)</p>\n<p>7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Blink Rate (BR)</p>\n<p>8.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Passive eye tracking will also be recorded for the patient.</p>\n<p>&nbsp;</p>\n<p><span style=\"text-decoration: underline;\">Challenge</span>: Our challenge was to capture eye movements in agitated patients known to have erratic and random eye movements. We have implemented our app in about four clinics for use with SLPs and some healthy older adult volunteers (Figure 4). We have obtained the above metrics and stored them in Google Fire store &reg;. We have also utilized the user input to enhance the stimulus capture and notifications to caregiver and provider. We continue to test usability and functionality.</p>\n<p><strong>Specific Aim 3</strong>: Patients with aphasia or ALS will provide eye tracking metrics that can be used to monitor depression.</p>\n<p>We are currently waiting for regulatory approval to test these three apps in a patient-caregiver-provider model. We will be able to address Specific Aim 3 in about 6 months.</p>\n<p><strong>Awards and recognition</strong>: During our NSF program we obtained &ldquo;People&rsquo;s Choice award&rdquo; (Figure 5) and &ldquo;NSF Spirit Award&rdquo; (Figure 6) in our cohort.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/02/2024<br>\nModified by: Maarten&nbsp;G&nbsp;Lansberg</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910281520_Fig3_Provider_Caregiverapp_020224--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910281520_Fig3_Provider_Caregiverapp_020224--rgov-800width.jpg\" title=\"Fig 3\"><img src=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910281520_Fig3_Provider_Caregiverapp_020224--rgov-66x44.jpg\" alt=\"Fig 3\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Provider App</div>\n<div class=\"imageCredit\">Soof</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Maarten&nbsp;G&nbsp;Lansberg\n<div class=\"imageTitle\">Fig 3</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910330264_Fig4_Customertraction_020224--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910330264_Fig4_Customertraction_020224--rgov-800width.jpg\" title=\"Fig 4\"><img src=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910330264_Fig4_Customertraction_020224--rgov-66x44.jpg\" alt=\"Fig 4\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Customer traction</div>\n<div class=\"imageCredit\">Soof</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Maarten&nbsp;G&nbsp;Lansberg\n<div class=\"imageTitle\">Fig 4</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910382764_Fig5_Peopleschoiceaward_2022--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910382764_Fig5_Peopleschoiceaward_2022--rgov-800width.jpg\" title=\"Fig 5\"><img src=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910382764_Fig5_Peopleschoiceaward_2022--rgov-66x44.jpg\" alt=\"Fig 5\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Award 1</div>\n<div class=\"imageCredit\">Soof</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Maarten&nbsp;G&nbsp;Lansberg\n<div class=\"imageTitle\">Fig 5</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910176104_Fig1_business_model_canvas_SOOF_NSFprojectoutcome_020224--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910176104_Fig1_business_model_canvas_SOOF_NSFprojectoutcome_020224--rgov-800width.jpg\" title=\"Fig 1\"><img src=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910176104_Fig1_business_model_canvas_SOOF_NSFprojectoutcome_020224--rgov-66x44.jpg\" alt=\"Fig 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Business Model Canvas</div>\n<div class=\"imageCredit\">Soof Solutions</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Maarten&nbsp;G&nbsp;Lansberg\n<div class=\"imageTitle\">Fig 1</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910412476_Fig6_NSFSpiritaward_2022--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910412476_Fig6_NSFSpiritaward_2022--rgov-800width.jpg\" title=\"Fig 6\"><img src=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910412476_Fig6_NSFSpiritaward_2022--rgov-66x44.jpg\" alt=\"Fig 6\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Award 2</div>\n<div class=\"imageCredit\">Soof</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Maarten&nbsp;G&nbsp;Lansberg\n<div class=\"imageTitle\">Fig 6</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910240130_Fig2_Patient_app_020224--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910240130_Fig2_Patient_app_020224--rgov-800width.jpg\" title=\"Figure 2\"><img src=\"/por/images/Reports/POR/2024/2245634/2245634_10838363_1706910240130_Fig2_Patient_app_020224--rgov-66x44.jpg\" alt=\"Figure 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Patient App</div>\n<div class=\"imageCredit\">Soof</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Maarten&nbsp;G&nbsp;Lansberg\n<div class=\"imageTitle\">Figure 2</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nProject Outcomes Report\n\n\nThe combined impact of communication loss, depression and risk of the next neurological impact is deleterious to the quality of life of older adults, their caregivers, and the healthcare system. Currently, there are no comprehensive solutions that target older adults with communication disorders resulting from adult injuries involving their caregivers and providers with monitoring, assessments, and alerts for increase in depression or indications for other neurological events.\nSoof Solutions, Inc. proposes to use vision neuroscience and artificial intelligence to develop a comprehensive communication-detection method to give voice to older adults who cannot speak due to various neurological insults with the following innovations: 1) safe, efficient, accurate, and improved communication with the care team, 2) reduced depression in the patient, and 3) increased power for detecting early neural biomarkers for depression, anxiety, stress, fatigue, cognitive decline and other neurological insults (such as another stroke).\n\n\nDuring NSF-ICorps, the Soof Solutions Team interviewed many speech pathologists, patients with stroke, caregivers, neurologists, and hospital nurses who worked in inpatient and outpatient rehabilitation services. We also spoke with experts in FDA, Medicare/Medicaid and Blue Cross Blue Shields executive in Rhode Island. A survey was conducted with speech pathologists (n = 55) to ask about what solutions they use and what they would like to see in the market. These interviews helped us validate our target market, the need for a comprehensive solution that can benefit the patient in the hospital, home and throughout the rehabilitation phase. We also confirmed that the market size is large, and our solution can help patients, caregivers, and providers (e.g., speech pathologists) in a substantial way. We identified the need for Soof Solutions Inc. because there is no comprehensive solution currently that can communicate and detect simultaneously or adapt to patient needs (Figure 1).\n\n\n\n\n\nBased on this research, we were able to focus on our aims to create our three Apple AR-Kit based apps that are technically feasible, will reduce technical risk and bring our solutions closer to being commercially viable:\n\n\n\n\n\nSpecific Aim 1: Patients with aphasia and/or ALS will consistently utilize the communication app on an iPad over a period of two months to communicate with their caregiver. Measure: number of tasks completed per day.\n\n\nIn order to accomplish this, we i) developed stimuli for basic and personal needs, and ii) adapted standardized assessments in the software for weekly monitoring of depression (e.g., mood emojis. Figures 2 & 3 illustrate the patient, caregiver, and provider app respectively.\n\n\n\n\n\nChallenge: Our challenge was to create an optimal layout of the screen with various personalized stimuli so the patient, provider and the caregiver use it consistently every day instead of switching to another app. We were able to create the layout, and our apps will be hosted on Apple Apps as soon as regulatory approval is obtained.\n\n\nSpecific Aim 3: The eye tracking metrics collected for the algorithm in patients with aphasia or ALS will be significantly different from healthy older adults during the use of the communication app. Measures:\n\n\n1. Time For Selection (TFS) Time from when eye contact is made on the stimuli screen to reasonable eye lock on chosen stimuli.\n\n\n2. Accuracy in Selection (AIS) Accuracy of eye selection of stimuli compared to actual stimuli wanted.\n\n\n3. Caregiver Response Time (CRT) Amount of time between sending of stimulus request to the acknowledgment of request on Caregiver device.\n\n\n4. Degradation in Selection Time (DST) Difference in (TFS) over a given time.\n\n\n5. Head Position Vector (HPV) Nodding off indication.\n\n\n6. Fixation Rate (FR)\n\n\n7. Blink Rate (BR)\n\n\n8. Passive eye tracking will also be recorded for the patient.\n\n\n\n\n\nChallenge: Our challenge was to capture eye movements in agitated patients known to have erratic and random eye movements. We have implemented our app in about four clinics for use with SLPs and some healthy older adult volunteers (Figure 4). We have obtained the above metrics and stored them in Google Fire store . We have also utilized the user input to enhance the stimulus capture and notifications to caregiver and provider. We continue to test usability and functionality.\n\n\nSpecific Aim 3: Patients with aphasia or ALS will provide eye tracking metrics that can be used to monitor depression.\n\n\nWe are currently waiting for regulatory approval to test these three apps in a patient-caregiver-provider model. We will be able to address Specific Aim 3 in about 6 months.\n\n\nAwards and recognition: During our NSF program we obtained Peoples Choice award (Figure 5) and NSF Spirit Award (Figure 6) in our cohort.\n\n\n\t\t\t\t\tLast Modified: 02/02/2024\n\n\t\t\t\t\tSubmitted by: MaartenGLansberg\n"
 }
}