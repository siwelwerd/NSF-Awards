{
 "awd_id": "2331223",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Travel: Student Travel Support for MVAPICH User Group (MUG) 2023 Conference",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032928104",
 "po_email": "shabagch@nsf.gov",
 "po_sign_block_name": "Sharmistha Bagchi-Sen",
 "awd_eff_date": "2023-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 10000.0,
 "awd_amount": 10000.0,
 "awd_min_amd_letter_date": "2023-06-16",
 "awd_max_amd_letter_date": "2023-06-16",
 "awd_abstract_narration": "This project focuses on the rapidly evolving landscape of Modern High-Performance Computing (HPC) systems and the importance of preparing the next generation of engineers and scientists to navigate these advancements. With the emergence of multi-/many-core platforms like Intel Xeons, AMD EPYC, GPUs from NVIDIA, AMD, and Intel, as well as FPGAs, ARM, and OpenPOWER architectures, coupled with RDMA-enabled networking technologies such as InfiniBand, RoCE, iWARP, Omni-Path, Rockport, and Slingshot 10/11, it is crucial to understand and utilize these technologies to design HPC software stacks. The MVAPICH2 open-source MPI library and its derivatives have played a significant role in exploiting the potential of RDMA-capable networks, resulting in the rapid growth and adoption of InfiniBand in the HPC community. The popularity of MVAPICH2 is evident, with over 3,300 organizations worldwide (in 90 countries) utilizing these libraries, resulting in more than 1.677 million downloads from the OSU website as of May 2023. Moreover, an annual MVAPICH User Group conference (MUG) provides a collaborative platform for users, researchers, administrators, and students to exchange knowledge, share experiences, and discuss optimization strategies, troubleshooting guidelines, and other relevant topics. This project not only advances the field of HPC but also supports education, promotes diversity, and benefits society at large.\r\n\r\nThe 2023 MVAPICH User group (MUG) Conference is a gathering of experts, including users, system administrators, researchers, engineers, and students, focused on sharing knowledge about the MVAPICH2 libraries. It provides a platform for discussions and presentations from renowned researchers, users, and system administrators in the field. The event also features contributed presentations selected by the MVAPICH team, focusing on tuning, optimization strategies, troubleshooting guidelines, and more. Scheduled to take place in Columbus, OH, from August 21-23, 2023, the conference is organized by a distinguished group of specialists in message passing (MPI) and networking technologies. To support student participation, the project will provide funding, enabling them to engage with the MVAPICH research and user community. The conference serves the national interest by fostering research dissemination, facilitating connections among researchers, and training the next generation of scholars, aligned with the mission of the National Science Foundation (NSF). The organizers aim to recruit students from diverse institutions, emphasizing inclusivity. Attending the conference offers students various benefits, such as 1) exposure to cutting-edge high-performance computing (HPC) technologies, 2) in-depth understanding of designing open-source software environments for HPC systems, 3) training on optimization techniques, and 4) opportunities for interaction with industry professionals and national laboratory experts. The NSF funding significantly impacts the future careers of researchers in\r\nHPC, networking, and message passing technologies, while promoting diversity within the field.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dhabaleswar",
   "pi_last_name": "Panda",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Dhabaleswar K Panda",
   "pi_email_addr": "panda.2@osu.edu",
   "nsf_id": "000487085",
   "pi_start_date": "2023-06-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Hari",
   "pi_last_name": "Subramoni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hari Subramoni",
   "pi_email_addr": "subramoni.1@osu.edu",
   "nsf_id": "000704577",
   "pi_start_date": "2023-06-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mustafa",
   "pi_last_name": "Abduljabbar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mustafa Abduljabbar",
   "pi_email_addr": "abduljabbar.1@osu.edu",
   "nsf_id": "000931934",
   "pi_start_date": "2023-06-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "1960 KENNY RD",
  "perf_city_name": "COLUMBUS",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736100",
   "pgm_ele_name": "EDUCATION AND WORKFORCE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7231",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE"
  },
  {
   "pgm_ref_code": "7556",
   "pgm_ref_txt": "CONFERENCE AND WORKSHOPS"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 10000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-42c91764-7fff-d178-2f6a-425d4e0773c1\"> </span></p>\n<p dir=\"ltr\"><span>Modern High-End Computing (HEC) systems allow scientists and engineers to tackle grand challenges in their respective domains and make significant contributions to their fields.&nbsp; Examples of such domains include astrophysics, earthquake analysis, weather prediction, nanoscience modeling, multi-scale and&nbsp;</span><span>multi-physics modeling, biological computations, computational fluid dynamics, etc. The design and deployment of such HEC systems are fueled by the increasing use of multi-/many-core environments (NVIDIA Grace, Intel Xeon/AMD Epyc CPUs, NVIDIA/AMD/Intel GPUs and IBM POWER architectures).</span></p>\n<p dir=\"ltr\"><span>The emergence of Remote Direct Memory Access (RDMA) enabled high-performance networking technologies like Slingshot, InfiniBand, RDMA over Converged Enhanced Ethernet (RoCE), and Internet Wide Area RDMA Protocol (iWARP), are fueling the growth of HEC systems and allowing multi-petascale systems to be designed with commodity cluster configurations at relatively modest costs.&nbsp; As the RDMA networking and I/O technology are steadily gaining importance in designing HPC systems, it is increasingly becoming critical to leverage this technology to design high-performance and scalable software stacks for current and next-generation HPC systems.&nbsp; It is also becoming critical to train students, researchers, and engineers about these technologies so that the next-generation HPC workforce can leverage this technology to the fullest extent.</span></p>\n<p dir=\"ltr\"><span>The PI&rsquo;s team has been working in this area for the last 23 years (since the inception of InfiniBand technology in 2000). During these years, the MVAPICH project has been able to provide open-source, high-performance and scalable MPI libraries to the HPC community to take advantage of the RDMA technology. As the number of users of the MVAPICH2 libraries has been steadily growing, an annual MVAPICH User Group (MUG) conference was created 11 years back to provide an open forum to exchange information on the design and usage of MVAPICH2 MPI libraries. Attendance for this meeting has been steadily rising.</span></p>\n<p dir=\"ltr\"><span>As the HPC field is heading into Exascale era and systems are becoming more complex, it is critical that the next-generation engineers and scientists get exposed to the modern HPC technologies (including RDMA), learn how to use the features of these technologies to design HPC software stacks, learn about the process of open-source software developments and its sustainability.</span></p>\n<p dir=\"ltr\"><span>This student travel grant aimed to achieve the above objectives by encouraging graduate students working in the HPC area (systems and applications) to participate in the annual MUG conference.&nbsp; The requested student travel funding was instrumental in attracting nine graduate students from U.S. institutions to participate in the event.&nbsp; The MUG '23 event was attended by more than 225 people. Attendees came from more than 100 organizations spanning 21 countries.</span></p>\n<p dir=\"ltr\"><span>The MUG '23 event included a set of tutorials, Keynote Talks, Invited Talks, hands-on sessions, and an open mic session.&nbsp; A special poster session was also organized where the students with travel funding presented their current research results.&nbsp; A presentation was also made by Prof. Ashok Srinivasan, NSF Program Director.&nbsp; The talk focused on opportunities for young researchers.</span></p>\n<p dir=\"ltr\"><span>The presentations from MUG '23 event have been archived under http://mug.mvapich.cse.ohio-state.edu/mug/23/.</span></p>\n<p dir=\"ltr\"><span>The participating students under the travel grant were exposed to the design and development effort of the open-source MVAPICH2 MPI library as well as its impact. The students were also exposed to cyberinfrastructure learning and workforce development innovations from the talk by Prof. Ashok Srinivasan, NSF Program Director. The students were also able to interact with leading professionals in the field. Overall, the student travel grant helped a set of Ph.D. students to enter the next-generation HPC workforce with a lot of training and practical knowledge on software design, reuse, and sustainability.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/30/2024<br>\nModified by: Dhabaleswar&nbsp;K&nbsp;Panda</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nModern High-End Computing (HEC) systems allow scientists and engineers to tackle grand challenges in their respective domains and make significant contributions to their fields. Examples of such domains include astrophysics, earthquake analysis, weather prediction, nanoscience modeling, multi-scale andmulti-physics modeling, biological computations, computational fluid dynamics, etc. The design and deployment of such HEC systems are fueled by the increasing use of multi-/many-core environments (NVIDIA Grace, Intel Xeon/AMD Epyc CPUs, NVIDIA/AMD/Intel GPUs and IBM POWER architectures).\n\n\nThe emergence of Remote Direct Memory Access (RDMA) enabled high-performance networking technologies like Slingshot, InfiniBand, RDMA over Converged Enhanced Ethernet (RoCE), and Internet Wide Area RDMA Protocol (iWARP), are fueling the growth of HEC systems and allowing multi-petascale systems to be designed with commodity cluster configurations at relatively modest costs. As the RDMA networking and I/O technology are steadily gaining importance in designing HPC systems, it is increasingly becoming critical to leverage this technology to design high-performance and scalable software stacks for current and next-generation HPC systems. It is also becoming critical to train students, researchers, and engineers about these technologies so that the next-generation HPC workforce can leverage this technology to the fullest extent.\n\n\nThe PIs team has been working in this area for the last 23 years (since the inception of InfiniBand technology in 2000). During these years, the MVAPICH project has been able to provide open-source, high-performance and scalable MPI libraries to the HPC community to take advantage of the RDMA technology. As the number of users of the MVAPICH2 libraries has been steadily growing, an annual MVAPICH User Group (MUG) conference was created 11 years back to provide an open forum to exchange information on the design and usage of MVAPICH2 MPI libraries. Attendance for this meeting has been steadily rising.\n\n\nAs the HPC field is heading into Exascale era and systems are becoming more complex, it is critical that the next-generation engineers and scientists get exposed to the modern HPC technologies (including RDMA), learn how to use the features of these technologies to design HPC software stacks, learn about the process of open-source software developments and its sustainability.\n\n\nThis student travel grant aimed to achieve the above objectives by encouraging graduate students working in the HPC area (systems and applications) to participate in the annual MUG conference. The requested student travel funding was instrumental in attracting nine graduate students from U.S. institutions to participate in the event. The MUG '23 event was attended by more than 225 people. Attendees came from more than 100 organizations spanning 21 countries.\n\n\nThe MUG '23 event included a set of tutorials, Keynote Talks, Invited Talks, hands-on sessions, and an open mic session. A special poster session was also organized where the students with travel funding presented their current research results. A presentation was also made by Prof. Ashok Srinivasan, NSF Program Director. The talk focused on opportunities for young researchers.\n\n\nThe presentations from MUG '23 event have been archived under http://mug.mvapich.cse.ohio-state.edu/mug/23/.\n\n\nThe participating students under the travel grant were exposed to the design and development effort of the open-source MVAPICH2 MPI library as well as its impact. The students were also exposed to cyberinfrastructure learning and workforce development innovations from the talk by Prof. Ashok Srinivasan, NSF Program Director. The students were also able to interact with leading professionals in the field. Overall, the student travel grant helped a set of Ph.D. students to enter the next-generation HPC workforce with a lot of training and practical knowledge on software design, reuse, and sustainability.\n\n\n\t\t\t\t\tLast Modified: 03/30/2024\n\n\t\t\t\t\tSubmitted by: DhabaleswarKPanda\n"
 }
}