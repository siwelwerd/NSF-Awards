{
 "awd_id": "2330934",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "I-Corps:  Artificial Intelligence and Haptic-Enabled Robotic Assistant for Surgeons",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032922160",
 "po_email": "rshuman@nsf.gov",
 "po_sign_block_name": "Ruth Shuman",
 "awd_eff_date": "2023-06-15",
 "awd_exp_date": "2025-11-30",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2023-06-22",
 "awd_max_amd_letter_date": "2023-06-22",
 "awd_abstract_narration": "The broader impact/commercial potential of this I-Corps project is the development of a wearable system capable of sensing and analyzing motion data of surgeons and surgical trainees to provide online, haptic guidance feedback on how to improve surgical performance.  Robotic surgery remains a complex and demanding psychomotor task that requires extensive training to master.  In addition, a growing shortage within the surgical workforce adds to the challenge of providing comprehensive training.  The proposed technology is designed to help surgeons improve their surgical skills. This technology may be particularly useful in rural areas or those impacted by surgeon workforce shortages as access to well-trained surgeon experts is limited.  The goal is to make it easier and safer for surgeons to learn and use surgical robots by lowering the barrier for surgeons to learn and use surgical robots through developing online analytics and haptic feedback strategies, ultimately improving patient outcomes.\r\n\r\nThis I-Corps project is based on the development of artificial intelligence (AI) and haptic enabled wearable technology to assist with analyzing surgical motions and providing surgical skill guidance in near real-time. The proposed technology uses human-centered, data driven models for surgical movement analysis and context awareness. These models are designed to detect a range of behaviors, including stylistic behaviors, expertise levels, bimanual coordination, and stress levels of surgeons during robotic surgery and training. The models leverage machine learning, deep learning, and mathematical methods to analyze kinematic data from surgical robots and their operators in real time, making them unique in their ability to provide immediate feedback and guidance using visual and haptic technologies to surgeons, augmenting their surgical performance. This technology has the potential to greatly reduce the training required to operate surgical robots effectively, making it easier for surgeons to integrate this technique into their practice. In addition, the proposed technology has the potential to improve patient outcomes by minimizing the risk of surgical errors and complications. The real-time feedback and guidance provided by the system enable surgeons to make more accurate and informed decisions during surgeries, reducing the likelihood of errors and complications. This has the potential to improve patient safety and reduce the need for postoperative interventions, ultimately leading to better clinical outcomes.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ann",
   "pi_last_name": "Majewicz Fey",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ann Majewicz Fey",
   "pi_email_addr": "Ann.MajewiczFey@utexas.edu",
   "nsf_id": "000678765",
   "pi_start_date": "2023-06-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "110 INNER CAMPUS DR",
  "perf_city_name": "AUSTIN",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121139",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802300",
   "pgm_ele_name": "I-Corps"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": null
}