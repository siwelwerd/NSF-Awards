{
 "awd_id": "2246815",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Learning Complex Stochastic Systems",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2023-08-15",
 "awd_exp_date": "2026-07-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2023-07-21",
 "awd_max_amd_letter_date": "2023-07-21",
 "awd_abstract_narration": "Differential equations are often used to model temporal evolutions of a variety of systems. However, most realistic systems including those arising from biology, environmental science, engineering, physics, medicine and financial markets exhibit randomness in their behavior. Accurate analysis of such systems thus needs differential equations that can incorporate this randomness. Stochastic differential equations are powerful tools for this purpose. Understanding behaviors of these systems requires not just building mathematical models but integrating them with available data. This in turn requires various types of learning algorithms. It is important to judge the effectiveness of these algorithms by rigorous mathematical analysis, which is the primary objective of this project. The dynamics of these stochastic systems are however intricate with convoluted correlation structures, and there is a critical lack of mathematical results in the literature investigating learning methods for such complex data. The work done by the investigator will fill some of this gap by deriving mathematical results that will not only be able to answer if the algorithms become more accurate with data observed over longer periods of time but will be able to provide valuable insight on how to fine-tune the key parameters for optimal efficiency.  Building such data-driven stochastic models backed by rigorous mathematics enhances our understanding of complex systems across multiple domains and empowers informed decision-making in the presence of randomness.  The project will involve undergraduate and graduate students and will teach them valuable skills through a combination of theoretical knowledge, practical application, and hands-on experience with coding. It will enable them to excel in the digital age and adapt to the demands of an increasingly data-driven and technologically advanced world.  The results of the project will be disseminated through publications in well-known scientific journals and presentations at domestic and international conferences.\r\n\r\nThe project will study important learning problems for a broad class of stochastic differential equations (SDEs). These problems lie on the interface of stochastic analysis and statistical learning theory, and there is a paucity of theoretical results in probability, statistics and machine learning literature addressing them. The project is divided into three interconnected parts, each of which plays an important role in the other. Part I will address important problems on parametric inference including point estimation and testing of hypotheses. It will derive asymptotic results including law of large numbers, central limit theorems and large deviation principles for estimators of a finite dimensional parameter of a broad class of SDEs. Unlike some existing works in this direction which assume data to be in the form of a continuous trajectory, the investigator's work will consider the realistic case of availability of only discrete data points. Since asymptotic analysis requires the time horizon to go to infinity, the effect of time-gap (or discretization step) between the observations on the accuracy of these estimators over long time is not clear, and it is known that naive discretization of estimators based on a continuous trajectory of an SDE can lead to erroneous inference. The project will introduce appropriate scaling frameworks to quantify this effect and analyze the errors in different scaling regimes. Next, these results will be utilized to design tests for composite hypotheses-testing problems so that the probability of type I error decays rapidly and which are asymptotically uniformly powerful within a class of tests having similar level of type I error. Part II of the project concerns itself with the important topic of decision-making. Decision-making involves (constrained) minimization of suitable cost functions depending on model parameters.  Since these latter quantities are unknown, data-driven versions of such minimization problems are necessary in practice. In particular, it is necessary to construct suitable estimators of the cost functions so that decisions based on their minimization are close to the true decisions. The investigator will study a novel approach based on large deviation analysis and results of Part I which aims to guarantee that under appropriate conditions this can be achieved with a very high probability. Part III is devoted to nonparametric learning of SDEs. The last part falls in the realm of infinite-dimensional learning theory where the goal is to learn the entire driving functions of the SDE-based models as opposed to estimating finite-dimensional parameters. A rigorous computational framework combining Bayesian techniques with the theory of Reproducing Kernel Hilbert Space will be developed toward this end, and the theoretical properties of the resulting learning algorithms will also be studied.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Arnab",
   "pi_last_name": "Ganguly",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Arnab Ganguly",
   "pi_email_addr": "aganguly@lsu.edu",
   "nsf_id": "000655754",
   "pi_start_date": "2023-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Louisiana State University",
  "inst_street_address": "202 HIMES HALL",
  "inst_street_address_2": "",
  "inst_city_name": "BATON ROUGE",
  "inst_state_code": "LA",
  "inst_state_name": "Louisiana",
  "inst_phone_num": "2255782760",
  "inst_zip_code": "708030001",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "LA06",
  "org_lgl_bus_name": "LOUISIANA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "ECQEYCHRNKJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Louisiana State University",
  "perf_str_addr": "202 HIMES HALL",
  "perf_city_name": "BATON ROUGE",
  "perf_st_code": "LA",
  "perf_st_name": "Louisiana",
  "perf_zip_code": "708030001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "LA06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126300",
   "pgm_ele_name": "PROBABILITY"
  },
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": null
}