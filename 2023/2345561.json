{
 "awd_id": "2345561",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: Probing Fairness of Ocular Biometrics Methods Across Demographic Variations",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2023-08-01",
 "awd_exp_date": "2024-10-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 111091.0,
 "awd_min_amd_letter_date": "2023-09-18",
 "awd_max_amd_letter_date": "2023-12-28",
 "awd_abstract_narration": "Biometrics technology related to recognizing the identities and traits of people has been widely adopted in intelligence gathering, law enforcement, and consumer applications. Recent studies suggest that face-based biometric technology does not work equitably across demographic variations. There is a pressing need to investigate fair biometric solutions and modalities toward accurate, fair, and trustworthy technology for enhanced security and public safety. Ocular biometrics, which consists of regions in and around the eyes, offers an alternate solution to face biometrics due to its accuracy and privacy. Furthermore, ocular biometrics can be acquired using regular cameras even in the presence of face covering.  This project investigates the fairness of ocular biometric technology and develops solutions to mitigate unequal accuracy gaps across demographic variations.  This project spans a highly multidisciplinary research area, which integrates engineering, statistics, mathematics, computing, and policy. The findings of this project are used to update the engineering curricula, including computer vision, image analysis, machine learning, deep learning, and biometrics. The advances made in this project are disseminated through publications, the investigator website, and seminars. This project also provides opportunities to broaden the participation of women, underrepresented minorities, and undergraduate students in computing.\r\n\r\nThis project investigates the fairness of ocular biometrics scanned in visible and NIR (Near-infrared) spectrum across demographic variations. The machine and deep learning models trained for ocular-based individual analysis are evaluated for unequal accuracy rates across demographic variations. The cause of unequal accuracy rates in the machine and deep learning algorithms are analyzed using explainable AI. The fairness-aware classifiers are developed by modifying the objective function, using ensemble techniques, and learning fair feature representation. Existing and publicly available ocular datasets are utilized for the evaluation and security analysis of the classifiers used in this study. The efficacy of the proposed solutions is assessed through standard biometric performance and fairness evaluation metrics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ajita",
   "pi_last_name": "Rattani",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ajita Rattani",
   "pi_email_addr": "ajita.rattani@unt.edu",
   "nsf_id": "000762575",
   "pi_start_date": "2023-09-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Texas",
  "inst_street_address": "1112 DALLAS DR STE 4000",
  "inst_street_address_2": "",
  "inst_city_name": "DENTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9405653940",
  "inst_zip_code": "762051132",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "TX13",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH TEXAS",
  "org_prnt_uei_num": "",
  "org_uei_num": "G47WN1XZNWX9"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Texas",
  "perf_str_addr": "1155 UNION CIR #305250",
  "perf_city_name": "DENTON",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "762035017",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "TX13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 95091.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>These outcomes of this project are discussed as follows:</p>\r\n<p><strong>1. Understanding Bias in Biometric Systems: </strong>This project provided a deeper understanding of the underlying causes of unequal accuracy rates in biometric systems, particularly ocular biometrics. Using explainable AI (XAI) techniques, the project offered transparency into the decision-making processes of biometric algorithms. These techniques allowed researchers to identify the specific factors contributing to performance disparities, whether related to data quality, algorithmic design, or demographic influences. By uncovering these causes, the project enabled researchers to create more effective and fair systems, providing insights into the mechanisms that caused bias.</p>\r\n<p><strong>2. Fair and Accurate Ocular Biometric Models: </strong>A primary outcome of this research is the development of fairness-aware machine learning and deep learning models designed to improve the accuracy of ocular biometric systems. The four new models developed ensured that ocular biometrics performed equitably across all demographic groups, providing a reliable, secure, and inclusive biometric solution. The proposed fairness-aware models used innovative techniques, such as modifying the objective function, employing ensemble methods, and learning fair feature representations. These approaches are specifically designed to ensure that the performance of biometric systems is consistent across different demographic groups. By developing fairness-aware systems, this research shifted the focus of biometric system design from just technical accuracy to also including fairness as a core component.</p>\r\n<p><strong>3. Evaluation of</strong> <strong>Ocular Biometric Datasets and Security: </strong>Another crucial outcome is the rigorous evaluation of ocular biometric datasets to assess the fairness and security of the developed classifiers. The project used publicly available datasets to benchmark the accuracy and fairness of the classifiers against established biometric performance metrics like accuracy, false acceptance rate (FAR), and false rejection rate (FRR). In addition to these performance metrics, fairness evaluation metrics were applied to measure how well the systems perform across various demographic groups.</p>\r\n<p><strong>4. Impact on Engineering Education: </strong>The findings from this project will have a significant impact on engineering education. The research has been incorporated into curricula for courses related to machine learning, and biometrics authentication. The insights gained from addressing fairness in ocular biometrics helped students better understand the importance of integrating fairness considerations into the development of AI and biometric technologies. This knowledge empowered the next generation of engineers to design systems that are not only technically effective but also socially responsible.</p>\r\n<p><strong>5. Broader Participation in Computing: </strong>A key outcome of the project is its ability to increase diversity and inclusion in the computing field, particularly in biometric research. By actively involving women, minorities, and undergraduate students in the research process, the project provided valuable mentorship and research opportunities. The project also contributed to the development of skills and knowledge among underrepresented groups, offering pathways for them to engage with cutting-edge technologies.</p>\r\n<p><strong>6. Publications and Dissemination of Knowledge: </strong>As part of the dissemination process, the project resulted in publications in top-tier academic journals and conferences. Additionally, the project offered seminars, two workshops, and updates on the investigator&rsquo;s website to further disseminate the findings. This dissemination ensured that the research reaches both academic and industry professionals, allowing for the widespread adoption of the fairness-aware techniques developed in the project. Further, the GitHub repository with the codebase for evaluation and development of fairness aware ocular biometrics solution has been made publicly available for further research and development.</p>\r\n<p><strong>7. Ethical and Policy Implications</strong>: The project provided valuable insights into the ethical guidelines and policy recommendations for the use of biometric technologies. As biometric systems continue to gain popularity, especially in law enforcement and security, there is an increasing need for ethical standards to ensure that these technologies are used responsibly. The research informed discussions on privacy, fairness, and transparency, helping to shape policies that protect individuals' rights while still allowing for the use of biometric technologies in security applications. The PI participated as a speaker (panelist) at Federal Identity (FedID) conference in Georgia, Atlanta 2022 to disseminate information on bias and fairness of ocular biometrics to law enforcement.</p>\r\n<p><strong>8. Real-World Applications</strong>: The advancements made in this project directly influenced real-world applications of biometric systems. By improving the fairness and accuracy of these systems, the project increased public trust in biometric technologies and making them more reliable for use in sensitive environments such as law enforcement, healthcare, and financial services.</p>\r\n<p>In <strong>conclusion</strong>, the outcomes of this project lead to the development of more equitable, accurate, and privacy-preserving biometric systems, with significant implications for research, education, policy, and industry. By developing fairness-aware models, evaluating their impact, and increasing diversity in computing, the project helped shape the future of biometric technology and ensured that it benefits all individuals, regardless of demographic background.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/09/2025<br>\nModified by: Ajita&nbsp;Rattani</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThese outcomes of this project are discussed as follows:\r\n\n\n1. Understanding Bias in Biometric Systems: This project provided a deeper understanding of the underlying causes of unequal accuracy rates in biometric systems, particularly ocular biometrics. Using explainable AI (XAI) techniques, the project offered transparency into the decision-making processes of biometric algorithms. These techniques allowed researchers to identify the specific factors contributing to performance disparities, whether related to data quality, algorithmic design, or demographic influences. By uncovering these causes, the project enabled researchers to create more effective and fair systems, providing insights into the mechanisms that caused bias.\r\n\n\n2. Fair and Accurate Ocular Biometric Models: A primary outcome of this research is the development of fairness-aware machine learning and deep learning models designed to improve the accuracy of ocular biometric systems. The four new models developed ensured that ocular biometrics performed equitably across all demographic groups, providing a reliable, secure, and inclusive biometric solution. The proposed fairness-aware models used innovative techniques, such as modifying the objective function, employing ensemble methods, and learning fair feature representations. These approaches are specifically designed to ensure that the performance of biometric systems is consistent across different demographic groups. By developing fairness-aware systems, this research shifted the focus of biometric system design from just technical accuracy to also including fairness as a core component.\r\n\n\n3. Evaluation of Ocular Biometric Datasets and Security: Another crucial outcome is the rigorous evaluation of ocular biometric datasets to assess the fairness and security of the developed classifiers. The project used publicly available datasets to benchmark the accuracy and fairness of the classifiers against established biometric performance metrics like accuracy, false acceptance rate (FAR), and false rejection rate (FRR). In addition to these performance metrics, fairness evaluation metrics were applied to measure how well the systems perform across various demographic groups.\r\n\n\n4. Impact on Engineering Education: The findings from this project will have a significant impact on engineering education. The research has been incorporated into curricula for courses related to machine learning, and biometrics authentication. The insights gained from addressing fairness in ocular biometrics helped students better understand the importance of integrating fairness considerations into the development of AI and biometric technologies. This knowledge empowered the next generation of engineers to design systems that are not only technically effective but also socially responsible.\r\n\n\n5. Broader Participation in Computing: A key outcome of the project is its ability to increase diversity and inclusion in the computing field, particularly in biometric research. By actively involving women, minorities, and undergraduate students in the research process, the project provided valuable mentorship and research opportunities. The project also contributed to the development of skills and knowledge among underrepresented groups, offering pathways for them to engage with cutting-edge technologies.\r\n\n\n6. Publications and Dissemination of Knowledge: As part of the dissemination process, the project resulted in publications in top-tier academic journals and conferences. Additionally, the project offered seminars, two workshops, and updates on the investigators website to further disseminate the findings. This dissemination ensured that the research reaches both academic and industry professionals, allowing for the widespread adoption of the fairness-aware techniques developed in the project. Further, the GitHub repository with the codebase for evaluation and development of fairness aware ocular biometrics solution has been made publicly available for further research and development.\r\n\n\n7. Ethical and Policy Implications: The project provided valuable insights into the ethical guidelines and policy recommendations for the use of biometric technologies. As biometric systems continue to gain popularity, especially in law enforcement and security, there is an increasing need for ethical standards to ensure that these technologies are used responsibly. The research informed discussions on privacy, fairness, and transparency, helping to shape policies that protect individuals' rights while still allowing for the use of biometric technologies in security applications. The PI participated as a speaker (panelist) at Federal Identity (FedID) conference in Georgia, Atlanta 2022 to disseminate information on bias and fairness of ocular biometrics to law enforcement.\r\n\n\n8. Real-World Applications: The advancements made in this project directly influenced real-world applications of biometric systems. By improving the fairness and accuracy of these systems, the project increased public trust in biometric technologies and making them more reliable for use in sensitive environments such as law enforcement, healthcare, and financial services.\r\n\n\nIn conclusion, the outcomes of this project lead to the development of more equitable, accurate, and privacy-preserving biometric systems, with significant implications for research, education, policy, and industry. By developing fairness-aware models, evaluating their impact, and increasing diversity in computing, the project helped shape the future of biometric technology and ensured that it benefits all individuals, regardless of demographic background.\r\n\n\n\t\t\t\t\tLast Modified: 02/09/2025\n\n\t\t\t\t\tSubmitted by: AjitaRattani\n"
 }
}