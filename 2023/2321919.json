{
 "awd_id": "2321919",
 "agcy_id": "NSF",
 "tran_type": "CoopAgrmnt",
 "awd_istr_txt": "Cooperative Agreement",
 "awd_titl_txt": "SBIR Phase II:  Automated Perception for Robotic Chopsticks Manipulating Small and Large Objects in Constrained Spaces",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032922936",
 "po_email": "emirowsk@nsf.gov",
 "po_sign_block_name": "Ela Mirowski",
 "awd_eff_date": "2023-10-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 999746.0,
 "awd_amount": 249937.0,
 "awd_min_amd_letter_date": "2023-09-20",
 "awd_max_amd_letter_date": "2024-02-21",
 "awd_abstract_narration": "The broader/commercial impact of this Small Business Innovation Research (SBIR) Phase II project supports the development of robotic solutions for unloading non-palletized packages of different shapes and sizes in logistics and similar industries.  This technology may provide workers with more skilled jobs that remove the need for physically strenuous labor in unhealthy environments. The project seeks to increase US competitiveness in supply chain logistics ($150 billion / year market in the US) by helping solve long-standing and worsening employee recruitment and retention problems. The project helps the US become an early leader in the robotic manipulation of diverse objects in constrained, unstructured environments while simultaneously training a workforce capable of remote manipulation in safe environments.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase II project supports the development of robotic solutions for unloading non-palletized packages of different shapes and sizes in logistics and similar industries. At present, there are few commercially-available automated solutions for this task. Those few machines are brittle, slow, and only work well with uniform packages. The research objectives include: 1) upgrading the robot\u2019s perception system to fuse high-speed vision and force sensory inputs, which will enable closed-loop picking with greater speed, more robustness, higher safety, and less package damage; 2) upgrading the robot\u2019s vision system to perceive object categories beyond boxes; 3) investigating a user interface to allow a human operator to most-easily correct inevitable perception system errors; and 4) field testing the robotic  system. The cumulative result will be a rigorously validated system that safely (for packages and users) operates at high speed with little manual intervention.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Evan",
   "pi_last_name": "Drumwright",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Evan Drumwright",
   "pi_email_addr": "drum@dextrousrobotics.com",
   "nsf_id": "000802056",
   "pi_start_date": "2023-09-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "DEXTROUS ROBOTICS, INC.",
  "inst_street_address": "802 ROZELLE ST",
  "inst_street_address_2": "",
  "inst_city_name": "MEMPHIS",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "9015980441",
  "inst_zip_code": "381045052",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TN09",
  "org_lgl_bus_name": "DEXTROUS ROBOTICS, INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "MD44UJ6AZMP5"
 },
 "perf_inst": {
  "perf_inst_name": "DEXTROUS ROBOTICS, INC.",
  "perf_str_addr": "1350 Concourse Ave Ste 465",
  "perf_city_name": "Memphis",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "381042027",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "TN09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6840",
   "pgm_ref_txt": "ROBOTICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01AB2324DB",
   "fund_name": "R&RA DRSA DEFC AAB",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 249936.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we made significant strides in the field of autonomous robotics by developing a bimanual manipulator robot capable of performing high-speed material handling tasks, such as truck loading and unloading, with minimal human intervention. This endeavor aimed to improve coordination between perception systems, operator interfaces, and electro-mechanical systems to achieve efficient, reliable, and mostly autonomous operation. The outcomes of this project not only demonstrate intellectual merit through novel technical achievements but also have broader impacts on labor efficiency, workplace safety, and the future of automation.</p>\r\n<h4>Intellectual Merit</h4>\r\n<p><strong>1. Fusing High-Speed Vision and Force Sensory Inputs</strong></p>\r\n<p>A key challenge in achieving high-speed autonomous operation is ensuring accurate interaction between a robot&rsquo;s sensory and mechanical systems. To address this, we developed a process for continuous online recalibration of the kinematic relationship between the robot&rsquo;s cameras and its end effectors (chopstick-like manipulators). This was critical because rapid movements and heavy loading caused microscopic deformations in the camera mounts, which impacted spatial accuracy. Our novel calibration process, which takes only seconds to complete, significantly improved system responsiveness compared to the previous method that required minutes and was only initiated when errors became evident.</p>\r\n<p>In addition, we implemented hybrid manipulation control strategies that integrated visual and force-based inputs. These strategies enhanced the robot&rsquo;s ability to handle objects requiring precision beyond the limits of visual sensing alone. These innovations lay the groundwork for advanced autonomous systems capable of operating efficiently in dynamic environments.</p>\r\n<p><strong>2. Perceiving Object Categories Beyond Boxes</strong></p>\r\n<p>While the primary focus was on manipulating boxes, we extended the robot&rsquo;s perception capabilities to identify and model elements of its environment, such as trailer walls, floors, and ceilings. Using neural networks and classical computational geometry techniques, we developed methods to build geometric models of these structures as truncated planes. These models contributed to a robust \"world model\" that prevented collisions between the robot, its manipulated objects, and the environment.&nbsp;</p>\r\n<p><strong>3. Developing an Easy-to-Use Human Interface</strong></p>\r\n<p>To facilitate quick corrections to perception errors, we designed an intuitive human-machine interface (HMI). Leveraging advanced tools such as the Segment Anything Model (SAM), operators could add undetected objects to the robot&rsquo;s model with minimal effort. Features such as click-and-drag resizing, rapid keyboard shortcuts for visualizing perceptual data, and a memory system for recognizing and estimating the mass of frequently observed packages further streamlined operations. Field tests demonstrated that new operators could become proficient in using this interface within minutes, highlighting its accessibility and effectiveness.</p>\r\n<h4>Broader Impacts</h4>\r\n<p><strong>1. Real-World Deployment and Testing</strong></p>\r\n<p>We field-tested the robot in 30 separate truck unloading tasks at The Armstrong Co., a subcontractor for AutoZone, Inc., handling diverse loads such as radiators, shop cloths, and windshield wipers. These tests demonstrated the robot&rsquo;s ability to manipulate objects of varying sizes and weights, from small items like sugar cubes to large boxes, without causing damage. Notably, the robot successfully handled large and irregularly shaped packages, outperforming suction-based competitors. This performance highlights the system&rsquo;s potential to transform material handling operations across industries.</p>\r\n<p><strong>2. Enhancing Workplace Safety and Efficiency</strong></p>\r\n<p>By automating physically demanding tasks, this project has the potential to reduce workplace injuries and increase efficiency. The robot&rsquo;s design allows it to handle up to 2,000 packages per hour&mdash;two to four times the speed of human operators&mdash;with a maximum payload of 264 lbs. This capability can alleviate the physical burden on workers and address labor shortages in logistics and warehousing sectors.</p>\r\n<p><strong>3. Expanding the Frontiers of Automation</strong></p>\r\n<p>Our advancements in sensory integration, perception systems, and human-machine interfaces pave the way for broader applications of robotics in unstructured environments. These innovations could extend beyond logistics to areas such as disaster response, healthcare, and manufacturing, where precision and adaptability are crucial.</p>\r\n<h3>Conclusion</h3>\r\n<p>This project represents a significant leap forward in autonomous robotics, combining technical excellence with practical applications that address societal needs. By enhancing the capabilities of autonomous systems and demonstrating their effectiveness in real-world scenarios, we contribute to a future where robots and humans collaborate to achieve greater efficiency, safety, and innovation in material handling and beyond.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/22/2025<br>\nModified by: Evan&nbsp;Drumwright</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project, we made significant strides in the field of autonomous robotics by developing a bimanual manipulator robot capable of performing high-speed material handling tasks, such as truck loading and unloading, with minimal human intervention. This endeavor aimed to improve coordination between perception systems, operator interfaces, and electro-mechanical systems to achieve efficient, reliable, and mostly autonomous operation. The outcomes of this project not only demonstrate intellectual merit through novel technical achievements but also have broader impacts on labor efficiency, workplace safety, and the future of automation.\r\nIntellectual Merit\r\n\n\n1. Fusing High-Speed Vision and Force Sensory Inputs\r\n\n\nA key challenge in achieving high-speed autonomous operation is ensuring accurate interaction between a robots sensory and mechanical systems. To address this, we developed a process for continuous online recalibration of the kinematic relationship between the robots cameras and its end effectors (chopstick-like manipulators). This was critical because rapid movements and heavy loading caused microscopic deformations in the camera mounts, which impacted spatial accuracy. Our novel calibration process, which takes only seconds to complete, significantly improved system responsiveness compared to the previous method that required minutes and was only initiated when errors became evident.\r\n\n\nIn addition, we implemented hybrid manipulation control strategies that integrated visual and force-based inputs. These strategies enhanced the robots ability to handle objects requiring precision beyond the limits of visual sensing alone. These innovations lay the groundwork for advanced autonomous systems capable of operating efficiently in dynamic environments.\r\n\n\n2. Perceiving Object Categories Beyond Boxes\r\n\n\nWhile the primary focus was on manipulating boxes, we extended the robots perception capabilities to identify and model elements of its environment, such as trailer walls, floors, and ceilings. Using neural networks and classical computational geometry techniques, we developed methods to build geometric models of these structures as truncated planes. These models contributed to a robust \"world model\" that prevented collisions between the robot, its manipulated objects, and the environment.\r\n\n\n3. Developing an Easy-to-Use Human Interface\r\n\n\nTo facilitate quick corrections to perception errors, we designed an intuitive human-machine interface (HMI). Leveraging advanced tools such as the Segment Anything Model (SAM), operators could add undetected objects to the robots model with minimal effort. Features such as click-and-drag resizing, rapid keyboard shortcuts for visualizing perceptual data, and a memory system for recognizing and estimating the mass of frequently observed packages further streamlined operations. Field tests demonstrated that new operators could become proficient in using this interface within minutes, highlighting its accessibility and effectiveness.\r\nBroader Impacts\r\n\n\n1. Real-World Deployment and Testing\r\n\n\nWe field-tested the robot in 30 separate truck unloading tasks at The Armstrong Co., a subcontractor for AutoZone, Inc., handling diverse loads such as radiators, shop cloths, and windshield wipers. These tests demonstrated the robots ability to manipulate objects of varying sizes and weights, from small items like sugar cubes to large boxes, without causing damage. Notably, the robot successfully handled large and irregularly shaped packages, outperforming suction-based competitors. This performance highlights the systems potential to transform material handling operations across industries.\r\n\n\n2. Enhancing Workplace Safety and Efficiency\r\n\n\nBy automating physically demanding tasks, this project has the potential to reduce workplace injuries and increase efficiency. The robots design allows it to handle up to 2,000 packages per hourtwo to four times the speed of human operatorswith a maximum payload of 264 lbs. This capability can alleviate the physical burden on workers and address labor shortages in logistics and warehousing sectors.\r\n\n\n3. Expanding the Frontiers of Automation\r\n\n\nOur advancements in sensory integration, perception systems, and human-machine interfaces pave the way for broader applications of robotics in unstructured environments. These innovations could extend beyond logistics to areas such as disaster response, healthcare, and manufacturing, where precision and adaptability are crucial.\r\nConclusion\r\n\n\nThis project represents a significant leap forward in autonomous robotics, combining technical excellence with practical applications that address societal needs. By enhancing the capabilities of autonomous systems and demonstrating their effectiveness in real-world scenarios, we contribute to a future where robots and humans collaborate to achieve greater efficiency, safety, and innovation in material handling and beyond.\r\n\n\n\t\t\t\t\tLast Modified: 01/22/2025\n\n\t\t\t\t\tSubmitted by: EvanDrumwright\n"
 }
}