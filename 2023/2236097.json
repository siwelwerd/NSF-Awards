{
 "awd_id": "2236097",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NSF Convergence Accelerator Track H: Smart Wearables for Expanding Workplace Access for People with Blindness and Low Vision",
 "cfda_num": "47.084",
 "org_code": "15020000",
 "po_phone": "7032927068",
 "po_email": "alvadati@nsf.gov",
 "po_sign_block_name": "Alex Vadati",
 "awd_eff_date": "2022-12-15",
 "awd_exp_date": "2024-11-30",
 "tot_intn_awd_amt": 743451.0,
 "awd_amount": 743451.0,
 "awd_min_amd_letter_date": "2022-12-08",
 "awd_max_amd_letter_date": "2024-04-16",
 "awd_abstract_narration": "There are 285 million people with blindness and low vision (pBLV) worldwide. One dramatic correlate of vision loss is unemployment, with rates as high as 81% in urban environments. Employment disparities stem, at least in large part, from difficulties with transportation to and from work and wayfinding within the workplace itself. This project takes a fundamental step in addressing the employment challenges of pBLV through a recently developed, powerful assistive-navigation platform, the Visually Impaired Smart Service System for Spatial Intelligence and Onboard Navigation (VIS4ION). VIS4ION is a discreet, instrumented backpack with an array of miniaturized sensors integrated into backstraps that connect to an embedded system for computational analysis. Real-time feedback is provided through a binaural bone conduction headset and an optional reconfigured waist strap turned haptic interface. The platform provides valuable micro-services for pBLV, including real-time navigation, scene analysis, and obstacle avoidance, all of which could transform workplace experiences for pBLV. To realize the goals of the project, a transdisciplinary team brings expertise across engineering (computer vision, haptics, sensors, and wireless), medicine (neurology and rehabilitation), and management (organizational change and behavior). These academic disciplines are partnered with a start-up, Tactile Navigation Tools (TNT), which is commercializing the technology, larger corporate firms in the space, a BLV-focused non-profit, Lighthouse Guild, and key government agencies. \r\nThe development and deployment of advanced, connected wearables in workplace environments faces significant technical, business, social, and logistical challenges. This translational project seeks to overcome these challenges by completing four inter-connected tasks: Task 1 performs the first detailed, workplace-oriented study that assesses navigation accuracy of VIS4ION, as measured using additional cameras and inertial measurement units; Task 2 builds on the current VIS4ION interfaces to establish scalable, reproducible, and lower-cost tactile feedback, together with a new audio-based \u201ccognitive assistant\u201d for guidance; Task 3 augments the wearable with high-speed 5G connectivity to provide real-time, powerful cloud-based machine vision processing; Task 4 affords a series of novel studies to assess the workplace experiences of pBLV via our partner, Lighthouse Guild. The assessment includes both \u201cthink aloud\u201d recordings and contextualized reflections. \r\nThe total cost of BLV was recently estimated to be $3 trillion dollars globally. The pathologic link between vision loss and inactivity precipitates debility and unemployment. Mobility losses lead to employment barriers, as pBLV struggle during daily commuting, wayfinding in the workplace, and variable work routines that have been intensified by the pandemic. This proposal aims to increase the safety profile and ease-of-use of a wearable platform toward \u2018connected\u2019 dynamic navigation in complex workplace environments. Future goals for scaling the platform include adding additional social capabilities such as face and emotion recognition to substantively improve quality of life in a smarter and more connected, disability-first city landscape. Beyond the envisioned impact on science, engineering, and improving the lives of pBLV, this program will foster formal and informal learning opportunities for graduate, undergraduate, medical, and high school students in clinical and fundamental STEM/STEAM research. These training opportunities will foster multi-pronged and multi-disciplinary team approaches critical to the development of innovative solutions for complex real-world problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "ITE",
 "org_div_long_name": "Innovation and Technology Ecosystems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John Ross",
   "pi_last_name": "Rizzo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "John Ross Rizzo",
   "pi_email_addr": "JohnRoss.Rizzo@nyumc.org",
   "nsf_id": "000734919",
   "pi_start_date": "2022-12-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Maurizio",
   "pi_last_name": "Porfiri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Maurizio Porfiri",
   "pi_email_addr": "mporfiri@nyu.edu",
   "nsf_id": "000240360",
   "pi_start_date": "2022-12-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sundeep",
   "pi_last_name": "Rangan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sundeep Rangan",
   "pi_email_addr": "srangan@nyu.edu",
   "nsf_id": "000558888",
   "pi_start_date": "2022-12-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Batia",
   "pi_last_name": "Wiesenfeld",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Batia M Wiesenfeld",
   "pi_email_addr": "bwiesenf@stern.nyu.edu",
   "nsf_id": "000651820",
   "pi_start_date": "2022-12-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "William",
   "pi_last_name": "Seiple",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "William H Seiple",
   "pi_email_addr": "wseiple@lighthouseguild.org",
   "nsf_id": "000661089",
   "pi_start_date": "2022-12-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University Medical Center",
  "inst_street_address": "550 1ST AVE",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2122638822",
  "inst_zip_code": "100166402",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NY12",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M5SZJ6VHUHN8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University Medical Center",
  "perf_str_addr": "240 East 38th Street",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100166402",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131Y00",
   "pgm_ele_name": "Convergence Accelerator Resrch"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 743451.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Introduction</strong><br /> The VIS<sup>4</sup>ION (Visually Impaired Smart Service System for Spatial Intelligence and Onboard Navigation) platform combines custom AI-systems with multimodal feedback to facilitate safe, efficient, and informed navigation for persons with blindness or low vision (pBLV). VIS<sup>4</sup>ION enhances opportunities for pBLV during travel to and around work, as well as for wider societal participation. VIS<sup>4</sup>ION provides end-to-end mapping, localization, and step-by-step navigation support, along with integrated scene understanding, which is communicated through speech and audio/tactile cues or alerts. All solutions are accessed through either an advanced wearable, configured as a lightweight, instrumented backpack, or a mobile application, available on most contemporary smartphones. Our approaches are infrastructure-free and do not require installing any hardware in the environment. This Phase 1 Convergence Accelerator grant has enabled the development of a robust hardware assistive technology platform and creation of new AI-systems that are available both locally 'on the device' or remotely through cell/WiFi access to our cloud-hosted AI-service ensembles. The VIS<sup>4</sup>ION wearable/application works to facilitate safe, informed pedestrian mobility outdoors, in public transport hubs, and within mapped indoor environments, such as workplaces, universities, and healthcare institutions.</p>\r\n<p><strong>Intellectual Merit</strong><br /> The project advances knowledge and technology in several domains through four key research thrusts:</p>\r\n<ol>\r\n<li><strong>Mapping      and Localization Software</strong><br /> Our infrastructure-free indoor/outdoor mapping, localization and      navigation AI-solution, 'UNav', has improved its localization      capabilities, along with higher-resolution trip-progression understanding - e.g., monitoring journeys through inertial measurement unit data and      optical flow processing. We explored the benefits of segmenting larger      maps into smaller, manageable sub-maps, and added inter-map connections to      allow for seamless travel between floors and buildings. We created      developer-friendly APIs, making this approach more accessible to      additional devices and projects, further enhancing the system's      scalability.</li>\r\n<li><strong>Wearable      Hardware with Haptics and Natural Language Interface</strong><br /> We have made enhancements to our haptic feedback belt that provides      tactile alerts and guidance to users. It now features a wireless, modular      design, improving its usability and versatility. We have integrated both      verbal dictation and speech synthesis software to facilitate natural      verbal interactions with the VIS<sup>4</sup>ION platform. We incorporated      visual-language models to provide scene descriptions and integrate      feedback from all of our AI-services into clear, concise summaries for the      user.</li>\r\n<li><strong>Wireless      Connectivity and Networking Infrastructure</strong><br /> The VIS<sup>4</sup>ION platform incorporates 5G wireless technologies and      adaptive video coding to ensure reliable remote AI processing even under      variable network conditions (good or bad cell/WiFi service). We use our      REBERA network rate-estimation methods to select optimal video compression      settings to ensure the best possible images are sent to servers to      maintain optimal AI performance. Our wearable and smartphone-agnostic      mobile application serve as gateways to this ecosystem, reimagining      application architecture and facilitating smooth transitions between      local-to-remote AI processing.</li>\r\n<li><strong>User      Trials</strong><br /> We conducted user trials of VIS<sup>4</sup>ION's UNav system with 20 pBLV.      We found that UNav significantly improved users' navigation abilities on a      wide range of metrics over standard, detailed, in-person travel      directions. This includes UNav enhancing the users' pathing efficiency,      reducing travel time, fewer wrong turns, reduced cane contacts, and      increased user confidence, with further benefits for fully blind users.      These findings validate the effectiveness of this technology and help      inform further development.</li>\r\n</ol>\r\n<p><strong>Broader Impacts</strong><br /> The VIS<sup>4</sup>ION project addresses the $3 trillion global cost of vision loss by targeting critical mobility challenges. Through its application in healthcare, the platform not only supports pBLV but also enhances wayfinding for all patients and staff. By improving navigation, the technology can reduce missed appointments, late arrivals, and related inefficiencies, representing a potential of $150 billion in annual savings in the U.S. healthcare sector.</p>\r\n<p>Moreover, the project fosters inclusion by tailoring solutions to the specific needs of the individual. It trains a diverse cohort of biomedical engineering students in interdisciplinary approaches, equipping them to tackle real-world challenges. Collaborations with non-profits, industry leaders, and government agencies ensure that the platform aligns with societal needs and regulatory standards.</p>\r\n<p><strong>Year 1 Achievements</strong><br /> The team achieved significant milestones, including:</p>\r\n<ul>\r\n<li>Developed      enhanced mapping, localization, and navigation features for UNav.</li>\r\n<li>Refined      haptic feedback and natural language interaction capabilities (more      user-friendly).</li>\r\n<li>Optimized      the VIS<sup>4</sup>ION platform and network infrastructure for robust      performance under diverse network conditions (i.e., opportunistic      computing).</li>\r\n<li>Conducted      successful user trials, highlighting measurable benefits in safety and      efficiency for pBLV across a series of experiments, including our      navigation-focused clinical trial.</li>\r\n</ul>\r\n<p><strong>Future Directions</strong><br /> Years 2 and 3 will focus on commercialization and sustainability, specifically targeting healthcare institutions. Planned pilot studies at NYU Langone facilities aim to showcase the system's effectiveness and financial benefits. These efforts will support broader adoption and integration into various sectors, transforming complex environments into accessible, connected spaces that support spatial equity.</p>\r\n<p><strong>Conclusion</strong><br /> The VIS<sup>4</sup>ION project exemplifies the potential of convergence research to address societal challenges. By advancing assistive technologies and fostering collaboration across disciplines, the project delivers tangible benefits to individuals and institutions while paving the way for future innovations across accessibility and inclusion.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/05/2025<br>\nModified by: John Ross&nbsp;Rizzo</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIntroduction\n The VIS4ION (Visually Impaired Smart Service System for Spatial Intelligence and Onboard Navigation) platform combines custom AI-systems with multimodal feedback to facilitate safe, efficient, and informed navigation for persons with blindness or low vision (pBLV). VIS4ION enhances opportunities for pBLV during travel to and around work, as well as for wider societal participation. VIS4ION provides end-to-end mapping, localization, and step-by-step navigation support, along with integrated scene understanding, which is communicated through speech and audio/tactile cues or alerts. All solutions are accessed through either an advanced wearable, configured as a lightweight, instrumented backpack, or a mobile application, available on most contemporary smartphones. Our approaches are infrastructure-free and do not require installing any hardware in the environment. This Phase 1 Convergence Accelerator grant has enabled the development of a robust hardware assistive technology platform and creation of new AI-systems that are available both locally 'on the device' or remotely through cell/WiFi access to our cloud-hosted AI-service ensembles. The VIS4ION wearable/application works to facilitate safe, informed pedestrian mobility outdoors, in public transport hubs, and within mapped indoor environments, such as workplaces, universities, and healthcare institutions.\r\n\n\nIntellectual Merit\n The project advances knowledge and technology in several domains through four key research thrusts:\r\n\r\nMapping      and Localization Software\n Our infrastructure-free indoor/outdoor mapping, localization and      navigation AI-solution, 'UNav', has improved its localization      capabilities, along with higher-resolution trip-progression understanding - e.g., monitoring journeys through inertial measurement unit data and      optical flow processing. We explored the benefits of segmenting larger      maps into smaller, manageable sub-maps, and added inter-map connections to      allow for seamless travel between floors and buildings. We created      developer-friendly APIs, making this approach more accessible to      additional devices and projects, further enhancing the system's      scalability.\r\nWearable      Hardware with Haptics and Natural Language Interface\n We have made enhancements to our haptic feedback belt that provides      tactile alerts and guidance to users. It now features a wireless, modular      design, improving its usability and versatility. We have integrated both      verbal dictation and speech synthesis software to facilitate natural      verbal interactions with the VIS4ION platform. We incorporated      visual-language models to provide scene descriptions and integrate      feedback from all of our AI-services into clear, concise summaries for the      user.\r\nWireless      Connectivity and Networking Infrastructure\n The VIS4ION platform incorporates 5G wireless technologies and      adaptive video coding to ensure reliable remote AI processing even under      variable network conditions (good or bad cell/WiFi service). We use our      REBERA network rate-estimation methods to select optimal video compression      settings to ensure the best possible images are sent to servers to      maintain optimal AI performance. Our wearable and smartphone-agnostic      mobile application serve as gateways to this ecosystem, reimagining      application architecture and facilitating smooth transitions between      local-to-remote AI processing.\r\nUser      Trials\n We conducted user trials of VIS4ION's UNav system with 20 pBLV.      We found that UNav significantly improved users' navigation abilities on a      wide range of metrics over standard, detailed, in-person travel      directions. This includes UNav enhancing the users' pathing efficiency,      reducing travel time, fewer wrong turns, reduced cane contacts, and      increased user confidence, with further benefits for fully blind users.      These findings validate the effectiveness of this technology and help      inform further development.\r\n\r\n\n\nBroader Impacts\n The VIS4ION project addresses the $3 trillion global cost of vision loss by targeting critical mobility challenges. Through its application in healthcare, the platform not only supports pBLV but also enhances wayfinding for all patients and staff. By improving navigation, the technology can reduce missed appointments, late arrivals, and related inefficiencies, representing a potential of $150 billion in annual savings in the U.S. healthcare sector.\r\n\n\nMoreover, the project fosters inclusion by tailoring solutions to the specific needs of the individual. It trains a diverse cohort of biomedical engineering students in interdisciplinary approaches, equipping them to tackle real-world challenges. Collaborations with non-profits, industry leaders, and government agencies ensure that the platform aligns with societal needs and regulatory standards.\r\n\n\nYear 1 Achievements\n The team achieved significant milestones, including:\r\n\r\nDeveloped      enhanced mapping, localization, and navigation features for UNav.\r\nRefined      haptic feedback and natural language interaction capabilities (more      user-friendly).\r\nOptimized      the VIS4ION platform and network infrastructure for robust      performance under diverse network conditions (i.e., opportunistic      computing).\r\nConducted      successful user trials, highlighting measurable benefits in safety and      efficiency for pBLV across a series of experiments, including our      navigation-focused clinical trial.\r\n\r\n\n\nFuture Directions\n Years 2 and 3 will focus on commercialization and sustainability, specifically targeting healthcare institutions. Planned pilot studies at NYU Langone facilities aim to showcase the system's effectiveness and financial benefits. These efforts will support broader adoption and integration into various sectors, transforming complex environments into accessible, connected spaces that support spatial equity.\r\n\n\nConclusion\n The VIS4ION project exemplifies the potential of convergence research to address societal challenges. By advancing assistive technologies and fostering collaboration across disciplines, the project delivers tangible benefits to individuals and institutions while paving the way for future innovations across accessibility and inclusion.\r\n\n\n\t\t\t\t\tLast Modified: 02/05/2025\n\n\t\t\t\t\tSubmitted by: John RossRizzo\n"
 }
}