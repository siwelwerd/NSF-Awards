{
 "awd_id": "2213235",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Real-Time Artificial Intelligence (AI) Bidirectional American Sign Language (ASL) Communication System",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2023-02-15",
 "awd_exp_date": "2024-12-31",
 "tot_intn_awd_amt": 256000.0,
 "awd_amount": 256000.0,
 "awd_min_amd_letter_date": "2023-02-08",
 "awd_max_amd_letter_date": "2024-08-27",
 "awd_abstract_narration": "The broader impact of this Small Business Innovation Research (SBIR) Phase I project is to improve the communication between Deaf and Hard of Hearing (D/HH) individuals and the hearing community through automated sign language recognition. In the United States alone there are over 48 million D/HH individuals, who in total possess $87 billion in purchasing power. It appears businesses are not adequately serving this community, as is evidenced by the plethora of Americans with Disabilities Act (ADA) lawsuits against numerous companies. The proposed technology will provide plug-and-play software for organizations to improve their interactions with D/HH individuals. Businesses and governments will be able to interact with their D/HH employees, customers, or constituents when interpreters are unavailable. This technology can be integrated into a variety of platforms, from retail point-of-sale equipment to chatbots and video/teleconferencing systems.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase 1 project aims to develop technology to perform unconstrained sign language recognition and natural sign language production. Specifically, current methods to train language translation models are ill-equipped to handle the sign language domain due to the lack of training data within this domain. Additionally, all currently established methods (apart from motion capture, which is unscalable) for producing American Sign Language (ASL) result in stilted, unnatural signing from an avatar. This project will develop solutions to these issues within the domain of ASL via semi-supervised expert-augmented models and data augmentation techniques. Technical hurdles include the lack of models to handle high-dimensional low-resource language domains, and lack of sufficiently large datasets. Technical milestones include creating semi-supervised datasets, engineering data augmentation techniques, generating a natural signing avatar, and performing extensive usability testing. This project aims to produce a method for automatically interpreting between a low-resource sign language and English to improve accessibility and increase equity for the Deaf and Hard of Hearing communities.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nicholas",
   "pi_last_name": "Wilkins",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Nicholas P Wilkins",
   "pi_email_addr": "nicholas@sign-speak.com",
   "nsf_id": "000837503",
   "pi_start_date": "2023-02-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SIGN-SPEAK Inc",
  "inst_street_address": "104 EAST AVE STE 205",
  "inst_street_address_2": "",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "3473731040",
  "inst_zip_code": "146042502",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "SIGN-SPEAK INC",
  "org_prnt_uei_num": "KUL7Z2MRFNM3",
  "org_uei_num": "KUL7Z2MRFNM3"
 },
 "perf_inst": {
  "perf_inst_name": "SIGN-SPEAK LLC",
  "perf_str_addr": "7290 Shallow Creek Trail APT F",
  "perf_city_name": "Victor",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "145649446",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6856",
   "pgm_ref_txt": "ARTIFICIAL INTELL & COGNIT SCI"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01AB2324DB",
   "fund_name": "R&RA DRSA DEFC AAB",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 256000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-48442fce-7fff-6f9e-9870-e7624887ed11\"> </span></p>\r\n<p dir=\"ltr\">The Phase I project, Real-Time AI Bidirectional ASL Communication System, aimed to develop the first-ever continuous sign language recognition model and sign language production through an avatar. Our goal was to create a technology capable of aiding interpretation and providing functional equivalence for the 48 million Deaf and Hard of Hearing (D/HH) Americans and, eventually, for the 466 million D/HH individuals globally who are marginalized and excluded from the economy. In the U.S., this marginalization is evidenced by a 16% employment gap (54% of D/HH individuals employed vs. 70% of hearing individuals), a 15% education gap (18% of D/HH individuals with bachelor's degrees vs. 33% of hearing individuals), and a 22.1% labor force participation disparity (42.9% of D/HH individuals not participating vs. 20.8% of hearing individuals). These disparities highlight the ongoing challenges faced by D/HH individuals in a world largely designed without effective sign language communication access, forcing them to constantly devise alternative solutions to achieve meaningful engagement and opportunities.</p>\r\n<p dir=\"ltr\"><span>These are not just statistics to our team - they represent disparities we live with every day. Therefore, this is not just a technical challenge but also a deeply personal mission for us.</span></p>\r\n<p dir=\"ltr\"><strong>Phase I Achievements</strong></p>\r\n<p dir=\"ltr\"><span>Before Phase I, we achieved promising preliminary results in continuous sign recognition using a limited dataset. However, we recognized this wasn't sufficient to meet the needs of stakeholders - whether businesses or the D/HH community. Phase I focused on addressing two significant challenges:</span></p>\r\n<p dir=\"ltr\"><span><span> </span></span><span>1.</span><span><span> </span></span><span>Developing a continuous sign language recognition model.</span></p>\r\n<p dir=\"ltr\"><span><span> </span></span><span>2.</span><span><span> </span></span><span>Creating a voice-to-sign language production avatar.</span></p>\r\n<p dir=\"ltr\"><span>These technologies required innovative approaches in generative AI and computer vision, as sign language lacks a written representation. Additionally, we aimed to integrate these models into a bi-directional software system and test them with businesses. During Phase I, we made remarkable progress:</span></p>\r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Technology Development:</span><span> Sign-Speak became the first to bring to market a continuous sign language recognition model paired with a human-like avatar capable of translating voice into sign language and vice versa.</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Research Contributions:</span><span> We published two peer-reviewed papers (one accepted and one currently undergoing peer review) - one on the necessity of bi-directional systems from a user experience perspective and another comparing our system's accuracy to current interpretation standards.</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Community Engagement:</span><span> We conducted hundreds of usability tests to ensure the system met real-world needs. Through these efforts, we fostered a strong community of users and advocates who provided invaluable feedback to refine the system.</span></p>\r\n</li>\r\n</ul>\r\n<p dir=\"ltr\"><strong>Broader Impacts</strong></p>\r\n<p dir=\"ltr\"><span>This work has significant implications for all stakeholders. Organizations can now provide a cost-effective solution, the D/HH community gains 24/7 access to interpretation, and human interpreters can focus on complex situations. Additionally, this innovation extends beyond interpretation. By enhancing the efficiency of sign language interpretation, Sign-Speak empowers D/HH individuals to participate more fully in environments reliant on voice-activated technology.</span></p>\r\n<p dir=\"ltr\"><span>Our efforts also raise awareness of the need for bi-directional communication systems, ensuring that technological advancements prioritize accessibility. Looking ahead to Phase II, we are committed to continuing this momentum and building upon the patented technologies and advancements we have achieved.</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/28/2025<br>\nModified by: Nicholas&nbsp;P&nbsp;Wilkins</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nThe Phase I project, Real-Time AI Bidirectional ASL Communication System, aimed to develop the first-ever continuous sign language recognition model and sign language production through an avatar. Our goal was to create a technology capable of aiding interpretation and providing functional equivalence for the 48 million Deaf and Hard of Hearing (D/HH) Americans and, eventually, for the 466 million D/HH individuals globally who are marginalized and excluded from the economy. In the U.S., this marginalization is evidenced by a 16% employment gap (54% of D/HH individuals employed vs. 70% of hearing individuals), a 15% education gap (18% of D/HH individuals with bachelor's degrees vs. 33% of hearing individuals), and a 22.1% labor force participation disparity (42.9% of D/HH individuals not participating vs. 20.8% of hearing individuals). These disparities highlight the ongoing challenges faced by D/HH individuals in a world largely designed without effective sign language communication access, forcing them to constantly devise alternative solutions to achieve meaningful engagement and opportunities.\r\n\n\nThese are not just statistics to our team - they represent disparities we live with every day. Therefore, this is not just a technical challenge but also a deeply personal mission for us.\r\n\n\nPhase I Achievements\r\n\n\nBefore Phase I, we achieved promising preliminary results in continuous sign recognition using a limited dataset. However, we recognized this wasn't sufficient to meet the needs of stakeholders - whether businesses or the D/HH community. Phase I focused on addressing two significant challenges:\r\n\n\n 1. Developing a continuous sign language recognition model.\r\n\n\n 2. Creating a voice-to-sign language production avatar.\r\n\n\nThese technologies required innovative approaches in generative AI and computer vision, as sign language lacks a written representation. Additionally, we aimed to integrate these models into a bi-directional software system and test them with businesses. During Phase I, we made remarkable progress:\r\n\r\n\r\n\n\nTechnology Development: Sign-Speak became the first to bring to market a continuous sign language recognition model paired with a human-like avatar capable of translating voice into sign language and vice versa.\r\n\r\n\r\n\n\nResearch Contributions: We published two peer-reviewed papers (one accepted and one currently undergoing peer review) - one on the necessity of bi-directional systems from a user experience perspective and another comparing our system's accuracy to current interpretation standards.\r\n\r\n\r\n\n\nCommunity Engagement: We conducted hundreds of usability tests to ensure the system met real-world needs. Through these efforts, we fostered a strong community of users and advocates who provided invaluable feedback to refine the system.\r\n\r\n\r\n\n\nBroader Impacts\r\n\n\nThis work has significant implications for all stakeholders. Organizations can now provide a cost-effective solution, the D/HH community gains 24/7 access to interpretation, and human interpreters can focus on complex situations. Additionally, this innovation extends beyond interpretation. By enhancing the efficiency of sign language interpretation, Sign-Speak empowers D/HH individuals to participate more fully in environments reliant on voice-activated technology.\r\n\n\nOur efforts also raise awareness of the need for bi-directional communication systems, ensuring that technological advancements prioritize accessibility. Looking ahead to Phase II, we are committed to continuing this momentum and building upon the patented technologies and advancements we have achieved.\r\n\n\n\t\t\t\t\tLast Modified: 01/28/2025\n\n\t\t\t\t\tSubmitted by: NicholasPWilkins\n"
 }
}