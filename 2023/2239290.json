{
 "awd_id": "2239290",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Building Next-Generation Language Models Based on Retrieval",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922972",
 "po_email": "emiltsak@nsf.gov",
 "po_sign_block_name": "Eleni Miltsakaki",
 "awd_eff_date": "2023-02-15",
 "awd_exp_date": "2028-01-31",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 236060.0,
 "awd_min_amd_letter_date": "2023-02-06",
 "awd_max_amd_letter_date": "2024-03-20",
 "awd_abstract_narration": "Large language models (LMs) have revolutionized the field of natural language processing, achieving state-of-the-art performance in a wide range of downstream tasks. Despite the success, these LMs are trained on enormous amounts of text data and cost a massive amount to create and run. Additionally, they are inherently difficult to interpret, challenging to update with ever-changing real-world information, and may leak private user information. This proposal seeks to develop an alternative paradigm to standard language modeling: retrieval-based language models, with the aim of reducing training and inference costs while also providing benefits such as better interpretability, adaptability, and privacy. This research will be integrated into education through new teaching modules in developing undergraduate and graduate natural language processing courses, promoting education for undergraduate research, and outreach to K-12 students and teachers from underrepresented communities.\r\n\r\nThis project addresses a full pipeline including training, scaling, adapting, and using retrieval-based language models and is organized into four components, including (1) building a general learning framework for retrieval-based LMs and developing scalable algorithms to support end-to-end learning; (2) investigating the scaling law of retrieval-based LMs and developing better data quantization to improve inference efficiency; (3) devising methods to quickly update retrieval-based LMs and adapt them to unseen and privacy-sensitive domains; (4) designing effective approaches to use retrieval-based LMs on downstream tasks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Danqi",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Danqi Chen",
   "pi_email_addr": "danqic@cs.princeton.edu",
   "nsf_id": "000819577",
   "pi_start_date": "2023-02-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "619 Alexander Road - Suite 102",
  "perf_city_name": "PRINCETON",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085406000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002728DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002627DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 117399.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 118661.0
  }
 ],
 "por": null
}