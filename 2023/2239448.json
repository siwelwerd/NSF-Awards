{
 "awd_id": "2239448",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Statistical Learning with Recursive Partitioning: Algorithms, Accuracy, and Applications",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2023-06-01",
 "awd_exp_date": "2028-05-31",
 "tot_intn_awd_amt": 450001.0,
 "awd_amount": 175410.0,
 "awd_min_amd_letter_date": "2022-12-28",
 "awd_max_amd_letter_date": "2024-08-23",
 "awd_abstract_narration": "As data-driven technologies continue to be adopted and deployed in high-stakes decision-making environments, the need for fast, interpretable algorithms has never been more important. As one such candidate, it has become increasingly common to use decision trees, a hierarchically organized data structure, for building a predictive or causal model. This trend is spurred by the appealing connection between decision trees and rule-based decision-making, particularly in clinical, legal, or business contexts, as the tree structure mimics the sequential way a human user may think and reason, thereby facilitating human-machine interaction. To make them fast to compute, decision trees are popularly constructed with an algorithm called recursive partitioning, in which the decision nodes of the tree are learned from the data in a greedy, top-down manner. The overarching goal of this project is to develop a precise understanding of the strengths and limitations of decision trees based on recursive partitioning, and, in doing so, gain insights on how to improve their performance in practice. In addition to this impact, high-school, undergraduate, and graduate research assistants will be vertically integrated and benefit both academically and professionally. Innovative curricula, workshops, and data and methods competitions involving students, academics, and industry professionals will facilitate outreach and encourage participation from a broad audience. \r\n\r\nThis proposal aims to provide a comprehensive study of the statistical properties of greedy recursive partitioning algorithms for training decision trees, as is demonstrated in two fundamental contexts. The first thrust of the project will develop a theoretical framework for the analysis of oblique decision trees, where, in contrast to conventional axis-aligned splits involving only a single covariate, the splits at each decision node occur at linear combinations of the covariates. While this methodology has garnered significant attention from the computer science and optimization communities since the mid-80s, the advantages they offer over their axis-aligned counterparts remain only empirically justified, and explanations for their success are largely based on heuristics. Filling this long-standing gap between theory and practice, the PI will investigate how oblique regression trees, constructed by recursively minimizing squared error, can adapt to a rich class of regression models consisting of linear combinations of ridge functions. This provides a quantitative baseline for a statistician to compare and contrast decision trees with other less interpretable methods, such as projection pursuit regression and neural networks, that target similar model forms. Crucially, to address the combinatorial complexity of finding the optimal splitting hyperplane at each decision node, the PI\u2019s framework can accommodate many existing computational tools in the literature. A major component of the research is derived from connections between recursive partitioning and sequential greedy approximation algorithms for convex optimization problems (e.g., orthogonal greedy algorithms).  The second thrust focuses on the delicate pointwise properties of axis-aligned recursive partitioning, with implications for heterogeneous causal effect estimation, where accurate pointwise estimates over the entire support of the covariates are essential for valid inference (e.g., testing hypotheses and constructing confidence intervals). Motivated by simple setting where decision trees provably fail to achieve optimal performance, the PI will investigate how the signal-to-noise ratio affects the quality of pointwise estimation. While the focus is on causal effect estimation directly using decision trees, the PI will also investigate implications for multi-step semi-parametric settings, where preliminary unknown functions (e.g., propensity scores) are estimated with machine learning tools, as well as conditional quantile regression, both of which require estimators with high pointwise accuracy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Klusowski",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Jason M Klusowski",
   "pi_email_addr": "jason.klusowski@princeton.edu",
   "nsf_id": "000791723",
   "pi_start_date": "2022-12-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "98 Charlton Street",
  "perf_city_name": "PRINCETON",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002627DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002728DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 84472.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 90938.0
  }
 ],
 "por": null
}