{
 "awd_id": "2232959",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "STTR Phase I:  Swine Automatic Lameness Sensor (SALS)",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032924392",
 "po_email": "amonk@nsf.gov",
 "po_sign_block_name": "Alastair Monk",
 "awd_eff_date": "2023-05-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 275000.0,
 "awd_amount": 295000.0,
 "awd_min_amd_letter_date": "2023-04-27",
 "awd_max_amd_letter_date": "2024-07-11",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase 1 project is to provide an in-farm sensing system that will notify sow (adult female swine) farmers of early signs of animal lameness, and thereby reduce early sow mortality and enhance farm productivity. The technology uses artificial intelligence to analyze pig locomotion in order to spot subtle patterns indicative of lameness. Early detection of lameness will enable farmers to take corrective actions rather than waiting for lameness to deteriorate to sow death or culling.  Early culling or sow death is a major economic cost to farmers and a large fraction of death and culls is due to animal lameness.  Successful application of the technology being developed in this project promises to reduce early sow mortality and culling, leading to additional litters per sow and so provide a significant economic boost to farmers.  With patent applications for key components of the sensing system, farmers will install sensors in hallways and obtain health measures for each sow when she moves between rooms. The projected annual revenue is $3.0 million.\r\n\r\nThis Small Business Technology Transfer (STTR) Phase I project proposes combining an imaging sensor with artificial intelligence to create a unique sensing system to unobtrusively and remotely diagnose lameness in sows (adult female swine) as they traverse hallways.  This project seeks to validate two key technical contributions. First, precise 3D animal posture and locomotion are estimated for sows moving beneath a ceiling-mounted sensor.  High accuracy is achieved through a novel annotation technique that overcomes difficulties in inaccurate manual location of skeletal landmarks.  Second, a data-driven approach is used to train a deep neural network to learn the most discriminating combinations of posture and gait for determining lameness in walking sows.  A self-supervised neural network sidesteps the need for extensive manual annotation and expert annotation is only required for lameness assessment.  Together, these two contributions will enable a transformative technical capability of a remote sensor that can automatically diagnose early-stage lameness in sows.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "McIntyre",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "John McIntyre",
   "pi_email_addr": "jmcintyre@motiongrazer.com",
   "nsf_id": "000882640",
   "pi_start_date": "2023-04-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yuzhen",
   "pi_last_name": "Lu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuzhen Lu",
   "pi_email_addr": "luyuzhen@msu.edu",
   "nsf_id": "000915614",
   "pi_start_date": "2023-04-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "MOTION GRAZER AI, INC.",
  "inst_street_address": "325 E GRAND RIVER AVE",
  "inst_street_address_2": "",
  "inst_city_name": "EAST LANSING",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "6168899611",
  "inst_zip_code": "488234384",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MI07",
  "org_lgl_bus_name": "MOTION GRAZER AI, INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "DWBFHYLTN9R7"
 },
 "perf_inst": {
  "perf_inst_name": "Motion Grazer AI, Inc.",
  "perf_str_addr": "325 E GRAND RIVER AVE",
  "perf_city_name": "EAST LANSING",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "488234384",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MI07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "150500",
   "pgm_ele_name": "STTR Phase I"
  },
  {
   "pgm_ele_code": "809100",
   "pgm_ele_name": "SBIR Outreach & Tech. Assist"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8018",
   "pgm_ref_txt": "Smart and Connected Health"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01AB2324DB",
   "fund_name": "R&RA DRSA DEFC AAB",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 275000.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 20000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The purpose of this project is to leverage computer vision and artificial intelligence (AI) for improved productivity and animal welfare, Motion Grazer AI, in collaboration with its subcontractor, Michigan State University, conducted a project aimed at addressing two major needs for sow farmers: (1) automatic detection of early-stage lameness, and (2) automatic detection of poor body condition. This project resulted in an upgraded sensor device for on-farm imagery collection of sows and computer vision algorithms developed for identifying lame sows and predicting sow caliper scores from live video streams.</p>\n<p>&nbsp;</p>\n<p>The sensor device, called SIMKit (SIM = Sows in Motion), mainly consists of a color-depth camera and an onboard computing unit housed in a protective enclosure. Mounted on the ceiling of a farm hallway, the device automatically collects top-down view imagery of sows at 30 frames per second when they move along a hallway and uploads sow data to cloud storage using the farm network. To support algorithm development, we made extensive efforts to collect sow data and manually annotate the dataset. In the course of 16 farm visits, 8.45 TB of data, including color and depth video streams, were acquired for a total of 1,275 sows including the data for 152 lame sows. Moreover, we implemented an updated neural network-based, sow posture detector and tracker in our software program, which enables precisely tracking each sow as it moves beneath the SIMKit. To achieve the objective of lameness detection, we proposed and trained a novel neural network-based pipeline that was capable of predicting the position and locomotion of sow landmarks over time and discriminating lame from healthy sows. Tested on our annotated dataset, the pipeline achieved an average accuracy of 2.1 cm in estimating the 3D landmarks, and 85.2% accuracy, 84.8% precision, 75.6% recall, and 76.1% Average Precision for lameness detection. For body condition scoring, we proposed a depth-based virtual caliper for automated prediction of sow caliper scores. The virtual caliper extracts geometric features from a slice along the last rib of sows and correlates them with the physical caliper measurement of the sows based on regression modeling. Our multilayer perception-based virtual caliper achieved respectable accuracy, 93.8% within &plusmn;1.5 caliper values of the ground truth.</p>\n<p>&nbsp;</p>\n<p>This project has demonstrated the promise of using computer vision with AI to complement or replace manual labor for lameness detection and body condition scoring.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 10/13/2024<br>\nModified by: John&nbsp;Mcintyre</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe purpose of this project is to leverage computer vision and artificial intelligence (AI) for improved productivity and animal welfare, Motion Grazer AI, in collaboration with its subcontractor, Michigan State University, conducted a project aimed at addressing two major needs for sow farmers: (1) automatic detection of early-stage lameness, and (2) automatic detection of poor body condition. This project resulted in an upgraded sensor device for on-farm imagery collection of sows and computer vision algorithms developed for identifying lame sows and predicting sow caliper scores from live video streams.\n\n\n\n\n\nThe sensor device, called SIMKit (SIM = Sows in Motion), mainly consists of a color-depth camera and an onboard computing unit housed in a protective enclosure. Mounted on the ceiling of a farm hallway, the device automatically collects top-down view imagery of sows at 30 frames per second when they move along a hallway and uploads sow data to cloud storage using the farm network. To support algorithm development, we made extensive efforts to collect sow data and manually annotate the dataset. In the course of 16 farm visits, 8.45 TB of data, including color and depth video streams, were acquired for a total of 1,275 sows including the data for 152 lame sows. Moreover, we implemented an updated neural network-based, sow posture detector and tracker in our software program, which enables precisely tracking each sow as it moves beneath the SIMKit. To achieve the objective of lameness detection, we proposed and trained a novel neural network-based pipeline that was capable of predicting the position and locomotion of sow landmarks over time and discriminating lame from healthy sows. Tested on our annotated dataset, the pipeline achieved an average accuracy of 2.1 cm in estimating the 3D landmarks, and 85.2% accuracy, 84.8% precision, 75.6% recall, and 76.1% Average Precision for lameness detection. For body condition scoring, we proposed a depth-based virtual caliper for automated prediction of sow caliper scores. The virtual caliper extracts geometric features from a slice along the last rib of sows and correlates them with the physical caliper measurement of the sows based on regression modeling. Our multilayer perception-based virtual caliper achieved respectable accuracy, 93.8% within 1.5 caliper values of the ground truth.\n\n\n\n\n\nThis project has demonstrated the promise of using computer vision with AI to complement or replace manual labor for lameness detection and body condition scoring.\n\n\n\t\t\t\t\tLast Modified: 10/13/2024\n\n\t\t\t\t\tSubmitted by: JohnMcintyre\n"
 }
}