{
 "awd_id": "2247309",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: PPoSS: Planning: Cross-layer Coordination and Optimization for Scalable and Sparse Tensor Networks (CROSS)",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927885",
 "po_email": "abanerje@nsf.gov",
 "po_sign_block_name": "Anindya Banerjee",
 "awd_eff_date": "2022-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 62490.0,
 "awd_amount": 62490.0,
 "awd_min_amd_letter_date": "2023-01-06",
 "awd_max_amd_letter_date": "2023-01-06",
 "awd_abstract_narration": "High-dimensional data computation or analytics are gaining importance in many domains, such as quantum chemistry/physics, quantum circuit simulation, brain processing, social networks, healthcare and machine/deep learning, to name a few. Tensors, a representation of high-dimensional data, are playing an increasingly critical role, and so are tensor methods. Tensor decompositions or factorizations of low-dimensional data (three to five dimensions) have been extensively studied over the past years from a high-performance computing and also compiler and computer architecture angles for their computational core operations, while tensor networks targeting very high-dimensional data (over ten dimensions) and extracting physically meaningful latent variables are underdeveloped because of their complicated mathematical nature, extremely high computational complexity, and more domain-dependent challenges. The project\u2019s novelties are manifold: 1) memory heterogeneity-aware representations with algorithm and system optimizations, which could be adopted to solve other problems such as irregular applications and sparse numerical methods; 2) hardware-software co-design of specialized, sparse-tensor network-accelerator architectures, that are among the first hardware implementations of sparse-tensor networks. The project\u2019s impacts are 1) advancing state-of-the-art tensor decomposition studies to model true higher-order and sparse data; 2) triggering a closer long-term collaboration ranging from academia to research labs to industry by studying solicitous applications; 3) bringing appropriate educational opportunities.\r\n\r\nThis project proposes Cross-layer cooRdination and Optimization for Scalable and Sparse-Tensor Networks (CROSS) for heterogeneous systems that are equipped with various types of accelerators, such as GPUs, TPUs and FPGAs, as well as heterogeneous memories with dynamic and non-volatile random-access memories (DRAM+NVRAM). This research aims to study the sparsity in widely used tensor networks by introducing constraints, regularization, dictionaries, and/or domain knowledge for better data compression, faster computation, lower memory usage and better interpretability. Besides the sparsity challenges, sparse-tensor networks also suffer from the curse of dimensionality, aggravated data randomness and irregular program and memory access behaviors. This planning project conducts preliminary research that aims to address these challenges from four perspectives: (1) memory heterogeneity-aware representations and data (re-)arrangement, (2) balanced sparse tensor contraction (SpTC) algorithms with smart page arrangement, (3) memoization and intelligent allocation to reduce computational cost, and (4) specialized accelerator architectures for sparse-tensor networks. The optimized sparse tensor networks will encompass efforts from high-performance computing, algorithms, compilers, computer architecture and performance modeling and will be tested under multiple application scenarios.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jiajia",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jiajia Li",
   "pi_email_addr": "jiajia.li@ncsu.edu",
   "nsf_id": "000848868",
   "pi_start_date": "2023-01-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "2601 WOLF VILLAGE WAY",
  "perf_city_name": "RALEIGH",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "27607",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 62490.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project initiates the planning stage for the PPoSS project. It has produced three software artifacts, one published paper, one accepted paper, two technical reports, one research poster, and one M.S. thesis.</p>\r\n<p>A significant outcome of this project -- although this work has not yet been published but is currently under preparation -- is the establishment of the validity of introducing data sparsity into factor tensors. This finding demonstrates that tensor networks, as a low-rank decomposition method, can introduce not only model sparsity (i.e., low rank) but also element-wise data sparsity. This insight lays the foundation for the PPoSS large project, showing that tensor network-decomposed data can be further compressed by incorporating data sparsity without compromising accuracy. Our collaborators from quantum application domains are particularly excited about this discovery.</p>\r\n<p>Additionally, we have explored different types of sparsity -- including symmetric, jagged, and diagonal sparsity -- across various applications such as graph neural networks, data analytics, and quantum circuit simulation. We have also developed corresponding algorithms by leveraging mathematical properties and/or architectural characteristics. These efforts have established valuable examples that provide insights for the PPoSS large project.</p>\r\n<p><span>The project benefits education at both the graduate and undergraduate levels. It has inspired more project and assignment topics. Additionally, a parallel algorithms class will be offered at NCSU, with a focus on algorithm analysis, particularly for irregular data.</span>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/04/2025<br>\nModified by: Jiajia&nbsp;Li</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project initiates the planning stage for the PPoSS project. It has produced three software artifacts, one published paper, one accepted paper, two technical reports, one research poster, and one M.S. thesis.\r\n\n\nA significant outcome of this project -- although this work has not yet been published but is currently under preparation -- is the establishment of the validity of introducing data sparsity into factor tensors. This finding demonstrates that tensor networks, as a low-rank decomposition method, can introduce not only model sparsity (i.e., low rank) but also element-wise data sparsity. This insight lays the foundation for the PPoSS large project, showing that tensor network-decomposed data can be further compressed by incorporating data sparsity without compromising accuracy. Our collaborators from quantum application domains are particularly excited about this discovery.\r\n\n\nAdditionally, we have explored different types of sparsity -- including symmetric, jagged, and diagonal sparsity -- across various applications such as graph neural networks, data analytics, and quantum circuit simulation. We have also developed corresponding algorithms by leveraging mathematical properties and/or architectural characteristics. These efforts have established valuable examples that provide insights for the PPoSS large project.\r\n\n\nThe project benefits education at both the graduate and undergraduate levels. It has inspired more project and assignment topics. Additionally, a parallel algorithms class will be offered at NCSU, with a focus on algorithm analysis, particularly for irregular data.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 02/04/2025\n\n\t\t\t\t\tSubmitted by: JiajiaLi\n"
 }
}