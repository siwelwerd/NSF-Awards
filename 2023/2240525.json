{
 "awd_id": "2240525",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Data Driven Predictive Auditory Cues for Safety and Fluency in Human-Robot Interaction",
 "cfda_num": "47.041, 47.070",
 "org_code": "05020000",
 "po_phone": "7032924702",
 "po_email": "cye@nsf.gov",
 "po_sign_block_name": "Cang Ye",
 "awd_eff_date": "2023-06-15",
 "awd_exp_date": "2026-05-31",
 "tot_intn_awd_amt": 422254.0,
 "awd_amount": 422254.0,
 "awd_min_amd_letter_date": "2023-06-06",
 "awd_max_amd_letter_date": "2023-06-06",
 "awd_abstract_narration": "Most industrial and social robots are not sufficiently aware of their surroundings, which leads to a wide range of injuries. This can hamper fluent and efficient human-robot work. This project focuses on developing, integrating, and testing a set of sound cues for robotic movements, with the goals of enhancing human safety and allowing fluent interaction. The sound cues are created using algorithms which provide rich information about the robots\u2019 current and future actions, alerting humans to potential hazards, and allowing them to prepare and adjust their work space. Such sound cues bear the promise of using aa non-distracting auditory channel to help humans plan their actions and responses to robotic actions. The system is based on music-driven robotic maneuvers using a novel audio generation method that will provide information about the robotic movements. The algorithm used is trained on a newly created dataset of audio clips with risk information.  The system can increase safety, fluency and trust building in human-robot interaction in industrial and personal robots, private and public spaces, addressing tasks in manufacturing, training, education, and others.\r\n\r\nTo address this goal the project is divided into four phases: Phase 1 - collection, analysis, labeling and feature extraction of a newly created dataset of audio clips. Phase 2 - development of a novel neural network model that will be trained on the collected dataset in correlation to labeled set of robotic movements. The output of the model will then be fed to a neural network that will generate long-context raw audio conditioned by musical and gestural features.   Phase 3 - integration of the generated audio cues into a large set of robotic gestures with the goal of representing robotic motion and future actions. Phase 4 - a comprehensive evaluation study of the sonified robotic gestures for safety and fluency in a variety of human-robot interaction scenarios.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gil",
   "pi_last_name": "Weinberg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gil Weinberg",
   "pi_email_addr": "gil.weinberg@coa.gatech.edu",
   "nsf_id": "000105700",
   "pi_start_date": "2023-06-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320415",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "144Y00",
   "pgm_ele_name": "FRR-Foundationl Rsrch Robotics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "6840",
   "pgm_ref_txt": "ROBOTICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 422254.0
  }
 ],
 "por": null
}