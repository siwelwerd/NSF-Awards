{
 "awd_id": "2327564",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Identifying and Producing Code-Switching in Languages from Spoken, Lexical and Socio-linguistic Features",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2023-06-15",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 108881.0,
 "awd_amount": 108881.0,
 "awd_min_amd_letter_date": "2023-06-05",
 "awd_max_amd_letter_date": "2023-06-05",
 "awd_abstract_narration": "This Early Grant for Exploratory Research investigates conversations between the vast number of persons in our world who speak multiple languages and who frequently switch back and forth between those languages in what is called \u201ccode-switching\u201d. It is important for speech dialogue systems and voice assistants to not only be able to identify when, why, and to what effect code-switching occurs, but also to correctly interpret what is said and to be able to generate similarly code-switched responses when interacting with such users. Advances in speech technology in recent years have resulted in widespread use of voice assistants such as Siri, Google Assistant and Alexa. They enable vast improvement in information access by voice for languages such as English, French, German, Cantonese, Mandarin, and Spanish. However, such access is limited to monolingual speech, which for many multilingual speakers is not the most natural form of speech production. Thus, code-switched speech is rarely understood correctly and is never able to be produced in assistant responses. A major barrier to enabling naturalistic and comfortable communication for these speakers is the lack of speech technology that can not only understand code-switched input but also produce similar human-like output. \r\n\r\nThis project addresses these issues by examining how spoken and written code-switching interacts with other aspects of language communication. It will explore research questions not yet studied in code-switching research including (1) whether speakers entrain, speak more similarly, on pronunciation and other strategies of code-switching in speech; (2) whether there is a quantifiable relationship between code-switching and empathy in speech, where empathy is a speaker's intention to convey that they understand another's problems and want to help address them; (3) whether the presence of named entities, such as names or geographical locations, primes code-switching; (4) which dialogue acts, such as questions or statements or backchannels, tend to be produced most often in code-switched speech; and (5) how speakers produce intonational contours when they code-switch (via choosing their intonation production to match either of the languages they are producing or by being different from both?) Statistical and machine-learning techniques will both be used to address these questions in the context of spoken and lexical-feature-tagged code-switched speech in Standard American English, Spanish, Mandarin Chinese, and Hindi. By identifying new aspects of code-switching, the project will seed further exploration of this phenomenon by the research community.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Julia",
   "pi_last_name": "Hirschberg",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Julia B Hirschberg",
   "pi_email_addr": "julia@cs.columbia.edu",
   "nsf_id": "000399629",
   "pi_start_date": "2023-06-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "202 LOW LIBRARY 535 W 116 ST MC 4309,",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "10027",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 108881.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In&nbsp;our NSF&nbsp;EAGER &ldquo;Identifying and Producing Code-Switching in Languages from Spoken, Lexical and Socio-linguistic Features&rdquo; we have developed methods and tools to identify multiple aspects of code-switching in text and speech&nbsp;Rapid advances in speech technology in recent years have resulted in widespread use of voice assistants such as Siri (Apple), Google Assistant and Alexa (Amazon). These advances enable vast improvements in information access by voice for languages such as English, French, German, Cantonese, Mandarin, and Spanish. However, such access is limited to &nbsp;<strong><em>monolingual</em></strong>&nbsp;speech, which for billions of multilingual speakers may not always be the most natural form of speech production. Thus, speakers who code-switch frequently must limit themselves to&nbsp;<strong><em>one</em></strong>&nbsp;of their spoken languages in order to enjoy the same communication and search capabilities that monolingual speakers do. &nbsp;A major barrier to enabling naturalistic and comfortable communication for these speakers is the lack of speech technology that can not only understand code-switched input, but also produce similar human-like output.&nbsp;Our overall goal has been to better prepare voice assistants to interact with persons&nbsp;who speak multiple languages.&nbsp;</p>\n<p>This past year we have studied several aspects of code-switching to this end:&nbsp;&nbsp;1) Do speakers&nbsp;<strong><em>entrain</em></strong>&nbsp;on pronunciation and other strategies of code-switching in speech, speaking more like one another? and 2) Is there a quantifiable relationship between code-switching and&nbsp;<strong><em>empathy</em></strong>&nbsp;in speech, where empathy is a speaker&rsquo;s intention to convey that they understand another&rsquo;s problems and want to help address them.&nbsp;We began our research with&nbsp;three existing code-switched language corpora:&nbsp;&nbsp;the Miami Bangor Corpus (English/Spanish) the SEAME Corpus (English/Mandarin), and the MaSaC Corpus (English/Hindi).</p>\n<p>From our early experiments on entrainment in the lexical features of code-switched conversations in one corpus, the Miami Bangor, we found statistically significant evidence of entrainment across all of the word classes considered (most frequent words in the corpus (top 100 and top 25), affirmative cue words, and filled pauses, as well as on overall language use within the corpus (the binary presence of CSW, amount of CSW, and the CSW strategy used) &nbsp;Our experiments on acoustic-prosodic features also provide evidence of entrainment in terms of turn-level&nbsp;<strong><em>proximity</em></strong>&nbsp;(similarity between speakers over a conversation) and&nbsp;<strong><em>convergence</em></strong>&nbsp;(increase in similarly across a conversation). &nbsp;Also, our preliminary qualitative gender analysis showed a slight skew toward entrainment taking place in same-gender over mixed-gender conversations.&nbsp;&nbsp;&nbsp;</p>\n<p>From our more recent experiments on empathetic speech in all three of our language pairs we developed models to classify empathy based on another English corpus labeled for empathetic, non-empathetic and neutral text and speech and translated our corpora into English. &nbsp;We&nbsp;found&nbsp;a stronger positive relationship of spoken CSW with the lexical correlates of empathy than with the acoustic-prosodic correlates, although there is some positive correlation in these as well.<em>&nbsp;</em>These findings serve as a first step toward validating prior qualitative work on ampathetic speech and answering the question of whether the expression of empathy is a motivation for CSW in speech.&nbsp;</p>\n<p>From this work we have published two papers:&nbsp;&nbsp;</p>\n<ul>\n<li>Debasmita Bhattacharya, Siying Ding, Alayna Nguyen, Julia Hirschberg, &ldquo;Measuring Entrainment in Spontaneous Code-Switched Speech,&rdquo; NAACL 2024, Mexico City, Mexico.</li>\n<li>Debasmita Bhattacharya, Eleanor Lin, Run Chen, Julia Hirschberg,&nbsp;&ldquo;Switching Tongues, Sharing Hearts: Identifying the Relationship between Empathy and Code-switching in Speech,&rdquo;&nbsp;Interspeech 2024, Kos Island, Greece.</li>\n</ul>\n<p>We hope that this work and its future extensions will contribute to innovation in interactive voice technology through the inclusion of appropriate paralinguistic features in voice assistant responses to code-switched input. This could lead to the specific benefits associated with enhancing empathy-producing aspects of voice assistant speech, as people have been shown to not only prefer interacting with empathetic dialogue systems, but also to have greater trust in such systems, even when they are aware they are not speaking to another human.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/03/2024<br>\nModified by: Julia&nbsp;B&nbsp;Hirschberg</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nInour NSFEAGER Identifying and Producing Code-Switching in Languages from Spoken, Lexical and Socio-linguistic Features we have developed methods and tools to identify multiple aspects of code-switching in text and speechRapid advances in speech technology in recent years have resulted in widespread use of voice assistants such as Siri (Apple), Google Assistant and Alexa (Amazon). These advances enable vast improvements in information access by voice for languages such as English, French, German, Cantonese, Mandarin, and Spanish. However, such access is limited to monolingualspeech, which for billions of multilingual speakers may not always be the most natural form of speech production. Thus, speakers who code-switch frequently must limit themselves tooneof their spoken languages in order to enjoy the same communication and search capabilities that monolingual speakers do. A major barrier to enabling naturalistic and comfortable communication for these speakers is the lack of speech technology that can not only understand code-switched input, but also produce similar human-like output.Our overall goal has been to better prepare voice assistants to interact with personswho speak multiple languages.\n\n\nThis past year we have studied several aspects of code-switching to this end:1) Do speakersentrainon pronunciation and other strategies of code-switching in speech, speaking more like one another? and 2) Is there a quantifiable relationship between code-switching andempathyin speech, where empathy is a speakers intention to convey that they understand anothers problems and want to help address them.We began our research withthree existing code-switched language corpora:the Miami Bangor Corpus (English/Spanish) the SEAME Corpus (English/Mandarin), and the MaSaC Corpus (English/Hindi).\n\n\nFrom our early experiments on entrainment in the lexical features of code-switched conversations in one corpus, the Miami Bangor, we found statistically significant evidence of entrainment across all of the word classes considered (most frequent words in the corpus (top 100 and top 25), affirmative cue words, and filled pauses, as well as on overall language use within the corpus (the binary presence of CSW, amount of CSW, and the CSW strategy used) Our experiments on acoustic-prosodic features also provide evidence of entrainment in terms of turn-levelproximity(similarity between speakers over a conversation) andconvergence(increase in similarly across a conversation). Also, our preliminary qualitative gender analysis showed a slight skew toward entrainment taking place in same-gender over mixed-gender conversations.\n\n\nFrom our more recent experiments on empathetic speech in all three of our language pairs we developed models to classify empathy based on another English corpus labeled for empathetic, non-empathetic and neutral text and speech and translated our corpora into English. Wefounda stronger positive relationship of spoken CSW with the lexical correlates of empathy than with the acoustic-prosodic correlates, although there is some positive correlation in these as well.These findings serve as a first step toward validating prior qualitative work on ampathetic speech and answering the question of whether the expression of empathy is a motivation for CSW in speech.\n\n\nFrom this work we have published two papers:\n\nDebasmita Bhattacharya, Siying Ding, Alayna Nguyen, Julia Hirschberg, Measuring Entrainment in Spontaneous Code-Switched Speech, NAACL 2024, Mexico City, Mexico.\nDebasmita Bhattacharya, Eleanor Lin, Run Chen, Julia Hirschberg,Switching Tongues, Sharing Hearts: Identifying the Relationship between Empathy and Code-switching in Speech,Interspeech 2024, Kos Island, Greece.\n\n\n\nWe hope that this work and its future extensions will contribute to innovation in interactive voice technology through the inclusion of appropriate paralinguistic features in voice assistant responses to code-switched input. This could lead to the specific benefits associated with enhancing empathy-producing aspects of voice assistant speech, as people have been shown to not only prefer interacting with empathetic dialogue systems, but also to have greater trust in such systems, even when they are aware they are not speaking to another human.\n\n\n\t\t\t\t\tLast Modified: 09/03/2024\n\n\t\t\t\t\tSubmitted by: JuliaBHirschberg\n"
 }
}