{
 "awd_id": "2318926",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "ATD:Understanding Adversarial Examples in Neural Network: Theory and Algorithms",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924885",
 "po_email": "tbartosz@nsf.gov",
 "po_sign_block_name": "Tomek Bartoszynski",
 "awd_eff_date": "2023-09-01",
 "awd_exp_date": "2026-08-31",
 "tot_intn_awd_amt": 240000.0,
 "awd_amount": 240000.0,
 "awd_min_amd_letter_date": "2023-08-11",
 "awd_max_amd_letter_date": "2023-08-11",
 "awd_abstract_narration": "While neural network-based models have shown exceptional power and versatility, their robustness against adversarial examples, which are inputs deliberately designed to mislead the model, has become a major area of concern. Adversarial training is currently the most widely used method to improve the robustness of neural networks against adversarial perturbations, but this approach has been found to have limitations, such as overfitting. In addition, the understanding of both attacks and adversarial training is still limited. In light of these challenges, this research aims to develop a theoretical analysis that sheds light on the robustness of neural network-based methods and the properties of adversarial training. This understanding is essential to the design of effective attack strategies and defense mechanisms for various machine learning models. This research has the potential to have a significant impact on a wide range of fields, such as cybersecurity, computer vision, natural language processing, healthcare, and financial services, where machine learning models play a crucial role. \r\n \r\nThe proposed project aims to contribute to the development of robust neural network-based models and algorithms through novel theoretical studies. Unlike existing works that primarily focus on the generalization error of the neural network algorithms, this project will focus on the robustness and stability. The research will leverage a range of mathematical and computational techniques, including statistical learning theory, random matrix theory, reproducing kernel Hilbert space, and optimization. The investigation of robustness will lead to the development of novel algorithms that are less vulnerable to adversarial attacks and can be implemented with greater security and stability\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Teng",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Teng Zhang",
   "pi_email_addr": "Teng.Zhang@ucf.edu",
   "nsf_id": "000711367",
   "pi_start_date": "2023-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "The University of Central Florida Board of Trustees",
  "inst_street_address": "4000 CENTRAL FLORIDA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "ORLANDO",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "4078230387",
  "inst_zip_code": "328168005",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "FL10",
  "org_lgl_bus_name": "THE UNIVERSITY OF CENTRAL FLORIDA BOARD OF TRUSTEES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RD7MXJV7DKT9"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Central Florida Board of Trustees",
  "perf_str_addr": "4000 CENTRAL FLORIDA BLVD",
  "perf_city_name": "ORLANDO",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "328168005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "FL10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "046Y00",
   "pgm_ele_name": "ATD-Algorithms for Threat Dete"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "6877",
   "pgm_ref_txt": "ALGORITHMS IN THREAT DETECTION"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 240000.0
  }
 ],
 "por": null
}