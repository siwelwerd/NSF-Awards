{
 "awd_id": "2308699",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Deep transformers for integrating protein sequence, structure and interaction data to predict function",
 "cfda_num": "47.074",
 "org_code": "08080000",
 "po_phone": "7032922224",
 "po_email": "jweller@nsf.gov",
 "po_sign_block_name": "Jennifer Weller",
 "awd_eff_date": "2023-06-01",
 "awd_exp_date": "2026-05-31",
 "tot_intn_awd_amt": 637928.0,
 "awd_amount": 637928.0,
 "awd_min_amd_letter_date": "2023-05-01",
 "awd_max_amd_letter_date": "2024-08-26",
 "awd_abstract_narration": "Proteins are fundamental macromolecules in the living systems. The knowledge about the function of proteins is important for biological research and technology development. However, the function of most proteins is still unknown. To fill the gap, this project aims to develop deep learning methods, one of the most powerful artificial intelligence (AI) technologies, to integrate multiple sources of protein data such as protein sequences, structures, and interaction to accurately predict protein function. The methods will advance the state of the art of protein function prediction and can be broadly applied in many domains such as life science research, biotechnology development, agriculture, and healthcare. The project will provide unique interdisciplinary research opportunities to train students at multiple levels including under-represented minority students with diverse backgrounds to apply AI to address fundamental scientific and technological problems. \r\n\r\nThe project will develop deep transformer models based on self-attention to integrate protein sequence, structure, and interaction data to significantly advance the prediction of both protein-level function and amino acid-level function. Specifically, it aims to achieve three objectives: (1) develop 1D and 3D transformers to predict protein function from multiple sequence alignments and structures; (2) develop 2D graph transformers to predict protein function from protein-protein interactions and integrate them with sequences and structures; and (3) implement transformers as user-friendly, accurate, robust open-source protein function prediction tools for the community. Cutting-edge deep transformer models based on the self-attention mechanism will be developed to integrate protein sequence, structure, and interaction data to predict protein function for the first time. 1D sequence-based transformer, 2D graph transformer, and 3D-equivariant graph transformer can extract amino acid conservation and long-range co-evolutionary signals in multiple sequence alignments, long-range interactions in protein-protein networks, and rotation- and translation-invariant/equivariant properties of protein structures better than the existing deep learning methods based on traditional convolutional and recurrent mechanisms. Predicting both overall protein-level function terms and residue-level function sites via multi-task learning and novel deep learning architectures can leverage the compliment of the two prediction tasks to provide more accurate, more complete, and more interpretable function prediction. The project will deliver user-friendly open-source tools for the community to accurately predict function from sequence, structure, and interaction data, which will help reduce the vast knowledge gap between protein sequence and function. The open-source deep learning tools can be used to predict and study protein function in many domains. The methods and tools will be leveraged to train students at multiple levels and increase the diversity in scientific research and education.   The results of the project can be found at https://calla.rnet.missouri.edu/cheng/nsf_protein_function.html\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "BIO",
 "org_dir_long_name": "Directorate for Biological Sciences",
 "div_abbr": "DBI",
 "org_div_long_name": "Division of Biological Infrastructure",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jianlin",
   "pi_last_name": "Cheng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jianlin Cheng",
   "pi_email_addr": "chengji@missouri.edu",
   "nsf_id": "000496463",
   "pi_start_date": "2023-05-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Missouri-Columbia",
  "inst_street_address": "121 UNIVERSITY HALL",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBIA",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "5738827560",
  "inst_zip_code": "652113020",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MO03",
  "org_lgl_bus_name": "UNIVERSITY OF MISSOURI SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "SZPJL5ZRCLF4"
 },
 "perf_inst": {
  "perf_inst_name": "The Curators of the University of Missouri",
  "perf_str_addr": "601 Turner Avenue",
  "perf_city_name": "Columbia",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "652110001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "MO03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164Y00",
   "pgm_ele_name": "Innovation: Bioinformatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1165",
   "pgm_ref_txt": "ADVANCES IN BIO INFORMATICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 204957.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 432971.0
  }
 ],
 "por": null
}