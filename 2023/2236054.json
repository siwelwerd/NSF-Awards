{
 "awd_id": "2236054",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NSF Convergence Accelerator Track H: Making Virtual Reality Meetings Accessible to Knowledge Workers with Visual Impairments",
 "cfda_num": "47.084",
 "org_code": "15020000",
 "po_phone": "7032927068",
 "po_email": "alvadati@nsf.gov",
 "po_sign_block_name": "Alex Vadati",
 "awd_eff_date": "2022-12-15",
 "awd_exp_date": "2024-11-30",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 750000.0,
 "awd_min_amd_letter_date": "2022-12-08",
 "awd_max_amd_letter_date": "2023-11-24",
 "awd_abstract_narration": "Virtual reality (VR) promises to be a widespread technology in the coming years, but it is\r\ncompletely inaccessible to people with visual impairments. One of the main advantages of VR is\r\nthat it emulates real-life interactions where people walk around and talk to one another using\r\nnaturalistic nonverbal behavior. Meanwhile, remote work has become commonplace due to the\r\nCOVID-19 pandemic. Employers, researchers, and workers alike are expressing the need for\r\nVR workplaces, where they can more easily brainstorm and hold discussions with others.\r\nHowever, while researchers and practitioners have proposed some methods for improving VR\r\naccessibility, no one has explored how interpersonal communication in VR can be made more\r\naccessible. While a blind person can hear someone speak, she misses out on onverbal cues\r\nlike proximity, gestures (waves, nods), and gaze. This proposal aims to investigate how such\r\nnonverbal cues can be made accessible to improve the overall accessibility of VR to support\r\nknowledge workers with visual impairments.\r\n\r\nThe overarching goal of the project is to develop user-centered, evidence-based guidelines that\r\nstipulate how VR can be made accessible to knowledge workers with visual impairments. The\r\ninvestigators will focus on remote work use cases, where interpersonal communication and\r\naccessible shared materials play central roles. Following user-centered design, they will conduct\r\nfocus groups with knowledge workers with visual impairments to identify key use cases and\r\nneeds. Then they will design low-fidelity prototypes to explore accessible features. They will\r\nevaluate these prototypes with target users and develop a preliminary set of guidelines. If\r\nfunded for Phase 2, they will iterate on the initial prototypes and rigorously evaluate them in\r\ncontrolled studies. In addition, they will refine the guidelines and work to incorporate them into\r\nexisting accessibility standards.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "ITE",
 "org_div_long_name": "Innovation and Technology Ecosystems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shiri",
   "pi_last_name": "Azenkot",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shiri Azenkot",
   "pi_email_addr": "shiri.azenkot@cornell.edu",
   "nsf_id": "000690878",
   "pi_start_date": "2022-12-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Andrea",
   "pi_last_name": "Won",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Andrea S Won",
   "pi_email_addr": "a.s.won@cornell.edu",
   "nsf_id": "000727233",
   "pi_start_date": "2022-12-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "LaPierre",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Charles M LaPierre",
   "pi_email_addr": "charlesl@benetech.org",
   "nsf_id": "000886594",
   "pi_start_date": "2022-12-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Mario",
   "pi_last_name": "Burton",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mario Burton",
   "pi_email_addr": "mburton@lighthouse-sf.org",
   "nsf_id": "000887350",
   "pi_start_date": "2022-12-08",
   "pi_end_date": "2023-11-24"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sean",
   "pi_last_name": "Dougherty",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Sean P Dougherty",
   "pi_email_addr": "SDougherty@lighthouse-sf.org",
   "nsf_id": "000937862",
   "pi_start_date": "2023-11-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "341 PINE TREE RD",
  "perf_city_name": "ITHACA",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148502820",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131Y00",
   "pgm_ele_name": "Convergence Accelerator Resrch"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 750000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-096ad782-7fff-72dc-61e6-8f84145974a5\"> </span></p>\r\n<p dir=\"ltr\"><strong>Overview</strong></p>\r\n<p dir=\"ltr\"><span>Virtual Reality (VR) is the next step in the evolution of platforms to connect people remotely for teamwork and collaboration. Unfortunately, this technology is largely inaccessible for blind and low-vision (BLV) people. While existing research has investigated how to address some aspects of accessibility in virtual reality, such as navigation, critical issues of communication of social information remain unexplored. Our project sought to do two things: first, explore how important nonverbal cues like eye contact, head movement, and grouping can be shared via haptics and sound; and second, to create guidelines and standards that could be adopted by industry.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><strong>User Research</strong></p>\r\n<p dir=\"ltr\"><span>We conducted multiple studies on the utilization and customization of nonvisual social cues, working with our partners at San Francisco LightHouse for the Blind and Visually Impaired to hold focus groups and run experiments with their community of BLV individuals. Participants told us that subtle cues in real life that could help them &ldquo;read the room,&rdquo; like feet shuffling and whispering, were often absent in VR, and that this combined with a lack of accessible materials during presentations were big obstacles to their participation in professional group settings. Testing of our prototypes showed that customizable haptic and audio cues for nonverbal behavior like eye contact, head shaking, and head nodding were effective in supporting small group conversations between BLV and sighted individuals. Additionally, our partners at Benetech prepared to tackle the &ldquo;accessible materials&rdquo; problem by developing machine vision tools that could provide visual descriptions in real time.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><strong>Stakeholder Research</strong></p>\r\n<p dir=\"ltr\"><span>In addition to BLV users, we also reached out to other key stakeholders such as VR developers and platform owners. We met with representatives of VR platform companies including Google, Microsoft, Meta, and Mozilla to discuss social VR dynamics and accessibility requirements. They emphasized the need for customization and platform flexibility, meeting users where they were in terms of both preferences and devices. They also reinforced the need to work with developers to reduce barriers to implementing accessibility, suggesting that open-source code, easily digestible videos, and QA-friendly guidelines all provide a framework that would support developers.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><strong>Branding &amp; Communication</strong></p>\r\n<p dir=\"ltr\"><span>In anticipation of further work on the project, we developed a recognizable brand and logo that could quickly communicate its essence to the public. Working with our partners at Benetech, we defined our incipient product as SocialSense XR, with the punchy tagline &ldquo;Perceive the Invisible,&rdquo; and produced an informative marketing video that could pave the way for stakeholders learning about us. We also debuted with a main stage presentation at the 2023 XR Access Symposium talking about the need for better social equity for BLV people in VR settings.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><strong>Conclusion</strong></p>\r\n<p dir=\"ltr\"><span>Virtual reality has the potential to create new levels of connection and collaboration at long distances. However, like all new technologies, it threatens to further widen the digital divide between abled and disabled people unless we think carefully about how to design for equity. Our project was a valuable step towards creating social virtual environments that support blind and low vision people in equitable communication.</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/23/2024<br>\nModified by: Shiri&nbsp;Azenkot</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986443560_SocialSense_XR_screenshot_2--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986443560_SocialSense_XR_screenshot_2--rgov-800width.png\" title=\"SocialSense at the 2023 XR Access Symposium\"><img src=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986443560_SocialSense_XR_screenshot_2--rgov-66x44.png\" alt=\"SocialSense at the 2023 XR Access Symposium\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Dylan Fox led a panel at the 2023 XR Access Symposium with SocialSense XR PIs Sean Dougherty (Lighthouse), Shiri Azenkot (Cornell), Charles LaPierre (Benetech), and Andrea Stevenson Won (Cornell) to introduce the public to these important concepts.</div>\n<div class=\"imageCredit\">Cornell</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">SocialSense at the 2023 XR Access Symposium</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986181714_SocialSense_XR_screenshot_1--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986181714_SocialSense_XR_screenshot_1--rgov-800width.png\" title=\"VR Prototype\"><img src=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986181714_SocialSense_XR_screenshot_1--rgov-66x44.png\" alt=\"VR Prototype\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A blind subject tries our VR prototype, in which nonverbal cues like eye contact, head nods, and more are communicated via sound and haptic signals, thus enabling her to perceive these visual social cues.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">VR Prototype</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986590979_Vertical_Logo_Tagline--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986590979_Vertical_Logo_Tagline--rgov-800width.jpg\" title=\"SocialSense XR: Perceive the Invisible\"><img src=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986590979_Vertical_Logo_Tagline--rgov-66x44.jpg\" alt=\"SocialSense XR: Perceive the Invisible\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our prototype logo and tagline were crafted as part of the NSF Accelerator's work on branding. We used it to quickly rally allies around our central idea: giving blind and low vision people new senses in XR, enabling them to  perceive what would otherwise be invisible.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">SocialSense XR: Perceive the Invisible</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734985774541_collaborators--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734985774541_collaborators--rgov-800width.png\" title=\"Collaborators: Cornell, Benetech, and Lighthouse combine forces\"><img src=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734985774541_collaborators--rgov-66x44.png\" alt=\"Collaborators: Cornell, Benetech, and Lighthouse combine forces\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Cornell, Benetech, and Lighthouse combined our talents to create SocialSense XR. Our diverse skills equipped us to pursue our goals: publishing new research and guidelines, fostering industry adoption and professional development, and sustainably promoting accessible XR.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Collaborators: Cornell, Benetech, and Lighthouse combine forces</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986053873_Timeline--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986053873_Timeline--rgov-800width.png\" title=\"Blind & Low Vision XR Timeline\"><img src=\"/por/images/Reports/POR/2024/2236054/2236054_10840017_1734986053873_Timeline--rgov-66x44.png\" alt=\"Blind & Low Vision XR Timeline\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Researchers and hobbyists have worked towards blind-accessible VR for years, but efforts have sped up recently. Timeline shows:\r\n1998: Audio Doom\r\n2004: Terraformers\r\n2019: SeeingVR (Zhao)\r\n2019: XR Access\r\n2021: W3C XAUR\r\n2022: Nonverbal Cues (Wieland)</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Blind & Low Vision XR Timeline</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nOverview\r\n\n\nVirtual Reality (VR) is the next step in the evolution of platforms to connect people remotely for teamwork and collaboration. Unfortunately, this technology is largely inaccessible for blind and low-vision (BLV) people. While existing research has investigated how to address some aspects of accessibility in virtual reality, such as navigation, critical issues of communication of social information remain unexplored. Our project sought to do two things: first, explore how important nonverbal cues like eye contact, head movement, and grouping can be shared via haptics and sound; and second, to create guidelines and standards that could be adopted by industry.\r\n\n\n\r\n\n\nUser Research\r\n\n\nWe conducted multiple studies on the utilization and customization of nonvisual social cues, working with our partners at San Francisco LightHouse for the Blind and Visually Impaired to hold focus groups and run experiments with their community of BLV individuals. Participants told us that subtle cues in real life that could help them read the room, like feet shuffling and whispering, were often absent in VR, and that this combined with a lack of accessible materials during presentations were big obstacles to their participation in professional group settings. Testing of our prototypes showed that customizable haptic and audio cues for nonverbal behavior like eye contact, head shaking, and head nodding were effective in supporting small group conversations between BLV and sighted individuals. Additionally, our partners at Benetech prepared to tackle the accessible materials problem by developing machine vision tools that could provide visual descriptions in real time.\r\n\n\n\r\n\n\nStakeholder Research\r\n\n\nIn addition to BLV users, we also reached out to other key stakeholders such as VR developers and platform owners. We met with representatives of VR platform companies including Google, Microsoft, Meta, and Mozilla to discuss social VR dynamics and accessibility requirements. They emphasized the need for customization and platform flexibility, meeting users where they were in terms of both preferences and devices. They also reinforced the need to work with developers to reduce barriers to implementing accessibility, suggesting that open-source code, easily digestible videos, and QA-friendly guidelines all provide a framework that would support developers.\r\n\n\n\r\n\n\nBranding & Communication\r\n\n\nIn anticipation of further work on the project, we developed a recognizable brand and logo that could quickly communicate its essence to the public. Working with our partners at Benetech, we defined our incipient product as SocialSense XR, with the punchy tagline Perceive the Invisible, and produced an informative marketing video that could pave the way for stakeholders learning about us. We also debuted with a main stage presentation at the 2023 XR Access Symposium talking about the need for better social equity for BLV people in VR settings.\r\n\n\n\r\n\n\nConclusion\r\n\n\nVirtual reality has the potential to create new levels of connection and collaboration at long distances. However, like all new technologies, it threatens to further widen the digital divide between abled and disabled people unless we think carefully about how to design for equity. Our project was a valuable step towards creating social virtual environments that support blind and low vision people in equitable communication.\r\n\n\n\t\t\t\t\tLast Modified: 12/23/2024\n\n\t\t\t\t\tSubmitted by: ShiriAzenkot\n"
 }
}