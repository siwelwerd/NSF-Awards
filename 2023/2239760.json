{
 "awd_id": "2239760",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Integrating Sensorimotor Models into Human-Robot Collaboration in Gait, Posture, and Unsteady Tasks",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922633",
 "po_email": "aleoness@nsf.gov",
 "po_sign_block_name": "Alex Leonessa",
 "awd_eff_date": "2023-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 698057.0,
 "awd_amount": 23098.0,
 "awd_min_amd_letter_date": "2023-05-30",
 "awd_max_amd_letter_date": "2024-06-13",
 "awd_abstract_narration": "This Faculty Early Career Development (CAREER) grant supports research that will contribute new knowledge to the design and control of wearable robotic devices for assisting everyday movements, such as walking and standing balance. Wearable robotic devices, such as exoskeletons, can be used to provide assistive forces to those who have impaired mobility or other disabilities. Although the symptoms driving disability are varied and specific to the individual, current robots do not yet have the intelligence to modify their behavior based on the user\u2019s specific needs. This award supports fundamental research on methods to use measurements of specific deficits in the user\u2019s primary senses to improve how humans and robots collaborate during movement tasks. The results of this research will advance interdisciplinary knowledge in robotics, biomechanics, neuroscience, and control. Reducing the impact of disability and helping people return to work will greatly benefit the U.S. economic and societal goals to advance science and promote human health. The broader impacts of this work include training and research partnerships with local high schools and a nearby historically black university to attract and retain women and underrepresented minorities. \r\n\r\nTo estimate the body\u2019s movement, humans use three primary sensory systems: visual, vestibular, and somatosensory. A major element of the brain\u2019s ability to control movement is to use information from multiple sensory systems to improve the ability to estimate self-motion and sensory feedback. When one of the sensory systems experiences a dynamic loss of accuracy, such as a step transition into soft ground or a loss of vision, the brain compensates by reducing the contribution of the impacted sense to the overall estimation in a process called sensory reweighting. This fundamental research in robot-assisted gait and posture will advance scientific understanding by studying how machines affect the dynamics of the perception of self-motion, cognition, and motor control in the presence of sensory deficits. In the first objective, the researchers will perform experiments with the exoskeleton and virtual reality to identify how exoskeletons affect the sensory reweighting processes in self-paced walking and standing balance. These results will provide novel measurements of the physical and sensory dynamics of human-robot collaboration. In the second objective, they will develop methods to incorporate individualized perceptual models of sensory reweighting into simulations of walking and standing. In the final objective, they will explore methods for real-time detection of sensory reweighting and the usage of haptic feedback as a tool to communicate state and trust between person and machine.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Jacobs",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel A Jacobs",
   "pi_email_addr": "dajacobs@temple.edu",
   "nsf_id": "000779153",
   "pi_start_date": "2023-05-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Temple University",
  "inst_street_address": "1805 N BROAD ST",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2157077547",
  "inst_zip_code": "191226104",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "PA02",
  "org_lgl_bus_name": "TEMPLE UNIVERSITY-OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "QD4MGHFDJKU1",
  "org_uei_num": "QD4MGHFDJKU1"
 },
 "perf_inst": {
  "perf_inst_name": "Temple University",
  "perf_str_addr": "1801 N BROAD ST",
  "perf_city_name": "PHILADELPHIA",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191226003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "PA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "058Y00",
   "pgm_ele_name": "M3X - Mind, Machine, and Motor"
  },
  {
   "pgm_ele_code": "104500",
   "pgm_ele_name": "CAREER: FACULTY EARLY CAR DEV"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "070E",
   "pgm_ref_txt": "INTEG OF HUMAN & COGNITIVE"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 23096.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project contributed to our understanding of how wearable robotic devices affect both the physical demands and the cognitive and sensory demands of standing balance and walking.&nbsp;</p>\n<p>In the control of human motion, three systems are used primarily in the control of movement: visual (eyes), somatosensory (touch and body positions), and vestibular (inner ear).&nbsp; When moving, the mind responds dynamically to perturbations in these three systems.&nbsp; Machine systems modulate this dynamic response, but the impact is not fully understood.</p>\n<p>The contribution of the project to our knowledge of physical human-machine collaboration occurred in two areas: 1) characterizing the impact of wearable machines (ankle exoskeletons) on the sensory information used to maintain standing balance, and 2) investigating the effect of machine environments (treadmill and virtual reality) on the perception of sensory information and the control of gait.</p>\n<p>Ankle Exoskeletons on Standing Balance</p>\n<p>We have developed a custom system to perturb vision using virtual reality technology and perturb the somatosensory system by using surfaces of different stiffness. Using this system, we systematically perturbed the sensory systems with the users maintain upright standing in their regular shoes and in the exoskeleton. Because the ankle exoskeleton changes the relationships between muscle activity and the force, its primary interaction is in the somatosensory system and was hypothesized to have a negative impact on the use of somatosensory information.&nbsp; Instead, our findings found that the ankle exoskeleton had a negative impact on how visual information and vestibular information was used.&nbsp; The results of this study will have an impact on the design of human-robot interaction across fields, especially in rehabilitation for those who have impairments in their sensorimotor system.</p>\n<p>Machine Environments on Gait</p>\n<p>Virtual reality environments can help improve the engagement and motivation of patients with their rehabilitation protocols.&nbsp; Virtual reality environments occur in multiple sizes, such as walking in front of a TV screen, using a head-mounted display, and walking in a room-sized project.&nbsp; However, we do not yet understand how differences in the machine presentation affect the control of movement and how to use machine intelligence to optimize how much is learned by the human mind.&nbsp; We replicated our custom system for walking studies to determine how machine elements that manipulate both physical environment and sensory information (treadmill speed controllers and virtual reality hardware) impacted the control of gait. Models of how humans interact in virtual reality with rehabilitation systems cannot yet describe why users walk at different speeds than what is presented in virtual reality. In one experiment, we demonstrated that the mismatch between walking velocity and the velocity of the virtual environment can be mitigated by speed feedback in the form of a virtual speedometer. In another experiment, we demonstrated that for certain conditions room sized projection and head mounted displays resulted in similar performance but that for other conditions, there were significant differences depending on which system was used.&nbsp; The results of this study will have an impact on the accessibility of virtual rehabilitation in society, as the cost and portability of head-mounted displays makes them available to smaller clinics and rural areas.&nbsp; They will also have an impact on the design of rehabilitation studies as researchers and clinicals will be able to better target the intervention of choice to available systems.</p>\n<p>Together, the results of these investigations advanced interdisciplinary knowledge across the fields of robotics, biomechanics, neuroscience, and control. The outcomes of this project also included peer-reviewed conference and journal papers, research seminars, and outreach to local K-12 schools.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 06/18/2024<br>\nModified by: Daniel&nbsp;A&nbsp;Jacobs</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project contributed to our understanding of how wearable robotic devices affect both the physical demands and the cognitive and sensory demands of standing balance and walking.\n\n\nIn the control of human motion, three systems are used primarily in the control of movement: visual (eyes), somatosensory (touch and body positions), and vestibular (inner ear). When moving, the mind responds dynamically to perturbations in these three systems. Machine systems modulate this dynamic response, but the impact is not fully understood.\n\n\nThe contribution of the project to our knowledge of physical human-machine collaboration occurred in two areas: 1) characterizing the impact of wearable machines (ankle exoskeletons) on the sensory information used to maintain standing balance, and 2) investigating the effect of machine environments (treadmill and virtual reality) on the perception of sensory information and the control of gait.\n\n\nAnkle Exoskeletons on Standing Balance\n\n\nWe have developed a custom system to perturb vision using virtual reality technology and perturb the somatosensory system by using surfaces of different stiffness. Using this system, we systematically perturbed the sensory systems with the users maintain upright standing in their regular shoes and in the exoskeleton. Because the ankle exoskeleton changes the relationships between muscle activity and the force, its primary interaction is in the somatosensory system and was hypothesized to have a negative impact on the use of somatosensory information. Instead, our findings found that the ankle exoskeleton had a negative impact on how visual information and vestibular information was used. The results of this study will have an impact on the design of human-robot interaction across fields, especially in rehabilitation for those who have impairments in their sensorimotor system.\n\n\nMachine Environments on Gait\n\n\nVirtual reality environments can help improve the engagement and motivation of patients with their rehabilitation protocols. Virtual reality environments occur in multiple sizes, such as walking in front of a TV screen, using a head-mounted display, and walking in a room-sized project. However, we do not yet understand how differences in the machine presentation affect the control of movement and how to use machine intelligence to optimize how much is learned by the human mind. We replicated our custom system for walking studies to determine how machine elements that manipulate both physical environment and sensory information (treadmill speed controllers and virtual reality hardware) impacted the control of gait. Models of how humans interact in virtual reality with rehabilitation systems cannot yet describe why users walk at different speeds than what is presented in virtual reality. In one experiment, we demonstrated that the mismatch between walking velocity and the velocity of the virtual environment can be mitigated by speed feedback in the form of a virtual speedometer. In another experiment, we demonstrated that for certain conditions room sized projection and head mounted displays resulted in similar performance but that for other conditions, there were significant differences depending on which system was used. The results of this study will have an impact on the accessibility of virtual rehabilitation in society, as the cost and portability of head-mounted displays makes them available to smaller clinics and rural areas. They will also have an impact on the design of rehabilitation studies as researchers and clinicals will be able to better target the intervention of choice to available systems.\n\n\nTogether, the results of these investigations advanced interdisciplinary knowledge across the fields of robotics, biomechanics, neuroscience, and control. The outcomes of this project also included peer-reviewed conference and journal papers, research seminars, and outreach to local K-12 schools.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 06/18/2024\n\n\t\t\t\t\tSubmitted by: DanielAJacobs\n"
 }
}