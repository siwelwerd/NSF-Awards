{
 "awd_id": "2306453",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RAPID/Collaborative Research: Datasets for Uncrewed Aerial System (UAS) and Remote Responder Performance from Hurricane Ian",
 "cfda_num": "47.041, 47.070",
 "org_code": "07030000",
 "po_phone": "7032925365",
 "po_email": "jberg@nsf.gov",
 "po_sign_block_name": "Jordan Berg",
 "awd_eff_date": "2023-02-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 144788.0,
 "awd_amount": 144788.0,
 "awd_min_amd_letter_date": "2023-02-15",
 "awd_max_amd_letter_date": "2023-12-11",
 "awd_abstract_narration": "This Grants for Rapid Response Research (RAPID) project will curate, supplement, and analyze data collected over a period of intensive uncrewed aerial system (UAS) operations, carried out as part of the State of Florida\u2019s response to Hurricane Ian. From September 27, 2022, just before Hurricane Ian made landfall, and continually for the next nine days, teams from Florida State University and Texas A&M University helped coordinate 24 UAS pilots flying 16 different models of fixed-wing and rotorcraft UAS over Charlotte, Lee, and Hardee counties. These missions obtained aerial imagery to survey wind and flood damage, direct ground response, support strategic planning and resource allocation, monitor threats to public safety, and provide documentation for subsequent emergency relief funding. Under this award, the research team will curate 55,000 images and videos collected during the disaster, comprising over 750 gigabytes of data, and supporting material such as flight schedules and log files. The curated data, and derived products such as aerial maps and edited video, will be made publicly available for open-source use. The project will analyze the mission logs and data products to assess pilot performance over time, and will document variables potentially influencing pilot performance, including pilot skill, prior training and experience, operations tempo, and fatiguing conditions, supplemented by individual and collective interviews with the UAS pilots. The image dataset and derived products will help the computer vision/machine learning (CV/ML) community design better algorithms for identifying threats to public safety, damaged structures, and people in distress.  The pilot performance dataset will be made available to the research community, to characterize human-robot performance, formulate best practices, and to understand deviation in behaviors and sources of mission error. The resulting insights into proper matching of vehicles, pilots, missions, and operational parameters will increase the ability of UAS platforms and pilots to save lives and accelerate economic recovery after a disaster. The datasets can help the domestic UAS industry improve products for response to a broad class of natural disasters, including wildfires and flooding, and for use in extreme environments, such as in oil and gas exploration and extraction and for in nuclear reactors and nuclear waste sites. The project will support the creation of better workflow procedures to reduce human error, increasing trust in the technology by UAS operators and other first responders, and facilitating adoption of UAS for emergency response. The project will broaden participation in science, with three women out of the four co-PIs, and will engage STEM students to help annotate the UAS imagery. \r\n\r\nThis project will curate vehicle and pilot data from the deployment of uncrewed aerial systems (UAS) during Hurricane Ian by Florida State University and Texas A&M University for the robotics, computer vision/machine learning (CV/ML), human-robot interaction, and geospatial land-use communities. The project has the following three objectives: 1) Curate the data (imagery, log files, flight schedules, etc.) and data products (images, video, orthomosaic maps, digital surface maps) collected during the disaster and make available for open-source use; 2) Interview the UAS pilots individually and collectively in order to capture human-robot performance, best practices, deviation in behaviors, and sources of error; and 3) Analyze the mission logs and data products for performance (quality or completeness) and document the quality over time by pilots, prior training, and frequency of flying the missions in normative conditions, the operations tempo, and fatiguing conditions. From a robotics perspective, it will contribute to the emerging model of how multiple agents may be used during disasters, and the consequences for design, performance specifications, the role of artificial intelligence, and wireless communications. Such a model can greatly increase the competitiveness of the domestic drone industry, as well as motivate novel directions in swarm research. Research stemming from this project will generate guidelines for data collection in future disasters, setting the stage for advances in engineering and computing for disasters. It will increase the availability of training data for CV/ML and serve as a testbed for transfer of learned features from other disasters; both of which could lead to fundamental advances in machine learning. From a human-factors perspective, it will generate a new methodology for creating human-robot datasets that combine on-site direct data (with no experimenters in the field) with post-event data. This methodology would overcome current barriers in conducting empirical investigations into scientific questions on extreme work environments because of the prohibition on embedded experimenters. This methodology is expected to transfer to other extreme work environments, such as nuclear, space, oil and gas industry, and the military. The human-robot data itself could lead to major findings in human error and workforce training. From a geospatial perspective, the data can help establish the impact of rising sea levels, the built environment, and prior storm surge and flooding mitigations. Overall, the project will benefit society by increasing the ability to save lives and accelerate economic recovery after a disaster with UAS and is expected to create findings and methods that will generalize to new technologies for extreme environments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robin",
   "pi_last_name": "Murphy",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Robin R Murphy",
   "pi_email_addr": "robin.r.murphy@tamu.edu",
   "nsf_id": "000511836",
   "pi_start_date": "2023-02-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "S. Camille",
   "pi_last_name": "Peres",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "S. Camille Peres",
   "pi_email_addr": "peres@tamu.edu",
   "nsf_id": "000367116",
   "pi_start_date": "2023-02-15",
   "pi_end_date": "2023-12-11"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Ranjana",
   "pi_last_name": "Mehta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ranjana Mehta",
   "pi_email_addr": "rmehta38@wisc.edu",
   "nsf_id": "000753400",
   "pi_start_date": "2023-02-15",
   "pi_end_date": "2023-08-30"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Moats",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Jason B Moats",
   "pi_email_addr": "jbmoats@tamu.edu",
   "nsf_id": "000799099",
   "pi_start_date": "2023-12-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "3112 TAMU",
  "perf_city_name": "COLLEGE STATION",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433112",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "144Y00",
   "pgm_ele_name": "FRR-Foundationl Rsrch Robotics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "6840",
   "pgm_ref_txt": "ROBOTICS"
  },
  {
   "pgm_ref_code": "7914",
   "pgm_ref_txt": "RAPID"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 144788.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project was a collaborative effort between Texas A&amp;M, Florida State University, and the Center for Robot-Assisted Search and Rescue (CRASAR). The project archived and curated drone and pilot data from Hurricane Ian. This open source data trove was then used by researchers in two ways. One use of the data by the team was to guide a human factors study which &nbsp;had the pilots from Ian to return and fly missions that duplicated the deployment and tasking conditions at Ian. Unlike at a real disaster, hosting an exercise meant human factors and ergonomic researchers could embed with the pilots as well as take biometric and cognitive measurements. The analysis of the exercise quantified concerns that had surfaced starting with Hurricane Harvey in 2017. Qualitative observations suggested the demanding conditions of flying drones during a disaster might be leading to dangerous levels of pilot fatigue and thus to human error. If that were true and the conditions known, then better human-robot systems could be tailored for off-normal situations. In parallel with the human factors exercise, other members of the team worked with 127 students at four high schools in Florida and Pennsylvania to label the damage in the drone imagery from Ian (plus nine other disasters) for use by the artificial intelligence community. Labeling damage from a single disaster can take years and the lack of damage datasets has been a major barrier for the machine learning community. The resulting massive dataset is known as &nbsp;CRASAR sUAS [D]isaster [R]esponse [O]verhead [I]nspection [D]ata[s]et (CRASAR-U-DROIDs) and is the largest such collection of imagery from drones used by agencies having jurisdiction during an event. The team also trained machine learning models to perform damage assessment as a way of demonstrating the quality of the labeling in the dataset. Fortuitously, the models were not just of academic interest to the machine learning community; they were used in Hurricanes Debby and Helene to classify building damage in Florida and Pennsylvania. The models can be run on a high end laptop, something responders would have in the field, plus the models do not rely on wireless connectivity which is normally not available for the first few days after a disaster. The long-term intellectual merit of the project is the open source imagery, mission logs, and CRASAR-U-DROIDs dataset; this archived data is expected to enable other research teams to make further scientific breakthroughs in machine learning, computer vision, aviation, and human factors. The short-term value has been to quantitatively investigate fatigue, a missing piece in the human factors drone safety puzzle that had been elusive due to the infrequency of disasters and the ethics of distracting responders by experimenting on them during a real event. These findings have been captured in eight scientific papers to date. The long-term societal benefit and broader impact is the value the data brings to the use of drones and artificial intelligence in rapidly and accurately classifying damage, thus speeding up the life-saving response and allowing agencies to start earlier, and plan better for, recovery. The short-term social benefit has been the opportunistic engagement of high school STEM students, many from vulnerable communities, in working with artificial intelligence and disaster management.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/16/2025<br>\nModified by: Robin&nbsp;R&nbsp;Murphy</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2306453/2306453_10847732_1737073507555_Damage_on_Zoom--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2306453/2306453_10847732_1737073507555_Damage_on_Zoom--rgov-800width.png\" title=\"Close up of labeled drone Imagery from Hurrican\"><img src=\"/por/images/Reports/POR/2025/2306453/2306453_10847732_1737073507555_Damage_on_Zoom--rgov-66x44.png\" alt=\"Close up of labeled drone Imagery from Hurrican\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Close up of drone Imagery from Hurricane Ian with colors showing the levels of building damage classified by the machine learning system.</div>\n<div class=\"imageCredit\">Texas A&M Engineering Experiment Station</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Robin&nbsp;R&nbsp;Murphy\n<div class=\"imageTitle\">Close up of labeled drone Imagery from Hurrican</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2306453/2306453_10847732_1737073442421_LCSO_ortho_Overview_with_Labeled_damage--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2306453/2306453_10847732_1737073442421_LCSO_ortho_Overview_with_Labeled_damage--rgov-800width.png\" title=\"Labeled orthomosiac from Hurricane Ian\"><img src=\"/por/images/Reports/POR/2025/2306453/2306453_10847732_1737073442421_LCSO_ortho_Overview_with_Labeled_damage--rgov-66x44.png\" alt=\"Labeled orthomosiac from Hurricane Ian\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Drone Imagery from Hurricane Ian with colors showing the levels of building damage classified by the machine learning system.</div>\n<div class=\"imageCredit\">Texas A&M Engineering Experiment Station</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Robin&nbsp;R&nbsp;Murphy\n<div class=\"imageTitle\">Labeled orthomosiac from Hurricane Ian</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project was a collaborative effort between Texas A&M, Florida State University, and the Center for Robot-Assisted Search and Rescue (CRASAR). The project archived and curated drone and pilot data from Hurricane Ian. This open source data trove was then used by researchers in two ways. One use of the data by the team was to guide a human factors study which had the pilots from Ian to return and fly missions that duplicated the deployment and tasking conditions at Ian. Unlike at a real disaster, hosting an exercise meant human factors and ergonomic researchers could embed with the pilots as well as take biometric and cognitive measurements. The analysis of the exercise quantified concerns that had surfaced starting with Hurricane Harvey in 2017. Qualitative observations suggested the demanding conditions of flying drones during a disaster might be leading to dangerous levels of pilot fatigue and thus to human error. If that were true and the conditions known, then better human-robot systems could be tailored for off-normal situations. In parallel with the human factors exercise, other members of the team worked with 127 students at four high schools in Florida and Pennsylvania to label the damage in the drone imagery from Ian (plus nine other disasters) for use by the artificial intelligence community. Labeling damage from a single disaster can take years and the lack of damage datasets has been a major barrier for the machine learning community. The resulting massive dataset is known as CRASAR sUAS [D]isaster [R]esponse [O]verhead [I]nspection [D]ata[s]et (CRASAR-U-DROIDs) and is the largest such collection of imagery from drones used by agencies having jurisdiction during an event. The team also trained machine learning models to perform damage assessment as a way of demonstrating the quality of the labeling in the dataset. Fortuitously, the models were not just of academic interest to the machine learning community; they were used in Hurricanes Debby and Helene to classify building damage in Florida and Pennsylvania. The models can be run on a high end laptop, something responders would have in the field, plus the models do not rely on wireless connectivity which is normally not available for the first few days after a disaster. The long-term intellectual merit of the project is the open source imagery, mission logs, and CRASAR-U-DROIDs dataset; this archived data is expected to enable other research teams to make further scientific breakthroughs in machine learning, computer vision, aviation, and human factors. The short-term value has been to quantitatively investigate fatigue, a missing piece in the human factors drone safety puzzle that had been elusive due to the infrequency of disasters and the ethics of distracting responders by experimenting on them during a real event. These findings have been captured in eight scientific papers to date. The long-term societal benefit and broader impact is the value the data brings to the use of drones and artificial intelligence in rapidly and accurately classifying damage, thus speeding up the life-saving response and allowing agencies to start earlier, and plan better for, recovery. The short-term social benefit has been the opportunistic engagement of high school STEM students, many from vulnerable communities, in working with artificial intelligence and disaster management.\r\n\n\n\t\t\t\t\tLast Modified: 01/16/2025\n\n\t\t\t\t\tSubmitted by: RobinRMurphy\n"
 }
}