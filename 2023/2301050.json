{
 "awd_id": "2301050",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Iterative Algorithms for Statistics: From Convergence Rates to Statistical Accuracy",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2022-10-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 259825.0,
 "awd_min_amd_letter_date": "2022-10-19",
 "awd_max_amd_letter_date": "2022-10-19",
 "awd_abstract_narration": "Science, engineering, and industry are all being revolutionized by the modern era of data science, in which increasingly large and rich forms of data are now available.  The applications are diverse and broadly significant, including data-driven discovery in astronomy, statistical machine learning approaches to drug design, and decision-making in robotics and automated driving, among many others.  This grant supports research on techniques and models for learning from such massive datasets, leading to computationally efficient algorithms that can be scaled to the large problem instances encountered in practice. The PI plans to integrate research and education through the involvement of graduate students in the research, the inclusion of the research results in courses at UC Berkeley and in publicly available web-based course materials, as well as in mini courses at summer schools and workshops. This project will also provide mentoring and support for graduate students and postdocs who are female or belong to URM communities.\r\n\r\n\r\nMany estimates in statistics are defined via an iterative algorithm applied to a data-dependent objective function (e.g., the EM algorithm for missing data and latent variable models; gradient-based methods and Newton's method for M-estimation; boosting algorithms used in non-parametric regression).  This projectl gives several research thrusts that are centered around exploiting the dynamics of these algorithms in order to answer statistical questions, with applications to statistical parameter estimation; selection of the number of components in a mixture model; and optimal bias-variance trade-offs in non-parametric regression.  In more detail, the aims of this project include (i) providing a general analysis of the EM algorithm for non-regular mixture models and related singular problems, in which very slow (sub-geometric) convergence is typically observed; (ii) developing a principled method for model selection based on the convergence rate of EM, and to prove theoretical guarantees on its performance; developing a general theoretical framework for combining the convergence rate of an algorithm with bounds on its (in)stability so as to establish bounds on the statistical estimation error; and (iii) providing a complete analysis of the full boosting path for various types of boosting updates, including kernel boosting, as well as gradient-boosted regression trees, and to analyze the \"overfitting\" regime, elucidating conditions under which overfitting does or does not occur.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Martin",
   "pi_last_name": "Wainwright",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Martin Wainwright",
   "pi_email_addr": "wainwrigwork@gmail.com",
   "nsf_id": "000060031",
   "pi_start_date": "2022-10-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 MASSACHUSETTS AVE",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 55537.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 100184.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 104104.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research grant focused on novel challenges that arise in modern<br />machine learning and statistics.&nbsp; Data sets are now so large that it<br />is essential to make use of efficient and iterative procedures for<br />computation.&nbsp; The research made fundamental contributions to our<br />understanding of such iterative algorithms, and their computational<br />complexity interacts with their statistical utility.&nbsp; It provided a<br />general framework for understanding the behavior of the<br />expectation-maximization algorithm, which is widely used for solving<br />problems with missing data or other forms of confounding.&nbsp; It<br />introduced and analyzed a general class of variance-reduced procedures<br />for stochastic approximation, that can be used to efficiently compute<br />solutions to fixed point equations.&nbsp; This family of algorithms is very<br />easy to implement---requiring only a two-line change from existing<br />code---and equipped with very strong mathematical guarantees.&nbsp; The<br />research also contributed to the area of reinforcement learning, in<br />which the goal is to design systems that learn from their own<br />self-generated data in an on-line manner.&nbsp; In particular, it<br />introduced new classes of iterative procedures for computing<br />non-parametric approximations to value functions in reinforcement<br />learning problems.<br /><br /><br />Understanding from the research in this grant has potential<br />consequences for a broad range of applications that involve the use of<br />iterative algorithms as applied to large data sets.&nbsp; Examples include<br />large-scaling training of deep neural networks via stochastic gradient<br />descent; training of systems for reinforcement learning via stochastic<br />approximation algorithms; and selection procedures for models of<br />large-scale data, including in medicine, finance, and genomics.<br />Moreover, the proposal is inter-disciplinary in nature, and so has the<br />potential to strengthen bridges between the information sciences<br />broadly defined, including electrical engineering and computer<br />science, as well as applied mathematics and statistics.<br /><br />The grant supported the training of multiple graduate students and<br />post-doctoral students, who now occupy faculty positions at<br />universities across North America (e.g., Carnegie Mellon University,<br />New York University, University of Toronto, Cornell University).&nbsp; It also supported the development of technical presentations and tutorial materials given at various short courses and departmental presentations, thereby disseminating the material more broadly.&nbsp; It also supported curricular development at the university level, with research from the grant being incorporated into various courses.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/18/2023<br>\nModified by: Martin&nbsp;Wainwright</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2301050/2301050_10680920_1702919714776_fig_eric_compare_disc09_tol01--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2301050/2301050_10680920_1702919714776_fig_eric_compare_disc09_tol01--rgov-800width.jpg\" title=\"Efficiency improvement via Bellman-Krylov Boosting\"><img src=\"/por/images/Reports/POR/2023/2301050/2301050_10680920_1702919714776_fig_eric_compare_disc09_tol01--rgov-66x44.jpg\" alt=\"Efficiency improvement via Bellman-Krylov Boosting\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of the efficiency gains achievable by the Krylov-Bellman boosting algorithm relative to the standard fitted value iteration (FVI) algorithm.</div>\n<div class=\"imageCredit\">Martin Wainwright and Eric Xia</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Martin&nbsp;Wainwright\n<div class=\"imageTitle\">Efficiency improvement via Bellman-Krylov Boosting</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/2301050/2301050_10680920_1702919515311_fig_discount99_100k--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2301050/2301050_10680920_1702919515311_fig_discount99_100k--rgov-800width.jpg\" title=\"Variance-reduced vesrus ordinary Q-learning\"><img src=\"/por/images/Reports/POR/2023/2301050/2301050_10680920_1702919515311_fig_discount99_100k--rgov-66x44.jpg\" alt=\"Variance-reduced vesrus ordinary Q-learning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of the computational speed-ups afforded by various reduced versions of the Q-learning algorithm, used to estimate the value function of a policy in a sequential decision-making problem.</div>\n<div class=\"imageCredit\">Martin Wainwright</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Martin&nbsp;Wainwright\n<div class=\"imageTitle\">Variance-reduced vesrus ordinary Q-learning</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis research grant focused on novel challenges that arise in modern\nmachine learning and statistics. Data sets are now so large that it\nis essential to make use of efficient and iterative procedures for\ncomputation. The research made fundamental contributions to our\nunderstanding of such iterative algorithms, and their computational\ncomplexity interacts with their statistical utility. It provided a\ngeneral framework for understanding the behavior of the\nexpectation-maximization algorithm, which is widely used for solving\nproblems with missing data or other forms of confounding. It\nintroduced and analyzed a general class of variance-reduced procedures\nfor stochastic approximation, that can be used to efficiently compute\nsolutions to fixed point equations. This family of algorithms is very\neasy to implement---requiring only a two-line change from existing\ncode---and equipped with very strong mathematical guarantees. The\nresearch also contributed to the area of reinforcement learning, in\nwhich the goal is to design systems that learn from their own\nself-generated data in an on-line manner. In particular, it\nintroduced new classes of iterative procedures for computing\nnon-parametric approximations to value functions in reinforcement\nlearning problems.\n\n\nUnderstanding from the research in this grant has potential\nconsequences for a broad range of applications that involve the use of\niterative algorithms as applied to large data sets. Examples include\nlarge-scaling training of deep neural networks via stochastic gradient\ndescent; training of systems for reinforcement learning via stochastic\napproximation algorithms; and selection procedures for models of\nlarge-scale data, including in medicine, finance, and genomics.\nMoreover, the proposal is inter-disciplinary in nature, and so has the\npotential to strengthen bridges between the information sciences\nbroadly defined, including electrical engineering and computer\nscience, as well as applied mathematics and statistics.\n\nThe grant supported the training of multiple graduate students and\npost-doctoral students, who now occupy faculty positions at\nuniversities across North America (e.g., Carnegie Mellon University,\nNew York University, University of Toronto, Cornell University). It also supported the development of technical presentations and tutorial materials given at various short courses and departmental presentations, thereby disseminating the material more broadly. It also supported curricular development at the university level, with research from the grant being incorporated into various courses.\n\n\n\t\t\t\t\tLast Modified: 12/18/2023\n\n\t\t\t\t\tSubmitted by: MartinWainwright\n"
 }
}