{
 "awd_id": "2310208",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Fractional Ridge Regression",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924551",
 "po_email": "jzhu@nsf.gov",
 "po_sign_block_name": "Jun Zhu",
 "awd_eff_date": "2023-09-01",
 "awd_exp_date": "2026-08-31",
 "tot_intn_awd_amt": 320000.0,
 "awd_amount": 320000.0,
 "awd_min_amd_letter_date": "2023-06-15",
 "awd_max_amd_letter_date": "2023-06-15",
 "awd_abstract_narration": "Technological advances make it possible to collect enormous amounts of data. Implications for how businesses run (online retailing, precision manufacturing, social media), how science is conducted (environmental science, climate modeling, chemoinformatics, biotechnology, engineering), and how governments operate (health care, public safety, homeland security, national defense, agriculture production) are correspondingly enormous. For many uses of massive data sets, not all of the available information is relevant. For example, of the estimated 100,000 human genes, often only a handful are relevant to understanding a particular disease and developing a cure (the challenge is identifying the handful of relevant genes).  A key feature in many big-data explorations is the identification and deemphasis of superfluous information with the corresponding identification and accentuation of the most relevant information (separating the wheat from the chaff). Fractional Ridge Regression (FRR) is designed to improve both prediction and interpretability of statistical analyses of large data sets relative to statistical methods currently in use.  FRR improves the identification and extraction of relevant information from large data sets thereby improving the many areas of business, science, and government policy that rely on the analysis and understanding of large data sets. With nearly limitless applications, FRR research is ideal for engaging diverse statistics students in research projects.  Computing algorithms and statistical software will make FRR available to researchers in all disciplines, thereby multiplying its potential benefits to education and diversity in numerous areas of data science. The investigator will identify sub-projects for undergraduate and graduate students with attention to student recruitment from under-represented groups.\r\n\r\nFractional ridge regression joins ridge regression and the lasso in the statistician's regression modeling toolbox.  Ridge regression was introduced by Hoerl and Kennard in 1970 and twenty-six years later was followed by the introduction of the lasso by Tibshirani. The body of research ensuing from these seminal papers is staggering, and has contributed immensely to our understanding of shrinkage and selection methodology and to the practice of regression modeling in many areas of science. In some applications of regression modeling the goal is simply to achieve the best possible predictions of future response values. In other applications, interpretation is important as a way to guide understanding of the process under investigation. Ridge regression is very good at prediction, although it is often eclipsed by the lasso in terms of both prediction and interpretation because the lasso also allows for selection. Fractional ridge regression (FRR) improves both prediction (measured by mean square error) and interpretability (measured by variable selection specificity) relative to the lasso.  FRR accomplishes these twin goals via a unique and clever penalty function that adaptively downweighs only a data-driven subset of regression model coefficients (a fraction), while allowing for the complementary subset of regression coefficients to vary freely in order to obtain an optimal fitted model.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Leonard",
   "pi_last_name": "Stefanski",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Leonard A Stefanski",
   "pi_email_addr": "stefansk@ncsu.edu",
   "nsf_id": "000117049",
   "pi_start_date": "2023-06-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "2601 WOLF VILLAGE WAY",
  "perf_city_name": "RALEIGH",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276957214",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 320000.0
  }
 ],
 "por": null
}