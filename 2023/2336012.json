{
 "awd_id": "2336012",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Designing Ultra-Energy-Efficient Intelligent Hardware with On-Chip Learning, Attention, and Inference",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2023-04-15",
 "awd_exp_date": "2024-02-29",
 "tot_intn_awd_amt": 472195.0,
 "awd_amount": 92079.0,
 "awd_min_amd_letter_date": "2023-09-21",
 "awd_max_amd_letter_date": "2023-09-21",
 "awd_abstract_narration": "Building intelligent computers that can perform cognitive tasks (e.g., learning, recognition) as well as humans do has been a long-standing goal of computing research. State-of-the-art deep learning and neuromorphic algorithms have recently advanced the software performance for cognitive applications. However, such algorithms are computation-memory-communication intensive, which makes the hardware design challenging to perform low-power real-time training and classification on portable platforms. Furthermore, to optimize system-level power, efficient power delivery and supply voltage regulation of such large-scale hardware systems also becomes a critical concern. This project will address these challenges across multiple disciplines of hardware and software design, towards the overarching goal of building brain-inspired intelligent computing systems that are ultra-energy-efficient for various cognitive tasks in computer vision, speech, robotics and biomedical applications. The success of this research is likely to impact many user-centric computing systems in  society and industry, including wearable, mobile, and edge computing. This project also entails integrative education and outreach plans through a new interdisciplinary coursework development, undergraduate/graduate student training, and a summer outreach program for high school students.\r\n\r\nIn this project, energy-efficient circuits, architectures and algorithms will be designed to incorporate learning, attention and inference computations in area-/power-constrained mobile/wearable hardware platforms. The particular technologies that will be developed to achieve large improvement in energy-efficiency include: (1) computation redundancy minimization of state-of-the-art deep learning algorithms with bio-inspired attention models, (2) novel memory compression schemes that apply to both software and hardware implementation, (3) real-time on-chip learning methods that consume low power on mobile/wearable devices, (4) efficient on-chip voltage regulators that can adapt to abrupt changes in cognitive workloads, and (5) cross-layer optimization of circuit, architecture and algorithm. The outcomes of this research will feature new very-large-scale integration (VLSI) systems that can learn and perform cognitive tasks in real-time with superior power efficiency, opening up possibilities for ubiquitous intelligence in small-form-factor devices.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jae-sun",
   "pi_last_name": "Seo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jae-sun Seo",
   "pi_email_addr": "js3528@cornell.edu",
   "nsf_id": "000929203",
   "pi_start_date": "2023-09-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "341 PINE TREE RD",
  "perf_city_name": "ITHACA",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148502820",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 92079.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has investigated innovative and interdisciplinary research to develop a reconfigurable and low-power hardware systems for artificial intelligence (AI) and neuromorphic algorithms that perform various cognitive tasks. The proposed research integrated learning, attention, and inference computations in area-/energy-constrained hardware platforms. Specifically, this project has developed research works that include: (1) computation redundancy minimization of state-of-the-art deep learning algorithms with bio-inspired attention models, (2) novel memory compression schemes that apply to both software and hardware implementation, (3) real-time on-chip learning methods that consume low power on mobile/wearable devices, and (4) cross-layer optimization of algorithm, circuit, and architecture.</p>\n<p>On the algorithm side, we investigated AI and neuromorphic algorithms that can achieve high accuracy especially for lightweight models with a reduced amount of computation and memory for both supervised and self-supervised learning schemes. For example, we presented a new self-supervised contrastive learning (CL) scheme, consisting of two technical components, Slimmed Asymmetrical Contrastive Learning (SACL) and Cross-Distillation (XD), which collectively enable efficient CL with compact models. Our proposed method trains CL models from scratch and outperforms them even without such an expensive requirement. Compared to the SoTA lightweight CL training (distillation) algorithms, SACL-XD achieves 1.79% ImageNet-1K accuracy improvement on MobileNet-V3 with 64&times; reduction in floating-point training operations.&nbsp;</p>\n<p>On the hardware side, we have developed several low-power accelerator prototype chips in scaled CMOS technology that can train and infer deep neural networks with high energy-efficiency, which benefits for a wide range of mobile applications and AI capabilities on edge devices. For example, we presented an 8-bit floating-point (FP8) training processor with highly parallel tensor cores that features hardware-efficient channel gating for dynamic output activation sparsity and group Lasso based dynamic weight sparsity. We developed a custom instruction set architecture (ISA) to flexibly support different AI model topologies. The 28nm prototype chip demonstrates large improvements in floating-point operations (FLOPs) reduction (7.3&times;), energy efficiency (16.4 TFLOPS/W), and overall training latency speedup (4.7&times;), for both supervised and self-supervised training tasks.&nbsp;</p>\n<p>In addition, to address the bottlenecks of data movement between memory and compute units for conventional AI accelerators, we presented a transformative approach called in-memory computing (IMC), which perform computation in-place inside the volatile or non-volatile memory in an analog or digital manner. Throughout this project, we have designed multiple prototype chips and made large advances across both analog and digital IMC designs based on both SRAM and resistive RAM (RRAM) technologies, demonstrating high energy-efficiency by reducing the on-chip memory access and communication cost.</p>\n<p>Overall, this project has largely integrated the research outcomes with education and outreach. A new interdisciplinary course on neuromorphic computing has been developed at Arizona State University and Cornell Tech for graduate students. Through the summer research program, high school students from nearby schools were invited and performed research through this project, who have pursued STEM majors in colleges that they subsequently attended. In addition, tutorials and special sessions have been organized at a number of IEEE conferences and workshops, disseminating the research outcomes of efficient AI hardware design, in-memory computing, and neuromorphic computing to graduate/undergraduate students, academic scholars, and working professionals.</p><br>\n<p>\n Last Modified: 06/04/2024<br>\nModified by: Jae-Sun&nbsp;Seo</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project has investigated innovative and interdisciplinary research to develop a reconfigurable and low-power hardware systems for artificial intelligence (AI) and neuromorphic algorithms that perform various cognitive tasks. The proposed research integrated learning, attention, and inference computations in area-/energy-constrained hardware platforms. Specifically, this project has developed research works that include: (1) computation redundancy minimization of state-of-the-art deep learning algorithms with bio-inspired attention models, (2) novel memory compression schemes that apply to both software and hardware implementation, (3) real-time on-chip learning methods that consume low power on mobile/wearable devices, and (4) cross-layer optimization of algorithm, circuit, and architecture.\n\n\nOn the algorithm side, we investigated AI and neuromorphic algorithms that can achieve high accuracy especially for lightweight models with a reduced amount of computation and memory for both supervised and self-supervised learning schemes. For example, we presented a new self-supervised contrastive learning (CL) scheme, consisting of two technical components, Slimmed Asymmetrical Contrastive Learning (SACL) and Cross-Distillation (XD), which collectively enable efficient CL with compact models. Our proposed method trains CL models from scratch and outperforms them even without such an expensive requirement. Compared to the SoTA lightweight CL training (distillation) algorithms, SACL-XD achieves 1.79% ImageNet-1K accuracy improvement on MobileNet-V3 with 64 reduction in floating-point training operations.\n\n\nOn the hardware side, we have developed several low-power accelerator prototype chips in scaled CMOS technology that can train and infer deep neural networks with high energy-efficiency, which benefits for a wide range of mobile applications and AI capabilities on edge devices. For example, we presented an 8-bit floating-point (FP8) training processor with highly parallel tensor cores that features hardware-efficient channel gating for dynamic output activation sparsity and group Lasso based dynamic weight sparsity. We developed a custom instruction set architecture (ISA) to flexibly support different AI model topologies. The 28nm prototype chip demonstrates large improvements in floating-point operations (FLOPs) reduction (7.3), energy efficiency (16.4 TFLOPS/W), and overall training latency speedup (4.7), for both supervised and self-supervised training tasks.\n\n\nIn addition, to address the bottlenecks of data movement between memory and compute units for conventional AI accelerators, we presented a transformative approach called in-memory computing (IMC), which perform computation in-place inside the volatile or non-volatile memory in an analog or digital manner. Throughout this project, we have designed multiple prototype chips and made large advances across both analog and digital IMC designs based on both SRAM and resistive RAM (RRAM) technologies, demonstrating high energy-efficiency by reducing the on-chip memory access and communication cost.\n\n\nOverall, this project has largely integrated the research outcomes with education and outreach. A new interdisciplinary course on neuromorphic computing has been developed at Arizona State University and Cornell Tech for graduate students. Through the summer research program, high school students from nearby schools were invited and performed research through this project, who have pursued STEM majors in colleges that they subsequently attended. In addition, tutorials and special sessions have been organized at a number of IEEE conferences and workshops, disseminating the research outcomes of efficient AI hardware design, in-memory computing, and neuromorphic computing to graduate/undergraduate students, academic scholars, and working professionals.\t\t\t\t\tLast Modified: 06/04/2024\n\n\t\t\t\t\tSubmitted by: Jae-SunSeo\n"
 }
}