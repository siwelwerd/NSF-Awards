{
 "awd_id": "2322424",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Overcoming interaction barriers in augmented reality via wearable multimodal sensing",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032924392",
 "po_email": "amonk@nsf.gov",
 "po_sign_block_name": "Alastair Monk",
 "awd_eff_date": "2023-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 275000.0,
 "awd_amount": 275000.0,
 "awd_min_amd_letter_date": "2023-08-22",
 "awd_max_amd_letter_date": "2023-08-22",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project enables people to intuitively interact with augmented reality technologies. Currently, augmented reality input modalities can be extremely unreliable, making it nearly impossible to use traditional mouse-and-keyboard applications on these devices. The company is proposing a device that may overcome this interaction barrier by allowing augmented reality manufacturers the ability to make more complex and meaningful applications. The device will turn human hands into cursors, turn any surface into a touchscreen, and unlock new human-computer interactions. Using this device with augmented reality headsets is expected to prove advantageous for potential applications in education, medicine, defense, and manufacturing. \r\n\r\nThe company is building an interface for augmented reality input, paving the way for a differentiated path to increasing augmented reality content. This technology could allow for rapid communication of information at a rate falling between that of typing on a computer and physically speaking, unlocking untapped productivity for users of augmented reality platforms. These outcomes will be met through the development of a reliable and functional wrist-worn, near-infrared sensor network that will improve the capture of input data. Unlike current devices, this technology is not hindered by the cameras' limited field of view or obstructions, or by the unreliability of other wearable input capture devices.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tyler",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tyler Chen",
   "pi_email_addr": "tnchen@stanford.edu",
   "nsf_id": "000913576",
   "pi_start_date": "2023-08-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SKYWALK INC.",
  "inst_street_address": "855 EL CAMINO REAL, SUITE 13A - 230",
  "inst_street_address_2": "",
  "inst_city_name": "PALO ALTO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "4846539259",
  "inst_zip_code": "943012305",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "SKYWALK INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "JD99M5ULRFJ9"
 },
 "perf_inst": {
  "perf_inst_name": "SKYWALK INC.",
  "perf_str_addr": "855 EL CAMINO REAL, SUITE 13A - 230",
  "perf_city_name": "PALO ALTO",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943012305",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8018",
   "pgm_ref_txt": "Smart and Connected Health"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01AB2324DB",
   "fund_name": "R&RA DRSA DEFC AAB",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 275000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">Input devices are the fundamental bridge between humans and the virtual world, shaping how we interact with technology in our daily lives. For spatial computing technologies like augmented reality (AR), hands-free input is essential for seamless integration, allowing users to perform tasks, hold objects, and navigate new systems with greater ease. Phase I of our project resulted in several notable intellectual achievements, including the design, fabrication, development and testing of the world&rsquo;s first high-channel-count optical and inertial sensing device capable of fully wireless operation for cross-user gesture interaction. In addition, we created environments for 3D and 2D data collection and gathered data from 140+ users in standardized and natural motion settings using both AR devices and traditional laptop screens. One of the key accomplishments was solving real-world usability challenges by developing a cross-user neural network that achieved over 90% accuracy across diverse users and settings. We optimized this model to function in real-time on a microcontroller, and the results showed no significant differences based on user demographics, indicating a high degree of translatability to real-world operation in the near future without negatively impacting subsets of the population. Furthermore, we expanded the device's gesture set, enabling complete control over AR, virtual reality (VR), and conventional devices such as tablets and laptops using wristworn gesture as a pointing device. We also developed a haptic feedback system to enhance usability and reduce user fatigue.</p>\n<p class=\"p1\">Based on live demonstrations with key industry stakeholders and leaders, we received positive feedback and offers for partnership and co-development, indicating the interface we&rsquo;ve developed has significant promise in meeting both technical objectives and broader impacts. With further performance refinements and integration with text input solutions, we believe this technology could be scaled for use by major industry players in AR and VR technologies with benefits for the broader American public, especially for individuals operating in industrial environments with a need to access computing devices while performing manual tasks. One notable societal impact of our technology is its ergonomic benefit, as it detects more subtle gestures, reducing the need for large gestures and minimizing fatigue over time. Our contributions in wrist-worn haptic discrimination further support long-term usability, unlocking the potential for AR technologies to extend beyond niche industrial applications and become more practical in everyday life.</p>\n<p class=\"p1\">Additionally, we've identified numerous opportunities to improve gesture and voice-based systems for hands-free text input and mouse control on wearable displays, which will address limitations of current systems. Current gesture control systems still suffer from false positives in daily life, and voice control systems fail in noisy environments or at low voice volumes. The development of high-fidelity gesture and voice-based systems for mouse control and text input opens a large number of important research questions. If we can address these challenges in future work, we will add significant value to the field by enabling mobile wearable displays that enable productivity levels comparable to or exceeding that of traditional laptop interfaces.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/30/2024<br>\nModified by: Tyler&nbsp;Chen</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nInput devices are the fundamental bridge between humans and the virtual world, shaping how we interact with technology in our daily lives. For spatial computing technologies like augmented reality (AR), hands-free input is essential for seamless integration, allowing users to perform tasks, hold objects, and navigate new systems with greater ease. Phase I of our project resulted in several notable intellectual achievements, including the design, fabrication, development and testing of the worlds first high-channel-count optical and inertial sensing device capable of fully wireless operation for cross-user gesture interaction. In addition, we created environments for 3D and 2D data collection and gathered data from 140+ users in standardized and natural motion settings using both AR devices and traditional laptop screens. One of the key accomplishments was solving real-world usability challenges by developing a cross-user neural network that achieved over 90% accuracy across diverse users and settings. We optimized this model to function in real-time on a microcontroller, and the results showed no significant differences based on user demographics, indicating a high degree of translatability to real-world operation in the near future without negatively impacting subsets of the population. Furthermore, we expanded the device's gesture set, enabling complete control over AR, virtual reality (VR), and conventional devices such as tablets and laptops using wristworn gesture as a pointing device. We also developed a haptic feedback system to enhance usability and reduce user fatigue.\n\n\nBased on live demonstrations with key industry stakeholders and leaders, we received positive feedback and offers for partnership and co-development, indicating the interface weve developed has significant promise in meeting both technical objectives and broader impacts. With further performance refinements and integration with text input solutions, we believe this technology could be scaled for use by major industry players in AR and VR technologies with benefits for the broader American public, especially for individuals operating in industrial environments with a need to access computing devices while performing manual tasks. One notable societal impact of our technology is its ergonomic benefit, as it detects more subtle gestures, reducing the need for large gestures and minimizing fatigue over time. Our contributions in wrist-worn haptic discrimination further support long-term usability, unlocking the potential for AR technologies to extend beyond niche industrial applications and become more practical in everyday life.\n\n\nAdditionally, we've identified numerous opportunities to improve gesture and voice-based systems for hands-free text input and mouse control on wearable displays, which will address limitations of current systems. Current gesture control systems still suffer from false positives in daily life, and voice control systems fail in noisy environments or at low voice volumes. The development of high-fidelity gesture and voice-based systems for mouse control and text input opens a large number of important research questions. If we can address these challenges in future work, we will add significant value to the field by enabling mobile wearable displays that enable productivity levels comparable to or exceeding that of traditional laptop interfaces.\n\n\n\t\t\t\t\tLast Modified: 09/30/2024\n\n\t\t\t\t\tSubmitted by: TylerChen\n"
 }
}