{
 "awd_id": "2303064",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CDS&E: Collaborative Research: HyLoC: Objective-driven Adaptive Hybrid Lossy Compression Framework for Extreme-Scale Scientific Applications",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032925184",
 "po_email": "awapon@nsf.gov",
 "po_sign_block_name": "Amy Apon",
 "awd_eff_date": "2022-10-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 270802.0,
 "awd_amount": 203483.0,
 "awd_min_amd_letter_date": "2022-10-28",
 "awd_max_amd_letter_date": "2022-10-28",
 "awd_abstract_narration": "Today's extreme-scale scientific simulations and instruments are producing huge amounts of data that cannot be transmitted or stored effectively. Lossy compression, a data compression approach leading to certain data distortion, has been considered as a promising solution, because it can significantly reduce the data size while maintaining high data fidelity. However, the existing lossy compression methods may not always work effectively on all datasets used in specific applications because of their distinct and diverse characteristics. Moreover, the user objectives in compression quality and performance may vary with applications, datasets or circumstances. This project aims to develop a hybrid lossy compression framework to automatically construct the best-fit compression for diverse user objectives in data-intensive scientific research. Educational and engagement activities are provided to develop new curriculum related to scientific data compression and promote research collaborations with national laboratories.\r\n\r\nDesigning an efficient, adaptive, hybrid framework that can always choose the best-fit compression strategy is nontrivial, since existing state-of-the-art lossy compression methods are developed with distinct principles. The project has a three-stage research plan. First, the project decouples the state-of-the-art error-bounded lossy compression approaches into multiple stages and effectively models the working efficiency (e.g., compression ratio, error, speed) of particular approaches in each stage. Second, the project develops a loosely-coupled framework to aggregate the decoupled compression stages together and also explores as many compression pipelines composed of different stages as possible, to optimize the classic compression efficiency, including compression quality and performance. Third, the project optimizes the synthetic data-movement performance regarding the external devices and resources, such as I/O performance. The team evaluates the proposed framework on multiple extreme-scale scientific applications, including cosmological simulations, light source instrument data analytics, quantum circuit simulations, and climate simulations. The project may create technologies that can increase the storage availability and improve the performance for extreme-scale scientific applications, opening opportunities for new discoveries.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dingwen",
   "pi_last_name": "Tao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dingwen Tao",
   "pi_email_addr": "ditao@iu.edu",
   "nsf_id": "000785936",
   "pi_start_date": "2022-10-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University",
  "perf_str_addr": "107 S INDIANA AVE",
  "perf_city_name": "BLOOMINGTON",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "474057000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IN09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808400",
   "pgm_ele_name": "CDS&E"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "8084",
   "pgm_ref_txt": "CDS&E"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 203483.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project has led to three key technical contributions: First, the team developed a general-purpose analytical model based on a prediction-based lossy compression framework. This model can accurately predict reduced data quality, compression ratio, and the impact of lossy compressed data on post-hoc analysis quality. The model significantly enhances prediction-based lossy compression in three use cases: (1) optimizing the predictor by selecting the best-fit model; (2) memory compression with a target ratio; and (3) in-situ compression optimization through fine-grained error-bound tuning for various data partitions. We evaluated our model on 10 scientific datasets, achieving an average accuracy of 93.47% and up to 18.7&times; lower computational cost compared to the trial-and-error approach. The model&rsquo;s efficiency was further validated across different applications in these three use cases. Additionally, our experiments demonstrated that the model-based approach reduces the time to store 3D RTM data with HDF5 by up to 3.4&times; using 128 CPU cores compared to traditional solutions. <strong>Second, </strong>the team proposed a deep integration of predictive lossy compression with HDF5 to significantly improve parallel-write performance. We developed analytical models to predict compression time and parallel-write time before actual compression, enabling the overlapping of compression and write operations. An additional space is introduced to handle possible data overflows due to prediction uncertainty in compression ratios. Moreover, we optimized the compression task order to increase overlapping efficiency. Experiments on up to 4,096 cores from Summit showed that our solution improves write performance by up to 4.5&times; over non-compression solutions and 2.9&times; over existing lossy compression solutions, with only 1.5% storage overhead compared to the original data in two real-world HPC applications. Third, we proposed an optimization approach for task scheduling that integrates computation, compression, and I/O. Our algorithm adaptively selects the optimal compression and I/O queue to minimize performance degradation during computation. Additionally, we introduced an intra-node I/O workload balancing mechanism to evenly distribute workloads across processes. We also designed a framework incorporating fine-grained compression, a compressed data buffer, and a shared Huffman tree to fully leverage the benefits of our task scheduling approach. Experimental results using up to 16 nodes and 64 GPUs on ORNL Summit, as well as real-world HPC applications, demonstrate that our solution reduces I/O overhead by up to 3.8&times; compared to non-compression solutions and 2.6&times; compared to asynchronous I/O solutions.</p>\n<p>In addition to research, the PI is committed to educational activities. The research findings have been integrated into three courses: \"High Performance Computing\" at UA and \"Cloud Computing\" at IU, both offered at the undergraduate and graduate levels. The project has involved six PhD students and two undergraduate students, and has contributed to the graduation of two PhD students.</p><br>\n<p>\n Last Modified: 08/26/2024<br>\nModified by: Dingwen&nbsp;Tao</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project has led to three key technical contributions: First, the team developed a general-purpose analytical model based on a prediction-based lossy compression framework. This model can accurately predict reduced data quality, compression ratio, and the impact of lossy compressed data on post-hoc analysis quality. The model significantly enhances prediction-based lossy compression in three use cases: (1) optimizing the predictor by selecting the best-fit model; (2) memory compression with a target ratio; and (3) in-situ compression optimization through fine-grained error-bound tuning for various data partitions. We evaluated our model on 10 scientific datasets, achieving an average accuracy of 93.47% and up to 18.7 lower computational cost compared to the trial-and-error approach. The models efficiency was further validated across different applications in these three use cases. Additionally, our experiments demonstrated that the model-based approach reduces the time to store 3D RTM data with HDF5 by up to 3.4 using 128 CPU cores compared to traditional solutions. Second, the team proposed a deep integration of predictive lossy compression with HDF5 to significantly improve parallel-write performance. We developed analytical models to predict compression time and parallel-write time before actual compression, enabling the overlapping of compression and write operations. An additional space is introduced to handle possible data overflows due to prediction uncertainty in compression ratios. Moreover, we optimized the compression task order to increase overlapping efficiency. Experiments on up to 4,096 cores from Summit showed that our solution improves write performance by up to 4.5 over non-compression solutions and 2.9 over existing lossy compression solutions, with only 1.5% storage overhead compared to the original data in two real-world HPC applications. Third, we proposed an optimization approach for task scheduling that integrates computation, compression, and I/O. Our algorithm adaptively selects the optimal compression and I/O queue to minimize performance degradation during computation. Additionally, we introduced an intra-node I/O workload balancing mechanism to evenly distribute workloads across processes. We also designed a framework incorporating fine-grained compression, a compressed data buffer, and a shared Huffman tree to fully leverage the benefits of our task scheduling approach. Experimental results using up to 16 nodes and 64 GPUs on ORNL Summit, as well as real-world HPC applications, demonstrate that our solution reduces I/O overhead by up to 3.8 compared to non-compression solutions and 2.6 compared to asynchronous I/O solutions.\n\n\nIn addition to research, the PI is committed to educational activities. The research findings have been integrated into three courses: \"High Performance Computing\" at UA and \"Cloud Computing\" at IU, both offered at the undergraduate and graduate levels. The project has involved six PhD students and two undergraduate students, and has contributed to the graduation of two PhD students.\t\t\t\t\tLast Modified: 08/26/2024\n\n\t\t\t\t\tSubmitted by: DingwenTao\n"
 }
}