{
 "awd_id": "2241975",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "The neural basis of top-down biasing of bottom-up visual processing",
 "cfda_num": "47.075, 47.083",
 "org_code": "04040000",
 "po_phone": "7032924502",
 "po_email": "dkravitz@nsf.gov",
 "po_sign_block_name": "Dwight Kravitz",
 "awd_eff_date": "2023-08-01",
 "awd_exp_date": "2026-07-31",
 "tot_intn_awd_amt": 699000.0,
 "awd_amount": 699000.0,
 "awd_min_amd_letter_date": "2023-07-27",
 "awd_max_amd_letter_date": "2024-09-12",
 "awd_abstract_narration": "Although much of what we see is simply driven by incoming visual stimuli and corresponds almost perfectly with physical reality, sometimes we can look at an image or the natural world and see things that aren\u2019t really there. This is the origin of many visual illusions. Other times, however, what we visually experience depends on how we process incoming visual stimuli. For example, we can even see what we intend to see, as when we choose to see the forms of animals or faces in clouds in the sky. So, our intentions can play a role in visual perception. Attention is also influential in how we see. For example, we are more likely to notice details about the things to which we are paying attention. Attention is not in the world or in the \u201cbottom-up\u201d visual stimulus, but is a \u201ctop-down\u201d focusing mechanism in our brain that enables us to selectively attend to the most relevant incoming stimuli.  Perception can therefore be influenced by both \u201ctop-down\u201d attention and \u201ctop-down\u201d intentions. We call this \u201ctop-down\u201d processing because what we see is not solely driven by what is in the \u201cbottom-up\u201d sensory input. It is also affected by where our attention is focused, and by the nature of our expectations and intentions. \r\n\r\nOne example of a kind of visual input that is driven by top-down processing is so-called \u201capparent motion.\u201d One example occurs when you are driving on the highway and see two flashing lights aligned diagonally, on opposite corners of an imaginary square. When these turn off, two different lights turn on at the other two corners of the invisible square. When the two light configurations toggle on and off sequentially, you are very likely to experience an illusion of apparent motion and see the flashed lights appear to jump back and forth, even though the lights are stationary at any given time. Interestingly, some people spontaneously see apparent motion in the horizontal direction, while others see apparent vertical motion between the lights when they flash on and off in pairs, even when no lightbulbs are actually moving at all. What is even more fascinating, is that people can often consciously decide whether they subsequently see illusory vertical or horizontal motion, so their intention can shape their perception of this apparent motion illusion. A central question driving this research is how a \u201ctop-down\u201d intention to see vertical vs horizontal motion is realized in the brain. The experiments in this project are designed to unravel the neural mechanisms that can lead to this voluntary reshaping of perception. Subjects perform these perceptual experiments while their brains are being imaged non-invasively, using functional magnetic resonance imaging (fMRI), so that brain activity can be monitored while subjects engage in the task. Visual perception experiments are conducted in the scanner with human subjects who are instructed to try to see either vertical or horizontal motion, and then switch directions when cued. There are visual motion processing areas in the cortex of the brain, with different sub-areas encoding vertical versus horizontal motion directions. After collecting fMRI data, it is possible to analyze brain images to precisely locate those brain areas that allow predictions of a subject\u2019s intention: i.e. whether subjects were intending to see horizontal or vertical motion, before any motion took place in the stimulus.  Based on brain activity, it is also possible to decipher what motion they would actually end up perceiving, given their intention to see horizontal or vertical motion. The ultimate goal of the research is to discover what areas of the brain allow us to decode these visual intentions, and to understand how top-down processing works in the brain for all forms of sensory perception. In addition to scholarly research, the project team also hopes to engage in multiple outreach efforts to broaden participation in science and improve public understanding of and interest in scientific research, including giving public talks on science, writing about science for the general public, and setting up a hall of visual illusions for a museum of science.\r\n\r\nThis project is jointly funded by the Cognitive Neuroscience Program, and the Established Program to Stimulate Competitive Research (EPSCoR).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Tse",
   "pi_mid_init": "U",
   "pi_sufx_name": "",
   "pi_full_name": "Peter U Tse",
   "pi_email_addr": "Peter.Tse@dartmouth.edu",
   "nsf_id": "000181886",
   "pi_start_date": "2023-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Dartmouth College",
  "inst_street_address": "7 LEBANON ST",
  "inst_street_address_2": "",
  "inst_city_name": "HANOVER",
  "inst_state_code": "NH",
  "inst_state_name": "New Hampshire",
  "inst_phone_num": "6036463007",
  "inst_zip_code": "037552170",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NH02",
  "org_lgl_bus_name": "TRUSTEES OF DARTMOUTH COLLEGE",
  "org_prnt_uei_num": "T4MWFG59C6R3",
  "org_uei_num": "EB8ASJBCFER9"
 },
 "perf_inst": {
  "perf_inst_name": "Dartmouth College",
  "perf_str_addr": "7 LEBANON ST",
  "perf_city_name": "HANOVER",
  "perf_st_code": "NH",
  "perf_st_name": "New Hampshire",
  "perf_zip_code": "037552170",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NH02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1699",
   "pgm_ref_txt": "COGNEURO"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 430000.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 269000.0
  }
 ],
 "por": null
}