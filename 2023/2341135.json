{
 "awd_id": "2341135",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "I-Corps: A Software Platform to Customize, Inspect and Improve Artificial Intelligence (AI) Systems",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032922061",
 "po_email": "jcamelio@nsf.gov",
 "po_sign_block_name": "Jaime A. Camelio",
 "awd_eff_date": "2023-09-15",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2023-08-21",
 "awd_max_amd_letter_date": "2023-08-21",
 "awd_abstract_narration": "The broader impact/commercial potential of this I-Corps project is the development of a software platform to make Artificial Intelligence (AI) models more reliable.  Artificial intelligence is rapidly becoming a part of everyday businesses and organizations. However, key concerns in using AI systems are their lack of reliability and explainability, and their lack of transparency with respect to internal workings where output inferences and predictions are not interpretable.  This makes the process of developing AI models and inspecting and mitigating their failure modes time-consuming and challenging. The proposed technology is designed to automate developing, inspecting and improving AI models using another AI system that uses human feedback in its optimization.  Understanding and mitigating reliability issues of AI models may mitigate the risks of their deployment in practice. In addition, these efforts may democratize the reliable use of AI systems by non-experts and increase human trust in these systems. \r\n\r\nThis I-Corps project is based on the development of an automated and unified software platform that provides multi-modal interpretability and reliability analysis and monitoring tools to design, train, inspect, and improve Artificial Intelligence (AI) systems. The proposed technology is designed to automatically uncover and address hidden reliability issues within AI models employing the user\u2019s unique data.  It simplifies the complex process of identifying and mitigating potential reliability risks and explainability challenges, which may help to ensure AI models deliver trustworthy and accurate results.  In addition, users may compare hundreds of AI models and select the ones with the maximum efficiency and reliability for their specific applications.  It also interactively incorporates user feedback in its optimization to improve reliability and explainability of AI models while reliability becomes transparent and manageable, empowering users to make informed decisions with increased confidence.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Soheil",
   "pi_last_name": "Feizi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Soheil Feizi",
   "pi_email_addr": "sfeizi@cs.umd.edu",
   "nsf_id": "000781480",
   "pi_start_date": "2023-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland, College Park",
  "perf_str_addr": "3112 LEE BLDG 7809 REGENTS DR",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207420001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "226y00",
   "pgm_ele_name": "Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6856",
   "pgm_ref_txt": "ARTIFICIAL INTELL & COGNIT SCI"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "4074XXXXDB",
   "fund_name": "NSF TRUST FUND",
   "fund_symb_id": "048960"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Project Outcomes Report</strong></p>\r\n<p>Artificial Intelligence (AI) is transforming the way we solve problems in healthcare, transportation, finance, and many other fields. Yet, despite the potential benefits, one of the greatest challenges facing AI systems today is ensuring that they are both trustworthy and reliable. Users, including businesses and the public, often worry that these &ldquo;black box&rdquo; models may be fragile, biased, or make puzzling decisions without a clear explanation. Our project focused on developing techniques and tools to make AI systems more transparent, more secure against unexpected failures, and more understandable to human users, thereby encouraging responsible and widespread adoption of AI.</p>\r\n<p><strong>Intellectual Merit:</strong><br />The core intellectual contribution of our research was to develop and refine software tools and methods designed to analyze, evaluate, and improve the reliability of AI models. Traditionally, ensuring that an AI system is both accurate and stable under different conditions has required a significant amount of human expertise and effort. By leveraging new algorithms for identifying hidden biases, detecting when models rely on misleading data patterns, and improving the model&rsquo;s performance through targeted data augmentation and user feedback, our work streamlines many of these tasks. These methods help reveal why a given model behaves the way it does, identify potential pitfalls before they cause problems in real-world use, and guide improvements in the model&rsquo;s accuracy and fairness over time.</p>\r\n<p>In addition, we conducted extensive interviews with professionals from a range of industries&mdash;medicine, manufacturing, transportation, and more&mdash;to understand the challenges they face in applying AI. These conversations provided invaluable insights that guided the refinement of our methods. By understanding real-world concerns&mdash;such as the difficulty of obtaining high-quality data, adapting to changes in data patterns over time, and ensuring that decision-making processes are fair and ethical&mdash;we were able to build methods that address actual needs, not just theoretical ones.</p>\r\n<p><strong>Broader Impacts:</strong><br />Our work has the potential to influence a wide spectrum of AI users and developers, from large companies deploying cutting-edge AI solutions to smaller organizations just beginning to explore how AI might help them. By making reliability assessments more automated, transparent, and accessible, we help lower the barrier to entry for adopting trustworthy AI. This benefits the public in several ways: it makes everyday AI tools&mdash;like those used in medical imaging or autonomous vehicles&mdash;safer and easier to trust; it allows non-experts to more confidently use AI for their goals, empowering small businesses and startups that may not have specialized AI teams; and it supports regulators, who can use these insights to craft better standards and guidelines for safe AI deployment.</p>\r\n<p>Moreover, by shedding light on how models make decisions and by helping users spot and correct unfair or harmful patterns, our work promotes equity and social responsibility in AI technologies. This can foster public trust in AI, as people see these systems acting more consistently and transparently, with fewer unpleasant surprises.</p>\r\n<p><strong>Summary of Outcomes:</strong><br />Over the course of this project, we developed new tools and frameworks that can automatically pinpoint weaknesses in AI models, help users fix those issues with less trial-and-error, and ultimately guide the creation of more dependable AI solutions. By engaging with industry experts, we ensured that the methods we invented are well-aligned with the needs of real practitioners. This combination of practical industry insight and technical innovation will help pave the way toward AI tools that are easier to understand and use responsibly, thereby accelerating the safe and beneficial integration of AI into everyday life.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/20/2024<br>\nModified by: Soheil&nbsp;Feizi</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nProject Outcomes Report\r\n\n\nArtificial Intelligence (AI) is transforming the way we solve problems in healthcare, transportation, finance, and many other fields. Yet, despite the potential benefits, one of the greatest challenges facing AI systems today is ensuring that they are both trustworthy and reliable. Users, including businesses and the public, often worry that these black box models may be fragile, biased, or make puzzling decisions without a clear explanation. Our project focused on developing techniques and tools to make AI systems more transparent, more secure against unexpected failures, and more understandable to human users, thereby encouraging responsible and widespread adoption of AI.\r\n\n\nIntellectual Merit:\nThe core intellectual contribution of our research was to develop and refine software tools and methods designed to analyze, evaluate, and improve the reliability of AI models. Traditionally, ensuring that an AI system is both accurate and stable under different conditions has required a significant amount of human expertise and effort. By leveraging new algorithms for identifying hidden biases, detecting when models rely on misleading data patterns, and improving the models performance through targeted data augmentation and user feedback, our work streamlines many of these tasks. These methods help reveal why a given model behaves the way it does, identify potential pitfalls before they cause problems in real-world use, and guide improvements in the models accuracy and fairness over time.\r\n\n\nIn addition, we conducted extensive interviews with professionals from a range of industriesmedicine, manufacturing, transportation, and moreto understand the challenges they face in applying AI. These conversations provided invaluable insights that guided the refinement of our methods. By understanding real-world concernssuch as the difficulty of obtaining high-quality data, adapting to changes in data patterns over time, and ensuring that decision-making processes are fair and ethicalwe were able to build methods that address actual needs, not just theoretical ones.\r\n\n\nBroader Impacts:\nOur work has the potential to influence a wide spectrum of AI users and developers, from large companies deploying cutting-edge AI solutions to smaller organizations just beginning to explore how AI might help them. By making reliability assessments more automated, transparent, and accessible, we help lower the barrier to entry for adopting trustworthy AI. This benefits the public in several ways: it makes everyday AI toolslike those used in medical imaging or autonomous vehiclessafer and easier to trust; it allows non-experts to more confidently use AI for their goals, empowering small businesses and startups that may not have specialized AI teams; and it supports regulators, who can use these insights to craft better standards and guidelines for safe AI deployment.\r\n\n\nMoreover, by shedding light on how models make decisions and by helping users spot and correct unfair or harmful patterns, our work promotes equity and social responsibility in AI technologies. This can foster public trust in AI, as people see these systems acting more consistently and transparently, with fewer unpleasant surprises.\r\n\n\nSummary of Outcomes:\nOver the course of this project, we developed new tools and frameworks that can automatically pinpoint weaknesses in AI models, help users fix those issues with less trial-and-error, and ultimately guide the creation of more dependable AI solutions. By engaging with industry experts, we ensured that the methods we invented are well-aligned with the needs of real practitioners. This combination of practical industry insight and technical innovation will help pave the way toward AI tools that are easier to understand and use responsibly, thereby accelerating the safe and beneficial integration of AI into everyday life.\r\n\n\n\t\t\t\t\tLast Modified: 12/20/2024\n\n\t\t\t\t\tSubmitted by: SoheilFeizi\n"
 }
}