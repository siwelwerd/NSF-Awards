{
 "awd_id": "2311286",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Grounding Natural Language Inference in Cognitive Processes",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922972",
 "po_email": "emiltsak@nsf.gov",
 "po_sign_block_name": "Eleni Miltsakaki",
 "awd_eff_date": "2023-05-01",
 "awd_exp_date": "2024-04-30",
 "tot_intn_awd_amt": 149570.0,
 "awd_amount": 149570.0,
 "awd_min_amd_letter_date": "2023-04-17",
 "awd_max_amd_letter_date": "2023-04-17",
 "awd_abstract_narration": "Being able to detect textual similarity is important for many applications including machine translation, detection of plagiarism, text generation, fact checking etc. At the word level, two words tend to mean the same thing when one can be swapped with the other with little or no consequence.  But how does this approach extend to the sentence level and beyond? According to an inference-centered view, a significant part of a sentence\u2019s meaning can be understood in terms of the \u201cinferential halo\u201d of each sentence.  The \u201cinferential halo\u201d is all the inferences, i.e., implied meanings, that a sentence has. Comparing the semantic similarity of two sentences or text would then be accomplished by comparing all the inferences that each sentence or text implies.  However, current Natural Language Processing approaches to detecting sentence and text similarities  are limited to measures based on word or substring similarities which do not capture adequately the meaning of a text. \r\n\r\nThis project addresses the limitations of previous approaches a) enriching the notion and representation of inference and b) learning how people naturally reason about semantic relations of texts.  The novelty of our approach draws on established work in Cognitive Psychology. For the enrichment of the notion of inference, we introduce the distinction between a) quick and automatic reasoning, known as Type 1, and b) slower and more deliberate reasoning, known as Type 2.  Type 1 reasoning applies when for example we recognize a face and Type 2 when we calculate the tip for a bill. To learn how people naturally reason, we collect data using data collection protocols established in Cognitive Psychology for quick and slow reasoning. For data collection, we experiment with a novel level of granularity to better capture the range of inferences made by humans and train new computational models of detecting semantic inferences. The proposed research will yield results, guidelines, and new computational models that will lead to a) a novel way of studying informal reasoning in language processing and b) improved metrics of textual similarity.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Licato",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "John Licato",
   "pi_email_addr": "licato@usf.edu",
   "nsf_id": "000782970",
   "pi_start_date": "2023-04-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of South Florida",
  "inst_street_address": "4202 E FOWLER AVE",
  "inst_street_address_2": "",
  "inst_city_name": "TAMPA",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "8139742897",
  "inst_zip_code": "336205800",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "FL15",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTH FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NKAZLXLL7Z91"
 },
 "perf_inst": {
  "perf_inst_name": "University of South Florida",
  "perf_str_addr": "4202 E FOWLER AVE",
  "perf_city_name": "TAMPA",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "336209951",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "FL15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 149570.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project set out to transform the notion of inference used in the natural language inference (NLI) paradigm by first introducing a six-option answer format that more richly captures naturalistic inference types. We used data collection protocols, drawn from cognitive psychology specifically to elicit Type 1 or Type 2 reasoning. We then applied the data and models to answer the following research questions: (1) Does a six-option format better capture natural language inference than the three-option format currently used? (2) Can the NLI paradigm effectively distinguish Type 1 and 2 reasoning, and if so, which data collection protocols should be used? (3) What sort of NLI items best highlight the differences identified in research questions 1 and 2?&nbsp;</p>\n<p>Our work resulted in the following.</p>\n<p><strong>INTELLECTUAL MERIT OUTCOMES</strong></p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [O1] After reviewing literature, we devised approximately 20 new methods of pseudo-automatically generating NLI problems, and created a large dataset of thousands of new questions.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [O2] We refined our design of four different data collection procedures designed to elicit both System 1 and System 2 reasoning, grounded in the cognitive psychology literature.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [O3] We obtained responses from hundreds of participants, resulting in several datasets.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [O4] We found that the &ldquo;memory&rdquo; procedure was most effective at differentiating System 1 and System 2 responses. We were also able to obtain detailed results about which question types were more effective at differentiating System 1 and System 2 responses. We were able to replicate some earlier results on belief-bias questions in the psychology literature, while extending them with a new technique for LLM-assisted annotation of participant-provided justifications.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [O5] We carried out an extensive surveying of modeling techniques, including classic machine learning algorithms and a variety of LLMs with prompting methods.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [O6] We devised a new technique (personality-based prompting with evolutionary weights) to model the distribution of responses from a human population. We were able to show this was much more effective than na&iuml;ve prompting methods.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Discovered a prompting technique that, when combined with weights learned through an evolutionary algorithm, can effectively capture a distribution of human responses on System 1 and System 2 questions.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Won first place in the 2024 COLIEE Legal Reasoning competition by applying our expertise on the NLI problem to the domain of automated legal reasoning.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Presented at the COLIEE 2024 competition, describing how our experiences working with NLI allowed us to achieve the first-place entry</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Peer-reviewed publication with COLIEE, published in the proceedings as: Bilgin, O., Fields, L., Laverghetta Jr., A., Marji, Z., Nighojkar, A., Steinle, S., &amp; Licato, J. 2024. Exploring Prompting Approaches in Legal Textual Entailment. The Review of Socionetwork Strategies.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Submission to &ldquo;Cognitive Processing&rdquo; journal, entitled &ldquo;Eliciting Belief-Bias with LLM-Generated Questions&rdquo;. Awaiting review.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Submission to &ldquo;Nature: Computational Intelligence&rdquo; journal. Finalizing submission, expected to submit in early September.</p>\n<p><strong>BROADER IMPACT OUTCOMES</strong></p>\n<p>OPPORTUNITIES FOR RESEARCH AND TEACHING: This project created publications, datasets, models, prompting techniques, and knowledge in general that will be incorporated into Dr. John Licato&rsquo;s popular NLP, Computer Ethics, and Computational Cognitive Modeling courses at the University of South Florida. It will also form the foundation of future research projects that the Advancing Machine and Human Reasoning (AMHR) lab will pursue.</p>\n<p>IMPROVED PERFORMANCE, SKILLS, ATTITUDES OF MEMBERS OF UNDERREPRESENTED GROUPS: We were fortunate enough to recruit a mixed-ethnic and mixed-gender group of highly talented undergraduate and graduate students to work and participate in this project. They all report overwhelmingly positive experiences working on this project, and their only complaint is that it ended after only one year. This project led to the completion of one PhD dissertation (Animesh Nighojkar).</p>\n<p>PROVIDED EXPOSURE TO SCIENCE AND TECHNOLOGY FOR YOUNG PEOPLE: This project provided research opportunities for two graduate-level student researchers and six undergraduate-level student researchers. These researchers became part of at least one paper submission, which was a first for many of them. They were therefore deeply involved with every aspect of the process&mdash;from idea development, to implementation, re-pivoting to deal with unexpected problems, data collection, data analysis, paper writing, and more.<strong></strong></p>\n<p>The project has resulted in datasets that will be used in educational contexts (Dr. Licato&rsquo;s future courses, where they are planned to be a part of future assignments).&nbsp;</p>\n<p>PI Licato introduced new course materials based on this proposal's work; e.g., his Computational Cognitive Modeling course will be re-designed to incorporate modeling the various sub-types of entailment for which we will collect empirical data. New modules will also be developed teaching best practices for crowdsourcing data, which is an increasingly important part of natural language processing and yet subject to many points of failure. His Natural Language Processing course will also be significantly enriched with new lessons focusing on entailment and inference-based semantics.&nbsp;</p><br>\n<p>\n Last Modified: 08/23/2024<br>\nModified by: John&nbsp;Licato</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project set out to transform the notion of inference used in the natural language inference (NLI) paradigm by first introducing a six-option answer format that more richly captures naturalistic inference types. We used data collection protocols, drawn from cognitive psychology specifically to elicit Type 1 or Type 2 reasoning. We then applied the data and models to answer the following research questions: (1) Does a six-option format better capture natural language inference than the three-option format currently used? (2) Can the NLI paradigm effectively distinguish Type 1 and 2 reasoning, and if so, which data collection protocols should be used? (3) What sort of NLI items best highlight the differences identified in research questions 1 and 2?\n\n\nOur work resulted in the following.\n\n\nINTELLECTUAL MERIT OUTCOMES\n\n\n- [O1] After reviewing literature, we devised approximately 20 new methods of pseudo-automatically generating NLI problems, and created a large dataset of thousands of new questions.\n\n\n- [O2] We refined our design of four different data collection procedures designed to elicit both System 1 and System 2 reasoning, grounded in the cognitive psychology literature.\n\n\n- [O3] We obtained responses from hundreds of participants, resulting in several datasets.\n\n\n- [O4] We found that the memory procedure was most effective at differentiating System 1 and System 2 responses. We were also able to obtain detailed results about which question types were more effective at differentiating System 1 and System 2 responses. We were able to replicate some earlier results on belief-bias questions in the psychology literature, while extending them with a new technique for LLM-assisted annotation of participant-provided justifications.\n\n\n- [O5] We carried out an extensive surveying of modeling techniques, including classic machine learning algorithms and a variety of LLMs with prompting methods.\n\n\n- [O6] We devised a new technique (personality-based prompting with evolutionary weights) to model the distribution of responses from a human population. We were able to show this was much more effective than nave prompting methods.\n\n\n- Discovered a prompting technique that, when combined with weights learned through an evolutionary algorithm, can effectively capture a distribution of human responses on System 1 and System 2 questions.\n\n\n- Won first place in the 2024 COLIEE Legal Reasoning competition by applying our expertise on the NLI problem to the domain of automated legal reasoning.\n\n\n- Presented at the COLIEE 2024 competition, describing how our experiences working with NLI allowed us to achieve the first-place entry\n\n\n- Peer-reviewed publication with COLIEE, published in the proceedings as: Bilgin, O., Fields, L., Laverghetta Jr., A., Marji, Z., Nighojkar, A., Steinle, S., & Licato, J. 2024. Exploring Prompting Approaches in Legal Textual Entailment. The Review of Socionetwork Strategies.\n\n\n- Submission to Cognitive Processing journal, entitled Eliciting Belief-Bias with LLM-Generated Questions. Awaiting review.\n\n\n- Submission to Nature: Computational Intelligence journal. Finalizing submission, expected to submit in early September.\n\n\nBROADER IMPACT OUTCOMES\n\n\nOPPORTUNITIES FOR RESEARCH AND TEACHING: This project created publications, datasets, models, prompting techniques, and knowledge in general that will be incorporated into Dr. John Licatos popular NLP, Computer Ethics, and Computational Cognitive Modeling courses at the University of South Florida. It will also form the foundation of future research projects that the Advancing Machine and Human Reasoning (AMHR) lab will pursue.\n\n\nIMPROVED PERFORMANCE, SKILLS, ATTITUDES OF MEMBERS OF UNDERREPRESENTED GROUPS: We were fortunate enough to recruit a mixed-ethnic and mixed-gender group of highly talented undergraduate and graduate students to work and participate in this project. They all report overwhelmingly positive experiences working on this project, and their only complaint is that it ended after only one year. This project led to the completion of one PhD dissertation (Animesh Nighojkar).\n\n\nPROVIDED EXPOSURE TO SCIENCE AND TECHNOLOGY FOR YOUNG PEOPLE: This project provided research opportunities for two graduate-level student researchers and six undergraduate-level student researchers. These researchers became part of at least one paper submission, which was a first for many of them. They were therefore deeply involved with every aspect of the processfrom idea development, to implementation, re-pivoting to deal with unexpected problems, data collection, data analysis, paper writing, and more.\n\n\nThe project has resulted in datasets that will be used in educational contexts (Dr. Licatos future courses, where they are planned to be a part of future assignments).\n\n\nPI Licato introduced new course materials based on this proposal's work; e.g., his Computational Cognitive Modeling course will be re-designed to incorporate modeling the various sub-types of entailment for which we will collect empirical data. New modules will also be developed teaching best practices for crowdsourcing data, which is an increasingly important part of natural language processing and yet subject to many points of failure. His Natural Language Processing course will also be significantly enriched with new lessons focusing on entailment and inference-based semantics.\t\t\t\t\tLast Modified: 08/23/2024\n\n\t\t\t\t\tSubmitted by: JohnLicato\n"
 }
}