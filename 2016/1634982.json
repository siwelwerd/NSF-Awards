{
 "awd_id": "1634982",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Green Simulation: A Methodology for Reusing the Output of Past Computer Simulation Experiments",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922443",
 "po_email": "gaklutke@nsf.gov",
 "po_sign_block_name": "Georgia-Ann Klutke",
 "awd_eff_date": "2017-01-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 299324.0,
 "awd_amount": 299324.0,
 "awd_min_amd_letter_date": "2016-08-09",
 "awd_max_amd_letter_date": "2020-01-22",
 "awd_abstract_narration": "The standard practice in computer simulation experiments is to run an experiment to answer a question, and use the experiment's output only to answer that question. When future questions arise that are not answered adequately by output from past experiments, that output is not used at all in answering them. Instead, a new experiment is run, as though it were the first experiment run with that simulation model. This award will help make methods to make computer simulation experiments more efficient by reusing the output of old experiments. Simulation modeling is an important tool in military, business, science, and engineering applications. Computer simulation experiments often occupy scarce, expensive high-performance computing facilities and consume substantial amounts of electricity. If successful, this research will benefit society by reducing the resources consumed by computer simulation experiments. The project includes opportunities to train Ph. D. students and to integrate research findings into Ph. D. courses on simulation. Efforts will be made to recruit students from underrepresented groups; several female students have been part of the principal investigator's research group.\r\n\r\nThe objective of this research is to improve the computational efficiency of stochastic simulation experiments in a setting in which there is a sequence of repeated experiments using the same simulation model with different inputs. Outputs of this research will include algorithms that have the potential to improve the efficiency of later experiments by storing and reusing the output of earlier experiments. The methods to be employed include the likelihood ratio method, metamodeling, and variance reduction techniques for stochastic simulation. This research is also applicable more broadly to statistics and analytics, not just to simulation. It will develop experimental designs that take into account the availability of data from previous experiments, determining what additional data may be acquired to answer the question at hand.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Barry",
   "pi_last_name": "Nelson",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Barry L Nelson",
   "pi_email_addr": "nelsonb@northwestern.edu",
   "nsf_id": "000294721",
   "pi_start_date": "2017-04-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Jeremy",
   "pi_last_name": "Staum",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Jeremy C Staum",
   "pi_email_addr": "j-staum@northwestern.edu",
   "nsf_id": "000194415",
   "pi_start_date": "2016-08-09",
   "pi_end_date": "2017-04-10"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Jing",
   "pi_last_name": "Dong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jing Dong",
   "pi_email_addr": "jd2736@columbia.edu",
   "nsf_id": "000680605",
   "pi_start_date": "2017-04-10",
   "pi_end_date": "2020-01-22"
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "2145 Sheridan Road",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602083113",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "006Y00",
   "pgm_ele_name": "OE Operations Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "071E",
   "pgm_ref_txt": "MFG ENTERPRISE OPERATIONS"
  },
  {
   "pgm_ref_code": "072E",
   "pgm_ref_txt": "NETWORKS & QUEUING SYSTEMS"
  },
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "077E",
   "pgm_ref_txt": "SIMULATION MODELS"
  },
  {
   "pgm_ref_code": "078E",
   "pgm_ref_txt": "ENTERPRISE DESIGN & LOGISTICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 299324.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Computer models, often called \"simulations,\" provide support for planning, improvement and risk management of large-scale, complex systems in industry, health care, government, finance and services. They do this by generating synthetic performance data for systems that do not yet exist, allowing options and improvements to be explored without expensive and possibly dangerous tampering with the real world. The upside: a simulation can include all relevant details. The downside: detailed simulations may execute very slowly, from seconds to hours to days for each scenario of interest. Fundamental to the simulations considered in this research is the presence of system  performance uncertainty or risk; sources of uncertainty are represented by probability  distributions.</p>\n<p>The premise of this research is that much more value can be derived from the expensive synthetic data generated by simulations than is current practice, thereby enhancing the value and cost-effectiveness of simulation. The foundation of the approach is strategic reuse of data. \"Resue\" can mean (a) transforming synthetic data generated to represent one scenario so that it also represents others; (b) obtaining additional information from synthetic data beyond simulated system performance; and (c) optimizing simulated system performance by learning collectively from all simulated scenarios.</p>\n<p>The project yielded new methods for reweighting synthetic data via a concept called \"likelihood ratios\" so that scenarios simulated under one set of probabiility distributions yield data representative of other distributions. It also produced new data-reuse methods for estimating the basic probability distributions in the simulation so that they are more representative of reality; the key idea is model averaging. The net result of this work is reducing the risk of simulation model error, while allowing broader assessment of system performance risk, all without requiring additional real-world or synthetic data.</p>\n<p>The project yielded new methods to obtain deeper insight from synthetic data, particularly in the area of sensitivity analysis. These methods assess the sensitivity of system performance to simulation model components without additional simulation effort or modification of the simulation itself. The sensitivity measures are easy for analysts to compute and interpret, but are the first such measures to be rigorously justified.</p>\n<p>The project yielded new methods for employing simulation to optimize system performance. What makes these methods \"new\" is the collective use of all simulated data to quickly infer scenarios that are highly likely NOT to be superior, and therefore eliminate them from further expensive simulation. As a bonus these methods are well adapted to implementation in high-performance, parallel computing environments.</p>\n<p>The project supported the education of two female Ph.D. students and a post-doctoral fellow. One Ph.D. student was placed in a U.S.-based analytics firm, and the postdoc is an assistant professor in a U.S. university. Publically available software in the CRAN respository was created for data resuse in probability distribution modeling, making is accessible to anyone programming in R. Open-source versions of the parallel simulation optimization software are in development. The new sensitivity measures were implemented in clinical trial and supply chain simulation projects at SAS Institute. Journal and conference proceedings publications document the results, which were also presented in a keynote lecture at the 2020 INFORMS Conference. Many of the methods created in this research are general purpose, and therefore ideally suited to the commercial simulation software products that engineers use every day.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/16/2021<br>\n\t\t\t\t\tModified by: Barry&nbsp;L&nbsp;Nelson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nComputer models, often called \"simulations,\" provide support for planning, improvement and risk management of large-scale, complex systems in industry, health care, government, finance and services. They do this by generating synthetic performance data for systems that do not yet exist, allowing options and improvements to be explored without expensive and possibly dangerous tampering with the real world. The upside: a simulation can include all relevant details. The downside: detailed simulations may execute very slowly, from seconds to hours to days for each scenario of interest. Fundamental to the simulations considered in this research is the presence of system  performance uncertainty or risk; sources of uncertainty are represented by probability  distributions.\n\nThe premise of this research is that much more value can be derived from the expensive synthetic data generated by simulations than is current practice, thereby enhancing the value and cost-effectiveness of simulation. The foundation of the approach is strategic reuse of data. \"Resue\" can mean (a) transforming synthetic data generated to represent one scenario so that it also represents others; (b) obtaining additional information from synthetic data beyond simulated system performance; and (c) optimizing simulated system performance by learning collectively from all simulated scenarios.\n\nThe project yielded new methods for reweighting synthetic data via a concept called \"likelihood ratios\" so that scenarios simulated under one set of probabiility distributions yield data representative of other distributions. It also produced new data-reuse methods for estimating the basic probability distributions in the simulation so that they are more representative of reality; the key idea is model averaging. The net result of this work is reducing the risk of simulation model error, while allowing broader assessment of system performance risk, all without requiring additional real-world or synthetic data.\n\nThe project yielded new methods to obtain deeper insight from synthetic data, particularly in the area of sensitivity analysis. These methods assess the sensitivity of system performance to simulation model components without additional simulation effort or modification of the simulation itself. The sensitivity measures are easy for analysts to compute and interpret, but are the first such measures to be rigorously justified.\n\nThe project yielded new methods for employing simulation to optimize system performance. What makes these methods \"new\" is the collective use of all simulated data to quickly infer scenarios that are highly likely NOT to be superior, and therefore eliminate them from further expensive simulation. As a bonus these methods are well adapted to implementation in high-performance, parallel computing environments.\n\nThe project supported the education of two female Ph.D. students and a post-doctoral fellow. One Ph.D. student was placed in a U.S.-based analytics firm, and the postdoc is an assistant professor in a U.S. university. Publically available software in the CRAN respository was created for data resuse in probability distribution modeling, making is accessible to anyone programming in R. Open-source versions of the parallel simulation optimization software are in development. The new sensitivity measures were implemented in clinical trial and supply chain simulation projects at SAS Institute. Journal and conference proceedings publications document the results, which were also presented in a keynote lecture at the 2020 INFORMS Conference. Many of the methods created in this research are general purpose, and therefore ideally suited to the commercial simulation software products that engineers use every day.\n\n\t\t\t\t\tLast Modified: 08/16/2021\n\n\t\t\t\t\tSubmitted by: Barry L Nelson"
 }
}