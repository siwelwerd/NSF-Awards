{
 "awd_id": "1564000",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: Medium: Dropping Convexity: New Algorithms, Statistical Guarantees and Scalable Software for Non-convex Matrix Estimation",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 902415.0,
 "awd_amount": 902415.0,
 "awd_min_amd_letter_date": "2016-03-28",
 "awd_max_amd_letter_date": "2019-08-23",
 "awd_abstract_narration": "An image from your camera is a matrix of numbers, but most matrices of numbers would not look like an image -- the matrix of numbers in an image reflect structure from the scene.  Many applications of data analysis across science, engineering, and business can be viewed as taking a matrix of observations and fitting low-rank or otherwise structured matrices to explain their relationships. Image and video analysis is not the only example; the problem arises in structural analysis of social networks, divining user preferences for new products and services, and many other analysis tasks. \r\n\r\nAs the scale and dimensionality of these problems increases, the data analyst is faced with a gap between rigor and scale: theoretically sound algorithms often have requirements (e.g. repeated/random access to data) that are feasible only on medium-scale datasets, and even then may not provide answers in \"interactive time\" (i.e. smallish time scales required for a human interactively analyzing data). Thus practice has turned towards methods that lack rigorous guarantees, but that are scalable and have been observed to provide decent approximation.\r\n\r\n This project aims to narrow this gap by two technical observations: \r\n(a) Recognizing that fast matrix inference necessitates non-convex algorithms, it focuses on developing a rigorous analysis of the same, and  \r\n(b) by explicitly incorporating big-data architectures (out of core, and distributed multicore) in the algorithm design and statistical analysis stage itself. it focuses on several specific tasks, including pass-efficient low-rank approximation, minimizing general convex functions over the non-convex set of low-rank matrices, robust matrix estimation, and non-linear and kernel matrix settings. \r\n\r\n\r\nThe project trains graduate students in the mathematical and computational development important for data analysis. The promise of big data can only be realized by scaling infrastructure with data to continue to provide statistically meaningful insights; this project aims to realize this promise for a large suite of matrix estimation problems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sujay",
   "pi_last_name": "Sanghavi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sujay Sanghavi",
   "pi_email_addr": "sanghavi@mail.utexas.edu",
   "nsf_id": "000535791",
   "pi_start_date": "2016-03-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Inderjit",
   "pi_last_name": "Dhillon",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Inderjit S Dhillon",
   "pi_email_addr": "inderjit@cs.utexas.edu",
   "nsf_id": "000200521",
   "pi_start_date": "2016-03-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "101 East 27th St., Suite 5.300",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121532",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7933",
   "pgm_ref_txt": "NUM, SYMBOL, & ALGEBRA COMPUT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 413752.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 240971.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 247692.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project was focused on non-convex algorithms for matrix estimation. Classically, matrix estimation has either involved methods built on singular value decomposition (SVD), or via formulations as the optima of convex optimization problems. In contrast, our aim was to investigate - theoretically and empirically - the power of non-convex methods in general-purpose matrix estimation settings. Of particular interest is settings where the matrix of interest is low-rank, but its relationship to the data it is trying to fit cannot be captured by a convex loss function.&nbsp;</p>\n<p>Framing tasks as non-convex problems is often more efficient terms of space and compute. In particular, our methods focus on explicitly representing large matrices as the outer product of much smaller matrices, and performing optimization in this compact representation.</p>\n<p>This style of approach has been missing rigorous performance guarantees because one cannot give a global optimality guarantee for a generic non-convex optimizer. Our works closed this gap, by using the specific form of non-convexity arising in matrix estimation to derive theoretical guarantees.</p>\n<p>This project has resulted in 30+ papers in the top venues in machine learning, and directly supported the PhDs of 6 students, all of whom have now successfully defended their theses. The research resulting from this work has inspired 3 graduate classes and 2 undergraduate classes at UT Austin.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/21/2023<br>\n\t\t\t\t\tModified by: Sujay&nbsp;Sanghavi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project was focused on non-convex algorithms for matrix estimation. Classically, matrix estimation has either involved methods built on singular value decomposition (SVD), or via formulations as the optima of convex optimization problems. In contrast, our aim was to investigate - theoretically and empirically - the power of non-convex methods in general-purpose matrix estimation settings. Of particular interest is settings where the matrix of interest is low-rank, but its relationship to the data it is trying to fit cannot be captured by a convex loss function. \n\nFraming tasks as non-convex problems is often more efficient terms of space and compute. In particular, our methods focus on explicitly representing large matrices as the outer product of much smaller matrices, and performing optimization in this compact representation.\n\nThis style of approach has been missing rigorous performance guarantees because one cannot give a global optimality guarantee for a generic non-convex optimizer. Our works closed this gap, by using the specific form of non-convexity arising in matrix estimation to derive theoretical guarantees.\n\nThis project has resulted in 30+ papers in the top venues in machine learning, and directly supported the PhDs of 6 students, all of whom have now successfully defended their theses. The research resulting from this work has inspired 3 graduate classes and 2 undergraduate classes at UT Austin. \n\n\t\t\t\t\tLast Modified: 07/21/2023\n\n\t\t\t\t\tSubmitted by: Sujay Sanghavi"
 }
}