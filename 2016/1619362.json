{
 "awd_id": "1619362",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: AF: Small: Deep Learning Theory",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2016-07-01",
 "awd_exp_date": "2019-12-31",
 "tot_intn_awd_amt": 490000.0,
 "awd_amount": 490000.0,
 "awd_min_amd_letter_date": "2016-06-10",
 "awd_max_amd_letter_date": "2016-06-10",
 "awd_abstract_narration": "Deep learning has recently emerged as a major advance in machine learning and AI.  This technology for learning from data has provided field-changing performance improvements in image classification and speech recognition, it has displayed impressive performance across a large variety of areas (including natural language processing, robotics, audio processing, and computational chemistry), and it has become a central ingredient in AI systems.  But despite these successes, our understanding of these methods is incomplete. The broad goal of this research project is to address this grand challenge: to develop analysis techniques that enable us to understand when and why deep learning methods will be successful, and to design effective methods with explicit performance guarantees.  Successful research outcomes have a significant potential for practical impact in the large and growing set of application areas where these methods are used.\r\n\r\nThe project aims to understand the performance of deep learning methods - in particular to elucidate what aspects are essential for their success - and hence to develop principled design techniques and performance guarantees.  The objectives are: to characterize the performance impacts of the critical features of current neural network architectures: scale, depth, nonlinearities, and regularization; to develop analysis techniques that facilitate our understanding of the approximation and estimation properties of deep architectures; to identify the boundary between easy and hard learning problems for deep networks; and to develop methods with explicit performance guarantees for optimization in deep neural networks.  Successful research outcomes are likely to increase our understanding of deep learning methods, to provide performance guarantees for these methods, and to facilitate the principled design of novel deep learning methods.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Bartlett",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Peter Bartlett",
   "pi_email_addr": "bartlett@stat.berkeley.edu",
   "nsf_id": "000489454",
   "pi_start_date": "2016-06-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947045940",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 490000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research project focused on identifying what aspects of deep learning methods are essential for their success. It has led to an improved understanding of the representation, generalization and optimization properties of deep neural networks.&nbsp; In particular, this work has led to insights into how depth helps in modeling pattern-label relationships, what determines when and how deep networks, trained on a sample of pattern-label pairs, can accurately predict labels of subsequent patterns, and why the solutions found by the simple optimization methods that are widely used for training deep networks might be so effective.&nbsp; The project supported five PhD students and involved 13 other students and postdocs as participants in the research activities. In reporting results from this project, junior researchers have had the opportunity to give 19 talks at major international conferences, including ICML, COLT and NeurIPS. The PI has presented aspects of the research in sixteen keynote, plenary and invited talks and in eighteen other invited seminars and colloquia. The PI also co-organized a semester-long collaborative research program on the Foundations of Machine Learning in 2017, a workshop on generalization in deep learning at ICML 2019, and, as Associate Director of the Simons Institute, played a major role in organizing an 11-week collaborative research program on the Foundations of Deep Learning in 2019.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/22/2020<br>\n\t\t\t\t\tModified by: Peter&nbsp;Bartlett</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis research project focused on identifying what aspects of deep learning methods are essential for their success. It has led to an improved understanding of the representation, generalization and optimization properties of deep neural networks.  In particular, this work has led to insights into how depth helps in modeling pattern-label relationships, what determines when and how deep networks, trained on a sample of pattern-label pairs, can accurately predict labels of subsequent patterns, and why the solutions found by the simple optimization methods that are widely used for training deep networks might be so effective.  The project supported five PhD students and involved 13 other students and postdocs as participants in the research activities. In reporting results from this project, junior researchers have had the opportunity to give 19 talks at major international conferences, including ICML, COLT and NeurIPS. The PI has presented aspects of the research in sixteen keynote, plenary and invited talks and in eighteen other invited seminars and colloquia. The PI also co-organized a semester-long collaborative research program on the Foundations of Machine Learning in 2017, a workshop on generalization in deep learning at ICML 2019, and, as Associate Director of the Simons Institute, played a major role in organizing an 11-week collaborative research program on the Foundations of Deep Learning in 2019.\n\n\t\t\t\t\tLast Modified: 02/22/2020\n\n\t\t\t\t\tSubmitted by: Peter Bartlett"
 }
}