{
 "awd_id": "1647427",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Planning Believable Narratives by Modeling Agent Beliefs",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 156969.0,
 "awd_amount": 156969.0,
 "awd_min_amd_letter_date": "2016-07-25",
 "awd_max_amd_letter_date": "2016-07-25",
 "awd_abstract_narration": "The goal of this project is to improve the software that generates stories automatically for virtual environments like training simulations and educational games. Specifically, the software will be able to reason about what is actually true, what each character thinks is true, what they think others think is true, and so on, to improve the way virtual characters act and make them seem more believable and more human. Current approaches to designing these narratives often assume agents know everything about others' beliefs and goals; this often leads to inconsistent or un-believable behaviors by the agents, which damage the credibility of the software and quality of the experience for their human users.  The proposal will extend the lead researcher's existing narrative planning system, using an approach that lets agents consider multiple sets of beliefs that are consistent with their own and others' actions so far, ruling out situations where agents have beliefs that are inconsistent with their actions.  Compared to existing approaches, this should allow the narrative planner to generate a wider variety of narratives that are also more believable to humans, as well as to handle situations such as trickery and uncertainty where reasoning about beliefs is crucial.  The research team will test the software and these assumptions through several experiments that ask people to compare narratives generated by the new software to those generated by state of the art methods.  If successful, the project sets the stage to improve the quality of systems where virtual agents interact with humans such as smart phone assistants, online games, automated customer chat tools, and educational software.  In particular, the work will lead to training scenarios where understanding others' beliefs is crucial, such as officer-citizen interactions.  The work is also interdisciplinary, ranging from computer science to psychology, and the lead researcher is committed to training young researchers to do work that crosses these intellectual boundaries and to recruiting researchers who might not otherwise participate in computer science-related research.\r\n\r\nIn the work, the lead researcher proposes to develop a model of agent belief based on doxastic modal logic and possible worlds reasoning suitable for use in a planning algorithm that coordinates a virtual environment. By supporting a single modal 'believes' predicate, the planner can treat the narrative search space as a Kripke structure to reason about epistemically accessible states. This improves on previous models by allowing arbitrarily nested beliefs while simultaneously reducing the burden on the virtual environment's author to write alternative scenarios, thus increasing their flexibility and expressiveness.  The research team will integrate this model of beliefs into a prototype system based on the Glaive narrative planner previously developed by the lead researcher.  This prototype will take advantage of Glaive's existing heuristic-driven state-space search techniques: in addition to expanding temporally accessible states, Glaive will also expand epistemically accessible states and track when an action taken by an epistemic child can be anticipated by its epistemic parent in the Kripke structure.  The initial prototype will be too slow for real-time use, but it will be suitable for conducting the proposed experiments that investigate to what extent such a model improves the believability of agent behavior in automatically generated stories.  In particular, the team will study whether the planner produces narratives whose structure better meets the expectations of a human audience: that is, the model will answer questions about agent beliefs more similarly to a human audience and the resulting planner will generate stories more like those composed by human authors.  Further, the prototype is expected to solve certain narrative planning problems which algorithms that lack a model of agent beliefs cannot solve.  These claims will be evaluated by having the new prototype and two state-of-the-art planners generate narratives for a library of scenarios to be developed by the team that rely on agents having a theory of mind for other agents, then asking both the systems and human users a number of questions about the generated narrative and agents' beliefs to evaluate how well the planners' output conforms with humans' expectations and believability.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Ware",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen G Ware",
   "pi_email_addr": "sgware@cs.uky.edu",
   "nsf_id": "000678546",
   "pi_start_date": "2016-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of New Orleans",
  "inst_street_address": "2000 LAKESHORE DR",
  "inst_street_address_2": "",
  "inst_city_name": "NEW ORLEANS",
  "inst_state_code": "LA",
  "inst_state_name": "Louisiana",
  "inst_phone_num": "5042806836",
  "inst_zip_code": "701483520",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "LA02",
  "org_lgl_bus_name": "UNIVERSITY OF NEW ORLEANS",
  "org_prnt_uei_num": "",
  "org_uei_num": "CL8JHK1LN291"
 },
 "perf_inst": {
  "perf_inst_name": "University of New Orleans",
  "perf_str_addr": "2000 Lakeshore Dr.",
  "perf_city_name": "New Orleans",
  "perf_st_code": "LA",
  "perf_st_name": "Louisiana",
  "perf_zip_code": "701480001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "LA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 156969.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Interactive virtual worlds provide a safe, immersive, cost-effective way to explore a situation. From virtual reality military training simulations, to adaptive educational games for the classroom, to therapeutic recreations of traumatic events, virtual worlds can help solve a wide variety of problems. Most of these virtual worlds invite the player to control one character while the other non-player characters are controlled by the system to create an interactive narrative. Players need to feel like their choices and actions matter, but writing interactive stories is difficult because one must account for the huge space of possibilities that arise form player choices.</p>\n<p>Artificial Intelligent can address this challenge by controlling the non-player characters in the virtual world to ensure they act realistically no matter what the player does. Instead of writing a story before the simulation starts, one is generated intelligently during the simulation. To accomplish this, we need models of how intelligent agents act.</p>\n<p>There are two broad approaches to this challenge in the AI community. The most common is the Strong Autonomy approach, which creates realistic individual characters. A virtual world full of realistic characters should create a realistic narrative; the problem is that the designer of the system cannot guarantee that the story has the right structure. When a virtual world is meant to teach or train people, it is essential that the designer be able to impose certain requirements. The second approach, called Strong Story, uses a centralized narrative planner to create a story which is guaranteed to meet the designer?s needs. The challenge for Strong Story systems is to ensure that characters will act believably in the process.</p>\n<p>This project improved a strong story narrative planning algorithm by reasoning about individual character beliefs. Character need to act as if they have individual, possibly wrong beliefs. They need to adjust their plans when they observe new information, and they need to realistically ignore new information that they have no way of observing. They need to be able to reason about what they believe others believe, and so on.</p>\n<p>The first major result from this project was a narrative algorithm that reasons about beliefs. It incorporated previous work on intentionality and redefined a realistic action to be one which the character believes will contribute to achieving its goal. Once characters have and act on their wrong beliefs, we can generate stories that feature things like ignorance and learning. Perhaps most importantly, this new model of character behavior allows one character to anticipate the actions of others. When one character believes that another character will take some action in the future, the first character can act as if they expect that action to happen. Anticipation enables many new narrative phenomena that are essential for generating stories in virtual worlds, such as cooperation and deception.</p>\n<p>The second major result validated that this model appears realistic to a human audience. First, we demonstrated that humans reading stories answer questions about what character believe similarly to our model. Second, we used our narrative planning algorithm to generate hundreds of stories that featured realistic behavior based on intentionality (characters doing things that will achieve their goals), belief (characters doing things that they expect to be possible), both, and neither. As we expected, audiences reported that the most realistic stories are those featuring both intentionality and belief. A character does not need to form a plan that will actually work in order to look realistic; they only need to think the plan will work. This evaluation was more comprehensive than many previous evaluations of strong story algorithms because, instead of studying a handful of specific stories generated by the algorithm, it sampled hundreds of stories from the entire space of possible stories that could be generated, thus demonstrating that the algorithm produces realistic stories in most situations.</p>\n<p>Finally, we put these models to use in two interactive virtual environment prototypes. The first was an entertainment-focused video game, and players reported that (1) virtual characters seems to pursue their goals, (2) virtual characters acted according to their own limited, possibly wrong beliefs, and (3) players felt their actions mattered in the story. The second environment was a virtual reality police use-of-force training exercise, which demonstrated that players get better at using minimal force the more they play. Both of these environments had no pre-scripted narrative content; the story was generated entirely at run time based on the actions that the players decided to take. Our algorithm guarantees a certain story structure while still ensuring the virtual character act realistically no matter what the player does.</p>\n<p>This model of realistic behavior with beliefs, the narrative planning algorithm, and the two proof-of-concept virtual environments have laid the foundation for the next generation of strong story interactive narrative virtual worlds to teach, train, and entertain.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/26/2019<br>\n\t\t\t\t\tModified by: Stephen&nbsp;G&nbsp;Ware</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nInteractive virtual worlds provide a safe, immersive, cost-effective way to explore a situation. From virtual reality military training simulations, to adaptive educational games for the classroom, to therapeutic recreations of traumatic events, virtual worlds can help solve a wide variety of problems. Most of these virtual worlds invite the player to control one character while the other non-player characters are controlled by the system to create an interactive narrative. Players need to feel like their choices and actions matter, but writing interactive stories is difficult because one must account for the huge space of possibilities that arise form player choices.\n\nArtificial Intelligent can address this challenge by controlling the non-player characters in the virtual world to ensure they act realistically no matter what the player does. Instead of writing a story before the simulation starts, one is generated intelligently during the simulation. To accomplish this, we need models of how intelligent agents act.\n\nThere are two broad approaches to this challenge in the AI community. The most common is the Strong Autonomy approach, which creates realistic individual characters. A virtual world full of realistic characters should create a realistic narrative; the problem is that the designer of the system cannot guarantee that the story has the right structure. When a virtual world is meant to teach or train people, it is essential that the designer be able to impose certain requirements. The second approach, called Strong Story, uses a centralized narrative planner to create a story which is guaranteed to meet the designer?s needs. The challenge for Strong Story systems is to ensure that characters will act believably in the process.\n\nThis project improved a strong story narrative planning algorithm by reasoning about individual character beliefs. Character need to act as if they have individual, possibly wrong beliefs. They need to adjust their plans when they observe new information, and they need to realistically ignore new information that they have no way of observing. They need to be able to reason about what they believe others believe, and so on.\n\nThe first major result from this project was a narrative algorithm that reasons about beliefs. It incorporated previous work on intentionality and redefined a realistic action to be one which the character believes will contribute to achieving its goal. Once characters have and act on their wrong beliefs, we can generate stories that feature things like ignorance and learning. Perhaps most importantly, this new model of character behavior allows one character to anticipate the actions of others. When one character believes that another character will take some action in the future, the first character can act as if they expect that action to happen. Anticipation enables many new narrative phenomena that are essential for generating stories in virtual worlds, such as cooperation and deception.\n\nThe second major result validated that this model appears realistic to a human audience. First, we demonstrated that humans reading stories answer questions about what character believe similarly to our model. Second, we used our narrative planning algorithm to generate hundreds of stories that featured realistic behavior based on intentionality (characters doing things that will achieve their goals), belief (characters doing things that they expect to be possible), both, and neither. As we expected, audiences reported that the most realistic stories are those featuring both intentionality and belief. A character does not need to form a plan that will actually work in order to look realistic; they only need to think the plan will work. This evaluation was more comprehensive than many previous evaluations of strong story algorithms because, instead of studying a handful of specific stories generated by the algorithm, it sampled hundreds of stories from the entire space of possible stories that could be generated, thus demonstrating that the algorithm produces realistic stories in most situations.\n\nFinally, we put these models to use in two interactive virtual environment prototypes. The first was an entertainment-focused video game, and players reported that (1) virtual characters seems to pursue their goals, (2) virtual characters acted according to their own limited, possibly wrong beliefs, and (3) players felt their actions mattered in the story. The second environment was a virtual reality police use-of-force training exercise, which demonstrated that players get better at using minimal force the more they play. Both of these environments had no pre-scripted narrative content; the story was generated entirely at run time based on the actions that the players decided to take. Our algorithm guarantees a certain story structure while still ensuring the virtual character act realistically no matter what the player does.\n\nThis model of realistic behavior with beliefs, the narrative planning algorithm, and the two proof-of-concept virtual environments have laid the foundation for the next generation of strong story interactive narrative virtual worlds to teach, train, and entertain.\n\n \n\n\t\t\t\t\tLast Modified: 06/26/2019\n\n\t\t\t\t\tSubmitted by: Stephen G Ware"
 }
}