{
 "awd_id": "1650080",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Novel sampling algorithms for scaling up spectral methods for unsupervised learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Weng-keen Wong",
 "awd_eff_date": "2016-08-15",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 90000.0,
 "awd_amount": 90000.0,
 "awd_min_amd_letter_date": "2016-08-23",
 "awd_max_amd_letter_date": "2016-08-23",
 "awd_abstract_narration": "In the era of big data, unsupervised learning has become increasingly important. At a high-level, unsupervised learning serves to reduce the data size, while capturing its important underlying structure. For a powerful and widely-used family of unsupervised learning techniques (those based on spectral methods), scaling up to large data sets poses significant computational challenges. This research project will develop extremely simple and lightweight sampling techniques for scaling up this family of unsupervised learning methods. Since big data is ubiquitous, these research advances are likely to be transformative to a range of fields. This project will benefit society through the research team's ongoing collaborations in climate science, agriculture, and finance. The team will also continue to engage the computer science community in this endeavor, by training students, developing tutorials, and broadening the participation of women and minorities in computing.\r\n\r\nThis project will advance machine learning research by scaling up spectral methods for the analysis of large data sets. While spectral methods for the unsupervised learning tasks of clustering and embedding have found wide success in a variety of practical applications, scaling them up to large data sets poses significant computational challenges. In particular, the storage and computation needed to handle the affinity matrix (a matrix of pairwise similarities between data points) can be prohibitive. An approach that has found promise is to instead approximate this matrix in some sense. The goal of this project is to provide simple approximation techniques that manage the tradeoff between their space and time complexity vs. the quality of the approximation. The proposed approach involves sampling techniques that address this goal by exploiting latent structure in a data set, in order to minimize the amount of information that needs to be stored to (approximately) represent it. This leads to techniques that speed up the computation and reduce the memory requirements of spectral methods, while simultaneously providing better approximations. The project will also continue the team's momentum on leveraging advances in machine learning for data-driven discovery.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Claire",
   "pi_last_name": "Monteleoni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Claire Monteleoni",
   "pi_email_addr": "cmontel@colorado.edu",
   "nsf_id": "000517989",
   "pi_start_date": "2016-08-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "George Washington University",
  "inst_street_address": "1918 F ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "WASHINGTON",
  "inst_state_code": "DC",
  "inst_state_name": "District of Columbia",
  "inst_phone_num": "2029940728",
  "inst_zip_code": "200520042",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DC00",
  "org_lgl_bus_name": "GEORGE WASHINGTON UNIVERSITY (THE)",
  "org_prnt_uei_num": "",
  "org_uei_num": "ECR5E2LU5BL6"
 },
 "perf_inst": {
  "perf_inst_name": "George Washington University",
  "perf_str_addr": "800 22nd Street NW",
  "perf_city_name": "Washington",
  "perf_st_code": "DC",
  "perf_st_name": "District of Columbia",
  "perf_zip_code": "200520058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DC00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 90000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; color: #454545} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; color: #454545; min-height: 14.0px} -->\n<p class=\"p1\">Many types of data are abundantly available in raw form, i.e. prior to being labeled for any classification or regression task (tasks typically referred to as supervised learning). For example, the number of photographs available online exceeds (astronomically) the number that have been labeled with meaningful text, let alone any text at all. When labels are not readily available, the resulting machine learning problem is known as unsupervised learning. The goal is typically to extract some latent structure in the data, such as features or clusters, in order to summarize the data, to make sense of it, or to reduce its size before further stages of the machine learning pipeline.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">Spectral methods for the unsupervised learning tasks of clustering and embedding data have found wide success in a variety of practical applications, particularly on data that can be represented as a graph. However scaling these methods up to large data sets poses significant computational challenges. In particular, the storage and computation needed to handle the affinity matrix (a matrix of pairwise similarities between data points) can be prohibitive. Our work on this project has contributed to scaling up spectral methods to big data. We have developed two light-weight algorithms that approximate the affinity matrix. Our approach helps to better manage the tradeoff between the computational burden and the quality of the approximation needed for finding meaningful clusters in the data.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">To gain insight in our clustering research, we also analyzed clustering heuristics that have demonstrated strong empirical performance on a variety of applications, but lacked solid theoretical foundations. In particular, we analyzed the convergence of stochastic k-means algorithm variants, e.g., online k-means and mini-batch k-means, which have enjoyed wide-spread success and deployment in several popular machine learning software packages.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">This project made impact on the fields of machine learning and artificial intelligence, and on the training of graduate students.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/08/2019<br>\n\t\t\t\t\tModified by: Claire&nbsp;Monteleoni</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany types of data are abundantly available in raw form, i.e. prior to being labeled for any classification or regression task (tasks typically referred to as supervised learning). For example, the number of photographs available online exceeds (astronomically) the number that have been labeled with meaningful text, let alone any text at all. When labels are not readily available, the resulting machine learning problem is known as unsupervised learning. The goal is typically to extract some latent structure in the data, such as features or clusters, in order to summarize the data, to make sense of it, or to reduce its size before further stages of the machine learning pipeline.\n \nSpectral methods for the unsupervised learning tasks of clustering and embedding data have found wide success in a variety of practical applications, particularly on data that can be represented as a graph. However scaling these methods up to large data sets poses significant computational challenges. In particular, the storage and computation needed to handle the affinity matrix (a matrix of pairwise similarities between data points) can be prohibitive. Our work on this project has contributed to scaling up spectral methods to big data. We have developed two light-weight algorithms that approximate the affinity matrix. Our approach helps to better manage the tradeoff between the computational burden and the quality of the approximation needed for finding meaningful clusters in the data.\n \nTo gain insight in our clustering research, we also analyzed clustering heuristics that have demonstrated strong empirical performance on a variety of applications, but lacked solid theoretical foundations. In particular, we analyzed the convergence of stochastic k-means algorithm variants, e.g., online k-means and mini-batch k-means, which have enjoyed wide-spread success and deployment in several popular machine learning software packages.\n \nThis project made impact on the fields of machine learning and artificial intelligence, and on the training of graduate students.\n\n \n\n\t\t\t\t\tLast Modified: 01/08/2019\n\n\t\t\t\t\tSubmitted by: Claire Monteleoni"
 }
}