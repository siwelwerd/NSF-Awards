{
 "awd_id": "1566481",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: Learning to Predict Temporal Interestingness for Videos",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2016-04-01",
 "awd_exp_date": "2020-03-31",
 "tot_intn_awd_amt": 174674.0,
 "awd_amount": 182634.0,
 "awd_min_amd_letter_date": "2016-04-14",
 "awd_max_amd_letter_date": "2017-04-25",
 "awd_abstract_narration": "This project examines the role that implicit feedback from viewers can play in learning a temporal interestingness function for videos. The key insight is that by leveraging pupil dilation as ground truth, supervised machine learning approaches can be applied to this problem. The ubiquitous presence of cameras in every phone, and the ability to share content with the entire world have made videos a powerful tool in the hands of everyday people. This project is to address the challenge that viewers have ever-shortening attention spans, and constructing a succinct message is hard. The project provides research and training opportunities for both undergraduate and graduate students in computer vision, machine learning, and human-centered computing.\r\n \r\nThis project collects a corpus of eye-tracking data as viewers watch a collection of videos, via an off-the-shelf eye-tracking device with the objective of investigating the effectiveness of a controlled brightness calibration method to separate pupillary light reflex from pupillary emotional response. The emotional response data is leveraged as dense labels for a supervised learning approach towards predicting a video interestingness function, and algorithms are developed to cut videos to their most succinct portions based on this interestingness function. A predictive model of video interestingness could potentially impact video retrieval, summarization, and search. This would impact fields as diverse as communication and online education. Further, just as research in image saliency was hugely accelerated by the use of eye-tracking data for training, validation, and benchmarking, this project can lead to a similar unification of diverse approaches, and efforts, around an implicit, temporally dense source of ground truth for temporal interestingness in videos.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eakta",
   "pi_last_name": "Jain",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eakta Jain",
   "pi_email_addr": "ejain@ufl.edu",
   "nsf_id": "000679375",
   "pi_start_date": "2016-04-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Florida",
  "inst_street_address": "1523 UNION RD RM 207",
  "inst_street_address_2": "",
  "inst_city_name": "GAINESVILLE",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "3523923516",
  "inst_zip_code": "326111941",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "FL03",
  "org_lgl_bus_name": "UNIVERSITY OF FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NNFQH1JAPEP3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Florida",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "326115500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "FL03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "026y00",
   "pgm_ele_name": "CRII CISE Research Initiation"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 174674.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 7960.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p style=\"background: white;\"><span style=\"font-size: 10.0pt;\">The major goal of this project is to leverage eye-tracking data, including gaze positions and pupil diameter changes, as an implicit annotation for video interesting-ness prediction. Eye tracking users as they watch videos provides a stream of rich data that contains patterns of attention and emotion. Gaze positions reveal where users focused their attention, and pupil diameter changes index their arousal state. Toward the major goal, the research funded by this award has developed pupillary light response models to account for brightness related pupil diameter changes and methods to measure and visualize interesting regions in images and videos, including omnidirectional 360 content. The premise of the project is the abundant availability of eye tracking data. With eye trackers being designed into laptops, phones, and mixed reality headsets, large scale eye tracking creates novel security and privacy challenges. The activities supported by this project pushed forward this new frontier for eye tracking research through research papers on specific privacy and security threats and mitigations. The project has produced eleven publications in total (eight in peer-reviewed conferences and journals) and two manuscripts currently under review. The award has funded six graduate students (three women, one Native American) and nine undergraduate students (four women, two African-American). </span></p>\n<!--  /* Font Definitions */  @font-face \t{font-family:\"Cambria Math\"; \tpanose-1:2 4 5 3 5 4 6 3 2 4; \tmso-font-charset:0; \tmso-generic-font-family:roman; \tmso-font-pitch:variable; \tmso-font-signature:-536869121 1107305727 33554432 0 415 0;} @font-face \t{font-family:Calibri; \tpanose-1:2 15 5 2 2 2 4 3 2 4; \tmso-font-charset:0; \tmso-generic-font-family:swiss; \tmso-font-pitch:variable; \tmso-font-signature:-469750017 -1073732485 9 0 511 0;} @font-face \t{font-family:ArialMT; \tpanose-1:2 11 6 4 2 2 2 2 2 4; \tmso-font-alt:Arial; \tmso-font-charset:0; \tmso-generic-font-family:roman; \tmso-font-pitch:auto; \tmso-font-signature:0 0 0 0 0 0;}  /* Style Definitions */  p.MsoNormal, li.MsoNormal, div.MsoNormal \t{mso-style-unhide:no; \tmso-style-qformat:yes; \tmso-style-parent:\"\"; \tmargin-top:0in; \tmargin-right:0in; \tmargin-bottom:8.0pt; \tmargin-left:0in; \tline-height:107%; \tmso-pagination:widow-orphan; \tfont-size:12.0pt; \tmso-bidi-font-size:11.0pt; \tfont-family:\"Times New Roman\",serif; \tmso-fareast-font-family:Calibri; \tmso-bidi-font-family:Calibri; \tmso-fareast-language:ZH-CN;} p \t{mso-style-priority:99; \tmso-margin-top-alt:auto; \tmargin-right:0in; \tmso-margin-bottom-alt:auto; \tmargin-left:0in; \tmso-pagination:widow-orphan; \tfont-size:12.0pt; \tfont-family:\"Times New Roman\",serif; \tmso-fareast-font-family:\"Times New Roman\";} .MsoChpDefault \t{mso-style-type:export-only; \tmso-default-props:yes; \tfont-family:\"Calibri\",sans-serif; \tmso-ascii-font-family:Calibri; \tmso-ascii-theme-font:minor-latin; \tmso-fareast-font-family:Calibri; \tmso-hansi-font-family:Calibri; \tmso-hansi-theme-font:minor-latin; \tmso-bidi-font-family:\"Times New Roman\"; \tmso-bidi-theme-font:minor-bidi;}size:8.5in 11.0in; \tmargin:1.0in 1.0in 1.0in 1.0in; \tmso-header-margin:.5in; \tmso-footer-margin:.5in; \tmso-paper-source:0;} div.WordSection1 \t{page:WordSection1;} --><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/25/2020<br>\n\t\t\t\t\tModified by: Eakta&nbsp;Jain</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The major goal of this project is to leverage eye-tracking data, including gaze positions and pupil diameter changes, as an implicit annotation for video interesting-ness prediction. Eye tracking users as they watch videos provides a stream of rich data that contains patterns of attention and emotion. Gaze positions reveal where users focused their attention, and pupil diameter changes index their arousal state. Toward the major goal, the research funded by this award has developed pupillary light response models to account for brightness related pupil diameter changes and methods to measure and visualize interesting regions in images and videos, including omnidirectional 360 content. The premise of the project is the abundant availability of eye tracking data. With eye trackers being designed into laptops, phones, and mixed reality headsets, large scale eye tracking creates novel security and privacy challenges. The activities supported by this project pushed forward this new frontier for eye tracking research through research papers on specific privacy and security threats and mitigations. The project has produced eleven publications in total (eight in peer-reviewed conferences and journals) and two manuscripts currently under review. The award has funded six graduate students (three women, one Native American) and nine undergraduate students (four women, two African-American). \n\n\n\t\t\t\t\tLast Modified: 05/25/2020\n\n\t\t\t\t\tSubmitted by: Eakta Jain"
 }
}