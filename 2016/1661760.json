{
 "awd_id": "1661760",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: DKA: Collaborative Research: Randomized Numerical Linear Algebra (RandNLA) for multi-linear and non-linear data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-08-15",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 246664.0,
 "awd_amount": 246664.0,
 "awd_min_amd_letter_date": "2016-09-21",
 "awd_max_amd_letter_date": "2016-09-21",
 "awd_abstract_narration": "Data are often modeled as matrices; and, as a result, linear algebraic algorithms such as matrix decompositions have proven extremely successful in the analysis of many data sets.  Randomized Numerical Linear Algebra (RandNLA) integrates the complementary perspectives that Theoretical Computer Science and Numerical Linear Algebra bring to matrix computations, and it is a new paradigm for the design and analysis of such algorithms and for using the resulting insight to solve important scientific and societal problems.  Current RandNLA algorithms extract linear structure from data matrices.  The proposed work will extend RandNLA methods to multi-linear and other non-linear structure in data matrices.\r\n\r\nIn more detail, the proposed work will investigate two important, non-linear, structural settings in order to start making progress towards using RandNLA approaches in situations where the underlying data exhibit non-linear structure: it will investigate how to design the next generation of RandNLA algorithms that can handle data that exhibit multi-linear structures captured by tensors; and it will investigate the applicability of RandNLA approaches to data that exhibit non-linear structure, as captured by non-linear dimensionality reduction techniques, local spectral methods, and related semi-supervised eigenvector tools. In addition, it will evaluate the proposed approaches on data applications where the PIs have significant expertise, such as the statistical analysis of population genetics data and astronomical data.  Broader impacts of the project include graduate and undergraduate training, workshops and code development for RandNLA.  For further information see the project web site at:http://www.stat.berkeley.edu/~mmahoney/projects/nsf-multilinear/",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Petros",
   "pi_last_name": "Drineas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Petros Drineas",
   "pi_email_addr": "pdrineas@purdue.edu",
   "nsf_id": "000117416",
   "pi_start_date": "2016-09-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "155 S. Grant Street",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072114",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 246664.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The research supported multiple extensions of Randomized Linear Algebra techniques beyond core Randomized Linear Algebra to include non-linear, multi-linear, first- and second-order optimization, and other areas.&nbsp; The theoretical methods used are central to the theoretical foundations of large-scale data analysis, and the broader impact lies in the usefulness of these methods in domains such as genetics, mass spec imaging, astronomy.&nbsp; Major accomplishments include demonstrating that Randomized Linear Algebra methods can improve optimization, that they can be used with local methods such as local spectral methods to identify non-linear structure in large-scale data, and that they can be used with multi-linear and non-negative constraints. We also presented multiple novel analyses of Krylov subspace methods, as well as tensor-based element-wise sampling algorithms.&nbsp; &nbsp;Implementations and applications of these methods in several scientific big data domains, including tera-byte scale implementations of low-rank approximation, and applications in populations genetics, were also supported.&nbsp; In terms of broader impact, major summer schools associated with SIAM (G2S3) and IAS (PCMI) were organized to disseminate these and related techniques to a broad range of students; and workshops and meetings such as MMDS 2016 and the Simons Institute program on Foundations of Data were organized.&nbsp; Related to these were articles such as a review article in CACM and a summary in SIAM News highlighting these events.&nbsp;</p>\n<p>Publications supported by this award include:</p>\n<p><strong>1.</strong> A. Chowdhuri, J. Yang, and P. Drineas, Randomized Iterative Algorithms for Fisher Discriminant Analysis, Conference on Uncertainty in Artificial Intelligence (UAI), 64, 2019. Selected for oral presentation.&nbsp;</p>\n<p><strong>2.</strong> P. Drineas and I. Ipsen, Low-Rank Matrix Approximations Do Not Need a Singular Value Gap, SIAM Journal on Matrix Analysis and Applications, 40(1),pp. 299-319, 2019.</p>\n<p><strong>3.</strong> A. Chowdhuri, J. Yang, and P. Drineas, An Iterative, Sketching-based Framework for Ridge Regression, Proceedings of the International Conference on Machine Learning (ICML), 2018.</p>\n<p><strong>4.</strong> P. Drineas, I. Ipsen, E. Kontopoulou, and M. Magdon-Ismail, Structural Convergence Results for Approximations of Dominant Subspaces from Block Krylov Spaces, SIAM Journal on Matrix Analysis and Applications, 39(2), pp. 567-586, 2018.</p>\n<p><strong>5.</strong> P. Drineas and M. W. Mahoney, Lectures on Randomized Numerical Linear Algebra, The Mathematics of Data, IAS/Park City Math. Ser., vol. 25, Amer. Math. Soc., Providence, RI, 2018.&nbsp;</p>\n<p><strong>6.</strong> K. Fountoulakis, A. Kundu, E. Kontopoulou, and P. Drineas, A Randomized Rounding Algorithm for Sparse PCA, ACM Transactions on Knowledge Discovery from Data (TKDD), 11(3), pp. 1-26, 2017.</p>\n<p><strong>7.</strong> K. Clarkson, P. Drineas, M. Magdon-Ismail, M. W. Mahoney, X. Meng, and D. P. Woodruff, Faster Robust Linear Regression, SIAM Journal on Computing, 45(3), pp. 763-810, 2016.</p>\n<p><strong>8.</strong> S. Paul, M. Magdon-Ismail, and P. Drineas, Feature Selection for Linear SVMs with Provable Guarantees, Pattern Recognition, 60, pp. 205-214, 2016.</p>\n<p><strong>9.</strong> M. W. Mahoney and P. Drineas, Structural Properties Underlying high-quality Randomized Numerical Linear Algebra algorithms, CRC Handbook on Big Data, pp. 137-154, 2016.</p>\n<p><strong>10.</strong> E. Gallopoulos, P. Drineas, I. Ipsen, and M. W. Mahoney, RandNLA, Pythons, and the CUR for your Data problems, SIAM News, p. 7, February 2016.</p>\n<p><strong>11.</strong> W. Mahoney and P. Drineas, RandNLA: Randomized Numerical Linear Algebra, Communications of the ACM (CACM), 59 (6), pp. 80-90, 2016.</p>\n<p><strong>12.</strong> A. Kundu, P. Drineas, and M. Magdon-Ismail, Approximating Sparse PCA from Incomplete Data, Proc. of Neural Information Processing Systems (NIPS), 2015.</p>\n<p><strong>13.</strong> S. Paul, M. Magdon-Ismail, and P. Drineas, Column Selection via Adaptive Sampling, Proc. of Neural Information Processing Systems (NIPS), 2015.</p>\n<p><strong>14.</strong> N. Nguyen, P. Drineas, and T. Tran, Tensor Sparsification via a Bound on the Spectral Norm of Random Tensors, Information and Inference: A Journal of the IMA, 4(3), pp. 195-229, 2015.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/24/2019<br>\n\t\t\t\t\tModified by: Petros&nbsp;Drineas</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe research supported multiple extensions of Randomized Linear Algebra techniques beyond core Randomized Linear Algebra to include non-linear, multi-linear, first- and second-order optimization, and other areas.  The theoretical methods used are central to the theoretical foundations of large-scale data analysis, and the broader impact lies in the usefulness of these methods in domains such as genetics, mass spec imaging, astronomy.  Major accomplishments include demonstrating that Randomized Linear Algebra methods can improve optimization, that they can be used with local methods such as local spectral methods to identify non-linear structure in large-scale data, and that they can be used with multi-linear and non-negative constraints. We also presented multiple novel analyses of Krylov subspace methods, as well as tensor-based element-wise sampling algorithms.   Implementations and applications of these methods in several scientific big data domains, including tera-byte scale implementations of low-rank approximation, and applications in populations genetics, were also supported.  In terms of broader impact, major summer schools associated with SIAM (G2S3) and IAS (PCMI) were organized to disseminate these and related techniques to a broad range of students; and workshops and meetings such as MMDS 2016 and the Simons Institute program on Foundations of Data were organized.  Related to these were articles such as a review article in CACM and a summary in SIAM News highlighting these events. \n\nPublications supported by this award include:\n\n1. A. Chowdhuri, J. Yang, and P. Drineas, Randomized Iterative Algorithms for Fisher Discriminant Analysis, Conference on Uncertainty in Artificial Intelligence (UAI), 64, 2019. Selected for oral presentation. \n\n2. P. Drineas and I. Ipsen, Low-Rank Matrix Approximations Do Not Need a Singular Value Gap, SIAM Journal on Matrix Analysis and Applications, 40(1),pp. 299-319, 2019.\n\n3. A. Chowdhuri, J. Yang, and P. Drineas, An Iterative, Sketching-based Framework for Ridge Regression, Proceedings of the International Conference on Machine Learning (ICML), 2018.\n\n4. P. Drineas, I. Ipsen, E. Kontopoulou, and M. Magdon-Ismail, Structural Convergence Results for Approximations of Dominant Subspaces from Block Krylov Spaces, SIAM Journal on Matrix Analysis and Applications, 39(2), pp. 567-586, 2018.\n\n5. P. Drineas and M. W. Mahoney, Lectures on Randomized Numerical Linear Algebra, The Mathematics of Data, IAS/Park City Math. Ser., vol. 25, Amer. Math. Soc., Providence, RI, 2018. \n\n6. K. Fountoulakis, A. Kundu, E. Kontopoulou, and P. Drineas, A Randomized Rounding Algorithm for Sparse PCA, ACM Transactions on Knowledge Discovery from Data (TKDD), 11(3), pp. 1-26, 2017.\n\n7. K. Clarkson, P. Drineas, M. Magdon-Ismail, M. W. Mahoney, X. Meng, and D. P. Woodruff, Faster Robust Linear Regression, SIAM Journal on Computing, 45(3), pp. 763-810, 2016.\n\n8. S. Paul, M. Magdon-Ismail, and P. Drineas, Feature Selection for Linear SVMs with Provable Guarantees, Pattern Recognition, 60, pp. 205-214, 2016.\n\n9. M. W. Mahoney and P. Drineas, Structural Properties Underlying high-quality Randomized Numerical Linear Algebra algorithms, CRC Handbook on Big Data, pp. 137-154, 2016.\n\n10. E. Gallopoulos, P. Drineas, I. Ipsen, and M. W. Mahoney, RandNLA, Pythons, and the CUR for your Data problems, SIAM News, p. 7, February 2016.\n\n11. W. Mahoney and P. Drineas, RandNLA: Randomized Numerical Linear Algebra, Communications of the ACM (CACM), 59 (6), pp. 80-90, 2016.\n\n12. A. Kundu, P. Drineas, and M. Magdon-Ismail, Approximating Sparse PCA from Incomplete Data, Proc. of Neural Information Processing Systems (NIPS), 2015.\n\n13. S. Paul, M. Magdon-Ismail, and P. Drineas, Column Selection via Adaptive Sampling, Proc. of Neural Information Processing Systems (NIPS), 2015.\n\n14. N. Nguyen, P. Drineas, and T. Tran, Tensor Sparsification via a Bound on the Spectral Norm of Random Tensors, Information and Inference: A Journal of the IMA, 4(3), pp. 195-229, 2015.\n\n\t\t\t\t\tLast Modified: 09/24/2019\n\n\t\t\t\t\tSubmitted by: Petros Drineas"
 }
}