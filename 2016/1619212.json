{
 "awd_id": "1619212",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: Developing Golden Speakers for Second-Language Pronunciation Training",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 179999.0,
 "awd_amount": 179999.0,
 "awd_min_amd_letter_date": "2016-08-10",
 "awd_max_amd_letter_date": "2021-08-16",
 "awd_abstract_narration": "People who learn a second language (L2) as adults often speak with a persistent foreign accent.  This can make them less intelligible, more subject to discrimination, and less confident when interacting with others. Surprisingly, though, L2 learners rarely receive formal training in pronunciation, in part because effective training must be customized to meet each learner's individual needs.  To address this gap, the investigators propose to develop algorithms to synthesize a personalized \"golden speaker\" for each learner: his or her own voice but with a native accent. The rationale is that, by listening to their own golden speaker, learners can more easily perceive differences between their actual and ideal pronunciations. This work focuses on developing the technology for golden speakers, which the investigators plan to evaluate in the future as a new tool for pronunciation learning systems. As such, this research can benefit a large number of workers in the US who are non-native speakers of English, particularly in higher education, health care and the technology sector. The project also provides opportunities for graduate and undergraduate students to conduct research in a multi-disciplinary team with expertise in signal processing, machine learning, and language acquisition. \r\n\r\nTwo types of golden-speaker model are proposed. The first type is based on a reformulation of parametric statistical models for voice conversion, where instead of force-aligning source (native) and target (non-native) frames, they are matched based on their phonetic similarity.  Several similarity metrics are proposed, from vocal-tract-length normalization to deep auto-encoders. The second type is based on a sparse representation of speech, which models individual frames as linear combinations of phonetic anchors.  This requires new techniques to transform the constellation of anchors in the L2 speech to match the structure of native anchors (e.g., pairwise distances). Two types of evaluation are proposed for the golden-speaker models: their ability to interpolate phones not included in the learner's inventory, and the accent, intelligibility and comprehensibility of the resulting speech, as rated by native English listeners.  For this purpose, the investigators propose to collect a large speech corpus from multiple Spanish and Korean learners of English and Indian speakers of English, each at different levels of English proficiency.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ricardo",
   "pi_last_name": "Gutierrez-Osuna",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ricardo Gutierrez-Osuna",
   "pi_email_addr": "rgutier@cs.tamu.edu",
   "nsf_id": "000255119",
   "pi_start_date": "2016-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "TEES State Headquarters Bldg",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778454645",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 179999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The two main outcomes of this project are (1) algorithms to transform non-native speech to sound more native-like, and (2) a speech corpus with recordings from 24 non-native speakers of American English. Both outcomes can facilitate the development of computerized pronunciation training tools that learners of a second language can use to improve their pronunciation.</p>\n<p>For the first outcome, the project produced several algorithms of increasing complexity and capabilities. The first generation (G1) of algorithms required that a separate &ldquo;accent conversion&rdquo; model be developed for each non-native speaker. These G1 algorithms worked by modifying an utterance from a native speaker so that it appeared to have been spoken by the non-native speaker.&nbsp;&nbsp; As such, these algorithms required that a native speaker produced the utterance before it could be transformed.&nbsp; The second generation (G2) algorithms relaxed this constraint, and allowed a non-native utterance to be transformed directly to sound more native-like, without needing a that a native speaker produced that utterance in advance.&nbsp; Finally, the third generation (G3) algorithms relaxed the constraint that a separate &ldquo;accent conversion&rdquo; model be developed for each non-native speaker.&nbsp; Instead, the G3 algorithms only required a short utterance (3-5 seconds) from the non-native speaker in order to resynthesize native-like speech that sounded like their voice.</p>\n<p>For the second outcome, the research team collected a large corpus of speech recordings that could be used to develop better &ldquo;accent conversion&rdquo; algorithms as well as speech recognition algorithms that could identify mispronunciations in non-native speech and use them as feedback to help non-native learners improve their pronunciation.&nbsp; The speech corpus, which is called L2-ARCTIC, includes recordings from 24 speakers, 2 female and 2 male speakers of American English whose first languages were Hindi, Korean, Mandarin, Spanish, Arabic and Vietnamese.&nbsp; Each speaker recorded over one hour of read speech of over 1,000 phonetically-balanced sentences. For each speaker, a team of trained linguists manually annotated 150 sentences: 100 sentences across the 24 speakers, and 50 sentences containing phonemes that were likely to be difficult to each speaker&rsquo;s first language.&nbsp;&nbsp; In addition, the L2-ARCTIC speech corpus contains word-level and phoneme-level transcriptions, including forced-aligned word and phoneme boundaries.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/27/2023<br>\n\t\t\t\t\tModified by: Ricardo&nbsp;Gutierrez-Osuna</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe two main outcomes of this project are (1) algorithms to transform non-native speech to sound more native-like, and (2) a speech corpus with recordings from 24 non-native speakers of American English. Both outcomes can facilitate the development of computerized pronunciation training tools that learners of a second language can use to improve their pronunciation.\n\nFor the first outcome, the project produced several algorithms of increasing complexity and capabilities. The first generation (G1) of algorithms required that a separate \"accent conversion\" model be developed for each non-native speaker. These G1 algorithms worked by modifying an utterance from a native speaker so that it appeared to have been spoken by the non-native speaker.   As such, these algorithms required that a native speaker produced the utterance before it could be transformed.  The second generation (G2) algorithms relaxed this constraint, and allowed a non-native utterance to be transformed directly to sound more native-like, without needing a that a native speaker produced that utterance in advance.  Finally, the third generation (G3) algorithms relaxed the constraint that a separate \"accent conversion\" model be developed for each non-native speaker.  Instead, the G3 algorithms only required a short utterance (3-5 seconds) from the non-native speaker in order to resynthesize native-like speech that sounded like their voice.\n\nFor the second outcome, the research team collected a large corpus of speech recordings that could be used to develop better \"accent conversion\" algorithms as well as speech recognition algorithms that could identify mispronunciations in non-native speech and use them as feedback to help non-native learners improve their pronunciation.  The speech corpus, which is called L2-ARCTIC, includes recordings from 24 speakers, 2 female and 2 male speakers of American English whose first languages were Hindi, Korean, Mandarin, Spanish, Arabic and Vietnamese.  Each speaker recorded over one hour of read speech of over 1,000 phonetically-balanced sentences. For each speaker, a team of trained linguists manually annotated 150 sentences: 100 sentences across the 24 speakers, and 50 sentences containing phonemes that were likely to be difficult to each speaker\u2019s first language.   In addition, the L2-ARCTIC speech corpus contains word-level and phoneme-level transcriptions, including forced-aligned word and phoneme boundaries.\n\n\t\t\t\t\tLast Modified: 03/27/2023\n\n\t\t\t\t\tSubmitted by: Ricardo Gutierrez-Osuna"
 }
}