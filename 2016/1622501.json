{
 "awd_id": "1622501",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "High-Performance, High-Level Tools for Statistical Inference and Unsupervised Learning",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Christopher Stark",
 "awd_eff_date": "2016-09-15",
 "awd_exp_date": "2020-12-31",
 "tot_intn_awd_amt": 480000.0,
 "awd_amount": 480000.0,
 "awd_min_amd_letter_date": "2016-09-13",
 "awd_max_amd_letter_date": "2020-06-16",
 "awd_abstract_narration": "Using the \"Julia\" language for scientific computing developed at MIT, the UC Davis, MIT, and Julia Computing, Inc. teams funded by this project will extend the Julia language and runtime to utilize massively-parallel graphics processing units (GPUs) as first-class processors for scientific computing. Julia offers the twin advantages of straightforward, high-level programmability as well as excellent performance; adding GPU capability within Julia opens the door to even greater performance. The team will use Julia and its new GPU capabilities to address emerging important problems in statistical inference and unsupervised learning, an application area that aims to draw useful conclusions from massive amounts of data. Using a high-level, high-performance language such as Julia will allow non-computer-science experts to address these important problems.\r\n\r\nThe project team brings together three threads of expertise to address the challenge of delivering best-of-breed performance from a high-level language in the context of the important application domain of statistical inference and unsupervised learning: (1) application experts in this domain; (2) the designers of the programming language Julia, which allows its users to express their ideas in high-level abstractions that are natural to statisticians and mathematicians; and (3) parallel computing experts, who will develop the new support within Julia to target high-performance GPUs as first-class processors. The major outcome of this project will be a significantly enhanced Julia language and runtime that will deliver both high-level programmability, targeted at scientists who are not parallel computing experts, and best-of-breed performance.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Owens",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "John D Owens",
   "pi_email_addr": "jowens@ece.ucdavis.edu",
   "nsf_id": "000377403",
   "pi_start_date": "2016-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Edelman",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Alan S Edelman",
   "pi_email_addr": "EDELMAN@MATH.MIT.EDU",
   "nsf_id": "000142969",
   "pi_start_date": "2016-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Fisher",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "John W Fisher",
   "pi_email_addr": "fisher@csail.mit.edu",
   "nsf_id": "000483362",
   "pi_start_date": "2016-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jeff",
   "pi_last_name": "Bezanson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jeff Bezanson",
   "pi_email_addr": "alan@juliacomputing.com",
   "nsf_id": "000687579",
   "pi_start_date": "2016-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "One Shields Avenue",
  "perf_city_name": "Davis",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956165270",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  },
  {
   "pgm_ele_code": "808400",
   "pgm_ele_name": "CDS&E"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "8084",
   "pgm_ref_txt": "CDS&E"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 164612.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 156667.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 158721.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong id=\"docs-internal-guid-a1444684-7fff-bc8d-8d4d-441585bbe5b3\" style=\"font-weight: normal;\"> </strong></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong id=\"docs-internal-guid-a1444684-7fff-bc8d-8d4d-441585bbe5b3\" style=\"font-weight: normal;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our work targeted the design, implementation, and use of building blocks for emerging machine learning applications, with the primary goal of making our primitives both high-performance and highly programmable. In this effort we used the Julia programming language, a relatively new language co-created by two of the principal investigators on this project. Julia has significant momentum in science and industry and is the ideal substrate for our twin goals of performance and portability.</span></strong></p>\n<p><strong id=\"docs-internal-guid-a1444684-7fff-bc8d-8d4d-441585bbe5b3\" style=\"font-weight: normal;\"> <br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">One major outcome is an open-source Julia library for unsupervised learning in unstructured data, specifically for Dirichlet and Hierarchical Direchlet process models. This library prioritizes scalability to large computers and large datasets, using Julia&rsquo;s support for parallel and distributed abstractions. The work involved both advances in techniques implemented inside this library as well as open-source development of the library for use by other researchers and professionals.</span></p>\n<br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The advanced algorithms included in this library include implementations of DP-means, collapsed Gibbs inference for Dirichlet Process Mixture Models (DPMM) and Hierarchical Dirichlet Process Mixture Models (HDPMM), quasi-collapsed Gibbs samplers, Direct Gibbs samplers, and finally parallel split/merge MCMC inference for DPMMs and HDPMMs.&nbsp;</span></p>\n<br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A second major outcome is the development of a Julia compiler framework (&ldquo;Tulpar&rdquo;) for implementing embedded domain-specific languages within Julia. The primary use for this library is for developers to create their own high-performance domain-specific languages with minimal effort, languages that take advantage of Julia&rsquo;s powerful language and libraries. We expect it will be useful across a variety of application domains, have currently implemented image-processing and database-query languages in it, and are preparing an open-source release.</span></p>\n<br />\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tulpar is based on the lightweight modular staging approach. Its features include a modular design for scheduling generic functions and fine-grained cooperative functional composition. The framework has three main layers: an LMS layer (major challenges: virtualizing Julia&rsquo;s non-function-call constructs, leveraging and enhancing Julia&rsquo;s type system for semi-automatic local binding-time analysis, and enabling polymorphic embedding with a trait system); a DSL layer (major challenges: designing the intermediate representation, DSL op-specific semantics, an effect tracking system, and an optimizer API); and an application layer (major challenge: debugging).</span></p>\n</strong><br class=\"Apple-interchange-newline\" /></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/07/2021<br>\n\t\t\t\t\tModified by: John&nbsp;D&nbsp;Owens</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nOur work targeted the design, implementation, and use of building blocks for emerging machine learning applications, with the primary goal of making our primitives both high-performance and highly programmable. In this effort we used the Julia programming language, a relatively new language co-created by two of the principal investigators on this project. Julia has significant momentum in science and industry and is the ideal substrate for our twin goals of performance and portability.\n\n \n\nOne major outcome is an open-source Julia library for unsupervised learning in unstructured data, specifically for Dirichlet and Hierarchical Direchlet process models. This library prioritizes scalability to large computers and large datasets, using Julia\u2019s support for parallel and distributed abstractions. The work involved both advances in techniques implemented inside this library as well as open-source development of the library for use by other researchers and professionals.\n\n\nThe advanced algorithms included in this library include implementations of DP-means, collapsed Gibbs inference for Dirichlet Process Mixture Models (DPMM) and Hierarchical Dirichlet Process Mixture Models (HDPMM), quasi-collapsed Gibbs samplers, Direct Gibbs samplers, and finally parallel split/merge MCMC inference for DPMMs and HDPMMs. \n\n\nA second major outcome is the development of a Julia compiler framework (\"Tulpar\") for implementing embedded domain-specific languages within Julia. The primary use for this library is for developers to create their own high-performance domain-specific languages with minimal effort, languages that take advantage of Julia\u2019s powerful language and libraries. We expect it will be useful across a variety of application domains, have currently implemented image-processing and database-query languages in it, and are preparing an open-source release.\n\n\nTulpar is based on the lightweight modular staging approach. Its features include a modular design for scheduling generic functions and fine-grained cooperative functional composition. The framework has three main layers: an LMS layer (major challenges: virtualizing Julia\u2019s non-function-call constructs, leveraging and enhancing Julia\u2019s type system for semi-automatic local binding-time analysis, and enabling polymorphic embedding with a trait system); a DSL layer (major challenges: designing the intermediate representation, DSL op-specific semantics, an effect tracking system, and an optimizer API); and an application layer (major challenge: debugging).\n\n\n \n\n\t\t\t\t\tLast Modified: 04/07/2021\n\n\t\t\t\t\tSubmitted by: John D Owens"
 }
}