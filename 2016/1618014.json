{
 "awd_id": "1618014",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CCF:  Small:  Accelerating Irregular Algorithms using Cache-Coherent FPGA Accelerators",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 330000.0,
 "awd_amount": 330000.0,
 "awd_min_amd_letter_date": "2016-07-27",
 "awd_max_amd_letter_date": "2016-07-27",
 "awd_abstract_narration": "Until recently, FPGA acceleration of computations has largely focused on algorithms that exhibit a high degree of regularity and predictability in their parallelism and memory access. The advent of high-capacity FPGA accelerators connected to the processor and main memory through a high-performance cache-coherent interconnect enables algorithms with irregular parallelism to be considered. These irregular algorithms, including many data analytic and machine learning kernels, operate on very large, memory-resident, pointer-based data structures. This project will study the opportunity to accelerate irregular algorithms for performance and energy efficiency on emerging cache-coherent FPGA accelerators. The outcome of this investigation has potential for practical commercial impact by helping to establish cache-coherent FPGA acceleration as a viable new platform option for accelerating irregular algorithms that are fundamental to datacenter workloads. This project will also provide valuable training to both graduate and undergraduate students, and improve graduate-level coursework.\r\n\r\nInstead of the traditional \"off-load\" model of FPGA acceleration, this project seek to develop a new tightly-coupled FPGA-processor collaboration model that takes advantage of the low-latency, fine-grain shared-memory interactions between the processor and FPGA that are now possible. The project studies fine-grain concurrent mappings of irregular algorithms where the processor and FPGA work together---each leveraging its own characteristic advantages, e.g., large cache, high frequency ALUs for the processor and energy-efficient spatial hardware concurrency for the FPGA---to outperform what either can achieve alone. An integral part of the investigation is also to develop new insights toward what should cache-coherent FPGA accelerators ultimately look like, especially with the support for irregular algorithms in mind.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Hoe",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "James Hoe",
   "pi_email_addr": "jhoe@cmu.edu",
   "nsf_id": "000464902",
   "pi_start_date": "2016-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 330000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Field Programmable Gate Arrays (FPGAs) have been undergoing a dramatic transformation from a logic technology to a computing technology.&nbsp;This project investigated the opportunity to leverage the newly arrived architecture of high-capacity FPGA accelerators connected to the processor and main memory through a high-performance cache-coherent interconnect.&nbsp; The project in particular looked to apply this new capability to accelerate irregular algorithms that were previously unsuitable for FPGA-offload acceleration.</p>\n<p>Irregular algorithms, including many data analytic and machine learning kernels, operate on very large, memory-resident, pointer-based data structures. Focusing on graph processing, we studied a new acceleration strategy of tightly-coupled FPGA-processor collaboration, working concurrently on in-memory data and each handling the aspects of the computation best suited to it.</p>\n<p>In comparison to processors, FPGA-based solutions often fall short in terms of processing throughput due to the lack of off-chip memory bandwidth. By tailoring the on-chip buffering according to the access patterns of different data types and preprocessing the input graphs using a novel vertex remapping optimization to improve locality, we were able to maximize the on-chip data reuse and reduce off-chip memory traffic. This allows the proposed accelerator to deliver computation throughputs comparable to state-of-the-art software on a processor that has significantly better memory bandwidth and latency.</p>\n<p>Scheduling optimization is highly effective but rarely used in graph processing accelerator designs due to the implementation complexity and memory access overhead. We developed a heterogeneous processing approach for priority scheduling on a shared-memory CPU-FPGA platform. By exploiting the closely coupled integration of the host processor and the FPGA accelerator, our system dynamically offloads (in the reverse direction) the task of scheduling from FPGA to a software scheduler on the processor for its programmability, high capacity cache and low memory latency, while the FPGA graph processing accelerator enjoys the scheduling benefit and delivers higher performance with excellent energy efficiency.</p>\n<p>Our solution is demonstrated and evaluated on working Intel HARP shared-memory CPU-FPGA &nbsp;systems. We compared&nbsp;our collaborative model of acceleration to traditional offload solutions for two scheduling schemes: the well-known Dijkstra scheduling for the Single Source Shortest Path and a new scheduling optimization we developed for improving the data locality of Breadth First Search. Whereas the FPGA-offload solution requires an impractical amount of on-chip storage to implement a priority queue, the proposed processor-assisted scheduling that moves the task of scheduling to the processor consumes a negligible load on the processor and retains most of the performance benefit from priority scheduling.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/02/2020<br>\n\t\t\t\t\tModified by: James&nbsp;Hoe</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nField Programmable Gate Arrays (FPGAs) have been undergoing a dramatic transformation from a logic technology to a computing technology. This project investigated the opportunity to leverage the newly arrived architecture of high-capacity FPGA accelerators connected to the processor and main memory through a high-performance cache-coherent interconnect.  The project in particular looked to apply this new capability to accelerate irregular algorithms that were previously unsuitable for FPGA-offload acceleration.\n\nIrregular algorithms, including many data analytic and machine learning kernels, operate on very large, memory-resident, pointer-based data structures. Focusing on graph processing, we studied a new acceleration strategy of tightly-coupled FPGA-processor collaboration, working concurrently on in-memory data and each handling the aspects of the computation best suited to it.\n\nIn comparison to processors, FPGA-based solutions often fall short in terms of processing throughput due to the lack of off-chip memory bandwidth. By tailoring the on-chip buffering according to the access patterns of different data types and preprocessing the input graphs using a novel vertex remapping optimization to improve locality, we were able to maximize the on-chip data reuse and reduce off-chip memory traffic. This allows the proposed accelerator to deliver computation throughputs comparable to state-of-the-art software on a processor that has significantly better memory bandwidth and latency.\n\nScheduling optimization is highly effective but rarely used in graph processing accelerator designs due to the implementation complexity and memory access overhead. We developed a heterogeneous processing approach for priority scheduling on a shared-memory CPU-FPGA platform. By exploiting the closely coupled integration of the host processor and the FPGA accelerator, our system dynamically offloads (in the reverse direction) the task of scheduling from FPGA to a software scheduler on the processor for its programmability, high capacity cache and low memory latency, while the FPGA graph processing accelerator enjoys the scheduling benefit and delivers higher performance with excellent energy efficiency.\n\nOur solution is demonstrated and evaluated on working Intel HARP shared-memory CPU-FPGA  systems. We compared our collaborative model of acceleration to traditional offload solutions for two scheduling schemes: the well-known Dijkstra scheduling for the Single Source Shortest Path and a new scheduling optimization we developed for improving the data locality of Breadth First Search. Whereas the FPGA-offload solution requires an impractical amount of on-chip storage to implement a priority queue, the proposed processor-assisted scheduling that moves the task of scheduling to the processor consumes a negligible load on the processor and retains most of the performance benefit from priority scheduling.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/02/2020\n\n\t\t\t\t\tSubmitted by: James Hoe"
 }
}