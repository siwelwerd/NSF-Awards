{
 "awd_id": "1640078",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "E2CDA: Type I: Collaborative Research: Energy Efficient Learning Machines (ENIGMA)",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 678480.0,
 "awd_amount": 678480.0,
 "awd_min_amd_letter_date": "2016-08-31",
 "awd_max_amd_letter_date": "2018-09-17",
 "awd_abstract_narration": "The project will aim to develop computing hardware and software that improve the energy efficiency of learning machines by many orders of magnitude. In doing so it will enable large societal adoption of such machines, paving the way for new applications in diverse areas such as manufacturing, healthcare, agriculture, and many others. For example, machines that learn the behavioral trends of individual human beings by collecting data from myriads of sensors may be able to design the most appropriate drugs. Similarly, one may envision machines that learn trends in the weather and thereby assist in predicting the most optimized preparations for the next crop cycle. The possibilities are literally endless. However, the canonical learning machines of today need huge amount of energy, significantly hindering their adoption for widespread applications. The goal of this project will be to explore, evaluate and innovate new hardware and software paradigms that could reduce energy dissipation in learning machines by a significant amount. The team of researchers consists of experts in mathematics, neuroscience, electronic devices and materials and computer circuit and system design that will foster a unique platform for both innovative research and interdisciplinary training of graduate students.\r\n\r\nWe are witnessing a regimental shift in the computing paradigm. For a  vast  number  of  applications, cognitive functions such as classification, recognition, synthesis, decision-making and  learning  are gaining rapid importance in a world that is infused with sensing modalities, often paraphrased under a common term of \"Big Data\", that are in critical need of efficient information-extraction. This is in sharp contrast to the past when the central objective of computing was to perform calculations on numbers and produce results with extreme numerical accuracy. We aim to approach this problem by exploiting cognitive models that have shown efficacy in \"one shot\" learning. In this approach, the information is represented by means of high dimensional (HD) vectors. These vectors follow a set of predetermined mathematical operations that ensure that the resulting vector after such operations is unique. The uniqueness can in turn be used as \"learning\" and the predefined nature of mathematical operations make the learning \"one shot\". When paired with traditional artificial neural network or deep learning algorithms,  such  \"one  shot\" learning could significantly reduce the number of necessary computing operations, leading to orders of magnitude reduction in energy dissipation. We shall explore the entire computer hierarchy, staring from materials and devices, all the way up to system design and optimization to exploit the unique capabilities afforded by the HD computing, with the ultimate objective of realizing energy efficient learning machines (ENIGMA).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Subhasish",
   "pi_last_name": "Mitra",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Subhasish Mitra",
   "pi_email_addr": "subh@stanford.edu",
   "nsf_id": "000069199",
   "pi_start_date": "2016-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "H S Philip",
   "pi_last_name": "Wong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "H S Philip Wong",
   "pi_email_addr": "hspwong@stanford.edu",
   "nsf_id": "000168006",
   "pi_start_date": "2016-08-31",
   "pi_end_date": "2018-08-22"
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "353 Serra Mall, Gates 334",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943059025",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "015Y00",
   "pgm_ele_name": "Energy Efficient Computing: fr"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 226160.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 226160.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 226160.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">Artificial Intelligence/Machine Learning (AI/ML) is becoming an indispensable part of all our lives. As a result, the field of AI/ML is witnessing rapid advances along several fronts: new AI/ML models, new algorithms utilizing these models, new hardware architectures for these algorithms, and new technologies for creating energy-efficient implementations of such hardware architectures. For example, conventional deep learning requires brute-force training of billions of weights in repetitive iterations. As a result, learning is slow and energy-hungry. Hece, it is essential to create new approaches which are capable of ?online? or 'one-shot' learning using very few samples, and for which there exists computational theories that bound the resource usage and complexity for a given task. At the same time, significant technological advances are required to create new physical devices for&nbsp; the building blocks of future learning machines. Recognizing these needs, this project explored an AI/ML approach beyond traditional deep learning -- a computational theory called Hyper Dimensional (HD) computing with unique features such as \"one-shot\" learning. This project created many firsts: for example, the first nano-electronic systems (built using emerging nanotechnologies such as carbon nanotube field-effect transistors, resistive random-access memory, and their monolithic 3D integration) that enable energy- and area-efficient hardware implementations of HD computing architectures. Such efficient implementations are achieved by exploiting several characteristics of the component nanotechnologies (e.g., energy-efficient logic circuits, dense memory, and circuits naturally enabled by the technologies) and their monolithic 3D integration (enabling tight integration of computing elements and memory), as well as various characteristics of the HD computing model (e.g., embracing randomness that allows us to utilize rather than avoid inherent variations in nanotechnologies, resilience to errors in the underlying hardware). The project experimentally demonstrated an end-to-end HD computing nano-electronic system for pairwise classification of 21 languages with measured mean accuracy of up to 98% on &gt;20 000 sentences (6.4 million characters) and training using onetext sample (100,000 characters) per language.</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/29/2022<br>\n\t\t\t\t\tModified by: Subhasish&nbsp;Mitra</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Artificial Intelligence/Machine Learning (AI/ML) is becoming an indispensable part of all our lives. As a result, the field of AI/ML is witnessing rapid advances along several fronts: new AI/ML models, new algorithms utilizing these models, new hardware architectures for these algorithms, and new technologies for creating energy-efficient implementations of such hardware architectures. For example, conventional deep learning requires brute-force training of billions of weights in repetitive iterations. As a result, learning is slow and energy-hungry. Hece, it is essential to create new approaches which are capable of ?online? or 'one-shot' learning using very few samples, and for which there exists computational theories that bound the resource usage and complexity for a given task. At the same time, significant technological advances are required to create new physical devices for  the building blocks of future learning machines. Recognizing these needs, this project explored an AI/ML approach beyond traditional deep learning -- a computational theory called Hyper Dimensional (HD) computing with unique features such as \"one-shot\" learning. This project created many firsts: for example, the first nano-electronic systems (built using emerging nanotechnologies such as carbon nanotube field-effect transistors, resistive random-access memory, and their monolithic 3D integration) that enable energy- and area-efficient hardware implementations of HD computing architectures. Such efficient implementations are achieved by exploiting several characteristics of the component nanotechnologies (e.g., energy-efficient logic circuits, dense memory, and circuits naturally enabled by the technologies) and their monolithic 3D integration (enabling tight integration of computing elements and memory), as well as various characteristics of the HD computing model (e.g., embracing randomness that allows us to utilize rather than avoid inherent variations in nanotechnologies, resilience to errors in the underlying hardware). The project experimentally demonstrated an end-to-end HD computing nano-electronic system for pairwise classification of 21 languages with measured mean accuracy of up to 98% on &gt;20 000 sentences (6.4 million characters) and training using onetext sample (100,000 characters) per language.\n \n \n \n \n\n \n\n\t\t\t\t\tLast Modified: 06/29/2022\n\n\t\t\t\t\tSubmitted by: Subhasish Mitra"
 }
}