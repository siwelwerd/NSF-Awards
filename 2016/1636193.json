{
 "awd_id": "1636193",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "New Methodologies for Markov Decision Processes and Stochastic Games Motivated by Inventory Control",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922443",
 "po_email": "gaklutke@nsf.gov",
 "po_sign_block_name": "Georgia-Ann Klutke",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2016-07-25",
 "awd_max_amd_letter_date": "2016-07-25",
 "awd_abstract_narration": "Inventory control is broadly used in production and service systems and in supply chains to manage operations and improve efficiency and reliability.  The analysis and optimization of several important classes of inventory control problems relies on the theory of Markov decision processes, an area of operations research dealing with sequential optimization of stochastic systems.  The two major research directions in the theory of Markov decision processes are: (i) to establish the structure of optimal and approximately optimal decisions, and (ii) to develop algorithms for their computation. This project will develop new methodologies for Markov decision processes, including models with incomplete information and risk-sensitive criteria, and for stochastic games. Although motivated by inventory control problems, potential applications of this project's methodological advances include many application areas, in particular to the control of electric storage for power systems. The project will also contribute to the development of human resources in science and engineering. First, it will support Ph.D. students at Stony Brook University including female students. Second, it will create research and educational projects for graduate and undergraduate students including students from underrepresented minority groups.\r\n\r\nThis project will advance solution methodologies for two groups of decision making models: Markov decision processes, including partially observable Markov decision processes, and stochastic games.   The initial motivation for solving such problems is inspired by inventory control applications, and this project will also advance the inventory control theory.  For Markov decision processes and partially observable Markov decision processes, the project will investigate discounted total cost and average cost objectives. It will also develop methodologies for decision making under risk, robust optimization, incomplete state information, and incomplete knowledge of model parameters.  Specifically, the project will establish new results on the validity of optimality equations and inequalities and the structure of optimal policies. It will develop algorithms and investigate their convergence and complexity for problems with classic and nonstandard criteria. For games the project will develop solution methodologies for one-step and sequential stochastic problems with complete and incomplete state observations with possibly unbounded payoffs.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eugene",
   "pi_last_name": "Feinberg",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Eugene A Feinberg",
   "pi_email_addr": "eugene.feinberg@sunysb.edu",
   "nsf_id": "000096894",
   "pi_start_date": "2016-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117943600",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "006Y00",
   "pgm_ele_name": "OE Operations Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "071E",
   "pgm_ref_txt": "MFG ENTERPRISE OPERATIONS"
  },
  {
   "pgm_ref_code": "072E",
   "pgm_ref_txt": "NETWORKS & QUEUING SYSTEMS"
  },
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "077E",
   "pgm_ref_txt": "SIMULATION MODELS"
  },
  {
   "pgm_ref_code": "078E",
   "pgm_ref_txt": "ENTERPRISE DESIGN & LOGISTICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The theory of Markov decision processes (MDPs) provides scientific foundations for sequential decision making.&nbsp; It has a large number of applications to various fields including electric and industrial engineering, artificial intelligence, operations management, economics, homeland security, and medical decision making.&nbsp; In particular, MDPs have been used for modeling and optimization of inventory control management since the studies of MDPs and inventory control originated almost 70 years ago.&nbsp; However, for a long time there was a significant gap between the available results for MDPs and the results needed for periodic-review inventory control problems dealing with non-compact decision sets, since the theory of MDPs could handle only problems with compact decision sets.&nbsp; This situation was resolved relatively recently, when it was understood that the two classic mathematical facts, Berge&rsquo;s maximum theorem and Fatou&rsquo;s lemma, hold under more general assumptions than the conditions stated in these theorems.&nbsp; Berge&rsquo;s maximum theorem and its versions, which claim continuity properties of optimality equations, hold for noncompact decision sets. And Fatou&rsquo;s lemma, which states semicontinuity properties of expectations, holds in a slightly weaker form for weakly converging probabilities. This project uses these discoveries to advance the main results on MDPs and stochastic games. These results describe the structure of optimal policies and provide algorithms for their calculations.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For periodic-review inventory control problems, this project described broad sufficient conditions for the existence of optimal (s,S)-policies and advanced algorithms for their computation.&nbsp; For problems with convex holding costs, we described the structure of optimal policies for situations when there are no (s,S)-policies minimizing expected total discounted costs.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For Markov decision processes, the project described sufficient conditions for the validity of optimality equations for MDPs with average-costs per unit time.&nbsp; For example, these conditions hold for periodic-review inventory control problems with backorders. We also described conditions under which problems with average costs per unit time can be reduced to problems with the expected total discounted costs. This project also contributed to the following four groups of problems: (i) finding algorithms for computing optimal and nearly-optimal policies, (ii) problems with multiple criteria, and (iii) continuous-time problems, and (iv) control of problems with incomplete information.&nbsp; In particular, a strongly polynomial algorithm for computing approximately optimal policies was identified. &nbsp;This algorithm uses the method of value iterations.&nbsp; For problems with multiple criteria and side constraints, we proved that, if initial and transition probabilities do not have atoms (that is, states that can be visited with positive probabilities), then for an arbitrary policy there exists a deterministic policy with the same expected total costs. For deterministic policies, decisions are nonrandomized, and they depend only on the current states of the system. This is a generalization to multi-step problems of a classic result established by Dvoretzky, Wald and Wolfowitz about 70 years ago for one-step problems.&nbsp; For continuous-time problems we proved that for an arbitrary policy there exists a policy with the same or better objective function, and this strategy uses only the information about the current state and time. Such policies are called Markov, and their existence significantly simplifies solutions of continuous-time problems.&nbsp; &nbsp;For problems with incomplete information, we identified broad conditions for the existence of optimal policies, validity of optimality equations, and convergence of value iteration algorithms for partially observable Markov decision processes with expected total discounted costs.</p>\n<p>For stochastic games, the project identified general sufficient conditions for the existence of optimal policies and the validity of minimax equations and the existence of optimal policies for zero-sum games with perfect information.&nbsp; We have proved a new form of the minimax equation for games with infinite-state spaces and provided conditions for the existence of solutions when decision sets may not be compact.</p>\n<p>Outside of Operations Research, we established a sufficient condition for the Fatou lemma to hold in its classic form for weakly converging probabilities.&nbsp; This is a contribution to two fields of mathematics: real analysis and probability theory.&nbsp; This fact was used to provide general conditions for the validity of optimality equations for average-cost MDPs with infinite state spaces and possibly noncompact action sets.</p>\n<p>A graduate student, Yan Liang, completed his dissertation dealing with optimality conditions for inventory control and MDPs.&nbsp; He continued his postdoctoral training at the University of Toronto. The results of the project provided problems and additional material to two graduate courses: Dynamic Programming and Probability. In addition to Yan Lang, three graduate students were investigating problems relevant to the project.&nbsp;&nbsp; The principle investigator gave a lecture on MDPs and artificial intelligence to high school students participating in a summer research program at his university.&nbsp; The PI was working on applying MDPs to medical decision making.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/23/2020<br>\n\t\t\t\t\tModified by: Eugene&nbsp;A&nbsp;Feinberg</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe theory of Markov decision processes (MDPs) provides scientific foundations for sequential decision making.  It has a large number of applications to various fields including electric and industrial engineering, artificial intelligence, operations management, economics, homeland security, and medical decision making.  In particular, MDPs have been used for modeling and optimization of inventory control management since the studies of MDPs and inventory control originated almost 70 years ago.  However, for a long time there was a significant gap between the available results for MDPs and the results needed for periodic-review inventory control problems dealing with non-compact decision sets, since the theory of MDPs could handle only problems with compact decision sets.  This situation was resolved relatively recently, when it was understood that the two classic mathematical facts, Berge\u2019s maximum theorem and Fatou\u2019s lemma, hold under more general assumptions than the conditions stated in these theorems.  Berge\u2019s maximum theorem and its versions, which claim continuity properties of optimality equations, hold for noncompact decision sets. And Fatou\u2019s lemma, which states semicontinuity properties of expectations, holds in a slightly weaker form for weakly converging probabilities. This project uses these discoveries to advance the main results on MDPs and stochastic games. These results describe the structure of optimal policies and provide algorithms for their calculations.\n\n               For periodic-review inventory control problems, this project described broad sufficient conditions for the existence of optimal (s,S)-policies and advanced algorithms for their computation.  For problems with convex holding costs, we described the structure of optimal policies for situations when there are no (s,S)-policies minimizing expected total discounted costs.\n\n               For Markov decision processes, the project described sufficient conditions for the validity of optimality equations for MDPs with average-costs per unit time.  For example, these conditions hold for periodic-review inventory control problems with backorders. We also described conditions under which problems with average costs per unit time can be reduced to problems with the expected total discounted costs. This project also contributed to the following four groups of problems: (i) finding algorithms for computing optimal and nearly-optimal policies, (ii) problems with multiple criteria, and (iii) continuous-time problems, and (iv) control of problems with incomplete information.  In particular, a strongly polynomial algorithm for computing approximately optimal policies was identified.  This algorithm uses the method of value iterations.  For problems with multiple criteria and side constraints, we proved that, if initial and transition probabilities do not have atoms (that is, states that can be visited with positive probabilities), then for an arbitrary policy there exists a deterministic policy with the same expected total costs. For deterministic policies, decisions are nonrandomized, and they depend only on the current states of the system. This is a generalization to multi-step problems of a classic result established by Dvoretzky, Wald and Wolfowitz about 70 years ago for one-step problems.  For continuous-time problems we proved that for an arbitrary policy there exists a policy with the same or better objective function, and this strategy uses only the information about the current state and time. Such policies are called Markov, and their existence significantly simplifies solutions of continuous-time problems.   For problems with incomplete information, we identified broad conditions for the existence of optimal policies, validity of optimality equations, and convergence of value iteration algorithms for partially observable Markov decision processes with expected total discounted costs.\n\nFor stochastic games, the project identified general sufficient conditions for the existence of optimal policies and the validity of minimax equations and the existence of optimal policies for zero-sum games with perfect information.  We have proved a new form of the minimax equation for games with infinite-state spaces and provided conditions for the existence of solutions when decision sets may not be compact.\n\nOutside of Operations Research, we established a sufficient condition for the Fatou lemma to hold in its classic form for weakly converging probabilities.  This is a contribution to two fields of mathematics: real analysis and probability theory.  This fact was used to provide general conditions for the validity of optimality equations for average-cost MDPs with infinite state spaces and possibly noncompact action sets.\n\nA graduate student, Yan Liang, completed his dissertation dealing with optimality conditions for inventory control and MDPs.  He continued his postdoctoral training at the University of Toronto. The results of the project provided problems and additional material to two graduate courses: Dynamic Programming and Probability. In addition to Yan Lang, three graduate students were investigating problems relevant to the project.   The principle investigator gave a lecture on MDPs and artificial intelligence to high school students participating in a summer research program at his university.  The PI was working on applying MDPs to medical decision making.\n\n \n\n\t\t\t\t\tLast Modified: 12/23/2020\n\n\t\t\t\t\tSubmitted by: Eugene A Feinberg"
 }
}