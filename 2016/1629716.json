{
 "awd_id": "1629716",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CI-SUSTAIN: Collaborative Research: Extending a Large Multimodal Corpus of Spontaneous Behavior for Automated Emotion Analysis",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Balakrishnan Prabhakaran",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 300955.0,
 "awd_amount": 300955.0,
 "awd_min_amd_letter_date": "2016-07-25",
 "awd_max_amd_letter_date": "2016-07-25",
 "awd_abstract_narration": "This project will extend and sustain a widely-used data infrastructure for studying human emotion, hosted at the lead investigator's university and available to the research community.  The first two versions of the dataset (BP4D and BP4D+) contain videos of people reacting to varied emotion-eliciting situations, their self-reported emotion, and expert annotations of their facial expression. Version 1, BP4D (n=41), has been used by over 100 research groups and supported a successful community competition around recognizing emotion.  The second version (BP4D+) adds participants (n = 140), thermal imaging, and measures of peripheral physiology.  The current project greatly broadens and extends this corpus to produce a new dataset (BP4D++) that enables deep-learning approaches, increases generalizability, and builds research infrastructure and community in computer and behavioral science.  The collaborators will (1) increase participant diversity; 2) add videos of pairs of people interacting to the current mix of individual and interviewer-mediated video; 3) increase the number of participants to meet the demands of recent advances in \"big data\" approaches to machine learning; and 4) expand the size and scope of annotations in the videos. They will also involve the community through an oversight and coordinating consortium that includes researchers in computer vision, biometrics, robotics, and cognitive and behavioral science. The consortium will be composed of special interest groups that focus on various aspects of the corpus, including groups responsible for completing the needed annotations, generating meta-data, and expanding the database application scope.  Having an infrastructure to support emotion recognition research matters because computer systems that interact with people (such as phone assistants or characters in virtual reality environments) will be more useful if they react appropriately to what people are doing, thinking, and feeling.  \r\n\r\nThe team will triple the number of participants in the combined corpora to 540.  They will develop a dyadic interaction task and capture data from 100 interacting dyads to support dynamic modeling of interpersonal influence across expressive behavior and physiology, as well as analysis of emotional synchrony.  They will increase the density of facial annotations to about 15 million frames in total, allowing the database to become sufficiently large to support deep-learning approaches to multimodal emotion detection. These annotations will be accomplished through a hybrid approach that combines expert coding using the Facial Action Coding System, automated face analysis, and crowdsourcing with expert input from the research community.  Finally, the recorded data will be augmented with a wide range of meta-data derived from 2D videos, 3D videos, thermal videos, and physiological signals.  To ensure the community is involved in sustaining the infrastructure, in addition to the governance consortium described above, the investigators will involve the community in jointly building both APIs that allow adding meta-data and annotations and tools to support the submission and evaluation of new recognition algorithms, then organizing community-wide competitions using those tools.  The research team will also reach out to new research communities around health computing, biometrics, and affective computing to widen the utility of the enhanced infrastructure, grow the community of expert annotators through training workshops, and build an educational community around the infrastructure that facilitates the development and sharing of course materials that use it.  Long-term, the infrastructure will be funded through a combination of commercial licensing and support from the lead university's system administration group.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Cohn",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey F Cohn",
   "pi_email_addr": "jeff@deliberate.ai",
   "nsf_id": "000211710",
   "pi_start_date": "2016-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pittsburgh",
  "inst_street_address": "4200 FIFTH AVENUE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4126247400",
  "inst_zip_code": "152600001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "MKAGLD59JRL1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pittsburgh",
  "perf_str_addr": "4327 Sennott Square",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152132303",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 300955.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 2\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p style=\"text-align: center;\"><strong>Project Outcomes Report</strong></p>\n<p>Facial expression communicates emotion, pain and physical and psychological state and contributes to the coordination of social behavior.&nbsp;&nbsp;Automatic detection and analysis of facial expression has widespread applications in psychology, child development, clinical science and practice, behavioral economics, forensics, national security, and neuroscience among other areas.&nbsp;&nbsp;To develop reliable, valid, efficient, and scalable measures of facial expression, access to well-annotated video of spontaneous facial behavior in a range of contexts is essential.&nbsp;&nbsp;The current project contributed to the final extension of a widely used database for training algorithms and for revealing connections between facial expression, emotion, and peripheral physiology. The project contributed to research infrastructure in computer and behavioral science.</p>\n<p>This final outcome report describes the database, its dissemination to and use by the research community, describes patents and publications by the University of Pittsburgh group, and reports on trainees.</p>\n<p><strong>Database.</strong></p>\n<p>In this final stage of the Binghamton-Pittsburgh Spontaneous 4D Facial Expression Database (BP4D), we completed detailed, anatomically-based annotation of occurrence and intensity of facial expression in over 200 participants beyond the original 41, added continuous annotation of perceived valence, and contributed to community support and involvement.&nbsp;&nbsp;The full database (referred to as BP4D+ and BP4D++) consists of over 200 adults that were video-recorded in 13 interpersonal tasks intended to elicit a wide range of emotion.&nbsp;&nbsp;Participants are ethnically diverse: 11% African-American, 10% Hispanic, 33% Asian, and the remainder European-American.&nbsp;&nbsp;Video resolution is high (1040 x 1392 pixels), the number of video frames exceeds one and half million, and for all participants 3D imaging and a variety of physiological measures (e.g. pulse oximeter and infra-red imaging) are available.&nbsp;&nbsp;Inter-observer reliability of the manually annotated video is good to excellent:&nbsp;&nbsp;Kappa coefficients for occurrence and intensity are 0.87 and 0.78, respectively.&nbsp;&nbsp;Intra-class correlation for effective reliability of perceived valence = 0.74.</p>\n<p><strong>Dissemination.</strong></p>\n<p>The database and specialized subsets are available from a variety of sources:&nbsp;</p>\n<ul>\n<li>Project webpage. (<a href=\"http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html\">http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html</a>)</li>\n<li>Affect Analysis Group,&nbsp;<a href=\"http://jeffcohn.net/Resources/\">http://jeffcohn.net/Resources/</a>.&nbsp;&nbsp;</li>\n<li>Facial Expression Recognition and Analysis FERA 2017) Grand Challenge&nbsp;http://sspnet.eu/fera2017/</li>\n<li>3D Face Alignment in the Wild (3DFAW) Challenge.&nbsp;<a href=\"http://mhug.disi.unitn.it/workshop/3dfaw/\">http://mhug.disi.unitn.it/workshop/3dfaw/</a></li>\n<li>3D Face Alignment in the Wild (3DFAW) &amp; Challenge Dense Reconstruction from Video (<a href=\"https://3dfaw.github.io/\">https://3dfaw.github.io/</a></li>\n</ul>\n<p><strong>Use by the Research Community</strong></p>\n<p>At last count, 436 research groups have obtained one or another version of the database and generated 1,479 citations. They have provided ongoing feedback and suggestions on data organization, meta-data correction, and coding. Some users have generated derivatives to enrich the use of the datasets, including derived data and improved results for public use.</p>\n<p><strong>Patents</strong></p>\n<p>Research by our Pittsburgh-based group with the data has resulted in two patents and two patent applications.</p>\n<ul>\n<li>De la Torre, F., Cohn, J.F. &amp; Huang, Dong (2017). ?System and Method for Processing Video to Provide Facial De-Identification.? U.S. Patent 9,799,096.&nbsp;&nbsp;A system and method for real-time image and video face de-identification that removes the identity of the subject while preserving the facial behavior is described&nbsp;</li>\n<li>Sebe, N., Alameda-Pineda, X., Tulyakov, S., Ricci, E., Yin, L, &amp; Cohn, J.F. (2019).&nbsp;&nbsp;Self-Adaptive Matrix Completion for Heart Rate Estimation from Face Videos. U.S. Patent Number 10335045B2.&nbsp;&nbsp;Contrary to previous approaches that estimate the HR by processing all the skin pixels inside a fixed region of interest, we introduce a strategy to dynamically select face regions for HR estimation.&nbsp;</li>\n<li>Patent Applications: Image Synthesis for Balanced Datasets; and Image Normalization for Facial Expression Coding.</li>\n</ul>\n<p><strong>Publications&nbsp;</strong></p>\n<p>We have published journal articles and conference papers on topics that include automatic facial action unit detection, cross-domain transfer, facial expression transfer from source to target faces, physiological correlates of emotion, and signaling of positive affect in facial expression among other topics.&nbsp;</p>\n<p><strong>Trainees</strong></p>\n<p>Trainees have included undergraduate and graduate students and postdoctoral fellows at the University of Pittsburgh, collaborative universities (University of Binghamton and Rennselaer Polytechnic Institute), the University of South Florida, Carnegie Mellon University, and visiting scholars from from Australia, China, Japan, and Italy.</p>\n<p>&nbsp;</p>\n<p>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n</div>\n</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/31/2020<br>\n\t\t\t\t\tModified by: Jeffrey&nbsp;F&nbsp;Cohn</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\nProject Outcomes Report\n\nFacial expression communicates emotion, pain and physical and psychological state and contributes to the coordination of social behavior.  Automatic detection and analysis of facial expression has widespread applications in psychology, child development, clinical science and practice, behavioral economics, forensics, national security, and neuroscience among other areas.  To develop reliable, valid, efficient, and scalable measures of facial expression, access to well-annotated video of spontaneous facial behavior in a range of contexts is essential.  The current project contributed to the final extension of a widely used database for training algorithms and for revealing connections between facial expression, emotion, and peripheral physiology. The project contributed to research infrastructure in computer and behavioral science.\n\nThis final outcome report describes the database, its dissemination to and use by the research community, describes patents and publications by the University of Pittsburgh group, and reports on trainees.\n\nDatabase.\n\nIn this final stage of the Binghamton-Pittsburgh Spontaneous 4D Facial Expression Database (BP4D), we completed detailed, anatomically-based annotation of occurrence and intensity of facial expression in over 200 participants beyond the original 41, added continuous annotation of perceived valence, and contributed to community support and involvement.  The full database (referred to as BP4D+ and BP4D++) consists of over 200 adults that were video-recorded in 13 interpersonal tasks intended to elicit a wide range of emotion.  Participants are ethnically diverse: 11% African-American, 10% Hispanic, 33% Asian, and the remainder European-American.  Video resolution is high (1040 x 1392 pixels), the number of video frames exceeds one and half million, and for all participants 3D imaging and a variety of physiological measures (e.g. pulse oximeter and infra-red imaging) are available.  Inter-observer reliability of the manually annotated video is good to excellent:  Kappa coefficients for occurrence and intensity are 0.87 and 0.78, respectively.  Intra-class correlation for effective reliability of perceived valence = 0.74.\n\nDissemination.\n\nThe database and specialized subsets are available from a variety of sources: \n\nProject webpage. (http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)\nAffect Analysis Group, http://jeffcohn.net/Resources/.  \nFacial Expression Recognition and Analysis FERA 2017) Grand Challenge http://sspnet.eu/fera2017/\n3D Face Alignment in the Wild (3DFAW) Challenge. http://mhug.disi.unitn.it/workshop/3dfaw/\n3D Face Alignment in the Wild (3DFAW) &amp; Challenge Dense Reconstruction from Video (https://3dfaw.github.io/\n\n\nUse by the Research Community\n\nAt last count, 436 research groups have obtained one or another version of the database and generated 1,479 citations. They have provided ongoing feedback and suggestions on data organization, meta-data correction, and coding. Some users have generated derivatives to enrich the use of the datasets, including derived data and improved results for public use.\n\nPatents\n\nResearch by our Pittsburgh-based group with the data has resulted in two patents and two patent applications.\n\nDe la Torre, F., Cohn, J.F. &amp; Huang, Dong (2017). ?System and Method for Processing Video to Provide Facial De-Identification.? U.S. Patent 9,799,096.  A system and method for real-time image and video face de-identification that removes the identity of the subject while preserving the facial behavior is described \nSebe, N., Alameda-Pineda, X., Tulyakov, S., Ricci, E., Yin, L, &amp; Cohn, J.F. (2019).  Self-Adaptive Matrix Completion for Heart Rate Estimation from Face Videos. U.S. Patent Number 10335045B2.  Contrary to previous approaches that estimate the HR by processing all the skin pixels inside a fixed region of interest, we introduce a strategy to dynamically select face regions for HR estimation. \nPatent Applications: Image Synthesis for Balanced Datasets; and Image Normalization for Facial Expression Coding.\n\n\nPublications \n\nWe have published journal articles and conference papers on topics that include automatic facial action unit detection, cross-domain transfer, facial expression transfer from source to target faces, physiological correlates of emotion, and signaling of positive affect in facial expression among other topics. \n\nTrainees\n\nTrainees have included undergraduate and graduate students and postdoctoral fellows at the University of Pittsburgh, collaborative universities (University of Binghamton and Rennselaer Polytechnic Institute), the University of South Florida, Carnegie Mellon University, and visiting scholars from from Australia, China, Japan, and Italy.\n\n \n\n.\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n\n\n\t\t\t\t\tLast Modified: 08/31/2020\n\n\t\t\t\t\tSubmitted by: Jeffrey F Cohn"
 }
}