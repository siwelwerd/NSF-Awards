{
 "awd_id": "1656459",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Efficient Algorithms for Uncertainty Quantification in High Dimensions",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Leland Jameson",
 "awd_eff_date": "2016-08-16",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 105833.0,
 "awd_amount": 105833.0,
 "awd_min_amd_letter_date": "2016-09-08",
 "awd_max_amd_letter_date": "2016-09-08",
 "awd_abstract_narration": "Uncertainty quantification (UQ) has become an integral part of today's scientific computing, as it is essential to the understanding of the impacts of various uncertain inputs (boundary and initial data, parameter values, geometry, etc.) to numerical predictions. UQ is thus critical to many important practical problems such as climate modeling, weather prediction, ocean dynamics, bio-chemical reactions, etc. One of the biggest challenges in UQ computations is the simulation cost, as UQ makes the traditional computations in much higher dimensional parameter spaces. For large and complex systems, the standard baseline deterministic simulations can be very time consuming, and conducing UQ simulations will further increase the simulation cost and can be prohibitively expensive. This is precisely the core issue this project intends to address and study. A novel set of highly efficient UQ algorithms will be developed to make UQ simulations amenable for large and complex systems. The new algorithms will significantly advance the current state-of-the-art of UQ methods. One prominent feature of the new algorithms is that they are designed to produce mathematically optimal UQ simulation results based on given affordable simulation capacity. While the traditional UQ methods seek to provide the smallest cost at fixed accuracy, the new methods will provide the best results at fixed affordable cost. This new feature thus makes the new algorithms ideally suited for practical UQ simulations of large and complex systems, and will have a profound impacts in various multidisciplinary fields where UQ is critical.\r\n\r\nThe core group of the new algorithms will be based on stochastic collation (SC). In order to provide optimal prediction under given and limited simulation capacity, this project is to develop a set of novel and efficient stochastic collocation methods, with a focus on high dimensional problems using very few number of samples. An important and  fundamental assumption is made: the number of the sample runs and the location of the samples are arbitrary and given by practitioners.  The goal is then to seek the best approximation in the potentially very high dimensional parameter space, based on the given samples.  The proposed research consists of three major approaches: (1) arbitrary interpolation type SC; (2) sparse regression type SC; and (3) multi-fidelity SC. In all cases, the number of high-fidelity simulations is assumed to be limited and given by the available simulation capacity, and methods are constructed to produce the best possible UQ simulations. All methods will be mathematically rigorous, as their constructions rely heavily on approximation theories in high dimensions; and also easy to implement, as they are the non-intrusive SC methods.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dongbin",
   "pi_last_name": "Xiu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dongbin Xiu",
   "pi_email_addr": "xiu.16@osu.edu",
   "nsf_id": "000483248",
   "pi_start_date": "2016-09-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8396",
   "pgm_ref_txt": "Clean Energy Technology"
  },
  {
   "pgm_ref_code": "8609",
   "pgm_ref_txt": "Other Energy Research"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 105833.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>Scientific computing has seen tremendous growth in recent years, and are moving into larger and larger scales. A centerpiece of these computing is uncertainty quantification (UQ), as it is critical to the predictability and decision-making of simulation-based sciences. UQ computations present significant simulation challenges, as all UQ simulations require many more, often significantly many more, runs than the baseline deterministic simulations.&nbsp;</span>On the other hand, the problems to be tackled by large-scale computing are becoming more and more complex and require ever larger simulation capability. For example, weather and climate modeling, power grid analysis, aircraft design and optimization, to name a few. Simulations of these problems can be so time consuming that, even with the help of the most powerful computers available, one can only afford to run a small number of high-fidelity simulations. For UQ studies, this requires the need for accurate UQ analysis using only a few realizations. Yet the extraordinary complexity of these problems often implies that there are a large number of uncertain inputs. In fact, it is not uncommon to encounter a complex problem, e.g., climate modeling, where there are many uncertain inputs, e.g., 30~50, and yet one can only afford to run O(10) simulations or even less, due to the limited simulation capacity. With such a large number of uncertain inputs and small number of simulation runs, none of the current UQ methods can deliver UQ analysis with reasonable accuracy. In fact, due to their design principle, most of the current UQ methods do not even apply in this situation, because the number of affordable simulation runs is far less than the bare minimum number of runs required by these methods.</p>\n</div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>We developed a set of new mathematical and numerical methods for large-scale UQ computations. The methods are&nbsp;</span><span>capacity-based</span><span>, in the sense that they are designed to, first and foremost, take in account the number of simulation runs allowed by simulation capacity, and then construct mathematically optimal solutions under the given (and limited) simulation capacity. The&nbsp;</span>new UQ algorithms represent a drastically different approach by constructing mathematically optimal solutions using a given number of simulation runs at arbitrarily given sample locations. We first prescribe the number of simulations allowed by the available simulation capacity&mdash;it can be a small number for large problems, and then seek to construct an effective UQ algorithms that deliver the most accurate result. By doing so, the level of accuracy is not specified prior to the simulations but is determined by the available simulation capacity. Obviously, the larger the simulation capacity the better the UQ results. The new methods are rigorous, as they are based on the use of approximation theory. On the other hand, the methods are easy to imnplement and widely applicable to many complex problems with access to data. Our methods also cover a wide range of possiblities in practice: high precision data; data with noises; data with different fidelities.</p>\n</div>\n</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/06/2018<br>\n\t\t\t\t\tModified by: Dongbin&nbsp;Xiu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\nScientific computing has seen tremendous growth in recent years, and are moving into larger and larger scales. A centerpiece of these computing is uncertainty quantification (UQ), as it is critical to the predictability and decision-making of simulation-based sciences. UQ computations present significant simulation challenges, as all UQ simulations require many more, often significantly many more, runs than the baseline deterministic simulations. On the other hand, the problems to be tackled by large-scale computing are becoming more and more complex and require ever larger simulation capability. For example, weather and climate modeling, power grid analysis, aircraft design and optimization, to name a few. Simulations of these problems can be so time consuming that, even with the help of the most powerful computers available, one can only afford to run a small number of high-fidelity simulations. For UQ studies, this requires the need for accurate UQ analysis using only a few realizations. Yet the extraordinary complexity of these problems often implies that there are a large number of uncertain inputs. In fact, it is not uncommon to encounter a complex problem, e.g., climate modeling, where there are many uncertain inputs, e.g., 30~50, and yet one can only afford to run O(10) simulations or even less, due to the limited simulation capacity. With such a large number of uncertain inputs and small number of simulation runs, none of the current UQ methods can deliver UQ analysis with reasonable accuracy. In fact, due to their design principle, most of the current UQ methods do not even apply in this situation, because the number of affordable simulation runs is far less than the bare minimum number of runs required by these methods.\n\n\n\n\n\n\n\nWe developed a set of new mathematical and numerical methods for large-scale UQ computations. The methods are capacity-based, in the sense that they are designed to, first and foremost, take in account the number of simulation runs allowed by simulation capacity, and then construct mathematically optimal solutions under the given (and limited) simulation capacity. The new UQ algorithms represent a drastically different approach by constructing mathematically optimal solutions using a given number of simulation runs at arbitrarily given sample locations. We first prescribe the number of simulations allowed by the available simulation capacity&mdash;it can be a small number for large problems, and then seek to construct an effective UQ algorithms that deliver the most accurate result. By doing so, the level of accuracy is not specified prior to the simulations but is determined by the available simulation capacity. Obviously, the larger the simulation capacity the better the UQ results. The new methods are rigorous, as they are based on the use of approximation theory. On the other hand, the methods are easy to imnplement and widely applicable to many complex problems with access to data. Our methods also cover a wide range of possiblities in practice: high precision data; data with noises; data with different fidelities.\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 12/06/2018\n\n\t\t\t\t\tSubmitted by: Dongbin Xiu"
 }
}