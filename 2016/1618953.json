{
 "awd_id": "1618953",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: Developing Golden Speakers for Second-Language Pronunciation \r\nTraining.",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 120000.0,
 "awd_amount": 120000.0,
 "awd_min_amd_letter_date": "2016-08-10",
 "awd_max_amd_letter_date": "2016-08-10",
 "awd_abstract_narration": "People who learn a second language (L2) as adults often speak with a persistent foreign accent. This can make them less intelligible, more subject to discrimination, and less confident when interacting with others. Surprisingly, though, L2 learners rarely receive formal training in pronunciation, in part because effective training must be customized to meet each learner's individual needs. To address this gap, the investigators propose to develop algorithms to synthesize a personalized \"golden speaker\" for each learner: his or her own voice but with a native accent. The rationale is that, by listening to their own golden speaker, learners can more easily perceive differences between their actual and ideal pronunciations. This work focuses on developing the technology for golden speakers, which the investigators plan to evaluate in the future as a new tool for pronunciation learning systems. As such, this research can benefit a large number of workers in the US who are non-native speakers of English, particularly in higher education, health care and the technology sector. The project also provides opportunities for graduate and undergraduate students to conduct research in a multi-disciplinary team with expertise in signal processing, machine learning, and language acquisition. \r\n\r\nTwo types of golden-speaker model are proposed. The first type is based on a reformulation of parametric statistical models for voice conversion, where instead of force-aligning source (native) and target (non-native) frames, they are matched based on their phonetic similarity. Several similarity metrics are proposed, from vocal-tract-length normalization to deep auto-encoders. The second type is based on a sparse representation of speech, which models individual frames as linear combinations of phonetic anchors. This requires new techniques to transform the constellation of anchors in the L2 speech to match the structure of native anchors (e.g., pairwise distances). Two types of evaluation are proposed for the golden-speaker models: their ability to interpolate phones not included in the learner's inventory, and the accent, intelligibility and comprehensibility of the resulting speech, as rated by native English listeners. For this purpose, the investigators propose to collect a large speech corpus from multiple Spanish and Korean learners of English and Indian speakers of English, each at different levels of English proficiency.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Levis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "John Levis",
   "pi_email_addr": "jlevis@iastate.edu",
   "nsf_id": "000710398",
   "pi_start_date": "2016-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Iowa State University",
  "inst_street_address": "1350 BEARDSHEAR HALL",
  "inst_street_address_2": "515 MORRILL ROAD",
  "inst_city_name": "AMES",
  "inst_state_code": "IA",
  "inst_state_name": "Iowa",
  "inst_phone_num": "5152945225",
  "inst_zip_code": "500112103",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IA04",
  "org_lgl_bus_name": "IOWA STATE UNIVERSITY OF SCIENCE AND TECHNOLOGY",
  "org_prnt_uei_num": "DQDBM7FGJPC5",
  "org_uei_num": "DQDBM7FGJPC5"
 },
 "perf_inst": {
  "perf_inst_name": "Iowa State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IA",
  "perf_st_name": "Iowa",
  "perf_zip_code": "500112207",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "IA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 120000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Developing Golden Speakers for second language pronunciation training</p>\n<p>The primary outcome for this project was the development of a corpus of speech from second language speakers of English. The corpus is called L2-Arctic and can be used for research involving voice conversion, accent conversion, speech synthesis and mispronunciation detection applications. L2-Arctic was developed for our own use as well as to further these goals for the wider speech community.&nbsp; For information about L2-Arctic and its structure, see <a href=\"https://psi.engr.tamu.edu/l2-arctic-corpus/\">https://psi.engr.tamu.edu/l2-arctic-corpus/</a>.</p>\n<p>The corpus is freely available to researchers around the world. Twenty-four speakers (four speakers each from six different first languages including Vietnamese, Chinese, Spanish, Arabic, Korean, and Hindi-related languages) took part in the recordings for this corpus. Each speaker read aloud the 1,132 sentences from the original Arctic database developed for native US English speech at Carnegie Mellon University or CMU (<a href=\"http://www.festvox.org/cmu_arctic/\">http://www.festvox.org/cmu_arctic/</a>). CMU Arctic originally included four speakers, two male and two female, speaking General American English using a phonetically balanced set of sentences. L2-Arctic also included two male and two female speakers from each first language. All were relatively advanced in L2 English proficiency and were able to read all sentences in sessions that took from 2.5 to 4 hours in length.</p>\n<p>The corpus recordings for each speaker resulted in approximately one hour of read speech, including orthographic and forced-aligned phonetic transcriptions. 150 utterances per speaker were phonetically annotated to identify different types of mispronunciation errors for vowel and consonant production: substitutions, deletions, additions, and distortions, making the corpus a resource for voice and accent conversion but also in computer-assisted pronunciation training. The 150 sentences for each speaker included 100 that were common for all speakers (to promote comparison across L1 backgrounds) and 50 sentences that were chosen as likely to include a greater number of L1-specific challenges for L2 English speech. In addition, the corpus includes annotated, forced-aligned recordings of spontaneous speech for each speaker using a picture narration task, The Suitcase Story (<a href=\"https://www.iris-database.org/iris/app/home/detail?id=york:822279\">https://www.iris-database.org/iris/app/home/detail?id=york:822279</a>). This part of L2-Arctic contains a smaller amount of speech, less than two minutes of speech on average for each speaker.</p>\n<p>L2-Arctic has been widely used by the research community since our publication about the corpus in 2018 (<a href=\"https://www.isca-speech.org/archive_v0/Interspeech_2018/pdfs/1110.pdf\">https://www.isca-speech.org/archive_v0/Interspeech_2018/pdfs/1110.pdf</a>), garnering 85 citations on Google Scholar. Since most spoken corpora that can be used for this kind of work are proprietary, we have also received thanks from many scholars around the world for making the corpus available because of its value in doing work on accent and voice conversion.</p>\n<p>We also used the Arabic subset of the corpus to demonstrate the value of a data-driven approach to mispronunciation detection in our 2019 paper in the journal <em>Language Teaching Research </em>(<a href=\"https://journals.sagepub.com/doi/pdf/10.1177/1362168820931888\">https://journals.sagepub.com/doi/pdf/10.1177/1362168820931888</a>). In this paper, we compared the types of errors identified by L2-Arctic and those identified by experts as typical of Arabic speakers of English. Our paper demonstrated that our data-driven approach resulted in results that often disagreed either in scope or in quantity from the priorities provided by experts.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/03/2022<br>\n\t\t\t\t\tModified by: John&nbsp;Levis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDeveloping Golden Speakers for second language pronunciation training\n\nThe primary outcome for this project was the development of a corpus of speech from second language speakers of English. The corpus is called L2-Arctic and can be used for research involving voice conversion, accent conversion, speech synthesis and mispronunciation detection applications. L2-Arctic was developed for our own use as well as to further these goals for the wider speech community.  For information about L2-Arctic and its structure, see https://psi.engr.tamu.edu/l2-arctic-corpus/.\n\nThe corpus is freely available to researchers around the world. Twenty-four speakers (four speakers each from six different first languages including Vietnamese, Chinese, Spanish, Arabic, Korean, and Hindi-related languages) took part in the recordings for this corpus. Each speaker read aloud the 1,132 sentences from the original Arctic database developed for native US English speech at Carnegie Mellon University or CMU (http://www.festvox.org/cmu_arctic/). CMU Arctic originally included four speakers, two male and two female, speaking General American English using a phonetically balanced set of sentences. L2-Arctic also included two male and two female speakers from each first language. All were relatively advanced in L2 English proficiency and were able to read all sentences in sessions that took from 2.5 to 4 hours in length.\n\nThe corpus recordings for each speaker resulted in approximately one hour of read speech, including orthographic and forced-aligned phonetic transcriptions. 150 utterances per speaker were phonetically annotated to identify different types of mispronunciation errors for vowel and consonant production: substitutions, deletions, additions, and distortions, making the corpus a resource for voice and accent conversion but also in computer-assisted pronunciation training. The 150 sentences for each speaker included 100 that were common for all speakers (to promote comparison across L1 backgrounds) and 50 sentences that were chosen as likely to include a greater number of L1-specific challenges for L2 English speech. In addition, the corpus includes annotated, forced-aligned recordings of spontaneous speech for each speaker using a picture narration task, The Suitcase Story (https://www.iris-database.org/iris/app/home/detail?id=york:822279). This part of L2-Arctic contains a smaller amount of speech, less than two minutes of speech on average for each speaker.\n\nL2-Arctic has been widely used by the research community since our publication about the corpus in 2018 (https://www.isca-speech.org/archive_v0/Interspeech_2018/pdfs/1110.pdf), garnering 85 citations on Google Scholar. Since most spoken corpora that can be used for this kind of work are proprietary, we have also received thanks from many scholars around the world for making the corpus available because of its value in doing work on accent and voice conversion.\n\nWe also used the Arabic subset of the corpus to demonstrate the value of a data-driven approach to mispronunciation detection in our 2019 paper in the journal Language Teaching Research (https://journals.sagepub.com/doi/pdf/10.1177/1362168820931888). In this paper, we compared the types of errors identified by L2-Arctic and those identified by experts as typical of Arabic speakers of English. Our paper demonstrated that our data-driven approach resulted in results that often disagreed either in scope or in quantity from the priorities provided by experts.\n\n \n\n\t\t\t\t\tLast Modified: 11/03/2022\n\n\t\t\t\t\tSubmitted by: John Levis"
 }
}