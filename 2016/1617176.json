{
 "awd_id": "1617176",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Modeling Idiosyncrasies of Speech for Automatic Spoken Language Processing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "D.  Langendoen",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 449999.0,
 "awd_amount": 449999.0,
 "awd_min_amd_letter_date": "2016-07-08",
 "awd_max_amd_letter_date": "2016-07-08",
 "awd_abstract_narration": "Spoken language encodes significant information in pitch and energy dynamics (prosody) and in disfluencies (self-edits) that human listeners use to understand a talker's meaning and the social/emotional context. Due to a lack of adequate models of these phenomena, current speech processing systems make little use of this information. This project tackles modeling limitations by focusing on unexpected speech phenomena, assuming that these events often carry the most valuable information, and by working with speech from a variety of social contexts. The work has applications that range from literacy assessment to improved human-computer interaction. Further, understanding the communicative role of different disfluencies in non-clinical speech will lead to more accurate clinical diagnoses. Educational aspects aim at broad exposure of the research methods to a diverse group of students at all academic levels through short courses, student TED talks, and work with a UW program for attracting and retaining low income students in STEM fields.\r\n\r\nThe goal of this project is to develop computational models that extract information from prosodic cues and disfluencies for use in a variety of spoken language processing applications. The approach leverages multiscale context in predictors of expected acoustic dynamics of speech in order to automatically identify regions of atypical timing or exaggeration. Specifically, it uses deep neural networks with parallel text and acoustic inputs to represent local dynamics in combination with point process models to characterize global rates of atypical events. Linguistic analyses and crowd-sourced perception studies are used to determine types of anomalies that are information bearing (vs. noise that should be ignored in language processing), leading to improved speech understanding models. Experiments make use of a variety of data sources to assess adaptation strategies and ensure generalizability of findings. Evaluation of computational models is in the context of multiple downstream applications in order to broadly explore potential contributions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Wright",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Richard A Wright",
   "pi_email_addr": "rawright@uw.edu",
   "nsf_id": "000480447",
   "pi_start_date": "2016-07-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mari",
   "pi_last_name": "Ostendorf",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mari Ostendorf",
   "pi_email_addr": "ostendor@uw.edu",
   "nsf_id": "000109813",
   "pi_start_date": "2016-07-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn Ave. NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 449999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Spoken language has two important differences compared to written language:&nbsp; speech incorporates prosodic information (how we say something), realized via modulations in pitch, energy, timing, etc., and it includes disfluencies, such as filled pauses, repetitions and self-corrections. Prosodic cues reinforce and sometimes disambiguate syntactic structure, but they are also used to indicate intent, focus of attention, and to communicate doubt, strength of conviction, sentiment, social goals, etc.&nbsp; Disfluencies and prosodic interruptions more generally, also communicate information about conversational control, social context, and speaker cognitive state. Most speech understanding systems make little use of prosodic and disfluency information, treating it as noise or simply one of many implicitly modeled sources of variability. When speech technology worked less well, systems were designed for highly constrained contexts or dealt with read speech where prosodic cues bear less information. As the technology has matured and has widespread use in a number of applications, users now expect more human-like understanding. Motivated by the well-established results showing that prosody carries information for human listeners, this work aimed to develop novel computational models of prosody that are demonstrably useful in speech understanding technologies. The key idea behind the approach is to conditionally extract prosodic features given recognized words so as to highlight seemingly anomalous or unexpected phenomena, since it is the infrequent events that typically carry the most information.</p>\n<p>The scientific contributions of the work reflected three main thrusts of the research, combining linguistic analysis with computational modeling and assessment of impact in multiple applications. &nbsp;We conducted linguistic analysis of disfluencies and atypical prosodic events in multi-party spontaneous speech to shed light on human perception of disfluencies and the co-occurrence of acoustically and linguistically surprising events in spoken language. Two computational models of prosody were introduced, leveraging novel neural network architectures that provide different mechanisms for coupling words and acoustic features, providing a means of automatically learning the timing and modulation characteristics of speech associated with different sources of information in spoken language.&nbsp; The computational models of prosody were demonstrated to improve performance of state-of-the-art systems for disfluency detection, syntactic parsing, and dialog act (intent) recognition as compared to using these systems with the speech transcripts alone.</p>\n<p>In addition to providing specific findings for the data sets of human-human conversations that the study used, the research has broader implications for speech science and technology. The combination of linguistic analysis and computational modeling provides new ways of exploring questions about human production and perception of spontaneous speech and thereby contributes to fundamental understanding of the role of prosody in human speech perception.&nbsp; For spoken language technology, the work provides an approach to integrating prosodic cues with speech recognition that can be leveraged in a variety of spoken language processing applications, since the computational models are designed to be directly integrated into the neural network machine learning frameworks that are typically used in state-of-the-art systems. Thus, they have potential impact on dialog systems, speech translation, call center data mining, video analysis and more. Human interaction with devices via voice is increasingly common, but social interaction capabilities are limited. A wider variety of capabilities will be enabled if speech technology can take better advantage of prosodic information in the speech signal. By leveraging prosodic information in understanding, spoken dialog system interactions will better match user expectations and thus be more acceptable to a broader group of people. Improved understanding of speech anomalies also has potential to impact other applications, including literacy assessment and clinical diagnostics.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/03/2021<br>\n\t\t\t\t\tModified by: Mari&nbsp;Ostendorf</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSpoken language has two important differences compared to written language:  speech incorporates prosodic information (how we say something), realized via modulations in pitch, energy, timing, etc., and it includes disfluencies, such as filled pauses, repetitions and self-corrections. Prosodic cues reinforce and sometimes disambiguate syntactic structure, but they are also used to indicate intent, focus of attention, and to communicate doubt, strength of conviction, sentiment, social goals, etc.  Disfluencies and prosodic interruptions more generally, also communicate information about conversational control, social context, and speaker cognitive state. Most speech understanding systems make little use of prosodic and disfluency information, treating it as noise or simply one of many implicitly modeled sources of variability. When speech technology worked less well, systems were designed for highly constrained contexts or dealt with read speech where prosodic cues bear less information. As the technology has matured and has widespread use in a number of applications, users now expect more human-like understanding. Motivated by the well-established results showing that prosody carries information for human listeners, this work aimed to develop novel computational models of prosody that are demonstrably useful in speech understanding technologies. The key idea behind the approach is to conditionally extract prosodic features given recognized words so as to highlight seemingly anomalous or unexpected phenomena, since it is the infrequent events that typically carry the most information.\n\nThe scientific contributions of the work reflected three main thrusts of the research, combining linguistic analysis with computational modeling and assessment of impact in multiple applications.  We conducted linguistic analysis of disfluencies and atypical prosodic events in multi-party spontaneous speech to shed light on human perception of disfluencies and the co-occurrence of acoustically and linguistically surprising events in spoken language. Two computational models of prosody were introduced, leveraging novel neural network architectures that provide different mechanisms for coupling words and acoustic features, providing a means of automatically learning the timing and modulation characteristics of speech associated with different sources of information in spoken language.  The computational models of prosody were demonstrated to improve performance of state-of-the-art systems for disfluency detection, syntactic parsing, and dialog act (intent) recognition as compared to using these systems with the speech transcripts alone.\n\nIn addition to providing specific findings for the data sets of human-human conversations that the study used, the research has broader implications for speech science and technology. The combination of linguistic analysis and computational modeling provides new ways of exploring questions about human production and perception of spontaneous speech and thereby contributes to fundamental understanding of the role of prosody in human speech perception.  For spoken language technology, the work provides an approach to integrating prosodic cues with speech recognition that can be leveraged in a variety of spoken language processing applications, since the computational models are designed to be directly integrated into the neural network machine learning frameworks that are typically used in state-of-the-art systems. Thus, they have potential impact on dialog systems, speech translation, call center data mining, video analysis and more. Human interaction with devices via voice is increasingly common, but social interaction capabilities are limited. A wider variety of capabilities will be enabled if speech technology can take better advantage of prosodic information in the speech signal. By leveraging prosodic information in understanding, spoken dialog system interactions will better match user expectations and thus be more acceptable to a broader group of people. Improved understanding of speech anomalies also has potential to impact other applications, including literacy assessment and clinical diagnostics.\n\n \n\n\t\t\t\t\tLast Modified: 02/03/2021\n\n\t\t\t\t\tSubmitted by: Mari Ostendorf"
 }
}