{
 "awd_id": "1618336",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Using Automatically Generated Paraphrases and Discriminative ASR Training to Author Robust Question-Answering Dialogue Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 458000.0,
 "awd_min_amd_letter_date": "2016-07-13",
 "awd_max_amd_letter_date": "2021-08-18",
 "awd_abstract_narration": "Question-answering (QA) dialogue systems are useful in a broad range of contexts where the primary role of a virtual character is to answer questions posed by the human user. In QA-based dialogue systems, the primary interpretation task can be framed as matching a user's question against a set of questions anticipated by the content author.  This project investigates for the first time how methods for automatically paraphrasing anticipated questions can aid authors in establishing a large set of expected question variants, making it possible to dramatically enhance interpretation robustness for both chatted and spoken language.  By evaluating the project with an existing virtual patient dialogue system and new virtual guide for Columbus, Ohio's COSI (Center of Science and Industry) science museum, the project will enhance medical education and provide an inspirational example of science in action to the children who attend the museum.  It also promises to enhance the effectiveness of short answer scoring in educational software and commercial QA systems for frequently asked questions.\r\n\r\n\r\nThe proposed approach is the first to explore the potential of advanced automatic paraphrasing techniques to enhance the robustness of interpretation in an easy-to-author QA dialogue system.  By employing paraphrasing at content authoring time, it becomes possible to take advantage of and make explicit the author's knowledge of the space of functionally equivalent questions, potentially leading to dramatic improvements in interpretation accuracy; furthermore, doing so makes it possible to set up an effective, task-relevant discrimination space for Automatic Speech Recognition (ASR) training.  To generate paraphrases, the project uses the grammar-based surface realizer OpenCCG for lexico-syntactic alternations together with broad coverage resources, vector space models of word meaning and multiword alignments. To train discriminative ASR models that make a difference in question interpretation, generated paraphrases are incorporated into the semantic error rate estimation.  Using data collected from medical students and museum visitors, the project assesses the approach via its impact on interpretation accuracy and qualitative measures of usability.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "White",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael White",
   "pi_email_addr": "mwhite@ling.ohio-state.edu",
   "nsf_id": "000069401",
   "pi_start_date": "2016-07-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Fosler-Lussier",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eric Fosler-Lussier",
   "pi_email_addr": "fosler@cse.ohio-state.edu",
   "nsf_id": "000182577",
   "pi_start_date": "2016-07-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 450000.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Building systems that interact using language has typically been a time-intensive activity that requires a lot of engineering and domain expertise.&nbsp; This project developed technologies that make is easier for domain experts to rapidly create spoken language systems by making systems more robust to variation in the ways that people say things and to errors made by cloud-based speech-to-text services. The testbed for the research was a Virtual Patient system developed to train medical students at The Ohio State University (Figure 1).&nbsp; This system is able to respond to questions that doctors might ask when taking a medical history and gathering information to perform a differential diagnosis.&nbsp; As a final proof of concept, a museum guide avatar was rapidly developed and deployed at the Center of Science and Industry (COSI) science museum, which also provided outreach to the public.</p>\n<p>The project outcomes relating to the project's intellectual merits include a series of new software innovations that allow conversational systems to better interpret a wider set of questions. &nbsp;One of the primary challenges within both the Virtual Patient and COSI domains is that there are many questions that can be asked, but while some of them are common, most may only have a few (or only one) example questions (Figure 2).&nbsp; To address this challenge, we pursued multiple data augmentation strategies in order to make&nbsp;recognition of these rare questions much more accurate without hurting performance on common questions.&nbsp;</p>\n<p>The most effective approach turned out to be a new \"pairwise mix-up\" interpretation system.&nbsp; The key idea is that training on interpolations between nearest-neighbor questions in a semantic space can help the system understand the space of questions better, even for rare classes.&nbsp; In another line of research, different models for paraphrasing questions were explored in order to help improve recognition of rare questions. &nbsp;The project explored a number of paraphrasing models, including both new models and existing large-language model paraphrasers. &nbsp;Interestingly, paraphrasing was quite effective for earlier attention-based neural understanding models, but less effective when used together with the pairwise mix-up model, indicating that there is some connection between mix-up interpolation and paraphrasing that needs further study.</p>\n<p>In another line of data augmentation research, the project sought to increase robustness to errors made by speech recognition systems by characterizing potential errors that speech recognizers might make, and using that to synthesize new errorful transcripts. &nbsp;The key outcome of this line of research was a new neural model that uses both word and phone confusion representations that significantly improves the abiltiy to characterize speech recognition system errors over previous confusion models. &nbsp;Utilizing synthesized transcripts to augment spoken language understanding training sets had a small but consistent improvement in spoken language understanding for the Virtual Patient task.</p>\n<p>To gather data to train understanding models, we took advantage of an existing rule-based, pattern matching implementation of the Virtual Patient to drive interactions with medical student users.&nbsp; To our surprise, the rule-based approach outperformed the machine learned models during much of the project, particulary on the least frequent questions.&nbsp; Consequently, we developed a novel hybrid approach to question interpretation by training a classifier to choose between the rule-based and machine learned outputs.&nbsp; This method achieved dramatic error reductions with our early neural understanding models and continued to outperform either component method even as our neural models improved, helping to achieve a gain in accuracy from under 80% to over 90% during the project.</p>\n<p>The project outcomes relating to the project's broader impacts have three components. &nbsp;First, the project was able to directly impact the education of medical students at the Ohio State University. &nbsp;More than 800 students were trained to take medical histories using the Virtual Patient system, allowing them to practice these skills prior to taking an evaluation exam using a human standardized patient.&nbsp;&nbsp;</p>\n<p>Second, development of the COSI museum avatar served both as community outreach and a launchpad for student-driven experiments. &nbsp;The team partnered with the Language Pod at COSI, OSU's language science outreach arm, to showcase and test the technology developed in the project. More than 160 users have utilized the COSI avatar during the testing period, with participants reporting that they enjoying the experience. Surveys indicated that participants also reported an increased interest in learning more about language science.</p>\n<p>Finally, the project has resulted in both software and new research ideas that can impact the direction of conversational AI research. &nbsp;The project explored an underresearched class of dialog agent problems: systems with long-tailed question distributions. The demonstration of both new understanding models that better handle long-tailed distribution as well as the exploration of techniques to introduce phonetic and textual variations as data augmentation can not only impact research directions in conversational AI but also be useful for understanding how systems can accept language input from a broader range of users.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/08/2022<br>\n\t\t\t\t\tModified by: Eric&nbsp;Fosler-Lussier</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1618336/1618336_10440077_1659793639019_avatar-anon--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1618336/1618336_10440077_1659793639019_avatar-anon--rgov-800width.jpg\" title=\"Screenshot of Virtual Patient System\"><img src=\"/por/images/Reports/POR/2022/1618336/1618336_10440077_1659793639019_avatar-anon--rgov-66x44.jpg\" alt=\"Screenshot of Virtual Patient System\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Screenshot of the Virtual Patient System, allowing medical students to practice patient interviews.  This version shows chatted (typed) input and output; subsequent versions used spoken language.  After the interview, students can obtain immediate, validated feedback on their questions.</div>\n<div class=\"imageCredit\">Jin et al. (2018), \"Using Paraphrasing and Memory-Augmented Models to Combat Data Sparsity in Question Interpretation with a Virtual Patient Dialogue System\"</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Michael&nbsp;White</div>\n<div class=\"imageTitle\">Screenshot of Virtual Patient System</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1618336/1618336_10440077_1659793986134_zipf_labels--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1618336/1618336_10440077_1659793986134_zipf_labels--rgov-800width.jpg\" title=\"Frequency of Class Labels with Quintiles\"><img src=\"/por/images/Reports/POR/2022/1618336/1618336_10440077_1659793986134_zipf_labels--rgov-66x44.jpg\" alt=\"Frequency of Class Labels with Quintiles\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Class label frequency distribution in the virtual patient dataset. The distribution of question class labels is very long-tailed, with few frequent labels and many infrequent labels. Quintiles are shown in different colors.</div>\n<div class=\"imageCredit\">Jin et al. (2018), \"Using Paraphrasing and Memory-Augmented Models to Combat Data Sparsity in Question Interpretation with a Virtual Patient Dialogue System\"</div>\n<div class=\"imageSubmitted\">Michael&nbsp;White</div>\n<div class=\"imageTitle\">Frequency of Class Labels with Quintiles</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nBuilding systems that interact using language has typically been a time-intensive activity that requires a lot of engineering and domain expertise.  This project developed technologies that make is easier for domain experts to rapidly create spoken language systems by making systems more robust to variation in the ways that people say things and to errors made by cloud-based speech-to-text services. The testbed for the research was a Virtual Patient system developed to train medical students at The Ohio State University (Figure 1).  This system is able to respond to questions that doctors might ask when taking a medical history and gathering information to perform a differential diagnosis.  As a final proof of concept, a museum guide avatar was rapidly developed and deployed at the Center of Science and Industry (COSI) science museum, which also provided outreach to the public.\n\nThe project outcomes relating to the project's intellectual merits include a series of new software innovations that allow conversational systems to better interpret a wider set of questions.  One of the primary challenges within both the Virtual Patient and COSI domains is that there are many questions that can be asked, but while some of them are common, most may only have a few (or only one) example questions (Figure 2).  To address this challenge, we pursued multiple data augmentation strategies in order to make recognition of these rare questions much more accurate without hurting performance on common questions. \n\nThe most effective approach turned out to be a new \"pairwise mix-up\" interpretation system.  The key idea is that training on interpolations between nearest-neighbor questions in a semantic space can help the system understand the space of questions better, even for rare classes.  In another line of research, different models for paraphrasing questions were explored in order to help improve recognition of rare questions.  The project explored a number of paraphrasing models, including both new models and existing large-language model paraphrasers.  Interestingly, paraphrasing was quite effective for earlier attention-based neural understanding models, but less effective when used together with the pairwise mix-up model, indicating that there is some connection between mix-up interpolation and paraphrasing that needs further study.\n\nIn another line of data augmentation research, the project sought to increase robustness to errors made by speech recognition systems by characterizing potential errors that speech recognizers might make, and using that to synthesize new errorful transcripts.  The key outcome of this line of research was a new neural model that uses both word and phone confusion representations that significantly improves the abiltiy to characterize speech recognition system errors over previous confusion models.  Utilizing synthesized transcripts to augment spoken language understanding training sets had a small but consistent improvement in spoken language understanding for the Virtual Patient task.\n\nTo gather data to train understanding models, we took advantage of an existing rule-based, pattern matching implementation of the Virtual Patient to drive interactions with medical student users.  To our surprise, the rule-based approach outperformed the machine learned models during much of the project, particulary on the least frequent questions.  Consequently, we developed a novel hybrid approach to question interpretation by training a classifier to choose between the rule-based and machine learned outputs.  This method achieved dramatic error reductions with our early neural understanding models and continued to outperform either component method even as our neural models improved, helping to achieve a gain in accuracy from under 80% to over 90% during the project.\n\nThe project outcomes relating to the project's broader impacts have three components.  First, the project was able to directly impact the education of medical students at the Ohio State University.  More than 800 students were trained to take medical histories using the Virtual Patient system, allowing them to practice these skills prior to taking an evaluation exam using a human standardized patient.  \n\nSecond, development of the COSI museum avatar served both as community outreach and a launchpad for student-driven experiments.  The team partnered with the Language Pod at COSI, OSU's language science outreach arm, to showcase and test the technology developed in the project. More than 160 users have utilized the COSI avatar during the testing period, with participants reporting that they enjoying the experience. Surveys indicated that participants also reported an increased interest in learning more about language science.\n\nFinally, the project has resulted in both software and new research ideas that can impact the direction of conversational AI research.  The project explored an underresearched class of dialog agent problems: systems with long-tailed question distributions. The demonstration of both new understanding models that better handle long-tailed distribution as well as the exploration of techniques to introduce phonetic and textual variations as data augmentation can not only impact research directions in conversational AI but also be useful for understanding how systems can accept language input from a broader range of users.\n\n\t\t\t\t\tLast Modified: 08/08/2022\n\n\t\t\t\t\tSubmitted by: Eric Fosler-Lussier"
 }
}