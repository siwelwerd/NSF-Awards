{
 "awd_id": "1553086",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: The optimal use of data",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2016-02-15",
 "awd_exp_date": "2021-01-31",
 "tot_intn_awd_amt": 497033.0,
 "awd_amount": 497033.0,
 "awd_min_amd_letter_date": "2016-02-16",
 "awd_max_amd_letter_date": "2020-03-25",
 "awd_abstract_narration": "Modern techniques for data gathering?arising from medicine and bioinformatics, internet applications such as web-search, physics and astronomy, mobile data gathering platforms?have yielded an explosion in the mass and diversity of data. Concurrently, statistics, decision theory, and machine learning have successfully laid a groundwork for answering questions about our world based on analysis of this data. As more information is collected, classical approaches for inference and learning are insufficient, as additional concerns arise?computational resources, privacy considerations, storage limitations, network communication constraints? outside of statistical accuracy. This prompts a basic question: how can multiple criteria be balanced while maintaining statistical performance?\r\n\r\nTo bring statistics and machine learning into closer contact with other desiderata, this research involves the development of procedures that trade between scarce resources in principled and optimal ways. Such trade-offs have been difficult to characterize, as current tools for providing fundamental limits (such as information theory in communication) do not connect disparate areas. Three concrete sub-areas serve as bases for this research. The investigators study the interplay of computing with learning, estimation, and optimization by connecting notions of computation?such as memory accesses or synchronization in distributed systems?to data analysis tasks. Second, the research investigates adaptive and robust procedures?and associated statistical costs?that will become more important given increasingly long-tailed and messy data. Thirdly, the investigators study privacy in estimation, using information and decision-theoretic tools to characterize the tensions between statistical accuracy and sensitive data disclosures. Combined, these lay the groundwork for a theory on the use of data in the face of constraints, along with a functional and practical understanding of procedures that balance scarce resources against statistical accuracy.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Duchi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "John Duchi",
   "pi_email_addr": "jduchi@stanford.edu",
   "nsf_id": "000697614",
   "pi_start_date": "2016-02-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943054020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 190703.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 99320.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 102080.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 104930.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this award, PI Duchi and his students developed methods for addressing many of the major challenges in modern theoretical statistics, data science, and machine learning. While much of the current theory of statistical learning focuses on optimality results--how much data does one need to derive various conclusions from a study, or to estimate a particular quantity of interest--little addresses additional but important desiderata, such as the privacy of participants in a study, or the computational and communication footprints of machine learning methods, which can use megawatt hours in their training and cost millions of dollars to fit.</p>\n<p>Thus, the main research in the award focused on developing methods and methodology to include such constraints on estimators. This included a two-pronged approach, with optimality theory (lower bounds) on the one hand and new methodology on the other.</p>\n<p>A few highlights include a sequence of papers on optimality results for nonlinear optimization, which (for the first time) showed algorithm-independent results for optimal rates of convergence in non-convex optimization, allowing application to modern problems in deep learning. Other results focused on privacy, including a theory of optimality for private algorithms that began a research agenda on instance optimal results--the difficulty of specific problems--as opposed to worst-case results, whose conservativeness makes them unusable.</p>\n<p>On the methods side, PI Duchi and collaborators developed new methods for optimization that are more robust, in that they work (well) for broader classes of optimization problems than previously possible, reducing (by an order of magnitude or so) the amount of work required to fit several large-scale models.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/03/2022<br>\n\t\t\t\t\tModified by: John&nbsp;Duchi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this award, PI Duchi and his students developed methods for addressing many of the major challenges in modern theoretical statistics, data science, and machine learning. While much of the current theory of statistical learning focuses on optimality results--how much data does one need to derive various conclusions from a study, or to estimate a particular quantity of interest--little addresses additional but important desiderata, such as the privacy of participants in a study, or the computational and communication footprints of machine learning methods, which can use megawatt hours in their training and cost millions of dollars to fit.\n\nThus, the main research in the award focused on developing methods and methodology to include such constraints on estimators. This included a two-pronged approach, with optimality theory (lower bounds) on the one hand and new methodology on the other.\n\nA few highlights include a sequence of papers on optimality results for nonlinear optimization, which (for the first time) showed algorithm-independent results for optimal rates of convergence in non-convex optimization, allowing application to modern problems in deep learning. Other results focused on privacy, including a theory of optimality for private algorithms that began a research agenda on instance optimal results--the difficulty of specific problems--as opposed to worst-case results, whose conservativeness makes them unusable.\n\nOn the methods side, PI Duchi and collaborators developed new methods for optimization that are more robust, in that they work (well) for broader classes of optimization problems than previously possible, reducing (by an order of magnitude or so) the amount of work required to fit several large-scale models.\n\n\t\t\t\t\tLast Modified: 11/03/2022\n\n\t\t\t\t\tSubmitted by: John Duchi"
 }
}