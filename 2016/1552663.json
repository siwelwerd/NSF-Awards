{
 "awd_id": "1552663",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Modeling User Touch and Motion Behaviors for Adaptive Interfaces in Mobile Devices",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2016-02-01",
 "awd_exp_date": "2023-01-31",
 "tot_intn_awd_amt": 501235.0,
 "awd_amount": 517235.0,
 "awd_min_amd_letter_date": "2016-01-29",
 "awd_max_amd_letter_date": "2020-04-15",
 "awd_abstract_narration": "In this project the PI will investigate how to apply mobile interaction data to automatically improve the usability and accessibility of mobile user interfaces.  The research will identify user abilities based on their behaviors, leading to mobile user interfaces that are more accessible to diverse user communities (e.g., veterans), in a variety of environments (including cases of situation-induced impairments).  In particular, the PI will explore the challenges of data-driven adaptive interface layouts based on user behavior and visual attention in mobile computing when the user is actually mobile.  The work will involve student researchers from under-represented groups currently advised by the PI, and will be evaluated by different populations engaged in realistic but varied activities.  This will allow the PI to release software and research findings that enable people to design interfaces that properly adapt to the abilities of members of the target communities.  The research ties directly into an educational plan to develop a student response tool for lecture-style user interface courses, that allows students to create wireframe interfaces, to design typefaces, and to draw visualizations during class on touchscreen and mobile devices, which can be displayed on the room screen for discussion and peer feedback.  The tool will be iteratively refined in a user interface course with a diverse student population, and deployed in a course at the PI's institution outside of computer science as well as in user interface courses at other universities.\r\n\r\n User interfaces on mobile devices are not one-size-fits-all, nor should they be.  Users' abilities may differ, or the situational context of the environment can introduce new challenges.  By their very nature, mobile devices are used in many different environments and with different postures.  For example, users may hold their tablet in both hands with the screen in landscape orientation to read in bed, swiping to different pages occasionally; at other times, they may be pushing a stroller while gripping their phone with one hand to navigate a map application.  Because manufacturers know this, smartphones and tablets, unlike desktop computers, can accept touch input and sense both motion and orientation, and data from these interactions can be captured by websites and apps to identify specific user abilities and context.  Over time, user interaction data collected at scale will enable personalization of the interface, say by reshaping touch targets to compensate for a user's habit of typically tapping to the right of a target, by relocating important buttons to more accessible locations on the screen, or by determining ideal text size by noting the zoom level a user often applies.  Thus, the work will comprise three research objectives.  The first objective is to investigate how to passively capture touch and motion data from mobile devices, to compute metrics representing user habits and mistakes as they perform touch interactions, and to determine the environmental context of the user from motion and touch behaviors.  The second objective is to incorporate orientation and touch sensors to train an eye tracking model using the front-facing camera to detect the user's attention.  The third objective, informed by findings from the first two, is to improve the usability and accessibility of existing interfaces, e.g., by adjusting the hittable area of targets, the text size, and interface layout.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeff",
   "pi_last_name": "Huang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jeff Huang",
   "pi_email_addr": "Jeff_Huang@brown.edu",
   "nsf_id": "000651625",
   "pi_start_date": "2016-01-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "Office of Sponsored Projects",
  "perf_city_name": "Providence",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129093",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 89859.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 96972.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 104194.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 103407.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 122803.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of the proposed work was to show how touch interactions and motion behavior can be used to automatically adapt the interface and interactions on mobile devices for usability and accessibility. A series of investigations explored different systems and methods for accomplishing this, disseminated through academic publications, university educational infrastructure, open source research artifacts, and in collaborative industry and academic technologies.One resulting publication from the work was \"Remotion: A Motion-Based Capture and Replay Platform of Mobile Device Interaction for Remote Usability Testing\" which investigated a system for inferring the user?s attention, emotional state, habits, and active hand posture based on mobile motion behaviors. These interactions were extended into the augmented reality domain, by using the camera on mobile devices for hand tracking in two papers, \"Portal-ble: Intuitive Free-Hand Manipulation in Unbounded Smartphone-based Augmented Reality\" where behavioral accommodations were automatically generated to infer what the user intended to do even when the behavior didn?t quite align with the intention, and \"FocalPoint: Adaptive Direct Manipulation for Selecting Small 3D Virtual Objects\" which expanded the accommodations to smaller virtual objects. In another approach, we examined textual adaptations, using eye tracking to infer user attention while typing in \"The Eye of the Typer: A Benchmark and Analysis of Gaze Behavior during Typing,\" and by investigating the effect of personalizing fonts in \"Towards Individuated Reading Experiences: Different Fonts Increase Reading Speed for Different Individuals.\" As proposed, webcam eye tracking was an approach used to infer attention and to personalized the interface for the user. This technique was determined to be comparable to stand-alone commercial eye-tracking devices in web search, in \"SearchGazer: Webcam Eye Tracking for Remote Studies of Web Search.\"As part of the educational goals, the integration of research and classroom deployments produced two systems, Sketchy and UX Factor, which were collaborative inspirational and evaluation systems, used across multiple classes of user interface and user experiences, as well as classes at other universities (Pomona College, NJIT, Wayne State University) and at the Adobe Jam in Japan. \"Sketchy: Drawing Inspiration from the Crowd\" was a paper describing the results of allowing virtual peeking by students at other students? sketches in real-live as they were engaged in an in-class activity. \"The UX Factor: Using Comparative Peer Review to Evaluate Designs through User Preferences\" was a double award-winning paper describing a process of using comparative peer review to evaluate user interface designs.The work produced from the award led to collaborations with Adobe Research and Snap Inc., specifically on examining the applicability of the adaptive interactions in next-generation augmented reality interfaces. One resulting publication was \"Dually Noted: Layout-Aware Annotations with Smartphone Augmented Reality\" for allowing annotations to cross between physical and virtual readings. An example application demonstrated these techniques on the Snap Spectacles, a pair of augmented reality glasses with limited release as an experimental technology.In total, 14 publications resulted from the funded work, appearing at reputable venues such as CHI, UIST, IMWUT, CSCW, IJCAI, DIS, the ToCHI journal, ETRA, and CHIIR. Papers were recognized with three different awards, a Best Paper Finalist, an Honorable Mention award, and an Impact Recognition award.The funded work resulted in software artifacts deployed to the public and in wide use. WebGazer was partly funded through this award, and has 500 forks on GitHub (instances of people building off the work), over 3,300 stars (people bookmarking the software package), and is a key library in a widely-used experimental psychology library jsPsych, and used as a core technology in multiple startup components. Portalble is an augmented reality library on mobile devices and is open source, serving as the foundation for hand tracking interactions for multiple applications. The educational tools, Sketchy and UX Factor have each been publicly hosted for 4 years, and allow other educators to use them in their own classrooms.Funding from the award substantially supported Ph.D. students who have taken positions in academia, one as a research assistant professor at New York University, and another as an assistant professor at the University of Rhode Island. Additionally, through the main grant as well as the REU supplement, undergraduate students participating in the research used their experiences to join Ph.D. programs at Stanford, Dartmouth, Carnegie Mellon University, Georgia Tech, New York University, the University of California Davis, and the University of Washington; half of these students were from underrepresented backgrounds.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/24/2023<br>\n\t\t\t\t\tModified by: Jeff&nbsp;Huang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of the proposed work was to show how touch interactions and motion behavior can be used to automatically adapt the interface and interactions on mobile devices for usability and accessibility. A series of investigations explored different systems and methods for accomplishing this, disseminated through academic publications, university educational infrastructure, open source research artifacts, and in collaborative industry and academic technologies.One resulting publication from the work was \"Remotion: A Motion-Based Capture and Replay Platform of Mobile Device Interaction for Remote Usability Testing\" which investigated a system for inferring the user?s attention, emotional state, habits, and active hand posture based on mobile motion behaviors. These interactions were extended into the augmented reality domain, by using the camera on mobile devices for hand tracking in two papers, \"Portal-ble: Intuitive Free-Hand Manipulation in Unbounded Smartphone-based Augmented Reality\" where behavioral accommodations were automatically generated to infer what the user intended to do even when the behavior didn?t quite align with the intention, and \"FocalPoint: Adaptive Direct Manipulation for Selecting Small 3D Virtual Objects\" which expanded the accommodations to smaller virtual objects. In another approach, we examined textual adaptations, using eye tracking to infer user attention while typing in \"The Eye of the Typer: A Benchmark and Analysis of Gaze Behavior during Typing,\" and by investigating the effect of personalizing fonts in \"Towards Individuated Reading Experiences: Different Fonts Increase Reading Speed for Different Individuals.\" As proposed, webcam eye tracking was an approach used to infer attention and to personalized the interface for the user. This technique was determined to be comparable to stand-alone commercial eye-tracking devices in web search, in \"SearchGazer: Webcam Eye Tracking for Remote Studies of Web Search.\"As part of the educational goals, the integration of research and classroom deployments produced two systems, Sketchy and UX Factor, which were collaborative inspirational and evaluation systems, used across multiple classes of user interface and user experiences, as well as classes at other universities (Pomona College, NJIT, Wayne State University) and at the Adobe Jam in Japan. \"Sketchy: Drawing Inspiration from the Crowd\" was a paper describing the results of allowing virtual peeking by students at other students? sketches in real-live as they were engaged in an in-class activity. \"The UX Factor: Using Comparative Peer Review to Evaluate Designs through User Preferences\" was a double award-winning paper describing a process of using comparative peer review to evaluate user interface designs.The work produced from the award led to collaborations with Adobe Research and Snap Inc., specifically on examining the applicability of the adaptive interactions in next-generation augmented reality interfaces. One resulting publication was \"Dually Noted: Layout-Aware Annotations with Smartphone Augmented Reality\" for allowing annotations to cross between physical and virtual readings. An example application demonstrated these techniques on the Snap Spectacles, a pair of augmented reality glasses with limited release as an experimental technology.In total, 14 publications resulted from the funded work, appearing at reputable venues such as CHI, UIST, IMWUT, CSCW, IJCAI, DIS, the ToCHI journal, ETRA, and CHIIR. Papers were recognized with three different awards, a Best Paper Finalist, an Honorable Mention award, and an Impact Recognition award.The funded work resulted in software artifacts deployed to the public and in wide use. WebGazer was partly funded through this award, and has 500 forks on GitHub (instances of people building off the work), over 3,300 stars (people bookmarking the software package), and is a key library in a widely-used experimental psychology library jsPsych, and used as a core technology in multiple startup components. Portalble is an augmented reality library on mobile devices and is open source, serving as the foundation for hand tracking interactions for multiple applications. The educational tools, Sketchy and UX Factor have each been publicly hosted for 4 years, and allow other educators to use them in their own classrooms.Funding from the award substantially supported Ph.D. students who have taken positions in academia, one as a research assistant professor at New York University, and another as an assistant professor at the University of Rhode Island. Additionally, through the main grant as well as the REU supplement, undergraduate students participating in the research used their experiences to join Ph.D. programs at Stanford, Dartmouth, Carnegie Mellon University, Georgia Tech, New York University, the University of California Davis, and the University of Washington; half of these students were from underrepresented backgrounds.\n\n\t\t\t\t\tLast Modified: 05/24/2023\n\n\t\t\t\t\tSubmitted by: Jeff Huang"
 }
}