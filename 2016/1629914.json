{
 "awd_id": "1629914",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "II-New: Infrastructure for Supporting Biomedical Application Algorithms, Runtime Development and Resource Management",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2016-07-20",
 "awd_max_amd_letter_date": "2016-07-20",
 "awd_abstract_narration": "The research supported by the funded computing infrastructure focuses on biomedical applications that must (i) process images or graph data of enormous size, (ii) handle data with noise and deformation, and (iii) require tremendous computational capability. One example problem being investigated is studying breast cancer brain metastasis using large 3D microscopy fluorescence images. Such biomedical applications can benefit greatly from both appropriate heterogeneous computing infrastructure and concerted effort spanning from algorithm development to effective mapping of algorithms to the computing infrastructure. Furthermore, the infrastructure will provide a platform for implementing a CS training plan of sufficient rigor for faculty, postdoctoral researchers and students to share with peer institutions, and will be integrated into multiple existing outreach programs to underrepresented minority, high school teachers and students such as the Notre Dame Summer Scholars Research Computing Track and the NSF REU Site program in Computational Science.\r\n\u00a0\r\nA top-down, vertically integrated theme is adopted in the proposed research effort. Successful completion of the work will advance computational approaches for tackling fundamental problems in treating cancer and other diseases, and for discovering new classes of algorithms that mimic the sensory processing and reasoning abilities of biological systems. The effort will also offer powerful visualization and graph analysis techniques for processing and making inferences from large and complex image/graph data, which are essential for many biomedical and other applications. Finally, it will lead to the development of application-driven resource management mechanisms that are indispensable for effectively exploiting any GPU-based heterogeneous computing infrastructure.\u00a0",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Chaoli",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chaoli Wang",
   "pi_email_addr": "chaoli.wang@nd.edu",
   "nsf_id": "000510655",
   "pi_start_date": "2016-07-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiaobo",
   "pi_last_name": "Hu",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaobo S Hu",
   "pi_email_addr": "xhu@nsf.gov",
   "nsf_id": "000166343",
   "pi_start_date": "2016-07-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Danny",
   "pi_last_name": "Chen",
   "pi_mid_init": "Z",
   "pi_sufx_name": "",
   "pi_full_name": "Danny Z Chen",
   "pi_email_addr": "dchen@nd.edu",
   "nsf_id": "000291339",
   "pi_start_date": "2016-07-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Nitesh",
   "pi_last_name": "Chawla",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nitesh Chawla",
   "pi_email_addr": "nchawla@nd.edu",
   "nsf_id": "000484327",
   "pi_start_date": "2016-07-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Walter",
   "pi_last_name": "Scheirer",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Walter J Scheirer",
   "pi_email_addr": "wscheire@nd.edu",
   "nsf_id": "000590702",
   "pi_start_date": "2016-07-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Notre Dame",
  "inst_street_address": "940 GRACE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "NOTRE DAME",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "5746317432",
  "inst_zip_code": "465565708",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "IN02",
  "org_lgl_bus_name": "UNIVERSITY OF NOTRE DAME DU LAC",
  "org_prnt_uei_num": "FPU6XGFXMBE9",
  "org_uei_num": "FPU6XGFXMBE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Notre Dame",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "465565708",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "IN02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"normal\">This NSF funding was used to build a heterogeneous, high-performance computing cluster. The acquisition received partial some supplemental funds from the Computer Science and Engineering department and Center for Research Computing at the University of Notre Dame. The cluster has provided critical computing supports to 24 postdoctoral fellows and graduate students as well as 4 undergraduate students. It has served an enabler to a top-down, vertically integrated research theme spanning from application-driven algorithm development to resource management for heterogeneous computing platforms. The research enabled by the cluster is grouped into four tiers and the associated most significant findings/outcomes are summarized below.</p>\n<p class=\"normal\"><strong>Application-Driven Algorithm Development:</strong> This tier aims to develop new machine learning (e.g., deep learning) approaches for medical, biomedical, and personal healthcare applications. Biomedical data, when represented as networks or graphs, inherently have multiple modalities. Imagine a disease network. The diseases can share links based on shared risk factors, co-occurrence patterns, or even shared genetic representation. We have developed representation learning based approaches, inspired by deep learning or matrix factorization. Extensive experiments show that our proposed approaches not only outperform state-of-the-art methods in various heterogeneous network-mining tasks, such as node classification, clustering, and similarity search, but also discern the structural and semantic correlations between diverse network objects.</p>\n<p class=\"normal\">Due to the diversity and complexity of biomedical image data, manually annotating large amounts of biomedical image data for training common deep learning models for biomedical applications is very time consuming and labor intensive. Biomedical experts are commonly involved in a long and iterative process of annotation, as in active learning type annotation schemes. We have proposed representative annotation (RA), a new deep learning framework for considerably reducing annotation effort for biomedical image segmentation. Our RA scheme leverages the ability of deep neural networks to learn better representations of image data and frees annotators from the iterative process of common active learning based annotation schemes. Evaluation results based on both 2D and 3D datasets show that RA yields competitive segmentation results in comparison with state-of-the-art methods.</p>\n<p class=\"normal\"><strong>Supporting Methods and Tools:</strong> The objective of this tier is to design and develop a suite of general techniques to support application-level algorithms and tools. We have focused specifically on visualization techniques. As one key outcome, we pioneered a data augmentation framework for scientific data analysis and visualization. Data augmentation in this context refers to the addition of spatial, temporal, and variable details to reduced simulation data by incorporating information derived from internal and external sources. The general approach developed could be transformative in that it will substantially improve the ability of visualization researchers and domain scientists to make the best use of limited simulation data output, which would energize visualization research and facilitate scientific discovery.</p>\n<p class=\"normal\"><strong>System and Runtime Support:</strong> This tier focuses on methods enabling the application-driven algorithms, supporting methods and tools developed in other tiers to fully exploit the computational capability provided by the GPU cluster. Although the computational performance of GPUs has been improving steadily, the memory size of modern GPUs is still quite limited, which restricts the sizes of deep neural networks that can be trained on GPUs, and hence raises serious challenges. We have introduced a framework, moDNN, to optimize the memory usage in DNN training on GPUs. moDNN supports automatic tuning of DNN training code to match any given memory budget. By taking full advantage of overlapping computations and data transfers, we developed new heuristics to judiciously schedule data offloading and prefetching transfers, together with convolution algorithm selection, to optimize memory usage. moDNN can save memory usage close to 60X, compared with an ideal case which assumes that the GPU memory is sufficient to hold all data. This work has been approved for patent filing by Notre Dame.</p>\n<p class=\"normal\">To assist rapid design space exploration when conducting resource mapping and scheduling in heterogeneous systems, we have developed a uniform modeling framework, Eva-DNN, which can estimate energy consumed by various types of deep neural network (DNN) accelerator hardware without any simulation.&nbsp; Eva-DNN can analytically and accurately model energy contributions from device technology, circuits, architecture, data mapping strategy, and network to a heterogeneous DNN accelerator. Experimental results from Eva-DNN agree with measurement results within 20% deviation.</p>\n<p class=\"normal\"><strong>Other Related Research Activities:</strong> The tier encompasses the development of various computationally intensive algorithms by the faculty members from the department and benefited from the GPU cluster. As one significant outcome, we introduced a new fast, flexible, learning-free method for sparse segmentation and reconstruction of neural volumes. This method overcomes the shortcomings of machine learning algorithms from computer vision, i.e., error-prone and computationally expensive. Compared to existing supervised learning methods, our method is both significantly faster (up to several orders of magnitude) and produces high-quality reconstructions that are robust to noise and artifacts.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/07/2019<br>\n\t\t\t\t\tModified by: Xiaobo&nbsp;S&nbsp;Hu</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2019/1629914/1629914_10442334_1575686211494_brain--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1629914/1629914_10442334_1575686211494_brain--rgov-800width.jpg\" title=\"Rodent_Brain\"><img src=\"/por/images/Reports/POR/2019/1629914/1629914_10442334_1575686211494_brain--rgov-66x44.jpg\" alt=\"Rodent_Brain\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We deploy massive-scale imaging techniques like synchrotron X-ray tomography on rodent brains to image and reconstruct the connectome of a significant portion of the visual cortex. To date, an effort to characterize the anatomy of such a large portion of a brain has never been attempted.</div>\n<div class=\"imageCredit\">Ali Shahbazi, Jeffery Kinnison, Rafael Vescovi, Ming Du, Robert Hill, Maximilian Joesch, Marc Takeno, Hongkui Zeng, Nuno da Costa, Jaime Grutzendler, Narayanan Kasthuri, Walter J. Scheirer</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Xiaobo&nbsp;S&nbsp;Hu</div>\n<div class=\"imageTitle\">Rodent_Brain</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1629914/1629914_10442334_1575685839229_tsr-tvd--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1629914/1629914_10442334_1575685839229_tsr-tvd--rgov-800width.jpg\" title=\"TSR-TVD (Temporal Super-Resolution for Time-Varying Data)\"><img src=\"/por/images/Reports/POR/2019/1629914/1629914_10442334_1575685839229_tsr-tvd--rgov-66x44.jpg\" alt=\"TSR-TVD (Temporal Super-Resolution for Time-Varying Data)\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">TSR-TVD learns the generation of intermediate volumes from a given pair of volumes. The network includes a generator G which consists of the predicting and blending modules and a discriminator D which distinguishes the synthesized volumes from the ground truth volumes.</div>\n<div class=\"imageCredit\">Jun Han, Chaoli Wang</div>\n<div class=\"imageSubmitted\">Chaoli&nbsp;S&nbsp;Wang</div>\n<div class=\"imageTitle\">TSR-TVD (Temporal Super-Resolution for Time-Varying Data)</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "This NSF funding was used to build a heterogeneous, high-performance computing cluster. The acquisition received partial some supplemental funds from the Computer Science and Engineering department and Center for Research Computing at the University of Notre Dame. The cluster has provided critical computing supports to 24 postdoctoral fellows and graduate students as well as 4 undergraduate students. It has served an enabler to a top-down, vertically integrated research theme spanning from application-driven algorithm development to resource management for heterogeneous computing platforms. The research enabled by the cluster is grouped into four tiers and the associated most significant findings/outcomes are summarized below.\nApplication-Driven Algorithm Development: This tier aims to develop new machine learning (e.g., deep learning) approaches for medical, biomedical, and personal healthcare applications. Biomedical data, when represented as networks or graphs, inherently have multiple modalities. Imagine a disease network. The diseases can share links based on shared risk factors, co-occurrence patterns, or even shared genetic representation. We have developed representation learning based approaches, inspired by deep learning or matrix factorization. Extensive experiments show that our proposed approaches not only outperform state-of-the-art methods in various heterogeneous network-mining tasks, such as node classification, clustering, and similarity search, but also discern the structural and semantic correlations between diverse network objects.\nDue to the diversity and complexity of biomedical image data, manually annotating large amounts of biomedical image data for training common deep learning models for biomedical applications is very time consuming and labor intensive. Biomedical experts are commonly involved in a long and iterative process of annotation, as in active learning type annotation schemes. We have proposed representative annotation (RA), a new deep learning framework for considerably reducing annotation effort for biomedical image segmentation. Our RA scheme leverages the ability of deep neural networks to learn better representations of image data and frees annotators from the iterative process of common active learning based annotation schemes. Evaluation results based on both 2D and 3D datasets show that RA yields competitive segmentation results in comparison with state-of-the-art methods.\nSupporting Methods and Tools: The objective of this tier is to design and develop a suite of general techniques to support application-level algorithms and tools. We have focused specifically on visualization techniques. As one key outcome, we pioneered a data augmentation framework for scientific data analysis and visualization. Data augmentation in this context refers to the addition of spatial, temporal, and variable details to reduced simulation data by incorporating information derived from internal and external sources. The general approach developed could be transformative in that it will substantially improve the ability of visualization researchers and domain scientists to make the best use of limited simulation data output, which would energize visualization research and facilitate scientific discovery.\nSystem and Runtime Support: This tier focuses on methods enabling the application-driven algorithms, supporting methods and tools developed in other tiers to fully exploit the computational capability provided by the GPU cluster. Although the computational performance of GPUs has been improving steadily, the memory size of modern GPUs is still quite limited, which restricts the sizes of deep neural networks that can be trained on GPUs, and hence raises serious challenges. We have introduced a framework, moDNN, to optimize the memory usage in DNN training on GPUs. moDNN supports automatic tuning of DNN training code to match any given memory budget. By taking full advantage of overlapping computations and data transfers, we developed new heuristics to judiciously schedule data offloading and prefetching transfers, together with convolution algorithm selection, to optimize memory usage. moDNN can save memory usage close to 60X, compared with an ideal case which assumes that the GPU memory is sufficient to hold all data. This work has been approved for patent filing by Notre Dame.\nTo assist rapid design space exploration when conducting resource mapping and scheduling in heterogeneous systems, we have developed a uniform modeling framework, Eva-DNN, which can estimate energy consumed by various types of deep neural network (DNN) accelerator hardware without any simulation.  Eva-DNN can analytically and accurately model energy contributions from device technology, circuits, architecture, data mapping strategy, and network to a heterogeneous DNN accelerator. Experimental results from Eva-DNN agree with measurement results within 20% deviation.\nOther Related Research Activities: The tier encompasses the development of various computationally intensive algorithms by the faculty members from the department and benefited from the GPU cluster. As one significant outcome, we introduced a new fast, flexible, learning-free method for sparse segmentation and reconstruction of neural volumes. This method overcomes the shortcomings of machine learning algorithms from computer vision, i.e., error-prone and computationally expensive. Compared to existing supervised learning methods, our method is both significantly faster (up to several orders of magnitude) and produces high-quality reconstructions that are robust to noise and artifacts.\n\n\t\t\t\t\tLast Modified: 12/07/2019\n\n\t\t\t\t\tSubmitted by: Xiaobo S Hu"
 }
}