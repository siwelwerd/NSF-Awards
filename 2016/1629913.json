{
 "awd_id": "1629913",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "II-New: Collaborative: A Mixed Reality Environment for Enabling Everywhere Data-Centric Work",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Balakrishnan Prabhakaran",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 399280.0,
 "awd_amount": 493140.0,
 "awd_min_amd_letter_date": "2016-07-25",
 "awd_max_amd_letter_date": "2018-08-29",
 "awd_abstract_narration": "This infrastructure project will develop an open source software toolkit, called OpenMR, to support building \"mixed reality\" data analysis systems that project data into the physical world using a new class of display devices such as Microsoft Hololens and Oculus Rift. Through OpenMR, these lightweight, wearable, mobile devices will tap into data-intensive infrastructures hosted in the cloud, with the goal of developing systems that allow users to perform data-intensive tasks from anywhere, without requiring heavy dedicated large-format displays supported by dedicated local computers.  To pursue this research, the investigators will acquire both dedicated cloud-computing servers (to support data analysis) and mixed reality hardware devices (to create the interfaces).  They will develop OpenMR to connect this hardware, to support common analysis tasks such as selecting, filtering, and classifying data, and to create data displays in the physical world. To both demonstrate the toolkit and advance data analysis research, they will build a number of prototype mixed reality interfaces for researchers whose work requires analyzing a large amount of data in domains including weather, biology, and medical imaging.  In addition to advancing those specific research areas, studying these prototypes with real users will support research around the underlying data analysis techniques, the cognitive science of how people interact with data in the physical world, and the design principles needed to build mixed reality systems.  This, in turn, will make these emerging technologies more likely to succeed and spread, and increase the chance of finding potential 'killer apps' for these systems.  The infrastructure will also directly support education and research at the partner universities around data visualization, computer graphics, computer vision, and machine learning, while the release of the toolkit will benefit the wider community.  This research is timely and important because as smart devices, in particular virtual and mixed reality devices such as Google Glass, Microsoft Hololens, Oculus Rift and Google Cardboard, become commonplace, these devices will play an increasingly important role relative to traditional laptop and digital computers when interacting with digital information. \r\n\r\nThe long-term vision of the project is to develop a mixed reality research infrastructure to support everywhere data-centric innovations, providing immersive, intuitive, location-free, advanced machine learning, data analysis, reduction, summary and storage tools.  This includes advanced support for the full pipeline of data-centric work in mixed reality spaces through the OpenMR open source toolkit, including front end visualization and interaction that leverages awareness of available rendering spaces and hardware along with effective visualization patterns in 2D and 3D spaces to optimize interaction; key components of data analysis and machine learning on the middle layers including automatic, generic feature engineering and joint optimization of classification performance and effective identification of discriminating features; and high-performance computing and cost-sensitive job management on the server.  The team will evaluate OpenMR's efficiency, stability, scalability, functionality, flexibility, and ease of adoption through a number of mechanisms, including self-evaluations and documentation of the design process, review from domain experts, and evaluation with both expert and novice users on data analysis tasks that cur across the specific application domains described above.  The toolkit itself will be released on the GitHub open source platform during the third year of the project after it has reached an initial level of maturity and usefulness.  The investigators will publicize OpenMR through a Youtube channel with a set of demonstration videos; outreach to relevant researchers interested in immersive visualization, visual analytics, multi-sensory human-computer interaction, machine learning with human-in-the-loop, and high-performance computing; and collaboration with undergraduates in the Students, Technology, Academia, Research, and Service Computing Corps consortium.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aidong",
   "pi_last_name": "Lu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aidong Lu",
   "pi_email_addr": "alu1@uncc.edu",
   "nsf_id": "000286346",
   "pi_start_date": "2016-07-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shaoting",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shaoting Zhang",
   "pi_email_addr": "szhang16@uncc.edu",
   "nsf_id": "000663841",
   "pi_start_date": "2016-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Charlotte",
  "inst_street_address": "9201 UNIVERSITY CITY BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTE",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "7046871888",
  "inst_zip_code": "282230001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NC12",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHARLOTTE",
  "org_prnt_uei_num": "NEYCH3CVBTR6",
  "org_uei_num": "JB33DT84JNA5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Charlotte",
  "perf_str_addr": "9201 University City Blvd",
  "perf_city_name": "Charlotte",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "282230001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NC12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "8551",
   "pgm_ref_txt": "IntgStrat Undst Neurl&Cogn Sys"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 399280.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 93860.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explores the concept of \"Everywhere Data Centric Work\" through designing innovative infrastructure of cloud computing and VR/AR devices. Different from desktop systems, our work supports users to perform various data analysis work with intuitive interaction methods supported by the latest VR/AR devices and powerful data processing capabilities by the cloud computing. This new infrastructure frees data work from the restriction of fixed displays and enables immersive analysis of virtual information in the context of real physical environments.</p>\n<p>We have explored immersive visualization methods that provide 3D visualization and intuitive interaction methods by combining gaze, voice, and hand gestures. We have developed a library of immersive visualization which covers a set of classical visualization methods. We have developed immersive visualization methods for analyzing collected sensor data from buildings, 3D visualization of neuron structures, 2D geospatial and time-varying datasets, and biodiversity and environmental data for Smokey Mountain National Park.</p>\n<p>In additional to stand alone VR/AR devices, we also explore methods to augment existed physical displays. We prototyped a multi-model interaction approach that allows users to work on both traditional displays and the immersive environment. We have studied and built-up the know-how to reliably calibrate device registration within a physical environment and with other display devices (spanning individual LCD screens and power-walls). We have also explored and published multiple coordinated spaces for visualizing complex datasets effectively in 3D environments through integrating distributed cognition theories.</p>\n<p>For web applications, we have developed a framework to simplify the creation of Augmented Reality (AR) extensions. Without modifying the original web applications, our AR extensions developed using Alpaca appear as a web-browser extension, and automatically bridge the Document Object Model (DOM) of the web with the SceneGraph model of AR. With our extension, the creation and control of augmented reality devices becomes transparent, as if they were natively part of the browser. We have also developed a method of immersive web browsing (ImWeb) to enable effective exploration of multiple datasets over the web with augmented reality (AR) techniques. We use an online 3D neuron database to demonstrate that ImWeb enables new experiences of exploring 3D datasets over the web.</p>\n<p>To enable collaborative analysis, we have developed a remote visualization system through providing co-presence, information sharing, and collaborative analysis functions based on mixed reality techniques. Our evaluation results confirm the effectiveness of comprehensive sharing among user, data, physical, and interaction spaces for improving remote collaborative analysis experience.&nbsp;</p>\n<p>Built upon the rich background of distributed and embodied cognition, we provide the formalization of multiple coordinated spaces and a set of generation, coordination, optimization methods to support interactive analysis in a connected, distributed set of subspaces. We also explored the sensemaking process in an immersive environment through studying both internal and external user behaviors with a classical visualization problem, a visual comparison and clustering task. We assessed how the layout of the interface and the challenge level of the task (low vs. high cognitive load) influenced the users' interactions, how these interactions changed over time, and how they influenced task performance.</p>\n<p>While we focused on virtual reality (VR) and augmented reality (AR) devices, our data infrastructure can be extended to a variety of mobile devices. With the improvement of technology on VR/AR devices, we expect that more attractive features can be provided.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/03/2021<br>\n\t\t\t\t\tModified by: Aidong&nbsp;Lu</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635954267069_floors01--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635954267069_floors01--rgov-800width.jpg\" title=\"Onsite immersive visualization\"><img src=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635954267069_floors01--rgov-66x44.jpg\" alt=\"Onsite immersive visualization\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This work presents an immersive visualization approach for investigating abnormal events in heterogeneous, multi-source, and time-series sensor data collections in real-time on the site of the event.</div>\n<div class=\"imageCredit\">Elias Mahfoud, Aidong Lu</div>\n<div class=\"imageSubmitted\">Aidong&nbsp;Lu</div>\n<div class=\"imageTitle\">Onsite immersive visualization</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964381076_onsite--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964381076_onsite--rgov-800width.jpg\" title=\"AR exploration\"><img src=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964381076_onsite--rgov-66x44.jpg\" alt=\"AR exploration\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This work investigates a mixed-reality platform to improve the coordination and situation awareness for multiple users performing real-time operations in smart buildings.</div>\n<div class=\"imageCredit\">Elias Mahfoud, Aidong Lu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Aidong&nbsp;Lu</div>\n<div class=\"imageTitle\">AR exploration</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964435629_MCS--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964435629_MCS--rgov-800width.jpg\" title=\"Biodiversity visualization\"><img src=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964435629_MCS--rgov-66x44.jpg\" alt=\"Biodiversity visualization\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This example demonstrates our design of Multiple Coordinated Spaces (MCS) using a multivariate, geo-spatial biodiversity application from Smoky Mountain National Park - multiple species distributions (layers with points) and environmental layers (grey scale maps).</div>\n<div class=\"imageCredit\">Tahir Mahmood, Aidong Lu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Aidong&nbsp;Lu</div>\n<div class=\"imageTitle\">Biodiversity visualization</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964496446_ImWeb--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964496446_ImWeb--rgov-800width.jpg\" title=\"Neuron database visualization\"><img src=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964496446_ImWeb--rgov-66x44.jpg\" alt=\"Neuron database visualization\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This project presents a method of immersive web browsing (ImWeb) to enable effective exploration of multiple datasets over the web with augmented reality (AR) techniques. Users can start from the web and use hand gestures to bring the data into the physical world for 3D interaction and analysis.</div>\n<div class=\"imageCredit\">Willis Fulmer, Aidong Lu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Aidong&nbsp;Lu</div>\n<div class=\"imageTitle\">Neuron database visualization</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964628323_collaboration--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964628323_collaboration--rgov-800width.jpg\" title=\"Remote Collaboration\"><img src=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964628323_collaboration--rgov-66x44.jpg\" alt=\"Remote Collaboration\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our remote collaboration system supports users to perform and share the visual analytics process, which enhances the information sharing and collaborative analysis experiences compared to traditional desktop visualizations.</div>\n<div class=\"imageCredit\">Tahir Mahmood, Aidong Lu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Aidong&nbsp;Lu</div>\n<div class=\"imageTitle\">Remote Collaboration</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964698573_sensemaking--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964698573_sensemaking--rgov-800width.jpg\" title=\"Sensemaking in Immersive Environments\"><img src=\"/por/images/Reports/POR/2021/1629913/1629913_10443308_1635964698573_sensemaking--rgov-66x44.jpg\" alt=\"Sensemaking in Immersive Environments\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This work examines user interactions during the sensemaking process in immersive visualization for spatial data clustering tasks. We found that increased interactions and cerebral hemodynamic responses were associated with more accurate performance, especially on cognitively demanding trials.</div>\n<div class=\"imageCredit\">Alexia Galati, Riley Schoppa, Aidong Lu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Aidong&nbsp;Lu</div>\n<div class=\"imageTitle\">Sensemaking in Immersive Environments</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project explores the concept of \"Everywhere Data Centric Work\" through designing innovative infrastructure of cloud computing and VR/AR devices. Different from desktop systems, our work supports users to perform various data analysis work with intuitive interaction methods supported by the latest VR/AR devices and powerful data processing capabilities by the cloud computing. This new infrastructure frees data work from the restriction of fixed displays and enables immersive analysis of virtual information in the context of real physical environments.\n\nWe have explored immersive visualization methods that provide 3D visualization and intuitive interaction methods by combining gaze, voice, and hand gestures. We have developed a library of immersive visualization which covers a set of classical visualization methods. We have developed immersive visualization methods for analyzing collected sensor data from buildings, 3D visualization of neuron structures, 2D geospatial and time-varying datasets, and biodiversity and environmental data for Smokey Mountain National Park.\n\nIn additional to stand alone VR/AR devices, we also explore methods to augment existed physical displays. We prototyped a multi-model interaction approach that allows users to work on both traditional displays and the immersive environment. We have studied and built-up the know-how to reliably calibrate device registration within a physical environment and with other display devices (spanning individual LCD screens and power-walls). We have also explored and published multiple coordinated spaces for visualizing complex datasets effectively in 3D environments through integrating distributed cognition theories.\n\nFor web applications, we have developed a framework to simplify the creation of Augmented Reality (AR) extensions. Without modifying the original web applications, our AR extensions developed using Alpaca appear as a web-browser extension, and automatically bridge the Document Object Model (DOM) of the web with the SceneGraph model of AR. With our extension, the creation and control of augmented reality devices becomes transparent, as if they were natively part of the browser. We have also developed a method of immersive web browsing (ImWeb) to enable effective exploration of multiple datasets over the web with augmented reality (AR) techniques. We use an online 3D neuron database to demonstrate that ImWeb enables new experiences of exploring 3D datasets over the web.\n\nTo enable collaborative analysis, we have developed a remote visualization system through providing co-presence, information sharing, and collaborative analysis functions based on mixed reality techniques. Our evaluation results confirm the effectiveness of comprehensive sharing among user, data, physical, and interaction spaces for improving remote collaborative analysis experience. \n\nBuilt upon the rich background of distributed and embodied cognition, we provide the formalization of multiple coordinated spaces and a set of generation, coordination, optimization methods to support interactive analysis in a connected, distributed set of subspaces. We also explored the sensemaking process in an immersive environment through studying both internal and external user behaviors with a classical visualization problem, a visual comparison and clustering task. We assessed how the layout of the interface and the challenge level of the task (low vs. high cognitive load) influenced the users' interactions, how these interactions changed over time, and how they influenced task performance.\n\nWhile we focused on virtual reality (VR) and augmented reality (AR) devices, our data infrastructure can be extended to a variety of mobile devices. With the improvement of technology on VR/AR devices, we expect that more attractive features can be provided.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/03/2021\n\n\t\t\t\t\tSubmitted by: Aidong Lu"
 }
}