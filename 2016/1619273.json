{
 "awd_id": "1619273",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Wearable Interfaces to Direct Agent Teams with Adaptive Autonomy",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 495628.0,
 "awd_amount": 495628.0,
 "awd_min_amd_letter_date": "2016-07-08",
 "awd_max_amd_letter_date": "2019-06-14",
 "awd_abstract_narration": "Unmanned robotic systems are set to revolutionize a number of vital human activities, including disaster response, public safety, citizen science, and agriculture, yet such systems are complex and require multiple pilots. As algorithms take over, and controls are simplified, workers benefit from directing, rather than controlling, these systems. Such simplifications could enable workers to use their hands and focus their perception in the physical world, relying on wearable interfaces (e.g., chording keyboards, gesture inputs) to manage teams of unmanned vehicles. Adaptive autonomy, in which unmanned systems alter their need for human attention in response to complexities in the environment, offers a solution in which workers can use minimal input to enact change. The present research combines wearable interfaces with adaptive autonomy to direct teams of software agents, which simulate unmanned robotic systems. The outcomes will support next-generation unmanned robotic system interfaces. \r\n\r\nThe objective of this project is to develop wearable interfaces for the direction of a team of software agents that make use of adaptive autonomy and ascertain the effectiveness of interface designs to direct agents. This research develops a testbed for wearable cyber-human system designs that uses software agents as unmanned robotic system simulations and uses adaptive-autonomy algorithms to drive the agents. The research develops a framework connecting wearable interface modalities to the activities they best support. Developed systems will be validated through mixed reality environments in which participants will direct software agents while acting in the physical world. The principal hypothesis is that a set of interconnected interfaces can be developed that, through appropriate control algorithms, maximizes an operator's control span over a team of agents and optimizes the operator's physical workload, mental workload, and situation awareness.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Phoebe",
   "pi_last_name": "Toups Dugas",
   "pi_mid_init": "O",
   "pi_sufx_name": "",
   "pi_full_name": "Phoebe O Toups Dugas",
   "pi_email_addr": "pdugas@cs.nmsu.edu",
   "nsf_id": "000652612",
   "pi_start_date": "2016-07-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Son",
   "pi_last_name": "Tran",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Son C Tran",
   "pi_email_addr": "tson@cs.nmsu.edu",
   "nsf_id": "000492136",
   "pi_start_date": "2016-07-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Igor",
   "pi_last_name": "Dolgov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Igor Dolgov",
   "pi_email_addr": "id@nmsu.edu",
   "nsf_id": "000636792",
   "pi_start_date": "2016-07-08",
   "pi_end_date": "2019-06-14"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Marlena",
   "pi_last_name": "Fraune",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Marlena R Fraune",
   "pi_email_addr": "mfraune@nmsu.edu",
   "nsf_id": "000706989",
   "pi_start_date": "2019-06-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New Mexico State University",
  "inst_street_address": "1050 STEWART ST.",
  "inst_street_address_2": "",
  "inst_city_name": "LAS CRUCES",
  "inst_state_code": "NM",
  "inst_state_name": "New Mexico",
  "inst_phone_num": "5756461590",
  "inst_zip_code": "88003",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NM02",
  "org_lgl_bus_name": "NEW MEXICO STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "J3M5GZAT8N85"
 },
 "perf_inst": {
  "perf_inst_name": "New Mexico State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NM",
  "perf_st_name": "New Mexico",
  "perf_zip_code": "880038002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NM02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 495628.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This is the outcomes report for project 1619273: CHS: Small: Wearable Interfaces to Direct Agent Teams with Adaptive Autonomy. This work was aimed at supporting future disaster response scenarios where operatives would benefit from multiple drones providing information. We explored how to direct those drones through wearable&nbsp; technology that did not inhibit movement or awareness. Specifically, we were interested in developing more intelligent multi-agent systems (i.e., software simulations of drones) and better user interfaces for disaster contexts (e.g., wearable computer configurations). The work made use of a simulation environment to enable human participants to don wearable computers, move and act outdoors, and interact with virtual drones.&nbsp;</p>\n<p><br />The grant has produced a set of reusable software and hardware configurations to investigate the value of wearable computers to support human-robot teams in the field (https://pixllab.github.io/URSDocumentation/). The system connects together multiple pieces of software to enable a mixed reality experience of working with virtual drones. In our scenarios to date, we look at using a game that is an analog of urban search and rescue. The player moves around in the real world while their location is tracked by the system. The player seeks out virtual goals. To assist the player, multiple drones can be deployed to find goals the player cannot reach (e.g., on top of buildings).<br />To provide game logic and a first-person gameworld user interface through an avatar, we use an engine built on Unity. A drone simulation platform, Gazebo, tracks and simulates the virtual drones, accounting for avoiding obstacles. A customized planner, taking inputs described by the Planning Domain Description Language (PDDL), provides the drones? intelligence and communicates with Gazebo using the Robot Operating System. Finally, a set of hardware and software interfaces connect with these components to create a user interface that provides information about the gameworld and the ability to interact with the virtual drones. In our first studies, we use NASA WorldWind for a map visualization, displayed on a wrist-worn touchscreen, and provide drone data on an HMD.</p>\n<p><br />The project produced a design framework to guide building composite wearable computers. Constructing the framework involved qualitatively analyzing over 100 sources for information about components that could be used to build wearable computers. It identifies four dimensions that are essential to choosing devices: type of interactivity provided, associated output modalities, mobility, and body location for each device identified for study. The framework supports designers in ensuring a resulting composite computer supports the types of interaction necessary for the designed tasks and that the combination of devices will be compatible on the human body. We expect the framework to support researchers and designers in building new wearable computers in a number of domains, enabling the selection of the right devices to support various contexts.&nbsp;</p>\n<p><br />The simulated environment for controlling teams of drones provides solutions for issues that arise from hybrid human-drone team coordination and planning. To the best of our knowledge, this is the first simulation environment that allows a single human to control a team of robots at the same time through wearable devices.&nbsp;We undertook a number of user studies, starting with early prototypes and concluding with a test of multiple wearable configurations identified through the framework. Overall, participants were able to use our mixed reality system as planned. We identified needs for further training and our studies show ways in which future wearable user interfaces might be improved.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/23/2020<br>\n\t\t\t\t\tModified by: Zachary&nbsp;O&nbsp;Toups</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1619273/1619273_10438853_1606161322005_outcomes-wearableGear--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1619273/1619273_10438853_1606161322005_outcomes-wearableGear--rgov-800width.jpg\" title=\"Sample wearable components\"><img src=\"/por/images/Reports/POR/2020/1619273/1619273_10438853_1606161322005_outcomes-wearableGear--rgov-66x44.jpg\" alt=\"Sample wearable components\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A lab member demonstrates combinations of wearable computer components used the summative user study.</div>\n<div class=\"imageCredit\">Ahmed Khalaf</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Zachary&nbsp;O&nbsp;Toups</div>\n<div class=\"imageTitle\">Sample wearable components</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1619273/1619273_10438853_1606161159465_outcomes-architecture--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1619273/1619273_10438853_1606161159465_outcomes-architecture--rgov-800width.jpg\" title=\"Architecture for a mixed reality drone testing environment\"><img src=\"/por/images/Reports/POR/2020/1619273/1619273_10438853_1606161159465_outcomes-architecture--rgov-66x44.jpg\" alt=\"Architecture for a mixed reality drone testing environment\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Block diagram of components forming the mixed reality drone simulation. A human user wears a number of devices and potentially uses a laptop computer (for in-lab studies). A gameworld is used to track the player, a game engine server tracks statistics, the Robot Operating System simulates drones.</div>\n<div class=\"imageCredit\">developed by Z O. Toups</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Zachary&nbsp;O&nbsp;Toups</div>\n<div class=\"imageTitle\">Architecture for a mixed reality drone testing environment</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1619273/1619273_10438853_1606161415182_ui--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1619273/1619273_10438853_1606161415182_ui--rgov-800width.jpg\" title=\"Drone map user interface\"><img src=\"/por/images/Reports/POR/2020/1619273/1619273_10438853_1606161415182_ui--rgov-66x44.jpg\" alt=\"Drone map user interface\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Sample map user interface showing aerial photograph map with vector details for streets. Shows the locations of drones in the mixed reality environment.</div>\n<div class=\"imageCredit\">Ahmed Khalaf</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Zachary&nbsp;O&nbsp;Toups</div>\n<div class=\"imageTitle\">Drone map user interface</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis is the outcomes report for project 1619273: CHS: Small: Wearable Interfaces to Direct Agent Teams with Adaptive Autonomy. This work was aimed at supporting future disaster response scenarios where operatives would benefit from multiple drones providing information. We explored how to direct those drones through wearable  technology that did not inhibit movement or awareness. Specifically, we were interested in developing more intelligent multi-agent systems (i.e., software simulations of drones) and better user interfaces for disaster contexts (e.g., wearable computer configurations). The work made use of a simulation environment to enable human participants to don wearable computers, move and act outdoors, and interact with virtual drones. \n\n\nThe grant has produced a set of reusable software and hardware configurations to investigate the value of wearable computers to support human-robot teams in the field (https://pixllab.github.io/URSDocumentation/). The system connects together multiple pieces of software to enable a mixed reality experience of working with virtual drones. In our scenarios to date, we look at using a game that is an analog of urban search and rescue. The player moves around in the real world while their location is tracked by the system. The player seeks out virtual goals. To assist the player, multiple drones can be deployed to find goals the player cannot reach (e.g., on top of buildings).\nTo provide game logic and a first-person gameworld user interface through an avatar, we use an engine built on Unity. A drone simulation platform, Gazebo, tracks and simulates the virtual drones, accounting for avoiding obstacles. A customized planner, taking inputs described by the Planning Domain Description Language (PDDL), provides the drones? intelligence and communicates with Gazebo using the Robot Operating System. Finally, a set of hardware and software interfaces connect with these components to create a user interface that provides information about the gameworld and the ability to interact with the virtual drones. In our first studies, we use NASA WorldWind for a map visualization, displayed on a wrist-worn touchscreen, and provide drone data on an HMD.\n\n\nThe project produced a design framework to guide building composite wearable computers. Constructing the framework involved qualitatively analyzing over 100 sources for information about components that could be used to build wearable computers. It identifies four dimensions that are essential to choosing devices: type of interactivity provided, associated output modalities, mobility, and body location for each device identified for study. The framework supports designers in ensuring a resulting composite computer supports the types of interaction necessary for the designed tasks and that the combination of devices will be compatible on the human body. We expect the framework to support researchers and designers in building new wearable computers in a number of domains, enabling the selection of the right devices to support various contexts. \n\n\nThe simulated environment for controlling teams of drones provides solutions for issues that arise from hybrid human-drone team coordination and planning. To the best of our knowledge, this is the first simulation environment that allows a single human to control a team of robots at the same time through wearable devices. We undertook a number of user studies, starting with early prototypes and concluding with a test of multiple wearable configurations identified through the framework. Overall, participants were able to use our mixed reality system as planned. We identified needs for further training and our studies show ways in which future wearable user interfaces might be improved. \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/23/2020\n\n\t\t\t\t\tSubmitted by: Zachary O Toups"
 }
}