{
 "awd_id": "1555079",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Biologically inspired neural network models for robust speech processing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2016-06-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 502210.0,
 "awd_amount": 502210.0,
 "awd_min_amd_letter_date": "2016-06-06",
 "awd_max_amd_letter_date": "2020-07-22",
 "awd_abstract_narration": "The recent parallel breakthroughs in deep neural network models and neuroimaging techniques have significantly advanced the current state of artificial and biological computing. However, there has been little interaction between these two disciplines, resulting in simplistic models of neural systems with limited prediction, learning and generalization abilities. The goal of this project is to create a coherent theoretical and mathematical framework to understand the computational role of distinctive features of biological neural networks, their contribution to the formation of robust signal representations, and to model and integrate them into the current artificial neural networks. These new bio-inspired models and algorithms will have adaptive and cognitive abilities, will better predict experimental observations, and will advance the knowledge of how the brain processes speech. In addition, the performance of these models should approach human abilities in tasks mimicking cognitive functions, and will motivate new experiments that can further impose realistic constraints on the models. \r\n\r\nThis interdisciplinary project lies at the intersection of neurolinguistics, speech engineering, and machine learning, uniting the historically separated disciplines of neuroscience and engineering. The proposed innovative approach integrates methods and expertise across various disciplines, including system identification, signal processing, neurophysiology, and systems neuroscience. The aim of this proposal is to analyze and transform the artificial neural network models to accurately reflect the computational and organizational principles of biological systems through three specific objectives: I) to create analytic methods that can provide insights into the transformations that occur in artificial neural network models by examining their representational properties and feature encoding, II) to model and implement the local, bottom-up, adaptive neural mechanisms that appear ubiquitously in biological systems, and III) to model the top-down, knowledge driven abilities of cognitive systems to implement new computations in response to the task requirements. Accurate computational models of the neural transformations will have an overarching impact in many disciplines including artificial intelligence, neurolinguistics, and systems neuroscience. More realistic neural network models will not only result in human-like pattern recognition technologies and better understanding of how the brain solves speech perception, but can also help explain how these processes are impaired in people with speech and language disorders. Therefore, the proposed project will advance the state-of-the-art in multiple disciplines.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nima",
   "pi_last_name": "Mesgarani",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nima Mesgarani",
   "pi_email_addr": "nm2764@columbia.edu",
   "nsf_id": "000675129",
   "pi_start_date": "2016-06-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 94594.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 97431.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 100354.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 180641.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 29190.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>In this research, we aimed to form a better understanding of how artificial neural networks compute and what representation they use. The second objective was to create neurally inspired neural network models and to compare the representational and computational characteristics of biological and artificial neural network models. To achieve these goals, we have formulated a computational framework for learning and interpreting neural network models that can accurately predict the neural responses to sound, in particular the nonlinear transformations that the brain applies to perceive sound. Moreover, we proposed several neurally inspired mechanisms that can be implemented in artificial neural network models to increase their efficacy and robustness. In a complementary approach, we addressed the general source separation problem with novel deep learning frameworks, including the ?attractor network? and ?time-domain audio separation network?. Our proposed model works by first generating a high-dimensional embedding for each time-frequency bin. We then form a reference point (attractor) for each source in the embedding space that pulls all the features belonging to that source toward itself. This method performed particularly well on a standard benchmark used for this task. In addition, we directly addressed several inherent problems of most speech separation algorithms that use spectrograms as their representation. Instead, we proposed a fully convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. The proposed speech separation algorithm significantly outperforms previous time-frequency methods on both objective and subjective tests, even when compared to the separation quality of several ideal time-frequency masks of the speakers. This research has also enabled the training of several graduate students who gained first-hand knowledge about the brain, and also became familiar with the latest computational modeling approaches and state-of-the-art speech processing methodologies.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/16/2021<br>\n\t\t\t\t\tModified by: Nima&nbsp;Mesgarani</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this research, we aimed to form a better understanding of how artificial neural networks compute and what representation they use. The second objective was to create neurally inspired neural network models and to compare the representational and computational characteristics of biological and artificial neural network models. To achieve these goals, we have formulated a computational framework for learning and interpreting neural network models that can accurately predict the neural responses to sound, in particular the nonlinear transformations that the brain applies to perceive sound. Moreover, we proposed several neurally inspired mechanisms that can be implemented in artificial neural network models to increase their efficacy and robustness. In a complementary approach, we addressed the general source separation problem with novel deep learning frameworks, including the ?attractor network? and ?time-domain audio separation network?. Our proposed model works by first generating a high-dimensional embedding for each time-frequency bin. We then form a reference point (attractor) for each source in the embedding space that pulls all the features belonging to that source toward itself. This method performed particularly well on a standard benchmark used for this task. In addition, we directly addressed several inherent problems of most speech separation algorithms that use spectrograms as their representation. Instead, we proposed a fully convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. The proposed speech separation algorithm significantly outperforms previous time-frequency methods on both objective and subjective tests, even when compared to the separation quality of several ideal time-frequency masks of the speakers. This research has also enabled the training of several graduate students who gained first-hand knowledge about the brain, and also became familiar with the latest computational modeling approaches and state-of-the-art speech processing methodologies.\n\n\t\t\t\t\tLast Modified: 12/16/2021\n\n\t\t\t\t\tSubmitted by: Nima Mesgarani"
 }
}