{
 "awd_id": "1625793",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:  The structure of the ASL lexicon: Experimental and statistical evidence from a large lexical database (ASL-LEX)",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tyler Kendall",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-02-29",
 "tot_intn_awd_amt": 191563.0,
 "awd_amount": 191563.0,
 "awd_min_amd_letter_date": "2016-08-12",
 "awd_max_amd_letter_date": "2016-08-12",
 "awd_abstract_narration": "This collaborative project will record and study the properties of lexical forms in American Sign Language. Almost everything we know about human language comes from the study of spoken languages. However, only by studying sign languages is it possible to discover which linguistic rules and constraints are universal to all human languages and which depend on the particular properties of an individual language. By studying sign languages researchers can uncover language patterns that are tied to the nature of the articulators (i.e., the hands vs. the vocal tract) or that are linked to the specific way a language is perceived (i.e., visually vs. auditorally). Researchers can also uncover language patterns that result from properties that systematically vary between spoken and signed languages, such as the high prevalence of iconic forms (words that resemble what they mean) in sign languages. Psychological and linguistic research on spoken languages has relied on lexical databases--repositories of information about the words of a language--to identify factors that influence how words are comprehended and produced, to understand how words are organized and structured in the mind and brain (in our \"mental lexicon\"), and to discover the linguistic patterns that are present in languages. Unfortunately however, there is currently no comparably large lexical database for American Sign Language (ASL), the sign language used by deaf and hearing people in the United States.\r\n\r\nA primary aim of this project is to create a large, searchable, and publically available database of approximately 2,500 ASL signs. The database (called ASL-LEX) will contain the following information for each sign: subjective frequency-of-use ratings, iconicity ratings from both deaf signers and hearing non-signers, sign duration measures, lexical category information (e.g., noun, verb, etc.), and codes for sign-based phonological features (e.g., location, handshape, movement) that can be used to calculate whether the form of a sign is relatively common (has many form 'neighbors') or relatively unique (has few 'neighbors'). A second aim is to use ASL-LEX to conduct the first quantitative analysis of the ASL lexicon in order to uncover regularities in the way that phonological features appear (or do not appear) in ASL signs and how these patterns are influenced by sign properties such as frequency and iconicity. A third aim is to conduct experiments to determine the psychological reality of these phonological patterns (e.g., do signers unconsciously know which patterns are common and which are rare?) and to discover how phonological and lexical properties impact how quickly a sign is recognized (using a novel sign recognition technique) and produced (using a picture-naming task). Data from these experiments and related materials (e.g. picture stimuli) will be made available to the public through ASL-LEX. These materials constitute essential tools that will allow scientists and educators to create well-controlled ASL stimuli for use in research and the classroom. ASL-LEX can also be used by educators and early intervention specialists to develop benchmarks for assessing vocabulary development in signing children, (e.g., do children know the most frequent signs?) and to support literacy development (e.g., to find sign-based \"rhymes\"). A parallel aim of the project is to increase the representation of deaf people in science by including deaf researchers on the project and by providing an accessible environment for deaf students to gain training and research experience.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Naomi",
   "pi_last_name": "Caselli",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Naomi Caselli",
   "pi_email_addr": "nkc@bu.edu",
   "nsf_id": "000715234",
   "pi_start_date": "2016-08-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "881 Commonwealth Avenue",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151300",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "7298",
   "pgm_ref_txt": "COLLABORATIVE RESEARCH"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 191563.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Understanding the structure and organization of the human lexicon is critical to both linguistic and psycholinguistic theories of language. &nbsp;Theories of the human lexicon have been based almost entirely on studies of spoken languages, in part because such advances depend upon a detailed information each item in the lexicon, and the necessary tools for these types of analyses have been unavailable for sign languages. As a consequence, linguistic properties that are pervasive in signed language but limited in speech played a marginal role in theories of language. This project represents the first large-scale quantitative analysis of the lexicon of American Sign Language (ASL), ASL-LEX, which in turn has enabled investigation of both universal and modality-specific properties of the mental lexicon.</p>\n<p>ASL-LEX is the first large-scale lexical database for ASL, and includes more than 100 pieces of information about each of 2,723 signs. For each sign, ASL-LEX now includes a detailed phonological description, phonological density and complexity measures, frequency ratings (from deaf signers), iconicity ratings (from hearing non-signers and deaf signers), transparency (\"guessability\") ratings (from non-signers), sign and videoclip durations, lexical class, and more.</p>\n<p>The ASL-LEX database is publicly available via an interactive online website that enables users to easily browse the ASL lexicon <a href=\"http://asl-lex.org/\">http://asl-lex.org/</a>. As depicted in Figure 1, signs are represented visually by nodes, with larger nodes indicating signs with higher frequency. Signs are organized into neighborhoods, where proximity between nodes indicates their similarity based on their phonological composition (e.g., locations, movements, handshapes). As illustrated in Figure 2, users can toggle between this network view and a scatterplot view in order to explore the relationships between different lexical and phonological features.&nbsp;&nbsp;</p>\n<p>Analyses of the lexicon revealed a complex interplay between iconicity, frequency, and phonological features, and highlighted a possible role of iconicity in shaping and structuring the lexicon. Frequent signs may become phonologically simpler and less iconic, while there might be specific pressures from iconicity on certain signs to remain phonologically complex. Such patterns might not have been detectable with smaller datasets.</p>\n<p>ASL-LEX has not only enabled investigation of these questions, it already been used by dozens of other research teams in stimuli development and/or analyses in research across a variety of fields including psycholinguistics, language acquisition, neuroscience, and computer science. Several lexical databases of other sign languages (e.g., German Sign Language, Israeli Sign Language, Spanish Sign Language, and a database of several African Sign Languages) have been modeled, in part, after ASL-LEX. &nbsp;This will enable cross-linguistic investigations of lexical and phonological patterns.</p>\n<p>ASL-LEX is also designed to be interoperable with other ASL resources including an ASL vocabulary test (the ASL-CDI), and two other lexical databases: ASL WordNet which catalogs semantic information about a subset of signs, and ASL SignBank which is used for coding naturalistic corpora in ASL. These integrations multiply the utility of all of these datasets, and enable investigations across domains (e.g., What lexical, phonological, and semantic properties of signs are children sensitive to when learning ASL vocabulary? How are phonological properties of signs related to semantic properties of signs?). Work is underway to link ASL-LEX to other datasets that would enable use of machine learning and artificial intelligence techniques in pursuit of automatic sign recognition and translation technology.</p>\n<p>ASL-LEX is an essential tool that will enable scientific inquiry about the human lexicon, vocabulary acquisition, sign recognition, and sign production.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/27/2020<br>\n\t\t\t\t\tModified by: Naomi&nbsp;Caselli</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1625793/1625793_10449990_1595883841532_FigureS6_Screenshots--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1625793/1625793_10449990_1595883841532_FigureS6_Screenshots--rgov-800width.jpg\" title=\"ASL-LEX Website Figure 1\"><img src=\"/por/images/Reports/POR/2020/1625793/1625793_10449990_1595883841532_FigureS6_Screenshots--rgov-66x44.jpg\" alt=\"ASL-LEX Website Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The ASL-LEX website. Each node represents a sign. Signs that share many phonological features are connected by an edge. Coloring is used to highlight related signs. Hovering over a node allows the user to see the sign, and several possible English translations from ASL SignBank (left panel).</div>\n<div class=\"imageCredit\">www.ASL-LEX.org</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Naomi&nbsp;Caselli</div>\n<div class=\"imageTitle\">ASL-LEX Website Figure 1</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1625793/1625793_10449990_1595883926885_FigureS7_Screenshots--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1625793/1625793_10449990_1595883926885_FigureS7_Screenshots--rgov-800width.jpg\" title=\"ASL-LEX Website Figure 2\"><img src=\"/por/images/Reports/POR/2020/1625793/1625793_10449990_1595883926885_FigureS7_Screenshots--rgov-66x44.jpg\" alt=\"ASL-LEX Website Figure 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Users can brush to select nodes from the network (left panel) to see scatterplots illustrating the relationship between the lexical properties of the selected signs (right panel). Users can also brush to select nodes from the scatterplots (right panel) see those signs in the network.</div>\n<div class=\"imageCredit\">www.asl-lex.org</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Naomi&nbsp;Caselli</div>\n<div class=\"imageTitle\">ASL-LEX Website Figure 2</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nUnderstanding the structure and organization of the human lexicon is critical to both linguistic and psycholinguistic theories of language.  Theories of the human lexicon have been based almost entirely on studies of spoken languages, in part because such advances depend upon a detailed information each item in the lexicon, and the necessary tools for these types of analyses have been unavailable for sign languages. As a consequence, linguistic properties that are pervasive in signed language but limited in speech played a marginal role in theories of language. This project represents the first large-scale quantitative analysis of the lexicon of American Sign Language (ASL), ASL-LEX, which in turn has enabled investigation of both universal and modality-specific properties of the mental lexicon.\n\nASL-LEX is the first large-scale lexical database for ASL, and includes more than 100 pieces of information about each of 2,723 signs. For each sign, ASL-LEX now includes a detailed phonological description, phonological density and complexity measures, frequency ratings (from deaf signers), iconicity ratings (from hearing non-signers and deaf signers), transparency (\"guessability\") ratings (from non-signers), sign and videoclip durations, lexical class, and more.\n\nThe ASL-LEX database is publicly available via an interactive online website that enables users to easily browse the ASL lexicon http://asl-lex.org/. As depicted in Figure 1, signs are represented visually by nodes, with larger nodes indicating signs with higher frequency. Signs are organized into neighborhoods, where proximity between nodes indicates their similarity based on their phonological composition (e.g., locations, movements, handshapes). As illustrated in Figure 2, users can toggle between this network view and a scatterplot view in order to explore the relationships between different lexical and phonological features.  \n\nAnalyses of the lexicon revealed a complex interplay between iconicity, frequency, and phonological features, and highlighted a possible role of iconicity in shaping and structuring the lexicon. Frequent signs may become phonologically simpler and less iconic, while there might be specific pressures from iconicity on certain signs to remain phonologically complex. Such patterns might not have been detectable with smaller datasets.\n\nASL-LEX has not only enabled investigation of these questions, it already been used by dozens of other research teams in stimuli development and/or analyses in research across a variety of fields including psycholinguistics, language acquisition, neuroscience, and computer science. Several lexical databases of other sign languages (e.g., German Sign Language, Israeli Sign Language, Spanish Sign Language, and a database of several African Sign Languages) have been modeled, in part, after ASL-LEX.  This will enable cross-linguistic investigations of lexical and phonological patterns.\n\nASL-LEX is also designed to be interoperable with other ASL resources including an ASL vocabulary test (the ASL-CDI), and two other lexical databases: ASL WordNet which catalogs semantic information about a subset of signs, and ASL SignBank which is used for coding naturalistic corpora in ASL. These integrations multiply the utility of all of these datasets, and enable investigations across domains (e.g., What lexical, phonological, and semantic properties of signs are children sensitive to when learning ASL vocabulary? How are phonological properties of signs related to semantic properties of signs?). Work is underway to link ASL-LEX to other datasets that would enable use of machine learning and artificial intelligence techniques in pursuit of automatic sign recognition and translation technology.\n\nASL-LEX is an essential tool that will enable scientific inquiry about the human lexicon, vocabulary acquisition, sign recognition, and sign production.\n\n\t\t\t\t\tLast Modified: 07/27/2020\n\n\t\t\t\t\tSubmitted by: Naomi Caselli"
 }
}