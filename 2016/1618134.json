{
 "awd_id": "1618134",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: AF: Small: Collaborative Research: Differentially Private Learning: From Theory to Applications",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 249940.0,
 "awd_amount": 249940.0,
 "awd_min_amd_letter_date": "2016-07-21",
 "awd_max_amd_letter_date": "2016-07-21",
 "awd_abstract_narration": "Practical privacy-preserving machine learning methods are currently of critical importance in medical, financial and consumer applications, among others. The aim of this project is to develop practical private machine learning algorithms that can be easily implemented by practitioners in any field that holds sensitive data, while keeping robust privacy guarantees. The proposed research will extend the existing rigorous theoretical guarantees of differential privacy to reach the requirements of modern machine learning algorithms in concrete practical settings. The generated intellectual merit therefore spans all the way from the theory to practical algorithms. The resulting methods have the potential to benefit existing real-world applications in many high impact domains. The PIs have several ongoing and successful collaborations with medical practitioners and researchers and will evaluate the resulting algorithms on real patient data in high impact medical applications. All algorithms will be made publicly available as open source. Both PIs are dedicated towards actively hiring minorities and involving undergraduate students in research.\r\n\r\nDifferential privacy (DP) is now recognized as one of the most rigorous and potentially usable notions of statistical privacy, and has become a full-fledged research field.  The aim of this research is to provide reliable privacy guarantees for practical machine learning algorithms, which is invaluable to protect individuals who volunteer their sensitive data for research purposes. We identify several areas with high impact potential and propose four concrete research thrusts. (1) Private Causal Inference. Causal inference is one of the most promising new directions in machine learning, that recently has become practical. Some of the most interesting causal questions deal however with medical or government policy data, which are inherently sensitive. We propose to unite the recent breakthroughs in both fields (causal inference and DP) and derive a practical and theoretically sound method to ensure differentially private causal inference. (2) Privacy for Bayesian Global Optimization. The success of deep learning has created a surge in popularity for Bayesian Global Optimization (BGO) for hyper-parameter tuning. Simultaneously, recent publications have tied the stability properties of differential privacy to generalization in adaptive data analysis. We propose to unite these recent developments and improve the generalization of BGO using insights from DP. Here, we are not protecting individuals from privacy leaks, but algorithms from overfitting-allowing for fine trade-offs of \"privacy\" vs. efficacy. (3) Private Communication-Efficient Distributed Learning. In response to the growth of data distributed over multiple machines, we aim to design practical private and communication-efficient algorithms for supervised and unsupervised learning problems. This work will build off our recent work on distributed learning and clustering algorithms. (4) Practical Private Active Learning. In the age of big data, there has been tremendous interest both in machine learning and its application areas on designing active learning algorithms that most efficiently utilize the available data, while minimizing the need for human intervention. Recently there have been exciting results on understanding statistical and computational principles (including work by the PIs). This research will develop new foundations and new practical well-founded active learning algorithms that are not only statistically and computationally efficient, but also differentially private.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kilian",
   "pi_last_name": "Weinberger",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kilian Weinberger",
   "pi_email_addr": "kilianweinberger@cornell.edu",
   "nsf_id": "000576980",
   "pi_start_date": "2016-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "410 Gates Hall",
  "perf_city_name": "Ithaca NY",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148535169",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 249940.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our research effort within this grant has focused on practical applications of theoretical machine learning frameworks in the realm of privacy and security.&nbsp;We have investigated practical defenses against adversarial attacks and also the feasibility of black-box attacks without access to a model's specifications.&nbsp;The result has been a series of successful research outcomes, all published at the very top machine learning conferences (ICML, Neurips, AISTATS, ICLR). We provide open source code for all our algorithms.&nbsp;As far as adversarial attacks are concerned, we have shown that black box attacks can be far simpler than previously imagined. There is no need to approximate the gradient of a model through repeated probing, but instead a simple random search for perturbations is typically more effective and can be orders of magnitudes faster. We refer to this approach as \"Simple Adversarial Black-box Attack (SIMBA)\".To defend against attacks, we show that natural images are particularly robust against Gaussian noise perturbations, and very vulnerable against adversarial perturbations. Incidentally, images that have experienced adversarial perturbations lose these properties and can therefore be detected. We show that this detection is very hard to circumvent.&nbsp;Another area of security in machine learning is the use of paper bidding for peer reviewing. We show that one can use robust machine learning algorithms to protect a conference or journal from attacks against the peer-review system. Essentially, paper bidding is used to train a robust machine learning system, which then is used to assign reviewers to papers. Instead of letting a potential reviewer influence their own assignments through bidding, we train a system on other reviewer's bids and generalize to this reviewer's assignment. This approach is inspired by theory on differential privacy, which guarantees that a classifier behaves the same, independent of whether it is trained on a given data instance or not. The final algorithm is very robust against attacks and is ready to be used by any peer-review conference or journal.&nbsp;During the duration of this grant, multiple graduate students were funded and have since then successfully graduated with their PhDs. The research effort has also lead to a new (graduate level) class at Cornell University on differential private algorithms and their applications.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/14/2021<br>\n\t\t\t\t\tModified by: Kilian&nbsp;Weinberger</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur research effort within this grant has focused on practical applications of theoretical machine learning frameworks in the realm of privacy and security. We have investigated practical defenses against adversarial attacks and also the feasibility of black-box attacks without access to a model's specifications. The result has been a series of successful research outcomes, all published at the very top machine learning conferences (ICML, Neurips, AISTATS, ICLR). We provide open source code for all our algorithms. As far as adversarial attacks are concerned, we have shown that black box attacks can be far simpler than previously imagined. There is no need to approximate the gradient of a model through repeated probing, but instead a simple random search for perturbations is typically more effective and can be orders of magnitudes faster. We refer to this approach as \"Simple Adversarial Black-box Attack (SIMBA)\".To defend against attacks, we show that natural images are particularly robust against Gaussian noise perturbations, and very vulnerable against adversarial perturbations. Incidentally, images that have experienced adversarial perturbations lose these properties and can therefore be detected. We show that this detection is very hard to circumvent. Another area of security in machine learning is the use of paper bidding for peer reviewing. We show that one can use robust machine learning algorithms to protect a conference or journal from attacks against the peer-review system. Essentially, paper bidding is used to train a robust machine learning system, which then is used to assign reviewers to papers. Instead of letting a potential reviewer influence their own assignments through bidding, we train a system on other reviewer's bids and generalize to this reviewer's assignment. This approach is inspired by theory on differential privacy, which guarantees that a classifier behaves the same, independent of whether it is trained on a given data instance or not. The final algorithm is very robust against attacks and is ready to be used by any peer-review conference or journal. During the duration of this grant, multiple graduate students were funded and have since then successfully graduated with their PhDs. The research effort has also lead to a new (graduate level) class at Cornell University on differential private algorithms and their applications. \n\n\t\t\t\t\tLast Modified: 12/14/2021\n\n\t\t\t\t\tSubmitted by: Kilian Weinberger"
 }
}