{
 "awd_id": "1636586",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Gaze Control during Scene Viewing: Behavioral and Computational Approaches",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Lawrence Gottlob",
 "awd_eff_date": "2015-10-30",
 "awd_exp_date": "2017-06-30",
 "tot_intn_awd_amt": 135071.0,
 "awd_amount": 135071.0,
 "awd_min_amd_letter_date": "2016-04-28",
 "awd_max_amd_letter_date": "2016-04-28",
 "awd_abstract_narration": "When we view the visual world, our eyes flit from one location to another about three times per second, in movements called saccades. Useful visual information is acquired only during fixations, brief periods of time when gaze rests on an object or scene feature. The cognitive and neural processes that direct saccades and fixations through a scene in real time fall under the term 'gaze control'. This project focuses on unraveling how human gaze control operates during active real-world scene perception.\r\n \r\nThis project approaches human gaze control by starting with the insight that understanding eye movement timing will provide key insight into the underlying cognitive and neural systems that control gaze. The research combines innovative eye-tracking methods with a working computational model that simulates eye movement control. In this research program, the empirical and computational threads are complementary and synergistic. On the one hand, we can test our understanding of gaze control by determining whether the model can produce eye movements that look like those produced by people. On the other hand, insights from the model can be used as a tool to enhance our theoretical understanding of gaze control, and these insights can be further tested with new experiments.\r\n \r\nThe results from this project will enhance basic scientific understanding of how humans perceive and understand the visual world. The project also has wide-ranging implications for the creation of new display technologies and machine interfaces that can be controlled by eye movements. And the results are relevant for the design of new artificial vision systems that actively track and focus on relevant information in the environment.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Henderson",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "John M Henderson",
   "pi_email_addr": "johnhenderson@ucdavis.edu",
   "nsf_id": "000104587",
   "pi_start_date": "2016-04-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "267 Cousteau Place",
  "perf_city_name": "Davis",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956186134",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 135071.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>When we view the visual world, our eyes flit from one location to another about three times each second. These frequent changes in gaze direction result from very fast eye movements that reorient the eyes through the scene. Useful visual information is acquired only during fixations, brief periods of time averaging about one third of a second when the eyes rests on an object or scene feature. Gaze control refers to the cognitive and neural processes that direct fixation through a scene in real time in the service of ongoing perception, cognition, and behavior. This project focused on unraveling how human gaze control operates during active real-world scene perception.</p>\n<p>Intellectual Merit: This project approached human gaze control by starting with the insight that understanding eye movement timing can provide key insight into the underlying cognitive and neural systems that control gaze. The research combined innovative eye-tracking methods with a working computational model that simulates eye movement control. In this research program, the empirical and computational threads were complementary and synergistic. On the one hand, we tested our theoretical understanding of gaze control by determining whether the model would produce eye movements that look like those produced by people. On the other hand, insights from the model were used as a tool to enhance our theoretical understanding of gaze control, and these insights were further tested with new experiments. This approach was extended to investigating gaze control in the brain using new neuroimaging methods deeloped under this project.</p>\n<p>Broader Impacts: The results from this project enhance basic scientific understanding of how humans perceive and understand the visual world. The project has wide-ranging implications for the creation of new display technologies and machine interfaces that can be controlled by eye movements. And the results are relevant for the design of new artificial vision systems that actively track and focus on relevant information in the environment.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/21/2017<br>\n\t\t\t\t\tModified by: John&nbsp;M&nbsp;Henderson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWhen we view the visual world, our eyes flit from one location to another about three times each second. These frequent changes in gaze direction result from very fast eye movements that reorient the eyes through the scene. Useful visual information is acquired only during fixations, brief periods of time averaging about one third of a second when the eyes rests on an object or scene feature. Gaze control refers to the cognitive and neural processes that direct fixation through a scene in real time in the service of ongoing perception, cognition, and behavior. This project focused on unraveling how human gaze control operates during active real-world scene perception.\n\nIntellectual Merit: This project approached human gaze control by starting with the insight that understanding eye movement timing can provide key insight into the underlying cognitive and neural systems that control gaze. The research combined innovative eye-tracking methods with a working computational model that simulates eye movement control. In this research program, the empirical and computational threads were complementary and synergistic. On the one hand, we tested our theoretical understanding of gaze control by determining whether the model would produce eye movements that look like those produced by people. On the other hand, insights from the model were used as a tool to enhance our theoretical understanding of gaze control, and these insights were further tested with new experiments. This approach was extended to investigating gaze control in the brain using new neuroimaging methods deeloped under this project.\n\nBroader Impacts: The results from this project enhance basic scientific understanding of how humans perceive and understand the visual world. The project has wide-ranging implications for the creation of new display technologies and machine interfaces that can be controlled by eye movements. And the results are relevant for the design of new artificial vision systems that actively track and focus on relevant information in the environment.\n\n\t\t\t\t\tLast Modified: 07/21/2017\n\n\t\t\t\t\tSubmitted by: John M Henderson"
 }
}