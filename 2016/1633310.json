{
 "awd_id": "1633310",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: Collaborative Research: From Visual Data to Visual Understanding",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 416000.0,
 "awd_min_amd_letter_date": "2016-08-22",
 "awd_max_amd_letter_date": "2017-05-12",
 "awd_abstract_narration": "The field of visual recognition, which focuses on creating computer algorithms for automatically understanding photographs and videos, has made tremendous gains in the past few years. Algorithms can now recognize and localize thousands of objects with reasonable accuracy as well as identify other visual content, such as scenes and activities. For instance, there are now smart phone apps that can automatically sift through a user's photos and find all party pictures, or all pictures of cars, or all sunset photos. However, the type of \"visual understanding\" done by these methods is still rather superficial, exhibiting mostly rote memorization rather than true reasoning. For example, current algorithms have a hard time telling if an image is typical (e.g., car on a road) or unusual (e.g., car in the sky), or answering simple questions about a photograph, e.g., \"what are the people looking at?\", \"what just happened?\", \"what might happen next?\" A central problem is that current methods lack the data about the world outside of the photograph. To achieve true human-like visual understanding, computers will have to reason about the broader spatial, temporal, perceptual, and social context suggested by a given visual input. This project is using big visual data to gather large-scale deep semantic knowledge about how events, physical and social interactions, and how people perceive the world and each other. The research focuses on developing methods to capture and represent this knowledge in a way that makes it broadly applicable to a range of visual understanding tasks. This will enable novel computer algorithms that have a deeper, more human-like, understanding of the visual world and can effectively function in complex, real-world situations and environments. For example, if a robot can predict what a person might do next in a given situation, then the robot can better aid the person in their task. Broader impacts will include new publicly-available software tools and data that can be used for various visual reasoning tasks. Additionally, the project will have a multi-pronged educational component, including incorporating aspects of the research in the graduate teaching curriculum, undergraduate and K-12 outreach, as well as special mentoring and focused events for advancement of women in computer science.\r\n\r\nThe main technical focus of this project is to advance computational recognition efforts toward producing a general human-like visual understanding of images and video that can function on previously unseen data, unseen tasks and settings. The aim of this project is to develop a new large-scale knowledge base called the visual Memex that extracts and stores vast set of visual relationships between data items in a multi-graph representation, with nodes corresponding to data items and edges indicating different types of relationships. This large knowledge base will be used in a lambda-calculus-powered reasoning engine to make inferences about visual data on a global scale. Additionally, the project will test computational recognition algorithms on several visual understanding tasks designed to evaluate progress on a variety of aspects of visual understanding, including: linguistic (evaluating our understanding about imagery through language tasks such as visual question-answering), to purely visual (evaluating our understanding of spatial context through visual fill-in-the-blanks), to temporal (evaluating our temporal understanding by predicting future states), to physical (evaluating our understanding of human-object and human-scene interactions by predicting affordances). Datasets, knowledge base, and evaluation tools will be hosted on the project web site (http://www.tamaraberg.com/grants/bigdata.html).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexei",
   "pi_last_name": "Efros",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Alexei A Efros",
   "pi_email_addr": "efros@eecs.berkeley.edu",
   "nsf_id": "000487848",
   "pi_start_date": "2016-08-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "724 Sutardja Dai Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947201776",
  "perf_ctry_code": "UP",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "Ukraine",
  "perf_ctry_flag": "0"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 400000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main technical focus of this collaborative project was to advance computational recognition efforts toward producing a general human-like visual understanding of images and video that can function on previously unseen data, unseen tasks and settings.&nbsp; The particular focus of the research conducted by the Berkeley team was in developing methods for finding visual correspondence between the various entities within big visual data collections.&nbsp; Such correspondences aim to serve as the building blocks for creating semantically meaningful visual representations out of raw visual data.&nbsp; &nbsp;The key outcomes of the project have been the development of novel algorithms for self-supervised learning of visual correspondences from unlabeled images (<em>Split-Brain Autoencoder</em>) and video (<em>TimeCycle</em> and <em>ContrastiveRandomWalk</em> algorithms), discovering and aligning visual patterns in art collections (<em>ArtMiner </em>and <em>RANSAC-Flow</em> algorithms), a computational method for modeling human visual perceptual similarity (<em>LPIPS</em> metric), and a series of methods for unpaired image-to-image translation (including the popular <em>CycleGAN</em> algorithm), among others.&nbsp; &nbsp;Based on this research, 12 scientific papers have been published at the top venues, which have already received over 14,000 citations, while the paper describing the <em>CycleGAN</em> algorithm is currently one of the ten most-cited papers in all of computer science in the last 5 years.&nbsp; In terms of broader impact, the <em>CycleGAN</em> algorithm is already being used in a wide range of fields (radiology, geology, robotics, data science, security, etc) as well as a tool for digital artists.&nbsp; Furthermore, the algorithm used in the<em>&nbsp;\"Everybody Dance Now\"</em>&nbsp;project is continuing to be developed as part of a digital dancing startup.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/30/2021<br>\n\t\t\t\t\tModified by: Alexei&nbsp;A&nbsp;Efros</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1633310/1633310_10453531_1627636792836_teaser_high_res--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1633310/1633310_10453531_1627636792836_teaser_high_res--rgov-800width.jpg\" title=\"CycleGAN algorithm\"><img src=\"/por/images/Reports/POR/2021/1633310/1633310_10453531_1627636792836_teaser_high_res--rgov-66x44.jpg\" alt=\"CycleGAN algorithm\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\", in IEEE International Conference on Computer Vision (ICCV), 2017.</div>\n<div class=\"imageCredit\">Zhu, Park, Isola, and Efros</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Alexei&nbsp;A&nbsp;Efros</div>\n<div class=\"imageTitle\">CycleGAN algorithm</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1633310/1633310_10453531_1627636920387_Everybody-Dance-Now--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1633310/1633310_10453531_1627636920387_Everybody-Dance-Now--rgov-800width.jpg\" title=\"Everybody Dance Now\"><img src=\"/por/images/Reports/POR/2021/1633310/1633310_10453531_1627636920387_Everybody-Dance-Now--rgov-66x44.jpg\" alt=\"Everybody Dance Now\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros, \"\"Everybody Dance Now\", ICCV 2019</div>\n<div class=\"imageCredit\">Chen, Ginosar, Zhou, Efros</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Alexei&nbsp;A&nbsp;Efros</div>\n<div class=\"imageTitle\">Everybody Dance Now</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1633310/1633310_10453531_1627637100530_teaser_animation(1)--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1633310/1633310_10453531_1627637100530_teaser_animation(1)--rgov-800width.jpg\" title=\"Contrastive Random Walk algorithm\"><img src=\"/por/images/Reports/POR/2021/1633310/1633310_10453531_1627637100530_teaser_animation(1)--rgov-66x44.jpg\" alt=\"Contrastive Random Walk algorithm\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Allan Jabri, Andrew Owens, Alexei A. Efros.Space-Time Correspondence as a Contrastive Random Walk.NeurIPS 2020</div>\n<div class=\"imageCredit\">Jabri, Owens, Efros</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Alexei&nbsp;A&nbsp;Efros</div>\n<div class=\"imageTitle\">Contrastive Random Walk algorithm</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe main technical focus of this collaborative project was to advance computational recognition efforts toward producing a general human-like visual understanding of images and video that can function on previously unseen data, unseen tasks and settings.  The particular focus of the research conducted by the Berkeley team was in developing methods for finding visual correspondence between the various entities within big visual data collections.  Such correspondences aim to serve as the building blocks for creating semantically meaningful visual representations out of raw visual data.   The key outcomes of the project have been the development of novel algorithms for self-supervised learning of visual correspondences from unlabeled images (Split-Brain Autoencoder) and video (TimeCycle and ContrastiveRandomWalk algorithms), discovering and aligning visual patterns in art collections (ArtMiner and RANSAC-Flow algorithms), a computational method for modeling human visual perceptual similarity (LPIPS metric), and a series of methods for unpaired image-to-image translation (including the popular CycleGAN algorithm), among others.   Based on this research, 12 scientific papers have been published at the top venues, which have already received over 14,000 citations, while the paper describing the CycleGAN algorithm is currently one of the ten most-cited papers in all of computer science in the last 5 years.  In terms of broader impact, the CycleGAN algorithm is already being used in a wide range of fields (radiology, geology, robotics, data science, security, etc) as well as a tool for digital artists.  Furthermore, the algorithm used in the \"Everybody Dance Now\" project is continuing to be developed as part of a digital dancing startup.\n\n \n\n\t\t\t\t\tLast Modified: 07/30/2021\n\n\t\t\t\t\tSubmitted by: Alexei A Efros"
 }
}