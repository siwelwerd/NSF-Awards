{
 "awd_id": "1563113",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Medium: PRISM: Platform for Rapid Investigation of efficient Scientific-computing & Machine-learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 960000.0,
 "awd_amount": 960000.0,
 "awd_min_amd_letter_date": "2016-08-03",
 "awd_max_amd_letter_date": "2017-07-18",
 "awd_abstract_narration": "Today's systems demand acceleration in processing and learning using massive datasets. Unfortunately, because of poor energy scaling and power limits, performance and power improvements due to technology scaling and instruction level parallelism in general-purpose processors have ended. It is well known that full custom, application-specific hardware accelerators can provide orders-of-magnitude improvements in energy/op for a variety of application domains. Therefore, there is a special interest in systems that can optimize and accelerate the building blocks of machine learning and data science routines. Many of these building blocks share the same characteristics as building blocks of high performance computing kernels working on matrices. \r\n\r\nSuch application specific solutions rely on joint optimization of algorithms and the hardware, but cost hundreds of millions of dollars. PRISM (Platform for Rapid Investigation of efficient Scientific- computing and Machine-learning accelerators) is proposed to amortize these costs. PRISM enables application designers to get rapid feedback about both the available parallelism and locality of their algorithm, and the efficiency of the resulting application/hardware design. PRISM platform consists of two coupled tools that incorporate design knowledge at both the hardware and algorithm level. This knowledge enables the tool to give application designers the ability to quickly evaluate the performance of their applications on the proposed/existing hardware, without the application designer needing to be an expert at hardware or algorithms. This platform will leverage tools created from the team's prior research. \r\n\r\nInitially, these tools will be used to create an efficient solution for each application, followed by a comparison of the resulting hardware designs. The possibility of creating platforms that span multiple classes of algorithms can then be explored. Finally, a comparison of these new architectures to existing heterogeneous architectures with GPUs and FPGAs will be made, to gain understanding about what modifications are necessary for these architectures to achieve higher levels of efficiency when supporting these classes of algorithms. The work on key applications will lead to better insight about the computation and communication intrinsic to these computations, and provide algorithms for these applications that will be effective on conventional and new architectures.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Horowitz",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Mark A Horowitz",
   "pi_email_addr": "horowitz@stanford.edu",
   "nsf_id": "000235625",
   "pi_start_date": "2016-08-03",
   "pi_end_date": "2017-07-18"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Oyekunle",
   "pi_last_name": "Olukotun",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Oyekunle A Olukotun",
   "pi_email_addr": "kunle@stanford.edu",
   "nsf_id": "000320046",
   "pi_start_date": "2017-07-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Jack",
   "pi_last_name": "Poulson",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Jack L Poulson",
   "pi_email_addr": "jpoulson@cc.gatech.edu",
   "nsf_id": "000668727",
   "pi_start_date": "2016-08-03",
   "pi_end_date": "2016-09-12"
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943054100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 960000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this research project was to evaluate the feasibility of creating a single accelerator design for machine learning and high-performance computing (HPC) applications.&nbsp;We investigated the performance impact of memory hierarchy organization in the overall accelerator system design.&nbsp;&nbsp;To minimize the performance lost in the memory hierarchy, we showed how structured sparsity in machine learning applications can be exploited to eliminate computation and reduce memory traffic. Using a simulator developed for the accelerator, we examined the feasibility of mapping&nbsp;singular value decomposition (SVD), which is an important HPC computation kernel. We investigated the impact of sparse training on training convergence and model accuracy. We showed that sparse training can improve the convergence accuracy of large networks like Resnet-50. The overall result of the project showed that sparse computation accelerators can be developed to support sparse machine learning and HPC applications. The research resulted in a journal paper, two conference presentations (1 best paper award), and one ArXiv paper. These publications have already been cited by other researchers over 300 times.</p>\n<p>&nbsp;</p>\n<p>The boarder impacts of this research are a new Stanford University course (Hardware accelerators for machine learning CS217) and three tutorials on designing accelerators for machine learning that were presented at the International Symposium of Computer Architecture (ISCA).</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/29/2022<br>\n\t\t\t\t\tModified by: Oyekunle&nbsp;A&nbsp;Olukotun</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this research project was to evaluate the feasibility of creating a single accelerator design for machine learning and high-performance computing (HPC) applications. We investigated the performance impact of memory hierarchy organization in the overall accelerator system design.  To minimize the performance lost in the memory hierarchy, we showed how structured sparsity in machine learning applications can be exploited to eliminate computation and reduce memory traffic. Using a simulator developed for the accelerator, we examined the feasibility of mapping singular value decomposition (SVD), which is an important HPC computation kernel. We investigated the impact of sparse training on training convergence and model accuracy. We showed that sparse training can improve the convergence accuracy of large networks like Resnet-50. The overall result of the project showed that sparse computation accelerators can be developed to support sparse machine learning and HPC applications. The research resulted in a journal paper, two conference presentations (1 best paper award), and one ArXiv paper. These publications have already been cited by other researchers over 300 times.\n\n \n\nThe boarder impacts of this research are a new Stanford University course (Hardware accelerators for machine learning CS217) and three tutorials on designing accelerators for machine learning that were presented at the International Symposium of Computer Architecture (ISCA).\n\n \n\n\t\t\t\t\tLast Modified: 07/29/2022\n\n\t\t\t\t\tSubmitted by: Oyekunle A Olukotun"
 }
}