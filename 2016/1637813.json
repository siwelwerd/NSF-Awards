{
 "awd_id": "1637813",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: Learning Adaptive Representations for Robust Mobile Robot Navigation from Multi-Modal Interactions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 289376.0,
 "awd_amount": 289376.0,
 "awd_min_amd_letter_date": "2016-08-10",
 "awd_max_amd_letter_date": "2016-08-10",
 "awd_abstract_narration": "Most existing autonomous systems reason over flat, task-dependent models of the world that do not scale to large, complex environments. This lack of scalability and generalizability is a significant barrier to the widespread adoption of robots for common tasks. This research will advance the state-of-the-art in robot perception, natural language understanding, and learning to develop new models and algorithms that significantly improve the scalability and efficiency of mapping and motion planning in large, complex environments. These contributions will impact the next generation of autonomous systems that interact with humans in many domains, including manufacturing, healthcare, and exploration. Outcomes will include the release of open source software and data, workshops, K-12 STEM outreach efforts, and undergraduate and graduate education in the unique, multidisciplinary fields of perception, natural language understanding, and motion planning. \r\n\r\nAs robots perform a wider variety of tasks within increasingly complex environments, their ability to learn and reason over expressive models of their environment becomes critical. The goal of this research is to develop models and algorithms for learning adaptive, hierarchical environment representations that afford efficient planning for mobility tasks. These representations will take the form of probabilistic models that capture the rich spatial-semantic properties of the robot's environment and are factorable to enable scalable inference. This research will develop algorithms that learn and adapt these representations by fusing knowledge conveyed through human-provided natural language utterances with information extracted from the robot's multimodal sensor streams. This research will develop algorithms that then reason over the complexity of these models in the context of the inferred task, thereby identifying simplifications that enable more efficient robot motion planning. \r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Howard",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas Howard",
   "pi_email_addr": "thomas.howard@rochester.edu",
   "nsf_id": "000681387",
   "pi_start_date": "2016-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "",
  "perf_city_name": "Rochester",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146270251",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 289376.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Current approaches for linguistic interaction with collaborative mobile robots rely on environment representations informed by rigid perception pipelines designed to classify a specific set of objects and provide a fixed level of detail.&nbsp; This research project explored topics that included language-guided adaptive perception, language understanding in partially observed environments, and adaptive recombinant motion planning search spaces for efficient human-robot interaction and mobile robot navigation.&nbsp; This work, performed in conjunction with researchers at the Toyota Technological Institute at Chicago, has enabled collaborative robots to adapt and inform their representation of the environment to interpret the meaning of instructions more efficiently, eliminate object classifications that are not salient to the inferred task, and more efficiently explore a priori unknown environments.&nbsp;&nbsp;</p>\n<p>The following summarizes the specific outcomes of this project:</p>\n<p><strong>Language-Guided Adaptive Perception:</strong> We developed a novel approach to symbol grounding informed by a perceived model of the world that makes inference in cluttered environments more efficient.&nbsp;&nbsp; This model uses language in several different ways.&nbsp; First it uses language to infer what subset of classifiers and/or observations are necessary to construct a minimal but sufficient environment representation to understand the meaning of an utterance.&nbsp; Then it uses this compact representation of the environment to perform symbol grounding more efficiently than an overly detailed representation that contains information that is irrelevant to the meaning of the statement.&nbsp;</p>\n<p><strong>Language Understanding in Partially Observed Environments:</strong> We developed and explored new ways of using the language-guided adaptive perception pipeline for symbol grounding in partially observed environments.&nbsp; This research uses language in yet another way, treating language as a sensing modality, to form observations that fuse with visual observations to produce a map suitable for performing operations that may require exploration.&nbsp; Physical experiments on a mobile manipulator demonstrated the ability to use spatial references contained within the utterance to hypothesize the presence of unseen objects to guide exploration during execution of desired actions.</p>\n<p><strong>Adaptive Recombinant Motion Planning Search Spaces:</strong> We developed a novel extension to adaptive recombinant motion planning search spaces called Predictively Adaptive State Lattices that used learned heuristics to inform whether to perform node optimization based on the aggregate cost of edge connectivity during heuristic search.&nbsp;</p>\n<p><strong>Transition and Training:</strong> This project produced five refereed conference papers, one workshop/symposium paper, one workshop abstract, and one journal article accepted for publication.&nbsp; &nbsp;Software contributions were also made to an open-source software project that enables language-guided adaptive perception.&nbsp; This research project supported one PhD student and two undergraduate research assistants, both of whom are now PhD students studying robotics at other institutions.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2021<br>\n\t\t\t\t\tModified by: Thomas&nbsp;Howard</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nCurrent approaches for linguistic interaction with collaborative mobile robots rely on environment representations informed by rigid perception pipelines designed to classify a specific set of objects and provide a fixed level of detail.  This research project explored topics that included language-guided adaptive perception, language understanding in partially observed environments, and adaptive recombinant motion planning search spaces for efficient human-robot interaction and mobile robot navigation.  This work, performed in conjunction with researchers at the Toyota Technological Institute at Chicago, has enabled collaborative robots to adapt and inform their representation of the environment to interpret the meaning of instructions more efficiently, eliminate object classifications that are not salient to the inferred task, and more efficiently explore a priori unknown environments.  \n\nThe following summarizes the specific outcomes of this project:\n\nLanguage-Guided Adaptive Perception: We developed a novel approach to symbol grounding informed by a perceived model of the world that makes inference in cluttered environments more efficient.   This model uses language in several different ways.  First it uses language to infer what subset of classifiers and/or observations are necessary to construct a minimal but sufficient environment representation to understand the meaning of an utterance.  Then it uses this compact representation of the environment to perform symbol grounding more efficiently than an overly detailed representation that contains information that is irrelevant to the meaning of the statement. \n\nLanguage Understanding in Partially Observed Environments: We developed and explored new ways of using the language-guided adaptive perception pipeline for symbol grounding in partially observed environments.  This research uses language in yet another way, treating language as a sensing modality, to form observations that fuse with visual observations to produce a map suitable for performing operations that may require exploration.  Physical experiments on a mobile manipulator demonstrated the ability to use spatial references contained within the utterance to hypothesize the presence of unseen objects to guide exploration during execution of desired actions.\n\nAdaptive Recombinant Motion Planning Search Spaces: We developed a novel extension to adaptive recombinant motion planning search spaces called Predictively Adaptive State Lattices that used learned heuristics to inform whether to perform node optimization based on the aggregate cost of edge connectivity during heuristic search. \n\nTransition and Training: This project produced five refereed conference papers, one workshop/symposium paper, one workshop abstract, and one journal article accepted for publication.   Software contributions were also made to an open-source software project that enables language-guided adaptive perception.  This research project supported one PhD student and two undergraduate research assistants, both of whom are now PhD students studying robotics at other institutions. \n\n\t\t\t\t\tLast Modified: 12/30/2021\n\n\t\t\t\t\tSubmitted by: Thomas Howard"
 }
}