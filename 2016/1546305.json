{
 "awd_id": "1546305",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Collaborative Research: IA: Quantifying Plankton Diversity with Taxonomy and Attribute Based Classifiers of Underwater Microscope Images",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Elizabeth Blood",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2019-09-30",
 "tot_intn_awd_amt": 283372.0,
 "awd_amount": 283372.0,
 "awd_min_amd_letter_date": "2016-09-14",
 "awd_max_amd_letter_date": "2016-09-14",
 "awd_abstract_narration": "Plankton play an essential role in the global ecosystem, forming the base of marine food webs, linking the atmosphere to the deep ocean, and regulating a myriad of ecologically and climatologically important processes. Despite their importance, however, the technology to assess abundances and distributions of plankton has been limited. Changes in abundances of individual species are particularly poorly resolved; this includes the harmful algal blooms that have profound economic, societal, and ecosystem effects in many coastal systems. Traditional tools such as nets and bottles can destroy fragile organisms during sampling. Underwater microscopes, on the other hand, allow observation of the organisms undisturbed, and in their natural setting. New underwater microscopes are generating many thousands of high-resolution images of individual plankton each day. Before these images can be used for scientific analyses, the imaged organisms must be identified and classified. However, the vast number of images generated by such microscopes has led to a serious bottleneck: identification and classification of the images takes an impossibly long time for individuals to accomplish. Fortunately, advances in computer vision science have shown great promise in accurately performing such classification tasks. The main goal of this award is to explore and develop computer vision approaches for plankton image classification. A team of instrumentation specialists, an ocean ecologist, and a computer scientist, including two graduate students and one post doctoral student, will formulate, implement, and test methods to advance the goal of efficient and accurate automated plankton image classification. The advances made in this award will enable both improved classification algorithms in computer science, and vast new data streams for plankton ecology.\r\n\r\nPlankton form the base of marine food webs, link the atmosphere to the deep ocean, and regulate  global biogeochemical cycles. Plankton are often studied either through bulk measures, or by manual enumeration of individual taxa. Novel underwater microscope systems such as the Scripps Plankton Camera System (SPCS) are generating tens of thousands of images of individual plankton daily. However, without accurate annotation of the images, the potential science is limited. This project will explore the use of many-layer, deep Convolutional Neural Nets (CNN) as automated computer recognition methods; these techniques hold promise for classifying the nearly one trillion underwater microscope images that have been collected by a variety of research groups around the globe. The primary source of images will be a pair of microscopes that have been operating for 2 years from the Scripps Inst. of Oceanography's pier, yielding 200 million regions of interest. The project will build a large data base of training sets using a novel approach: a bench-top imaging system that is capable of rapidly producing thousands of annotated images showing organisms in all orientations and configurations identical to that in the field. Based on these automatically collected training sets, and hand annotation of in situ images from experts, a deep  (many layer) CNN will embed taxonomic and attribute constraints, and will be used to classify the organisms imaged. With success, this massive, growing, taxonomically classified dataset will enable unprecedented, transformative, taxon-specific explorations of the dynamics of the planktonic ecosystem on time scales from hours to decades.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nuno",
   "pi_last_name": "Vasconcelos",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Nuno M Vasconcelos",
   "pi_email_addr": "nuno@ece.ucsd.edu",
   "nsf_id": "000104017",
   "pi_start_date": "2016-09-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "116500",
   "pgm_ele_name": "ADVANCES IN BIO INFORMATICS"
  },
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 283372.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p style=\"text-align: left;\"><span>This project aimed to develop technology to enable biologists to automatically estimate densities of plankton, in real time, from underwater cameras. This was accomplished with recourse to artificial intelligence techniques, based on deep learning. In particular, the major goals of the project were to collect datasets of plankton images and train image classifiers based on deep learning to automatically classify them. The image collection leveraged the Scripps Plankton Camera System, which is currently deployed in one of the piers of the Scripps Institute of Oceanography and collecting thousands of plankton images per day. </span></p>\n<p>The major outcomes of the project were the&nbsp;development of a new protocol for image labeling and new deep learning classifiers that address specific plankton challenges. The labeling protocol was composed of two major elements. The first is the collection of images themselves, which is performed through a new set-up where plankton specimens are imaged in vivo, using a microscope in the lab. This allows the collection of thousands of images of each species under a large diversity of poses, specimen age, etc. The second is a protocol to label the images, using a combination of annotations by non-experts on Amazon Mechanical Turk and biologists.</p>\n<p>The deep learning classifiers&nbsp;are based on a new convolutional neural network (CNN) architecture, which leverage plankton attributes as constraints for recognition. Attributes are properties such as the existence of antenae, specimen size, etc. Two encodings of attribute constraints are used for network training. The first is a loss-based regularizer that introduces a generalization constraint on each attribute predictor. The second is a codeword regularizer that favors attribute-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art in attribute recognition were achieved on several datasets. We have also shown that the inclusion of attributes enhances the classification of plankton images and eables the transfer to classes for which there is no training data.</p>\n<p><span>Using these classifiers, we then studied the role of pose invariance in image recognition and retrieval. This is important for plankton classification, because plankton specimens can approach an underwater camera at any orientation. A taxonomic classification of embeddings, according to their level of invariance, was introduced and used to clarify connections between existing embeddings, identify missing approaches, and propose invariant generalizations. This lead to a new family of pose invariant embeddings (PIEs), derived from existing approaches by a combination of two models, which follow from the interpretation of CNNs as estimators of class posterior probabilities: a view-to-object model and an object-to-class model. The new pose-invariant models were shown to have interesting properties, both theoretically and through experiments, where they outperformed existing multiview approaches. Most notably, they achieve good performance for both 1) classification and retrieval, and 2) single and multiview inference.</span></p>\n<p>Altogether, the project sucessfully advanced the state of the art for real-time classification of plankton specimans by addressing both the collection of images and training sets for plankton classification and the design of powerfull AI systems to perform this classification. We believe that these contributions will be of use to marine biologists, allowing more precise measurements of plankton density than those possible with today's technology. This would enhance the accuracy of studies relating plankton densities to atmospheric phenomena and could be used for the detection of important biological events, such as algae blooms or red tides.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/13/2020<br>\n\t\t\t\t\tModified by: Nuno&nbsp;M&nbsp;Vasconcelos</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "This project aimed to develop technology to enable biologists to automatically estimate densities of plankton, in real time, from underwater cameras. This was accomplished with recourse to artificial intelligence techniques, based on deep learning. In particular, the major goals of the project were to collect datasets of plankton images and train image classifiers based on deep learning to automatically classify them. The image collection leveraged the Scripps Plankton Camera System, which is currently deployed in one of the piers of the Scripps Institute of Oceanography and collecting thousands of plankton images per day. \n\nThe major outcomes of the project were the development of a new protocol for image labeling and new deep learning classifiers that address specific plankton challenges. The labeling protocol was composed of two major elements. The first is the collection of images themselves, which is performed through a new set-up where plankton specimens are imaged in vivo, using a microscope in the lab. This allows the collection of thousands of images of each species under a large diversity of poses, specimen age, etc. The second is a protocol to label the images, using a combination of annotations by non-experts on Amazon Mechanical Turk and biologists.\n\nThe deep learning classifiers are based on a new convolutional neural network (CNN) architecture, which leverage plankton attributes as constraints for recognition. Attributes are properties such as the existence of antenae, specimen size, etc. Two encodings of attribute constraints are used for network training. The first is a loss-based regularizer that introduces a generalization constraint on each attribute predictor. The second is a codeword regularizer that favors attribute-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art in attribute recognition were achieved on several datasets. We have also shown that the inclusion of attributes enhances the classification of plankton images and eables the transfer to classes for which there is no training data.\n\nUsing these classifiers, we then studied the role of pose invariance in image recognition and retrieval. This is important for plankton classification, because plankton specimens can approach an underwater camera at any orientation. A taxonomic classification of embeddings, according to their level of invariance, was introduced and used to clarify connections between existing embeddings, identify missing approaches, and propose invariant generalizations. This lead to a new family of pose invariant embeddings (PIEs), derived from existing approaches by a combination of two models, which follow from the interpretation of CNNs as estimators of class posterior probabilities: a view-to-object model and an object-to-class model. The new pose-invariant models were shown to have interesting properties, both theoretically and through experiments, where they outperformed existing multiview approaches. Most notably, they achieve good performance for both 1) classification and retrieval, and 2) single and multiview inference.\n\nAltogether, the project sucessfully advanced the state of the art for real-time classification of plankton specimans by addressing both the collection of images and training sets for plankton classification and the design of powerfull AI systems to perform this classification. We believe that these contributions will be of use to marine biologists, allowing more precise measurements of plankton density than those possible with today's technology. This would enhance the accuracy of studies relating plankton densities to atmospheric phenomena and could be used for the detection of important biological events, such as algae blooms or red tides.\n\n \n\n\t\t\t\t\tLast Modified: 02/13/2020\n\n\t\t\t\t\tSubmitted by: Nuno M Vasconcelos"
 }
}