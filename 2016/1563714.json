{
 "awd_id": "1563714",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: Medium: Collaborative Research: Econometric Inference and Algorithmic Learning in Games",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2016-04-01",
 "awd_exp_date": "2022-03-31",
 "tot_intn_awd_amt": 701267.0,
 "awd_amount": 701267.0,
 "awd_min_amd_letter_date": "2016-03-29",
 "awd_max_amd_letter_date": "2019-03-26",
 "awd_abstract_narration": "Classical work on economic analysis of the interactions of strategic agents starts with players that have valuations for outcomes, such as items or sets of items they may win in an auction, and analyzes equilibria of the resulting game, where players optimize their strategies to improve their outcomes. To empirically test the prediction of such a theory, one needs to recover valuations of the players. Most econometric methods used to recover valuations rely on the assumption that the game is at a stable equilibrium (known as a Nash equilibrium). It is not surprising that such a framework provides a poor fit to the data in changing or new markets. At the same time, there is a growing theoretical literature in algorithmic game theory that allows one to study games where the game is not at a stable equilibrium. The PIs' program focuses on developing a methodology for inference without relying on the standard notions of the stability of outcomes in dynamically changing environments, such as online auctions. The goal of this project is to develop a theory that allows the researchers to take advantage of new dynamic data sets from electronic markets available on the Internet, and using the findings from the data to further the underlying theory.\r\n \r\nThe results of the project are intended to enable to application and development of Data Science tools for analysis and prediction in  non-stable and new market settings. This will affect a broad community of empirical researchers such as market analysts, by allowing them to study economic markets that have previously been considered hard or impossible to analyze.\r\n\r\nThe research program is based on using the theoretical results from algorithmic game theory on game outcomes when players use no-regret learning rules and combine these results with econometric techniques that allow one to estimate the best responses of players from the data using a set of non-parametric estimation techniques. The goal of the program, which PIs initiated in a paper in the ACM Conference on Economics and Computation in 2014, is to combine these approaches to develop a set of analytic tools for empirical analysis of games in non-equilibrium settings. Algorithmic game theory helps one to characterize the properties of outcomes in games (such as approximating factors for revenue and welfare in various cases), where the game is not at a stable equilibrium, assuming players use strategies that guarantee a certain no-regret property in place of the stronger equilibrium best response assumption. The project is aimed at combining the insights from algorithmic game theory with econometric methods to enable the analysis of dynamic markets. The intellectual merit of the project is twofold: (i) providing a methodology for inference in games (i.e., estimation of the payoff functions of players and the distribution of player types) in cases where the players use general classes of learning strategies; (ii) providing tools for the analysis of outcomes in non-equilibrium environments, including the analysis of statistical properties of the outcomes constructed using inferred preferences and types.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eva",
   "pi_last_name": "Tardos",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eva Tardos",
   "pi_email_addr": "eva@cs.cornell.edu",
   "nsf_id": "000443465",
   "pi_start_date": "2016-03-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "Gates Hall",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148537501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7932",
   "pgm_ref_txt": "COMPUT GAME THEORY & ECON"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 343979.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 175847.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 181441.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Previous work in the area, by both the PIs as well as others have considered learning outcomes in repeated games and inferring incentives of the participant from the interaction data. A very strong assumption in all the previous work is that at each iteration involves a fresh copy of the same interaction is played, that is, there is no effect of previous iterations on the game itself, beyond what the players learn from them. This is a very strong assumption and is rarely true in repeated interaction. For a simple example: consider routing: if the time scale considered is the day-to-day rush hour traffic, then the assumption seems well founded: no matter how bad Monday's rush-hour was, the cars are all gone by Tuesday morning. However, the same is not true for the millisecond-by-millisecond packet traffic on the internet. If a router is congested, a few milliseconds later the same packets are either still there or some of them have been dropped and are being resent. That is, in this case, there is a strong carry-over effect: the same packets are causing congestion. The same is true in repeated auctions when buyers have budgets (which is almost always the case): the player winning an auction has less money in the future, a direct effect of the outcome on future rounds. We worked on this issue in two different context: both in queuing systems as well as in repeated auctions.&nbsp;The goal of the project on learning in repeated games&nbsp;was to understand learning outcomes in such games with carry-over effect, understand what are the right ways to learn in this case (e.g., is no-regret learning commonly used in the repeated games literature still the right way to learn), and then finally use this knowledge to infer players incentives from how they play. A standard model for learning used in theoretical works is the assumption that learners involved in repeated interaction satisfy the no-regret condition: do at least as well as any single action would have done in hindsight, if used consistently. In the context of a queuing system, we show that no-regret learning is still reasonably efficient. Given increased system capacity, players with no-regret learning behavior can get their packets served. Unfortunately, we also show that no-regret is a someone myopic way to evaluate experience but have not (yet) identified a learning paradigm that would be doing better. We continue to actively work on this issue.</p>\n<p>In a different direction we considered a fair resource sharing game modeling how shared systems, such as Amazon's cloud computing system, can share resources. We show that a natural and simple sharing system is only approximately incentive compatible.&nbsp;The goal of the study of the fair sharing game&nbsp;related to the main goal of this project is to understand if players are likely to report truthfully in light of the fact that misreporting has dangers of large efficiency losses, but also have a chance of increased allocation.&nbsp;Our results so far measure the deviation from incentive compatibility of the system. We also have an active system implementation that will allow us to measure user behavior in the near future.&nbsp;</p>\n<p>We have also worked on pricing in ride-sharing networks, showing that a global convex optimization based pricing system, using dynamic pricing results in all (approximate) equilibria close to being socially optimal.&nbsp;The goal of the pricing project in ride-sharing is the show how one can make pricing robust to drivers who use learning, rather than the classical best response, that requires too much knowledge (or trust) to be a viable behavioral model.&nbsp;We show that all approximate equilibria have approximately optimal welfare is important for the learning context: it ensures that if all (or almost all) drivers satisfy a version of the no-regret learning criteria than such pricing results in good social outcome. This is as opposed to the static pricing version of such a system that has good equilibria but is not at all robust.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/06/2022<br>\n\t\t\t\t\tModified by: Eva&nbsp;Tardos</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nPrevious work in the area, by both the PIs as well as others have considered learning outcomes in repeated games and inferring incentives of the participant from the interaction data. A very strong assumption in all the previous work is that at each iteration involves a fresh copy of the same interaction is played, that is, there is no effect of previous iterations on the game itself, beyond what the players learn from them. This is a very strong assumption and is rarely true in repeated interaction. For a simple example: consider routing: if the time scale considered is the day-to-day rush hour traffic, then the assumption seems well founded: no matter how bad Monday's rush-hour was, the cars are all gone by Tuesday morning. However, the same is not true for the millisecond-by-millisecond packet traffic on the internet. If a router is congested, a few milliseconds later the same packets are either still there or some of them have been dropped and are being resent. That is, in this case, there is a strong carry-over effect: the same packets are causing congestion. The same is true in repeated auctions when buyers have budgets (which is almost always the case): the player winning an auction has less money in the future, a direct effect of the outcome on future rounds. We worked on this issue in two different context: both in queuing systems as well as in repeated auctions. The goal of the project on learning in repeated games was to understand learning outcomes in such games with carry-over effect, understand what are the right ways to learn in this case (e.g., is no-regret learning commonly used in the repeated games literature still the right way to learn), and then finally use this knowledge to infer players incentives from how they play. A standard model for learning used in theoretical works is the assumption that learners involved in repeated interaction satisfy the no-regret condition: do at least as well as any single action would have done in hindsight, if used consistently. In the context of a queuing system, we show that no-regret learning is still reasonably efficient. Given increased system capacity, players with no-regret learning behavior can get their packets served. Unfortunately, we also show that no-regret is a someone myopic way to evaluate experience but have not (yet) identified a learning paradigm that would be doing better. We continue to actively work on this issue.\n\nIn a different direction we considered a fair resource sharing game modeling how shared systems, such as Amazon's cloud computing system, can share resources. We show that a natural and simple sharing system is only approximately incentive compatible. The goal of the study of the fair sharing game related to the main goal of this project is to understand if players are likely to report truthfully in light of the fact that misreporting has dangers of large efficiency losses, but also have a chance of increased allocation. Our results so far measure the deviation from incentive compatibility of the system. We also have an active system implementation that will allow us to measure user behavior in the near future. \n\nWe have also worked on pricing in ride-sharing networks, showing that a global convex optimization based pricing system, using dynamic pricing results in all (approximate) equilibria close to being socially optimal. The goal of the pricing project in ride-sharing is the show how one can make pricing robust to drivers who use learning, rather than the classical best response, that requires too much knowledge (or trust) to be a viable behavioral model. We show that all approximate equilibria have approximately optimal welfare is important for the learning context: it ensures that if all (or almost all) drivers satisfy a version of the no-regret learning criteria than such pricing results in good social outcome. This is as opposed to the static pricing version of such a system that has good equilibria but is not at all robust. \n\n \n\n\t\t\t\t\tLast Modified: 08/06/2022\n\n\t\t\t\t\tSubmitted by: Eva Tardos"
 }
}