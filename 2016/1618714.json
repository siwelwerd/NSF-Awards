{
 "awd_id": "1618714",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: AF: Small: Collaborative Research: Differentially Private Learning: From Theory To Applications",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 249729.0,
 "awd_amount": 249729.0,
 "awd_min_amd_letter_date": "2016-07-21",
 "awd_max_amd_letter_date": "2016-07-21",
 "awd_abstract_narration": "Practical privacy-preserving machine learning methods are currently of critical importance in medical, financial and consumer applications, among others. The aim of this project is to develop practical private machine learning algorithms that can be easily implemented by practitioners in any field that holds sensitive data, while keeping robust privacy guarantees. The proposed research will extend the existing rigorous theoretical guarantees of differential privacy to reach the requirements of modern machine learning algorithms in concrete practical settings. The generated intellectual merit therefore spans all the way from the theory to practical algorithms. The resulting methods have the potential to benefit existing real-world applications in many high impact domains. The PIs have several ongoing and successful collaborations with medical practitioners and researchers and will evaluate the resulting algorithms on real patient data in high impact medical applications. All algorithms will be made publicly available as open source. Both PIs are dedicated towards actively hiring minorities and involving undergraduate students in research.\r\n\r\nDifferential privacy (DP) is now recognized as one of the most rigorous and potentially usable notions of statistical privacy, and has become a full-fledged research field.  The aim of this research is to provide reliable privacy guarantees for practical machine learning algorithms, which is invaluable to protect individuals who volunteer their sensitive data for research purposes. We identify several areas with high impact potential and propose four concrete research thrusts. (1) Private Causal Inference. Causal inference is one of the most promising new directions in machine learning, that recently has become practical. Some of the most interesting causal questions deal however with medical or government policy data, which are inherently sensitive. We propose to unite the recent breakthroughs in both fields (causal inference and DP) and derive a practical and theoretically sound method to ensure differentially private causal inference. (2) Privacy for Bayesian Global Optimization. The success of deep learning has created a surge in popularity for Bayesian Global Optimization (BGO) for hyper-parameter tuning. Simultaneously, recent publications have tied the stability properties of differential privacy to generalization in adaptive data analysis. We propose to unite these recent developments and improve the generalization of BGO using insights from DP. Here, we are not protecting individuals from privacy leaks, but algorithms from overfitting-allowing for fine trade-offs of \"privacy\" vs. efficacy. (3) Private Communication-Efficient Distributed Learning. In response to the growth of data distributed over multiple machines, we aim to design practical private and communication-efficient algorithms for supervised and unsupervised learning problems. This work will build off our recent work on distributed learning and clustering algorithms. (4) Practical Private Active Learning. In the age of big data, there has been tremendous interest both in machine learning and its application areas on designing active learning algorithms that most efficiently utilize the available data, while minimizing the need for human intervention. Recently there have been exciting results on understanding statistical and computational principles (including work by the PIs). This research will develop new foundations and new practical well-founded active learning algorithms that are not only statistically and computationally efficient, but also differentially private.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Maria-Florina",
   "pi_last_name": "Balcan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Maria-Florina Balcan",
   "pi_email_addr": "ninamf@cs.cmu.edu",
   "nsf_id": "000537870",
   "pi_start_date": "2016-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 249729.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Differential privacy is a compelling and widely embraced notion of privacy for applying machine learning to sensitive data. Differential privacy provides individuals a guarantee that any output of the machine learning system would occur with nearly the same probability whether the individual chooses to opt in or opt out, or even opt in with fake personal information.&nbsp; The challenge is to provide learning procedures with such strong privacy guarantees for the entities providing the training data, while at the same time extracting meaningful information over the underlying source of data. This project advanced private machine learning by developing efficient private learning algorithms with such strong differential privacy guarantees for important paradigms including clustering, distributed learning, and hyper-parameter tuning for combinatorial settings.</p>\n<p>For clustering problems, where the goal is to group a given set of objects into natural groups, we provided efficient private clustering algorithms that achieve good performance guarantees with respect to the widely popular k-means and k-median objectives in the challenging case of high-dimensional data.&nbsp; For supervised learning we analyzed differentially private algorithms for decision tree learning in distributed settings, that is, for learning from data where different portions are held by different parties. For hyper-parameter tuning we provided strong guarantees for&nbsp;using data to tune parameters of algorithms for solving hard combinatorial problems, to improve their performance on future data.</p>\n<p>The research developed in this project was published in top machine learning and theory of computing venues, and it has contributed to training of several students.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2019<br>\n\t\t\t\t\tModified by: Maria-Florina&nbsp;Balcan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDifferential privacy is a compelling and widely embraced notion of privacy for applying machine learning to sensitive data. Differential privacy provides individuals a guarantee that any output of the machine learning system would occur with nearly the same probability whether the individual chooses to opt in or opt out, or even opt in with fake personal information.  The challenge is to provide learning procedures with such strong privacy guarantees for the entities providing the training data, while at the same time extracting meaningful information over the underlying source of data. This project advanced private machine learning by developing efficient private learning algorithms with such strong differential privacy guarantees for important paradigms including clustering, distributed learning, and hyper-parameter tuning for combinatorial settings.\n\nFor clustering problems, where the goal is to group a given set of objects into natural groups, we provided efficient private clustering algorithms that achieve good performance guarantees with respect to the widely popular k-means and k-median objectives in the challenging case of high-dimensional data.  For supervised learning we analyzed differentially private algorithms for decision tree learning in distributed settings, that is, for learning from data where different portions are held by different parties. For hyper-parameter tuning we provided strong guarantees for using data to tune parameters of algorithms for solving hard combinatorial problems, to improve their performance on future data.\n\nThe research developed in this project was published in top machine learning and theory of computing venues, and it has contributed to training of several students.\n\n\t\t\t\t\tLast Modified: 12/30/2019\n\n\t\t\t\t\tSubmitted by: Maria-Florina Balcan"
 }
}