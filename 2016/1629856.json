{
 "awd_id": "1629856",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CI-SUSTAIN: Collaborative Research: Extending a Large Multimodal Corpus of Spontaneous Behavior for Automated Emotion Analysis",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Balakrishnan Prabhakaran",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 215426.0,
 "awd_amount": 223426.0,
 "awd_min_amd_letter_date": "2016-07-25",
 "awd_max_amd_letter_date": "2021-07-09",
 "awd_abstract_narration": "This project will extend and sustain a widely-used data infrastructure for studying human emotion, hosted at the lead investigator's university and available to the research community.  The first two versions of the dataset (BP4D and BP4D+) contain videos of people reacting to varied emotion-eliciting situations, their self-reported emotion, and expert annotations of their facial expression. Version 1, BP4D (n=41), has been used by over 100 research groups and supported a successful community competition around recognizing emotion.  The second version (BP4D+) adds participants (n = 140), thermal imaging, and measures of peripheral physiology.  The current project greatly broadens and extends this corpus to produce a new dataset (BP4D++) that enables deep-learning approaches, increases generalizability, and builds research infrastructure and community in computer and behavioral science.  The collaborators will (1) increase participant diversity; 2) add videos of pairs of people interacting to the current mix of individual and interviewer-mediated video; 3) increase the number of participants to meet the demands of recent advances in \"big data\" approaches to machine learning; and 4) expand the size and scope of annotations in the videos. They will also involve the community through an oversight and coordinating consortium that includes researchers in computer vision, biometrics, robotics, and cognitive and behavioral science. The consortium will be composed of special interest groups that focus on various aspects of the corpus, including groups responsible for completing the needed annotations, generating meta-data, and expanding the database application scope.  Having an infrastructure to support emotion recognition research matters because computer systems that interact with people (such as phone assistants or characters in virtual reality environments) will be more useful if they react appropriately to what people are doing, thinking, and feeling.  \r\n\r\nThe team will triple the number of participants in the combined corpora to 540.  They will develop a dyadic interaction task and capture data from 100 interacting dyads to support dynamic modeling of interpersonal influence across expressive behavior and physiology, as well as analysis of emotional synchrony.  They will increase the density of facial annotations to about 15 million frames in total, allowing the database to become sufficiently large to support deep-learning approaches to multimodal emotion detection. These annotations will be accomplished through a hybrid approach that combines expert coding using the Facial Action Coding System, automated face analysis, and crowdsourcing with expert input from the research community.  Finally, the recorded data will be augmented with a wide range of meta-data derived from 2D videos, 3D videos, thermal videos, and physiological signals.  To ensure the community is involved in sustaining the infrastructure, in addition to the governance consortium described above, the investigators will involve the community in jointly building both APIs that allow adding meta-data and annotations and tools to support the submission and evaluation of new recognition algorithms, then organizing community-wide competitions using those tools.  The research team will also reach out to new research communities around health computing, biometrics, and affective computing to widen the utility of the enhanced infrastructure, grow the community of expert annotators through training workshops, and build an educational community around the infrastructure that facilitates the development and sharing of course materials that use it.  Long-term, the infrastructure will be funded through a combination of commercial licensing and support from the lead university's system administration group.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Qiang",
   "pi_last_name": "Ji",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qiang Ji",
   "pi_email_addr": "qji@ecse.rpi.edu",
   "nsf_id": "000350734",
   "pi_start_date": "2016-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rensselaer Polytechnic Institute",
  "inst_street_address": "110 8TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "TROY",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5182766000",
  "inst_zip_code": "121803590",
  "inst_country_name": "United States",
  "cong_dist_code": "20",
  "st_cong_dist_code": "NY20",
  "org_lgl_bus_name": "RENSSELAER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "U5WBFKEBLMX3"
 },
 "perf_inst": {
  "perf_inst_name": "Rensselaer Polytechnic Institute",
  "perf_str_addr": "110 8th stree",
  "perf_city_name": "Troy",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "121803522",
  "perf_ctry_code": "US",
  "perf_cong_dist": "20",
  "perf_st_cong_dist": "NY20",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 215426.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Emotion is a complex psychophysiological experience of an individual's state of mind.&nbsp; Recognizing and understanding emotion is crucial for many fields. &nbsp;Built on our existing efforts, this project significantly extends our existing spontaneous emotion database in the number of subjects, in subject interaction types, in data modalities, in both emotion annotation amount and granularity, and finally in types and amount of meta-data generation. &nbsp;With these improvements, the dataset utility to various research communities, including AI, HCI, and psychology, are significantly improved.</p>\n<p>The specific objectives for RPI portion of this project include: 1) develop and improve facial processing algorithms and software to extract various facial meta-data, including 2D/3D facial landmarks, 3D facial mesh, head pose, 3D eye mesh, 3D eye gaze and state for both visible and thermal images; and 2) develop a crowd-sourcing software and the associated user interface for large scale facial action annotation using Amazon Mechanical Turk.&nbsp; The primary accomplishments for this projects include: 1)&nbsp; A crowd-sourcing software for large scale AU annotations using Amazon Mechanical Turk; 2)&nbsp; Algorithms and software for facial landmark detection, 3D facial mesh reconstruction, head pose estimation, and 3D eye gaze/eyelid state estimation; 3)&nbsp; Generated meta data and annotated AUs for the newly acquired subject data; 4)&nbsp; Training of 5 Ph.D level graduate students and 7 undergraduate students, including 6 female students; and 5)&nbsp; 9 publications &nbsp;in top computer vision conferences and journals.&nbsp; Furthermore, empirical evaluation showed the AU annotation software achieved 90% accuracy for both posed dataset (CK+) and the spontaneous dataset (BP4D) at about 2 cents per AU. &nbsp;The results demonstrate the software?s promise and potential for accurate, &nbsp;large scale, and affordable AU annotation via crowd-sourcing.</p>\n<p>Besides contributions to the proposed emotion dataset, this research has broad impacts in several fields, including AI, health-care, affective computing, HCI, psychology, and cognitive science.&nbsp; This research also contributes to the education and training of 5 Ph.D students and 7 undergraduate students in computer vision, deep learning, and in their applications for automatic facial behavior analysis.&nbsp; &nbsp;&nbsp;The research results for this work are widely disseminated in the research communities through 9 publications in top AI/ML conferences and journals, including Neurips, ICCV, CVPR, and ECCV, and IJCV.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/26/2022<br>\n\t\t\t\t\tModified by: Qiang&nbsp;Ji</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nEmotion is a complex psychophysiological experience of an individual's state of mind.  Recognizing and understanding emotion is crucial for many fields.  Built on our existing efforts, this project significantly extends our existing spontaneous emotion database in the number of subjects, in subject interaction types, in data modalities, in both emotion annotation amount and granularity, and finally in types and amount of meta-data generation.  With these improvements, the dataset utility to various research communities, including AI, HCI, and psychology, are significantly improved.\n\nThe specific objectives for RPI portion of this project include: 1) develop and improve facial processing algorithms and software to extract various facial meta-data, including 2D/3D facial landmarks, 3D facial mesh, head pose, 3D eye mesh, 3D eye gaze and state for both visible and thermal images; and 2) develop a crowd-sourcing software and the associated user interface for large scale facial action annotation using Amazon Mechanical Turk.  The primary accomplishments for this projects include: 1)  A crowd-sourcing software for large scale AU annotations using Amazon Mechanical Turk; 2)  Algorithms and software for facial landmark detection, 3D facial mesh reconstruction, head pose estimation, and 3D eye gaze/eyelid state estimation; 3)  Generated meta data and annotated AUs for the newly acquired subject data; 4)  Training of 5 Ph.D level graduate students and 7 undergraduate students, including 6 female students; and 5)  9 publications  in top computer vision conferences and journals.  Furthermore, empirical evaluation showed the AU annotation software achieved 90% accuracy for both posed dataset (CK+) and the spontaneous dataset (BP4D) at about 2 cents per AU.  The results demonstrate the software?s promise and potential for accurate,  large scale, and affordable AU annotation via crowd-sourcing.\n\nBesides contributions to the proposed emotion dataset, this research has broad impacts in several fields, including AI, health-care, affective computing, HCI, psychology, and cognitive science.  This research also contributes to the education and training of 5 Ph.D students and 7 undergraduate students in computer vision, deep learning, and in their applications for automatic facial behavior analysis.    The research results for this work are widely disseminated in the research communities through 9 publications in top AI/ML conferences and journals, including Neurips, ICCV, CVPR, and ECCV, and IJCV.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/26/2022\n\n\t\t\t\t\tSubmitted by: Qiang Ji"
 }
}