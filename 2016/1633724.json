{
 "awd_id": "1633724",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Collaborative Research: F: Algorithmic Fairness: A Systemic and Foundational Treatment of Nondiscriminatory Data Mining",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 484074.0,
 "awd_amount": 484074.0,
 "awd_min_amd_letter_date": "2016-08-31",
 "awd_max_amd_letter_date": "2016-08-31",
 "awd_abstract_narration": "Data-driven modeling has moved beyond the realm of consumer predictions and recommendations into areas of policy and planning that have a profound impact on our daily lives. The tools of data analysis are being harnessed to predict crime, select candidates for jobs, identify security threats, determine credit risk, and even decide treatment plans and interventions for patients. Automated learning and mining tools can crunch incredible amounts and variety of data in order to detect patterns and make predictions. As is rapidly becoming clear, these tools can also introduce discriminatory behavior and amplify biases in the systems they are trained on. In this project, the PIs will study the problems of discrimination and bias in algorithmic decision-making. By studying all aspects of the data pipeline (from data preparation to learning, evaluation, and feedback), they will develop tools for analyzing, auditing, and designing automated decision-making systems that will be fair, accountable, and transparent. As specific goals to broaden the impact of this research, the PIs will develop a course curriculum to educate the next generation of data scientists on the ethical, legal, and societal implications of algorithmic decision-making, with the intent that they will then take this understanding into their jobs as they enter the workforce. Initial efforts by the PIs have attracted students from underrepresented groups in computer science, and they will continue these efforts. The PIs will also explore the legal and policy ramifications of this research, and develop best practice guidelines for the use of their tools by policy makers, lawyers, journalists, and other practitioners.\r\n\r\nThe PIs will explore the technical subject of this project in three ways. Firstly, they will develop a sound theoretical framework for reasoning about algorithmic fairness. This framework carefully separates mechanisms, beliefs, and assumptions in order to make explicit implicitly held assumptions about the nature of fairness in learning. Secondly, by examining the entire pipeline of tasks associated with learning, they will identify hitherto unexplored areas where bias may be unintentionally introduced into learning as well as novel problems associated with ensuring fairness. These include the initial stages of data preparation, various kinds of fairness-aware learning, and evaluation. They will also investigate the problem of feedback: when actions based on a biased learned model might cause a feedback loop that changes reality and leads to more bias.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Suresh",
   "pi_last_name": "Venkatasubramanian",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Suresh Venkatasubramanian",
   "pi_email_addr": "suresh_venkatasubramanian@brown.edu",
   "nsf_id": "000074075",
   "pi_start_date": "2016-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Utah",
  "inst_street_address": "201 PRESIDENTS CIR",
  "inst_street_address_2": "",
  "inst_city_name": "SALT LAKE CITY",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8015816903",
  "inst_zip_code": "841129049",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "UT01",
  "org_lgl_bus_name": "UNIVERSITY OF UTAH",
  "org_prnt_uei_num": "",
  "org_uei_num": "LL8GLEVH6MG3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Utah",
  "perf_str_addr": "50 S. Centra Campus Drive",
  "perf_city_name": "Salt Lake City",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "841129249",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "UT01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 484074.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p dir=\"ltr\"><span>The work on this award focused on understanding and developing technological interventions to help address the potential for discrimination resulting from algorithmic decision making.&nbsp; It especially focused on these algorithms in the context of the criminal justice system, such as criminal risk assessments and predictive policing systems.&nbsp; This included work that identified an issue with statistical bias in predictive policing systems where police were repeatedly sent back to neighborhoods with previous arrests solely due to those previous arrests and not because of an underlying higher crime rate.&nbsp; Another work created a framework for identifying potential issues with algorithms when deployed in society by understanding these algorithms as existing within a sociotechnical system that requires taking human actions into account when assessing algorithms' efficacy.&nbsp; Overall, the 11 papers published under this award introduced technical fairness definitions and analyses, algorithms to audit algorithmic decision-making systems for bias, and identified issues with feedback loops in predictive policing and risk assessment algorithms.</span></p>\n<p dir=\"ltr\"><span>This award was critical to the broader development of the field of algorithmic fairness as well as the development of the workforce.</span></p>\n<p dir=\"ltr\"><span>Academic and public outreach:&nbsp;&nbsp;</span>During this grant, the PIs helped to co-found the main conference in this area, the ACM Conference on Fairness, Accountability, and Transparency.&nbsp; This conference has had a major impact on the work in computer science and in adjacent fields, serving as a high profile publication venue and growing to 500+ attendees each year.&nbsp; The PIs have also done public outreach on this topic, e.g., via interviews on NPR.&nbsp; Public service has also included participation in a research advisory council for the First Judicial District of Pennsylvania as they consider the implementation of a pretrial risk assessment system.</p>\n<p dir=\"ltr\"><span>Workforce development:&nbsp;&nbsp;</span>Course development work has incorporated fairness concerns in the form of a new course on the ethics of data science. This research has been the basis for 5 Ph.D dissertations and two undergraduate theses.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/24/2021<br>\n\t\t\t\t\tModified by: Suresh&nbsp;Venkatasubramanian</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The work on this award focused on understanding and developing technological interventions to help address the potential for discrimination resulting from algorithmic decision making.  It especially focused on these algorithms in the context of the criminal justice system, such as criminal risk assessments and predictive policing systems.  This included work that identified an issue with statistical bias in predictive policing systems where police were repeatedly sent back to neighborhoods with previous arrests solely due to those previous arrests and not because of an underlying higher crime rate.  Another work created a framework for identifying potential issues with algorithms when deployed in society by understanding these algorithms as existing within a sociotechnical system that requires taking human actions into account when assessing algorithms' efficacy.  Overall, the 11 papers published under this award introduced technical fairness definitions and analyses, algorithms to audit algorithmic decision-making systems for bias, and identified issues with feedback loops in predictive policing and risk assessment algorithms.\nThis award was critical to the broader development of the field of algorithmic fairness as well as the development of the workforce.\nAcademic and public outreach:  During this grant, the PIs helped to co-found the main conference in this area, the ACM Conference on Fairness, Accountability, and Transparency.  This conference has had a major impact on the work in computer science and in adjacent fields, serving as a high profile publication venue and growing to 500+ attendees each year.  The PIs have also done public outreach on this topic, e.g., via interviews on NPR.  Public service has also included participation in a research advisory council for the First Judicial District of Pennsylvania as they consider the implementation of a pretrial risk assessment system.\nWorkforce development:  Course development work has incorporated fairness concerns in the form of a new course on the ethics of data science. This research has been the basis for 5 Ph.D dissertations and two undergraduate theses. \n\n \n\n \n\n\t\t\t\t\tLast Modified: 03/24/2021\n\n\t\t\t\t\tSubmitted by: Suresh Venkatasubramanian"
 }
}