{
 "awd_id": "1618903",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Probabilistic Hierarchical Models for Multi-Task Visual Recognition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 449989.0,
 "awd_amount": 449989.0,
 "awd_min_amd_letter_date": "2016-07-06",
 "awd_max_amd_letter_date": "2016-07-06",
 "awd_abstract_narration": "This project studies biologically-inspired architectures for visual recognition. The human visual system can perform a remarkable number of tasks, from estimating the 3D shape of an object that is grasped to inferring subtle differences between two similar makes and models of cars. Such diverse sets of visual tasks are required of a range of autonomous agents, including self-driving cars or humanoid robotics. Such autonomous platforms have the potential to increase general welfare and health of the overall population. This project attempts to build a computational model capable of such diverse visual tasks. Motivated by biological evidence, this project explores the use of feedback logic to enable such computational reasoning. The project provides research opportunities for both undergraduate and graduate students and for increasing diversity in the fields of computer and human vision.  \r\n\r\nThis research focuses on development of a unified hierarchical probabilistic model that can be used to solve multiple fine-grained visual tasks. Feedforward hierarchical models, of which the most ubiquitous are Convolutional Neural Nets (CNNs), have demonstrated remarkable performance in recent history. This project introduces hierarchical models for vision-with-scrutiny tasks, such as 3D articulated pose estimation and part segmentation. Rather than focusing on increasing performance on established benchmark performance, this research provides a theoretical framework for analyzing bottom-up (feedforward) CNNs and imbuing them with novel top-down reasoning capabilities. It does so by exploring a link between three dominant but disparate paradigms for visual recognition: feedforward neural models, generative probabilistic models (Boltzmann machines), and discriminative latent-variable models (deformable part models). The models introduced in this proposal allow CNNs to be used for large-scale multi-task learning, where tasks span both coarse-grained tasks (such as rapid scene categorization) and fine-grained tasks (such as 3D articulated pose estimation). By addressing multiple fine-grained tasks with a single hierarchical architecture, resource requirements for memory and speed are vastly decreased, important for embedded visual perception applications such as autonomous robots and vehicles.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Deva",
   "pi_last_name": "Ramanan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Deva Ramanan",
   "pi_email_addr": "deva@cs.cmu.edu",
   "nsf_id": "000083629",
   "pi_start_date": "2016-07-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 449989.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This award explored the use of biologically-inspired architectures for neural image understanding. While recent deep networks have revolutionized performance on cognitive tasks such as naming objects in an image, contemporary architectures fail to make use of deeper insights from biological and human vision. This award explored the use of biologically-inspired phenomena, such as neural feedback, bottom-up saliency, and top-down attention for improving performance across a variety of visual understanding tasks.</p>\n<p><br />Outcomes. Outcomes of this award include architectures that are not only more accurate on the given tasks, but are also more efficient in terms of biological energy expenditure (often measured by the seconds of compute necessary to perform the task). Specifically, this award produced several novel benchmarks and architecures for online streaming perception that are more robust, reactive, efficient, and accurate than prior models. These benchmarks span tasks such as activity understanding, spatiotemporal tracking, and real-time scene uderstanding from both 2D sensors (cameras) and 3D sensors (stereopsis and lidar). Unique properties of the emergent solutions surfaced by these benchmarks is that they make heavy use of contextual reasoning, both spatially (spatially-nearby pixels should effect the local processing of a given pixel) and temporally (prior frames should influence the local processing of a given pixel). The resulting architectures are far more efficient when processing a continuous stream of sensor data.</p>\n<p><br />Impact. Because the visual understanding tasks explored in this award are firmly motivated by an embodied perceptive (requiring an active agent to \"see\" the world with which its about to interact), they have direct application to industrial settings such as self-driving cars and low-energy-consuming neural networks. Finally, this award produced over a dozen peer-reviewed publications, supported two workshops, and supported several graduate students and undergraduate students, including under-represented minorities.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/24/2021<br>\n\t\t\t\t\tModified by: Deva&nbsp;Ramanan</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1618903/1618903_10438245_1624903245292_face_selfie--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1618903/1618903_10438245_1624903245292_face_selfie--rgov-800width.jpg\" title=\"Spatial top-down feedback\"><img src=\"/por/images/Reports/POR/2021/1618903/1618903_10438245_1624903245292_face_selfie--rgov-66x44.jpg\" alt=\"Spatial top-down feedback\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Top-down feedback is crucial for finding small objects, where bottom-up pixel evidence can be weak.</div>\n<div class=\"imageCredit\">https://www.cs.cmu.edu/~peiyunh/tiny/</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Deva&nbsp;Ramanan</div>\n<div class=\"imageTitle\">Spatial top-down feedback</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1618903/1618903_10438245_1624903603956_streaming-long--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1618903/1618903_10438245_1624903603956_streaming-long--rgov-800width.jpg\" title=\"Temporal top-down feedback\"><img src=\"/por/images/Reports/POR/2021/1618903/1618903_10438245_1624903603956_streaming-long--rgov-66x44.jpg\" alt=\"Temporal top-down feedback\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">By intelligently making use of temporal context provided by processing the last frame, one can dramatically improve the accuracy of real-time systems for sensor processing (such as those needed for autonomous navigation).</div>\n<div class=\"imageCredit\">https://www.cs.cmu.edu/~mengtial/proj/streaming/</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Deva&nbsp;Ramanan</div>\n<div class=\"imageTitle\">Temporal top-down feedback</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis award explored the use of biologically-inspired architectures for neural image understanding. While recent deep networks have revolutionized performance on cognitive tasks such as naming objects in an image, contemporary architectures fail to make use of deeper insights from biological and human vision. This award explored the use of biologically-inspired phenomena, such as neural feedback, bottom-up saliency, and top-down attention for improving performance across a variety of visual understanding tasks.\n\n\nOutcomes. Outcomes of this award include architectures that are not only more accurate on the given tasks, but are also more efficient in terms of biological energy expenditure (often measured by the seconds of compute necessary to perform the task). Specifically, this award produced several novel benchmarks and architecures for online streaming perception that are more robust, reactive, efficient, and accurate than prior models. These benchmarks span tasks such as activity understanding, spatiotemporal tracking, and real-time scene uderstanding from both 2D sensors (cameras) and 3D sensors (stereopsis and lidar). Unique properties of the emergent solutions surfaced by these benchmarks is that they make heavy use of contextual reasoning, both spatially (spatially-nearby pixels should effect the local processing of a given pixel) and temporally (prior frames should influence the local processing of a given pixel). The resulting architectures are far more efficient when processing a continuous stream of sensor data.\n\n\nImpact. Because the visual understanding tasks explored in this award are firmly motivated by an embodied perceptive (requiring an active agent to \"see\" the world with which its about to interact), they have direct application to industrial settings such as self-driving cars and low-energy-consuming neural networks. Finally, this award produced over a dozen peer-reviewed publications, supported two workshops, and supported several graduate students and undergraduate students, including under-represented minorities.\n\n\t\t\t\t\tLast Modified: 09/24/2021\n\n\t\t\t\t\tSubmitted by: Deva Ramanan"
 }
}