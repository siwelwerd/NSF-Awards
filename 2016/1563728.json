{
 "awd_id": "1563728",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Medium: Collaborative Research: Wizard: Exploiting Disk Performance Signatures for Cost-Effective Management of Large-Scale Storage Systems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2016-08-15",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2016-08-05",
 "awd_max_amd_letter_date": "2016-08-05",
 "awd_abstract_narration": "The tremendous advances in low-cost, high-capacity magnetic hard disk drives, flash-based solid state drives and non-volatile memory have been among the key factors supporting big data applications and various computing-storage services that the modern society deeply relies on. However, storage drives are reported to be the most commonly replaced hardware components because of failures. This causes service downtime and even data loss, costing enterprises multi-trillion dollars per year. Existing disk failure management approaches are mostly reactive and incur high overheads; they do not provide a cost-effective solution to managing large-scale production storage systems. The goal of this project is to achieve a deep understanding of the reliability of the real-world storage systems, and to develop a cost-effective data and storage resource management system for reliability enhancement.\r\n \r\nThe investigators' approach to building reliable, large-scale storage systems is carefully designed to support storage health monitoring, modeling, prediction and proactive recovery in a systematic fashion. In particular, they first categorize and model storage failures to derive disk performance signatures and explore disk performance signatures to forecast occurrences of disk failures. They then characterize I/O workload dependency in disk performance degradation and integrate the performance signatures of heterogeneous disk devices to effectively reconfigure and manage storage resources. Furthermore, the project will provide easy-to-use APIs for storage users and developers to employ the developed tools and techniques for proactive data rescue and preventive disk reliability enhancement. Finally, the project provides excellent opportunities for training graduate students, especially minority and female students, and for developing new curriculum materials on reliable storage systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Weisong",
   "pi_last_name": "Shi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Weisong Shi",
   "pi_email_addr": "weisong@udel.edu",
   "nsf_id": "000119930",
   "pi_start_date": "2016-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Patrick",
   "pi_last_name": "Gossman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Patrick Gossman",
   "pi_email_addr": "pgossman@wayne.edu",
   "nsf_id": "000620880",
   "pi_start_date": "2016-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Wayne State University",
  "inst_street_address": "5700 CASS AVE STE 4900",
  "inst_street_address_2": "",
  "inst_city_name": "DETROIT",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "3135772424",
  "inst_zip_code": "482023692",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "MI13",
  "org_lgl_bus_name": "WAYNE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M6K6NTJ2MNE5"
 },
 "perf_inst": {
  "perf_inst_name": "Wayne State University",
  "perf_str_addr": "5057 Woodward Ave",
  "perf_city_name": "Detroit",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "482024050",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "MI13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to gain a deep understanding of storage reliability and develop novel approaches to enhance the reliability of modern large-scale storage systems. Previous works on disk drive failures cannot differentiate prediction or handling of disk failures with different manifestations, because the information of failure types is not available. This project first addressed these issues and analyzed disk health data collected from a production data center. We proposed novel approaches to categorize disk failures based on their distinctive manifestations and properties and characterized the degradation of disk errors to failures by deriving the degradation signatures for each failure category.</p>\n<p>First, we conducted a field study of disks based on a large-scale dataset collected from one of the biggest e-commerce production data centers, including SMART (Self-Monitoring, Analysis, and Reporting Technology) attributes, performance metrics (e.g., throughput, network), and disk location markers. We discovered that performance metrics are good indicators of disk failures. We also found that location markers can improve the accuracy of disk failure prediction. Besides, we also trained machine learning models, including neural network models to predict disk failures with 0.95 accuracies for 10 days prediction horizon.</p>\n<p>Next, we investigated when and how machine learning (ML) models fail to achieve high prediction accuracy over space and time.&nbsp; First, we found that ML models are somewhat less effective at predicting high accuracy and recall in areas where the concentration of failures is relatively lower. This is reasonable since ML models are not able to collect enough failed disk samples. ML models are by definition less effective for cases they have not been trained or situations they have not encountered before. This observation is important for data center operators as it emphasizes the need for adding location markers in disk failure prediction models. We observed that the need for sufficiently long testing periods before concluding the prediction quality of ML models.</p>\n<p>Furthermore, we showed how congestion events could impact application performance. We discussed many interesting insights derived from our analysis. Interconnect faults like lane degrades are continuous and vary significantly among lanes. Link inactive errors do not have a temporal or a spatial correlation with lane degrades, while interconnect errors have a high correlation with link inactive/failed errors. We show that these characteristics can be exploited for different purposes. We also demonstrate that multiple applications can cause multiple congestion events within a short period. Furthermore, surprisingly, these applications can be small in job size, not scheduled evenly across the cabinet, and have a many-to-few communication pattern.</p>\n<p>Finally, we evaluated different machine learning models to predict applications encountering throttle events. Given limited literature on field data and analysis on interconnect error, our study addresses an important topic and would be useful for current and future systems. A real-world test of the model on a future HPC system and looking into the impact of other system attributes on the model in the future can further strengthen the model to detect network congestion. Furthermore, the researchers can investigate system and application attributes to detect a specific application causing congestion while multiple applications are running simultaneously in an HPC system.</p>\n<p>Specifically, we presented one of our work at the 18th USENIX Conference on File and Storage Technologies (FAST' 20), and released the disk dataset and data analysis software to the research community, and it has been downloaded by researchers from 116 institutions globally by the end of July 2021.&nbsp;&nbsp;<a href=\"https://github.com/SidiLu001/disk_failure_prediction\">https://github.com/SidiLu001/disk_failure_prediction</a></p>\n<p>Two Ph.D. students, including one female student, &nbsp;involved in this project received the Michael Conrad Research Award, which is the most prestigious award for a Ph.D. student in the Department of Computer Science, Wayne State University.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/02/2021<br>\n\t\t\t\t\tModified by: Weisong&nbsp;Shi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project aims to gain a deep understanding of storage reliability and develop novel approaches to enhance the reliability of modern large-scale storage systems. Previous works on disk drive failures cannot differentiate prediction or handling of disk failures with different manifestations, because the information of failure types is not available. This project first addressed these issues and analyzed disk health data collected from a production data center. We proposed novel approaches to categorize disk failures based on their distinctive manifestations and properties and characterized the degradation of disk errors to failures by deriving the degradation signatures for each failure category.\n\nFirst, we conducted a field study of disks based on a large-scale dataset collected from one of the biggest e-commerce production data centers, including SMART (Self-Monitoring, Analysis, and Reporting Technology) attributes, performance metrics (e.g., throughput, network), and disk location markers. We discovered that performance metrics are good indicators of disk failures. We also found that location markers can improve the accuracy of disk failure prediction. Besides, we also trained machine learning models, including neural network models to predict disk failures with 0.95 accuracies for 10 days prediction horizon.\n\nNext, we investigated when and how machine learning (ML) models fail to achieve high prediction accuracy over space and time.  First, we found that ML models are somewhat less effective at predicting high accuracy and recall in areas where the concentration of failures is relatively lower. This is reasonable since ML models are not able to collect enough failed disk samples. ML models are by definition less effective for cases they have not been trained or situations they have not encountered before. This observation is important for data center operators as it emphasizes the need for adding location markers in disk failure prediction models. We observed that the need for sufficiently long testing periods before concluding the prediction quality of ML models.\n\nFurthermore, we showed how congestion events could impact application performance. We discussed many interesting insights derived from our analysis. Interconnect faults like lane degrades are continuous and vary significantly among lanes. Link inactive errors do not have a temporal or a spatial correlation with lane degrades, while interconnect errors have a high correlation with link inactive/failed errors. We show that these characteristics can be exploited for different purposes. We also demonstrate that multiple applications can cause multiple congestion events within a short period. Furthermore, surprisingly, these applications can be small in job size, not scheduled evenly across the cabinet, and have a many-to-few communication pattern.\n\nFinally, we evaluated different machine learning models to predict applications encountering throttle events. Given limited literature on field data and analysis on interconnect error, our study addresses an important topic and would be useful for current and future systems. A real-world test of the model on a future HPC system and looking into the impact of other system attributes on the model in the future can further strengthen the model to detect network congestion. Furthermore, the researchers can investigate system and application attributes to detect a specific application causing congestion while multiple applications are running simultaneously in an HPC system.\n\nSpecifically, we presented one of our work at the 18th USENIX Conference on File and Storage Technologies (FAST' 20), and released the disk dataset and data analysis software to the research community, and it has been downloaded by researchers from 116 institutions globally by the end of July 2021.  https://github.com/SidiLu001/disk_failure_prediction\n\nTwo Ph.D. students, including one female student,  involved in this project received the Michael Conrad Research Award, which is the most prestigious award for a Ph.D. student in the Department of Computer Science, Wayne State University.\n\n \n\n\t\t\t\t\tLast Modified: 09/02/2021\n\n\t\t\t\t\tSubmitted by: Weisong Shi"
 }
}