{
 "awd_id": "1622292",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Towards Scalable Error Detection for Parallel Software Systems on Emerging Computing Platforms",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Sushil K Prasad",
 "awd_eff_date": "2015-08-15",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 252154.0,
 "awd_amount": 262848.0,
 "awd_min_amd_letter_date": "2015-12-16",
 "awd_max_amd_letter_date": "2016-08-31",
 "awd_abstract_narration": "Extreme scale computing introduces many new challenges to parallel program design, where a computation may involve hundreds of thousands of processes with multiple-level parallelism. It is very difficult to debug such large-scale parallel programs. Scalable and light-weight correctness tools are critical to combat this challenge.\r\n\r\nThis research seeks to design innovative algorithms and develop a scalable toolkit to efficiently and effectively analyze parallel programs and detect potential errors on the emerging heterogeneous and extreme scale computing platforms. Specifically, the objectives of the research are to: (1) develop instrumentation tools and optimized monitoring systems to support building tools for error detection, (2) design various optimization strategies and techniques to improve scalability and reduce overhead, (3) integrate static and dynamic program analyses to improve reporting accuracy and code coverage, (4) design more accurate and efficient detection techniques on large-scale parallel systems, and (5) investigate domain-specific techniques for error detection and optimization.\r\n\r\nThis research will greatly help the development of extreme scale parallel programs for scientific computing and discover hard-to-find errors in early stage. It will significantly reduce the burden of tedious debugging activities, so researchers can focus on scientific problems. The toolkit is targeted for general computing platforms, from local clusters to extreme scale supercomputers. In the education thrust, the research results will facilitate the development of new courses and enhance existing ones. High-school, undergraduate, and graduate students will have opportunities to get involved in the research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Liqiang",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Liqiang Wang",
   "pi_email_addr": "lwang@cs.ucf.edu",
   "nsf_id": "000248300",
   "pi_start_date": "2015-12-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "The University of Central Florida Board of Trustees",
  "inst_street_address": "4000 CENTRAL FLORIDA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "ORLANDO",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "4078230387",
  "inst_zip_code": "328168005",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "FL10",
  "org_lgl_bus_name": "THE UNIVERSITY OF CENTRAL FLORIDA BOARD OF TRUSTEES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RD7MXJV7DKT9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Central Florida",
  "perf_str_addr": "",
  "perf_city_name": "ORLANDO",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "328168005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "FL10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "104500",
   "pgm_ele_name": "CAREER: FACULTY EARLY CAR DEV"
  },
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "736100",
   "pgm_ele_name": "EDUCATION AND WORKFORCE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "019Z",
   "pgm_ref_txt": "Grad Prep APG:Enhan. Experience"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 252154.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 10694.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.</p>\n<p><strong>Intellectual Merit: </strong>The overall research goal is to design innovative algorithms and develop a scalable toolkit to efficiently and effectively analyze and optimize parallel programs and detect potential errors and performance issues on the emerging computing platforms, including heterogeneous and extreme-scale computing, as well as cloud and big data computing. &nbsp;We also work with domain (e.g. geography, geophysics) experts to design novel parallel systems for better performance, scalability, and reliability. This project greatly helps the development of parallel and big data computing platforms for scientific computing. The major research products include the following.</p>\n<p>(1) We designed a novel approach and developed a toolkit by integrating static and dynamic program analyses to detect coding and performance problems in parallel programs using MPI and openMP, including thread-safety violations in hybrid MPI/OpenMP programs.</p>\n<p>(2) We designed an automatic approach to tune the performance of MPI program in Cloud using container technology; specifically, we use adaptive resource reassignments enabled by virtualization techniques to automatically tune performance of MPI programs in cloud computing platforms.</p>\n<p>(3) We designed a scalable and distributed geographic information system, called Dart, to support social media (such as Twitter) analytics based on Hadoop and HBase. It supports massive spatial data analysis such as K-Nearest Neighbors (KNN) and Geometric Median Distribution.</p>\n<p>(4) We propose a concurrent coordination algorithm for distributed generalized deadlock detection. The algorithm aims to avoid false negatives and improve the performance when concurrently executing deadlock detection in a distributed system.</p>\n<p>(5) We designed a Hadoop extension, namely MRapid, to significantly speed up the execution of short jobs on Hadoop. The major ideas are to reuse allocated resources, avoid unnecessary operations, and expand parallelization. MRapid is completely backward compatible to Hadoop, and imposes negligible overhead.</p>\n<p>(6) We designed and implemented several approaches to detect abnormality and analyzes root causes from Spark log files using statistical methods and the state-of-art deep neural networks.</p>\n<p>(7) We develop an efficient performance optimization engine called Hedgehog to evaluate the performance and give an optimal configuration setting for Spark system.</p>\n<p><strong>Education:</strong> The research project has educated 8 Ph.D. students, 4 of them have graduated with Ph.D. degree in Computer Science. There are more a dozen of M.S. and undergraduate students involved the research development. Five new courses have been developed in the areas of parallel computing and big data processing. Other courses also greatly benefit from the research projects. Many lectures and homework assignments are based on or closely related to our research. Besides the teaching for undergraduate and graduate students, the PI also collaborated with many domain experts and help them improve essential skills on parallel computing and big data processing frameworks.</p>\n<p><strong>Broader Impacts:</strong> Research project products were finally integrated into several open source toolkits, which are useful for domain scientists to develop dependable parallel computing software. Our optimization techniques help build more efficient infrastructures of high-performance computing, cloud and big data computing. It can help automatic detect performance and design problems in HPC and big data programs. The developed tools improve the scalability of parallel computing systems. The research results can also help monitor the execution of parallel systems, detect anomaly, and report the root causes of problems. In summary, our research products signi?cantly reduce the burden of tedious debugging activities for scientists, which allows them to be more productive on their research. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/04/2018<br>\n\t\t\t\t\tModified by: Liqiang&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.\n\nIntellectual Merit: The overall research goal is to design innovative algorithms and develop a scalable toolkit to efficiently and effectively analyze and optimize parallel programs and detect potential errors and performance issues on the emerging computing platforms, including heterogeneous and extreme-scale computing, as well as cloud and big data computing.  We also work with domain (e.g. geography, geophysics) experts to design novel parallel systems for better performance, scalability, and reliability. This project greatly helps the development of parallel and big data computing platforms for scientific computing. The major research products include the following.\n\n(1) We designed a novel approach and developed a toolkit by integrating static and dynamic program analyses to detect coding and performance problems in parallel programs using MPI and openMP, including thread-safety violations in hybrid MPI/OpenMP programs.\n\n(2) We designed an automatic approach to tune the performance of MPI program in Cloud using container technology; specifically, we use adaptive resource reassignments enabled by virtualization techniques to automatically tune performance of MPI programs in cloud computing platforms.\n\n(3) We designed a scalable and distributed geographic information system, called Dart, to support social media (such as Twitter) analytics based on Hadoop and HBase. It supports massive spatial data analysis such as K-Nearest Neighbors (KNN) and Geometric Median Distribution.\n\n(4) We propose a concurrent coordination algorithm for distributed generalized deadlock detection. The algorithm aims to avoid false negatives and improve the performance when concurrently executing deadlock detection in a distributed system.\n\n(5) We designed a Hadoop extension, namely MRapid, to significantly speed up the execution of short jobs on Hadoop. The major ideas are to reuse allocated resources, avoid unnecessary operations, and expand parallelization. MRapid is completely backward compatible to Hadoop, and imposes negligible overhead.\n\n(6) We designed and implemented several approaches to detect abnormality and analyzes root causes from Spark log files using statistical methods and the state-of-art deep neural networks.\n\n(7) We develop an efficient performance optimization engine called Hedgehog to evaluate the performance and give an optimal configuration setting for Spark system.\n\nEducation: The research project has educated 8 Ph.D. students, 4 of them have graduated with Ph.D. degree in Computer Science. There are more a dozen of M.S. and undergraduate students involved the research development. Five new courses have been developed in the areas of parallel computing and big data processing. Other courses also greatly benefit from the research projects. Many lectures and homework assignments are based on or closely related to our research. Besides the teaching for undergraduate and graduate students, the PI also collaborated with many domain experts and help them improve essential skills on parallel computing and big data processing frameworks.\n\nBroader Impacts: Research project products were finally integrated into several open source toolkits, which are useful for domain scientists to develop dependable parallel computing software. Our optimization techniques help build more efficient infrastructures of high-performance computing, cloud and big data computing. It can help automatic detect performance and design problems in HPC and big data programs. The developed tools improve the scalability of parallel computing systems. The research results can also help monitor the execution of parallel systems, detect anomaly, and report the root causes of problems. In summary, our research products signi?cantly reduce the burden of tedious debugging activities for scientists, which allows them to be more productive on their research.  \n\n \n\n\t\t\t\t\tLast Modified: 07/04/2018\n\n\t\t\t\t\tSubmitted by: Liqiang Wang"
 }
}