{
 "awd_id": "1553284",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Scalable learning with combinatorial structure",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2016-01-15",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 493059.0,
 "awd_amount": 493059.0,
 "awd_min_amd_letter_date": "2016-01-15",
 "awd_max_amd_letter_date": "2022-06-27",
 "awd_abstract_narration": "Advances in science and technology increasingly rely on inference and prediction from data such as videos, molecules, networks, or sets of purchased goods.  Such data consists of several elements that participate in a collective structure. Effective inference and prediction in turn rely on (i) concise and accurate representations of latent interdependence structure in data; and (ii) fast learning and optimization algorithms that can process modern large data sets. This CAREER project addresses these challenges by building new algorithmic foundations that open a wider set of mathematical tools for practical data analysis. \r\n\r\nIn particular, this project explores and exploits key structural properties and representations. For example, a wide spectrum of important dependence structures (and consequently numerous learning tasks) are well captured by the ubiquitous combinatorial concept of submodular functions on sets, characterized by the property of diminishing returns. Building on this insight and other new tools, this research develops a suite of scalable optimization procedures with theoretical guarantees, as well as new tools for probabilistic modeling and fast inference. The resulting combinatorial learning methods are deployed in novel applications addressing the development of new materials, reducing  environmental impact, in video analytics, and healthcare. Thereby, the proposed methods foster progress and deliver insights beyond computer science. Parts of this project are integrated into a new advanced graduate class and a new hands-on undergraduate class on data analytics that combines statistical modeling with computation, forming a core part of a new educational program. Undergraduate students are involved in the application part of the research, and selected results will serve to motivate high school students to pursue STEM careers. For the research community, the project includes interdisciplinary workshops and tutorials, and further the confluence of discrete optimization and machine learning. Educational materials, data and code are made publicly available.\r\n\r\nThe research questions of this project include three main threads:\r\n\r\n1) Developing a set of new, scalable optimization techniques with theoretical guarantees for combinatorial learning problems. The algorithms will combine combinatorial and continuous optimization, exploit suitable mathematical properties, relaxations, and compact representations, and will implement new ways to leverage data-dependent properties that distinguish practical cases from the worst case.\r\n\r\n2) Extending insights from optimization to probabilistic modeling and inference. This transfer will enable new models and new, fast computational procedures for sampling and probabilistic inference that exploit similar properties as the optimization algorithms.\r\n\r\n3) Real-world applications of the new models and algorithms. Via interdisciplinary collaborations, the third thread explores new applications of combinatorial learning methods to video analytics, instruction, healthcare, and materials design.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stefanie",
   "pi_last_name": "Jegelka",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefanie Jegelka",
   "pi_email_addr": "stefje@mit.edu",
   "nsf_id": "000692422",
   "pi_start_date": "2016-01-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 92361.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 95371.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 98494.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 101735.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 105098.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Intellectual Merit:</strong></p>\n<p>Advances in science and technology increasingly rely on inference and predictions from structured, discrete data such as videos, molecules, networks or sets of purchased goods. Effective inference and prediction from such data relies on concisely and accurately representing latent interdependence structure and fast learning and optimization algorithms that reliably work on modern massive data sets. This project has developed algorithmic foundations that address these challenges and open a wider set of mathematical tools for practical data analysis. In particular, the project aimed to exploit combinatorial, geometric and algebraic structure for discrete machine learning problems. It uses, combines and develops concepts from discrete optimization (the property of submodularity), negative dependence in discrete probability, optimal transport, Bayesian inference and deep learning for developing new machine learning methods and algorithms, and understanding the reliability of existing algorithms.<br /><br />More concretely, under this project we obtained new results on the following topics:<br /><br />(1) Submodular optimization. Submodularity is a beneficial mathematical structure. We demonstrated new ways to exploit submodularity in machine learing, for instance, to allow inferring correspondences that respect broader data geometry. We also derived new algorithms for optimizing (approximately) submodular functions, and for robust and stochastic submodular optimization with theoretical guarantees.<br /><br />(2) Discrete probability. We showed new sampling and approximate inference algorithms with guarantees. In particular, stricter versions of log-submodularity include concepts of strong negative dependence of discrete random variables, which lead to beneficial sampling properties. We extend these concepts and show how to exploit them for outlier detection, sampling diverse subsets, matrix approximation, experiment design and Bayesian black-box optimization.<br /><br />(3) Bayesian optimization. We developed new, particularly scalable algorithms for computing uncertainty and selecting informative subsets, i.e., for optimizing black box functions. Our methods have inspired many follow-up works.<br /><br />(4) Graph representation learning. We established theoretical foundations for deep learning with graph inputs, i.e., Graph Neural Networks. We described their learning and generalization properties, within-distribution and under distribution shifts. These works have already had substantial impact in machine learning and beyond.<br /><br />(5) Deep learning for combinatorial optimization. Inspired by techniques from submodular optimization and graph algorithms, we analyze the capacity of deep learning models to learn combinatorial algorithms, and we derive a framework for neural-network-friendly loss functions for discrete optimization.<br /><br />These results have impact in many application areas; as part of this project, we especially collaborated on problems in materials design, robotics, sensing, computer vision and cancer genetics.<br /><br />As another achievement, the project resulted in a Master's thesis and a PhD thesis that received prestigious thesis awards at ETH Zurich and MIT.<br /><br /><br /><strong>Broader Impacts:</strong><br /><br />The results of this project advance fundamental machine learning techniques, which have been used in areas as diverse as materials and drug design, robotics, traffic routing, exploiting machine learning for optimization, natural language processing, and understanding cancer, among others. Subset selection, finding correspondences, and making predictions on graph data are key problems in many applications, and all of these have the potential to benefit from these results.<br /><br />To build community, as part of this project, we organized 3 workshops on non-convex optimization in machine learning and on graph representation learning. In 3 tutorials given at NeurIPS and the Simons Institute, and in 3 summer school lectures, we educated the community about new ideas of exploiting mathematical structure in machine learning, including submodularity, concepts of negative dependence, and analyses of graph representation learning. In numerous talks, the results of this project were not only presented to the core machine learning community, but also to audiences in many different fields, including Operations Research, Mathematics, Machine Learning for Science, Signal Processing.<br /><br />To broaden participation in machine learning and data science and to support junior researchers, the PI served as a co-organizer or faculty advisor for the annual \"Women in Data Science\" workshop and for the Learning Theory Alliance, and actively participated in events as a panelist, mentor or speaker. She also supervised students, postdocs and a high school student from underrepresented minorities.<br /><br />Part of this project was also the development of three new classes, including one class that became the capstone course for the undergraduate Statistics and Data Science minor at MIT.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/22/2023<br>\n\t\t\t\t\tModified by: Stefanie&nbsp;Jegelka</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIntellectual Merit:\n\nAdvances in science and technology increasingly rely on inference and predictions from structured, discrete data such as videos, molecules, networks or sets of purchased goods. Effective inference and prediction from such data relies on concisely and accurately representing latent interdependence structure and fast learning and optimization algorithms that reliably work on modern massive data sets. This project has developed algorithmic foundations that address these challenges and open a wider set of mathematical tools for practical data analysis. In particular, the project aimed to exploit combinatorial, geometric and algebraic structure for discrete machine learning problems. It uses, combines and develops concepts from discrete optimization (the property of submodularity), negative dependence in discrete probability, optimal transport, Bayesian inference and deep learning for developing new machine learning methods and algorithms, and understanding the reliability of existing algorithms.\n\nMore concretely, under this project we obtained new results on the following topics:\n\n(1) Submodular optimization. Submodularity is a beneficial mathematical structure. We demonstrated new ways to exploit submodularity in machine learing, for instance, to allow inferring correspondences that respect broader data geometry. We also derived new algorithms for optimizing (approximately) submodular functions, and for robust and stochastic submodular optimization with theoretical guarantees.\n\n(2) Discrete probability. We showed new sampling and approximate inference algorithms with guarantees. In particular, stricter versions of log-submodularity include concepts of strong negative dependence of discrete random variables, which lead to beneficial sampling properties. We extend these concepts and show how to exploit them for outlier detection, sampling diverse subsets, matrix approximation, experiment design and Bayesian black-box optimization.\n\n(3) Bayesian optimization. We developed new, particularly scalable algorithms for computing uncertainty and selecting informative subsets, i.e., for optimizing black box functions. Our methods have inspired many follow-up works.\n\n(4) Graph representation learning. We established theoretical foundations for deep learning with graph inputs, i.e., Graph Neural Networks. We described their learning and generalization properties, within-distribution and under distribution shifts. These works have already had substantial impact in machine learning and beyond.\n\n(5) Deep learning for combinatorial optimization. Inspired by techniques from submodular optimization and graph algorithms, we analyze the capacity of deep learning models to learn combinatorial algorithms, and we derive a framework for neural-network-friendly loss functions for discrete optimization.\n\nThese results have impact in many application areas; as part of this project, we especially collaborated on problems in materials design, robotics, sensing, computer vision and cancer genetics.\n\nAs another achievement, the project resulted in a Master's thesis and a PhD thesis that received prestigious thesis awards at ETH Zurich and MIT.\n\n\nBroader Impacts:\n\nThe results of this project advance fundamental machine learning techniques, which have been used in areas as diverse as materials and drug design, robotics, traffic routing, exploiting machine learning for optimization, natural language processing, and understanding cancer, among others. Subset selection, finding correspondences, and making predictions on graph data are key problems in many applications, and all of these have the potential to benefit from these results.\n\nTo build community, as part of this project, we organized 3 workshops on non-convex optimization in machine learning and on graph representation learning. In 3 tutorials given at NeurIPS and the Simons Institute, and in 3 summer school lectures, we educated the community about new ideas of exploiting mathematical structure in machine learning, including submodularity, concepts of negative dependence, and analyses of graph representation learning. In numerous talks, the results of this project were not only presented to the core machine learning community, but also to audiences in many different fields, including Operations Research, Mathematics, Machine Learning for Science, Signal Processing.\n\nTo broaden participation in machine learning and data science and to support junior researchers, the PI served as a co-organizer or faculty advisor for the annual \"Women in Data Science\" workshop and for the Learning Theory Alliance, and actively participated in events as a panelist, mentor or speaker. She also supervised students, postdocs and a high school student from underrepresented minorities.\n\nPart of this project was also the development of three new classes, including one class that became the capstone course for the undergraduate Statistics and Data Science minor at MIT.\n\n\t\t\t\t\tLast Modified: 05/22/2023\n\n\t\t\t\t\tSubmitted by: Stefanie Jegelka"
 }
}