{
 "awd_id": "1637927",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: A Cognitive Navigation Assistant for the Blind",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922568",
 "po_email": "wnilsen@nsf.gov",
 "po_sign_block_name": "Wendy Nilsen",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 1000000.0,
 "awd_amount": 1000000.0,
 "awd_min_amd_letter_date": "2016-07-27",
 "awd_max_amd_letter_date": "2021-11-05",
 "awd_abstract_narration": "The focus of this project is on implementation of a navigation assistant that uses a collection of sensing modalities and algorithms to guide a blind person through the knowledge landscape (e.g., social context, visual landmarks, scene functionality) of an unfamiliar environment.  The approach is based on a portfolio of complex processes that provide within a single framework a coherent account of the state of the world, with the help of novel techniques which meld information at various levels of abstraction.  In the near term, project outcomes will directly improve the quality of life for those with visual impairments through public release of a smartphone app.  In the longer term, the societal impact of this research will extend beyond improving sensory capabilities for the blind in that it describes an approach towards human augmentation through the use of machine intelligence.  The work will directly shed light on the variety of environmental knowledge which can be automatically acquired using machine perception, and how that information can be conveyed through a physical co-robot interface.  From an educational perspective, this work will develop important models for integrating knowledge obtained by intelligent machines into one source, and will also develop new theories regarding the translation of that rich knowledge in a manner which can be easily understood by the user.\r\n\r\nLeveraging prior work, sensing modalities such as Bluetooth low energy beacons, depth sensors, color cameras and wearable inertial motion units will be used to enable continuous localization within a novel environment.  An additional layer of higher-order algorithms will further build upon physical measurements of location to develop computational contextual awareness, enabling the navigation assistant to understand the knowledge landscape by identifying meaningful visual landmarks, modes of interaction (functionality) within the environment and social context.  This knowledge structure will then be conveyed to the blind user to enable contextual hyper-awareness, that is to say a contextual understanding of the environment which goes beyond normative sensing capabilities, in order to augment the user's ability to navigate the knowledge landscape of the environment.  The navigation assistant will be instantiated as two concrete manifestations: a compact wearable interface, and a physical robotic interface.  The wearable interface will be a smartphone-based system that gives audio-based navigation feedback to facilitate the creation of a cognitive map.  The robotic interface will be a wheeled hardware platform that guides the user through haptic feedback to further reduce the cognitive load of interpreting and following audio feedback.  Both platforms will be refined and evaluated in real-world scenarios based on principles derived from rigorous user studies.  Project outcomes will include a navigation assistant that can help a blind person walk a path through novel indoor or outdoor suburban environments to a desired destination.  The two physical interfaces will also be used to develop working theories and models for co-robot scenarios that must take into account situational context and the preferential dynamics of the user.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kris",
   "pi_last_name": "Kitani",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kris Kitani",
   "pi_email_addr": "kkitani@cs.cmu.edu",
   "nsf_id": "000663208",
   "pi_start_date": "2016-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Manuela",
   "pi_last_name": "Veloso",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Manuela M Veloso",
   "pi_email_addr": "veloso@cs.cmu.edu",
   "nsf_id": "000094404",
   "pi_start_date": "2016-07-27",
   "pi_end_date": "2021-11-05"
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 1000000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project outcomes consist of various topics related to cyber-physical navigation interfaces for visual impairments such as human factors, real world deployment, navigational behavior analysis, indoor localization, social interaction method, and social policy learning.</p>\n<p>We have developed both wearable and robotic navigation interfaces and evaluated them in several different environments with the target users who have visual impairments.&nbsp;</p>\n<p>Smartphone navigation system, NavCog has been deployed on university campus, shopping mall, airport, hotel, hospital, and museum. The system utilizes its IMU and camera image, and also BLE signals from off-the-shelf beacons placed in the environment to provide accurate (1-2 meters in average) localization accuracy, which is enough for navigation for visually impaired people. Several user studies have been conducted and proved that the system effectively help the users navigate in those environments. We have reported detailed analysis of user's behavior during the navigation in terms of navigational context, expertise in navigation, personalized dynamics, environmental factors, and validity of reactions to instructions.&nbsp;</p>\n<p>Additional works explored virtual navigation experience with a smartphone system, which enabled users to explore and build mental maps of the environments before actually navigating in the real world.&nbsp;</p>\n<p>As for the robotic interfaces, we mainly focused on social interactions. We have been developing a mobile robot that navigates the users in indoor space. Besides, we have studied state of the art algorithms to understand social dynamics in the environment to avoid social conflicts with the robotic interface including techniques of collision prediction with pedestrians, social policy learning, recognition of interaction among groups of people, recognition of head gestures and body parts. A robotic interface which implements collision prediction and alert function has been tested in airport environments and proved its effectiveness.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/20/2021<br>\n\t\t\t\t\tModified by: Kris&nbsp;Kitani</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project outcomes consist of various topics related to cyber-physical navigation interfaces for visual impairments such as human factors, real world deployment, navigational behavior analysis, indoor localization, social interaction method, and social policy learning.\n\nWe have developed both wearable and robotic navigation interfaces and evaluated them in several different environments with the target users who have visual impairments. \n\nSmartphone navigation system, NavCog has been deployed on university campus, shopping mall, airport, hotel, hospital, and museum. The system utilizes its IMU and camera image, and also BLE signals from off-the-shelf beacons placed in the environment to provide accurate (1-2 meters in average) localization accuracy, which is enough for navigation for visually impaired people. Several user studies have been conducted and proved that the system effectively help the users navigate in those environments. We have reported detailed analysis of user's behavior during the navigation in terms of navigational context, expertise in navigation, personalized dynamics, environmental factors, and validity of reactions to instructions. \n\nAdditional works explored virtual navigation experience with a smartphone system, which enabled users to explore and build mental maps of the environments before actually navigating in the real world. \n\nAs for the robotic interfaces, we mainly focused on social interactions. We have been developing a mobile robot that navigates the users in indoor space. Besides, we have studied state of the art algorithms to understand social dynamics in the environment to avoid social conflicts with the robotic interface including techniques of collision prediction with pedestrians, social policy learning, recognition of interaction among groups of people, recognition of head gestures and body parts. A robotic interface which implements collision prediction and alert function has been tested in airport environments and proved its effectiveness. \n\n\t\t\t\t\tLast Modified: 12/20/2021\n\n\t\t\t\t\tSubmitted by: Kris Kitani"
 }
}