{
 "awd_id": "1625061",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI: Acquisition of a Flexible High-Performance Computing System for Data and Compute Driven Scientific Discovery",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rita Rodriguez",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2019-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2016-09-15",
 "awd_max_amd_letter_date": "2016-09-15",
 "awd_abstract_narration": "This project, acquiring High-Performance Computing (HPC) resources for data and compute-driven scientific discovery, aims to replace and extend outdated current equipment at the core of the institution's HPC infrastructure. Since the proposed configuration is designed to particularly address deficiencies of the current infrastructure and thus adapt it to the needs of its widening user base, acquiring this instrumentation should enhance current HPC facilities and enable transformational research over a broad front. Central, shared, on-campus HPC facilities reduce barriers to access computational resources, especially for junior scientists, students, and faculty from disciplines that are not traditional HPC users. This equipment complements existing and future investments and helps drive related faculty and undergraduate research and educational activities across the College of Science and Technology (CST) and the entire Temple community. With an enrollment around 38,000 graduate and undergraduate students, the institution boasts of a very diverse student body. With its large shared memory pool, the requested machine will be the first of its kind in the Greater Philadelphia region that has a high concentration of high-tech and IT-related industries. \r\n\r\nThey will obtain 1. A large shared memory machine with 9TB addressable RAM, 2. An extensible, scalable parallel storage appliance with 1.5PB capacity, and 3 A 60-node conventional Linux cluster tuned for small to medium size parallel calculations. Computational science, materials science, biophysics, data analytics, and genomics are major themes running through the most active areas of CST research. Various centers and institutes reflect these themes, such as the 1. Institute for Genomics and Evolutionary Medicine, 2, Institute for Computational Molecular Science, 3.Center for Data Analytics and Biomedical Informatics, 4. Center for Biophysics and Computational Biology, and 5. Center for Computational Design of Functional Layered Materials. CST's, in tandem with the Colleges of Engineering and Liberal Arts, the Schools of Business and Pharmacy, the Fox Chase Cancer Center, and the School of Medicine aim to broadly impact research in big data, nano-scale science, imaging, neuroscience, as well as energy and health.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Klein",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Klein",
   "pi_email_addr": "mlklein@temple.edu",
   "nsf_id": "000526476",
   "pi_start_date": "2016-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Zoran",
   "pi_last_name": "Obradovic",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zoran Obradovic",
   "pi_email_addr": "zoran.obradovic@temple.edu",
   "nsf_id": "000205559",
   "pi_start_date": "2016-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sudhir",
   "pi_last_name": "Kumar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sudhir Kumar",
   "pi_email_addr": "s.kumar@temple.edu",
   "nsf_id": "000248267",
   "pi_start_date": "2016-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sergei",
   "pi_last_name": "Pond",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Sergei K Pond",
   "pi_email_addr": "spond@temple.edu",
   "nsf_id": "000397072",
   "pi_start_date": "2016-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Temple University",
  "inst_street_address": "1805 N BROAD ST",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2157077547",
  "inst_zip_code": "191226104",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "PA02",
  "org_lgl_bus_name": "TEMPLE UNIVERSITY-OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "QD4MGHFDJKU1",
  "org_uei_num": "QD4MGHFDJKU1"
 },
 "perf_inst": {
  "perf_inst_name": "Temple University",
  "perf_str_addr": "1925 N. 12th St.",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191226018",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "PA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "118900",
   "pgm_ele_name": "Major Research Instrumentation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1189",
   "pgm_ref_txt": "MAJOR RESEARCH INSTRUMENTATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The objective of the project was to purchase and operate HPC equipmentfor data and compute driven scientific discovery to replace outdatedequipment at the core of Temple University's High-Performance Computinginfrastructure operated by the College of Science and Technology.&nbsp; Thisincluded several large RAM shared memory machines, several GPUaccelerated compute nodes, and many conventional Linux cluster nodes forsmall to medium size parallel calculations as well as an extensible,scalable parallel storage appliance. All units are interconnected with ahigh-speed network.<br />During the 3-year funded project period, the HPC infrastructure hasprocessed almost 500 thousand jobs from more than 200 users representing30 different research groups. The HPC infrastructure has provided almost100 million CPU hours out of a theoretical maximum of about 120 millionCPU hours. Supported are research groups in Physics, Chemistry, Biology,Mathematics, Computer Science, Statistical Science, Data Analytics,Psychology, Earth and Environmental Science, Mechanical Engineering,Electrical Engineering, Medical Science, Economics, Public Health, andSociology.<br />Users of the HPC infrastructure have thus far reported over 75publications in scientific journals and conference proceedings thatacknowledge using the resources supported by the grant during early useraccess and regular production.<br />The project served to build the expertise of the HPC staff, which isalso employing the experience gained to develop classes on HPC hardwaredeployment taught in a new Professional Science Master inHigh-Performance Computing, both at Temple and at an external Master Programfor High-Performance Computing.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/28/2020<br>\n\t\t\t\t\tModified by: Michael&nbsp;L&nbsp;Klein</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1625061/1625061_10462270_1580220986481_cluster-picture--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1625061/1625061_10462270_1580220986481_cluster-picture--rgov-800width.jpg\" title=\"Cluster\"><img src=\"/por/images/Reports/POR/2020/1625061/1625061_10462270_1580220986481_cluster-picture--rgov-66x44.jpg\" alt=\"Cluster\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Picture of the Owlsnest</div>\n<div class=\"imageCredit\">Axel Kohlmeyer</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Klein</div>\n<div class=\"imageTitle\">Cluster</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1625061/1625061_10462270_1580221059037_mri-user-distribution--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1625061/1625061_10462270_1580221059037_mri-user-distribution--rgov-800width.jpg\" title=\"User Distribution\"><img src=\"/por/images/Reports/POR/2020/1625061/1625061_10462270_1580221059037_mri-user-distribution--rgov-66x44.jpg\" alt=\"User Distribution\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">User Distribution Table</div>\n<div class=\"imageCredit\">Axel Kohlmeyer</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Klein</div>\n<div class=\"imageTitle\">User Distribution</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe objective of the project was to purchase and operate HPC equipmentfor data and compute driven scientific discovery to replace outdatedequipment at the core of Temple University's High-Performance Computinginfrastructure operated by the College of Science and Technology.  Thisincluded several large RAM shared memory machines, several GPUaccelerated compute nodes, and many conventional Linux cluster nodes forsmall to medium size parallel calculations as well as an extensible,scalable parallel storage appliance. All units are interconnected with ahigh-speed network.\nDuring the 3-year funded project period, the HPC infrastructure hasprocessed almost 500 thousand jobs from more than 200 users representing30 different research groups. The HPC infrastructure has provided almost100 million CPU hours out of a theoretical maximum of about 120 millionCPU hours. Supported are research groups in Physics, Chemistry, Biology,Mathematics, Computer Science, Statistical Science, Data Analytics,Psychology, Earth and Environmental Science, Mechanical Engineering,Electrical Engineering, Medical Science, Economics, Public Health, andSociology.\nUsers of the HPC infrastructure have thus far reported over 75publications in scientific journals and conference proceedings thatacknowledge using the resources supported by the grant during early useraccess and regular production.\nThe project served to build the expertise of the HPC staff, which isalso employing the experience gained to develop classes on HPC hardwaredeployment taught in a new Professional Science Master inHigh-Performance Computing, both at Temple and at an external Master Programfor High-Performance Computing.\n\n\t\t\t\t\tLast Modified: 01/28/2020\n\n\t\t\t\t\tSubmitted by: Michael L Klein"
 }
}