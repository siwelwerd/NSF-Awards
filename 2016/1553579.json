{
 "awd_id": "1553579",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: SentientCache: Rethinking the Cache Abstraction",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2016-01-01",
 "awd_exp_date": "2020-12-31",
 "tot_intn_awd_amt": 536539.0,
 "awd_amount": 552532.0,
 "awd_min_amd_letter_date": "2015-11-27",
 "awd_max_amd_letter_date": "2020-02-27",
 "awd_abstract_narration": "The SentientCache project focuses on improving the performance and cost-effectiveness of large-scale distributed systems by automatically learning what data should be memorized.  Inadequate performance of distributed systems, including those supporting major websites, is frustrating and costly. Performance problems are frequently addressed by using dedicated computers, so-called cache servers, for storing answers to common requests to reduce response time while also shielding the databases supporting the system from disruptively high loads.  The SentientCache project aims to revamp the design of modern caches to better utilize server resources. The project will produce theoretically and experimentally validated prototypes of cache systems that factor in external costs and contextual information, and learn from experience.\r\n\r\nThe SentientCache project will engage graduate and undergraduate students in innovative research through real-world workload analysis, software engineering and large-scale system experimentation.  As well, there is an educational outreach plan to expose undergraduates, high school and K-12 students to research in computer systems with exciting competition components for high-school and undergraduate students to promote computer science careers.\r\n\r\nSuccessful research outcomes from the project should advance the field of computer systems and shift the cache paradigm towards endowing system caches with greater intelligence and interaction with their environments in several ways:  The research will systematically characterize changes in workloads, scale, performance requirements and memory constraints of distributed caches, use advanced machine learning and other prediction techniques within systems research, and better determine how profiling can help to automatically perform cross-cutting performance optimization within large-scale systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ymir",
   "pi_last_name": "Vigfusson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ymir Vigfusson",
   "pi_email_addr": "ymir@mathcs.emory.edu",
   "nsf_id": "000687988",
   "pi_start_date": "2015-11-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Emory University",
  "inst_street_address": "201 DOWMAN DR NE",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4047272503",
  "inst_zip_code": "303221061",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "EMORY UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "S352L5PJLMP8"
 },
 "perf_inst": {
  "perf_inst_name": "Emory University",
  "perf_str_addr": "400 Dowman Drive",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303220001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 305840.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 79743.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 82208.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 84741.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>When we use the Internet, we want web sites to load fast. But with potentially billions of requests per second on some services, we need surprising amount of engineering to make such fast response possible. One of the key ideas is the&nbsp;<em>cache</em>: to select some data to store in fast but much smaller memory, and other data to store in much slower, but much larger memory. Today?s tech companies, large and small, have deployed many millions of computers exclusively as caches for web site data and help the web be faster. Yet little is understood about how exactly we should be using all these cache servers.</span></p>\n<p><span>The SentientCache project investigated how we can be even smarter about caches, focusing on developing better strategies regarding which data we should keep in fast memory, making decisions where the dollar costs are folded into the calculation, and developing a stronger theoretical foundation for this decision making. As part of the award, we made numerous research contributions towards these goals.</span></p>\n<p><span>First, we created&nbsp;</span><a title=\"Mithril\" href=\"https://www.ymsir.com/papers/mithril-socc.pdf\" target=\"_blank\"><span>Mithril</span></a><span>: a system that learns patterns of data being requested to ``pre-fetch?? data into a cache before it is even needed. It was able to make significantly better use of memory, yielding data from fast memory 36-55% more often than slow memory compared to run-of-the-mill cache approaches (LRU and AMP).</span></p>\n<p><span>We observed that on a local machine, say your laptop, caches and other ephemeral computation can fill up the available, and often precious, storage space. By being aware of the applications, we created a system,&nbsp;</span><a title=\"Carillon\" href=\"https://www.ymsir.com/papers/harmonium-systor.pdf\" target=\"_blank\"><span>Carillon</span></a><span>, that can imitate ``elastic?? storage ? allowing storage to look larger than it is and recreating or caching data on demand without needing input from the user. Our publication received a Best Student Paper award at the SYSTOR 2016 conference.</span></p>\n<p><span>When any of the millions of cache servers goes down, say the power cuts off the computer, there is a question regarding how to get it back up and running again. As part of our project, we devised a&nbsp;</span><a title=\"rule-of-thumb formula\" href=\"https://www.ymsir.com/papers/warmup-hotcloud.pdf\" target=\"_blank\"><span>rule-of-thumb formula</span></a><span>&nbsp;for determining the warm-up time of a cache until it is usable, allowing service to resume faster than before.</span></p>\n<p><span>Caches can also have significant impact on databases, particularly when we care about dollar costs. Through&nbsp;</span><a title=\"Mutant\" href=\"https://www.ymsir.com/papers/mutant-socc.pdf\" target=\"_blank\"><span>Mutant</span></a><span>, we enhanced databases built on a recent technology, so-called LVM-trees, to automatically and transparently move older, less-used data to slower but cheaper storage -- effectively turning the entire database into a cache.&nbsp;</span></p>\n<p><span>With the advent of non-volatile memories (NVM), we analyzed how they should be used together with existing memory technologies for caches. We&nbsp;</span><a title=\"CHOPT\" href=\"https://www.ymsir.com/papers/chopt-sigmetrics.pdf\" target=\"_blank\"><span>CHOPT</span></a><span>, an algorithm to determine what should optimally be kept in memory for a given workload of requests, which showed us that NVM for cache offers significant room for improvements over existing schemes. In our paper, we develop a theory that explains how and why the efficiency of a cache scheme can be estimated concisely without needing to run it on all possible inputs. We also show that when reasoning about caches on multiple layers, as NVM memory would entail, the analysis moves from studying what data to keep to a question about where data should be placed.&nbsp;CHOPT received a Best Student Paper award at the SIGMETRICS 2020 conference.</span></p>\n<p><span>We also observed the importance of smart caches in the machine learning systems that are becoming ubiquitous in most sectors. Machine learning models commonly require dedicated hardware when responding to requests, so-called inferences, but ineffective caching when serving multiple models can generate large waste and delay responses from the machine. We developed the&nbsp;</span><a title=\"Clockwork\" href=\"https://www.ymsir.com/papers/clockwork-osdi.pdf\" target=\"_blank\"><span>Clockwork&nbsp;</span></a><span>system, building on insights from our SentientCache work, to advance the state-of-the-art in model serving systems in machine learning, in particular making nearly 100% use of the available hardware resources while responding sufficiently quickly to each inference.&nbsp;Clockwork received a Distinguished Artifact Student Award award at the OSDI 2020 conference.</span></p>\n<p><span>To enable any scientist or enthusiast to investigate smarter cache strategies, we created an open platform, a Python-package called&nbsp;<a title=\"PyMimirCache\" href=\"http://mimircache.info/\" target=\"_blank\"><span>PyMimirCache</span></a>. Here, users can create new strategies and compare them against the myriad existing ones from the scientific literature and standardized traces. It is out hope that MimirCache both facilitates future research and engages students to study computer systems.</span></p>\n<p><span>To showcase MimirCache, we hosted an annual event at the Atlanta Science Festival, geared for K-12 and high-school students, and the general public. Here, we ``Unveiled the Internet?? by providing hands-on exercised about how these cache systems work under the hood. These events were attended by 50-75 young people each time, more than a third of whom identify as Latinx or Black (BIPOC).</span></p>\n<p><span>Finally, we also developed&nbsp;</span><a title=\"120-slide deck\" href=\"http://ymsir.com/fast19\" target=\"_blank\"><span>120-slide deck</span></a><span>&nbsp;about the current state of cache theory that we gave as 4-6 hours research conference tutorials on three separate occasions, and which has been adopted into some computer science curricula.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/18/2021<br>\n\t\t\t\t\tModified by: Ymir&nbsp;Vigfusson</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1553579/1553579_10411748_1621309195161_sigmetrics-award--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1553579/1553579_10411748_1621309195161_sigmetrics-award--rgov-800width.jpg\" title=\"Best Student Paper Award at SIGMETRICS 2020\"><img src=\"/por/images/Reports/POR/2021/1553579/1553579_10411748_1621309195161_sigmetrics-award--rgov-66x44.jpg\" alt=\"Best Student Paper Award at SIGMETRICS 2020\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Best Student Paper Award at SIGMETRICS 2020</div>\n<div class=\"imageCredit\">SIGMETRICS</div>\n<div class=\"imageSubmitted\">Ymir&nbsp;Vigfusson</div>\n<div class=\"imageTitle\">Best Student Paper Award at SIGMETRICS 2020</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1553579/1553579_10411748_1621369704053_osdi-award--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1553579/1553579_10411748_1621369704053_osdi-award--rgov-800width.jpg\" title=\"OSDI 2020 award\"><img src=\"/por/images/Reports/POR/2021/1553579/1553579_10411748_1621369704053_osdi-award--rgov-66x44.jpg\" alt=\"OSDI 2020 award\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Distinguished Artifact Award at the OSDI conference 2020.</div>\n<div class=\"imageCredit\">USENIX</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Ymir&nbsp;Vigfusson</div>\n<div class=\"imageTitle\">OSDI 2020 award</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nWhen we use the Internet, we want web sites to load fast. But with potentially billions of requests per second on some services, we need surprising amount of engineering to make such fast response possible. One of the key ideas is the cache: to select some data to store in fast but much smaller memory, and other data to store in much slower, but much larger memory. Today?s tech companies, large and small, have deployed many millions of computers exclusively as caches for web site data and help the web be faster. Yet little is understood about how exactly we should be using all these cache servers.\n\nThe SentientCache project investigated how we can be even smarter about caches, focusing on developing better strategies regarding which data we should keep in fast memory, making decisions where the dollar costs are folded into the calculation, and developing a stronger theoretical foundation for this decision making. As part of the award, we made numerous research contributions towards these goals.\n\nFirst, we created Mithril: a system that learns patterns of data being requested to ``pre-fetch?? data into a cache before it is even needed. It was able to make significantly better use of memory, yielding data from fast memory 36-55% more often than slow memory compared to run-of-the-mill cache approaches (LRU and AMP).\n\nWe observed that on a local machine, say your laptop, caches and other ephemeral computation can fill up the available, and often precious, storage space. By being aware of the applications, we created a system, Carillon, that can imitate ``elastic?? storage ? allowing storage to look larger than it is and recreating or caching data on demand without needing input from the user. Our publication received a Best Student Paper award at the SYSTOR 2016 conference.\n\nWhen any of the millions of cache servers goes down, say the power cuts off the computer, there is a question regarding how to get it back up and running again. As part of our project, we devised a rule-of-thumb formula for determining the warm-up time of a cache until it is usable, allowing service to resume faster than before.\n\nCaches can also have significant impact on databases, particularly when we care about dollar costs. Through Mutant, we enhanced databases built on a recent technology, so-called LVM-trees, to automatically and transparently move older, less-used data to slower but cheaper storage -- effectively turning the entire database into a cache. \n\nWith the advent of non-volatile memories (NVM), we analyzed how they should be used together with existing memory technologies for caches. We CHOPT, an algorithm to determine what should optimally be kept in memory for a given workload of requests, which showed us that NVM for cache offers significant room for improvements over existing schemes. In our paper, we develop a theory that explains how and why the efficiency of a cache scheme can be estimated concisely without needing to run it on all possible inputs. We also show that when reasoning about caches on multiple layers, as NVM memory would entail, the analysis moves from studying what data to keep to a question about where data should be placed. CHOPT received a Best Student Paper award at the SIGMETRICS 2020 conference.\n\nWe also observed the importance of smart caches in the machine learning systems that are becoming ubiquitous in most sectors. Machine learning models commonly require dedicated hardware when responding to requests, so-called inferences, but ineffective caching when serving multiple models can generate large waste and delay responses from the machine. We developed the Clockwork system, building on insights from our SentientCache work, to advance the state-of-the-art in model serving systems in machine learning, in particular making nearly 100% use of the available hardware resources while responding sufficiently quickly to each inference. Clockwork received a Distinguished Artifact Student Award award at the OSDI 2020 conference.\n\nTo enable any scientist or enthusiast to investigate smarter cache strategies, we created an open platform, a Python-package called PyMimirCache. Here, users can create new strategies and compare them against the myriad existing ones from the scientific literature and standardized traces. It is out hope that MimirCache both facilitates future research and engages students to study computer systems.\n\nTo showcase MimirCache, we hosted an annual event at the Atlanta Science Festival, geared for K-12 and high-school students, and the general public. Here, we ``Unveiled the Internet?? by providing hands-on exercised about how these cache systems work under the hood. These events were attended by 50-75 young people each time, more than a third of whom identify as Latinx or Black (BIPOC).\n\nFinally, we also developed 120-slide deck about the current state of cache theory that we gave as 4-6 hours research conference tutorials on three separate occasions, and which has been adopted into some computer science curricula.\n\n \n\n\t\t\t\t\tLast Modified: 05/18/2021\n\n\t\t\t\t\tSubmitted by: Ymir Vigfusson"
 }
}