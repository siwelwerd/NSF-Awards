{
 "awd_id": "1617761",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: A Theory of Human Microstrategy Selection and Integration in Human-Computer Interaction.",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2016-07-15",
 "awd_exp_date": "2020-06-30",
 "tot_intn_awd_amt": 76815.0,
 "awd_amount": 76815.0,
 "awd_min_amd_letter_date": "2016-07-26",
 "awd_max_amd_letter_date": "2016-07-26",
 "awd_abstract_narration": "Better science is needed to predict how people will interact with new computer systems in dual-task settings, such to operate on board navigation systems while driving. Existing psychological theory can inform the problem but does not adequately address many issues such as that, in many dual-task circumstances, two subtasks become so interleaved that they become an altogether new third task. This project will investigate the cognitive strategies that people form, sometimes subconsciously, to integrate human information processes (such eye movements and hand movements). The project will provide scientific theory that can be used to analyze and improve the usability of a broad range of user interfaces that are used multitasking settings, including commonly-used handheld devices and in-vehicle interfaces, as well as specialized interfaces in mission control centers, nuclear power plants, and emergency rooms.\r\n\r\nThe project will (a) develop a theory of Human Microstrategy Selection and Integration (HMSI) that will predict how two tasks would be interleaved, with a special emphasis on the formation of complex cognitive strategies, (b) apply the HMSI theory as the foundation for a new methodology that analysts can use to transform two conventional hierarchical task analysis (HTA) diagrams, one for each of the two tasks, into an interleaved set of microstrategies, and (c) evaluate the HMSI theory and methodology with human experiments, including the analysis of the eye movements that people make when interacting with an on board navigation system in a driving simulator. The project will advance a theory of cognitive psychology and provide a scientific approach for solving human-computer interaction problems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Kieras",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "David E Kieras",
   "pi_email_addr": "kieras@umich.edu",
   "nsf_id": "000372630",
   "pi_start_date": "2016-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "2260 Hayward",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 76815.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project advances an understanding of how people interact with new computer systems in a dual-task setting, such operating a new on-board navigation system while driving. It is generally understood that people develop cognitive strategies - mental plans for how to get things done - for individual tasks. But it is less well-understood how, when faced with a dual task - doing two different things at the same time - people will integrate two single-task strategies into a dual-task strategy. For example, a driver might look at the navigation system screen, look back at the road, and then push a button on the navigation system while keeping his or her eyes on the road. But it is not well-understood how the driver would subconsciously decide on the exact order of eye and hand movements that will safely accomplish both tasks at the same time.&nbsp;</p>\n<p>This effort contributes to the field of human-computer interaction by providing a theoretical approach that can predict, in advance, how well people could use a new computer system before that system is even built. The theory thus informs the design of useful, usable, and safe user-interfaces.</p>\n<p>The project achieves this new theoretical understanding by means of a detailed analysis of human actions, including eye movements, from a dual-task experiment, and then building computer programs called <em>cognitive models</em> that simulate people doing the experimental dual-task. The cognitive models represent the human strategies in the form of <em>production rules</em>, that describe the individual steps in the strategy, as in \"If you need to check how far until the next exit and there are no nearby cars, then move the eyes to the navigation screen.\"</p>\n<p>The project explored three fundamentally different characterizations of how people integrate, or coordinate, single-task strategies in a dual-task setting. The three approaches are (a) Specialized Executive with Stand-Alone Tasks, (b) General Executive with Specialized Tasks, and (c) Single Integrated Task.&nbsp;</p>\n<p>(a) The Specialized Executive with Stand-Alone Tasks approach assumes that the two single-task strategies are maintained independently; for example, the single-task strategy for driving is not modified at all to assist with the navigation task. Rather, a third task strategy, a \"Specialized Executive\", moves the eyes and starts and stops the two single-task strategies as needed to ensure that both tasks get done adequately well. There are some \"overhead\" time delays in this process that reduce performance.</p>\n<p>(b) The General Executive with Specialized Tasks approach is similar to (a) but uses a general-purpose executive strategy that allows each single-task strategy to request use of the eyes or hands according to a priority scheme; for example, the driving task can claim the eyes from the navigation task if the eyes have been off the road for too long. Compared to approach (a), performance is better and the overhead is less, because the two task strategies, with the help of the executive, simply share use of the eyes and hands as needed.</p>\n<p>(c) The Single Integrated Task approach assumes that people combine the two single-task strategies into a completely new complex single-task strategy that performs the entire dual-task as if it were one complicated task, such as \"navigating-and-driving\". If the new strategy is carefully constructed, this approach has by far the least overhead and best performance.</p>\n<p>Each of these three approaches represents a different theory of how people organize dual-task strategies, but there is also the practical consideration of how easy it is for an interface designer to construct a model to predict user performance. For example, to use the Specialized Executive with Stand-Alone Tasks approach, a designer only needs to describe the strategy necessary to use a new navigation interface, and then describe the executive strategy that coordinates the new strategy with a previously-existing strategy for driving. But the Single Integrated Task approach, at the other extreme, would require the designer to create a substantially more complicated and intricate strategy.&nbsp;</p>\n<p>The results of the project demonstrate that all three approaches can predict dual-task performance, and that these tradeoffs in terms of ease-of-use and predicting streamlined performance should guide the choice of modeling approach that is chosen for a particular design setting.</p>\n<p>These scientific advances benefit society by providing fundamental and practical theory that can be used to analyze and improve the usability of a broad range of user interfaces in multitasking settings, including commonly-used handheld and in-vehicle interfaces, as well as specialized interfaces in mission control centers, nuclear power plants, and emergency rooms.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/16/2020<br>\n\t\t\t\t\tModified by: David&nbsp;E&nbsp;Kieras</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project advances an understanding of how people interact with new computer systems in a dual-task setting, such operating a new on-board navigation system while driving. It is generally understood that people develop cognitive strategies - mental plans for how to get things done - for individual tasks. But it is less well-understood how, when faced with a dual task - doing two different things at the same time - people will integrate two single-task strategies into a dual-task strategy. For example, a driver might look at the navigation system screen, look back at the road, and then push a button on the navigation system while keeping his or her eyes on the road. But it is not well-understood how the driver would subconsciously decide on the exact order of eye and hand movements that will safely accomplish both tasks at the same time. \n\nThis effort contributes to the field of human-computer interaction by providing a theoretical approach that can predict, in advance, how well people could use a new computer system before that system is even built. The theory thus informs the design of useful, usable, and safe user-interfaces.\n\nThe project achieves this new theoretical understanding by means of a detailed analysis of human actions, including eye movements, from a dual-task experiment, and then building computer programs called cognitive models that simulate people doing the experimental dual-task. The cognitive models represent the human strategies in the form of production rules, that describe the individual steps in the strategy, as in \"If you need to check how far until the next exit and there are no nearby cars, then move the eyes to the navigation screen.\"\n\nThe project explored three fundamentally different characterizations of how people integrate, or coordinate, single-task strategies in a dual-task setting. The three approaches are (a) Specialized Executive with Stand-Alone Tasks, (b) General Executive with Specialized Tasks, and (c) Single Integrated Task. \n\n(a) The Specialized Executive with Stand-Alone Tasks approach assumes that the two single-task strategies are maintained independently; for example, the single-task strategy for driving is not modified at all to assist with the navigation task. Rather, a third task strategy, a \"Specialized Executive\", moves the eyes and starts and stops the two single-task strategies as needed to ensure that both tasks get done adequately well. There are some \"overhead\" time delays in this process that reduce performance.\n\n(b) The General Executive with Specialized Tasks approach is similar to (a) but uses a general-purpose executive strategy that allows each single-task strategy to request use of the eyes or hands according to a priority scheme; for example, the driving task can claim the eyes from the navigation task if the eyes have been off the road for too long. Compared to approach (a), performance is better and the overhead is less, because the two task strategies, with the help of the executive, simply share use of the eyes and hands as needed.\n\n(c) The Single Integrated Task approach assumes that people combine the two single-task strategies into a completely new complex single-task strategy that performs the entire dual-task as if it were one complicated task, such as \"navigating-and-driving\". If the new strategy is carefully constructed, this approach has by far the least overhead and best performance.\n\nEach of these three approaches represents a different theory of how people organize dual-task strategies, but there is also the practical consideration of how easy it is for an interface designer to construct a model to predict user performance. For example, to use the Specialized Executive with Stand-Alone Tasks approach, a designer only needs to describe the strategy necessary to use a new navigation interface, and then describe the executive strategy that coordinates the new strategy with a previously-existing strategy for driving. But the Single Integrated Task approach, at the other extreme, would require the designer to create a substantially more complicated and intricate strategy. \n\nThe results of the project demonstrate that all three approaches can predict dual-task performance, and that these tradeoffs in terms of ease-of-use and predicting streamlined performance should guide the choice of modeling approach that is chosen for a particular design setting.\n\nThese scientific advances benefit society by providing fundamental and practical theory that can be used to analyze and improve the usability of a broad range of user interfaces in multitasking settings, including commonly-used handheld and in-vehicle interfaces, as well as specialized interfaces in mission control centers, nuclear power plants, and emergency rooms.\n\n \n\n\t\t\t\t\tLast Modified: 11/16/2020\n\n\t\t\t\t\tSubmitted by: David E Kieras"
 }
}