{
 "awd_id": "1558151",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Multimodal State Estimation through Neural Coherence in the Parieto-Frontal Network",
 "cfda_num": "47.074",
 "org_code": "08090000",
 "po_phone": "7032928167",
 "po_email": "ethiels@nsf.gov",
 "po_sign_block_name": "Edda Thiels",
 "awd_eff_date": "2016-09-15",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 1271492.0,
 "awd_amount": 1271492.0,
 "awd_min_amd_letter_date": "2016-09-15",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "To distinguish parts of our body from other objects around us, the brain needs to build an internal image of our body by merging information from the skin, muscles and joints with information from our eyes.  This project is aimed at characterizing how we build our sense of self, by using multisite brain recordings and virtual reality technologies.  The results will impact national needs in the consumer, healthcare, military, and industrial settings by advancing the fundamental engineering and neuroscience knowledge necessary to create the next generation of brain-machine interfaces, which are envisioned to support the integration of artificial and natural sensory information.  Optimizing these systems depends critically on understanding how natural sensory signals interact within and among brain areas, a knowledge gap that directly addressed by this project.  The educational goals associated with the project are designed to advance discovery and understanding of engineering and neuroscience, while also promoting teaching, training, and learning beyond the regular bounds of these disciplines. These goals are achieved by: 1) Engaging high school students underrepresented in STEM fields through the development of hands-on instructional modules, 2) Promoting interdisciplinary undergraduate research opportunities via internships at Arizona State University, 3) Mentoring students in the broader implications of scientific research through exposure to organizations engaged in the ethical, societal, and policy implications of neuroscience research, and 4) Engaging the public in scientific discourse through public lectures and exhibits, and thereby promoting broad dissemination of the work to enhance scientific and technological understanding.\r\n\r\nEstimating the state of the body through the integration of available sensory cues (multimodal state estimation) is a critical integrative function for most organisms.  Although much is known about state estimation for the upper limb at the behavioral level, the underlying neural mechanisms remain poorly understood in cortical areas, particularly at the network level.  This is due to several factors: 1) the cortical areas believed to play a role in limb state estimation are heterogenous with regard to the relative strength of their sensory inputs and display both multisensory enhancement and suppression depending on context; 2) technical limitations mean functional interactions among these areas have been challenging to characterize; 3) the relation between sensitivity to visual and somatic cues and prevailing computational theories of multisensory integration have been incompletely explored; 4) multimodal areas are thought to contribute to both perceptual and action-based body representations but how these representations interact at the neural and behavioral levels is not well understood.  As a result, it is unclear how a coherent multimodal estimate of the state of the upper limb is constructed and maintained.  The proposed series of studies address these issues by quantifying changes in neural spiking, local field potentials, and neural coherence within and among fronto-parietal areas of the monkey implicated in state estimation using virtual reaching tasks that alter the reliability and semantic information of visual cues.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "BIO",
 "org_dir_long_name": "Directorate for Biological Sciences",
 "div_abbr": "IOS",
 "org_div_long_name": "Division Of Integrative Organismal Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Buneo",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher A Buneo",
   "pi_email_addr": "christopher.buneo@asu.edu",
   "nsf_id": "000177914",
   "pi_start_date": "2016-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852876011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "534200",
   "pgm_ele_name": "Disability & Rehab Engineering"
  },
  {
   "pgm_ele_code": "771400",
   "pgm_ele_name": "Modulation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1096",
   "pgm_ref_txt": "NEURAL SYSTEMS"
  },
  {
   "pgm_ref_code": "1228",
   "pgm_ref_txt": "MINORITY INVOLVEMENT -- BIO"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 721492.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 200000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 250000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"Default\">Multisensory integration is the process by which information from different sensory modalities is combined by the nervous system to facilitate perception and action. Understanding this process is important not only from a basic science perspective but also for translational reasons, e.g., for the development of closed-loop neural prostheses. Here, a mixed reality platform was developed to study the neural mechanisms of multisensory integration for the upper limb (&lsquo;arm&rsquo;) during movement planning. The platform allowed for selection of different virtual avatar arms and manipulation of the locations of physical and virtual starting positions and targets in the environment (Fig. 1). The system was tested with two non-human primates trained to reach to multiple virtual targets in a reaction time task. Arm kinematic data as well as neural data (i.e., spikes and local field potentials) were recorded and analyzed. In one animal (monkey Q), neural data were obtained from four chronically implanted, 32-channel microelectrode arrays - two each in motor cortex (M1) and dorsal premotor cortex (PMd). In the second animal, six chronically implanted, 32-channel microelectrode arrays were used - two each in M1, PMd, and posterior parietal cortex (PPC).</p>\n<p class=\"Default\">The behavioral paradigm involved manipulating visual information about initial arm position by rendering the virtual avatar arm either in its actual position (veridical (V) condition) or in a shifted position (perturbed (P) condition) prior to movement onset. Tactile feedback was modulated in blocks by placing or removing the physical start cue on the table (tactile (T) and no-tactile (NT) conditions, respectively). Behaviorally, errors in initial movement direction were found to be larger when the physical start cue was absent. In addition, slightly larger directional errors were found in the P condition compared to the V condition for some movement directions (Figs. 2-3). Both effects are consistent with the idea that erroneous or reduced information about initial hand location led to movement direction-dependent reach planning errors. Moreover, the results suggest that tactile cues are integrated, along with visual and proprioceptive ones, to form an accurate and precise representation of initial arm position that is used for movement planning. To our knowledge, this result has not been previously demonstrated in either animal or human subjects.</p>\n<p class=\"Default\">Neural correlates of these behavioral effects have been probed to date using population decoding techniques. We have initially focused on the decoding of movement/target direction from spiking activity recorded in each of the four conditions.&nbsp;For 6 cm backward shifts in the visual position of the arm, small differences in decoding accuracy between the T and NT conditions were observed, but only in PMd (Fig. 4). No differences in accuracy were observed in M1. For larger visual shifts (9 cm), decoding accuracy decreased in the NT condition, but this effect was again mostly evident in PMd (Fig. 5). We also explored the effect of varying the initial starting position of the arm on neural activity and found larger reductions in decoding accuracy in the NT condition for 6 cm shifts when a less familiar starting position was used. Again, these differences in accuracy were only observed in PMd. Thus, activity in PMd, but not M1, may reflect the uncertainty in reach planning that results when sensory cues regarding initial hand position are erroneous or absent.</p>\n<p class=\"Default\">Overall, our behavioral results indicate that visual and tactile cues influence the planning of reaches performed in a virtual environment. The consistent nature of the observed effects suggest that visual and tactile cues are integrated along with proprioception to form a robust estimate of initial arm position that is used in movement planning. Moreover, our neural results suggest that activity in PMd, but not M1, appears to reflect this integration process. This information may help to advance the development of closed-loop neural prostheses and related assistive technologies designed to improve the quality of life of individuals with sensorimotor disabilities.</p><br>\n<p>\n Last Modified: 02/28/2024<br>\nModified by: Christopher&nbsp;A&nbsp;Buneo</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nMultisensory integration is the process by which information from different sensory modalities is combined by the nervous system to facilitate perception and action. Understanding this process is important not only from a basic science perspective but also for translational reasons, e.g., for the development of closed-loop neural prostheses. Here, a mixed reality platform was developed to study the neural mechanisms of multisensory integration for the upper limb (arm) during movement planning. The platform allowed for selection of different virtual avatar arms and manipulation of the locations of physical and virtual starting positions and targets in the environment (Fig. 1). The system was tested with two non-human primates trained to reach to multiple virtual targets in a reaction time task. Arm kinematic data as well as neural data (i.e., spikes and local field potentials) were recorded and analyzed. In one animal (monkey Q), neural data were obtained from four chronically implanted, 32-channel microelectrode arrays - two each in motor cortex (M1) and dorsal premotor cortex (PMd). In the second animal, six chronically implanted, 32-channel microelectrode arrays were used - two each in M1, PMd, and posterior parietal cortex (PPC).\n\n\nThe behavioral paradigm involved manipulating visual information about initial arm position by rendering the virtual avatar arm either in its actual position (veridical (V) condition) or in a shifted position (perturbed (P) condition) prior to movement onset. Tactile feedback was modulated in blocks by placing or removing the physical start cue on the table (tactile (T) and no-tactile (NT) conditions, respectively). Behaviorally, errors in initial movement direction were found to be larger when the physical start cue was absent. In addition, slightly larger directional errors were found in the P condition compared to the V condition for some movement directions (Figs. 2-3). Both effects are consistent with the idea that erroneous or reduced information about initial hand location led to movement direction-dependent reach planning errors. Moreover, the results suggest that tactile cues are integrated, along with visual and proprioceptive ones, to form an accurate and precise representation of initial arm position that is used for movement planning. To our knowledge, this result has not been previously demonstrated in either animal or human subjects.\n\n\nNeural correlates of these behavioral effects have been probed to date using population decoding techniques. We have initially focused on the decoding of movement/target direction from spiking activity recorded in each of the four conditions.For 6 cm backward shifts in the visual position of the arm, small differences in decoding accuracy between the T and NT conditions were observed, but only in PMd (Fig. 4). No differences in accuracy were observed in M1. For larger visual shifts (9 cm), decoding accuracy decreased in the NT condition, but this effect was again mostly evident in PMd (Fig. 5). We also explored the effect of varying the initial starting position of the arm on neural activity and found larger reductions in decoding accuracy in the NT condition for 6 cm shifts when a less familiar starting position was used. Again, these differences in accuracy were only observed in PMd. Thus, activity in PMd, but not M1, may reflect the uncertainty in reach planning that results when sensory cues regarding initial hand position are erroneous or absent.\n\n\nOverall, our behavioral results indicate that visual and tactile cues influence the planning of reaches performed in a virtual environment. The consistent nature of the observed effects suggest that visual and tactile cues are integrated along with proprioception to form a robust estimate of initial arm position that is used in movement planning. Moreover, our neural results suggest that activity in PMd, but not M1, appears to reflect this integration process. This information may help to advance the development of closed-loop neural prostheses and related assistive technologies designed to improve the quality of life of individuals with sensorimotor disabilities.\t\t\t\t\tLast Modified: 02/28/2024\n\n\t\t\t\t\tSubmitted by: ChristopherABuneo\n"
 }
}