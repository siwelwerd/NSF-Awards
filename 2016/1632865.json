{
 "awd_id": "1632865",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Nonlinear and Data-Adaptive Compressive Sampling for Big Data Processing",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "chengshan xiao",
 "awd_eff_date": "2015-12-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 114204.0,
 "awd_amount": 114204.0,
 "awd_min_amd_letter_date": "2016-02-29",
 "awd_max_amd_letter_date": "2016-02-29",
 "awd_abstract_narration": "As pervasive sensors continuously collect and record massive amounts of\r\nhigh-dimensional data from communication, social, and biological networks,\r\nand growing storage as well as processing capacities of modern computers\r\nhave provided new and powerful ways to dig into such huge quantities of\r\ninformation, the need for novel analytic tools to comb through these \"big\r\ndata\" becomes imperative. The objective of this project is to develop a\r\nnovel framework for nonlinear, data-adaptive (de)compression algorithms to\r\nlearn the latent structure within large-scale, incomplete or corrupted\r\ndatasets for compressing and storing only the essential information, for\r\nrunning analytics in real time, inferring missing pieces of a dataset, and\r\nfor reconstructing the original data from their compressed renditions. \r\n\r\nThe intellectual merit lies in the exploration of the fertile but largely\r\nunexplored areas of manifold learning, nonlinear dimensionality reduction,\r\nand sparsity-aware techniques for compression and recovery of missing and\r\ncompromised measurements. Capitalizing on recent advances in machine\r\nlearning and signal processing, differential geometry, sparsity, and\r\ndictionary learning are envisioned as key enablers. Effort will be put also\r\ninto developing online and distributed (non)linear dimensionality reduction\r\nalgorithms to allow for streaming analytics of sequential measurements\r\nusing parallel processors. \r\n\r\nThe broader impact is to contribute to the development of novel \r\ncomputational methods and tools useful for data inference, cleansing, \r\nforecasting, and collaborative filtering, with direct impact to \r\nstatistical signal processing and machine learning applications\r\nto large-scale data analysis, including communication, social, and\r\nbiological networks.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Konstantinos",
   "pi_last_name": "Slavakis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Konstantinos Slavakis",
   "pi_email_addr": "kslavaki@buffalo.edu",
   "nsf_id": "000637068",
   "pi_start_date": "2016-02-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Buffalo",
  "inst_street_address": "520 LEE ENTRANCE STE 211",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "7166452634",
  "inst_zip_code": "142282577",
  "inst_country_name": "United States",
  "cong_dist_code": "26",
  "st_cong_dist_code": "NY26",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "GMZUKXFDJMA9",
  "org_uei_num": "LMCJKRFW5R81"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Buffalo",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "142607016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "26",
  "perf_st_cong_dist": "NY26",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "154E",
   "pgm_ref_txt": "Computat systems & security"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8084",
   "pgm_ref_txt": "CDS&E"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 114204.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The turn of the decade has trademarked society and computing research with a ``data deluge.'' As the number of smart and internet-capabledevices increases, so does the amount of data that is generated andcollected. While it is desirable to mine information from this data,their sheer amount and dimensionality introduces numerous challengesin their processing, since available statistical inference and machinelearning approaches do not necessarily scale well with the number ofdata and their dimensionality. In addition, as the cost of cloudcomputing is rapidly declining, there is a need for redesigning thosetraditional approaches to take advantage of the flexibility that hasemerged from distributing required computations to multiple nodes, aswell as reducing the per-node computational burden.&nbsp;This project aimed at novel methods for revealing nonlinear,data-adaptive (de)compression, and (unsupervised) learning fromhigh-dimensional data, as well as fundamental insights into thevarious mathematical and statistical trade-offs involved, and atoffering algorithms which overcome the emerging practicalissues. Capitalizing on advances in machine learning and signalprocessing, topics such as stochastic optimization, differentialgeometry, sparsity, dictionary learning, and randomized algorithmsplayed key roles in the development of the development of theresearch. Emphasis was placed on the following research thrusts:</p>\n<p><br />(R1) Clustering (unsupervised classification) methods which identifylatent information within high-dimensional data, where a novelmulti-manifold modeling (MMM) approach was developed to accommodatedata that lie in Riemannian manifolds;</p>\n<p><br />(R2) Data-adaptive random-sketching techniques for efficient learningfrom voluminous data, where a general framework for efficientclustering of huge sets of (possibly high-dimensional) data based onthe random sampling and consensus (RANSAC) ideas was established; and</p>\n<p><br />(R3) Online (non)linear dimensionality reduction algorithms to allowfor streaming analytics of sequential measurements, where a highlymodular, online-learning framework for block-wise convex (non-convexin general) tasks was developed by jointly leveraging the stochasticapproximation paradigm with advances on acceleration schemes that relyon the first-order information of convex objective functions.</p>\n<p><br />The developed methods have facilitated several application domainssuch as computer vision, (functional) magnetic resonance imaging, andsequential non-linear system identification, among others. Moreover,they have layed solid foundations and spurred discussions for furtheradvancements on dimensionality-reduction methods and algorithms inmachine learning and signal processing. More specifically, they haveestablished firm links between modern statistical tools, includingdictionary and manifold learning, with the fundamental tasks of signalcompression and recovery at computationally affordable and scalablelevels. The present research has helped also three PhD students,(co-)advised by the PI, to develop a better understanding ofcutting-edge research tools related to signal-processing andmachine-learning tasks. Concluding, the application of the developedtheoretical tools to important applications such as medical imagingand brain-network analytics will also play a principal role inpromoting the societal embracing of the recently emerging big-datascience.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/28/2016<br>\n\t\t\t\t\tModified by: Konstantinos&nbsp;Slavakis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe turn of the decade has trademarked society and computing research with a ``data deluge.'' As the number of smart and internet-capabledevices increases, so does the amount of data that is generated andcollected. While it is desirable to mine information from this data,their sheer amount and dimensionality introduces numerous challengesin their processing, since available statistical inference and machinelearning approaches do not necessarily scale well with the number ofdata and their dimensionality. In addition, as the cost of cloudcomputing is rapidly declining, there is a need for redesigning thosetraditional approaches to take advantage of the flexibility that hasemerged from distributing required computations to multiple nodes, aswell as reducing the per-node computational burden. This project aimed at novel methods for revealing nonlinear,data-adaptive (de)compression, and (unsupervised) learning fromhigh-dimensional data, as well as fundamental insights into thevarious mathematical and statistical trade-offs involved, and atoffering algorithms which overcome the emerging practicalissues. Capitalizing on advances in machine learning and signalprocessing, topics such as stochastic optimization, differentialgeometry, sparsity, dictionary learning, and randomized algorithmsplayed key roles in the development of the development of theresearch. Emphasis was placed on the following research thrusts:\n\n\n(R1) Clustering (unsupervised classification) methods which identifylatent information within high-dimensional data, where a novelmulti-manifold modeling (MMM) approach was developed to accommodatedata that lie in Riemannian manifolds;\n\n\n(R2) Data-adaptive random-sketching techniques for efficient learningfrom voluminous data, where a general framework for efficientclustering of huge sets of (possibly high-dimensional) data based onthe random sampling and consensus (RANSAC) ideas was established; and\n\n\n(R3) Online (non)linear dimensionality reduction algorithms to allowfor streaming analytics of sequential measurements, where a highlymodular, online-learning framework for block-wise convex (non-convexin general) tasks was developed by jointly leveraging the stochasticapproximation paradigm with advances on acceleration schemes that relyon the first-order information of convex objective functions.\n\n\nThe developed methods have facilitated several application domainssuch as computer vision, (functional) magnetic resonance imaging, andsequential non-linear system identification, among others. Moreover,they have layed solid foundations and spurred discussions for furtheradvancements on dimensionality-reduction methods and algorithms inmachine learning and signal processing. More specifically, they haveestablished firm links between modern statistical tools, includingdictionary and manifold learning, with the fundamental tasks of signalcompression and recovery at computationally affordable and scalablelevels. The present research has helped also three PhD students,(co-)advised by the PI, to develop a better understanding ofcutting-edge research tools related to signal-processing andmachine-learning tasks. Concluding, the application of the developedtheoretical tools to important applications such as medical imagingand brain-network analytics will also play a principal role inpromoting the societal embracing of the recently emerging big-datascience.\n\n \n\n\t\t\t\t\tLast Modified: 11/28/2016\n\n\t\t\t\t\tSubmitted by: Konstantinos Slavakis"
 }
}