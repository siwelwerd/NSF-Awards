{
 "awd_id": "1619098",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TWC: Small: Automatic Techniques for Evaluating and Hardening Machine Learning Classifiers in the Presence of Adversaries",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Wei-Shinn Ku",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 494884.0,
 "awd_amount": 494884.0,
 "awd_min_amd_letter_date": "2016-09-01",
 "awd_max_amd_letter_date": "2018-04-11",
 "awd_abstract_narration": "New security exploits emerge far faster than manual analysts can analyze them, driving growing interest in automated machine learning tools for computer security. Classifiers based on machine learning algorithms have shown promising results for many security tasks including malware classification and network intrusion detection, but classic machine learning algorithms are not designed to operate in the presence of adversaries. Intelligent and adaptive adversaries may actively manipulate the information they present in attempts to evade a trained classifier, leading to a competition between the designers of learning systems and attackers who wish to evade them. This project is developing automated techniques for predicting how well classifiers will resist the evasions of adversaries, along with general methods to automatically harden machine-learning classifiers against adversarial evasion attacks.\r\n\r\nAt the junction between machine learning and computer security, this project involves two main tasks: (1) developing a framework that can automatically assess the robustness of a classifier by using evolutionary techniques to simulate an adversary's efforts to evade that classifier; and (2) improving the robustness of classifiers by developing generic machine learning architectures that employ randomized models and co-evolution to automatically harden machine-learning classifiers against adversaries. Our system aims to allow a classifier designer to understand how the classification performance of a model degrades under evasion attacks, enabling better-informed and more secure design choices. The framework is general and scalable, and takes advantage of the latest advances in machine learning and computer security.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yanjun",
   "pi_last_name": "Qi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yanjun Qi",
   "pi_email_addr": "yq2h@virginia.edu",
   "nsf_id": "000663606",
   "pi_start_date": "2016-09-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Evans",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "David E Evans",
   "pi_email_addr": "evans@virginia.edu",
   "nsf_id": "000319935",
   "pi_start_date": "2016-09-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Westley",
   "pi_last_name": "Weimer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Westley Weimer",
   "pi_email_addr": "weimerw@umich.edu",
   "nsf_id": "000205470",
   "pi_start_date": "2016-09-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia",
  "perf_str_addr": "P. O. Box 400195",
  "perf_city_name": "Charlottesville",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229044195",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 494884.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-837b968a-7fff-c8db-ba0f-f1af775da5f2\">\n<p dir=\"ltr\"><span>At the junction between machine learning and computer security, this project&nbsp; developed automated techniques for predicting how well classifiers will resist the evasions of adversaries, along with general methods to automatically harden machine-learning classifiers against adversarial evasion attacks.</span></p>\n<p dir=\"ltr\"><span>This project has made contributions via four fronts:&nbsp;</span></p>\n<p dir=\"ltr\"><span>(1) Improving the robustness of classifiers by developing methods to automatically harden machine-learning classifiers against adversaries. Our main contribution is the &ldquo;feature squeezing&rdquo; techniques that reduce the search space for adversaries by coalescing inputs.&nbsp;</span></p>\n<p dir=\"ltr\"><span>(2) Developing techniques to automatically assess the robustness of a classifier.&nbsp; Our main contribution is a genetic-programming based evolutionary framework to automatically find variants that preserve malicious behavior but evade a target classifier.</span></p>\n<p dir=\"ltr\"><span>(3) </span><span>Understanding how deep learning classification performance degrades under evasion attacks, enabling better-informed and more secure design choices. Our contribution is a so-called &ldquo;deepWordBug&rdquo; framework generating natural language adversarial examples via greedy search under the black-box setting. We later revised deepWordBug via Monte Carlo Tree Search and Homoglyph Attack.&nbsp;</span></p>\n<p dir=\"ltr\"><span>(4) Understanding how deep learning classification performance degrades under evasion attacks and consider a budget. Our main contribution is a new suite of hybrid batch attacks finding black-box adversarial examples with limited queries.</span></p>\n<p dir=\"ltr\"><span>Our systems will allow a classifier designer to understand how the classification performance of a model degrades under evasion attacks, enabling better-informed and more secure design choices. The developed methods are general and scalable, taking advantage of the latest advances in machine learning and computer security. Computational methods, tools, and datasets developed in this proposal are available via url{https://qdata.github.io/secureml-web/}. </span></p>\n</span></p>\n<p>&nbsp;</p>\n<div></div><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/18/2019<br>\n\t\t\t\t\tModified by: Yanjun&nbsp;Qi</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2019/1619098/1619098_10457751_1576693833644_squeezing--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1619098/1619098_10457751_1576693833644_squeezing--rgov-800width.jpg\" title=\"Feature Squeezing\"><img src=\"/por/images/Reports/POR/2019/1619098/1619098_10457751_1576693833644_squeezing--rgov-66x44.jpg\" alt=\"Feature Squeezing\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Reduce search space for adversaries by coalescing inputs. (Top row shows &#8467;0 adversarial examples, squeezed by median smoothing.)</div>\n<div class=\"imageCredit\">Weilin Xu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yanjun&nbsp;Qi</div>\n<div class=\"imageTitle\">Feature Squeezing</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n\nAt the junction between machine learning and computer security, this project  developed automated techniques for predicting how well classifiers will resist the evasions of adversaries, along with general methods to automatically harden machine-learning classifiers against adversarial evasion attacks.\nThis project has made contributions via four fronts: \n(1) Improving the robustness of classifiers by developing methods to automatically harden machine-learning classifiers against adversaries. Our main contribution is the \"feature squeezing\" techniques that reduce the search space for adversaries by coalescing inputs. \n(2) Developing techniques to automatically assess the robustness of a classifier.  Our main contribution is a genetic-programming based evolutionary framework to automatically find variants that preserve malicious behavior but evade a target classifier.\n(3) Understanding how deep learning classification performance degrades under evasion attacks, enabling better-informed and more secure design choices. Our contribution is a so-called \"deepWordBug\" framework generating natural language adversarial examples via greedy search under the black-box setting. We later revised deepWordBug via Monte Carlo Tree Search and Homoglyph Attack. \n(4) Understanding how deep learning classification performance degrades under evasion attacks and consider a budget. Our main contribution is a new suite of hybrid batch attacks finding black-box adversarial examples with limited queries.\nOur systems will allow a classifier designer to understand how the classification performance of a model degrades under evasion attacks, enabling better-informed and more secure design choices. The developed methods are general and scalable, taking advantage of the latest advances in machine learning and computer security. Computational methods, tools, and datasets developed in this proposal are available via url{https://qdata.github.io/secureml-web/}. \n\n\n \n\n\n\t\t\t\t\tLast Modified: 12/18/2019\n\n\t\t\t\t\tSubmitted by: Yanjun Qi"
 }
}