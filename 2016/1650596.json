{
 "awd_id": "1650596",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Concurrent Data Structures",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-06-30",
 "tot_intn_awd_amt": 265044.0,
 "awd_amount": 265044.0,
 "awd_min_amd_letter_date": "2016-08-31",
 "awd_max_amd_letter_date": "2016-08-31",
 "awd_abstract_narration": "Most computer programming describes a sequence of steps to be carried out one by one by a single processor core.  As the rate of increase in processor speed has slowed, CPU manufacturers have responded by building systems with many processor cores that together carry out multiple tasks simultaneously.  These processors share a common memory that they use both for their own individual computations and for communication with other processors.  Organizing this shared memory so that the processors can make progress without being delayed by other processors requires careful coordination and the design of specialized data structures and communication protocols to allow efficient cooperation without conflicts.  The project will study how letting processors make random choices between different ways to accomplish the same task to improve the efficiency and amount of memory used by these data structures, an approach that appears to be necessary given known impossibility results for non-randomized methods.  This may significantly improve our ability to exploit the power of multicore machines, while simplifying the work of programmers of these machines.  In addition to this impact on computer science and the practice of programming, the project will directly impact both undergraduate and graduate research.  Because concurrent data structures are well-suited to undergraduate implementation projects, which avoid difficulties that often arise with involving undergraduates in more theoretical research, the project will serve as a bridge for recruiting students into cutting-edge, high-stakes research, including students from under-represented groups.  At the graduate level, results from the project will feed directly into the PI's teaching, including updates to the PI's publicly-available lecture notes already in use by many students at other institutions.\r\n\r\nThe main question considered by the project is: Can we remove bottlenecks in concurrent executions of programs in shared-memory system using data structures that avoid traditional concurrency control tools like locking?  Two techniques that appear promising are the use of randomization, which avoids problems where bad timing consistently produces bad executions in which different processes interfere with each others' operations, and limited-use assumptions, where shared data structures are designed under the assumption that they will only be used for a limited number of operations, allowing for significant reductions in cost and complexity.  In addition to applying these techniques individually to the design of concurrent shared-memory data structures, the project will also consider how these methods can complement each other, for example by the use of randomized helping techniques to transform short-lived limited-use data structures into long-lived unlimited-use data structures.  A key element of this work will be the development of improved cost measures for shared-memory data structures, including dynamic measures of space complexity that more accurately reflect practical memory usage than the worst-case measures of maximum memory consumption found in previous work.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Aspnes",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "James Aspnes",
   "pi_email_addr": "aspnes@cs.yale.edu",
   "nsf_id": "000193087",
   "pi_start_date": "2016-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "AKWatson Hall",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208285",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7934",
   "pgm_ref_txt": "PARAL/DISTRIBUTED ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 265044.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The primary goal of the project was to study space complexity in distributed algorithms. Space complexity measures the amount of memory needed to carry out some task. Distributed algorithms are methods for coordinating multiple computers across some sort of network or other communication medium.</p>\n<p>Previous work on space complexity looked at the worst-case cost, the maximum amount of memory needed to carry out a task in the worst possible execution. The justification for this approach has been the assumption that memory cannot be used without being pre-allocated. This assumption is violated by modern computer systems that allow allocation of memory as needed, or when it is tolerable for an algorithm to fail in the rare cases where extra memory turns out to be needed. The cost of requiring pre-allocation is that many problems in distributed algorithms are known to have very high space costs in the worst case.</p>\n<p>The project considered an alternative approach where an algorithm is charged only for the memory it uses in a particular execution. We showed that this allows for much more space-efficient algorithms for fundamental coordination problems like mutual exclusion, where we wish to prevent more than one of several processes from accessing some shared resource at any given time.</p>\n<p>Additional work on the project looked at similar issues for other models of distributed computation, including the population protocol model, a representation of loosely-coordinated computation by very limited computational agents. (Examples include computations by chemical reactions, in which the agents are individual molecules in a solution.) Here the concern was showing that computations could be carried out without requiring too much memory on any individual agent. The project produced significant new results on computing the majority function with limited space in this model.</p>\n<p>Broader impacts of the project included contributions to educational development, via participation of students in the work of the project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/09/2021<br>\n\t\t\t\t\tModified by: James&nbsp;Aspnes</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe primary goal of the project was to study space complexity in distributed algorithms. Space complexity measures the amount of memory needed to carry out some task. Distributed algorithms are methods for coordinating multiple computers across some sort of network or other communication medium.\n\nPrevious work on space complexity looked at the worst-case cost, the maximum amount of memory needed to carry out a task in the worst possible execution. The justification for this approach has been the assumption that memory cannot be used without being pre-allocated. This assumption is violated by modern computer systems that allow allocation of memory as needed, or when it is tolerable for an algorithm to fail in the rare cases where extra memory turns out to be needed. The cost of requiring pre-allocation is that many problems in distributed algorithms are known to have very high space costs in the worst case.\n\nThe project considered an alternative approach where an algorithm is charged only for the memory it uses in a particular execution. We showed that this allows for much more space-efficient algorithms for fundamental coordination problems like mutual exclusion, where we wish to prevent more than one of several processes from accessing some shared resource at any given time.\n\nAdditional work on the project looked at similar issues for other models of distributed computation, including the population protocol model, a representation of loosely-coordinated computation by very limited computational agents. (Examples include computations by chemical reactions, in which the agents are individual molecules in a solution.) Here the concern was showing that computations could be carried out without requiring too much memory on any individual agent. The project produced significant new results on computing the majority function with limited space in this model.\n\nBroader impacts of the project included contributions to educational development, via participation of students in the work of the project.\n\n\t\t\t\t\tLast Modified: 01/09/2021\n\n\t\t\t\t\tSubmitted by: James Aspnes"
 }
}