{
 "awd_id": "1619818",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Closing the Duality Gap:  Decomposition of High-Dimensional Nonconvex Optimization",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Leland Jameson",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2016-08-23",
 "awd_max_amd_letter_date": "2017-07-12",
 "awd_abstract_narration": "In the modern computerized age, nonconvex optimization remains a critical computational challenge. Nonconvexity of an optimization problem implies a combinatorial structure, which often makes the computation problem fundamentally hard. Efficient computation tools with global approximation guarantees are in high demand. The principal investigator will study a class of nonconvex optimization problems that naturally arise from distributed intelligence systems, sparse estimation and data analysis.  The proposed research will contribute new computation tools for data analytics, statistic and machine learning, distributed and parallel computing, and multi-agent intelligence systems. The project will also develop two new courses for both undergraduate and graduate students at Princeton and also will involve undergraduate students in the research project via the Princeton undergraduate summer research program.\r\n\r\nThis research project aims to tackle an important class of nonconvex problems utilizing their geometric structure via a systematic dualization approach. The result is expected to advance the non-convex optimization theory as well as to provide algorithmic solutions to a large variety of distributed systems. Specifically, the principal investigator plans to study the non-convex duality for a class of non-convex optimization problems that admit a near-separable structure, with extensions to minimax problems and variational inequalities, and to develop computation tools that produce approximate global optimal solutions with complexity guarantees. In addition to the fundamental aspects, the principal investigator aims to investigate practical algorithms tailored to specific problems in high-dimensional structural estimation, sparse learning, and distributed optimization. The theoretical results and new methodology are expected to advance the theory of non-convex optimization as well as to provide algorithmic solutions to a variety of computational challenges.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mengdi",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mengdi Wang",
   "pi_email_addr": "mengdiw@princeton.edu",
   "nsf_id": "000712266",
   "pi_start_date": "2016-08-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "226 Sherrerd Hall",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 97006.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 102994.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; min-height: 14.0px} span.s1 {font: 12.0px 'Apple Symbols'} -->\n<p class=\"p1\">Project Outcome Report</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">1 Problem Overview</p>\n<p class=\"p1\">In the big-data era, non-convex optimization poses a tremendous challenge. The work [1] developed the analytic framework and algorithmic solution for a class of&nbsp;multi-agent&nbsp;non-convex problems that naturally arise from sparse learning, distributed intelligence systems, and equilibrium problems. The objective of multi-agent nonconvex optimization problem is a sum of individual costs plus a social cost to which every individual contributes. Each&nbsp;pi,gi,Xi&nbsp;can be nonconvex. Due to the lack of convexity, it remained an open question whether the problem is tractable, let alone how to design efficient algorithms to achieve global coordination. In fact, it is shown to be strongly NP-hard in [1].</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">[1] Vanishing Price of Decentralization in Large Coordinative Nonconvex Optimization,&nbsp;M. Wang, SIAM Journal on Optimization, 2017.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">One application is cooperative optimization in multi-agent intelligence systems, e.g., aircraft or vehicle coordination, robot navigation, smart grid control, communication and sensor networks. Such a distributed system involves a large number of agents that attempt to collaborate and reach a global consensus or maximize a common objective. Meanwhile, each agent is likely to possess a private task or self interests. Another application is sparse optimization from machine learning, statistics, compressive sensing, biotechnology, and network problems. These applications require optimization with l0&nbsp;or other non-convex penalties/constraints. Nonconvex separable optimization arises from learning on large sparse graphs, in which&nbsp;xi&nbsp;is the local estimate on the&nbsp;ith node. An intriguing question is how to leverage known properties (e.g., sparsity, small tree width) of a graph to facilitate the optimization modeling and computation.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">2.&nbsp; Main Results</p>\n<p class=\"p1\"><br /> In this project, we studied the non-convex geometry of separable multi-agent optimization, mathematically quantified its duality gap, and tackled the non-convexity using a dual convexification approach. In particular, the paper [1] established a nonconvex duality framework for such optimization problems and characterizes the duality gap using a fact from convex geometry:&nbsp;the sum of a large number of non-convex sets tends to be convex&nbsp;(Shapley-Folkman Lemma). Through exploiting the high-dimensional non-convex geometry, it discovered a curious phenomenon -</p>\n<p class=\"p1\">The price of decentralization of autonomous multi-agent system is a form of non-convex duality gap, and the duality gap vanishes to zero as the underlying system scales up.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">The analysis of nonconvex duality leads to a semi-decentralized algorithm that is based on dual decomposition and a coordination mechanism. The algorithm is able to produce an approximate global optimum at a linear rate of convergence, where the price of decentralization (optimization error) vanishes to zero at the rate of&nbsp;O(1/N). The key is the coordination mechanism. Without a mechanism to enable and encourage coordination, individual best responses can lead to arbitrarily bad social welfare. These results give an encouraging message: Coordination of individual best responses leads to an approximate&nbsp;global&nbsp;optimum without convexity.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">The theoretical framework and methodology established by this project has fostered novel applications in big data analysis and machine learning. Two examples are given below.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">[2] Blessing of Massive Scale: Spatial Graphical Model Inference with a Total Cardinality Constraint, E. Fang, H. Liu,&nbsp;M. Wang, Mathematical Programming, 2018.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">Our work [2] proposed a novel application of our nonconvex duality approach to sparse<span>&nbsp; </span>statistical learning with applications to brain network reconstruction. In estimation of high-dimension models, sparsity on model parameters is usually imposed by adding an&nbsp;l1&nbsp;penalty, which is a convex relaxation of the&nbsp;l0&nbsp;penalty. We leverage the separable structure of graphs to find a sufficiently accurate approximate solution for an&nbsp;l0&nbsp;penalized estimation problem. The main contribution of [2] is to propose a new estimator based on optimization with&nbsp;l0&nbsp;sparsity constraint. The&nbsp;l0&nbsp;constraint exactly enforces the modeling assumption that &ldquo;the average degree of the graph is bounded by a known number.&rdquo; The use of&nbsp;l0&nbsp;norm&nbsp;avoids the statistical bias induced by using&nbsp;lq&nbsp;relaxation, where&nbsp;q&nbsp;<span class=\"s1\">&isin;</span>&nbsp;(0,&nbsp;1]. In [2] we theoretically prove that the duality-inspired estimator is not only tractable but also statistically optimal for large graphs.</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">3.&nbsp; Broader Impacts<span>&nbsp;</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">Our theoretical findings fill in a gap of nonconvex optimization theory. They also imply a necessary mechanism to coordinate multiple agents in order to maximize their social welfare when they have nonconvex selfish preferences. These results have implications in nonconvex statistical learning, decentralized computation and multi-agent mechanism design.</p>\n<p>&nbsp;</p>\n</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/20/2019<br>\n\t\t\t\t\tModified by: Mengdi&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\nProject Outcome Report\n \n1 Problem Overview\nIn the big-data era, non-convex optimization poses a tremendous challenge. The work [1] developed the analytic framework and algorithmic solution for a class of multi-agent non-convex problems that naturally arise from sparse learning, distributed intelligence systems, and equilibrium problems. The objective of multi-agent nonconvex optimization problem is a sum of individual costs plus a social cost to which every individual contributes. Each pi,gi,Xi can be nonconvex. Due to the lack of convexity, it remained an open question whether the problem is tractable, let alone how to design efficient algorithms to achieve global coordination. In fact, it is shown to be strongly NP-hard in [1].\n \n[1] Vanishing Price of Decentralization in Large Coordinative Nonconvex Optimization, M. Wang, SIAM Journal on Optimization, 2017.\n \nOne application is cooperative optimization in multi-agent intelligence systems, e.g., aircraft or vehicle coordination, robot navigation, smart grid control, communication and sensor networks. Such a distributed system involves a large number of agents that attempt to collaborate and reach a global consensus or maximize a common objective. Meanwhile, each agent is likely to possess a private task or self interests. Another application is sparse optimization from machine learning, statistics, compressive sensing, biotechnology, and network problems. These applications require optimization with l0 or other non-convex penalties/constraints. Nonconvex separable optimization arises from learning on large sparse graphs, in which xi is the local estimate on the ith node. An intriguing question is how to leverage known properties (e.g., sparsity, small tree width) of a graph to facilitate the optimization modeling and computation.\n \n2.  Main Results\n\n In this project, we studied the non-convex geometry of separable multi-agent optimization, mathematically quantified its duality gap, and tackled the non-convexity using a dual convexification approach. In particular, the paper [1] established a nonconvex duality framework for such optimization problems and characterizes the duality gap using a fact from convex geometry: the sum of a large number of non-convex sets tends to be convex (Shapley-Folkman Lemma). Through exploiting the high-dimensional non-convex geometry, it discovered a curious phenomenon -\nThe price of decentralization of autonomous multi-agent system is a form of non-convex duality gap, and the duality gap vanishes to zero as the underlying system scales up.\n \nThe analysis of nonconvex duality leads to a semi-decentralized algorithm that is based on dual decomposition and a coordination mechanism. The algorithm is able to produce an approximate global optimum at a linear rate of convergence, where the price of decentralization (optimization error) vanishes to zero at the rate of O(1/N). The key is the coordination mechanism. Without a mechanism to enable and encourage coordination, individual best responses can lead to arbitrarily bad social welfare. These results give an encouraging message: Coordination of individual best responses leads to an approximate global optimum without convexity.\n \nThe theoretical framework and methodology established by this project has fostered novel applications in big data analysis and machine learning. Two examples are given below.\n \n[2] Blessing of Massive Scale: Spatial Graphical Model Inference with a Total Cardinality Constraint, E. Fang, H. Liu, M. Wang, Mathematical Programming, 2018.\n \nOur work [2] proposed a novel application of our nonconvex duality approach to sparse  statistical learning with applications to brain network reconstruction. In estimation of high-dimension models, sparsity on model parameters is usually imposed by adding an l1 penalty, which is a convex relaxation of the l0 penalty. We leverage the separable structure of graphs to find a sufficiently accurate approximate solution for an l0 penalized estimation problem. The main contribution of [2] is to propose a new estimator based on optimization with l0 sparsity constraint. The l0 constraint exactly enforces the modeling assumption that \"the average degree of the graph is bounded by a known number.\" The use of l0 norm avoids the statistical bias induced by using lq relaxation, where q &isin; (0, 1]. In [2] we theoretically prove that the duality-inspired estimator is not only tractable but also statistically optimal for large graphs.\n \n3.  Broader Impacts \n \nOur theoretical findings fill in a gap of nonconvex optimization theory. They also imply a necessary mechanism to coordinate multiple agents in order to maximize their social welfare when they have nonconvex selfish preferences. These results have implications in nonconvex statistical learning, decentralized computation and multi-agent mechanism design.\n\n \n\n\n\n\n\t\t\t\t\tLast Modified: 11/20/2019\n\n\t\t\t\t\tSubmitted by: Mengdi Wang"
 }
}