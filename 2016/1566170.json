{
 "awd_id": "1566170",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CRII: CHS: Towards Understanding the Capability of Spatial Audio Feedback in Virtual Environments for People with Visual Impairments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2016-05-15",
 "awd_exp_date": "2019-04-30",
 "tot_intn_awd_amt": 169497.0,
 "awd_amount": 169497.0,
 "awd_min_amd_letter_date": "2016-05-27",
 "awd_max_amd_letter_date": "2017-05-30",
 "awd_abstract_narration": "Virtual Reality (VR) applications have been developed for physical rehabilitation, education, healthcare and other domains; the beneficiaries of VR technology include children, the elderly, and persons with physical and mobility impairments.  Platforms for the safe training of personnel in diverse occupations such as aircraft pilots or operators of conveyer belts in mines are an area where the contributions of the technology are especially important.  Unfortunately, however, the majority of VR research to date has been based on visual feedback, thereby excluding persons with visual impairments from enjoying the potential benefits of the technology.  The PI's goal in this project is to establish a research program that will ultimately overcome this shortcoming by developing techniques that allow persons with visual impairments to use 3D audio for perception and movement in VR, and by providing designers with inclusive guidelines for future systems.  Project outcomes will not only benefit users with visual impairments; rather, they will also be useful to people without visual impairments for whom the VR user experience will be enhanced.  This project will afford undergraduate student research opportunities at the PI's institution, and will provide trainees at Atlanta's Center for the Visually Impaired which is located close to the PI's campus with new safe-practice opportunities.\r\n\r\nThe research will be carried out in three phases.  In Phase 1 the PI will investigate to what extent persons who are visually impaired can accurately estimate the depth of, and direction of, sound sources.  In Phase 2 he will explore the design of interfaces and compare alternative audio-based techniques for navigation in virtual environments, with a particular focus on determining key factors that affect user self-confidence for members of the target population.  Finally, in Phase 3 the PI will develop and evaluate a virtual training environment for persons with visual impairments.  Each phase of the work will be conducted in collaboration with the Center for the Visually Impaired, both with respect to study design and data analysis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rongkai",
   "pi_last_name": "Guo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rongkai Guo",
   "pi_email_addr": "rguo@kennesaw.edu",
   "nsf_id": "000701946",
   "pi_start_date": "2016-05-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Kennesaw State University Research and Service Foundation",
  "inst_street_address": "1000 CHASTAIN RD NW",
  "inst_street_address_2": "",
  "inst_city_name": "KENNESAW",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4705786381",
  "inst_zip_code": "301445588",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "GA11",
  "org_lgl_bus_name": "KENNESAW STATE UNIVERSITY RESEARCH AND SERVICE FOUNDATION, INC",
  "org_prnt_uei_num": "",
  "org_uei_num": "G8DZHNRKWTN3"
 },
 "perf_inst": {
  "perf_inst_name": "Kennesaw State University",
  "perf_str_addr": "1000 Chastain Road, MD 0111",
  "perf_city_name": "Kennesaw",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "301445591",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "GA11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "026Y00",
   "pgm_ele_name": "CRII CISE Research Initiation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 92208.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 77289.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In the past year, we finished data analysis of exploring if/how we could recreate a usable indoor virtual environment from people&rsquo;s behavior data, such as traffic pattern. We used Beacons to collect the traffic data in the building, then applied some filters to do the post-processing. The results showed that in a larger scale indoor place, such as in the hallway of a building, with an aggregate collection of user position data, it is possible to create estimates of the test area&rsquo;s dimensions and borders, and were further able to convert that approximation into a walkable 2D map. However, in a smaller scale indoor space, such as an office space, the estimation showed significantly more errors. The problems might come from the errors from the Beacons since they are Bluetooth 4.0. The new Bluetooth 5.0 will have more accurate localization data. The purpose of this work is trying to explore a reasonable way to help create more useful information for people with visual impairments and potentially allow them to input data for the community. The work has been published in IEEE VS-Games 2018.</p>\n<p>Murphy, Nick, Devan Patel, Drew Savas, Derek Martin, Chao Mei, and Rongkai Guo. \"Recreating Virtual Environments from User Traffic Pattern.\" In <em>2018 10th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)</em>, pp. 1-4. IEEE, 2018.</p>\n<p>At the same time, we designed a month-long usability study to compare mobility trainings for people with visual impairments using a traditional method and a Mixed/Virtual Reality technology. To design this study, we did pilot a 2-day experiment to evaluate and refine our design. The feedback of the piloted study included the participants thought using MR/VR could potentially help the training. There was no significant difference found. However, from the piloting procedure, we were able to notice some minor problems with the UI design. We redesigned the UI for the month-long study. To get more feedback, we published the piloted study at one of the 2019 IEEE VR Workshop.</p>\n<p>Kim, Karis, Devan Patel, Nick Murphy, and Rongkai Guo. \"Exploring Virtual and Mixed Reality Environments as Mobility Assistance for People with Visual Impairments.\" 2019 <em>The First IEEE VR Workshop on Smart Work Technologies</em><em>.</em></p>\n<p>We finished the month-long study in April 2019 and submitted the work to ACM VRST 2019. The study showed some very interesting results. The goal of this work is investigating if we could use low-cost VR/MR technology to help people with visual impairments do or practice their mobility training, which is very time and cost consuming. We have our consultant from Center for the Visually Impair at Atlanta to help us pick and design the mobility training for the usability test. Every participant will finish five-session, one session per week, which lasts about a month. The traditional mobility training length is about 6-8 weeks. Since our indoor course is simpler than the normal mobility training and with the opinion of our consultant, we decided five weeks should be a good choice. The first four sessions are training sessions, and the fifth session is a testing session. We found more differences during the first four training sessions, such as MR training have longer total completion time, faster walking speeds and a higher percentage of time facing the correct direction. However, in session 5, there were no significant differences at all for all those metrics. Hopefully, the new technology could benefit more visually impaired people in their mobility training in the future.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/14/2019<br>\n\t\t\t\t\tModified by: Rongkai&nbsp;Guo</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn the past year, we finished data analysis of exploring if/how we could recreate a usable indoor virtual environment from people?s behavior data, such as traffic pattern. We used Beacons to collect the traffic data in the building, then applied some filters to do the post-processing. The results showed that in a larger scale indoor place, such as in the hallway of a building, with an aggregate collection of user position data, it is possible to create estimates of the test area?s dimensions and borders, and were further able to convert that approximation into a walkable 2D map. However, in a smaller scale indoor space, such as an office space, the estimation showed significantly more errors. The problems might come from the errors from the Beacons since they are Bluetooth 4.0. The new Bluetooth 5.0 will have more accurate localization data. The purpose of this work is trying to explore a reasonable way to help create more useful information for people with visual impairments and potentially allow them to input data for the community. The work has been published in IEEE VS-Games 2018.\n\nMurphy, Nick, Devan Patel, Drew Savas, Derek Martin, Chao Mei, and Rongkai Guo. \"Recreating Virtual Environments from User Traffic Pattern.\" In 2018 10th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games), pp. 1-4. IEEE, 2018.\n\nAt the same time, we designed a month-long usability study to compare mobility trainings for people with visual impairments using a traditional method and a Mixed/Virtual Reality technology. To design this study, we did pilot a 2-day experiment to evaluate and refine our design. The feedback of the piloted study included the participants thought using MR/VR could potentially help the training. There was no significant difference found. However, from the piloting procedure, we were able to notice some minor problems with the UI design. We redesigned the UI for the month-long study. To get more feedback, we published the piloted study at one of the 2019 IEEE VR Workshop.\n\nKim, Karis, Devan Patel, Nick Murphy, and Rongkai Guo. \"Exploring Virtual and Mixed Reality Environments as Mobility Assistance for People with Visual Impairments.\" 2019 The First IEEE VR Workshop on Smart Work Technologies.\n\nWe finished the month-long study in April 2019 and submitted the work to ACM VRST 2019. The study showed some very interesting results. The goal of this work is investigating if we could use low-cost VR/MR technology to help people with visual impairments do or practice their mobility training, which is very time and cost consuming. We have our consultant from Center for the Visually Impair at Atlanta to help us pick and design the mobility training for the usability test. Every participant will finish five-session, one session per week, which lasts about a month. The traditional mobility training length is about 6-8 weeks. Since our indoor course is simpler than the normal mobility training and with the opinion of our consultant, we decided five weeks should be a good choice. The first four sessions are training sessions, and the fifth session is a testing session. We found more differences during the first four training sessions, such as MR training have longer total completion time, faster walking speeds and a higher percentage of time facing the correct direction. However, in session 5, there were no significant differences at all for all those metrics. Hopefully, the new technology could benefit more visually impaired people in their mobility training in the future.\n\n\t\t\t\t\tLast Modified: 05/14/2019\n\n\t\t\t\t\tSubmitted by: Rongkai Guo"
 }
}