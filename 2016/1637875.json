{
 "awd_id": "1637875",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: Autonomous Quadrotors for 3D Modeling and Inspection of Outdoor Infrastructure",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 830280.0,
 "awd_amount": 830280.0,
 "awd_min_amd_letter_date": "2016-08-18",
 "awd_max_amd_letter_date": "2021-08-03",
 "awd_abstract_narration": "This project develops technologies to collect visual and inertial data necessary for constructing, offline, high-accuracy 3D maps of the structure for civil and industrial infrastructure such as bridges, power plants, and refineries.  It also develops technologies for online processing including localization, path planning and obstacle avoidance. The project builds a system that employs quadrotors to assist their human co-workers in visual inspections of the outdoor infrastructure to enhance efficiency and effectiveness of such operations.  The research advances the current state of the art in key areas of sensing, estimation, and control necessary for enabling small-size quadrotors to assist humans in visual inspections. In addition to improving the reliability of the nation's infrastructure, the project benefits researchers, developers, educators, and end-users in robotics by developing open-source, modular algorithms for quadrotors. The project offers educational and community outreach activities aligned with local efforts and state-wide initiatives, and seeks to increase diversity and attract underrepresented groups to Science, Technology, Engineering, and Mathematics (STEM) via a partnership with local high schools. \r\n\r\nThis research addresses the fundamental challenges stemming from sensing and processing limitations that prevent the use of low-cost, small-size quadrotors in visual-inspection tasks. It focuses on a four-step process, where initially a quadrotor is tele-operated at a safe distance from the structure of interest to collect visual and inertial data necessary for constructing, offline, high-accuracy 3D maps of the structure. These maps are then used, by the inspection engineer, to designate areas of interest. Lastly, the quadrotor employs its onboard sensors to precisely localize with respect to the structure and navigate along the inspection route, while collecting additional data for increasing the accuracy and improving the reliability of future inspections. A key innovation is making information available in multiple forms and levels of abstraction so as to meet the often-conflicting needs of offline (e.g., visualization of inspection areas and planning information-rich paths) and online (e.g., map-based localization and obstacle avoidance) uses. Also critical is an information-driven approach for making maximum use of the limited sensing and processing resources available to the quadrotor. Lastly, a key advantage of the proposed approach is that it provides the foundation for continual improvement in accuracy and efficiency after each inspection flight.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Stergios",
   "pi_last_name": "Roumeliotis",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Stergios I Roumeliotis",
   "pi_email_addr": "stergios@cs.umn.edu",
   "nsf_id": "000487045",
   "pi_start_date": "2016-08-18",
   "pi_end_date": "2020-03-20"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Junaed",
   "pi_last_name": "Sattar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Junaed Sattar",
   "pi_email_addr": "junaed@umn.edu",
   "nsf_id": "000678398",
   "pi_start_date": "2020-03-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Seiler",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Peter J Seiler",
   "pi_email_addr": "pseiler@umich.edu",
   "nsf_id": "000526307",
   "pi_start_date": "2016-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota-Twin Cities",
  "perf_str_addr": "200 Union Street SE",
  "perf_city_name": "",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554552070",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 830280.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-6c827203-7fff-bcf7-a718-39f48346fb6e\" style=\"line-height: 1.295; text-align: justify; margin-top: 0pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Calibri,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project, titled \"NRI: Collaborative Research: Autonomous Quadrotors for 3D Modeling and Inspection of Outdoor Infrastructure\", developed technologies to collect visual and inertial data necessary for constructing, offline, high-accuracy 3D maps of the structure of civil and industrial infrastructure such as bridges, power plants, and refineries. Moreover, it also introduced algorithms for online processing including localization, path planning and obstacle avoidance.&nbsp; This research advanced the current state of the art in key areas of sensing, estimation, and control necessary for enabling small-size quadrotors to assist humans in visual inspections. In addition to improving the reliability of the nation's infrastructure, the project contributed to researchers, developers, educators, and end-users in robotics by developing and releasing open-source, modular algorithms. During the duration of this project, the PIs and their students have published a number of novel findings. Examples include:&nbsp;</span></p>\n<ol style=\"margin-top: 0; margin-bottom: 0; padding-inline-start: 48px;\">\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: Calibri,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Calibri,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The use of deep machine learning for distance (also referred to as depth) estimation enables robots to localize themselves more accurately by combining inertial measurements with a number of images taken in a sequence.</span> </li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: Calibri,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Calibri,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">By measuring variations in intensity in a sequence of images (aka through ?direct? methods) can assist robots to localize more accurately in environments devoid of unique visual textures or landmarks.&nbsp;</span> </li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: Calibri,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Calibri,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Deep machine learning can also assist in measuring salient features in a visual scene, which in turn can enable robots to focus their attention on those salient features, making autonomous detection of unusual patterns or faults possible.&nbsp;</span> </li>\n</ol>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.295; text-align: justify; margin-top: 0pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Calibri,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Additionally, the project enabled development of novel materials for college-level classes in robotics, control, and computer vision. Research funded by this grant has produced two Ph.D. theses, eight peer-reviewed journal and conference publications, and a suite of open-source, freely available software for robot localization, visual scene improvement, and scene reconstruction. A number of outreach events were also conducted, particularly for K-12 students belonging to underrepresented groups in STEM and industrial entities. As part of a number of such outreach events conducted as part of this grant, the broader public in general was educated about the utility of outdoor robots and their numerous benefits towards the inspection, maintenance, and generally improving the reliability of the nation's infrastructure as a whole.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/28/2022<br>\n\t\t\t\t\tModified by: Junaed&nbsp;Sattar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "This project, titled \"NRI: Collaborative Research: Autonomous Quadrotors for 3D Modeling and Inspection of Outdoor Infrastructure\", developed technologies to collect visual and inertial data necessary for constructing, offline, high-accuracy 3D maps of the structure of civil and industrial infrastructure such as bridges, power plants, and refineries. Moreover, it also introduced algorithms for online processing including localization, path planning and obstacle avoidance.  This research advanced the current state of the art in key areas of sensing, estimation, and control necessary for enabling small-size quadrotors to assist humans in visual inspections. In addition to improving the reliability of the nation's infrastructure, the project contributed to researchers, developers, educators, and end-users in robotics by developing and releasing open-source, modular algorithms. During the duration of this project, the PIs and their students have published a number of novel findings. Examples include: \n\nThe use of deep machine learning for distance (also referred to as depth) estimation enables robots to localize themselves more accurately by combining inertial measurements with a number of images taken in a sequence. \nBy measuring variations in intensity in a sequence of images (aka through ?direct? methods) can assist robots to localize more accurately in environments devoid of unique visual textures or landmarks.  \nDeep machine learning can also assist in measuring salient features in a visual scene, which in turn can enable robots to focus their attention on those salient features, making autonomous detection of unusual patterns or faults possible.  \n\n\n \nAdditionally, the project enabled development of novel materials for college-level classes in robotics, control, and computer vision. Research funded by this grant has produced two Ph.D. theses, eight peer-reviewed journal and conference publications, and a suite of open-source, freely available software for robot localization, visual scene improvement, and scene reconstruction. A number of outreach events were also conducted, particularly for K-12 students belonging to underrepresented groups in STEM and industrial entities. As part of a number of such outreach events conducted as part of this grant, the broader public in general was educated about the utility of outdoor robots and their numerous benefits towards the inspection, maintenance, and generally improving the reliability of the nation's infrastructure as a whole.\n\n\t\t\t\t\tLast Modified: 12/28/2022\n\n\t\t\t\t\tSubmitted by: Junaed Sattar"
 }
}