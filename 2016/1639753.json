{
 "awd_id": "1639753",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Earthcube Building Blocks: Collaborative Proposal: Polar Data Insights and Search Analytics for the Deep and Scientific Web",
 "cfda_num": "47.050",
 "org_code": "06010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Eva Zanzerkia",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 514999.0,
 "awd_amount": 514999.0,
 "awd_min_amd_letter_date": "2016-09-16",
 "awd_max_amd_letter_date": "2016-09-16",
 "awd_abstract_narration": "This project develops an NSF EarthCube Building Block focused on Polar Data Science. The system will build upon work in Information Retrieval and Data Science and upon existing investment from NSF Polar, EarthCube, and from DARPA and NASA in this area. The system will collect, analyze, and make interactive the wealth of textual and scientific Polar data collected to date across the Deep web of scientific information -- scientific journals, multimedia information, scientific data, web pages, etc. The system builds upon fundamental research in text analysis, search, and visualization. Its primary goal is to unlock unstructured scientific data from 90+ data formats and to scale to 10s-100s of millions of records using the NSF XSEDE supercomputing resources. The system will perform information retrieval and machine learning on data crawled from the Polar Deep and Scientific web. Crawling will be informed by science questions crowdsourced through the EarthCube and Polar communities. The project is a collaboration with NSIDC, Ronin Institute, and the broader community including the newly funded Arctic Data Center led by NCEAS, to build our proposed system.\r\n\r\nThe result of periodic and regular crawling will be a Crawl Data Repository (CDR) of raw textual data e.g., web pages containing richly curated dataset abstract descriptions, news stories tied to datasets, ASCII note files and dataset descriptions, and other textual data available on or pointed to by Polar repositories as well as scientific data (HDF, Grib, NetCDF, Matlab, etc.). The CDR will be made available for historical and future analysis by the broader EarthCube and Polar communities. In addition, an extraction pipeline will generate an Extraction Data Repository (EDR) of machine learning features not previously present (geospatial, temporal, people, places, scientific publications and topics, etc.) that will be the basis of interactive, visual analytics over the Polar data resources. Information collected will assist in answering scientific questions such as these derived from the President?s National Strategy for the Arctic Region. To date, the team has also crowd sourced 30+ questions from the Polar community represented on CRYOLIST https://goo.gl/4dDyIS and will continue to solicit this feedback and use the information collected to aid science as prioritized by the community. They will also engage the community to assist in validating our system. This is not a predictive tool per-se ? though it can help to enable such predictions. Its focus is on building an operational and core capability for textual scientific data analysis, both retrospective, and prospective.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "GEO",
 "org_dir_long_name": "Directorate for Geosciences",
 "div_abbr": "RISE",
 "org_div_long_name": "Integrative and Collaborative Education and Research (ICER)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chris",
   "pi_last_name": "Mattmann",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Chris A Mattmann",
   "pi_email_addr": "mattmann@usc.edu",
   "nsf_id": "000516532",
   "pi_start_date": "2016-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S. Flower St.",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900890001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "CA37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "540700",
   "pgm_ele_name": "Polar Cyberinfrastructure"
  },
  {
   "pgm_ele_code": "807400",
   "pgm_ele_name": "EarthCube"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8048",
   "pgm_ref_txt": "Data Infrstr Bldg Blocks-DIBBs"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "02XX",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "0100XXXXDB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 514999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-4e3a4690-7fff-8e8f-3bdd-b8e73afdfeed\">\n<p dir=\"ltr\"><span>The project focuses on building a Polar Deep Insights system that collects, analyzes, and makes interactive the wealth of textual and scientific Polar data mined from the deep and scientific web. Our project combines three existing EarthCube Building Blocks &ndash; Bcube, GeoDeepDive, and OntoSoft/GPF &ndash; and builds on NSF Polar CyberInfrastructure prior work and community workshops in 2013 and in 2014 along with investments from the DARPA MEMEX effort. Our system will crawl the Deep Polar Web, and will extract knowledge from text using information retrieval and data science (IRDS) techniques that bring together unstructured and structured science data to provide transformative insights. The web data we collect will be actively reviewed by the EarthCube Polar geoscientists at ESIP, AGU, via NSIDC and as part of the Polar Research Coordination Network (RCN).&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>Our proposed project will develop new information retrieval techniques and analytics that automatically combine textual and scientific data. These techniques are not specific to Polar and can be applied in other contexts relevant to EarthCube (Diasters; Air Quality; Geology, etc.) and also to other sponsors such as DARPA and NASA. In addition we will contribute deep crawling and extraction techniques and a communal crawl data and extraction data repository. Software from our work will be pushed upstream into the Apache Software Foundation, whose work is widely used and easily integrated into other NSF EarthCube performers, commercial, academic and government partners. We leverage our NSF XSEDE Wrangler allocation for this work.</span></p>\n<div><span><br /></span></div>\n</span></p>\n<p><span id=\"docs-internal-guid-bdba48f0-7fff-d180-23d4-6099ded2d84f\">\n<p dir=\"ltr\"><span>We built an automated domain discovery system using </span><span>Sparkler</span><span>, an open-source, extensible, horizontally scalable crawler which facilitates high throughput and focused crawling of documents only pertinent to the polar domain. With this set of highly domain relevant documents, we show that it is possible to answer analytical questions about the polar domain, accurately. The scientific web is vast and ever growing. It encompasses millions of textual, scientific and multimedia documents describing research in a multitude of scientific streams. Most of these documents are hidden behind forms which require user action to retrieve and thus can&rsquo;t be directly accessed by content crawlers. These documents are hosted on web servers across the world most often on outdated hardware and network infrastructure. Hence it is difficult and time-consuming to aggregate documents from the scientific web, more so those relevant to a specific domain. This makes it hard to generate any meaningful domain-specific insights.&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>We use machine learning for focusing our crawler to only choose highly relevant documents during crawl time. Our machine learning algorithm uses annotations of documents by Subject Matter Experts (SMEs) on a scale from highly relevant to irrelevant to learn the relevance of documents &ndash; besides our co-PIs Duerr and Khalsa, we also included SME expertise from Jay Pearlman and Ketil Koop-Jakobsen. We extend Sparkler with this machine learning model to focus crawling to polar specific documents based on the input received.</span></p>\n<br />\n<p dir=\"ltr\"><span>In our presentation at All Hands Earth Cube in Seattle 2017, through our poster we explained machine learning driven domain discovery and the advantages of focused crawling toward generating more accurate insights.</span></p>\n<br />\n<p dir=\"ltr\"><span>We built four new machine learning models for focused crawling using Sparkler, using Na&iuml;ve Bayes, Support Vector Machines, Neural Networks, and Random Forest classifiers. We also built a Cosine Similarity model to evaluate URL similarity and crawl result relevancy. These algorithms were chosen both for their ease-of-use - we can easily train models using Python&rsquo;s Scikit Learn module &ndash; and then import them into Sparkler using a focused crawling web service &ndash; but also for their applicability to the web data. Random Forest considers the web graph as a dense forest of URLs, and then explores it heuristically; SVM and Na&iuml;ve Bayes also treat the graph input similarly. For Deep Learning, we constructed a simple network that tried to learn relevant URL choice features based on the SME input.</span></p>\n<div><span><br /></span></div>\n</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/22/2019<br>\n\t\t\t\t\tModified by: Chris&nbsp;A&nbsp;Mattmann</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThe project focuses on building a Polar Deep Insights system that collects, analyzes, and makes interactive the wealth of textual and scientific Polar data mined from the deep and scientific web. Our project combines three existing EarthCube Building Blocks &ndash; Bcube, GeoDeepDive, and OntoSoft/GPF &ndash; and builds on NSF Polar CyberInfrastructure prior work and community workshops in 2013 and in 2014 along with investments from the DARPA MEMEX effort. Our system will crawl the Deep Polar Web, and will extract knowledge from text using information retrieval and data science (IRDS) techniques that bring together unstructured and structured science data to provide transformative insights. The web data we collect will be actively reviewed by the EarthCube Polar geoscientists at ESIP, AGU, via NSIDC and as part of the Polar Research Coordination Network (RCN). \n\n\nOur proposed project will develop new information retrieval techniques and analytics that automatically combine textual and scientific data. These techniques are not specific to Polar and can be applied in other contexts relevant to EarthCube (Diasters; Air Quality; Geology, etc.) and also to other sponsors such as DARPA and NASA. In addition we will contribute deep crawling and extraction techniques and a communal crawl data and extraction data repository. Software from our work will be pushed upstream into the Apache Software Foundation, whose work is widely used and easily integrated into other NSF EarthCube performers, commercial, academic and government partners. We leverage our NSF XSEDE Wrangler allocation for this work.\n\n\n\n\n\nWe built an automated domain discovery system using Sparkler, an open-source, extensible, horizontally scalable crawler which facilitates high throughput and focused crawling of documents only pertinent to the polar domain. With this set of highly domain relevant documents, we show that it is possible to answer analytical questions about the polar domain, accurately. The scientific web is vast and ever growing. It encompasses millions of textual, scientific and multimedia documents describing research in a multitude of scientific streams. Most of these documents are hidden behind forms which require user action to retrieve and thus can\u2019t be directly accessed by content crawlers. These documents are hosted on web servers across the world most often on outdated hardware and network infrastructure. Hence it is difficult and time-consuming to aggregate documents from the scientific web, more so those relevant to a specific domain. This makes it hard to generate any meaningful domain-specific insights. \n\n\nWe use machine learning for focusing our crawler to only choose highly relevant documents during crawl time. Our machine learning algorithm uses annotations of documents by Subject Matter Experts (SMEs) on a scale from highly relevant to irrelevant to learn the relevance of documents &ndash; besides our co-PIs Duerr and Khalsa, we also included SME expertise from Jay Pearlman and Ketil Koop-Jakobsen. We extend Sparkler with this machine learning model to focus crawling to polar specific documents based on the input received.\n\n\nIn our presentation at All Hands Earth Cube in Seattle 2017, through our poster we explained machine learning driven domain discovery and the advantages of focused crawling toward generating more accurate insights.\n\n\nWe built four new machine learning models for focused crawling using Sparkler, using Na&iuml;ve Bayes, Support Vector Machines, Neural Networks, and Random Forest classifiers. We also built a Cosine Similarity model to evaluate URL similarity and crawl result relevancy. These algorithms were chosen both for their ease-of-use - we can easily train models using Python\u2019s Scikit Learn module &ndash; and then import them into Sparkler using a focused crawling web service &ndash; but also for their applicability to the web data. Random Forest considers the web graph as a dense forest of URLs, and then explores it heuristically; SVM and Na&iuml;ve Bayes also treat the graph input similarly. For Deep Learning, we constructed a simple network that tried to learn relevant URL choice features based on the SME input.\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 11/22/2019\n\n\t\t\t\t\tSubmitted by: Chris A Mattmann"
 }
}