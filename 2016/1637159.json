{
 "awd_id": "1637159",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RIDIR: Collaborative Research: Computational and Historical Resources on Nations and Organizations for the Social Sciences (CHRONOS)",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Patricia Van Zandt",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 480488.0,
 "awd_amount": 480488.0,
 "awd_min_amd_letter_date": "2016-08-11",
 "awd_max_amd_letter_date": "2017-09-11",
 "awd_abstract_narration": "This project will collect, process, and analyze millions of U.S. government records concerning international relations, develop tools to explore these records, and make all of them available on a single website with an Application Programming Interface. The project will demonstrate how computational techniques can aid both qualitative and quantitative social science research on a range of areas of major public interest, expanding knowledge about terrorism, intelligence, international trade and aid. Among its broader impacts, it will improve the infrastructure available for multidisciplinary research and teaching, and also give citizens, journalists, and civil society organizations much better access to information about international relations. Participating student researchers will both learn about -- and contribute to -- practical methods to keep government transparent and accountable in the age of \"big data.\"\r\n\r\nThe exponential growth in digitized or \"born digital\" documents will make such methods increasingly important in years to come. To meet the challenge, the team will draw on new work in Natural Language Processing, customize existing tools that turn text into data, and develop new tools for use with historical documents. They will employ Named Entity Recognition techniques to extract names of people, countries, and organizations, enabling users to track the absolute and relative frequency of mentions in large digital archives. Through Topic Modeling, they will summarize the thematic content and show how the most important subjects change over time. And Social Network extraction will allow them to reveal the informal relationships that shape policy from day-to-day. By bringing together all of this quantitative declassified data in a single platform, the project will advance work by political scientists as well as scholars of communications and social networks who seek to understand agenda-setting, power, and influence within and between organizations. The platform will enable researchers to test fundamental questions in IR theory, such as whether policymakers generally speak to one another in terms of \"state security,\" as realists insist, or \"international norms,\" as liberalism assumes. It will allow users to shift between different levels of analysis, from an aggregate view of whole archives, to filtered subsets of metadata, to the specific words in one sentence that produced a single data point. It will bring together quantitative and qualitative approaches to research in international relations, and make both more transparent, rigorous, and replicable.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Jervis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Robert Jervis",
   "pi_email_addr": "rlj1@columbia.edu",
   "nsf_id": "000149066",
   "pi_start_date": "2016-08-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Owen",
   "pi_last_name": "Rambow",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Owen C Rambow",
   "pi_email_addr": "owen.rambow@stonybrook.edu",
   "nsf_id": "000237191",
   "pi_start_date": "2016-08-11",
   "pi_end_date": "2017-09-11"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Connelly",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew J Connelly",
   "pi_email_addr": "mjc96@columbia.edu",
   "nsf_id": "000571120",
   "pi_start_date": "2016-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277922",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "829400",
   "pgm_ele_name": "Data Infrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 480488.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-089b0dd1-7fff-db9e-2a40-28904e131a8e\">\n<p dir=\"ltr\"><span>Original proposal</span></p>\n<p dir=\"ltr\"><span>&nbsp;&nbsp;</span><span>The goals of the project are twofold. First, we aim to aggregate large textual corpora and extract metadata to create a dataset related to U.S foreign policy. Second, we will make our database and codebase available to other researchers to maximize its utility as a resource for data science and social science.</span></p>\n<p dir=\"ltr\"><span>The computer science fields of Natural Language Processing (NLP) and Machine Learning are producing a whole array of new techniques to process millions of documents, extract quantitative data, and reveal patterns in communications streams, social networks, and agenda-setting. These techniques may prove essential for preserving meaningful access to government records, such as by automating and accelerating the review of documents for declassification. But they can also make it possible to host them on a free and open platform, one that can support different approaches to IR research and may actually bring them together. Because the data will be extracted from documents &ndash; and be linked to those documents &ndash; researchers will be able to shift seamlessly between different levels of analysis, from an aggregate view of whole archives, to the specific words in one sentence that produced a single data point.</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span></p>\n<br /><br />\n<p dir=\"ltr\"><span>Outcomes</span></p>\n<br />\n<p dir=\"ltr\"><span><span> </span></span><span>As a result of the NSF grant, we were able to substantially increase the number of documents in our existing collections and to add several new collections. The number of documents in the collections we held at the beginning of the NSF grant increased by about 764,000 documents. Additionally, we were able to process and ingest 5 new collections, totalling more than 1 million documents. These new collections expand our understanding of International Relations by adding documents from other countries (the British Cabinet papers and the Azeredo da Silveira Papers from Brazil) and an international organization (World Bank&rsquo;s Robert McNamara papers) in addition to the General CIA Records and the President&rsquo;s Daily Briefs.</span></p>\n<p dir=\"ltr\"><span><span> </span></span><span>We also successfully added metadata to all of the different collections. We ran Latent Dirichlet Allocation on all the collections in order to derive the underlying topics in different documents. This will allow researchers to quickly get a sense of the contents of a document as well as to easily find other similar documents.</span></p>\n<p dir=\"ltr\"><span><span> </span></span><span>Additionally, we complete Named Entity Recognition and entity linking on all the collections--except for the Azeredo da Silveira Papers because of language differences. Individuals, locations, and organizations in the collections were extracted and then linked to specific entities using a WikidataID. This allows researchers to search for a standardized entity name and to return all variants of that name within our database. That is, rather than having to look up all variants of &ldquo;United States&rdquo; (USA, U.S., United States) we have completed the work for them.</span></p>\n<p dir=\"ltr\"><span><span> </span></span><span>The data will be available through an API or as a data download by request. The API will allow users to directly interface with the data and submit specific requests. There are also interfaces with the API available for both the Stata and R statistical packages. The full-text of documents is available via the API but there is a hard limit of 10,000 results to ensure that there are no slow-downs in performance. Users who would like a larger number of records could directly request an SQL dump of the underlying data.&nbsp;</span></p>\n<p dir=\"ltr\"><span><span> </span></span><span>Our work should have an impact on several disciplines including political science, history, and data science. For Political Science, we provide a wealth of documentary evidence in a single place and allow users to search for people, places, or topics across multiple collections of documents about foreign policy. Historians will also benefit from the ease of access to the documents. Finally, the tools we develop to derive metadata will be useful to data scientists interested in natural language processing on government documents.&nbsp;</span></p>\n</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/15/2020<br>\n\t\t\t\t\tModified by: Robert&nbsp;Jervis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nOriginal proposal\n  The goals of the project are twofold. First, we aim to aggregate large textual corpora and extract metadata to create a dataset related to U.S foreign policy. Second, we will make our database and codebase available to other researchers to maximize its utility as a resource for data science and social science.\nThe computer science fields of Natural Language Processing (NLP) and Machine Learning are producing a whole array of new techniques to process millions of documents, extract quantitative data, and reveal patterns in communications streams, social networks, and agenda-setting. These techniques may prove essential for preserving meaningful access to government records, such as by automating and accelerating the review of documents for declassification. But they can also make it possible to host them on a free and open platform, one that can support different approaches to IR research and may actually bring them together. Because the data will be extracted from documents &ndash; and be linked to those documents &ndash; researchers will be able to shift seamlessly between different levels of analysis, from an aggregate view of whole archives, to the specific words in one sentence that produced a single data point.\n \n\n\n\nOutcomes\n\n\n As a result of the NSF grant, we were able to substantially increase the number of documents in our existing collections and to add several new collections. The number of documents in the collections we held at the beginning of the NSF grant increased by about 764,000 documents. Additionally, we were able to process and ingest 5 new collections, totalling more than 1 million documents. These new collections expand our understanding of International Relations by adding documents from other countries (the British Cabinet papers and the Azeredo da Silveira Papers from Brazil) and an international organization (World Bank\u2019s Robert McNamara papers) in addition to the General CIA Records and the President\u2019s Daily Briefs.\n We also successfully added metadata to all of the different collections. We ran Latent Dirichlet Allocation on all the collections in order to derive the underlying topics in different documents. This will allow researchers to quickly get a sense of the contents of a document as well as to easily find other similar documents.\n Additionally, we complete Named Entity Recognition and entity linking on all the collections--except for the Azeredo da Silveira Papers because of language differences. Individuals, locations, and organizations in the collections were extracted and then linked to specific entities using a WikidataID. This allows researchers to search for a standardized entity name and to return all variants of that name within our database. That is, rather than having to look up all variants of \"United States\" (USA, U.S., United States) we have completed the work for them.\n The data will be available through an API or as a data download by request. The API will allow users to directly interface with the data and submit specific requests. There are also interfaces with the API available for both the Stata and R statistical packages. The full-text of documents is available via the API but there is a hard limit of 10,000 results to ensure that there are no slow-downs in performance. Users who would like a larger number of records could directly request an SQL dump of the underlying data. \n Our work should have an impact on several disciplines including political science, history, and data science. For Political Science, we provide a wealth of documentary evidence in a single place and allow users to search for people, places, or topics across multiple collections of documents about foreign policy. Historians will also benefit from the ease of access to the documents. Finally, the tools we develop to derive metadata will be useful to data scientists interested in natural language processing on government documents. \n\n\n\t\t\t\t\tLast Modified: 12/15/2020\n\n\t\t\t\t\tSubmitted by: Robert Jervis"
 }
}