{
 "awd_id": "1617390",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: SMALL: Collaborative Research: Cloud Mentoring: Guiding Cloud Users for Cost Performance through Testing and Recommendation",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 311883.0,
 "awd_amount": 311883.0,
 "awd_min_amd_letter_date": "2016-07-21",
 "awd_max_amd_letter_date": "2016-07-21",
 "awd_abstract_narration": "Cloud computing is growing rapidly, with businesses, institutions and individuals moving their workloads to clouds. Cloud users benefit from low cost ownership, a pay-as-you-go pricing model where they only pay for the procured resource usage, and the ability to dynamically scale the resource usage up and down. However, the applications running in clouds usually experience unpredictable performance, which makes it extremely challenging for the users to choose resource configurations that meet their cost and performance requirements. This problem is further complicated as users do not have physical control over cloud computers and are forced to make their decisions based on convoluted cloud performance reports.\r\n\r\nThis research addresses the need to support users in achieving their cost-performance requirements as they port their applications to various cloud services. In particular, the research is embodied in an envisioned testing and recommendation system that determines proper resource management policies that meet performance and cost requirements. By taking a software-testing-based approach, the research provides solutions using only user-accessible information to satisfy user requirements, addresses the limits of static analysis techniques that rely on performance predictability.\r\n\r\nSuch a testing and recommendation system enables non-experts to port their applications to various clouds in a cost effective way. The novel framework, testing and recommendation approaches, data sets, and experimental infrastructure developed within the project will be released open source to advance knowledge and understanding within software engineering and cloud computing. The PIs will continue to involve students of underrepresented groups and continue their involvement in mentoring workshops for students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mary Lou",
   "pi_last_name": "Soffa",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mary Lou Soffa",
   "pi_email_addr": "soffa@cs.virginia.edu",
   "nsf_id": "000203853",
   "pi_start_date": "2016-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia",
  "perf_str_addr": "P. O. Box 400195",
  "perf_city_name": "Charlottesville",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229044195",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 311883.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-8d054ae4-7fff-0c5f-0599-752970453a86\"> <span id=\"docs-internal-guid-8d054ae4-7fff-0c5f-0599-752970453a86\"> </span></span></p>\n<p dir=\"ltr\"><span>Cloud computing has been widely adopted by businesses and organizations to deploy and execute their applications. A recent large survey reported that 93% of 930 surveyed technical professionals have adopted Cloud Computing in their organizations. To fully realize the cost and benefits of cloud services, users usually need to reliably know the execution performance of their applications in the cloud. However, due to the random performance fluctuations experienced by cloud applications, the hidden nature of public cloud operations and the cloud usage costs, testing on clouds to acquire accurate performance results is extremely difficult. In this research, we address this challenge through several projects, as described below.&nbsp; We involved graduate and undergraduate students in our research, providing them with the training they need to work in cloud computing in the future.&nbsp;&nbsp;</span></p>\n<p>&nbsp;</p>\n<ol>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>In our first project, we specify cloud uncertainty criteria, and design a test-based strategy that characterizes the black box&rsquo;s cloud performance resource usage and the cloud baseline performance of the application to be deployed. We first developed testing coverage criteria for cloud performance uncertainty.&nbsp; We then developed a novel performance testing approach that employs cloud/application characterizations and a smart oracle to determine whether an application can meet its performance requirements on public clouds with high accuracy and low testing cost. Experimental results indicate that this testing strategy is a cost-effective approach to test for performance effects of cloud uncertainty when porting an application to the cloud.</span><span><br /><br /></span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>In another project, we developed a novel cloud performance testing methodology called &ldquo;PT4Cloud,&rdquo; which employs non-parametric statistical approaches to provide reliable stopping conditions to obtain highly accurate performance distributions. PT4Cloud also allows the users to specify intuitive accuracy objectives and trade accuracy for testing cost. To reduce the cost of testing, we developed two test reduction techniques that can significantly reduce the number of test runs while retaining a high level of accuracy.&nbsp; We experimentally evaluated PT4Cloud and found that it provides testing results with 95.4% accuracy on average while reducing the number of test runs by 62%. Our two test execution reduction techniques for PT4Cloud can reduce the number of test runs by 90.1% while retaining an average accuracy of 91%. </span><span><br /><br /></span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>In the third project, we improved the PT4Cloud to test for performance in terms of point estimates, such as the mean performance or performance percentiles. This new performance testing technique is called Metior.&nbsp; Metior includes a new test execution methodology using daily performance samples and a novel stop condition using non-parametric statistics. Experiment results showed that Metior can provide accurate performance results, in terms of means and percentiles, with only 1.4% error on average, and ensured 99.3% of the results to have less-than-3% errors. Compared to PT4Cloud, Metior could reduce the testing cost by 5.8 times for point estimates. We also applied Metior to two recently-published cloud performance prediction techniques. Experiment results showed that, by providing reliable training or input data, Metior could reduce the prediction error by 12.5% and reduce the input data size by 19.2% on average.</span><span><br /><br /></span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>While the previous projects involved static workloads, we introduced a project that works on dynamic workloads and elasticity; that is, we investigated how to adapt the PT4Cloud cloud performance testing methodology to work in the presence of dynamic workloads and elasticity.&nbsp; Dynamic workloads such as web traffic are volatile and workload fluctuations occur for various factors which would require an IaaS elastic service. Elasticity is the ability of a system to adapt to workload changes by provisioning and&nbsp; de-provisioning&nbsp; cloud&nbsp; resources to match changing demands and is challenging for performance testing. Users can utilize auto-scaling policies to achieve elasticity and&nbsp; enhance&nbsp; performance&nbsp; and&nbsp; cost&nbsp; satisfaction. A major challenge when creating effective scaling policies to utilize elasticity&nbsp; is that users are expected to have precise knowledge of the performance of their application.&nbsp; Our experiment results with several applications and auto-scaling configurations showed&nbsp; that the statistics-based performance testing methodology can be applied with reliable stop conditions and achieve highly accurate performance distributions with confidence bands. Our results show testing results with similar accuracy on average to results achieved for applications with static workloads also with significant reduction in test runs in dynamic workloads.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Besides performance testing, this NSF award also supported the PIs&rsquo; other research projects on cloud computing, including the development of several cloud job scheduling algorithms for time-sensitive cloud jobs, the development of a generic cloud workload predictor with automated machine-learning optimizations, and the first benchmarking framework for interactive cloud 3D applications.</span></p>\n</li>\n</ol>\n<p>&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/19/2020<br>\n\t\t\t\t\tModified by: Mary Lou&nbsp;Soffa</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n  \nCloud computing has been widely adopted by businesses and organizations to deploy and execute their applications. A recent large survey reported that 93% of 930 surveyed technical professionals have adopted Cloud Computing in their organizations. To fully realize the cost and benefits of cloud services, users usually need to reliably know the execution performance of their applications in the cloud. However, due to the random performance fluctuations experienced by cloud applications, the hidden nature of public cloud operations and the cloud usage costs, testing on clouds to acquire accurate performance results is extremely difficult. In this research, we address this challenge through several projects, as described below.  We involved graduate and undergraduate students in our research, providing them with the training they need to work in cloud computing in the future.  \n\n \n\n\nIn our first project, we specify cloud uncertainty criteria, and design a test-based strategy that characterizes the black box\u2019s cloud performance resource usage and the cloud baseline performance of the application to be deployed. We first developed testing coverage criteria for cloud performance uncertainty.  We then developed a novel performance testing approach that employs cloud/application characterizations and a smart oracle to determine whether an application can meet its performance requirements on public clouds with high accuracy and low testing cost. Experimental results indicate that this testing strategy is a cost-effective approach to test for performance effects of cloud uncertainty when porting an application to the cloud.\n\n\n\n\nIn another project, we developed a novel cloud performance testing methodology called \"PT4Cloud,\" which employs non-parametric statistical approaches to provide reliable stopping conditions to obtain highly accurate performance distributions. PT4Cloud also allows the users to specify intuitive accuracy objectives and trade accuracy for testing cost. To reduce the cost of testing, we developed two test reduction techniques that can significantly reduce the number of test runs while retaining a high level of accuracy.  We experimentally evaluated PT4Cloud and found that it provides testing results with 95.4% accuracy on average while reducing the number of test runs by 62%. Our two test execution reduction techniques for PT4Cloud can reduce the number of test runs by 90.1% while retaining an average accuracy of 91%. \n\n\n\n\nIn the third project, we improved the PT4Cloud to test for performance in terms of point estimates, such as the mean performance or performance percentiles. This new performance testing technique is called Metior.  Metior includes a new test execution methodology using daily performance samples and a novel stop condition using non-parametric statistics. Experiment results showed that Metior can provide accurate performance results, in terms of means and percentiles, with only 1.4% error on average, and ensured 99.3% of the results to have less-than-3% errors. Compared to PT4Cloud, Metior could reduce the testing cost by 5.8 times for point estimates. We also applied Metior to two recently-published cloud performance prediction techniques. Experiment results showed that, by providing reliable training or input data, Metior could reduce the prediction error by 12.5% and reduce the input data size by 19.2% on average.\n\n\n\n\nWhile the previous projects involved static workloads, we introduced a project that works on dynamic workloads and elasticity; that is, we investigated how to adapt the PT4Cloud cloud performance testing methodology to work in the presence of dynamic workloads and elasticity.  Dynamic workloads such as web traffic are volatile and workload fluctuations occur for various factors which would require an IaaS elastic service. Elasticity is the ability of a system to adapt to workload changes by provisioning and  de-provisioning  cloud  resources to match changing demands and is challenging for performance testing. Users can utilize auto-scaling policies to achieve elasticity and  enhance  performance  and  cost  satisfaction. A major challenge when creating effective scaling policies to utilize elasticity  is that users are expected to have precise knowledge of the performance of their application.  Our experiment results with several applications and auto-scaling configurations showed  that the statistics-based performance testing methodology can be applied with reliable stop conditions and achieve highly accurate performance distributions with confidence bands. Our results show testing results with similar accuracy on average to results achieved for applications with static workloads also with significant reduction in test runs in dynamic workloads.\n\n\nBesides performance testing, this NSF award also supported the PIs\u2019 other research projects on cloud computing, including the development of several cloud job scheduling algorithms for time-sensitive cloud jobs, the development of a generic cloud workload predictor with automated machine-learning optimizations, and the first benchmarking framework for interactive cloud 3D applications.\n\n\n\n \n \n\n \n\n\t\t\t\t\tLast Modified: 10/19/2020\n\n\t\t\t\t\tSubmitted by: Mary Lou Soffa"
 }
}