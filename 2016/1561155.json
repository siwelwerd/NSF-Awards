{
 "awd_id": "1561155",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: ArguLex -  Applying Automated Analysis to a Learning Progression for Argumentation.",
 "cfda_num": "47.076",
 "org_code": "11040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Finbarr Sloane",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 117986.0,
 "awd_amount": 117986.0,
 "awd_min_amd_letter_date": "2016-08-29",
 "awd_max_amd_letter_date": "2016-08-29",
 "awd_abstract_narration": "Argumentation is fundamental to both science and science education.  This perspective is reflected in the Next Generation Science Standards where argumentation is presented as one of eight fundamental science and engineering practices through which students learn both the core ideas of science and the crosscutting concepts of science. Argumentation, however, is not measured well using standard multiple choice items. An alternative that is well suited to measuring argumentation skills is the assessment of student written work; but this approach is both expensive and time consuming. This research project addresses this issue through research into using language technology to automate the scoring of student written work to assess their argumentation skills.\r\n\r\nThis project applies lexical analysis and machine learning technologies to develop an efficient, valid, reliable and automated measure of middle school students' abilities to engage in scientific argumentation. The project will build upon prior work that developed high quality assessments for a learning progression for argumentation. However, these assessments are time and resource intensive to score. But when used with automated approaches, such assessments will allow measurement of argumentation to be taken to scale with rapid formative feedback, and will be an invaluable resource for STEM teachers, researchers, and teacher educators. The project brings together BSCS researchers who have experience measuring argumentation; Michigan State University's Automated Analysis of Constructed Response research group which provides expertise across a range of scientific disciplines refining analysis for formative educational purposes; Stanford University which provides expertise in learning progressions for argumentation in science; and Western Michigan University which serves as an external evaluator.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mary Anne",
   "pi_last_name": "Sydlik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mary Anne Sydlik",
   "pi_email_addr": "maryanne.sydlik@wmich.edu",
   "nsf_id": "000076990",
   "pi_start_date": "2016-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Western Michigan University",
  "inst_street_address": "1903 W MICHIGAN AVE",
  "inst_street_address_2": "",
  "inst_city_name": "KALAMAZOO",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "2693878298",
  "inst_zip_code": "490085200",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MI04",
  "org_lgl_bus_name": "WESTERN MICHIGAN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "J7WULLYGFRH1"
 },
 "perf_inst": {
  "perf_inst_name": "Western Michigan University",
  "perf_str_addr": "1903 W. Michigan Ave",
  "perf_city_name": "Kalamazoo",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "490085200",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MI04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "798000",
   "pgm_ele_name": "ECR-EDU Core Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8244",
   "pgm_ref_txt": "EHR CL Opportunities (NSF 14-302)"
  },
  {
   "pgm_ref_code": "8817",
   "pgm_ref_txt": "STEM Learning & Learning Environments"
  }
 ],
 "app_fund": [
  {
   "app_code": "0416",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001617DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 117986.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Argumentation is fundamental to science education, both as a prominent feature of scientific reasoning and as an effective mode of learning. This perspective is reflected in contemporary frameworks and standards, the successful implementation of which requires a paradigm shift in science assessment from the measurement of knowledge and understanding to the measurement of performance and knowledge in use. Argumentation performance tasks must capture the many ways students can construct and evaluate arguments in science, which often require constructed response (i.e. open ended) assessment tasks, yet such tasks are both expensive and resource-intensive to score. In this project we explored how automated lexical analyses and machine learning techniques can be applied to develop efficient, valid, and reliable constructed response measures of student&rsquo;s competency with written scientific argumentation that are aligned with a validated argumentation learning progression.</p>\n<p>Data came from 1,000 middle school students in the San Francisco Bay Area and are based on three argumentation item sets in different science contexts. The findings demonstrate that we have been able to develop machine learning based text&nbsp; scoring models that can achieve substantial to almost perfect agreement between human-assigned and computer-predicted scores. Computer model performance was slightly weaker for harder items targeting higher levels of the learning progression, largely due to the linguistic complexity of these responses and the sparsity of higher-level responses in the data set used to train the computer models. Comparing the efficacy of different scoring approaches revealed that breaking down students&rsquo; arguments into multiple components (e.g. the presence of an accurate claim or providing sufficient evidence); developing computer models for each component; and combining predicted scores from these analytic components into a single holistic score, produced better results than developing computer models based on holistic scoring approaches.</p>\n<p>This project also explored the extent to which computer scoring models for assessing students&rsquo; argumentation in science might be more or less severe when scoring students who have been designated as English Learner (EL) students than humans scoring the same data. We found that while no one machine scoring approach demonstrated significant bias, performance on certain items demonstrated that analytic scoring models had significant potential to widen the performance gaps between EL students and non-EL students, while holistic scoring models appeared to maintain the performance gaps introduced by the human, if not shrink them. This is despite the fact that holistic models were significantly more severe than the other two raters across all students on the assessment as a whole. These findings can inform researchers choosing between machine learning approaches , and highlights how routine psychometrics should be run on automated assessments, the ethical imperative to recruit diverse teams of humans to train machine algorithms in education, and the feasibility and ethics of creating automated text scoring models for education.</p>\n<p>Interest in and use of automated assessment continues to grow. For example, the California Assessment for Science Tests has been implemented which uses partially automated assessment. In addition, the latest OECD PISA tests increasingly make use of automated assessments. There is also an increasing number of research publications which leverage the results from automated assessments to return feedback to students, including assessments that target key scientific practices. Our research using automated analysis for argumentation contributes to these larger fields, as well as advances the effort as we attempt to use automated analysis tied to an assessment aligned with an underlying cognitive model of student learning. This project has been highly impactful, and has led to multiple research papers and presentations at national and international conferences. It has also developed capacity within each of the partner institutions, and trained multiple graduate students, post-doctoral fellows, and research scientists.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/16/2021<br>\n\t\t\t\t\tModified by: Mary Anne&nbsp;Sydlik</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nArgumentation is fundamental to science education, both as a prominent feature of scientific reasoning and as an effective mode of learning. This perspective is reflected in contemporary frameworks and standards, the successful implementation of which requires a paradigm shift in science assessment from the measurement of knowledge and understanding to the measurement of performance and knowledge in use. Argumentation performance tasks must capture the many ways students can construct and evaluate arguments in science, which often require constructed response (i.e. open ended) assessment tasks, yet such tasks are both expensive and resource-intensive to score. In this project we explored how automated lexical analyses and machine learning techniques can be applied to develop efficient, valid, and reliable constructed response measures of student\u2019s competency with written scientific argumentation that are aligned with a validated argumentation learning progression.\n\nData came from 1,000 middle school students in the San Francisco Bay Area and are based on three argumentation item sets in different science contexts. The findings demonstrate that we have been able to develop machine learning based text  scoring models that can achieve substantial to almost perfect agreement between human-assigned and computer-predicted scores. Computer model performance was slightly weaker for harder items targeting higher levels of the learning progression, largely due to the linguistic complexity of these responses and the sparsity of higher-level responses in the data set used to train the computer models. Comparing the efficacy of different scoring approaches revealed that breaking down students\u2019 arguments into multiple components (e.g. the presence of an accurate claim or providing sufficient evidence); developing computer models for each component; and combining predicted scores from these analytic components into a single holistic score, produced better results than developing computer models based on holistic scoring approaches.\n\nThis project also explored the extent to which computer scoring models for assessing students\u2019 argumentation in science might be more or less severe when scoring students who have been designated as English Learner (EL) students than humans scoring the same data. We found that while no one machine scoring approach demonstrated significant bias, performance on certain items demonstrated that analytic scoring models had significant potential to widen the performance gaps between EL students and non-EL students, while holistic scoring models appeared to maintain the performance gaps introduced by the human, if not shrink them. This is despite the fact that holistic models were significantly more severe than the other two raters across all students on the assessment as a whole. These findings can inform researchers choosing between machine learning approaches , and highlights how routine psychometrics should be run on automated assessments, the ethical imperative to recruit diverse teams of humans to train machine algorithms in education, and the feasibility and ethics of creating automated text scoring models for education.\n\nInterest in and use of automated assessment continues to grow. For example, the California Assessment for Science Tests has been implemented which uses partially automated assessment. In addition, the latest OECD PISA tests increasingly make use of automated assessments. There is also an increasing number of research publications which leverage the results from automated assessments to return feedback to students, including assessments that target key scientific practices. Our research using automated analysis for argumentation contributes to these larger fields, as well as advances the effort as we attempt to use automated analysis tied to an assessment aligned with an underlying cognitive model of student learning. This project has been highly impactful, and has led to multiple research papers and presentations at national and international conferences. It has also developed capacity within each of the partner institutions, and trained multiple graduate students, post-doctoral fellows, and research scientists.\n\n\t\t\t\t\tLast Modified: 12/16/2021\n\n\t\t\t\t\tSubmitted by: Mary Anne Sydlik"
 }
}