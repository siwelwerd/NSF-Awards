{
 "awd_id": "1527558",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: Multimodal Brain Computer Interface for Human-Robot Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2016-05-15",
 "awd_exp_date": "2021-04-30",
 "tot_intn_awd_amt": 308077.0,
 "awd_amount": 308077.0,
 "awd_min_amd_letter_date": "2016-05-27",
 "awd_max_amd_letter_date": "2016-05-27",
 "awd_abstract_narration": "Human Robot Interaction (HRI) is research that is a key component in making robots part of our everyday life. Current interface modalities such as video, keyboard, tactile, audio, and speech can all contribute to an HRI interface. However, an emerging area is the use of Brain-Computer Interfaces (BCI) for communication and information exchange between humans and robots. BCIs can provide another channel of communication with more direct access to physiological changes in the brain. BCIs vary widely in their capabilities, particularly with respect to spatial resolution, temporal resolution and noise. This project is aimed at exploring the use of multimodal BCIs for HRI. Multimodal BCIs, also referred to as hybrid BCIs (hBCI), have been shown to improve performance over single modality interfaces. This project is focused on using a novel suite of sensors (Electroencephalography (EEG), eye-tracking, pupillary size, computer vision, and functional Near Infrared Spectroscopy (fNIRS)) to improve current HRI systems. Each of these sensing modalities can reinforce and complement each other, and when used together, can address a major shortcoming of current BCIs which is the determination of the user state or situational awareness (SA). SA is a necessary component of any complex interaction between agents, as each agent has its own expectations and assumptions about the environment. Traditional BCI systems have difficulty recognizing state and context, and accordingly can become confusing and unreliable. This project will develop techniques to recognize state from multiple modalities, and will also allow the robot and human to learn about each other's state and expectations using the hBCI we are developing. The goal is to build a usable hBCI for real physical robot environments, with noise, real-time constraints, and added complexity.\r\n\r\nThe technical contributions of this project include:\r\n1. Characterization of a novel hBCI interface for visual recognition and labeling tasks with real physical data and environments.\r\n2. Integration of fNIRS sensing with EEG and other modalities in human robot interaction tasks. We will test our ability in the temporal domain to determine at what timescale we can correctly classify movement components that would predict a correct (rewarding) trial or non-rewarding/incorrect movement.\r\n3. Analysis and validation of the hBCI in complex robotic tele-operation tasks with human subject operators such as open door, grasp object on table, pick up item off floor etc.\r\n4. Use of hBCI to characterize human/robot state and create a learning method to recognize state over time.\r\n5. Use of augmented reality for HRI decision making.\r\n6. Further develop hBCI for tracking cognitive states related to reward, motivation, attention and value.\r\nA new class of HRI interfaces will be developed that can expand the ability of humans to work with robots; promote the use and acceptance of robot agent systems in everyday life; expand the use of hBCIs in areas other than robotics for human-machine interaction; further the development of hBCIs as our system will be tapping into reward modulated activity that will be used via reinforcement learning to autonomously update the learning machinery; and bridge the educational divide between Engineering and Neuroscience.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Joseph",
   "pi_last_name": "Francis",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Joseph T Francis",
   "pi_email_addr": "jtfranci@Central.UH.EDU",
   "nsf_id": "000576170",
   "pi_start_date": "2016-05-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Houston",
  "inst_street_address": "4300 MARTIN LUTHER KING BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "HOUSTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7137435773",
  "inst_zip_code": "772043067",
  "inst_country_name": "United States",
  "cong_dist_code": "18",
  "st_cong_dist_code": "TX18",
  "org_lgl_bus_name": "UNIVERSITY OF HOUSTON SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "QKWEF8XLMTT3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Houston",
  "perf_str_addr": "4800 Calhoun Blvd",
  "perf_city_name": "Houston",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "772042015",
  "perf_ctry_code": "US",
  "perf_cong_dist": "18",
  "perf_st_cong_dist": "TX18",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 308077.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our work aimed to move towards the multi-modal recording of neural activity patterns to be used by non-human computational agents, such as robotic systems. Towards this goal, we recorded changes in brain blood oxygenation via functional near-infrared spectroscopy (fNIRS) while simultaneously recording electroencephalography (EEG) in human subjects. We also utilized data from other sources, such as local field potentials (LFPs) recorded within the brain. We showed that we could use both sensor recording methods together to predict aspects of Alzheimer's and determine subjects' neural representation of their emotional states, such as changes due to expectations of reward and punishment and even effort. Neural responses to reward and punishment were used to determine the subjective value and level of motivation. In turn, these can be used by the computational agent as feedback information towards optimizing their behavior, under the assumption that the human or animal subject wishes the agent to be successful, such as when the agent is a brain-machine interface (BMI) acting to retrieve rewards for the user. Under these circumstances, the BMI agent is tapping into the user's neural activity to decode movement intention and obtain feedback on the user's subjective value and how the movement changed the user's motivational/arousal level. This can allow the BMI to alter its behavior to increase the user's emotional state.&nbsp; &nbsp;&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/12/2021<br>\n\t\t\t\t\tModified by: Joseph&nbsp;T&nbsp;Francis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur work aimed to move towards the multi-modal recording of neural activity patterns to be used by non-human computational agents, such as robotic systems. Towards this goal, we recorded changes in brain blood oxygenation via functional near-infrared spectroscopy (fNIRS) while simultaneously recording electroencephalography (EEG) in human subjects. We also utilized data from other sources, such as local field potentials (LFPs) recorded within the brain. We showed that we could use both sensor recording methods together to predict aspects of Alzheimer's and determine subjects' neural representation of their emotional states, such as changes due to expectations of reward and punishment and even effort. Neural responses to reward and punishment were used to determine the subjective value and level of motivation. In turn, these can be used by the computational agent as feedback information towards optimizing their behavior, under the assumption that the human or animal subject wishes the agent to be successful, such as when the agent is a brain-machine interface (BMI) acting to retrieve rewards for the user. Under these circumstances, the BMI agent is tapping into the user's neural activity to decode movement intention and obtain feedback on the user's subjective value and how the movement changed the user's motivational/arousal level. This can allow the BMI to alter its behavior to increase the user's emotional state.    \n\n\t\t\t\t\tLast Modified: 10/12/2021\n\n\t\t\t\t\tSubmitted by: Joseph T Francis"
 }
}