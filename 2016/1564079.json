{
 "awd_id": "1564079",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Beyond the Black Box: Understanding and Designing for User Expectations of Algorithmic Media",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2016-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 353644.0,
 "awd_amount": 353644.0,
 "awd_min_amd_letter_date": "2016-08-02",
 "awd_max_amd_letter_date": "2021-07-01",
 "awd_abstract_narration": "This research will study human and machine co-curation of online news feeds and develop new software to compare the user's intent with respect to idealized feed curation, when algorithms filter what appears in online news feeds. This will be combined with research that investigates the user's perception of the algorithm's actions and the user's perceptions of feed settings in control panels. Not only are these algorithmic processes opaque to most users, but many users don't even know that algorithms are making decisions on their behalf. People who might be aware of algorithmic processes at work have no way to verify their existence; changes in a user's search results, for example, might have resulted from a change in the algorithm, or from a change in the user's activity.  This work has the potential to increase public algorithm awareness that may lead to more in-depth exploration and possibly to learning about algorithms, with implications for the public understanding of science and technology.\r\n \r\nThis research will generate evidence-based knowledge in five main areas: (a) the level of social media algorithm awareness across a general population, (b) how awareness levels and interaction behavior change when exposed to facets of social feeds via an interactive social feed visualization tool, (c) whether the use of feed settings results in better feed perception, (d) what people want to see in their feeds, and (e) how we can communicate algorithmic process through design of feed content and feed interfaces. The outcomes of this work will help researchers and practitioners in various fields (Human Computer Interaction, Social Science, Design, Engineering, Law, Ethics) critically rethink current computation and design practice and lead to interfaces that help people understand the algorithms that shape their lives. Code templates to program unique algorithms will be provided to encourage algorithm creation, and public repositories will be made available to encourage sharing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christian",
   "pi_last_name": "Sandvig",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christian Sandvig",
   "pi_email_addr": "csandvig@umich.edu",
   "nsf_id": "000486088",
   "pi_start_date": "2016-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "426 Thompson St.",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481061248",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 353644.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The work funded by this grant centered around algorithm awareness, algorithm discovery, folk theories around algorithmic systems, people's behaviors around algorithmic intervention in systems, algorithm explanations, people's efforts and desire for agency, game theoretical influences in the design of algorithmic systems, ethics surrounding &nbsp;algorithmic decision making, and education &amp; mentoring around algorithmic systems for our students and the general public. We highlight some of the milestones below.</p>\n<p>Beginning with narrative visualization systems that guided viewers to see alternate outcomes of their own algorithmically curated feeds, we found that these mirrors of self offered foci for discussion, comparison, reflection, and interrogation. &nbsp;People formed folk theories, in some cases spanning multiple algorithmic systems. The exploration of these and other folk theories emerged as a valuable method for learning and for guiding design -- and is an ongoing research thread today.</p>\n<p>The creation of folk theories was made possible by a series of narrative visualization interfaces that highlighted alternate outcomes/outputs and counterfactuals. We created such narrative visualizations in the contexts of several social media feeds, recommender systems, aggregator systems, advertising interfaces, content moderation interfaces, and more. These narrative visualizations became a form of algorithm audit across these spaces, available to everyday people, and they were used internationally in computer science, design, and informatics departments. And the code was requested by social media platforms.</p>\n<p>These visualizations were a form of algorithm audit and led us to explore collective audits -- where the general public motivates the audit and ultimately participates collectively. These interfaces further provided a method for studying forms of agency. &nbsp;Two such forms arose--control and engagement.&nbsp;</p>\n<p>From the narrative visualizations, people learned that engagement offered some general level of control (e.g., the more I click, the more this happens). Control was further expected via control settings. That said, there were many settings -- people often forgot what control settings were in place, didn't know where to find them, and often, the action people perceived would happen from a setting differed from the outcome. And few people changed their settings. This is leading to new work in this space.</p>\n<p>With fewer settings in the immediate interface, however, people did engage. In this control space, we found that people attributed power to algorithmically-backed settings, even when they didn't function. &nbsp;In one study, we presented participants with functioning and non-functioning settings. The participants engaged with both and felt both the functioning and \"placebo\" settings performed well.</p>\n<p>This work led to studies around contestability -- and how one engages with an algorithmic to expose the reasoning behind decisions and contest the outcome -- specifically around content moderation. This work involved nine workshops with communities affected by content moderation decisions and two general workshops. Findings highlighted the importance of making clear the values and reasoning embedded within algorithmic systems and the importance of decision-making by a jury of one's peers. This work is moving us towards new forms of algorithm-human interaction.</p>\n<p><span>In addition to the narrative visualizations, we built systems to investigate how consumers can protect themselves against unwanted and/or adverse learning performed at their expense. More precisely, using optimal stopping time tools, we addressed a fundamental tradeoff faced by a consumer facing a seller equipped with a personalized&nbsp;</span>pricing algorithm: allowing the user to learn their own valuation without revealing too much information to the algorithm.&nbsp;<span>This opens the door to more research towards the development of general (counter)-ML tools for users.</span></p>\n<p>Additional work in this space included the creation of an infrastructure for auditing online housing (ads and prioritized homes), an infrastructure for interrogating ad explanations, an analysis on the benefits of \"slow\" algorithms for reflection, and people's behaviors around imperfect algorithms (in the context of an auto-grader) in a university classroom setting.</p>\n<p>This grant supported the work of over seven PhD students, four MS students, twelve undergraduates, and two post docs.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/31/2023<br>\n\t\t\t\t\tModified by: Christian&nbsp;Sandvig</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe work funded by this grant centered around algorithm awareness, algorithm discovery, folk theories around algorithmic systems, people's behaviors around algorithmic intervention in systems, algorithm explanations, people's efforts and desire for agency, game theoretical influences in the design of algorithmic systems, ethics surrounding  algorithmic decision making, and education &amp; mentoring around algorithmic systems for our students and the general public. We highlight some of the milestones below.\n\nBeginning with narrative visualization systems that guided viewers to see alternate outcomes of their own algorithmically curated feeds, we found that these mirrors of self offered foci for discussion, comparison, reflection, and interrogation.  People formed folk theories, in some cases spanning multiple algorithmic systems. The exploration of these and other folk theories emerged as a valuable method for learning and for guiding design -- and is an ongoing research thread today.\n\nThe creation of folk theories was made possible by a series of narrative visualization interfaces that highlighted alternate outcomes/outputs and counterfactuals. We created such narrative visualizations in the contexts of several social media feeds, recommender systems, aggregator systems, advertising interfaces, content moderation interfaces, and more. These narrative visualizations became a form of algorithm audit across these spaces, available to everyday people, and they were used internationally in computer science, design, and informatics departments. And the code was requested by social media platforms.\n\nThese visualizations were a form of algorithm audit and led us to explore collective audits -- where the general public motivates the audit and ultimately participates collectively. These interfaces further provided a method for studying forms of agency.  Two such forms arose--control and engagement. \n\nFrom the narrative visualizations, people learned that engagement offered some general level of control (e.g., the more I click, the more this happens). Control was further expected via control settings. That said, there were many settings -- people often forgot what control settings were in place, didn't know where to find them, and often, the action people perceived would happen from a setting differed from the outcome. And few people changed their settings. This is leading to new work in this space.\n\nWith fewer settings in the immediate interface, however, people did engage. In this control space, we found that people attributed power to algorithmically-backed settings, even when they didn't function.  In one study, we presented participants with functioning and non-functioning settings. The participants engaged with both and felt both the functioning and \"placebo\" settings performed well.\n\nThis work led to studies around contestability -- and how one engages with an algorithmic to expose the reasoning behind decisions and contest the outcome -- specifically around content moderation. This work involved nine workshops with communities affected by content moderation decisions and two general workshops. Findings highlighted the importance of making clear the values and reasoning embedded within algorithmic systems and the importance of decision-making by a jury of one's peers. This work is moving us towards new forms of algorithm-human interaction.\n\nIn addition to the narrative visualizations, we built systems to investigate how consumers can protect themselves against unwanted and/or adverse learning performed at their expense. More precisely, using optimal stopping time tools, we addressed a fundamental tradeoff faced by a consumer facing a seller equipped with a personalized pricing algorithm: allowing the user to learn their own valuation without revealing too much information to the algorithm. This opens the door to more research towards the development of general (counter)-ML tools for users.\n\nAdditional work in this space included the creation of an infrastructure for auditing online housing (ads and prioritized homes), an infrastructure for interrogating ad explanations, an analysis on the benefits of \"slow\" algorithms for reflection, and people's behaviors around imperfect algorithms (in the context of an auto-grader) in a university classroom setting.\n\nThis grant supported the work of over seven PhD students, four MS students, twelve undergraduates, and two post docs.\n\n \n\n\t\t\t\t\tLast Modified: 10/31/2023\n\n\t\t\t\t\tSubmitted by: Christian Sandvig"
 }
}