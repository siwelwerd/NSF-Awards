{
 "awd_id": "1617884",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Fundamental Analysis and Design of Repair-Efficient Cloud Storage Systems: a Linear Perspective",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2016-06-15",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 499618.0,
 "awd_amount": 499618.0,
 "awd_min_amd_letter_date": "2016-06-10",
 "awd_max_amd_letter_date": "2020-04-27",
 "awd_abstract_narration": "Cloud storage systems are increasingly being adopted by a wide spectrum of data intensive applications such as web search, cloud computing, distributed file sharing, and various types of data networks with high access demand, including social networks, health and medical databases, and banking system. A cloud storage system consists of several data centers and computer servers that are connected through Internet. While individual components of the system are subject to several types of failure, reliability, in the sense of data availability, and ability of regenerating failed storage nodes are two key features to design cloud storage systems. As the size of stored data and the number of users accessing the data continues to increase, designing robust, efficient, and scalable systems is a challenging and yet critically important problem. This research will develop a solid theoretical foundation for design and analysis of efficient cloud storage systems. \r\n\r\nOn the technical side, this research will focus on (1) studying the tradeoff between storage overhead and repair requirements in cloud storage systems, and (2) development of efficient coding schemes with guarantees on data recovery and failed node regeneration. While characterizing the general information-theoretic tradeoff appears to be intractable, progress can be made by limiting the problem to the class of practically relevant and low-complexity linear codes, and exploiting the duality between linear codes and multi-dimensional subspaces over finite fields. An optimization problem is formulated for this question, where an intelligent interpretation of the optimum solution provides insightful guidelines to devise code construction mechanisms. Lastly, a unified framework is proposed for code construction, to not only encompass several ad-hoc designs, but also generalize construction to all optimum operating points of the system.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Soheil",
   "pi_last_name": "Mohajer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Soheil Mohajer",
   "pi_email_addr": "soheil@umn.edu",
   "nsf_id": "000679394",
   "pi_start_date": "2016-06-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota-Twin Cities",
  "perf_str_addr": "200 Union Street SE",
  "perf_city_name": "Minneapolis",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554550170",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 499618.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The dynamic, large and disparate volume of data garnered from social media, Internet-driven technologies, financial records, and clinical research has arisen an increasing demand for reliable and scalable storage technologies. The focus of the storage industry has been shifted from central systems to distributed storage systems (DSS) in recent years. These systems are widely used in modern data centers, such as Google File System, Facebook Distributed File System, Microsoft Azure as well as peer-to-peer storage settings. In this project, we studied several aspects of distributed storage systems. A DSS includes several storage units, which distributively store a huge amount of data. However, individual storage nodes are unreliable due to various hardware and software failures. Consequently, redundancy is introduced to improve the system's reliability in the presence of node failures. While such redundancy improves the reliability of the system, it reduces the storage efficiency. Moreover, in order to be durable, it is necessary for a storage system to repair the failed nodes. The repair process consists of downloading (part of) the content of a number of surviving nodes to reconstruct the missing content of the failed nodes. The conventional erasure codes suffer from high repair bandwidth, the total size of data to be downloaded for the repair of each failed node. Regenerating code is a class of erasure codes that have gained popularity in this context, due to their low repair bandwidth, while providing the same level of fault tolerance as erasure codes.</span><br /><br /><span>For the course of this project, we have studied regenerating codes for DSS from several different aspects. It turns out that there is a trade-off between the storage efficiency and repair communication cost of a DSS. Such a trade-off is unknown in general. We have developed an optimality condition for a range of system parameters. &nbsp;Designing efficient codes with optimum performance is one of the major outcomes of this project. We first used a probabilistic approach to show that we can scale the size of the system, without degrading its performance. Our effort in this direction led to the design of determinant codes, which can be designed for a certain range of system parameters, and achieve the optimum desired trade-off. Later, we relaxed the limitation of the construction and developed a generic constructive class of codes, known as cascade codes, which can be generated for any set of system parameters. The cascade codes subsume and outperform all the existing constructions in the literature, and they are state-of-the-art in the design of regenerating codes.</span><br /><br /><span>In many applications, the data stored in a DSS is sensitive and needs to be protected against unauthorized or malicious users who wish to access (passive adversary model) or modify (active adversary model) the data. We studied the security and privacy of the data in DSS, and developed classes of codes that guarantee no information leakage to an eavesdropper with different types of attacks. We also designed error-resilient regenerating codes, which are robust to the presence of a certain number of erroneous nodes in the network. &nbsp;</span><br /><br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/30/2021<br>\n\t\t\t\t\tModified by: Soheil&nbsp;Mohajer</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe dynamic, large and disparate volume of data garnered from social media, Internet-driven technologies, financial records, and clinical research has arisen an increasing demand for reliable and scalable storage technologies. The focus of the storage industry has been shifted from central systems to distributed storage systems (DSS) in recent years. These systems are widely used in modern data centers, such as Google File System, Facebook Distributed File System, Microsoft Azure as well as peer-to-peer storage settings. In this project, we studied several aspects of distributed storage systems. A DSS includes several storage units, which distributively store a huge amount of data. However, individual storage nodes are unreliable due to various hardware and software failures. Consequently, redundancy is introduced to improve the system's reliability in the presence of node failures. While such redundancy improves the reliability of the system, it reduces the storage efficiency. Moreover, in order to be durable, it is necessary for a storage system to repair the failed nodes. The repair process consists of downloading (part of) the content of a number of surviving nodes to reconstruct the missing content of the failed nodes. The conventional erasure codes suffer from high repair bandwidth, the total size of data to be downloaded for the repair of each failed node. Regenerating code is a class of erasure codes that have gained popularity in this context, due to their low repair bandwidth, while providing the same level of fault tolerance as erasure codes.\n\nFor the course of this project, we have studied regenerating codes for DSS from several different aspects. It turns out that there is a trade-off between the storage efficiency and repair communication cost of a DSS. Such a trade-off is unknown in general. We have developed an optimality condition for a range of system parameters.  Designing efficient codes with optimum performance is one of the major outcomes of this project. We first used a probabilistic approach to show that we can scale the size of the system, without degrading its performance. Our effort in this direction led to the design of determinant codes, which can be designed for a certain range of system parameters, and achieve the optimum desired trade-off. Later, we relaxed the limitation of the construction and developed a generic constructive class of codes, known as cascade codes, which can be generated for any set of system parameters. The cascade codes subsume and outperform all the existing constructions in the literature, and they are state-of-the-art in the design of regenerating codes.\n\nIn many applications, the data stored in a DSS is sensitive and needs to be protected against unauthorized or malicious users who wish to access (passive adversary model) or modify (active adversary model) the data. We studied the security and privacy of the data in DSS, and developed classes of codes that guarantee no information leakage to an eavesdropper with different types of attacks. We also designed error-resilient regenerating codes, which are robust to the presence of a certain number of erroneous nodes in the network.  \n\n\n\n\n\t\t\t\t\tLast Modified: 09/30/2021\n\n\t\t\t\t\tSubmitted by: Soheil Mohajer"
 }
}