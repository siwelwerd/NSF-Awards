{
 "awd_id": "1562364",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Broad-Coverage Semantic Parsing: Linguistic Representation Learning from Crowd-Scale Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "D.  Langendoen",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 1005977.0,
 "awd_amount": 1203556.0,
 "awd_min_amd_letter_date": "2016-03-09",
 "awd_max_amd_letter_date": "2019-06-17",
 "awd_abstract_narration": "Automated understanding of text is a capability that will advance a wide range of language technologies, including information extraction, question answering, opinion analysis, and translation between languages.  Such technologies have been in demand in the intelligence and defense communities for many years, and they now underlie many commercially available information-management tools.  This project develops robust algorithms that understand natural language expressions by mapping them to formal representations of their meaning, a technique known as semantic parsing.  For semantic parsing to be employed in technologies like those listed above, it needs to overcome the fundamental challenge of broad coverage, the ability to handle any text input, in multiple languages.  This project meets this challenge by creating new methods for gathering large repositories of semantically annotated data at greatly reduced cost; these are then used to train much more accurate broad-coverage parsing models.  The results of this project include open-source implementations, high-quality annotated corpora on an unprecedented scale, and reusable distributed semantic representations for use by the community of natural language processing researchers and practitioners. \r\n\r\nThe goal of broad-coverage semantic parsing can only be achieved by simultaneously focusing on new, large scale sources of data with semantically meaningful annotations and new learning algorithms for inducing models with the representational capacity to make full use of such data.  For scalable data collection, this project introduces new techniques that rely on two key complementary insights: (1) any reader who understands a text can answer questions about it, and (2) questions can be constructed whose answers probe any aspect of semantics that need to be recovered.  These observations allow designing new data collection techniques that reduce the burden of semantic annotation by providing simple questions and answers about texts.  This QA-style annotation can be done for any text in any language, given only native speakers, bypassing the significant effort that currently goes into defining detailed annotation standards.  It also allows gathering new datasets on a much larger scale, and for more diverse text types, than ever before.  In addition, the project develops new representation learning techniques that tie together a wide range of semantic annotation styles, including the new crowdsourced ones, in a multitask learning setup.  Continuous representations (e.g., of word types) provide a powerful way to allow sharing of statistical strength across a large vocabulary, many of whose elements are sparsely observed.  While past work has emphasized learning word embeddings, this project employs a shared continuous space (\"framespace\") that can capture abstract frames and roles used in predicate-argument (and logical) semantics.  The usefulness of these representations depends on the tasks they are trained to perform, and using multiple related tasks can lead to benefits on all of them, by sharing of statistical strength across task-specific representations, across elements of the semantic lexicon, and even across languages.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Noah",
   "pi_last_name": "Smith",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Noah A Smith",
   "pi_email_addr": "noah@allenai.org",
   "nsf_id": "000228357",
   "pi_start_date": "2016-03-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Luke",
   "pi_last_name": "Zettlemoyer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Luke Zettlemoyer",
   "pi_email_addr": "lsz@cs.washington.edu",
   "nsf_id": "000581613",
   "pi_start_date": "2016-03-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 313528.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 716449.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 173579.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-c73f9452-7fff-88e1-3b78-cfc47a2018c2\" style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The ability to take in natural language utterances as input and draw inferences about its intended meaning is a longstanding challenge of artificial intelligence.&nbsp; Today, large-scale data sources -- including raw text collections and low-cost annotated datasets built by crowdsourcing -- are central to this pursuit.&nbsp; A major attraction of this large-scale, data-driven paradigm is its potential to deliver systems with broad coverage of linguistic phenomena and even different languages.&nbsp; The period of the project was one of rapid advances in natural language processing, with language models pretrained on very large raw text and transformer neural network architectures emerging as nearly ubiquitous ingredients in natural language processing research systems.&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project contributed a range of new, robust methods for natural language processing that aim to make better use of large-scale data sources, including:&nbsp; the first human-in-the-loop parser, multiple state-of-the-art models for semantic role labeling and semantic dependency parsing, an algorithm for backpropagating through structured &ldquo;hard&rdquo; decisions in a neural network, learning techniques for exploiting syntactic signals when learning semantic structure, new learning methods that combine data from different languages to benefit those with less data, new methods for reducing sequence-to-sequence tasks to multidimensional regression problems, methods for incorporating semantic graphs alongside pretrained language models to benefit various tasks, a language modeling architecture that avoids vocabulary-specific parameters by making all word representations fully compositional, a new formulation of the self-attention mechanism in transformers with linear runtime (as opposed to quadratic as in the original model), and a modular approach to building language models that specialize to different text domains without sacrificing the benefits of a single large model.&nbsp; These new methods were evaluated on standard benchmarks used by the natural language processing community.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">An important formal contribution of this project was rational recurrences, a family of neural network models that can also be understood as weighted finite-state automata.&nbsp; This allows a new way to unify many existing models, an elegant way to derive new models that are efficient and perform well, and theoretical advances on the limitations of different families widely in use, including a formal hierarchy of recurrent neural network architectures with implications for the transformer.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project delivered protocols for collecting and understanding datasets required to build natural language processing modules.&nbsp; Notably, by recasting a wide range of linguistic annotation tasks as question answering tasks, the newly created datasets advanced the state of the art on semantic role labeling, open information extraction, and more.&nbsp; It also allowed us to study fundamental challenges in question answering, which will impact on a much broader range of end-tasks given these new connections. This challenges include conversational machine reading where initial questions can be received with clarifying follow ups, ambiguous question answering where the user may have information gathering needs that are hard to specify in an singe unambiguous query, and commonsense question answering which requires reasoning about commonly used but rarely stated facts about how the world works.&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Understanding a natural language processing dataset, once it is collected, is a relatively new research direction; this project introduced data maps, which visualize the instances in a dataset in terms of how easy they are to learn and how ambiguous they are to a learner.&nbsp; Another new contribution comes in the form of contrast sets, where experts perturb test examples to identify generalization failures of a system.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Together, our advances have targeted a very wide range of challenges in semantic understanding of text, from the lens of question answering. This includes everything from robust methods with formal analysis, new question answering connections and applications, and new data with more realistic question answering challenges and tools for understanding data. They have laid out the framework for new ways of studying these problems, which we hope will support significant future research efforts. </span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/07/2021<br>\n\t\t\t\t\tModified by: Noah&nbsp;A&nbsp;Smith</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The ability to take in natural language utterances as input and draw inferences about its intended meaning is a longstanding challenge of artificial intelligence.  Today, large-scale data sources -- including raw text collections and low-cost annotated datasets built by crowdsourcing -- are central to this pursuit.  A major attraction of this large-scale, data-driven paradigm is its potential to deliver systems with broad coverage of linguistic phenomena and even different languages.  The period of the project was one of rapid advances in natural language processing, with language models pretrained on very large raw text and transformer neural network architectures emerging as nearly ubiquitous ingredients in natural language processing research systems. \nThis project contributed a range of new, robust methods for natural language processing that aim to make better use of large-scale data sources, including:  the first human-in-the-loop parser, multiple state-of-the-art models for semantic role labeling and semantic dependency parsing, an algorithm for backpropagating through structured \"hard\" decisions in a neural network, learning techniques for exploiting syntactic signals when learning semantic structure, new learning methods that combine data from different languages to benefit those with less data, new methods for reducing sequence-to-sequence tasks to multidimensional regression problems, methods for incorporating semantic graphs alongside pretrained language models to benefit various tasks, a language modeling architecture that avoids vocabulary-specific parameters by making all word representations fully compositional, a new formulation of the self-attention mechanism in transformers with linear runtime (as opposed to quadratic as in the original model), and a modular approach to building language models that specialize to different text domains without sacrificing the benefits of a single large model.  These new methods were evaluated on standard benchmarks used by the natural language processing community.\nAn important formal contribution of this project was rational recurrences, a family of neural network models that can also be understood as weighted finite-state automata.  This allows a new way to unify many existing models, an elegant way to derive new models that are efficient and perform well, and theoretical advances on the limitations of different families widely in use, including a formal hierarchy of recurrent neural network architectures with implications for the transformer.\nThis project delivered protocols for collecting and understanding datasets required to build natural language processing modules.  Notably, by recasting a wide range of linguistic annotation tasks as question answering tasks, the newly created datasets advanced the state of the art on semantic role labeling, open information extraction, and more.  It also allowed us to study fundamental challenges in question answering, which will impact on a much broader range of end-tasks given these new connections. This challenges include conversational machine reading where initial questions can be received with clarifying follow ups, ambiguous question answering where the user may have information gathering needs that are hard to specify in an singe unambiguous query, and commonsense question answering which requires reasoning about commonly used but rarely stated facts about how the world works. \nUnderstanding a natural language processing dataset, once it is collected, is a relatively new research direction; this project introduced data maps, which visualize the instances in a dataset in terms of how easy they are to learn and how ambiguous they are to a learner.  Another new contribution comes in the form of contrast sets, where experts perturb test examples to identify generalization failures of a system.\nTogether, our advances have targeted a very wide range of challenges in semantic understanding of text, from the lens of question answering. This includes everything from robust methods with formal analysis, new question answering connections and applications, and new data with more realistic question answering challenges and tools for understanding data. They have laid out the framework for new ways of studying these problems, which we hope will support significant future research efforts. \n\n\t\t\t\t\tLast Modified: 12/07/2021\n\n\t\t\t\t\tSubmitted by: Noah A Smith"
 }
}