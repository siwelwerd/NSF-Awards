{
 "awd_id": "1628384",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: FULL: Emerging Nonvolatile Memory for Analog-iterative Numerical Methods",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 825000.0,
 "awd_amount": 833000.0,
 "awd_min_amd_letter_date": "2016-08-22",
 "awd_max_amd_letter_date": "2020-09-30",
 "awd_abstract_narration": "A new type of computer memory - crosspoint resistive memory - has\r\nemerged as a likely candidate to replace current memory technology in\r\nfuture computing systems. This memory allows for potential computer\r\ndesigns with high memory capacity and with memory incorporated\r\ndirectly into processing units.  Novel thinking about computational\r\nmethods is required to exploit the potential of these novel\r\nsystems. This project will explore new, fundamental methods in the\r\nfield of numerical optimization that are suited to implementation on\r\ncomputer systems that incorporate crosspoint resistive memory. The\r\nfield of optimization is chosen as a testbed because of its importance\r\nto a wide range of scientific disciplines.\r\n\r\nCrosspoint memory has unprecedented advantages in capacity and access\r\nlatency. Substantial innovation is required to fully exploit the\r\npotential benefits of integrating memory into processing units;\r\ncurrent algorithms are unsuitable, because they are constrained by the\r\nvon Neumann bottleneck. The PIs will design the Gigascale Analog Iterative\r\nNetwork Solver (GAINS), a system architecture to enable efficient\r\nin-situ data processing. GAINS alters the application-, architecture-,\r\nand logic/circuit-level abstractions that enable designers and\r\ndevelopers at each layer to work independently. (i) It promotes\r\nmatrices to a first-class data type. (ii) It integrates computataion\r\nand memory, avoiding pitfalls of conventional memory\r\nhierarchies. (iii) It exploits multi-valued representations in storage\r\nand computation.  (iv) It replaces binary logic circuits with\r\nmulti-valued analog circuits, reducing area overhead and power\r\nconsumption. The PIs will investigate the effects of this changed paradigm\r\non the design of algorithms in numerical optimization and machine\r\nlearning.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mikko",
   "pi_last_name": "Lipasti",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Mikko H Lipasti",
   "pi_email_addr": "mikko@engr.wisc.edu",
   "nsf_id": "000290802",
   "pi_start_date": "2020-06-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Jing",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jing Li",
   "pi_email_addr": "janeli@seas.upenn.edu",
   "nsf_id": "000702445",
   "pi_start_date": "2016-08-22",
   "pi_end_date": "2020-06-02"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Wright",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen J Wright",
   "pi_email_addr": "swright@cs.wisc.edu",
   "nsf_id": "000485636",
   "pi_start_date": "2016-08-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Mikko",
   "pi_last_name": "Lipasti",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Mikko H Lipasti",
   "pi_email_addr": "mikko@engr.wisc.edu",
   "nsf_id": "000290802",
   "pi_start_date": "2016-08-22",
   "pi_end_date": "2020-06-02"
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "1415 Engineering Drive",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061607",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 825000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project investigated a range of algorithms that are suitable for deployment on the Gigascale Analog Iterative Network Solver (GAINS), a system architecture that closely couples an analog computation substrate with crosspoint resistive memory arrays in a massively parallel, scalable design that is well suited for solving optimization and linear algebra problems with very large data sets.&nbsp; The project also investigated deploying these algorithms on a near-term substrate realizable with conventional CMOS logic, demonstrating many of the algorithmic and hardware advantages of GAINS without waiting for the availability of novel, hence high-risk, resistive memory hardware. To increase researcher?s productivity and broaden the appeal of these novel approaches, a new mechanism was developed to make it compatible with existing popular domain-specific programming frameworks such as Pytorch, as well as a new domain-specific programming language, BitSAD, was invented, coupled with an optimizing compiler that streamlined algorithm development, simulation, debugging, and rapid prototyping of hardware substrates.&nbsp; Two releases of this compiler, along with the first benchmark suite for this class of algorithms, were released under an open-source license. This research has substantially advanced the design and co-design of algorithms and computation substrates which are capable of tackling optimization, linear algebra, and machine learning applications, providing designs with much higher levels of performance and energy efficiency than existing, conventional approaches.</p>\n<p><br />The research conducted during this project led to new approaches for deploying complex numerical algorithms on energy-efficient, scalable, reduced-precision substrates.&nbsp; For example, the work conducted to date has led to the first mathematical framework for determine scaling factors and error bounds when deploying a recurrent numerical solver on limited precision neural hardware. The developed analytical approaches and the autoML framework serve as a concrete basis for analyzing a broad class of artificial neural networks and developing novel hardware substrates that can optimally support the applications. The BitBench suite, the first publicly-available suite of kernels and applications for evaluating bitstream computing substrates, will streamline development and comparative assessment of future novel approaches for such computing substrates. The BitSAD domain-specific language and optimizing compiler will similarly streamline the development and deployment of new algorithms , including complex deep neural networks, on these substrates.&nbsp; Ongoing work in development of new learning rules and training regimes for biologically-inspired neural networks, which leverages these tools, will lead to advances that can dramatically improve the utility and performance of such networks in numerous domains, including safety-critical applications where assessment of prediction confidence plays an important role.</p>\n<p><br />Without the kinds of innovations described here, that dramatically alter the design and architecture of future computing substrates, the continued device scaling of future nanometer semiconductor technologies will fail to provide substantial returns in terms of improvements in utility or performance. As a result, the microprocessor industry, and by extension, the computer industry as a whole, faces serious challenges in maintaining the growth-based business model that has sustained it for four decades. This research has had broad industry- and economy-wide impact by helping to address or avert these impending challenges, while effectively training graduate students in the technical skills required to accomplish these goals, and leading to technology transfer via internships and permanent employment.<br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/13/2022<br>\n\t\t\t\t\tModified by: Mikko&nbsp;H&nbsp;Lipasti</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project investigated a range of algorithms that are suitable for deployment on the Gigascale Analog Iterative Network Solver (GAINS), a system architecture that closely couples an analog computation substrate with crosspoint resistive memory arrays in a massively parallel, scalable design that is well suited for solving optimization and linear algebra problems with very large data sets.  The project also investigated deploying these algorithms on a near-term substrate realizable with conventional CMOS logic, demonstrating many of the algorithmic and hardware advantages of GAINS without waiting for the availability of novel, hence high-risk, resistive memory hardware. To increase researcher?s productivity and broaden the appeal of these novel approaches, a new mechanism was developed to make it compatible with existing popular domain-specific programming frameworks such as Pytorch, as well as a new domain-specific programming language, BitSAD, was invented, coupled with an optimizing compiler that streamlined algorithm development, simulation, debugging, and rapid prototyping of hardware substrates.  Two releases of this compiler, along with the first benchmark suite for this class of algorithms, were released under an open-source license. This research has substantially advanced the design and co-design of algorithms and computation substrates which are capable of tackling optimization, linear algebra, and machine learning applications, providing designs with much higher levels of performance and energy efficiency than existing, conventional approaches.\n\n\nThe research conducted during this project led to new approaches for deploying complex numerical algorithms on energy-efficient, scalable, reduced-precision substrates.  For example, the work conducted to date has led to the first mathematical framework for determine scaling factors and error bounds when deploying a recurrent numerical solver on limited precision neural hardware. The developed analytical approaches and the autoML framework serve as a concrete basis for analyzing a broad class of artificial neural networks and developing novel hardware substrates that can optimally support the applications. The BitBench suite, the first publicly-available suite of kernels and applications for evaluating bitstream computing substrates, will streamline development and comparative assessment of future novel approaches for such computing substrates. The BitSAD domain-specific language and optimizing compiler will similarly streamline the development and deployment of new algorithms , including complex deep neural networks, on these substrates.  Ongoing work in development of new learning rules and training regimes for biologically-inspired neural networks, which leverages these tools, will lead to advances that can dramatically improve the utility and performance of such networks in numerous domains, including safety-critical applications where assessment of prediction confidence plays an important role.\n\n\nWithout the kinds of innovations described here, that dramatically alter the design and architecture of future computing substrates, the continued device scaling of future nanometer semiconductor technologies will fail to provide substantial returns in terms of improvements in utility or performance. As a result, the microprocessor industry, and by extension, the computer industry as a whole, faces serious challenges in maintaining the growth-based business model that has sustained it for four decades. This research has had broad industry- and economy-wide impact by helping to address or avert these impending challenges, while effectively training graduate students in the technical skills required to accomplish these goals, and leading to technology transfer via internships and permanent employment.\n\n\n\n\t\t\t\t\tLast Modified: 01/13/2022\n\n\t\t\t\t\tSubmitted by: Mikko H Lipasti"
 }
}