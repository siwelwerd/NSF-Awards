{
 "awd_id": "1564055",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TWC: Medium: Collaborative: Efficient Repair of Learning Systems via Machine Unlearning",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 600068.0,
 "awd_amount": 600068.0,
 "awd_min_amd_letter_date": "2016-03-01",
 "awd_max_amd_letter_date": "2016-03-01",
 "awd_abstract_narration": "Today individuals and organizations leverage machine learning systems to adjust room temperature, provide recommendations, detect malware, predict earthquakes, forecast weather, maneuver vehicles, and turn Big Data into insights. Unfortunately, these systems are prone to a variety of malicious attacks with potentially disastrous consequences. For example, an attacker might trick an Intrusion Detection System into ignoring the warning signs of a future attack by injecting carefully crafted samples into the training set for the machine learning model (i.e., \"polluting\" the model). This project is creating an approach to machine unlearning and the necessary algorithms, techniques, and systems to efficiently and effectively repair a learning system after it has been compromised. Machine unlearning provides a last resort against various attacks on learning systems, and is complementary to other existing defenses.  \r\n\r\nThe key insight in machine unlearning is that most learning systems can be converted into a form that can be updated incrementally without costly retraining from scratch. For instance, several common learning techniques (e.g., naive Bayesian classifier) can be converted to the non-adaptive statistical query learning form, which depends only on a constant number of summations, each of which is a sum of some efficiently computable transformation of the training data samples. To repair a compromised learning system in this form, operators add or remove the affected training sample and re-compute the trained model by updating a constant number of summations. This approach yields huge speedup -- the asymptotic speedup over retraining is equal to the size of the training set. With unlearning, operators can efficiently correct a polluted learning system by removing the injected sample from the training set, strengthen an evaded learning system by adding evasive samples to the training set, and prevent system inference attacks by forgetting samples stolen by the attacker so that no future attacks can infer anything about the samples.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Junfeng",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Junfeng Yang",
   "pi_email_addr": "junfeng@cs.columbia.edu",
   "nsf_id": "000509381",
   "pi_start_date": "2016-03-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277922",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 600068.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-a5d8a1ed-7fff-098e-7f39-754db776c073\"> </span></p>\n<p dir=\"ltr\"><span>The major outcomes of the award include a blooming new field, called &ldquo;Machine Unlearning&rdquo;. The start of the project is general machine unlearning upon traditional machine learning algorithms, such as Support Vector Machine (SVM) and Naive Bayes.&nbsp; Then, the PIs developed causal unlearning to trace back machine learning results to specific training samples and the also applied unlearning to different problems like autonomous vehicles.&nbsp; After that, the field starts to bloom together with deep learning (including deep neural networks).&nbsp; Until today, at the time of the final report, the field of machine unlearning under the help of multiple researchers has involved multiple different areas, such as federated unlearning, few-shot unlearning, deep unlearning, private unlearning, and graph unlearning.&nbsp;&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The PIs believe that the project has broad impacts on different aspects of people&rsquo;s life, such as self-driving cars, medical domains, and privacy.&nbsp; First, in the PIs&rsquo; SOSP 2017 best papers, the PIs demonstrate how Machine Learning models may misbehave when being used in autonomous vehicles, such as self-driving cars.&nbsp; Second, the PIs show that privacy issues may exist in learning models when being applied in the medical domain (e.g., retinal diseases and Lyme disease).&nbsp; Lastly, the PIs also show that machine unlearning (particularly forgetting) is tightly related to privacy policies, like GDRP and CCPA. Such a strong connection will help lawmakers to better design their policies. Third, the methods, techniques, concepts, datasets, and open-source software invented in this project lead to more secure and privacy-preserving machine learning.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/01/2022<br>\n\t\t\t\t\tModified by: Junfeng&nbsp;Yang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe major outcomes of the award include a blooming new field, called \"Machine Unlearning\". The start of the project is general machine unlearning upon traditional machine learning algorithms, such as Support Vector Machine (SVM) and Naive Bayes.  Then, the PIs developed causal unlearning to trace back machine learning results to specific training samples and the also applied unlearning to different problems like autonomous vehicles.  After that, the field starts to bloom together with deep learning (including deep neural networks).  Until today, at the time of the final report, the field of machine unlearning under the help of multiple researchers has involved multiple different areas, such as federated unlearning, few-shot unlearning, deep unlearning, private unlearning, and graph unlearning.  \n\n \nThe PIs believe that the project has broad impacts on different aspects of people\u2019s life, such as self-driving cars, medical domains, and privacy.  First, in the PIs\u2019 SOSP 2017 best papers, the PIs demonstrate how Machine Learning models may misbehave when being used in autonomous vehicles, such as self-driving cars.  Second, the PIs show that privacy issues may exist in learning models when being applied in the medical domain (e.g., retinal diseases and Lyme disease).  Lastly, the PIs also show that machine unlearning (particularly forgetting) is tightly related to privacy policies, like GDRP and CCPA. Such a strong connection will help lawmakers to better design their policies. Third, the methods, techniques, concepts, datasets, and open-source software invented in this project lead to more secure and privacy-preserving machine learning.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 09/01/2022\n\n\t\t\t\t\tSubmitted by: Junfeng Yang"
 }
}