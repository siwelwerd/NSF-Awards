{
 "awd_id": "1633387",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Collaborative Research: F: Algorithmic Fairness: A Systemic and Foundational Treatment of Nondiscriminatory Data Mining",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 172742.0,
 "awd_amount": 172742.0,
 "awd_min_amd_letter_date": "2016-08-31",
 "awd_max_amd_letter_date": "2016-08-31",
 "awd_abstract_narration": "Data-driven modeling has moved beyond the realm of consumer predictions and recommendations into areas of policy and planning that have a profound impact on our daily lives. The tools of data analysis are being harnessed to predict crime, select candidates for jobs, identify security threats, determine credit risk, and even decide treatment plans and interventions for patients. Automated learning and mining tools can crunch incredible amounts and variety of data in order to detect patterns and make predictions. As is rapidly becoming clear, these tools can also introduce discriminatory behavior and amplify biases in the systems they are trained on. In this project, the PIs will study the problems of discrimination and bias in algorithmic decision-making. By studying all aspects of the data pipeline (from data preparation to learning, evaluation, and feedback), they will develop tools for analyzing, auditing, and designing automated decision-making systems that will be fair, accountable, and transparent. As specific goals to broaden the impact of this research, the PIs will develop a course curriculum to educate the next generation of data scientists on the ethical, legal, and societal implications of algorithmic decision-making, with the intent that they will then take this understanding into their jobs as they enter the workforce. Initial efforts by the PIs have attracted students from underrepresented groups in computer science, and they will continue these efforts. The PIs will also explore the legal and policy ramifications of this research, and develop best practice guidelines for the use of their tools by policy makers, lawyers, journalists, and other practitioners.\r\n\r\nThe PIs will explore the technical subject of this project in three ways. Firstly, they will develop a sound theoretical framework for reasoning about algorithmic fairness. This framework carefully separates mechanisms, beliefs, and assumptions in order to make explicit implicitly held assumptions about the nature of fairness in learning. Secondly, by examining the entire pipeline of tasks associated with learning, they will identify hitherto unexplored areas where bias may be unintentionally introduced into learning as well as novel problems associated with ensuring fairness. These include the initial stages of data preparation, various kinds of fairness-aware learning, and evaluation. They will also investigate the problem of feedback: when actions based on a biased learned model might cause a feedback loop that changes reality and leads to more bias.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sorelle",
   "pi_last_name": "Friedler",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sorelle Friedler",
   "pi_email_addr": "sfriedle@haverford.edu",
   "nsf_id": "000628873",
   "pi_start_date": "2016-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Haverford College",
  "inst_street_address": "370 W LANCASTER AVE",
  "inst_street_address_2": "",
  "inst_city_name": "HAVERFORD",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "6108961000",
  "inst_zip_code": "190411336",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "PA05",
  "org_lgl_bus_name": "THE CORPORATION OF HAVERFORD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "QAP9NNDEWGZ8"
 },
 "perf_inst": {
  "perf_inst_name": "Haverford College",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "190411336",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "PA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 172742.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-1cc62521-7fff-50a4-64f2-684fd4951853\"> </span></p>\n<p dir=\"ltr\"><span>The work on this award focused on understanding and developing technological interventions to help address the potential for discrimination resulting from algorithmic decision making.&nbsp; It especially focused on these algorithms in the context of the criminal justice system, such as criminal risk assessments and predictive policing systems.&nbsp; This included work that identified an issue with statistical bias in predictive policing systems where police were repeatedly sent back to neighborhoods with previous arrests solely due to those previous arrests and not because of an underlying higher crime rate.&nbsp; Another work created a framework for identifying potential issues with algorithms when deployed in society by understanding these algorithms as existing within a sociotechnical system that requires taking human actions into account when assessing algorithms' efficacy.&nbsp; Overall, the 12 papers published under this award introduced technical fairness definitions and analyses, algorithms to audit algorithmic decision-making systems for bias, and identified issues with feedback loops in predictive policing and risk assessment algorithms.</span></p>\n<p dir=\"ltr\"><span>This award was critical to the broader development of the field of algorithmic fairness as well as the development of the workforce.</span></p>\n<p dir=\"ltr\"><span>Academic and public outreach:&nbsp;&nbsp;</span>During this grant, the PIs helped to co-found the main conference in this area, the ACM Conference on Fairness, Accountability, and Transparency.&nbsp; This conference has had a major impact on the work in computer science and in adjacent fields, serving as a high profile publication venue and growing to 500+ attendees each year.&nbsp; The PIs have also done public outreach on this topic, e.g., via interviews on NPR.&nbsp; Public service has also included participation in a research advisory council for the First Judicial District of Pennsylvania as they consider the implementation of a pretrial risk assessment system.</p>\n<p dir=\"ltr\"><span>Workforce development:&nbsp;&nbsp;</span>Course development work has incorporated fairness concerns, especially in the context of criminal justice, into an introductory programming course. 15 undergraduates have been co-authors on work published for this award, 14 have written undergraduate theses on these topics, and an additional 9 have had the opportunity to do research as part of this work during the summer or academic year.&nbsp; Three of these students are now pursuing a Ph.D. in Computer Science related to this work, while many more have brought their understanding of these issues to their current work in industry.</p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/11/2021<br>\n\t\t\t\t\tModified by: Sorelle&nbsp;Friedler</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe work on this award focused on understanding and developing technological interventions to help address the potential for discrimination resulting from algorithmic decision making.  It especially focused on these algorithms in the context of the criminal justice system, such as criminal risk assessments and predictive policing systems.  This included work that identified an issue with statistical bias in predictive policing systems where police were repeatedly sent back to neighborhoods with previous arrests solely due to those previous arrests and not because of an underlying higher crime rate.  Another work created a framework for identifying potential issues with algorithms when deployed in society by understanding these algorithms as existing within a sociotechnical system that requires taking human actions into account when assessing algorithms' efficacy.  Overall, the 12 papers published under this award introduced technical fairness definitions and analyses, algorithms to audit algorithmic decision-making systems for bias, and identified issues with feedback loops in predictive policing and risk assessment algorithms.\nThis award was critical to the broader development of the field of algorithmic fairness as well as the development of the workforce.\nAcademic and public outreach:  During this grant, the PIs helped to co-found the main conference in this area, the ACM Conference on Fairness, Accountability, and Transparency.  This conference has had a major impact on the work in computer science and in adjacent fields, serving as a high profile publication venue and growing to 500+ attendees each year.  The PIs have also done public outreach on this topic, e.g., via interviews on NPR.  Public service has also included participation in a research advisory council for the First Judicial District of Pennsylvania as they consider the implementation of a pretrial risk assessment system.\nWorkforce development:  Course development work has incorporated fairness concerns, especially in the context of criminal justice, into an introductory programming course. 15 undergraduates have been co-authors on work published for this award, 14 have written undergraduate theses on these topics, and an additional 9 have had the opportunity to do research as part of this work during the summer or academic year.  Three of these students are now pursuing a Ph.D. in Computer Science related to this work, while many more have brought their understanding of these issues to their current work in industry.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 02/11/2021\n\n\t\t\t\t\tSubmitted by: Sorelle Friedler"
 }
}