{
 "awd_id": "1618061",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Concatenative Resynthesis for Very High Quality Speech Enhancement",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2016-06-15",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 449958.0,
 "awd_amount": 457958.0,
 "awd_min_amd_letter_date": "2016-06-17",
 "awd_max_amd_letter_date": "2017-07-03",
 "awd_abstract_narration": "Environmental noise is one of the largest problem for users of voice technologies, such as hearing aids, mobile phones, and automatic speech recognition. Current approaches to source separation and speech enhancement typically attempt to modify the noisy signal in order to make it more like the original, leading to distortions in target speech and residual noise. In contrast, this project uses the innovative approach of driving a speech synthesizer using information extracted from the noisy signal to create a brand new, high quality, noise-free version of the original sentence.  Improvements in noise suppression and speech quality from this approach are expected to have important broader impacts for both the 36 million Americans who are hearing impaired and the 200 million Americans who use smart phones. The project is also being incorporated into the curriculum in a diverse urban college and into established outreach programs to nearby high schools with the goal of encouraging members of under-represented groups to pursue careers in science and engineering.\r\n\r\nThis project aims to produce a high quality speech resynthesis system by modifying a concatenative speech synthesizer to use a unit-selection function based on a novel deep neural network (DNN) architecture. Preliminary results have shown this approach to work well for a small-vocabulary, speaker-dependent task, and the current project expands this to the large-vocabulary, speaker-dependent setting in three ways. First, it seeks to improve the intelligibility of the synthesized speech by utilizing perceptually motivated input features, more flexible training signals, and traditional speech enhancement. Second, it seeks to improve the system's scalability by training DNNs to embed noisy and clean speech into a joint low-dimensional space in which similarity can be efficiently computed. And third, it seeks to improve the quality of the synthesized speech by incorporating sequential models of speech units based on acoustic, phonetic, and linguistic compatibility. The use of speech synthesis models in speech enhancement is a departure from traditional approaches and has the potential to make a transformative impact on the quality of enhanced speech.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Mandel",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Michael I Mandel",
   "pi_email_addr": "mim@sci.brooklyn.cuny.edu",
   "nsf_id": "000634756",
   "pi_start_date": "2016-06-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "CUNY Brooklyn College",
  "inst_street_address": "2900 BEDFORD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BROOKLYN",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "7189515622",
  "inst_zip_code": "112102850",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "NY09",
  "org_lgl_bus_name": "RESEARCH FOUNDATION OF THE CITY UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "XNAKYW3FTSE1"
 },
 "perf_inst": {
  "perf_inst_name": "CUNY Brooklyn College",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "112102889",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "NY09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 449958.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-ff5c38de-7fff-6f7d-b78c-f32d1d6e1f7c\"> </span></p>\n<p dir=\"ltr\">The presence of noise during a speech utterance of interest is incredibly common, but diminishes both the quality and intelligibility of the speech as well as increasing the effort required of listeners to understand it. Traditional approaches to speech enhancement and source separation attempt to mitigate these issues by modifying a noisy signal to make it sound more like the original. These modifications, however, lead to distortions, both from over-suppression of the target speech and from under-suppression of the interfering noise. This project started from the hypothesis that higher quality speech enhancement could be produced by resynthesizing clean signals that convey the same information rather than attempting to filter out the noise.</p>\n<p dir=\"ltr\"><span>During the course of this project, several key technological advances were made towards the goal of enhancement-by-synthesis. These advances include several systems performing enhancement via concatenative synthesis, which is high quality, but data intensive, and several others performing enhancement via parametric synthesis, which is more scalable to new speakers and conditions. At the beginning of the project, concatenative synthesis was the highest quality method available, but during the course of the project, high quality parametric models were developed in the research community, thus this project transitioned to focusing on them.</span></p>\n<p dir=\"ltr\"><span>A crucial component of concatenative enhancement-by-synthesis is a method for matching noisy segments of speech to a dictionary of clean segments of speech and a means of sequencing compatible clean segments. Key outcomes in enabling concatenative enhancement-by-synthesis included the development of a scalable matching method using a deep learning architecture called siamese networks and algorithms for efficiently finding approximate nearest neighbors as well as the development of a more robust matching method that incorporated phonetic and prosodic information in its training process.</span></p>\n<p dir=\"ltr\"><span>Key outcomes in enabling parametric enhancement-by-synthesis included introducing the method using traditional speech synthesis components called vocoders, then improving the quality of the output by transitioning to newly introduced high quality neural vocoders trained on the speech of a single speaker, and finally demonstrating that when using a neural vocoder trained on the speech of many speakers, parametric enhancement-by-synthesis could generalize to unseen speakers.</span></p>\n<p dir=\"ltr\"><span>In terms of broader impacts, the primary outcome has been in advancing the state of the art in very high quality speech enhancement. At the time this method was introduced, there were no methods for producing very high quality outputs from speech enhancement systems and there was little cross-pollination between the fields of speech enhancement and synthesis. This project helped introduce the idea into the research community that the output of a speech enhancement system could be as high quality as a speech synthesizer and demonstrated that such a goal was attainable using methods from both fields. Subsequent work in speech enhancement has continued to build upon this idea, including commercial products like Adobe's Podcast Enhance, and it can inform the development of many speech processing systems like telecommunications, professional audio, and hearing aids. The project led to one patent application and the founding of a company, Vocal Clarity, Inc., by the PI and one of the PhD students working on this project among a gender-diverse leadership team that is working to commercialize this patent in the film and television industries.</span></p>\n<p dir=\"ltr\"><span>In terms of dissemination, this project has produced 7 published conference papers, 1 journal paper, 1 PhD dissertation during the reporting period, and 2 PhD dissertations after the reporting period ended. The PI has given presentations to executives from Fortune 500 media companies at the NYC MediaLab&rsquo;s Machines+Media and SyntheticMedia working groups as well as to the Google Sound Understanding Group.</span></p>\n<p dir=\"ltr\"><span>The project provided training opportunities for 5 PhD students (1 woman), 5 undergraduates (2 women), and 1 Masters student. Many of the students who were initially on the project have since graduated and are making significant contributions to the field of computer science and related areas. The PI incorporated a lecture based on material from this project into his classes at the undergraduate, masters, and doctoral levels. He has also given talks about the project at the CUNY Summer Undergraduate Research Program in 2017 and 2019.</span></p>\n<p dir=\"ltr\"><span>By utilizing elements of speech synthesis in speech enhancement, this project has demonstrated that very high quality speech enhancement is possible, advancing the field and the research community scientifically, commercially, and more broadly.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/18/2023<br>\n\t\t\t\t\tModified by: Michael&nbsp;I&nbsp;Mandel</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe presence of noise during a speech utterance of interest is incredibly common, but diminishes both the quality and intelligibility of the speech as well as increasing the effort required of listeners to understand it. Traditional approaches to speech enhancement and source separation attempt to mitigate these issues by modifying a noisy signal to make it sound more like the original. These modifications, however, lead to distortions, both from over-suppression of the target speech and from under-suppression of the interfering noise. This project started from the hypothesis that higher quality speech enhancement could be produced by resynthesizing clean signals that convey the same information rather than attempting to filter out the noise.\nDuring the course of this project, several key technological advances were made towards the goal of enhancement-by-synthesis. These advances include several systems performing enhancement via concatenative synthesis, which is high quality, but data intensive, and several others performing enhancement via parametric synthesis, which is more scalable to new speakers and conditions. At the beginning of the project, concatenative synthesis was the highest quality method available, but during the course of the project, high quality parametric models were developed in the research community, thus this project transitioned to focusing on them.\nA crucial component of concatenative enhancement-by-synthesis is a method for matching noisy segments of speech to a dictionary of clean segments of speech and a means of sequencing compatible clean segments. Key outcomes in enabling concatenative enhancement-by-synthesis included the development of a scalable matching method using a deep learning architecture called siamese networks and algorithms for efficiently finding approximate nearest neighbors as well as the development of a more robust matching method that incorporated phonetic and prosodic information in its training process.\nKey outcomes in enabling parametric enhancement-by-synthesis included introducing the method using traditional speech synthesis components called vocoders, then improving the quality of the output by transitioning to newly introduced high quality neural vocoders trained on the speech of a single speaker, and finally demonstrating that when using a neural vocoder trained on the speech of many speakers, parametric enhancement-by-synthesis could generalize to unseen speakers.\nIn terms of broader impacts, the primary outcome has been in advancing the state of the art in very high quality speech enhancement. At the time this method was introduced, there were no methods for producing very high quality outputs from speech enhancement systems and there was little cross-pollination between the fields of speech enhancement and synthesis. This project helped introduce the idea into the research community that the output of a speech enhancement system could be as high quality as a speech synthesizer and demonstrated that such a goal was attainable using methods from both fields. Subsequent work in speech enhancement has continued to build upon this idea, including commercial products like Adobe's Podcast Enhance, and it can inform the development of many speech processing systems like telecommunications, professional audio, and hearing aids. The project led to one patent application and the founding of a company, Vocal Clarity, Inc., by the PI and one of the PhD students working on this project among a gender-diverse leadership team that is working to commercialize this patent in the film and television industries.\nIn terms of dissemination, this project has produced 7 published conference papers, 1 journal paper, 1 PhD dissertation during the reporting period, and 2 PhD dissertations after the reporting period ended. The PI has given presentations to executives from Fortune 500 media companies at the NYC MediaLab\u2019s Machines+Media and SyntheticMedia working groups as well as to the Google Sound Understanding Group.\nThe project provided training opportunities for 5 PhD students (1 woman), 5 undergraduates (2 women), and 1 Masters student. Many of the students who were initially on the project have since graduated and are making significant contributions to the field of computer science and related areas. The PI incorporated a lecture based on material from this project into his classes at the undergraduate, masters, and doctoral levels. He has also given talks about the project at the CUNY Summer Undergraduate Research Program in 2017 and 2019.\nBy utilizing elements of speech synthesis in speech enhancement, this project has demonstrated that very high quality speech enhancement is possible, advancing the field and the research community scientifically, commercially, and more broadly.\n\n \n\n\t\t\t\t\tLast Modified: 02/18/2023\n\n\t\t\t\t\tSubmitted by: Michael I Mandel"
 }
}