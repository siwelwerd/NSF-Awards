{
 "awd_id": "1629564",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: FULL: Collaborative Research: Parallel and Distributed Circuit Programming for Structured Prediction",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927885",
 "po_email": "abanerje@nsf.gov",
 "po_sign_block_name": "Anindya Banerjee",
 "awd_eff_date": "2016-08-15",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 415000.0,
 "awd_amount": 415000.0,
 "awd_min_amd_letter_date": "2016-08-11",
 "awd_max_amd_letter_date": "2016-08-11",
 "awd_abstract_narration": "This project develops a system for \"circuit programming,\" which allows a programmer to focus on the high-level solution to a problem rather than on the details of how the computation is organized. Circuit programming consists of writing rules that describe how data items depend on one another. The intellectual merits lie in the design of a new programming language for specifying these rules, along with the algorithms whereby the computer automatically finds efficient strategies for managing the necessary computations on available parallel hardware.  The project's broader significance and importance lie in its potential to streamline work in areas such as artificial intelligence and machine learning.  With the growing complexity of systems in these areas and their need to process big data in depth, research and teaching typically get bogged down in programming details, especially for parallel platforms; this project aims to delegate those details to automatic methods.\r\n\r\nThe research develops a programming system for Dyna, a circuit programming language that enables concise specification of large function graphs that may be cyclic and/or infinite. Dyna employs (1) a pattern-matching notation that augments pure Prolog with evaluation and aggregation and (2) an object-like mechanism for dynamically defining new sub-circuits as modifications of old ones.  This project is building an adaptive system that can mix forward and backward chaining to seek a fixpoint of the circuit and to update this fixpoint as the inputs change.  The system will perform compile-time and runtime analysis of the Dyna program and will map it to Habanero, a system for scheduling parallel computations on multicore processors, with extensions for task priorities, task cancellation, GPU execution, and distributed execution.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Eisner",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Jason M Eisner",
   "pi_email_addr": "jason@cs.jhu.edu",
   "nsf_id": "000185365",
   "pi_start_date": "2016-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N. Charles St.",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182682",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 415000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>INTELLECTUAL MERIT:<br />We designed new algorithms for executing and optimizing code written in the declarative Dyna programming language, which extends logic programming with evaluation and aggregation.&nbsp; We designed and built three implementations of Dyna, based respectively on forward chaining, non-ground mixed chaining, and term rewriting, and investigated how to connect them to parallel programming abstractions developed by our collaborators at Georgia Tech and Rice.&nbsp; We also designed and implemented systems for static type analysis and runtime analysis of Dyna programs, and a system that searches for optimizations of Dyna programs in the form of source-to-source transformations, often finding optimizations that were missed in manually written expert algorithms.&nbsp; We further investigated opportunities for adaptive optimization via dynamic code generation and reinforcement learning.&nbsp; Finally, as an application to dynamic neural networks (trained by back-propagation), we developed a flexible formalism that harnesses logic programming notation and infrastructure to allow the specification of informed neural-symbolic models of how real-world facts and events and their representations evolve over time.</p>\n<p><br />BROADER IMPACTS:<br />4 Ph.D. students at Johns Hopkins gained experience in logic programming, static analysis and optimization, programming language design and implementation, parallel programming, reinforcement learning, and neural-symbolic modeling.&nbsp; All 4 students will complete their dissertations using work done under this award; 2 of them have already done so.&nbsp; The methods and software developed under this award will be used to support future users of Dyna, including researchers in data science and artificial intelligence.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/02/2021<br>\n\t\t\t\t\tModified by: Jason&nbsp;M&nbsp;Eisner</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nINTELLECTUAL MERIT:\nWe designed new algorithms for executing and optimizing code written in the declarative Dyna programming language, which extends logic programming with evaluation and aggregation.  We designed and built three implementations of Dyna, based respectively on forward chaining, non-ground mixed chaining, and term rewriting, and investigated how to connect them to parallel programming abstractions developed by our collaborators at Georgia Tech and Rice.  We also designed and implemented systems for static type analysis and runtime analysis of Dyna programs, and a system that searches for optimizations of Dyna programs in the form of source-to-source transformations, often finding optimizations that were missed in manually written expert algorithms.  We further investigated opportunities for adaptive optimization via dynamic code generation and reinforcement learning.  Finally, as an application to dynamic neural networks (trained by back-propagation), we developed a flexible formalism that harnesses logic programming notation and infrastructure to allow the specification of informed neural-symbolic models of how real-world facts and events and their representations evolve over time.\n\n\nBROADER IMPACTS:\n4 Ph.D. students at Johns Hopkins gained experience in logic programming, static analysis and optimization, programming language design and implementation, parallel programming, reinforcement learning, and neural-symbolic modeling.  All 4 students will complete their dissertations using work done under this award; 2 of them have already done so.  The methods and software developed under this award will be used to support future users of Dyna, including researchers in data science and artificial intelligence.\n\n \n\n\t\t\t\t\tLast Modified: 12/02/2021\n\n\t\t\t\t\tSubmitted by: Jason M Eisner"
 }
}