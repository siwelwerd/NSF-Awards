{
 "awd_id": "1551590",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "INT: Collaborative Research: Detecting, Predicting and Remediating Student Affect and Grit Using Computer Vision",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925126",
 "po_email": "abaylor@nsf.gov",
 "po_sign_block_name": "Amy Baylor",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 134960.0,
 "awd_amount": 166720.0,
 "awd_min_amd_letter_date": "2016-08-26",
 "awd_max_amd_letter_date": "2019-05-16",
 "awd_abstract_narration": "The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments.  Integration (INT) projects refine and study emerging genres of learning technologies that have already undergone several years of iterative refinement in the context of rigorous research on how people learn with such technologies; INT projects contribute to our understanding of how the prototype tools might generalize to a larger category of learning technologies. This INT project integrates prior work from two well-developed NSF-sponsored projects on (i) advanced computer vision and (ii) affect detection in intelligent tutoring systems.  The latter work in particular developed instruments to detect student emotion (interest, confusion, frustration and boredom) and showed that when a computer tutor responded to negative student affect, learning performance improved. The current project will expand this focus beyond emotion to attempt to also detect persistence, self-efficacy, and the trait called 'grit.' The project will measure the impact of these constructs on student learning and explore whether the grit trait (a persistent tendency towards sustained initiative and interest) can be improved and whether and how it depends on other recently experienced emotions. The technological innovation enabling this research into the genre of broadly affectively aware instruction is Smartutors, a tool that uses advanced computer vision techniques to view a student's gaze, hand gestures, head, and face to increase the \"bandwidth\" for automatically detecting their affect. One goal is to reorient students to more productive attitudes once waning attention is recognized.\r\n\r\nThis research team brings together a unique blend of leading interdisciplinary researchers in computer vision; adaptive education technology and computer science; mathematics education; learning companions; and meta-cognition, emotion, self-efficacy and motivation. Nine experiments will provide valuable data to extend and validate existing models of grit and emotion. In particular, the team will gather fine-grained data on grit, assess the impact of tutor interventions in real-time, and contribute thereby to a theory of grit. Visual data of student behavior will be integrated with advanced analytics of log data of students' actions based on the behavior of over 10,000 prior students (e.g., hint requests, topic mastery) to provide individualized guidance and tutor responses in a timely fashion. This will allow the researchers to measure the impact of interventions on student performance and attitude, and it will uncover how grit levels relate to emotion and what impact emotions and grit combined have on overall student initiative. By identifying interventions that are sensitive to individual differences, this research will refine theories of motivation and emotion and will reveal principles about how to respond to student grit and affect, especially when attention and persistence begin to wane. To ensure classroom success, the PIs will evaluate Smartutors with 1,600 students and explore its transferability by testing it in a more difficult mathematics domain\u00a0with older students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Magee",
   "pi_mid_init": "J",
   "pi_sufx_name": "IV",
   "pi_full_name": "John J Magee",
   "pi_email_addr": "jmagee@clarku.edu",
   "nsf_id": "000662076",
   "pi_start_date": "2016-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Clark University",
  "inst_street_address": "950 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "WORCESTER",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "5084213835",
  "inst_zip_code": "016101400",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "TRUSTEES OF CLARK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "LD3WUVEUK2N5"
 },
 "perf_inst": {
  "perf_inst_name": "CLARK UNIVERSITY",
  "perf_str_addr": "950 MAIN ST",
  "perf_city_name": "Worcester",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "016101400",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "8233",
   "pgm_ref_txt": "Integration Projects (INT)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 134960.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 31760.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-fe203161-7fff-03e4-5802-b95de8cfe45c\"> </span></p>\n<p dir=\"ltr\"><span>We showed that detection of affect and \"state of grit\" in intelligent tutors is a promising avenue for improving online learning and education. The state of grit entails working to overcome challenges and maintaining effort over time despite failures. We used computer vision techniques to analyze students? faces and gestures and to develop methods to respond to their perceived engagement. The facial expression detection mechanisms worked in real-time so that the online system both recognized grit and responded to it.</span></p>\n<p>Predicting whether students have trouble with problems supports online systems to provide interventions, such as hints or encouragement. We detected students? grit state in real-time; investigated which interventions were effective when students? initiative began to fade; and contributed to theories of affect and motivation. These studies were conducted in the context of MathSpring.org, an intelligent tutor that supports practice with Common Core mathematics problems online.&nbsp;&nbsp;</p>\n<p>In order to measure grit, we first created videos of online student engagement. While students work online, they demonstrated various levels of engagement and emotions (e.g., confusion, boredom, excitement). Having such information automatically accessible to teachers aids them to understand students? progress, suggesting when and who needs further assistance.</p>\n<p>We classified facial expressivity to predict student behavior. Specifically, we developed automated facial expression detectors that differentiated between grit and its absence, relatively early (e.g., a few seconds into the learning session). Accuracy in detecting grit grew with the number of problems attempted by each student. The most accurate detectors of grit were based on a combination of machine learning models and human-engineered factors. The system could personalize its interventions, such as hints and encouragement.&nbsp;</p>\n<p>The computer vision system predicted student behavior based on students? facial expression, seconds before students responded or answers were made. It made predictions with relatively high accuracy using only several seconds of video footage, well before students had solved the mathematics problems.&nbsp;</p>\n<p>We also developed Affective Teacher Tools, including a report card that presents measures of students? engagement and a live affective states Dashboard which senses students? affect and performance. We designed several prototypes of these tools. We produced both a student ?Emotion Chart? and an ?Effort Chart? that visualized students? subjective self-reports of their frustration, excitement, interest, and confidence while they solved mathematical problems.&nbsp;</p>\n<p>Our facial expression recognition software detected students' engagement and emotional expressions and inferred student disengagement. The detector achieved 97.94% accuracy as compared with human annotations and earlier system predictions. By analyzing the estimation results of Head Pose Detector on MathSpring videos, we further found invaluable information in the juxtaposition of head position and the logs of problem-solving activity. For example, it appears that a ?head tilt\" is a sign of concentration and cognitive engagement. We showed that finetuning of the affect network with age-appropriate images and video further improves performance in this scenario.&nbsp;</p>\n<p>We manually created our own \"ground truth\" about recognizing student engagement by examining multiple videos to determine the angles for each student?s head position while they solved problems or were distracted. The machine learning model automated this process.</p>\n<p>Finally, we collected and made public a video dataset of nearly three thousand frames of college students solving math problems (https://www.cs.bu.edu/faculty/betke/research/learning/).&nbsp;</p>\n<p>We also worked in Latin America with a version of the mathematics system translated into Spanish. This data set contains over 35 hours of facial expressions of 11-year-old children using MathSpring to practice math problem solving as part of their regular mathematics classes in either Spanish speaking or bilingual schools.&nbsp;</p>\n<p>We collected an experimental dataset with participants middle-school aged participants from Worcester and Amherst Massachusetts. The dataset will be used to evaluate the affective tutor and identify future directions for this work.</p>\n<p><span id=\"docs-internal-guid-fe203161-7fff-03e4-5802-b95de8cfe45c\"> </span></p>\n<p dir=\"ltr\"><span>In sum, we showed that an intelligent tutor can estimate the real-time gaze of students and respond to loss of grit, and consequently disengagement. We produced and tested Teacher Tools that identified students' affective states and provided enhanced information to teachers. Previous work has shown that real time signals from students can be used to improve learning.&nbsp;Our publications summarized both the report card and the affective dashboard, the research studies, results, the implications, and future planned experiments. These results pave the way for future improvements for this task and future tutor systems may use our outcome prediction models to deliver real-time interventions to improve students? learning.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2022<br>\n\t\t\t\t\tModified by: John&nbsp;Magee</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nWe showed that detection of affect and \"state of grit\" in intelligent tutors is a promising avenue for improving online learning and education. The state of grit entails working to overcome challenges and maintaining effort over time despite failures. We used computer vision techniques to analyze students? faces and gestures and to develop methods to respond to their perceived engagement. The facial expression detection mechanisms worked in real-time so that the online system both recognized grit and responded to it.\n\nPredicting whether students have trouble with problems supports online systems to provide interventions, such as hints or encouragement. We detected students? grit state in real-time; investigated which interventions were effective when students? initiative began to fade; and contributed to theories of affect and motivation. These studies were conducted in the context of MathSpring.org, an intelligent tutor that supports practice with Common Core mathematics problems online.  \n\nIn order to measure grit, we first created videos of online student engagement. While students work online, they demonstrated various levels of engagement and emotions (e.g., confusion, boredom, excitement). Having such information automatically accessible to teachers aids them to understand students? progress, suggesting when and who needs further assistance.\n\nWe classified facial expressivity to predict student behavior. Specifically, we developed automated facial expression detectors that differentiated between grit and its absence, relatively early (e.g., a few seconds into the learning session). Accuracy in detecting grit grew with the number of problems attempted by each student. The most accurate detectors of grit were based on a combination of machine learning models and human-engineered factors. The system could personalize its interventions, such as hints and encouragement. \n\nThe computer vision system predicted student behavior based on students? facial expression, seconds before students responded or answers were made. It made predictions with relatively high accuracy using only several seconds of video footage, well before students had solved the mathematics problems. \n\nWe also developed Affective Teacher Tools, including a report card that presents measures of students? engagement and a live affective states Dashboard which senses students? affect and performance. We designed several prototypes of these tools. We produced both a student ?Emotion Chart? and an ?Effort Chart? that visualized students? subjective self-reports of their frustration, excitement, interest, and confidence while they solved mathematical problems. \n\nOur facial expression recognition software detected students' engagement and emotional expressions and inferred student disengagement. The detector achieved 97.94% accuracy as compared with human annotations and earlier system predictions. By analyzing the estimation results of Head Pose Detector on MathSpring videos, we further found invaluable information in the juxtaposition of head position and the logs of problem-solving activity. For example, it appears that a ?head tilt\" is a sign of concentration and cognitive engagement. We showed that finetuning of the affect network with age-appropriate images and video further improves performance in this scenario. \n\nWe manually created our own \"ground truth\" about recognizing student engagement by examining multiple videos to determine the angles for each student?s head position while they solved problems or were distracted. The machine learning model automated this process.\n\nFinally, we collected and made public a video dataset of nearly three thousand frames of college students solving math problems (https://www.cs.bu.edu/faculty/betke/research/learning/). \n\nWe also worked in Latin America with a version of the mathematics system translated into Spanish. This data set contains over 35 hours of facial expressions of 11-year-old children using MathSpring to practice math problem solving as part of their regular mathematics classes in either Spanish speaking or bilingual schools. \n\nWe collected an experimental dataset with participants middle-school aged participants from Worcester and Amherst Massachusetts. The dataset will be used to evaluate the affective tutor and identify future directions for this work.\n\n \nIn sum, we showed that an intelligent tutor can estimate the real-time gaze of students and respond to loss of grit, and consequently disengagement. We produced and tested Teacher Tools that identified students' affective states and provided enhanced information to teachers. Previous work has shown that real time signals from students can be used to improve learning. Our publications summarized both the report card and the affective dashboard, the research studies, results, the implications, and future planned experiments. These results pave the way for future improvements for this task and future tutor systems may use our outcome prediction models to deliver real-time interventions to improve students? learning.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/30/2022\n\n\t\t\t\t\tSubmitted by: John Magee"
 }
}