{
 "awd_id": "1622515",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SCH: INT: Collaborative Research: Computer Guided Laparoscopy Training",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922568",
 "po_email": "wnilsen@nsf.gov",
 "po_sign_block_name": "Wendy Nilsen",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 769382.0,
 "awd_amount": 769382.0,
 "awd_min_amd_letter_date": "2016-08-01",
 "awd_max_amd_letter_date": "2016-08-01",
 "awd_abstract_narration": "IIS-1622589 SCH: INT: Collaborative Research: Computer Guided Laparoscopy Training\r\n\r\n\r\nLaparoscopic surgery, when performed by a well-trained surgeon, is a remarkably effective procedure that minimizes complications associated with open incisions, blood loss and post-operative pain. It also reduces recovery time. However, the procedure is more challenging than conventional surgery due to restricted vision, hand-eye coordination problems, limited working space, and lack of tactile sensation. Therefore, effective training and guidance methods are needed to minimize the potential risks inherent in such procedures. The goal of this project is to develop and validate techniques for computer-guided laparoscopic surgical training in a simulated, non-patient based environment. A computer-aided surgical trainer (CAST) will physically guide trainees' instruments during surgical skills practice sessions by utilizing assistive force with augmented reality displays. Guided training will be validated through a pilot experimental study, in which the expertise of computer-guided trainees will be compared to that of instructor-guided trainees. Data such as the time it takes a trainee to execute a particular surgical task, how accurate he or she is, etc., will be collected to analyze task performance precisely and objectively. New scientific methods for motion trajectory planning and path following using assistive force and augmented reality techniques will result from this work. It is anticipated that computer-guided practice will speed up learning and reinforce appropriate techniques, ultimately, leading to better surgical outcomes and improved patient safety. The CAST system should serve as a sophisticated, yet still low-cost, training solution for fundamental medical skills training.  \r\n\r\nThe specific objectives are a) to refine and implement a memory- and time-efficient hybrid offline-online optimal path planner for computer-guided training of basic laparoscopic skills. In this task, collision-free trajectory planning methods (such as those used in robotics) will be generated by incorporating offline-online hybrid techniques with memory and computational time efficient path repository. Thus, basic laparoscopic tasks can be planned and guided automatically, using haptic force and augmented reality visualization; b) to design and implement an intelligent, adaptive guidance controller for surgical space navigation, where a fuzzy logic and machine learning-based methods will be developed that will take into account trainees' skill levels so that optimal amount of training assistance can be provided in mastering surgical tasks; c) to design and implement visual guidance techniques through augmented reality overlays that provide 'navigational' cues, supplementing force-based control of surgical instruments; and d) to validate guided training through a pilot study. In this task, trainees' performance using computer guidance methods will be compared, using statistical analysis, to that of unguided trainees. The principal investigators will aim to increase the participation of undergraduate students, and in particular of underrepresented groups, through collaboration with the well-established programs at both PIs'  institutions and through sponsorship of senior projects and independent study courses.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Henry",
   "pi_last_name": "Fuchs",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Henry Fuchs",
   "pi_email_addr": "fuchs@cs.unc.edu",
   "nsf_id": "000451367",
   "pi_start_date": "2016-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Chapel Hill",
  "perf_str_addr": "201 S. Columbia St.",
  "perf_city_name": "Chapel Hill",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275993175",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801800",
   "pgm_ele_name": "Smart and Connected Health"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8018",
   "pgm_ref_txt": "Smart and Connected Health"
  },
  {
   "pgm_ref_code": "8062",
   "pgm_ref_txt": "SCH Type II: INT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 769382.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-7c79374c-7fff-083a-185e-cc38730bb904\">&nbsp;</span></p>\n<p dir=\"ltr\"><span>Laparoscopic surgery is a remarkably effective procedure but is often more complicated to perform than conventional open surgery due to limited visibility of the surgical site and the difficulty in manipulating the laparoscopic surgical instruments. This project improved the capability of laparoscopic surgical training procedures by using Virtual Reality (VR)&nbsp; and Augmented Reality (AR) techniques and devices.&nbsp; These techniques and devices improved the visualization of the surgical site in a computer-aided laparoscopic surgical trainer (CAST) by presenting the surgical trainee a three-dimensional, stereoscopic view of the surgical site, and allowed the view to change naturally as the trainee moved his or her head. This new visualization conveys the three-dimensional situation in the surgical site more effectively than the traditional view, which is simply the two-dimensional video imagery from the laparoscopic camera. In the traditional method, if the surgeon (or surgical trainee) wishes to view the surgical site from a different perspective, he or she has to physically move the camera. With our new VR and AR techniques and devices, the surgeon (or surgical trainee) obtains a visualization that is somewhat akin to viewing the surgical site like that of open surgery, without portions of the patient's anatomy occluding the surgeon's view.</span></p>\n<p dir=\"ltr\"><span>A major technical challenge was acquiring an accurate 3D model of the surgical site and updating that 3D model as the surgical trainee manipulated objects within that site with the laparoscopic instruments. This project focused on a simplified surgical site for training situations, a bed of pegs and small plastic donuts. As part of standard laparoscopic surgical training, these small plastic donuts have to be moved between pegs with the laparoscopic instruments.&nbsp; This laparoscopic peg transfer task is a required part of most laparoscopic training programs because it challenges the surgical trainee to develop fine manipulation skills with the laparoscopic instruments. Since the peg board is stationary, the most important aspect of the 3D model is the manipulation of the small donut using laparoscopic graspers. To ensure the most accurate determination of the donut?s position and orientation, we placed several markers on it that would be visible to the cameras inside the surgical site but not visible to the surgical trainee in the AR or VR headset. We also found that since all but the most expensive cameras do not have synchronization capabilities, the quality of the 3D reconstruction suffers from inaccuracies due to the slight variation in the times of image capture of the different cameras.&nbsp; We developed a correction for this inaccuracy based on both (or all) cameras observing a rapidly changing set of time-coded LEDs. &nbsp; Results of these and other advances have been disseminated in international meetings and in technical publications.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/27/2021<br>\n\t\t\t\t\tModified by: Henry&nbsp;Fuchs</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1622515/1622515_10445560_1635348761136_UNCCSImpactReport2021_10-13-COVER--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1622515/1622515_10445560_1635348761136_UNCCSImpactReport2021_10-13-COVER--rgov-800width.jpg\" title=\"Cover of UNC Chapel Hill Department of Computer Science Fall 2021 Impact Report\"><img src=\"/por/images/Reports/POR/2021/1622515/1622515_10445560_1635348761136_UNCCSImpactReport2021_10-13-COVER--rgov-66x44.jpg\" alt=\"Cover of UNC Chapel Hill Department of Computer Science Fall 2021 Impact Report\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">UNC PhD student Xinran Lu operating experimental laparoscopic training system with Augmented Reality headset</div>\n<div class=\"imageCredit\">UNC Department of Computer Science</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Henry&nbsp;Fuchs</div>\n<div class=\"imageTitle\">Cover of UNC Chapel Hill Department of Computer Science Fall 2021 Impact Report</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nLaparoscopic surgery is a remarkably effective procedure but is often more complicated to perform than conventional open surgery due to limited visibility of the surgical site and the difficulty in manipulating the laparoscopic surgical instruments. This project improved the capability of laparoscopic surgical training procedures by using Virtual Reality (VR)  and Augmented Reality (AR) techniques and devices.  These techniques and devices improved the visualization of the surgical site in a computer-aided laparoscopic surgical trainer (CAST) by presenting the surgical trainee a three-dimensional, stereoscopic view of the surgical site, and allowed the view to change naturally as the trainee moved his or her head. This new visualization conveys the three-dimensional situation in the surgical site more effectively than the traditional view, which is simply the two-dimensional video imagery from the laparoscopic camera. In the traditional method, if the surgeon (or surgical trainee) wishes to view the surgical site from a different perspective, he or she has to physically move the camera. With our new VR and AR techniques and devices, the surgeon (or surgical trainee) obtains a visualization that is somewhat akin to viewing the surgical site like that of open surgery, without portions of the patient's anatomy occluding the surgeon's view.\nA major technical challenge was acquiring an accurate 3D model of the surgical site and updating that 3D model as the surgical trainee manipulated objects within that site with the laparoscopic instruments. This project focused on a simplified surgical site for training situations, a bed of pegs and small plastic donuts. As part of standard laparoscopic surgical training, these small plastic donuts have to be moved between pegs with the laparoscopic instruments.  This laparoscopic peg transfer task is a required part of most laparoscopic training programs because it challenges the surgical trainee to develop fine manipulation skills with the laparoscopic instruments. Since the peg board is stationary, the most important aspect of the 3D model is the manipulation of the small donut using laparoscopic graspers. To ensure the most accurate determination of the donut?s position and orientation, we placed several markers on it that would be visible to the cameras inside the surgical site but not visible to the surgical trainee in the AR or VR headset. We also found that since all but the most expensive cameras do not have synchronization capabilities, the quality of the 3D reconstruction suffers from inaccuracies due to the slight variation in the times of image capture of the different cameras.  We developed a correction for this inaccuracy based on both (or all) cameras observing a rapidly changing set of time-coded LEDs.   Results of these and other advances have been disseminated in international meetings and in technical publications.\n\n\n\n \n\n\t\t\t\t\tLast Modified: 10/27/2021\n\n\t\t\t\t\tSubmitted by: Henry Fuchs"
 }
}