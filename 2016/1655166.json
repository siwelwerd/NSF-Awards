{
 "awd_id": "1655166",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Distributed-Computing: What Does it Compute?",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rahul Shah",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2016-09-06",
 "awd_max_amd_letter_date": "2016-09-06",
 "awd_abstract_narration": "In this project, the PI, Prof. Eli Gafni, proposes to develop guidelines, adapted from the PI's research in area of Theoretical Distributed-Computing, and apply them as guidelines to the area of Distributed-Systems Design. If successful, building distributed systems can become less of an ad-hoc process. It will push the designer to answer questions like ``what-if'' and ``why-not'' akin, but not as rudimentary, to the success of check-lists in the practice of medicine. The PI will collaborate with practitioners to test and tweak the development of these guidelines, as well as collaborate with colleagues of teach software/system-design to test such guidelines in class. This project will also advance workforce development through the training of graduate students, as well as through education activities for undergraduates and graduate students.\r\n\r\nThe project is based on the realization within the Theoretical Distributed-Computing community that ``one-shot tasks are to distributed-computing what functions are to centralized-computing.'' This has contributed to the PI's research immensely by pushing the PI to ask the right questions. The PI feels, as already demonstrated on small scale, that pushing a designer to identify the core tasks the system solved, will not be unlike pushing a centralized system designer to realize that the system designed solves an NP-complete problem. Such realization pushes a designer to expose hidden undocumented assumptions, and expose more avenues the design might have taken, that need to be justified if they are to be abandoned.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eliezer",
   "pi_last_name": "Gafni",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Eliezer M Gafni",
   "pi_email_addr": "eli@cs.ucla.edu",
   "nsf_id": "000316763",
   "pi_start_date": "2016-09-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "UCLA Computer Science Dept.",
  "perf_str_addr": "Box 951596, 3731F Boelter Hall",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951596",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7934",
   "pgm_ref_txt": "PARAL/DISTRIBUTED ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Distributed Algorithms:</p>\n<p>Unlike Centralized Algorithms which are executed on&nbsp; single computer and the main concern is efficiency in solving a Mathematical Problem, Distributed Algorithms are about few computers coordinating their actions. The most famous coordination action is that of reaching consensus among computers with competing agendas. But coordination may come in many ways most of them not equivalent to the strongest coordination, that of consensus.</p>\n<p>To coordinate computers need to communicate. Communication may happen in two main mechanisms. One is Message Passing, and the other is through ``writing on the board'' that is, applying actions to shared objects. In 1985 with a celebrated result called FLP it became clear that under only the most stringent conditions can consensus be achieved. This gave rise to the question what can be achieved under less that the most stringent condition?&nbsp;</p>\n<p>This gave rise to the area in distributed computing called computability. What conditions are required minimally to solve a given problem. The next question than is if we identify conditions with models are there infinitely many models, and do these collections of models differ if communication is through message passing or shared objects.</p>\n<p>The proposal was to show that the models can be characterized by their set-consensus power immaterial if communication is accomplished through Message passing or Shared-Objects. We knew that in Shared Objects the only ones that make sense are those called deterministic. But classifyingShared Objects models through set consensus power faced major hurdle which has not been resolved since it was realized in 1994. The hurdle is to show that the power classification makes sense in that it is invariant to the number of shared object used. This was called the Robustness problem.</p>\n<p>We were delayed by this question and we resolved it just for consensus rather than set consensus. We fell short of the grand objective. But we accomplished some side results.&nbsp;</p>\n<p>We showed the existence of pure set consensus power in the context of shared objects as we know this power exists in the message passing realm.</p>\n<p>And, we applied our knowledge of distributed algorithms a proof methodology to improve and prove algorithms in the Stellar Block-Chain system - a work of cooperation between the project and Prof David Mazieres&nbsp; from Stanford who is the CTO of Stellar.</p>\n<p>As for the Verification Aspect:&nbsp;</p>\n<pre class=\"aLF-aPX-K0-aPE\">Computer-assisted design (CAD) software allows civil engineers to predict the\nbehavior of a bridge under the constraints of traffic and weather to ensure\nthat it will be safe. Similarly, formal verification tools allow software\nengineers to predict the behavior of software and make sure that the software\nwill operate safely. However, unlike CAD software for bridges, formal verification tools\nare still not widely available to practitioners, which translates into undetected bugs affecting the end user.\n\nMost existing verification tools can only be used effectively by academic experts in\nthe field, which makes their use in the industry prohibitively expensive or\njust impossible, as there are only a few experts available to serve the market.\nOne reason which makes formal verification tools so hard to use in practice is\nthat, although those tools try to help the user by performing automatic\nmathematical reasoning to predict software behavior, they succeed or fail for\nreasons that are obscure to the non-expert. When such verification\ntools fail to produce a verdict about the conforming or non-conforming behavior\nof software, the user has no information to correct the problem and make the\ntool work. Ideally, formal verification tools would be transparent and explain\nthe reason for their failure to the user.\n\nAnother problem is that software verification tools are unstable:\nseemingly trivial changes to the software being verified can render the tools\nunusable when it worked well before those changes, again for reasons that are\nobscure to the user. The opaque failures and the instability of\nstate-of-the-art formal verification are often cited as the number one obstacle\nto their productive use and adoption in software engineering practice.\n\nTo curb the problems of opaque failures and instability of formal verification\ntools, we have pursued an approach based on decidable logics. Crucially, decidable\nlogics have the property that there is an algorithm, called a decision procedure, that can decide the satisfiability of every proposition. Moreover, whether a proposition belongs to a decidable logic is easy to check and can be explained to the user.\nThis guarantees that a proposition about software correctness can either be\ndecided (true or not true) or that it falls outside the decidable logics of\ninterest, with an explanation as to why this is the case.\n\nWe have shown that in many interesting cases pertaining to the verification of distributed systems, decidable logics form the basis for effective verification tools that do not suffer from the opaque failure and instability problems. </pre>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/27/2018<br>\n\t\t\t\t\tModified by: Eliezer&nbsp;M&nbsp;Gafni</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDistributed Algorithms:\n\nUnlike Centralized Algorithms which are executed on  single computer and the main concern is efficiency in solving a Mathematical Problem, Distributed Algorithms are about few computers coordinating their actions. The most famous coordination action is that of reaching consensus among computers with competing agendas. But coordination may come in many ways most of them not equivalent to the strongest coordination, that of consensus.\n\nTo coordinate computers need to communicate. Communication may happen in two main mechanisms. One is Message Passing, and the other is through ``writing on the board'' that is, applying actions to shared objects. In 1985 with a celebrated result called FLP it became clear that under only the most stringent conditions can consensus be achieved. This gave rise to the question what can be achieved under less that the most stringent condition? \n\nThis gave rise to the area in distributed computing called computability. What conditions are required minimally to solve a given problem. The next question than is if we identify conditions with models are there infinitely many models, and do these collections of models differ if communication is through message passing or shared objects.\n\nThe proposal was to show that the models can be characterized by their set-consensus power immaterial if communication is accomplished through Message passing or Shared-Objects. We knew that in Shared Objects the only ones that make sense are those called deterministic. But classifyingShared Objects models through set consensus power faced major hurdle which has not been resolved since it was realized in 1994. The hurdle is to show that the power classification makes sense in that it is invariant to the number of shared object used. This was called the Robustness problem.\n\nWe were delayed by this question and we resolved it just for consensus rather than set consensus. We fell short of the grand objective. But we accomplished some side results. \n\nWe showed the existence of pure set consensus power in the context of shared objects as we know this power exists in the message passing realm.\n\nAnd, we applied our knowledge of distributed algorithms a proof methodology to improve and prove algorithms in the Stellar Block-Chain system - a work of cooperation between the project and Prof David Mazieres  from Stanford who is the CTO of Stellar.\n\nAs for the Verification Aspect: \nComputer-assisted design (CAD) software allows civil engineers to predict the\nbehavior of a bridge under the constraints of traffic and weather to ensure\nthat it will be safe. Similarly, formal verification tools allow software\nengineers to predict the behavior of software and make sure that the software\nwill operate safely. However, unlike CAD software for bridges, formal verification tools\nare still not widely available to practitioners, which translates into undetected bugs affecting the end user.\n\nMost existing verification tools can only be used effectively by academic experts in\nthe field, which makes their use in the industry prohibitively expensive or\njust impossible, as there are only a few experts available to serve the market.\nOne reason which makes formal verification tools so hard to use in practice is\nthat, although those tools try to help the user by performing automatic\nmathematical reasoning to predict software behavior, they succeed or fail for\nreasons that are obscure to the non-expert. When such verification\ntools fail to produce a verdict about the conforming or non-conforming behavior\nof software, the user has no information to correct the problem and make the\ntool work. Ideally, formal verification tools would be transparent and explain\nthe reason for their failure to the user.\n\nAnother problem is that software verification tools are unstable:\nseemingly trivial changes to the software being verified can render the tools\nunusable when it worked well before those changes, again for reasons that are\nobscure to the user. The opaque failures and the instability of\nstate-of-the-art formal verification are often cited as the number one obstacle\nto their productive use and adoption in software engineering practice.\n\nTo curb the problems of opaque failures and instability of formal verification\ntools, we have pursued an approach based on decidable logics. Crucially, decidable\nlogics have the property that there is an algorithm, called a decision procedure, that can decide the satisfiability of every proposition. Moreover, whether a proposition belongs to a decidable logic is easy to check and can be explained to the user.\nThis guarantees that a proposition about software correctness can either be\ndecided (true or not true) or that it falls outside the decidable logics of\ninterest, with an explanation as to why this is the case.\n\nWe have shown that in many interesting cases pertaining to the verification of distributed systems, decidable logics form the basis for effective verification tools that do not suffer from the opaque failure and instability problems. \n\n \n\n\t\t\t\t\tLast Modified: 12/27/2018\n\n\t\t\t\t\tSubmitted by: Eliezer M Gafni"
 }
}