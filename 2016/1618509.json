{
 "awd_id": "1618509",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Enabling Efficient Context Switching and Effective Latency Hiding in GPUs",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 330000.0,
 "awd_amount": 330000.0,
 "awd_min_amd_letter_date": "2016-07-27",
 "awd_max_amd_letter_date": "2016-07-27",
 "awd_abstract_narration": "Graphics processing units (GPUs), initially designed for computer graphics, are becoming widely used for general purpose computing. This project addresses two important challenges in GPU computing. First, it investigates schemes to enable GPUs to be preempted efficiently, which is critical for GPUs to satisfy the quality of service (QOS) requirement in the cloud environment. Second, the project looks into approaches to significantly improve the latency hiding capability of GPUs. This interdisciplinary research has two practical uses, efficient preemption empowering GPUs as truly shared resource and effective latency hiding improving both the GPU performance and energy efficiency. Graduate student advising and industry collaboration are two key aspects of the project.\r\n\r\nThe design philosophy of GPUs is to exploit very high degrees of data-level parallelism (DLP), expressed as thread-level parallelism (TLP), to hide long instruction latency. As a side effect, GPUs feature high amounts of on-chip resources to store the contexts or the architectural states of the large numbers of concurrent threads. The large contexts result in long latency for context switching, which makes it difficult for GPUs to be truly shared in cloud servers. This research project leverages the nature of the single-instruction multiple-thread (SIMT) execution model to drastically reduce and compress the GPU context size. Software and hardware approaches are integrated to enable instruction-level preemption for GPUs to meet the QOS requirements. Fast context switching is also used to switch out stalled threads and switch in new ones such that the otherwise idle computing resources can be utilized to provide much higher latency-hiding capability. It essentially achieves higher TLP on GPUs without enlarging their critical on-chip resources.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Huiyang",
   "pi_last_name": "Zhou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Huiyang Zhou",
   "pi_email_addr": "hzhou@ncsu.edu",
   "nsf_id": "000250126",
   "pi_start_date": "2016-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276957911",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 330000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project improves upon the computing architecture of graphics processing units (GPUs) and has three main focuses. First, it develops both hardware and software schemes to enable prompt preemption for GPU such that the device can respond to user requests in time and support various scheduling algorithms. Second, it leverages hardware accelerated preemption to enable new latency hiding opportunities so as to enhance the efficiency of GPU program execution. Third, this project investigates the concurrent kernel execution paradigm, which enables multiple programs to share the GPU device, pinpoints its performance bottlenecks, and devises novel architectural approaches to improve the efficacy of concurrent kernel execution. The research scope of the project is further extended into GPU memory models for emerging non-volatile memory, side-channel-resistant crypto algorithm implementations on GPU, and code optimizations in OpenCL for FPGA.</p>\n<p>This project advances the state-of-the-art GPU architecture design and enables GPU devices to be utilized more efficiently. It features close collaboration between leading industry and academic researchers and offers the participating graduate students interdisciplinary research experiences in both hardware and software development. Research findings from this project have been published in premium conferences and journals in the computer architecture and supercomputing fields and incorporated in a graduate level course focusing on GPU architecture. Open source software has been also released for the reproducibility of the research results.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/01/2019<br>\n\t\t\t\t\tModified by: Huiyang&nbsp;Zhou</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project improves upon the computing architecture of graphics processing units (GPUs) and has three main focuses. First, it develops both hardware and software schemes to enable prompt preemption for GPU such that the device can respond to user requests in time and support various scheduling algorithms. Second, it leverages hardware accelerated preemption to enable new latency hiding opportunities so as to enhance the efficiency of GPU program execution. Third, this project investigates the concurrent kernel execution paradigm, which enables multiple programs to share the GPU device, pinpoints its performance bottlenecks, and devises novel architectural approaches to improve the efficacy of concurrent kernel execution. The research scope of the project is further extended into GPU memory models for emerging non-volatile memory, side-channel-resistant crypto algorithm implementations on GPU, and code optimizations in OpenCL for FPGA.\n\nThis project advances the state-of-the-art GPU architecture design and enables GPU devices to be utilized more efficiently. It features close collaboration between leading industry and academic researchers and offers the participating graduate students interdisciplinary research experiences in both hardware and software development. Research findings from this project have been published in premium conferences and journals in the computer architecture and supercomputing fields and incorporated in a graduate level course focusing on GPU architecture. Open source software has been also released for the reproducibility of the research results.\n\n\t\t\t\t\tLast Modified: 09/01/2019\n\n\t\t\t\t\tSubmitted by: Huiyang Zhou"
 }
}