{
 "awd_id": "1645737",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Synergy: Image-Based Indoor Navigation for Visually Impaired Users",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Lawrence Goldberg",
 "awd_eff_date": "2017-02-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 749993.0,
 "awd_amount": 749993.0,
 "awd_min_amd_letter_date": "2016-08-26",
 "awd_max_amd_letter_date": "2021-10-25",
 "awd_abstract_narration": "Severe visual impairment and blindness preclude many essential activities of daily living. Among these is independent navigation in unfamiliar indoor spaces without the assistance of a sighted companion. We propose to develop PERCEPT-V: an organic vision-driven, smartphone-based indoor navigation system, in which the user can navigate in open spaces without requiring retrofit of the environment. When the user seeks to obtain navigation instructions to a chosen destination, the smartphone will record observations from multiple onboard sensors in order to perform user localization. Once the location and orientation of the user are estimated, they are used to calculate the coordinates of the navigation landmarks surrounding the user. The system can then provide directions to the chosen destination, as well as an optional description of the landmarks around the user.\r\n\r\nWe will focus on addressing the cyber-physical systems technology shortcomings usually encountered in the development of indoor navigation systems. More specifically, our project will consider the following transformative aspects in the design of PERCEPT-V: (i) Image-Based Indoor Localization and Orientation: PERCEPT-V will feature new computer vision-based localization algorithms that reduce the dependence on highly controlled image capture and richly informative images, increasing the reliability of localization from images taken by blind subjects in crowded environments; (ii) Customized Navigation Instructions: PERCEPT-V will deliver customized navigation instructions for sight-impaired users that accounts for diverse levels of confidence and operator capabilities. A thorough final evaluation study featuring visually impaired participants will assess our hypotheses driving the design and refinements of PERCEPT-V using rigorous statistical analysis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marco",
   "pi_last_name": "Duarte",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Marco F Duarte",
   "pi_email_addr": "mduarte@ecs.umass.edu",
   "nsf_id": "000600744",
   "pi_start_date": "2016-08-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Aura",
   "pi_last_name": "Ganz",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aura Ganz",
   "pi_email_addr": "ganz@ecs.umass.edu",
   "nsf_id": "000099379",
   "pi_start_date": "2016-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "100 Natural Resources Road",
  "perf_city_name": "Amherst",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039292",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "153E",
   "pgm_ref_txt": "Wireless comm & sig processing"
  },
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 749993.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project is to develop PERCEPT-V: an organic vision-driven, smartphone-based indoor navigation system, in which a blind or visually impaired (BVI) user can navigate in open spaces without requiring retrofit of the environment. When the user seeks a smartphone app to obtain navigation instructions to a chosen destination, the smartphone will record observations from multiple onboard sensors in order to perform user localization. Once the location and orientation of the user are estimated, they are used to calculate the coordinates of the navigation landmarks surrounding the user. The system can then provide directions to the chosen destination, as well as an optional description of the landmarks around the user.</p>\n<p>We have built a prototype system on an iPhone/iOS platform that interfaces with an image localization service and a graph-based navigation service running on a cloud server. The former identifies the location of the user, while the latter provides instructions for the user to navigate the environment to the desired destination. To improve the usability of the system for the BVI population, we have developed an accessible picture guidance service (APGS) to assist the user in taking optimal pictures of the indoor environment to be used for localization. APGS provides haptic and audio feedback to the user in order to guide the pan and rotation of the handheld smartphone to improve the orientation of the images obtained for tracking and localization. Usability studies for PERCEPT-V with BVI testers found high levels of satisfaction with the system, ranking in a 1-7 Likert scale with an average score of 6.3.&nbsp;</p>\n<p>Our image-based navigation system can also be used to support a training framework that renders a realistic representation of the physical navigation environment using panoramic videos. The framework performs video capturing and stitching, followed by camera trajectory estimation via computer vision, and the generation of a 3D virtual environment with hand-annotated cues that are integrated into the resulting panoramic video. We show that the proposed framework processing time is less than a minute per meter of the recorded path in the physical environment which is a significant reduction compared to traditional 3D modeling approaches. The proposed framework provides an opportunity to develop applications (e.g. simulation-based training) with limited budgets and still meet the requirement of a very accurate representation of the physical indoor environment.</p>\n<p>Finally, we developed two systems that allow for pre-deployment testing of the PERCEPT-V platform for BVI users. The first system uses a virtual user model that includes specific human perception and information processing capabilities to test a virtual environment instantiation of the navigation platform; the navigation instructions are fed to the virtual user to predict the level of success that a true user would be able to achieve in practice. This system will provide a level of validation of the navigation instructions in new environments before human trials are attempted. The second system relies on user feedback in the form of a stream of comments that are linked to the spatiotemporal context of the user during human trials. The system performs textual analysis of the user comments and links their interpretation to the specific navigation instructions presented to the user at the time and the current location of the user, in order to identify common points of confusion in the operation and use of the navigation platform. The system also provides a visualization of the user comments that presents the sentiment analysis of the user comments overlaid with the spatial map of the space used in navigation.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/28/2022<br>\n\t\t\t\t\tModified by: Marco&nbsp;F&nbsp;Duarte</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project is to develop PERCEPT-V: an organic vision-driven, smartphone-based indoor navigation system, in which a blind or visually impaired (BVI) user can navigate in open spaces without requiring retrofit of the environment. When the user seeks a smartphone app to obtain navigation instructions to a chosen destination, the smartphone will record observations from multiple onboard sensors in order to perform user localization. Once the location and orientation of the user are estimated, they are used to calculate the coordinates of the navigation landmarks surrounding the user. The system can then provide directions to the chosen destination, as well as an optional description of the landmarks around the user.\n\nWe have built a prototype system on an iPhone/iOS platform that interfaces with an image localization service and a graph-based navigation service running on a cloud server. The former identifies the location of the user, while the latter provides instructions for the user to navigate the environment to the desired destination. To improve the usability of the system for the BVI population, we have developed an accessible picture guidance service (APGS) to assist the user in taking optimal pictures of the indoor environment to be used for localization. APGS provides haptic and audio feedback to the user in order to guide the pan and rotation of the handheld smartphone to improve the orientation of the images obtained for tracking and localization. Usability studies for PERCEPT-V with BVI testers found high levels of satisfaction with the system, ranking in a 1-7 Likert scale with an average score of 6.3. \n\nOur image-based navigation system can also be used to support a training framework that renders a realistic representation of the physical navigation environment using panoramic videos. The framework performs video capturing and stitching, followed by camera trajectory estimation via computer vision, and the generation of a 3D virtual environment with hand-annotated cues that are integrated into the resulting panoramic video. We show that the proposed framework processing time is less than a minute per meter of the recorded path in the physical environment which is a significant reduction compared to traditional 3D modeling approaches. The proposed framework provides an opportunity to develop applications (e.g. simulation-based training) with limited budgets and still meet the requirement of a very accurate representation of the physical indoor environment.\n\nFinally, we developed two systems that allow for pre-deployment testing of the PERCEPT-V platform for BVI users. The first system uses a virtual user model that includes specific human perception and information processing capabilities to test a virtual environment instantiation of the navigation platform; the navigation instructions are fed to the virtual user to predict the level of success that a true user would be able to achieve in practice. This system will provide a level of validation of the navigation instructions in new environments before human trials are attempted. The second system relies on user feedback in the form of a stream of comments that are linked to the spatiotemporal context of the user during human trials. The system performs textual analysis of the user comments and links their interpretation to the specific navigation instructions presented to the user at the time and the current location of the user, in order to identify common points of confusion in the operation and use of the navigation platform. The system also provides a visualization of the user comments that presents the sentiment analysis of the user comments overlaid with the spatial map of the space used in navigation.\n\n\t\t\t\t\tLast Modified: 12/28/2022\n\n\t\t\t\t\tSubmitted by: Marco F Duarte"
 }
}