{
 "awd_id": "1613060",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Prediction Models Based on Large Scale Image Data",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 100000.0,
 "awd_amount": 100000.0,
 "awd_min_amd_letter_date": "2016-07-25",
 "awd_max_amd_letter_date": "2016-07-25",
 "awd_abstract_narration": "Research in statistics involves the development and understanding of models based on data. Generally, these data are in the form of numbers, but more recently, statisticians have begun to develop models for data in the form of images. These functional image models have broad applications in neuroscience, engineering, and biomedical practice. This research will further the development of these image models. This project will also include the development of new courses at the undergraduate and graduate levels to train students in the use and understanding of these models. \r\n\r\nThis project is to develop an integrated research program that studies a broad class of large scale functional image models. The PI aims to develop the adaptive and/or local region regression, the finite mixture regression, and the transformation survival regression with ultra-high dimensional image data. The key advantages of these models are to preserve sharp edges for better interpretation, to incorporate the heterogeneity in the population for better representation, and to handle sophisticated censored data. The theoretical contributions of the proposed research are made towards addressing fundamental issues across several disciplines, including nonparametric statistics and machine learning. These functional image models have broad applications in neuroscience, engineering, and biomedical practice. Courses will be developed to train students in the use and understanding of these models.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiao",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiao Wang",
   "pi_email_addr": "wangxiao@purdue.edu",
   "nsf_id": "000326345",
   "pi_start_date": "2016-07-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "250 N University Street",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072066",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The PI has finished two papers this year. The title of the first paper is \"High-Dimensional Spatial Quantile Function-on-Scalar Regression\", which is revised and resubmitted to Journal of the American Statistical Association (JASA).&nbsp;This research develops a novel spatial quantile function-on-scalar regression model, which studies the conditional spatial distribution of a high-dimensional functional response given scalar predictors. With the strength of both quantile regression and copula modeling, we are able to explicitly characterize the conditional distribution of the functional or image response on the whole spatial domain. This model provides a comprehensive understanding of the effect of scalar covariates on functional responses across different quantile levels and also gives a practical way to generate new images for given covariate values. Theoretically, the paper establishes the minimax rates of convergence for estimating coefficient functions under both fixed and random designs. This research further develops an efficient primal-dual algorithm to handle high-dimensional image data. Simulations and real data analysis are conducted to examine the finite-sample performance. The PI has applied the proposed method to analyze the diffusion tensor imaging (DTI) data in the Alzheumer's Disease Neuroimaging Initiative (ADNI) study and displayed advantages over general functional mean regression.&nbsp;Specifically the PI proposes a novel spatial quantile function-on-scalar regression model (denoted by SQR) that studies the conditional spatial distribution of a functional response given scalar predictors. The PI is particularly interested in the case where the responses are high-dimensional functions or images obtained from n independent subjects. Let the functional response be $\\{Y(s): s\\in {\\cal S}\\}$ on a field ${\\cal S}$ and the scalar predictors be ${\\bf x} \\in \\real^p$. The SQR model includes two major components. The first component is to model the marginal conditional distribution of $Y(s)$ given ${\\bf x}$ at a fixed location $s$. This task can be achieved by&nbsp; assuming that the $\\tau$-level conditional quantile of $Y(s)$ is assumed to be a linear function of ${\\bf x}$.&nbsp; The second component is to model the joint conditional distribution of $Y(s)$ among spatial locations via a copula model.&nbsp; With these two components, the PI is able to explicitly characterize the conditional joint distribution of $Y(s)$ on the whole spatial domain ${\\cal S}$. The proposed method provides a framework to comprehensively understand the effects of scalar covariates (e.g., age, gender and disease status) on an image response,&nbsp; and a practical way to generate new images given covariates.&nbsp;</p>\n<p>The title of the second paper is \"Sparse Deep Neural Netowkrs Using $L_{1, \\infty}$-Weight Normalization\". This paper is accepted by Statistica Sinica.&nbsp;Deep neural networks have recently demonstrated an amazing performance on many challenging tasks. Overfitting is one of the notorious features for DNNs. Empirical evidence suggests that inducing sparsity can relieve overfitting, and weight normalization can accelerate the algorithm convergence. The PI studies $L_{1,\\infty}$-weight normalization for deep neural networks with bias neurons to achieve the sparse architecture.&nbsp; This research theoretically establishes the generalization error bounds for both regression and classification under the $L_{1,\\infty}$-weight normalization. It is shown that the upper bounds are independent of the network width and $\\sqrt{k}$-dependence on the network depth $k$, which are the best available bounds for networks with bias neurons. These results provide theoretical justifications on the usage of such weight normalization to reduce the generalization error. This paper also develops an easily implemented gradient projection descent algorithm to practically obtain a sparse neural network. The PI performs various experiments to validate our theory and demonstrate the effectiveness of the resulting approach. Specifically, empirical evidence suggests that inducing sparsity can relieve overfitting and save computation resources. A common strategy is to apply sparsity-inducing regularizers such as $L_0$ penalty or the total number of parameters in the network. However, theoretical investigations or justifications on sparse DNNs are less explored in the literature.&nbsp;Weight normalization, by bounding the Euclidean norm of the incoming weights of each unit, has shown to be able to accelerate the convergence of stochastic gradient descent optimization across many applications. The PI borrows the strength of weight normalization and induce the sparsity by bounding the $L_{1, \\infty}$ norm of the weight matrix (including bias) for each layer. By doing this, the PI is able to induce the sparsity in a systematic way. Furthermore, the paper has developed capacity control for such models. It is shown that the generalization error upper bounds are independent of the network width and $\\sqrt{k}$-dependence on the depth $k$ of the network, which are the best available bounds for networks with bias neurons. These results provide theoretical justifications on the usage of such weight normalization, which leads to a sparse DNN. At the same time, the generalization error has the minimal dependence on the network architecture.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2019<br>\n\t\t\t\t\tModified by: Xiao&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe PI has finished two papers this year. The title of the first paper is \"High-Dimensional Spatial Quantile Function-on-Scalar Regression\", which is revised and resubmitted to Journal of the American Statistical Association (JASA). This research develops a novel spatial quantile function-on-scalar regression model, which studies the conditional spatial distribution of a high-dimensional functional response given scalar predictors. With the strength of both quantile regression and copula modeling, we are able to explicitly characterize the conditional distribution of the functional or image response on the whole spatial domain. This model provides a comprehensive understanding of the effect of scalar covariates on functional responses across different quantile levels and also gives a practical way to generate new images for given covariate values. Theoretically, the paper establishes the minimax rates of convergence for estimating coefficient functions under both fixed and random designs. This research further develops an efficient primal-dual algorithm to handle high-dimensional image data. Simulations and real data analysis are conducted to examine the finite-sample performance. The PI has applied the proposed method to analyze the diffusion tensor imaging (DTI) data in the Alzheumer's Disease Neuroimaging Initiative (ADNI) study and displayed advantages over general functional mean regression. Specifically the PI proposes a novel spatial quantile function-on-scalar regression model (denoted by SQR) that studies the conditional spatial distribution of a functional response given scalar predictors. The PI is particularly interested in the case where the responses are high-dimensional functions or images obtained from n independent subjects. Let the functional response be $\\{Y(s): s\\in {\\cal S}\\}$ on a field ${\\cal S}$ and the scalar predictors be ${\\bf x} \\in \\real^p$. The SQR model includes two major components. The first component is to model the marginal conditional distribution of $Y(s)$ given ${\\bf x}$ at a fixed location $s$. This task can be achieved by  assuming that the $\\tau$-level conditional quantile of $Y(s)$ is assumed to be a linear function of ${\\bf x}$.  The second component is to model the joint conditional distribution of $Y(s)$ among spatial locations via a copula model.  With these two components, the PI is able to explicitly characterize the conditional joint distribution of $Y(s)$ on the whole spatial domain ${\\cal S}$. The proposed method provides a framework to comprehensively understand the effects of scalar covariates (e.g., age, gender and disease status) on an image response,  and a practical way to generate new images given covariates. \n\nThe title of the second paper is \"Sparse Deep Neural Netowkrs Using $L_{1, \\infty}$-Weight Normalization\". This paper is accepted by Statistica Sinica. Deep neural networks have recently demonstrated an amazing performance on many challenging tasks. Overfitting is one of the notorious features for DNNs. Empirical evidence suggests that inducing sparsity can relieve overfitting, and weight normalization can accelerate the algorithm convergence. The PI studies $L_{1,\\infty}$-weight normalization for deep neural networks with bias neurons to achieve the sparse architecture.  This research theoretically establishes the generalization error bounds for both regression and classification under the $L_{1,\\infty}$-weight normalization. It is shown that the upper bounds are independent of the network width and $\\sqrt{k}$-dependence on the network depth $k$, which are the best available bounds for networks with bias neurons. These results provide theoretical justifications on the usage of such weight normalization to reduce the generalization error. This paper also develops an easily implemented gradient projection descent algorithm to practically obtain a sparse neural network. The PI performs various experiments to validate our theory and demonstrate the effectiveness of the resulting approach. Specifically, empirical evidence suggests that inducing sparsity can relieve overfitting and save computation resources. A common strategy is to apply sparsity-inducing regularizers such as $L_0$ penalty or the total number of parameters in the network. However, theoretical investigations or justifications on sparse DNNs are less explored in the literature. Weight normalization, by bounding the Euclidean norm of the incoming weights of each unit, has shown to be able to accelerate the convergence of stochastic gradient descent optimization across many applications. The PI borrows the strength of weight normalization and induce the sparsity by bounding the $L_{1, \\infty}$ norm of the weight matrix (including bias) for each layer. By doing this, the PI is able to induce the sparsity in a systematic way. Furthermore, the paper has developed capacity control for such models. It is shown that the generalization error upper bounds are independent of the network width and $\\sqrt{k}$-dependence on the depth $k$ of the network, which are the best available bounds for networks with bias neurons. These results provide theoretical justifications on the usage of such weight normalization, which leads to a sparse DNN. At the same time, the generalization error has the minimal dependence on the network architecture. \n\n\t\t\t\t\tLast Modified: 10/30/2019\n\n\t\t\t\t\tSubmitted by: Xiao Wang"
 }
}