{
 "awd_id": "1619078",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Modeling and Learning Visual Similarities Under Adverse Visual Conditions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 440000.0,
 "awd_amount": 440000.0,
 "awd_min_amd_letter_date": "2016-07-27",
 "awd_max_amd_letter_date": "2016-07-27",
 "awd_abstract_narration": "In many emerging applications such as autonomous/assisted driving, intelligent video surveillance, and rescue robots, the performances of visual sensing and analytics are largely jeopardized by various adverse visual conditions in complex unconstrained environments, e.g., bad weather and illumination conditions. This project studies how and to what extend such adverse visual conditions can be coped with. It will advance and enrich the fundamental research of computer vision, and bring significant impact on developing \"all-weather\"computer vision systems that benefit security/safety, autonomous driving, and robotics. The project contributes to education through curriculum development, student training, and knowledge dissemination. It also includes interactions with K-12 students for participation and research opportunities.  \r\n\r\nThis research seeks innovative solution to overcome adverse visual conditions for visual sensing and analytics. It explores a unified approach that avoids explicit image restoration that is in general computationally demanding. It is focused on learning the \"alignment\" between the two image spaces under adverse and normal conditions, rather than learn everything from scratch. Acting on low-quality data directly without image restoration, this research leads to innovative and computationally efficient solutions to handle adverse visual conditions. Visual restoration can also be done as by-products, and the same approach also provides a general solution to target attribute estimation. The research is focused on: (1) constructing a principled model, called space alignment that models and learns visual similarity, and its theoretical foundation, (2) developing new effective visual matching and tracking approaches based on learning the appropriate visual similarity under various adverse visual condition, (3) investigating visual attribute estimation and identification via learning reconstruction-based visual regression, and (4) developing effective and efficient tools and prototype systems for visual detection, identification, tracking and recognition.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ying",
   "pi_last_name": "Wu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ying Wu",
   "pi_email_addr": "yingwu@northwestern.edu",
   "nsf_id": "000299558",
   "pi_start_date": "2016-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "2145 Sheridan Road",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602083118",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 440000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Many emerging applications need to perform real-time image/video analytics in unconstrained environments, such as intelligent video surveillance, autonomous driving, rescue robots, etc. As contemporary computer vision techniques generally assume mild visual conditions and depend on good quality imagery, their performances are largely jeopardized by adverse visual conditions, e.g., induced by bad weather conditions, when the visual quality of the data is seriously degraded and the visual details may be obscured, contaminated, or lost in such perceptually ``low-quality\" imagery. Most existing solutions are based on image restoration pre-processing. However, as restoration is generally very difficult and computationally demanding, this approach is not practical. Satisfactory and efficient solutions are still yet to be found, which has impeded the development of ``all-weather\" vision systems.</p>\n<p>&nbsp;</p>\n<p>This project produces: (1) A principled model and its theoretical foundation. A general approach called space alignment is proposed to model and learn image similarity that applies to various adverse visual conditions. (2) Visual matching and tracking. The new approach learns appropriate similarity metrics under various adverse conditions for matching and tracking, by steering and aligning the known metric in good/well conditions to adverse conditions. This makes the visual learning tasks manageable and efficient, because the known knowledge does not need to be re-learned.&nbsp; (3) Visual attribute estimation and identification. The new approach estimates visual attributes via learning the reconstruction-based visual regression. (4) Tools and prototype systems. Effective and efficient tools are developed for visual matching and visual attribute estimation that can be widely used in many computer vision tasks. Prototype systems are developed for visual target tracking and identification that handles various adverse visual conditions.</p>\n<p>&nbsp;</p>\n<p>These methods advance the research of video scene modeling and understanding, and result in an important enabling technology for a wide range of applications, including image/video search, intelligent surveillance and security, human-computer interactions, social networks, etc.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/15/2021<br>\n\t\t\t\t\tModified by: Ying&nbsp;Wu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany emerging applications need to perform real-time image/video analytics in unconstrained environments, such as intelligent video surveillance, autonomous driving, rescue robots, etc. As contemporary computer vision techniques generally assume mild visual conditions and depend on good quality imagery, their performances are largely jeopardized by adverse visual conditions, e.g., induced by bad weather conditions, when the visual quality of the data is seriously degraded and the visual details may be obscured, contaminated, or lost in such perceptually ``low-quality\" imagery. Most existing solutions are based on image restoration pre-processing. However, as restoration is generally very difficult and computationally demanding, this approach is not practical. Satisfactory and efficient solutions are still yet to be found, which has impeded the development of ``all-weather\" vision systems.\n\n \n\nThis project produces: (1) A principled model and its theoretical foundation. A general approach called space alignment is proposed to model and learn image similarity that applies to various adverse visual conditions. (2) Visual matching and tracking. The new approach learns appropriate similarity metrics under various adverse conditions for matching and tracking, by steering and aligning the known metric in good/well conditions to adverse conditions. This makes the visual learning tasks manageable and efficient, because the known knowledge does not need to be re-learned.  (3) Visual attribute estimation and identification. The new approach estimates visual attributes via learning the reconstruction-based visual regression. (4) Tools and prototype systems. Effective and efficient tools are developed for visual matching and visual attribute estimation that can be widely used in many computer vision tasks. Prototype systems are developed for visual target tracking and identification that handles various adverse visual conditions.\n\n \n\nThese methods advance the research of video scene modeling and understanding, and result in an important enabling technology for a wide range of applications, including image/video search, intelligent surveillance and security, human-computer interactions, social networks, etc.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 09/15/2021\n\n\t\t\t\t\tSubmitted by: Ying Wu"
 }
}