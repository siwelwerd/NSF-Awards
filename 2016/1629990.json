{
 "awd_id": "1629990",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CI-P: Planning for AudioNet: A New Community Infrastructure for Audio Annotations for Acoustic Event Identification",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2016-07-01",
 "awd_exp_date": "2018-12-31",
 "tot_intn_awd_amt": 100000.0,
 "awd_amount": 100000.0,
 "awd_min_amd_letter_date": "2016-06-21",
 "awd_max_amd_letter_date": "2016-06-21",
 "awd_abstract_narration": "This effort lays the groundwork for AudioNet, a public-domain corpus of audio labels for the nearly 800,000 videos in the open-access YFCC100M dataset. Audio information provides an important complement to visual information in the automatic analysis of video data, allowing systems to detect situations that may not be clearly identifiable from the visual stream alone. However, there are as yet no truly large-scale labeled audio datasets of the kind needed as input to build flexible, accurate analysis systems. Creating such a large-scale corpus will serve as an impetus for better multimedia algorithms to be developed by more researchers and computer science students, translating into an impact on the everyday life of the public at large. Social media videos are increasingly used for scientific research, as they provide an opportunity to observe and model many phenomena in the social sciences, economics, meteorology, and medicine. New capabilities for content analysis will therefore impact many scientific fields. In addition, audio analysis could be used in real-time security surveillance and in robotics applications like autonomous vehicles and household robots to aid and monitor the elderly.\r\n\r\nAudioNet is part of a multi-institution collaboration, the Multimedia Commons initiative, which is developing a variety of resources around the YFCC100M dataset of Creative Commons-licensed photos and videos. AudioNet is annotating the audio tracks from the YFCC100M videos, focusing on audio concepts. Audio concepts can be thought of as acoustic \"objects\": concrete, localizable units of sound like \"crowd cheering\" or \"fire alarm\". The approach will be modeled on ImageNet, an image dataset labeled and organized using the WordNet hierarchy of synsets (groups of synonyms); ImageNet has enabled major enabled advances in image processing. However, while ImageNet focuses largely on entities (noun synsets), audio data is inherently temporal. The label set for AudioNet will therefore focus on events and actions, though similarly organized using semantic resources like WordNet.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gerald",
   "pi_last_name": "Friedland",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gerald Friedland",
   "pi_email_addr": "fractor@icsi.berkeley.edu",
   "nsf_id": "000084347",
   "pi_start_date": "2016-06-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Julia",
   "pi_last_name": "Bernd",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Julia Bernd",
   "pi_email_addr": "jbernd@icsi.berkeley.edu",
   "nsf_id": "000716874",
   "pi_start_date": "2016-06-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "International Computer Science Institute",
  "inst_street_address": "2150 SHATTUCK AVE",
  "inst_street_address_2": "SUITE 250",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106662900",
  "inst_zip_code": "947041345",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "INTERNATIONAL COMPUTER SCIENCE INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GSRMP1QCXU74"
 },
 "perf_inst": {
  "perf_inst_name": "International Computer Science Institute",
  "perf_str_addr": "1947 Center Street, Suite 600",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947041159",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span style=\"font-size: 12pt; font-family: 'Times New Roman'; color: #000000; background-color: #ffffff; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Under the AudioNet planning grant, researchers at the International Computer Science Institute set out to determine what types of data researchers working on automatic analysis of environmental audio most need to move their field forward and explore new approaches. In addition to surveys and informal conversations with audio researchers about needs and priorities, we analyzed requirements for video retrieval for use in the sciences. Finally, we experimented with procedures for potentially creating a large-scale human-annotated audio dataset based on the open-source YFCC100M video dataset.</span></p>\n<p><span style=\"font-size: 12pt; font-family: 'Times New Roman'; color: #000000; background-color: #ffffff; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">As part of the needs analysis,</span><span style=\"font-size: 12pt; font-family: 'Times New Roman'; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> PI Friedland co-authored a paper called \"Audition for Multimedia Computing\", published in 2018 in </span><span style=\"font-size: 12pt; font-family: 'Times New Roman'; color: #000000; background-color: transparent; font-weight: 400; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Frontiers of Multimedia Research</span><span style=\"font-size: 12pt; font-family: 'Times New Roman'; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. This paper defines and sets the stage for the developing field of Computer Audition; it suggests we need to look beyond simply describing and classifying sounds, and rather work towards systems that will be able to make actionable inferences about the world based on sound (e.g., identifying what probably happened to cause a sound). In the process, the authors analyze what kind of audio datasets will be needed to achieve those goals and to evaluate success, emphasizing the need to move beyond simple categorization to structured representation.</span></p>\n<p><span style=\"font-size: 12pt; font-family: 'Times New Roman'; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In addition, we collaborated with other multimedia researchers on a generalized framework for multimedia big data studies (MMBDS). The goal of this framework is to allow natural scientists, social scientists, and data scientists to quickly retrieve and filter videos and images relevant to their research from the YFCC100M. Our work with scientists to lay out the requirements and structure for this framework served in part as a needs assessment for AudioNet as well. It provided us with the opportunity to examine what kinds of strong audio annotations could help audio researchers provide the most useful (and intuitive) content-based retrieval models for the framework.</span></p>\n<p><span style=\"font-size: 12pt; font-family: 'Times New Roman'; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">For our pilot experiments, we chose to focus on examining the trade-offs involved in providing precise timepoints for each labeled sound in a video (there is currently no large, open-source dataset of consumer-produced videos with such detailed meta-annotations), using more and less constrained labeling schemes.</span><span style=\"font-size: 12pt; font-family: 'Times New Roman'; color: #000000; background-color: #ffffff; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Timepoint localization is more time-consuming than simply annotating whether a given sound appears in a video, but is useful to researchers studying how to take advantage of the temporal nature of audio data, i.e., how patterns of co-occurrence and sound sequences can be used to make more fine-grained categorizations of videos. We described the results of our experiments in a report-back to interested researchers in the audio community.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/18/2019<br>\n\t\t\t\t\tModified by: Julia&nbsp;Bernd</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nUnder the AudioNet planning grant, researchers at the International Computer Science Institute set out to determine what types of data researchers working on automatic analysis of environmental audio most need to move their field forward and explore new approaches. In addition to surveys and informal conversations with audio researchers about needs and priorities, we analyzed requirements for video retrieval for use in the sciences. Finally, we experimented with procedures for potentially creating a large-scale human-annotated audio dataset based on the open-source YFCC100M video dataset.\n\nAs part of the needs analysis, PI Friedland co-authored a paper called \"Audition for Multimedia Computing\", published in 2018 in Frontiers of Multimedia Research. This paper defines and sets the stage for the developing field of Computer Audition; it suggests we need to look beyond simply describing and classifying sounds, and rather work towards systems that will be able to make actionable inferences about the world based on sound (e.g., identifying what probably happened to cause a sound). In the process, the authors analyze what kind of audio datasets will be needed to achieve those goals and to evaluate success, emphasizing the need to move beyond simple categorization to structured representation.\n\nIn addition, we collaborated with other multimedia researchers on a generalized framework for multimedia big data studies (MMBDS). The goal of this framework is to allow natural scientists, social scientists, and data scientists to quickly retrieve and filter videos and images relevant to their research from the YFCC100M. Our work with scientists to lay out the requirements and structure for this framework served in part as a needs assessment for AudioNet as well. It provided us with the opportunity to examine what kinds of strong audio annotations could help audio researchers provide the most useful (and intuitive) content-based retrieval models for the framework.\n\nFor our pilot experiments, we chose to focus on examining the trade-offs involved in providing precise timepoints for each labeled sound in a video (there is currently no large, open-source dataset of consumer-produced videos with such detailed meta-annotations), using more and less constrained labeling schemes. Timepoint localization is more time-consuming than simply annotating whether a given sound appears in a video, but is useful to researchers studying how to take advantage of the temporal nature of audio data, i.e., how patterns of co-occurrence and sound sequences can be used to make more fine-grained categorizations of videos. We described the results of our experiments in a report-back to interested researchers in the audio community.\n\n\t\t\t\t\tLast Modified: 04/18/2019\n\n\t\t\t\t\tSubmitted by: Julia Bernd"
 }
}