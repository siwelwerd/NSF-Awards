{
 "awd_id": "1649087",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Using Machine Learning to Increase the Operational Efficiency of Large Distributed Systems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 299994.0,
 "awd_amount": 299994.0,
 "awd_min_amd_letter_date": "2016-09-01",
 "awd_max_amd_letter_date": "2016-09-01",
 "awd_abstract_narration": "Large, distributed systems are nowadays ubiquitous and part of sustainable IT solutions to a broad range of customers and applications.  Data centers in the private or public cloud and high performance computing systems are two examples of complex, highly distributed systems: the former are used by almost everyone on a daily basis, the latter are used by computational scientists for advancing science and engineering.  High availability and reliability of these complex systems are important for the quality of user experience.  Efficient management of such systems contributes to their availability and reliability, and relies on a priori knowledge of the timing of the collective demands of users and a priori knowledge of certain performance measures (e.g., usage, temperature, power) of various systems components.\r\n\r\nThis  project aims to provide a systematic methodology to improve the operational efficiency of complex, distributed systems by developing neural networks that can efficiently and accurately predict the incoming workload within fine and coarse time scales. Such workload prediction can dramatically improve the operational efficiency of data centers and high performance systems by driving proactive management strategies that specifically aim to enhance reliability.  For datacenters, the focus is on actively reducing performance tickets that are automatically triggered by pro-actively managing virtual machine resizing and migration. For high performance computing systems the focus is on predicting hardware faults to autonomically improve the scheduler's efficiency, direct cooling, and improve performance and memory bandwidth.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Evgenia",
   "pi_last_name": "Smirni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Evgenia Smirni",
   "pi_email_addr": "esmirni@cs.wm.edu",
   "nsf_id": "000346140",
   "pi_start_date": "2016-09-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "College of William and Mary",
  "inst_street_address": "1314 S MOUNT VERNON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "WILLIAMSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "7572213965",
  "inst_zip_code": "23185",
  "inst_country_name": "United States",
  "cong_dist_code": "08",
  "st_cong_dist_code": "VA08",
  "org_lgl_bus_name": "COLLEGE OF WILLIAM AND MARY",
  "org_prnt_uei_num": "EVWJPCY6AD97",
  "org_uei_num": "EVWJPCY6AD97"
 },
 "perf_inst": {
  "perf_inst_name": "College of William and Mary",
  "perf_str_addr": "",
  "perf_city_name": "Williamsburg",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "231878795",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "VA01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 299994.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Workloads in today&rsquo;s distributed, large scale systems are diverse, continuously evolving, and often following complex short- and long-term patterns. Future workload knowledge is pivotal for effective systems management that aims to improve reliability and availability. The important issue becomes the following: if one could know a priori certain performance measures (e.g., usage, temperature, power) for selected system components, and/or the demands and timings of the ensemble of jobs to be executed simultaneously, then robust management solutions could be found. Workload discovery through trace analysis is key for the development of such solutions.</p>\n<p>The proposed research aims at providing a systematic methodology to increase the reliability and availability of large, distributed systems by developing neural networks and other machine learning models that can efficiently and accurately predict the upcoming workload. Such workload discovery can dramatically improve system operational efficiency via proactive management strategies driven by workload forecasting to increase system reliability and availability. In this project we focused on three data sets that were found in-the-wild: 1)<strong>&nbsp;data centers in the IBM private cloud</strong>, 2) a&nbsp;<strong>high performance computing system at Oak Ridge National Lab</strong>, and 3) the&nbsp;<strong>storage system of a Google data center</strong>. We have developed various machine learning workload predictors in the above three environments and have validate them extensively with the common goal to improving system reliability and availability.</p>\n<p><strong>Data Centers</strong>: The focus here is on improving system reliability via active ticket management. Performance ticketing systems provide the means to data centers to interactively improve user experience and maintain high system availability at all times. Typically, tickets are issued either automatically via system monitoring (e.g., when high resource usage is observed due to transient workload dynamics) or manually by the users themselves when a performance violation is encountered (e.g., unresponsive or slower than expected service). Performance ticket resolution is an expensive operation as a significant amount of manual labor is required for root-cause analysis. In highly virtualized cloud data centers where physical boxes host multiple virtual machines, a large body of tickets arise from resource usage warnings, e.g., CPU and RAM usages that exceed predefined thresholds. We demonstrated that we can use neural networks for effective time series prediction that can drastically reduce the number of performance tickets via pro-active virtual machine resizing and virtual machine migration policies.</p>\n<p><strong>HPC Systems:</strong>&nbsp;Applications that execute on HPC systems are typically large-scale, long-running scientific applications, i.e., they may execute from several hours to days. GPU reliability becomes an important issue as potential hardware faults could jeopardize application completion. In collaboration with colleagues at ORNL, we have conducted a very detailed characterization study of soft errors on the Titan&rsquo;s GPU nodes including single bit errors (SBEs), dynamic page retirement errors, and double bit errors [29]. The trace of a fully operating system was collected from February 2015 to June 2015 (more than 60 million node hours) across all 18,688 GPUs on the Titan. Our characterization study has shed some light into the conditions that may trigger faults, but this characterization alone cannot provide any clear prediction of when and where the faults will occur. For this problem, we have developed several machine learning predictors that can effectively predict the occurence and location of soft errors on the Titan.</p>\n<p><strong>Solid State Drive (SSD) Reliability:</strong>In recent years, solid state drives (SSDs) have become a staple of high-performance data centers for their speed and energy efficiency. We study the failure characteristics of 30,000 drives from a Google data center spanning six years. We characterize the workload conditions that lead to failures and illustrate that their root causes differ from common expectation but remain difficult to discern. Particularly, we study failure incidents that result in manual intervention from the repair &nbsp;process. We observe high levels of infant mortality and characterize the differences between infant &nbsp;and non-infant failures. We develop several machine learning failure prediction models that are &nbsp;shown to be surprisingly accurate, achieving high recall and low false positive rates. These models are used beyond simple prediction as they aid us to untangle the complex interaction of workload &nbsp;characteristics that lead to failures and identify failure root causes from monitored symptoms.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/13/2019<br>\n\t\t\t\t\tModified by: Evgenia&nbsp;Smirni</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWorkloads in today\u2019s distributed, large scale systems are diverse, continuously evolving, and often following complex short- and long-term patterns. Future workload knowledge is pivotal for effective systems management that aims to improve reliability and availability. The important issue becomes the following: if one could know a priori certain performance measures (e.g., usage, temperature, power) for selected system components, and/or the demands and timings of the ensemble of jobs to be executed simultaneously, then robust management solutions could be found. Workload discovery through trace analysis is key for the development of such solutions.\n\nThe proposed research aims at providing a systematic methodology to increase the reliability and availability of large, distributed systems by developing neural networks and other machine learning models that can efficiently and accurately predict the upcoming workload. Such workload discovery can dramatically improve system operational efficiency via proactive management strategies driven by workload forecasting to increase system reliability and availability. In this project we focused on three data sets that were found in-the-wild: 1) data centers in the IBM private cloud, 2) a high performance computing system at Oak Ridge National Lab, and 3) the storage system of a Google data center. We have developed various machine learning workload predictors in the above three environments and have validate them extensively with the common goal to improving system reliability and availability.\n\nData Centers: The focus here is on improving system reliability via active ticket management. Performance ticketing systems provide the means to data centers to interactively improve user experience and maintain high system availability at all times. Typically, tickets are issued either automatically via system monitoring (e.g., when high resource usage is observed due to transient workload dynamics) or manually by the users themselves when a performance violation is encountered (e.g., unresponsive or slower than expected service). Performance ticket resolution is an expensive operation as a significant amount of manual labor is required for root-cause analysis. In highly virtualized cloud data centers where physical boxes host multiple virtual machines, a large body of tickets arise from resource usage warnings, e.g., CPU and RAM usages that exceed predefined thresholds. We demonstrated that we can use neural networks for effective time series prediction that can drastically reduce the number of performance tickets via pro-active virtual machine resizing and virtual machine migration policies.\n\nHPC Systems: Applications that execute on HPC systems are typically large-scale, long-running scientific applications, i.e., they may execute from several hours to days. GPU reliability becomes an important issue as potential hardware faults could jeopardize application completion. In collaboration with colleagues at ORNL, we have conducted a very detailed characterization study of soft errors on the Titan\u2019s GPU nodes including single bit errors (SBEs), dynamic page retirement errors, and double bit errors [29]. The trace of a fully operating system was collected from February 2015 to June 2015 (more than 60 million node hours) across all 18,688 GPUs on the Titan. Our characterization study has shed some light into the conditions that may trigger faults, but this characterization alone cannot provide any clear prediction of when and where the faults will occur. For this problem, we have developed several machine learning predictors that can effectively predict the occurence and location of soft errors on the Titan.\n\nSolid State Drive (SSD) Reliability:In recent years, solid state drives (SSDs) have become a staple of high-performance data centers for their speed and energy efficiency. We study the failure characteristics of 30,000 drives from a Google data center spanning six years. We characterize the workload conditions that lead to failures and illustrate that their root causes differ from common expectation but remain difficult to discern. Particularly, we study failure incidents that result in manual intervention from the repair  process. We observe high levels of infant mortality and characterize the differences between infant  and non-infant failures. We develop several machine learning failure prediction models that are  shown to be surprisingly accurate, achieving high recall and low false positive rates. These models are used beyond simple prediction as they aid us to untangle the complex interaction of workload  characteristics that lead to failures and identify failure root causes from monitored symptoms.\n\n \n\n\t\t\t\t\tLast Modified: 11/13/2019\n\n\t\t\t\t\tSubmitted by: Evgenia Smirni"
 }
}