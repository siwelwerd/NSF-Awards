{
 "awd_id": "1566281",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CIF: Towards Linear-Time Computation of Structured Data Representations",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2016-04-15",
 "awd_exp_date": "2019-03-31",
 "tot_intn_awd_amt": 173282.0,
 "awd_amount": 173282.0,
 "awd_min_amd_letter_date": "2016-04-12",
 "awd_max_amd_letter_date": "2016-04-12",
 "awd_abstract_narration": "Estimating an unknown object from noisy, nonlinear, and incomplete observations constitutes a basic problem in data science. Standard solutions first assume that the unknown object obeys a structured mathematical representation, and then develop optimization algorithms for recovering the parameters of the representation. Moreover, rigorous analysis reveals that several such algorithms are statistically optimal. However, despite these advances in statistical understanding, the role of computation is far less well understood; in many cases, even the best methods incur a computation time proportional to a high-degree polynomial in terms of the data size. Therefore, to reap the full benefits of these methods in applications involving massive data, new algorithmic approaches are necessary. This research project introduces new theory and computational tools for learning structured data representations in linear running time. \r\n\r\n\r\nThe new algorithmic approaches are based on the intuition that challenging optimization problems encountered in data analysis can be circumvented if the answers are merely approximate, rather than exact. Establishing precise tradeoffs between statistical performance, approximation quality, and running time is a key focus. With this intuition, the project addresses three specific problems: (i) Reconstructing signals and images from nonlinear observations. (ii) Recovering graphs from partial observations, such as linear sketches or inter-node distance measurements. (iii) Estimating multidimensional probability distributions from random samples. Algorithms developed within the scope of the project impact applications ranging from medical imaging, computer network monitoring, social network analysis, and non-destructive evaluation (NDE). All publications, data, and source code are publicly available. The project involves the active participation of both graduate and undergraduate students, and exposes them to a span of areas including mathematics, statistics, computer science, and optimization.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chinmay",
   "pi_last_name": "Hegde",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chinmay Hegde",
   "pi_email_addr": "chinmay.h@nyu.edu",
   "nsf_id": "000701789",
   "pi_start_date": "2016-04-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Iowa State University",
  "inst_street_address": "1350 BEARDSHEAR HALL",
  "inst_street_address_2": "515 MORRILL ROAD",
  "inst_city_name": "AMES",
  "inst_state_code": "IA",
  "inst_state_name": "Iowa",
  "inst_phone_num": "5152945225",
  "inst_zip_code": "500112103",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IA04",
  "org_lgl_bus_name": "IOWA STATE UNIVERSITY OF SCIENCE AND TECHNOLOGY",
  "org_prnt_uei_num": "DQDBM7FGJPC5",
  "org_uei_num": "DQDBM7FGJPC5"
 },
 "perf_inst": {
  "perf_inst_name": "Iowa State University",
  "perf_str_addr": "2215 Coover Hall",
  "perf_city_name": "Ames",
  "perf_st_code": "IA",
  "perf_st_name": "Iowa",
  "perf_zip_code": "500113060",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "IA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "026y00",
   "pgm_ele_name": "CRII CISE Research Initiation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7797",
   "pgm_ref_txt": "COMM & INFORMATION FOUNDATIONS"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 173282.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Estimating an unknown object from noisy, coarse, and incomplete observations is a basic problem in data science. Standard estimation algorithms first assume that the unknown object under study obeys a structured mathematical representation, and then develop optimization algorithms for recovering the parameters of the representation. Moreover, rigorous theoretical results have shown that several such algorithms are statistically <em>optimal</em>: no other estimation method can possibly require substantially fewer number of observations to solve the given estimation problem.</span></p>\n<p><span>However, despite these advances in <em>statistical</em> understanding, the role of <em>computation</em>&nbsp;is far less well understood. In many cases, even the best estimation methods incur a computation time proportional to a <em>high-degree polynomial</em> in terms of the input data size. This constitutes a significant obstacle, particularly in the \"big-data\" era where the scale of the problem is well beyond the reach of traditional statistical estimation methods. Therefore, new algorithmic approaches are necessary.&nbsp;</span>This research project introduced new theoretical and computational tools for learning structured data representations from coarse observations in <em>linear</em> running time, thus advancing the frontier of what was computationally tractable. Linear-time algorithms are considered to be the gold standard for a broad range of statistical analysis problems.</p>\n<p>This new approach was based on the intuition that challenging optimization problems encountered in statistics, signal processing, and data science can be circumvented if the answers are merely approximate, rather than exact. Establishing precise tradeoffs between statistical performance, approximation quality, and running time was a key focus. With this intuition, the project addressed three specific estimation problems: (i) Reconstruction of signals and images from nonlinear observations. (ii) Reconstruction of graphs from partial linear sketches. (iii) Estimating probability distributions from random samples. For each of these estimation problems, the techniques developed within the purview of the project either directly led to (or indirectly impacted the development of) fast algorithms whose running time was (nearly) linear, meaning that they are close to the best possible. The new algorithms developed in this project were validated on datasets arising from a range of application domains, including computational imaging, power networks, and transportation analytics. &nbsp;&nbsp;</p>\n<p>The project supported the active participation of both graduate and undergraduate students (including three individuals from under-represented minority groups) at Iowa State University, and exposed them to a span of areas including statistics, electrical engineering and computer science, and optimization. The project also supported the PI in the development of new course curricula on data science at Iowa State.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/29/2019<br>\n\t\t\t\t\tModified by: Chinmay&nbsp;Hegde</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nEstimating an unknown object from noisy, coarse, and incomplete observations is a basic problem in data science. Standard estimation algorithms first assume that the unknown object under study obeys a structured mathematical representation, and then develop optimization algorithms for recovering the parameters of the representation. Moreover, rigorous theoretical results have shown that several such algorithms are statistically optimal: no other estimation method can possibly require substantially fewer number of observations to solve the given estimation problem.\n\nHowever, despite these advances in statistical understanding, the role of computation is far less well understood. In many cases, even the best estimation methods incur a computation time proportional to a high-degree polynomial in terms of the input data size. This constitutes a significant obstacle, particularly in the \"big-data\" era where the scale of the problem is well beyond the reach of traditional statistical estimation methods. Therefore, new algorithmic approaches are necessary. This research project introduced new theoretical and computational tools for learning structured data representations from coarse observations in linear running time, thus advancing the frontier of what was computationally tractable. Linear-time algorithms are considered to be the gold standard for a broad range of statistical analysis problems.\n\nThis new approach was based on the intuition that challenging optimization problems encountered in statistics, signal processing, and data science can be circumvented if the answers are merely approximate, rather than exact. Establishing precise tradeoffs between statistical performance, approximation quality, and running time was a key focus. With this intuition, the project addressed three specific estimation problems: (i) Reconstruction of signals and images from nonlinear observations. (ii) Reconstruction of graphs from partial linear sketches. (iii) Estimating probability distributions from random samples. For each of these estimation problems, the techniques developed within the purview of the project either directly led to (or indirectly impacted the development of) fast algorithms whose running time was (nearly) linear, meaning that they are close to the best possible. The new algorithms developed in this project were validated on datasets arising from a range of application domains, including computational imaging, power networks, and transportation analytics.   \n\nThe project supported the active participation of both graduate and undergraduate students (including three individuals from under-represented minority groups) at Iowa State University, and exposed them to a span of areas including statistics, electrical engineering and computer science, and optimization. The project also supported the PI in the development of new course curricula on data science at Iowa State. \n\n\t\t\t\t\tLast Modified: 07/29/2019\n\n\t\t\t\t\tSubmitted by: Chinmay Hegde"
 }
}