{
 "awd_id": "1553169",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Designing a Predictable Database - An Overlooked Virtue",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-02-01",
 "awd_exp_date": "2021-01-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 532000.0,
 "awd_min_amd_letter_date": "2016-01-29",
 "awd_max_amd_letter_date": "2020-01-21",
 "awd_abstract_narration": "Four decades of research on database systems has mostly focused on improving average raw performance. This competition for faster performance has, understandably, neglected predictability of our database management systems. However, as database systems have become more complex, their erratic and unpredictable performance has become a major challenge facing database users and administrators alike. With the increasing reliance of mission-critical business applications on their databases, maintaining high levels of database performance (i.e., service level guarantees) is now more important than ever. Cloud users find it challenging to provision and tune their database instances, due to the highly non-linear and unpredictable nature of today's databases. Even for deployed databases, performance tuning has become somewhat of a black art, rendering qualified database administrators a scare resource.\r\n\r\nIn this project, we restore the missing virtue of predictability in the design of database systems. First, we quantify the major sources of uncertainty in a database in a principled manner. Then, by rethinking the traditional design of a database system, we architect a new generation of databases that treat predictability as a first class-citizen in their various stages of query processing, from physical design to memory management and query scheduling. Moreover, to accommodate existing database systems (which are not predictable by design), we provide effective tools and methodologies for predicting their performance more accurately. Building a predictable database in a bottom-up fashion and in a principled manner, offers great insight into improving existing database products and can instigate a radical shift in the way that future databases are designed and implemented. For more information on this project, please visit https://web.eecs.umich.edu/~mozafari/predictabledb",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Barzan",
   "pi_last_name": "Mozafari",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Barzan Mozafari",
   "pi_email_addr": "mozafari@umich.edu",
   "nsf_id": "000648726",
   "pi_start_date": "2016-01-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "3003 S. State Street",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091274",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 199533.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 147115.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 83403.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 85949.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><ol>\n<li>A fundamental understanding of the major sources of performance variance in a database system.</li>\n<li>Architectural principles for a new breed of database systems that treat predictability as a first class-citizen in their various stages of query processing.</li>\n<li>Novel lock scheduling algorithms, VATS (Variance-Aware Transaction Scheduling) and&nbsp;CATS (Contention-Aware Lock Scheduling), which have&nbsp;improved on state-of-the-art first-come, first-served policy and have been adopted by the most popoular open-source database systems (MySQL, MariaDB, Percona Server).</li>\n<li>VProfiler, as the first profiling tool capable of efficiently decomposing overall variance of transaction latencies and identifying the functions that most contribute to it.</li>\n<li>Several peer-reviewed research papers published in top-tier database conferences.</li>\n<li><span>A selectivity learning framework, called QuickSel, which can continuously learn from each query and yield increasingly more accurate selectivity estimates over time. Unlike query-driven histograms, QuickSel relies on a mixture model and a new optimization algorithm for training its model.</span></li>\n<li><span><span>The first approximate algorithm for <span>Maximum Inner Product Search (MIPS)</span> that does not require any preprocessing, while allowing users to control and bound the suboptimality of the results.</span></span></li>\n<li><span><span><span>A comprehensive study of the limitations of offline samples in approximating join queries: given an offline sampling budget, how well can one approximate the join of two tables? We have finally answered this open question in terms of two different success metrics: output size and estimator variance. We have shown that maximizing output size is easy, while there is an information-theoretical lower bound on the lowest variance achievable by any sampling strategy. We have also defined a hybrid sampling scheme that captures all combinations of stratified, universe, and Bernoulli sampling, and have shown that this scheme with our optimal parameters achieves the theoretical lower bound within a constant factor. Since computing these optimal parameters requires shuffling statistics across the network, we have also proposed a decentralized variant where each node acts autonomously using minimal statistics.</span></span></span></li>\n</ol><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/10/2021<br>\n\t\t\t\t\tModified by: Barzan&nbsp;Mozafari</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nA fundamental understanding of the major sources of performance variance in a database system.\nArchitectural principles for a new breed of database systems that treat predictability as a first class-citizen in their various stages of query processing.\nNovel lock scheduling algorithms, VATS (Variance-Aware Transaction Scheduling) and CATS (Contention-Aware Lock Scheduling), which have improved on state-of-the-art first-come, first-served policy and have been adopted by the most popoular open-source database systems (MySQL, MariaDB, Percona Server).\nVProfiler, as the first profiling tool capable of efficiently decomposing overall variance of transaction latencies and identifying the functions that most contribute to it.\nSeveral peer-reviewed research papers published in top-tier database conferences.\nA selectivity learning framework, called QuickSel, which can continuously learn from each query and yield increasingly more accurate selectivity estimates over time. Unlike query-driven histograms, QuickSel relies on a mixture model and a new optimization algorithm for training its model.\nThe first approximate algorithm for Maximum Inner Product Search (MIPS) that does not require any preprocessing, while allowing users to control and bound the suboptimality of the results.\nA comprehensive study of the limitations of offline samples in approximating join queries: given an offline sampling budget, how well can one approximate the join of two tables? We have finally answered this open question in terms of two different success metrics: output size and estimator variance. We have shown that maximizing output size is easy, while there is an information-theoretical lower bound on the lowest variance achievable by any sampling strategy. We have also defined a hybrid sampling scheme that captures all combinations of stratified, universe, and Bernoulli sampling, and have shown that this scheme with our optimal parameters achieves the theoretical lower bound within a constant factor. Since computing these optimal parameters requires shuffling statistics across the network, we have also proposed a decentralized variant where each node acts autonomously using minimal statistics.\n\n\n\t\t\t\t\tLast Modified: 08/10/2021\n\n\t\t\t\t\tSubmitted by: Barzan Mozafari"
 }
}