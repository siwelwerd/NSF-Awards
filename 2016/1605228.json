{
 "awd_id": "1605228",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "A Shared Autonomy Approach to Robotic Arm Assistance with Daily Activities",
 "cfda_num": "47.041",
 "org_code": "07020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Grace Hwang",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 298503.0,
 "awd_amount": 298503.0,
 "awd_min_amd_letter_date": "2016-08-23",
 "awd_max_amd_letter_date": "2020-11-05",
 "awd_abstract_narration": "Persons with high-level paralysis rely on the care of others and the modification of their environment for accomplishing the activities of daily living (ADL). Due to the degree of support required, paralysis is costly to provide care for. Recovering the ability to manipulate through technological means would lead to significant improvements to the quality of life for the user, and would reduce the long-term economic impact of paralysis and contribute to society by freeing up caregiver time and labor (usually a family-member or friend). To that end, the project proposes to design and validate a wheelchair-mounted robotic-arm with an augmented reality interface for enabling non-tactile human-robot interaction. Importantly, the design will involve a multi-modal interface approach, which includes a recently developed tongue drive interface, a head orientation sensor, and a speech recognition system. User-centric and participatory design methods will ensure that the engineered system will be seen favorably by persons with tetraplegia. The award also supports fundamental research into the design of collaborative human-robot assistive devices, including the interface design and the underlying robot vision and planning algorithms. The research contribution includes improved understanding on how to effectively coordinate the higher level reasoning and thought processes of humans with the autonomous operation capabilities and limitations of current robotic arms. The social significance of the robotics application will be capitalized to create engaging educational and outreach activities in promotion of engineering mathematics.\r\n\r\nThe long-term goal is to transform the lives of wheelchair bound persons with limited to no manipulation abilities by enabling them to independently perform the activities of daily living (ADL) irrespective of their environment. Technology meeting the varied needs of the ADL tends to have high control complexity, which impedes adoption. It is essential for these technologies to request high-level (guiding) commands rather than low-level control signals, and to request feedback in a manner compatible with the user's conception of the world. The research goal is to engineer and validate a wheelchair-fitted robotic-arm with an augmented reality interface and a multi-modal user interface for coupling the human command and intent control loop with the robotic-arm decision and control loop.  The novelty of the proposed system is that it is a shared control and a shared sensing assistive technology. The coupled system requires human input to overcome the perceptual limitations of robot vision, and employs robotic planning and manipulation to overcome the physical limitations of the user. By involving the human for scene interpretation and by providing ego-centric information to the robot arm, the coupled system minimizes and simplifies user input during complex manipulation tasks. The associated research objectives are to (I) engineer a human-robot augmented reality interface system with coupled feedback to the two systems (human and robot) for communicating intent and requesting high-level user feedback for complex manipulation tasks, (II) identify the manipulation assistance needs and user interface design through a participatory design process, (III) evaluate the user interface with respect to the desired command and control objectives, and (IV) assess the impact of the operational system with respect to task performance and cognitive burden. Engineering the unique shared control and shared sensing system and achieving the proposed research objectives would transform the way that assistive robotic technologies couple the user to technology. Incorporating the research into existing education and outreach activities would promote engineering and robotics to students and prepare them for participation in an increasingly automated world.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CBET",
 "org_div_long_name": "Division of Chemical, Bioengineering, Environmental, and Transport Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Patricio",
   "pi_last_name": "Vela",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Patricio A Vela",
   "pi_email_addr": "pvela@ece.gatech.edu",
   "nsf_id": "000495667",
   "pi_start_date": "2016-08-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Maysam",
   "pi_last_name": "Ghovanloo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Maysam Ghovanloo",
   "pi_email_addr": "mgh@getech.edu",
   "nsf_id": "000320172",
   "pi_start_date": "2016-08-23",
   "pi_end_date": "2019-09-09"
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "85 Fifth Street NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320250",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "534200",
   "pgm_ele_name": "Disability & Rehab Engineering"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 298503.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The research goal of the project was to design and validate an augmented reality interface for a wheelchair mountable robotic arm that couples the human command and intent decision loop with the robotic arm perceive, plan, act loop. The coupling operates as a shared autonomy approach for teleguidance of assistive robots. The intent behind the shared autonomy design was to create an assistive robot arm for enabling persons with tetraplegia to interact with objects in the world around them using the robotic arm as a proxy. A tongue-drive system was used to permit communication between the user and the shared autonomy robotic arm due to it being a hands-free interface. Three main activity threads were pursued. They were categorized as: user informed design and performance evaluation, algorithm development for perception-in-the-loop algorithms focusing on facilitating shared autonomy, and integration into a single operational shared autonomy assistive robot arm.</p>\n<p>Since state-of-the-art manipulation and scene interpretation methods were not fully capable of completing the desired tasks, effort was put into developing high performance grasping and task-relevant scene interpretation algorithms enabled by advances in machine learning. The scene interpretation algorithms focused on identifying object affordances in the immediate, visible vicinity of the user. Object affordances are action opportunities that the manipulator can perform with the object, or between pairs of objects. Examples of the latter include cutting with a knife, pouring into a container, scooping, etc. The importance of recovering affordances from the local scene are that they can inform a dynamic menuing system within the augmented reality interface that communicates to the user what activities the manipulator can perform based on its visual awareness. All manipulation starts by grasping an object to do something with it, thus advancing the state-of-the-art in grasping is essential to having a highly functional assistive robot. Our efforts on these fronts have led to one advanced and two state-of-the-art single-view grasping algorithms that employ different geometric properties to identify where to grasp. They have also led to state-of-the-art affordance segmentation and affordance pose estimation algorithms that are object-agnostic and can hypothesize the affordances of novel or unknown objects. All of these results also illuminate what critical geometry is necessary to encode within machine learning algorithms for near-human performance. The derived algorithms establish the baseline capabilities of the robotic arm.</p>\n<p>The augmented reality interface was integrated into a robot autonomy system programmed with the manipulation algorithms implemented at the time of integration. Integration included the design of the menuing system; communication between the augmented reality (AR) hardware, the tongue drive system, and the robot autonomy stack; and sharing of perceptual data between the AR hardware and the robot perception sub-system. The integrated system could complete manipulation tasks in a hands-free manner by the human user. Human assistance is required to don the augmented reality and tongue drive (AR+TDS) hardware, which was mounted to a headset to operate as a single head-mounted device.</p>\n<p>Informal feedback from persons with tetraplegia provided design guidance and constraints on the operational design of the assistive robot arm. Human subjects experiments with able-bodied persons provided design guidance on limiting or constraining factors of the AR+TDS system, primary elements to improve, and other subjective feedback on the shared autonomy robotic arm. The same experiments could not be done with persons with tetraplegia due to the covid-19 pandemic. Prime areas for improvement include enhancing the TDS command recognition capabilities to reduce fatigue, or replacing the TDS with other input modalities that minimize donned hardware (such as voice commands). The users had neutral to favorable views of the shared autonomy system (no negative views). The majority of human subjects had a positive perception of the shared autonomy assistive robot and did not report any cognitive burden regarding its use.&nbsp;</p>\n<p>Additionally, human subjects experiments captured several performance metrics of novice users, which were compared to an expert user. The combined system was shown to require a short use period before performance peaked, indicating that untrained users could quickly learn to use the shared autonomy interface. Peak performance was high relative to published works investigating other interface modalities for hands-free operation of an assistive robot arm. Furthermore, the time cost of the interface was found to be comparable to or lower than existing interfaces. The system has one of the best task success to command input performance outcomes. Importantly, the limiting factors for the system were software-based, meaning that performance enhancements are easy to achieve. The ease of upgrading performance is important because perceptions of usability were linked to robot performance. Since many of the performance metrics can be immediately improved through software enhancements, we conjecture that enhanced autonomy will lead to increased perceptions of usability.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/04/2022<br>\n\t\t\t\t\tModified by: Patricio&nbsp;A&nbsp;Vela</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe research goal of the project was to design and validate an augmented reality interface for a wheelchair mountable robotic arm that couples the human command and intent decision loop with the robotic arm perceive, plan, act loop. The coupling operates as a shared autonomy approach for teleguidance of assistive robots. The intent behind the shared autonomy design was to create an assistive robot arm for enabling persons with tetraplegia to interact with objects in the world around them using the robotic arm as a proxy. A tongue-drive system was used to permit communication between the user and the shared autonomy robotic arm due to it being a hands-free interface. Three main activity threads were pursued. They were categorized as: user informed design and performance evaluation, algorithm development for perception-in-the-loop algorithms focusing on facilitating shared autonomy, and integration into a single operational shared autonomy assistive robot arm.\n\nSince state-of-the-art manipulation and scene interpretation methods were not fully capable of completing the desired tasks, effort was put into developing high performance grasping and task-relevant scene interpretation algorithms enabled by advances in machine learning. The scene interpretation algorithms focused on identifying object affordances in the immediate, visible vicinity of the user. Object affordances are action opportunities that the manipulator can perform with the object, or between pairs of objects. Examples of the latter include cutting with a knife, pouring into a container, scooping, etc. The importance of recovering affordances from the local scene are that they can inform a dynamic menuing system within the augmented reality interface that communicates to the user what activities the manipulator can perform based on its visual awareness. All manipulation starts by grasping an object to do something with it, thus advancing the state-of-the-art in grasping is essential to having a highly functional assistive robot. Our efforts on these fronts have led to one advanced and two state-of-the-art single-view grasping algorithms that employ different geometric properties to identify where to grasp. They have also led to state-of-the-art affordance segmentation and affordance pose estimation algorithms that are object-agnostic and can hypothesize the affordances of novel or unknown objects. All of these results also illuminate what critical geometry is necessary to encode within machine learning algorithms for near-human performance. The derived algorithms establish the baseline capabilities of the robotic arm.\n\nThe augmented reality interface was integrated into a robot autonomy system programmed with the manipulation algorithms implemented at the time of integration. Integration included the design of the menuing system; communication between the augmented reality (AR) hardware, the tongue drive system, and the robot autonomy stack; and sharing of perceptual data between the AR hardware and the robot perception sub-system. The integrated system could complete manipulation tasks in a hands-free manner by the human user. Human assistance is required to don the augmented reality and tongue drive (AR+TDS) hardware, which was mounted to a headset to operate as a single head-mounted device.\n\nInformal feedback from persons with tetraplegia provided design guidance and constraints on the operational design of the assistive robot arm. Human subjects experiments with able-bodied persons provided design guidance on limiting or constraining factors of the AR+TDS system, primary elements to improve, and other subjective feedback on the shared autonomy robotic arm. The same experiments could not be done with persons with tetraplegia due to the covid-19 pandemic. Prime areas for improvement include enhancing the TDS command recognition capabilities to reduce fatigue, or replacing the TDS with other input modalities that minimize donned hardware (such as voice commands). The users had neutral to favorable views of the shared autonomy system (no negative views). The majority of human subjects had a positive perception of the shared autonomy assistive robot and did not report any cognitive burden regarding its use. \n\nAdditionally, human subjects experiments captured several performance metrics of novice users, which were compared to an expert user. The combined system was shown to require a short use period before performance peaked, indicating that untrained users could quickly learn to use the shared autonomy interface. Peak performance was high relative to published works investigating other interface modalities for hands-free operation of an assistive robot arm. Furthermore, the time cost of the interface was found to be comparable to or lower than existing interfaces. The system has one of the best task success to command input performance outcomes. Importantly, the limiting factors for the system were software-based, meaning that performance enhancements are easy to achieve. The ease of upgrading performance is important because perceptions of usability were linked to robot performance. Since many of the performance metrics can be immediately improved through software enhancements, we conjecture that enhanced autonomy will lead to increased perceptions of usability.\n\n\t\t\t\t\tLast Modified: 01/04/2022\n\n\t\t\t\t\tSubmitted by: Patricio A Vela"
 }
}