{
 "awd_id": "1657939",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Transforming data analysis via new algorithms for feature extraction",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2016-06-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 329094.0,
 "awd_amount": 329094.0,
 "awd_min_amd_letter_date": "2016-08-30",
 "awd_max_amd_letter_date": "2018-06-29",
 "awd_abstract_narration": "Analysis and exploration of data, including classification, inference, and retrieval, are ubiquitous tasks in science and applied fields. Given any such task, a fundamental paradigm is the extraction of features that are relevant. In the design of algorithms for the analysis and exploration of data, feature extraction techniques act as basic building blocks or primitives that can be combined to model complex behavior. Some of the fundamental feature extraction tools include Principal Component Analysis (PCA), Independent Component Analysis (ICA), and half-space-based learning and classification. Data rarely satisfy the precise assumptions of these models and feature extraction tools, and combining these tools amplifies errors. This motivates the challenging task of designing new algorithms that are robust against noise and that can be combined as building blocks while keeping the error propagation under control.\r\n\r\nThe proposed work will:\r\n(1) Raise ICA from a very successful practical tool to an algorithmic primitive with strong theoretical guarantees and applicability to a rich family of problems beyond independence.\r\n(2) Find reasonable assumptions and algorithms that allow efficient learning of intersections of half-spaces.\r\n(3) Systematically study the following well-motivated refinement of PCA known as the subset selection problem. This refinement aims to select relevant features among the given features of the input data, unlike PCA, which creates new and possibly artificial features.\r\n\r\nNew feature extraction algorithms enhance the toolbox available to researchers in data-intensive fields such as biology, signal processing and computer vision. They also enable improved data analysis by practitioners in security, marketing, business and government processes, and essentially any field that involves the analysis of feature-rich data. The proposed work includes the implementation of the more practical algorithms.\r\n\r\nEducation and outreach aspects of this project include the mentoring of young researchers, the design of a new course for graduate and undergraduate students incorporating some of the PI's research, and the involvement of pre-college students and local communities into science and research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Luis",
   "pi_last_name": "Rademacher",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Luis Rademacher",
   "pi_email_addr": "lrademac@ucdavis.edu",
   "nsf_id": "000558524",
   "pi_start_date": "2016-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956186134",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 43066.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 92308.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 95302.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 98418.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>One can think of a dataset as a set of data points where each point has a number of explicit features. One aspect of the understanding of a given dataset is the identification of relevant latent features computed from the explicitly given features. The project improved this aspect in several ways, by designing methods that are guaranteed to work in broader situations. One of these situations is the case when the dataset has heavy tails, that is, the case where extreme values are not particularly rare. This is a case where many traditional methods have no guarantees. A second situation is the underdetermined case, that is, when the number of latent features is larger than the number of explicit features.</p>\n<p>The proposed methods established new connections between data analysis and high-dimensional geometry. In the context of this connection, the Principal Investigator organized a workshop on high-dimensional geometry for the American Mathematical Society, which included some of the main experts in the subject.</p>\n<p>All the proposed methods have strong mathematical guarantees and several of them led to practical implementations, including algorithms for Independent Component Analysis, a standard feature extraction technique, when the data is corrupted by noise and when the data has heavy tails.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2021<br>\n\t\t\t\t\tModified by: Luis&nbsp;Rademacher</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOne can think of a dataset as a set of data points where each point has a number of explicit features. One aspect of the understanding of a given dataset is the identification of relevant latent features computed from the explicitly given features. The project improved this aspect in several ways, by designing methods that are guaranteed to work in broader situations. One of these situations is the case when the dataset has heavy tails, that is, the case where extreme values are not particularly rare. This is a case where many traditional methods have no guarantees. A second situation is the underdetermined case, that is, when the number of latent features is larger than the number of explicit features.\n\nThe proposed methods established new connections between data analysis and high-dimensional geometry. In the context of this connection, the Principal Investigator organized a workshop on high-dimensional geometry for the American Mathematical Society, which included some of the main experts in the subject.\n\nAll the proposed methods have strong mathematical guarantees and several of them led to practical implementations, including algorithms for Independent Component Analysis, a standard feature extraction technique, when the data is corrupted by noise and when the data has heavy tails.\n\n\t\t\t\t\tLast Modified: 10/29/2021\n\n\t\t\t\t\tSubmitted by: Luis Rademacher"
 }
}