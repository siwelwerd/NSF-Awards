{
 "awd_id": "1617315",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Efficient In-Memory Computing Architecture Based on RRAM Crossbar Arrays",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2016-06-15",
 "awd_exp_date": "2019-05-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2016-06-03",
 "awd_max_amd_letter_date": "2016-06-03",
 "awd_abstract_narration": "Conventional digital computers, with separate processor and memory, face increasing challenges in today?s ?big data? era as the constant movement of data between the processor and the memory causes significant delay and energy consumption. This problem, termed the ?von Neumann bottleneck?, affects performance for both complex tasks such as image and video processing as well as embedded applications such as distributed sensor networks where high speed and low power are critical. This project aims to develop a new computer architecture based on emerging resistive random access memory (RRAM) crossbar arrays, where the memory and logic functions exist at the same physical locations and computation is achieved in the physical memory by directly reading out stored outputs for a given operation. By leveraging the unique properties of emerging devices with a new computation architecture, this project will profoundly advance the frontier of nanoscale device and computer architecture research, and enable high-speed and low-power computation for applications ranging from servers to Internet of Things (IoTs). This program will have significant impact on the research community and the semiconductor industry, while providing interdisciplinary training of graduate and undergraduate students, and draw broad participation of students of different levels and backgrounds in collaborative research and education.\r\n\r\nThis approach takes full advantage of the high-storage density, non-volatility, and random-access capabilities of RRAM arrays. RRAM devices operate based on the resistance change when the device is subjected to a programming or reset pulse. Consequently, the resistance not only stores information but also directly regulates information (i.e. current) flow in the circuit, thus implementing both memory and logic functions simultaneously. Previous studies on RRAM-based circuits focus on soft computing tasks where the environment and the tasks are complex but inaccuracies and approximations are tolerated. This project aims to develop a computing system that can perform accurate arithmetic operations efficiently using RRAM crossbar arrays. The system will be optimized for throughput and energy, and experimentally demonstrated using fabricated high-density RRAM arrays, along with the development of a toolset for design automation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Wei",
   "pi_last_name": "Lu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wei Lu",
   "pi_email_addr": "wluee@eecs.umich.edu",
   "nsf_id": "000492897",
   "pi_start_date": "2016-06-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "1301 Beal Ave",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092122",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to develop a computing system that can perform key operations for a broad range of data-intensive tasks including Artificial Intelligence (AI) and numerical simulations efficiently, using a new hardware architecture based on arrays of resistive random-access memory (RRAM) devices. RRAMs are two-terminal devices whose resistance can be modulated by applying a programming voltage, and the new resistance values can be maintained even without power. Consequently, the resistance not only stores information but also can be used to directly regulate information (i.e. current) flow in the circuit, thus allowing both memory and logic functions to be simultaneously implemented in the same circuit. This type of ?in-memory? computing system will significantly reduce the energy and latency costs associated with moving large amounts of data between memory and the processor in a conventional system, and can lead to highly efficient computing hardware.</p>\n<p>&nbsp;</p>\n<p>The project will test the working principles of this RRAM-based in-memory computing concept, build prototypes, and develop necessary simulation and software systems to support the hardware operation. The system will be optimized for throughput and energy, and experimentally demonstrated using fabricated high-density RRAM arrays, and supported by the development of a toolset for design automation and validation. The goal is to take full advantage of the high-storage density, non-volatility, and random-access capabilities of RRAM arrays, and allow the system to provide efficient computing capabilities beyond data storage.</p>\n<p>&nbsp;</p>\n<p>Specifically, during the past 3 years, the PI?s team has made significant progress in the proposed tasks, and obtained the following outcomes:</p>\n<p>&nbsp;1)&nbsp;&nbsp;&nbsp;&nbsp; We have developed the framework for a complete, RRAM-based in-memory computing system that can perform efficient arithmetic, data storage, and neuromorphic operations on a common physical platform. Examples include:</p>\n<p>a)&nbsp;&nbsp;&nbsp;&nbsp; We experimentally demonstrated that RRAM-arrays can be used to efficiently perform vector-matrix operations in machine learning and neuromorphic computing tasks (Image 1). This work was published in 2017 in Nature Nanotechnology.</p>\n<p>b)&nbsp;&nbsp;&nbsp;&nbsp; We experimentally verified that RRAM-based in-memory computing systems can not only solve ?soft? computing tasks such as neural networks, but also ?hard? computing tasks that require high precision, i.e. solving partial differential equations (PDEs) (Image 2). This work was published in 2018 in Nature Electronics.</p>\n<p>c)&nbsp;&nbsp;&nbsp;&nbsp; We developed a new reservoir computing approach by utilizing the internal device dynamics to directly process temporal data and nonlinear time-series analysis (Image 3). This work was published in 2017 in Nature Communications.</p>\n<p>d)&nbsp;&nbsp;&nbsp;&nbsp; We have developed new RRAM devices based on Li intercalation in 2D materials, and developed ionic-coupled networks that natively show synaptic cooperation and synaptic competition effects. (Image 4). This work was published in Nature Materials in 2019.</p>\n<p>2)&nbsp;&nbsp;&nbsp;&nbsp; We have performed extensive analysis on the performance of the RRAM-based in-memory computing system and showed that it is flexible, scalable, fast and highly energy efficient.</p>\n<p>3)&nbsp;&nbsp;&nbsp;&nbsp; We have developed and fabricated a complete RRAM-based in-memory computing hardware system, by integrating an RRAM array directly on top of CMOS circuitry. The system is programmable and can map different machine learning algorithms on chip, with the RRAM array performing the compute-intensive vector-matrix multiplication tasks. (Image 5). This work was published in 2019 in Nature Electronics as a Cover Article.</p>\n<p>The results have been broadly disseminated to the research community and the general public. The PI and his students have published 14 papers in prestigious journals, given invited talks at major internal conferences, and written 1 book chapter on findings of this project. 2 Ph.D. students and 1 Postdoc researcher have been systematically trained based on funding from NSF. The findings have also been incorporated in a graduate-level course the PI developed, and shared with the general public through publications, patent applications, and website development. Several new research directions have been established based on findings in this project. The findings have also inspired a number of new students who have since become actively engaged in research in related topics.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/13/2019<br>\n\t\t\t\t\tModified by: Wei&nbsp;Lu</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568400312609_Image5--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568400312609_Image5--rgov-800width.jpg\" title=\"Image 5. Integrated RRAM chip that can perform different machine-learning tasks.\"><img src=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568400312609_Image5--rgov-66x44.jpg\" alt=\"Image 5. Integrated RRAM chip that can perform different machine-learning tasks.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">a. Cover image showing RRAM array integrated on top of CMOS circuitry. b. Optical image of the integrated chip and test board. c. Example of a bi-layer network mapped on the chip.</div>\n<div class=\"imageCredit\">Wei Lu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wei&nbsp;Lu</div>\n<div class=\"imageTitle\">Image 5. Integrated RRAM chip that can perform different machine-learning tasks.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568399794249_Image2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568399794249_Image2--rgov-800width.jpg\" title=\"Image 2. Using RRAM array to perform in-memory arithmetic operations.\"><img src=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568399794249_Image2--rgov-66x44.jpg\" alt=\"Image 2. Using RRAM array to perform in-memory arithmetic operations.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">a-c. schematic of mapping a partial differential equation (PDE) system on to the RRAM crossbar hardware, where the RRAM array performs vector-matrix multiplications. d. experimental setup. e. Results of solving a Poisson equation.</div>\n<div class=\"imageCredit\">Wei Lu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wei&nbsp;Lu</div>\n<div class=\"imageTitle\">Image 2. Using RRAM array to perform in-memory arithmetic operations.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568400046049_Image3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568400046049_Image3--rgov-800width.jpg\" title=\"Image 3. Using the internal dynamic of RRAM devices to directly process features in streaming data.\"><img src=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568400046049_Image3--rgov-66x44.jpg\" alt=\"Image 3. Using the internal dynamic of RRAM devices to directly process features in streaming data.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">In this example, data in an image are converted to streaming inputs and fed to RRAM devices acting as a reservoir. The input excites the reservoir to distinct states, depending on the temporal features in the input. The mapped outputs are then processed by a readout layer.</div>\n<div class=\"imageCredit\">Wei Lu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wei&nbsp;Lu</div>\n<div class=\"imageTitle\">Image 3. Using the internal dynamic of RRAM devices to directly process features in streaming data.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568400204218_Image4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568400204218_Image4--rgov-800width.jpg\" title=\"Image 4. RRAM and ionic-coupled network based on 2D materials.\"><img src=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568400204218_Image4--rgov-66x44.jpg\" alt=\"Image 4. RRAM and ionic-coupled network based on 2D materials.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">a. RRAM device based on Li ion intercalation in MoS2. b. schematic of a biologic network through ion/protein coupling. d. Scanning electron image of a similar network through ion coupling. d. results from such a network showing synaptic competition.</div>\n<div class=\"imageCredit\">Wei Lu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wei&nbsp;Lu</div>\n<div class=\"imageTitle\">Image 4. RRAM and ionic-coupled network based on 2D materials.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568399671838_Image1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568399671838_Image1--rgov-800width.jpg\" title=\"Image 1. Sparse coding algorithm implemented in a 32x32 RRAM array.\"><img src=\"/por/images/Reports/POR/2019/1617315/1617315_10431024_1568399671838_Image1--rgov-66x44.jpg\" alt=\"Image 1. Sparse coding algorithm implemented in a 32x32 RRAM array.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Examples of sparse coding using RRAM-based forward and backward vector-matrix multiplication operations to obtain the neuron output and calculate the residual.</div>\n<div class=\"imageCredit\">Wei Lu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wei&nbsp;Lu</div>\n<div class=\"imageTitle\">Image 1. Sparse coding algorithm implemented in a 32x32 RRAM array.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project aims to develop a computing system that can perform key operations for a broad range of data-intensive tasks including Artificial Intelligence (AI) and numerical simulations efficiently, using a new hardware architecture based on arrays of resistive random-access memory (RRAM) devices. RRAMs are two-terminal devices whose resistance can be modulated by applying a programming voltage, and the new resistance values can be maintained even without power. Consequently, the resistance not only stores information but also can be used to directly regulate information (i.e. current) flow in the circuit, thus allowing both memory and logic functions to be simultaneously implemented in the same circuit. This type of ?in-memory? computing system will significantly reduce the energy and latency costs associated with moving large amounts of data between memory and the processor in a conventional system, and can lead to highly efficient computing hardware.\n\n \n\nThe project will test the working principles of this RRAM-based in-memory computing concept, build prototypes, and develop necessary simulation and software systems to support the hardware operation. The system will be optimized for throughput and energy, and experimentally demonstrated using fabricated high-density RRAM arrays, and supported by the development of a toolset for design automation and validation. The goal is to take full advantage of the high-storage density, non-volatility, and random-access capabilities of RRAM arrays, and allow the system to provide efficient computing capabilities beyond data storage.\n\n \n\nSpecifically, during the past 3 years, the PI?s team has made significant progress in the proposed tasks, and obtained the following outcomes:\n\n 1)     We have developed the framework for a complete, RRAM-based in-memory computing system that can perform efficient arithmetic, data storage, and neuromorphic operations on a common physical platform. Examples include:\n\na)     We experimentally demonstrated that RRAM-arrays can be used to efficiently perform vector-matrix operations in machine learning and neuromorphic computing tasks (Image 1). This work was published in 2017 in Nature Nanotechnology.\n\nb)     We experimentally verified that RRAM-based in-memory computing systems can not only solve ?soft? computing tasks such as neural networks, but also ?hard? computing tasks that require high precision, i.e. solving partial differential equations (PDEs) (Image 2). This work was published in 2018 in Nature Electronics.\n\nc)     We developed a new reservoir computing approach by utilizing the internal device dynamics to directly process temporal data and nonlinear time-series analysis (Image 3). This work was published in 2017 in Nature Communications.\n\nd)     We have developed new RRAM devices based on Li intercalation in 2D materials, and developed ionic-coupled networks that natively show synaptic cooperation and synaptic competition effects. (Image 4). This work was published in Nature Materials in 2019.\n\n2)     We have performed extensive analysis on the performance of the RRAM-based in-memory computing system and showed that it is flexible, scalable, fast and highly energy efficient.\n\n3)     We have developed and fabricated a complete RRAM-based in-memory computing hardware system, by integrating an RRAM array directly on top of CMOS circuitry. The system is programmable and can map different machine learning algorithms on chip, with the RRAM array performing the compute-intensive vector-matrix multiplication tasks. (Image 5). This work was published in 2019 in Nature Electronics as a Cover Article.\n\nThe results have been broadly disseminated to the research community and the general public. The PI and his students have published 14 papers in prestigious journals, given invited talks at major internal conferences, and written 1 book chapter on findings of this project. 2 Ph.D. students and 1 Postdoc researcher have been systematically trained based on funding from NSF. The findings have also been incorporated in a graduate-level course the PI developed, and shared with the general public through publications, patent applications, and website development. Several new research directions have been established based on findings in this project. The findings have also inspired a number of new students who have since become actively engaged in research in related topics.\n\n\t\t\t\t\tLast Modified: 09/13/2019\n\n\t\t\t\t\tSubmitted by: Wei Lu"
 }
}