{
 "awd_id": "1622256",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Cloud Based Artificial Intelligence for Trend Analysis Using Sensor Data",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rick Schwerdtfeger",
 "awd_eff_date": "2016-07-01",
 "awd_exp_date": "2017-06-30",
 "tot_intn_awd_amt": 225000.0,
 "awd_amount": 225000.0,
 "awd_min_amd_letter_date": "2016-06-20",
 "awd_max_amd_letter_date": "2016-06-20",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase 1 project is to address the problem of data interpretation, one of the most important and fastest growing issues caused by the influx of wearable technologies. As with all technology, wearable devices are increasing in popularity and decreasing in cost every day. Businesses rushing to catch this wave of technology paradigm are met with the complex problem of how to interpret the data gathered in a way that is accurate and useful to consumers. Many of these businesses are companies with products that were previously completely unrelated with computer or smartphone technologies. As such, they do not have the in-house expertise to not only correctly gather such data, but then analyze it for patterns that could be deemed useful in identifying behavior or conditions for the consumer. By providing a boxed solution that makes machine learning based data analysis possible for the average engineer, our project is aimed to help these businesses cross that hurdle.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase 1 project seeks to bring Artificial Intelligence and Machine Learning systems for use in the hands of non-data/computer scientists. While machine learning and AI techniques are widely used these days in many applications ranging from Google search to Uber rides, they remain fairly esoteric topics with a high learning curve just to understand, let alone apply. We plan to address this issue differently from previous competitors by building a highly intuitive web UI on top of our existing hardware sensor platform. This allows us to leverage the data gathering and processing consistency of our hardware, along with our proprietary SDKs to ensure properly labelled and clean data. As a result, we will have a much easier time developing basic digital processing filters as well as applying machine learning techniques to the data in order to solve generic classification problems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Laura",
   "pi_last_name": "Kassovic",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Laura Kassovic",
   "pi_email_addr": "laura@mbientlab.com",
   "nsf_id": "000696041",
   "pi_start_date": "2016-06-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "MbientLab",
  "inst_street_address": "848 GIRARD ST",
  "inst_street_address_2": "",
  "inst_city_name": "SAN FRANCISCO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "4084069149",
  "inst_zip_code": "941341920",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "CA15",
  "org_lgl_bus_name": "MBIENTLAB INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "KSUFDPTF77H5"
 },
 "perf_inst": {
  "perf_inst_name": "MbientLab",
  "perf_str_addr": "848 Girard St",
  "perf_city_name": "San Francisco",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "941347334",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "CA15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "033E",
   "pgm_ref_txt": "Smart and responsive structures"
  },
  {
   "pgm_ref_code": "152E",
   "pgm_ref_txt": "Cyber-Physical Systems"
  },
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "8033",
   "pgm_ref_txt": "Hardware Software Integration"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 225000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In our Phase 1 proposal, we identified a major problem that is growing more prevalent in the Internet of Things and wearable industry. With billions of tech devices floating around in all shapes and forms, the amount of data gathered is massive in scale. The type of data gathered by these devices is also often not well defined. For example, it is easy to collect 3-axis accelerometer data from a person swinging a tennis racquet, but it is rather difficult to determine what the raw data means without a team of experts versed in both data analysis and tennis swing techniques. The goal of our Phase 1 project was to reduce the amount of expertise needed in both the mathematical sciences and the application sciences when interpreting the data from all types of motion sensors. We proposed to solve at least a major part of this problem by building a sensor cloud platform with &ldquo;automagic&rdquo; machine learning tools for easy data visualization, filtering, and interpretation.</p>\n<p>Building such a cloud platform is extremely challenging, due to various factors that are hard to control, such as the type of hardware used, type of data, type of sensor, operating system type, file format, and etc. We realized very early on that in order to have a chance of being successful, we would have to limit our initial scope to create a proof of concept, then slowly expand our functionality as we gather more users and feedback.&nbsp;</p>\n<p>Based on our previous work in designing various algorithms for predicting, analyzing, and classifying motion for our customers, we had confidence in leveraging that experience into an AI driven pattern recognition system for motion data as long as we had control of the input data types and format. Thus, we set out to build a limited-scope cloud platform that would initially work exclusively with our MetaWear family of devices. This allowed us to own a fully vertically integrated solution (hardware to cloud) and make sure that the incoming data stream was 100% suitable and compatible with our cloud platform</p>\n<p>Our work in Phase I served as the basis of research on which we built a successful and complete prototype of the machine learning platform as described above. We provided a user-friendly interface that allows anyone, even those with minimal technical expertise, to analyze and distinguish the difference between different types of motion and movements (e.g. jumping vs. running, near vs. far, punching vs. crouching, knee bent vs. knee extended) based on their uploaded sensor data. We achieved this through the use of a carefully tuned pre-processor in combination with carefully selected SVM kernel parameters. At the end of our Phase I project, we managed to achieve &gt;80% accuracy in the majority of motion classification problems.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/06/2017<br>\n\t\t\t\t\tModified by: Laura&nbsp;Kassovic</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1622256/1622256_10434075_1498813258179_Capture_combined2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1622256/1622256_10434075_1498813258179_Capture_combined2--rgov-800width.jpg\" title=\"Pattern Training + Matching\"><img src=\"/por/images/Reports/POR/2017/1622256/1622256_10434075_1498813258179_Capture_combined2--rgov-66x44.jpg\" alt=\"Pattern Training + Matching\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Top 3 patterns are the \"training\" features. Bottom pattern is the \"testing\" feature, which is a match based on the training data set.</div>\n<div class=\"imageCredit\">MbientLab Inc.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Laura&nbsp;Kassovic</div>\n<div class=\"imageTitle\">Pattern Training + Matching</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1622256/1622256_10434075_1498813565113_Capture_combined3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1622256/1622256_10434075_1498813565113_Capture_combined3--rgov-800width.jpg\" title=\"Matched Patterns Identified By ML Algorithm\"><img src=\"/por/images/Reports/POR/2017/1622256/1622256_10434075_1498813565113_Capture_combined3--rgov-66x44.jpg\" alt=\"Matched Patterns Identified By ML Algorithm\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Additional \"test\" samples that were identified as matching patterns based on the trained Machine Learning algorithm.</div>\n<div class=\"imageCredit\">MbientLab Inc.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Laura&nbsp;Kassovic</div>\n<div class=\"imageTitle\">Matched Patterns Identified By ML Algorithm</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIn our Phase 1 proposal, we identified a major problem that is growing more prevalent in the Internet of Things and wearable industry. With billions of tech devices floating around in all shapes and forms, the amount of data gathered is massive in scale. The type of data gathered by these devices is also often not well defined. For example, it is easy to collect 3-axis accelerometer data from a person swinging a tennis racquet, but it is rather difficult to determine what the raw data means without a team of experts versed in both data analysis and tennis swing techniques. The goal of our Phase 1 project was to reduce the amount of expertise needed in both the mathematical sciences and the application sciences when interpreting the data from all types of motion sensors. We proposed to solve at least a major part of this problem by building a sensor cloud platform with \"automagic\" machine learning tools for easy data visualization, filtering, and interpretation.\n\nBuilding such a cloud platform is extremely challenging, due to various factors that are hard to control, such as the type of hardware used, type of data, type of sensor, operating system type, file format, and etc. We realized very early on that in order to have a chance of being successful, we would have to limit our initial scope to create a proof of concept, then slowly expand our functionality as we gather more users and feedback. \n\nBased on our previous work in designing various algorithms for predicting, analyzing, and classifying motion for our customers, we had confidence in leveraging that experience into an AI driven pattern recognition system for motion data as long as we had control of the input data types and format. Thus, we set out to build a limited-scope cloud platform that would initially work exclusively with our MetaWear family of devices. This allowed us to own a fully vertically integrated solution (hardware to cloud) and make sure that the incoming data stream was 100% suitable and compatible with our cloud platform\n\nOur work in Phase I served as the basis of research on which we built a successful and complete prototype of the machine learning platform as described above. We provided a user-friendly interface that allows anyone, even those with minimal technical expertise, to analyze and distinguish the difference between different types of motion and movements (e.g. jumping vs. running, near vs. far, punching vs. crouching, knee bent vs. knee extended) based on their uploaded sensor data. We achieved this through the use of a carefully tuned pre-processor in combination with carefully selected SVM kernel parameters. At the end of our Phase I project, we managed to achieve &gt;80% accuracy in the majority of motion classification problems.\n\n\t\t\t\t\tLast Modified: 07/06/2017\n\n\t\t\t\t\tSubmitted by: Laura Kassovic"
 }
}