{
 "awd_id": "1650390",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "DCL: NSF INCLUDES Conference on Multi-Scale Evaluation in STEM Education",
 "cfda_num": "47.041",
 "org_code": "07050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Paige Smith",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 248397.0,
 "awd_amount": 248397.0,
 "awd_min_amd_letter_date": "2016-09-12",
 "awd_max_amd_letter_date": "2016-09-12",
 "awd_abstract_narration": "This project will develop, organize and host a Conference to assist in the planning of the NSF INCLUDES (Inclusion across the Nation of Communities of Learners of Underrepresented Discoverers in Engineering and Science) Alliances and National Network. The focus of this project is the design and implementation of program evaluations necessary for the INCLUDES Alliances and National Network. The conference will be the first to emphasize the multiple scales at which evaluations occur for programs that include components based at single institutions, at interdisciplinary centers/institutes within larger institutions, at alliances/partnerships between several institutions and at entities with a national scope. As the INCLUDES Alliances and National Network will operate across multiple scales, the proposed Conference will provide guidance on the technical aspects of designing and planning multi-scale evaluations. \r\n\r\nThe target audience for the activities are those individuals involved in current INCLUDES projects, those considering collaborating in such projects and STEM educators considering inclusion of formal evaluation in their projects. The overall goals are to (i) enhance the knowledge of the participants about evaluation methods; (ii) present the experiences of individuals who have successfully developed alliances and carried out evaluation efforts for these; and (iii) provide advice regarding evaluation methods for those planning to participate in future requests for INCLUDES Alliances and/or the National Network. The methods proposed include three components: (a) an initial webinar open to the broad community of STEM educators to introduce key concepts and vocabulary concerning evaluation methods; (b) a pre-Conference Tutorial with 30 participants to provide a one-day overview of modern methods in evaluation and connect these to projects suggested by the participants; and (c) a two-day Conference with 80 participants including as speakers leading experts on program evaluation as well as leaders of programs in STEM education that have operated at multiple scales. Virtual participation will be integrated throughout the three components of the project and a formal evaluation of the project will be carried out using the Systems Evaluation Methodology that has been in use at the host NSF-sponsored Center (NIMBioS) for many years.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "EEC",
 "org_div_long_name": "Division of Engineering Education and Centers",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Louis",
   "pi_last_name": "Gross",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Louis J Gross",
   "pi_email_addr": "lgross@utk.edu",
   "nsf_id": "000216178",
   "pi_start_date": "2016-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Suzanne",
   "pi_last_name": "Lenhart",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Suzanne Lenhart",
   "pi_email_addr": "slenhart@utk.edu",
   "nsf_id": "000342134",
   "pi_start_date": "2016-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ernest",
   "pi_last_name": "Brothers",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ernest Brothers",
   "pi_email_addr": "ebrother@utk.edu",
   "nsf_id": "000629336",
   "pi_start_date": "2016-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Pamela",
   "pi_last_name": "Bishop",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Pamela R Bishop",
   "pi_email_addr": "pbaird@utk.edu",
   "nsf_id": "000663478",
   "pi_start_date": "2016-09-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Knoxville",
  "inst_street_address": "201 ANDY HOLT TOWER",
  "inst_street_address_2": "",
  "inst_city_name": "KNOXVILLE",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "8659743466",
  "inst_zip_code": "379960001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "TN02",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "LXG4F9K8YZK5",
  "org_uei_num": "FN2YCS2YAUW3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Knoxville",
  "perf_str_addr": "1 Circle Park",
  "perf_city_name": "",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "379960003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "TN02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "032Y00",
   "pgm_ele_name": "Eddie Bernice Johnson INCLUDES"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "030Z",
   "pgm_ref_txt": "Backbone Org INCLUDES"
  },
  {
   "pgm_ref_code": "7556",
   "pgm_ref_txt": "CONFERENCE AND WORKSHOPS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 248397.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><dl class=\"clearing\"><dd>\n<div class=\"tinyMCEContent\">\n<p>A live webinar, &ldquo;Program Evaluation 101&rdquo;, was presented by Dr. Pamela Bishop and Sondra LoRe on February 09, 2017. Dr. Louis Gross moderated the event. The webinar focused on: 1) an overview of program evaluation, 2) approaches to evaluation, 3) working with an evaluator, and 4) information about the evaluation process. A total of 140 attendees participated in the live webinar. Additionally, to date, the webinar has been viewed over 170&nbsp; times on YouTube <a rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=ZGsNJ1jIJD0\">https://www.youtube.com/watch?v=ZGsNJ1jIJD0</a></p>\n<p>A one-day pre-conference tutorial with 33 attendees (including four speakers) was held February 22, 2017. Tutorial applicants were asked if they would be interested in providing their current projects as case studies to use as examples during the tutorial.&nbsp; Four INCLUDES projects of tutorial participants were selected as case studies. Examples of the breakout session activities are provided in Appendix D of the attached Evaluation Report. Tutorial participants were placed into breakout sessions by conference organizers according to the information they provided on their applications. Participants were matched as closely to a project in their area of interest as possible. Four breakout sessions with 8-10 participants each (including one session leader and one facilitator) met for two 1.5 hour breakout sessions using the participant case studies to walk through several hands-on activities, including as time allowed: (1) mapping a project, (2) determining key stakeholders, (3) developing evaluation questions, and (4) determining data sources to answer evaluation questions. Online participation was allowed in breakout sessions via Zoom. Three breakout groups each had one active online participant.</p>\n<p>A subsequent two-day conference with 100 attendees (including all tutorial participants, 11 speakers, and two additional organizers) was held February 23-24, 2017.&nbsp; The conference included a combination of presentations, breakout sessions, poster presentations, and panel discussions. Seven presenters, including STEM program evaluators and leaders of diversity-focused STEM initiatives, gave talks about issues such as creating an maintaining alliances, evaluating projects at multiple scales, and cultural contexts for evaluation (see Appendix B of the attached Evaluation Report for a full listing of presentations). Panel discussions fielded questions from the audience on Day one, and a formative evaluation at the end of Day one guided the theme of the panel discussions on Day two to focus more on multi-level, multi-site evaluation issues. Day one ended with a poster session and reception. A list of the 35 participant poster presentations is provided in Appendix F of the attached Evaluation Report. Several small-group breakout sessions were offered on both days. Session topics were selected from participant pre-survey responses regarding issues they would like to explore further. Session topics included data visualization, development of program models, research vs. evaluation, and best practices in creating evaluation reports.</p>\n<p>Beyond these activities, the project developed and presented five live webinars to help STEM educators develop evaluation plans that meet the needs of INCLUDES Pilot and Alliance Projects.</p>\n<p style=\"margin-top: 8px; margin-bottom: 8px;\">Webinars were targeted at individuals involved in current or upcoming INCLUDES projects, those considering collaborating in such projects, and STEM educators considering the inclusion of formal evaluation in their projects.</p>\n<p style=\"margin-top: 8px; margin-bottom: 8px;\">Webinar topics and dates were as follows: <strong>June 7, 2018:</strong> <em>Evaluation Strategies for Measuring the Broader Impacts (BI) of NSF INCLUDES Projects</em>; <strong>May 3, 2018:</strong> <em>Qualitative Data in Culturally Rich Evaluations of NSF INCLUDES Projects; <strong>April 5, 2018:</strong> <em>Engaging Diverse Populations in Evaluations of NSF INCLUDES Projects; <strong>March 1, 2018:</strong> <em>Program Models as a Tool for Scaling up NSF INCLUDES Projects; and <strong>Feb 1, 2018:</strong> <em>Evaluating Social Media Impact in NSF INCLUDES Projects. </em></em></em></em>Recordings of all webinars, as well as webinar slides and chat transcriptions can be found here: http://www.nimbios.org/IncludesConf/webinars</p>\n</div>\n</dd></dl><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/10/2018<br>\n\t\t\t\t\tModified by: Pamela&nbsp;R&nbsp;Bishop</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\nA live webinar, \"Program Evaluation 101\", was presented by Dr. Pamela Bishop and Sondra LoRe on February 09, 2017. Dr. Louis Gross moderated the event. The webinar focused on: 1) an overview of program evaluation, 2) approaches to evaluation, 3) working with an evaluator, and 4) information about the evaluation process. A total of 140 attendees participated in the live webinar. Additionally, to date, the webinar has been viewed over 170  times on YouTube https://www.youtube.com/watch?v=ZGsNJ1jIJD0\n\nA one-day pre-conference tutorial with 33 attendees (including four speakers) was held February 22, 2017. Tutorial applicants were asked if they would be interested in providing their current projects as case studies to use as examples during the tutorial.  Four INCLUDES projects of tutorial participants were selected as case studies. Examples of the breakout session activities are provided in Appendix D of the attached Evaluation Report. Tutorial participants were placed into breakout sessions by conference organizers according to the information they provided on their applications. Participants were matched as closely to a project in their area of interest as possible. Four breakout sessions with 8-10 participants each (including one session leader and one facilitator) met for two 1.5 hour breakout sessions using the participant case studies to walk through several hands-on activities, including as time allowed: (1) mapping a project, (2) determining key stakeholders, (3) developing evaluation questions, and (4) determining data sources to answer evaluation questions. Online participation was allowed in breakout sessions via Zoom. Three breakout groups each had one active online participant.\n\nA subsequent two-day conference with 100 attendees (including all tutorial participants, 11 speakers, and two additional organizers) was held February 23-24, 2017.  The conference included a combination of presentations, breakout sessions, poster presentations, and panel discussions. Seven presenters, including STEM program evaluators and leaders of diversity-focused STEM initiatives, gave talks about issues such as creating an maintaining alliances, evaluating projects at multiple scales, and cultural contexts for evaluation (see Appendix B of the attached Evaluation Report for a full listing of presentations). Panel discussions fielded questions from the audience on Day one, and a formative evaluation at the end of Day one guided the theme of the panel discussions on Day two to focus more on multi-level, multi-site evaluation issues. Day one ended with a poster session and reception. A list of the 35 participant poster presentations is provided in Appendix F of the attached Evaluation Report. Several small-group breakout sessions were offered on both days. Session topics were selected from participant pre-survey responses regarding issues they would like to explore further. Session topics included data visualization, development of program models, research vs. evaluation, and best practices in creating evaluation reports.\n\nBeyond these activities, the project developed and presented five live webinars to help STEM educators develop evaluation plans that meet the needs of INCLUDES Pilot and Alliance Projects.\nWebinars were targeted at individuals involved in current or upcoming INCLUDES projects, those considering collaborating in such projects, and STEM educators considering the inclusion of formal evaluation in their projects.\nWebinar topics and dates were as follows: June 7, 2018: Evaluation Strategies for Measuring the Broader Impacts (BI) of NSF INCLUDES Projects; May 3, 2018: Qualitative Data in Culturally Rich Evaluations of NSF INCLUDES Projects; April 5, 2018: Engaging Diverse Populations in Evaluations of NSF INCLUDES Projects; March 1, 2018: Program Models as a Tool for Scaling up NSF INCLUDES Projects; and Feb 1, 2018: Evaluating Social Media Impact in NSF INCLUDES Projects. Recordings of all webinars, as well as webinar slides and chat transcriptions can be found here: http://www.nimbios.org/IncludesConf/webinars\n\n\n\n\t\t\t\t\tLast Modified: 09/10/2018\n\n\t\t\t\t\tSubmitted by: Pamela R Bishop"
 }
}