{
 "awd_id": "1563727",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Text-to-Image Reference Resolution for Image Understanding and Manipulation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2016-06-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 550000.0,
 "awd_amount": 550000.0,
 "awd_min_amd_letter_date": "2016-03-23",
 "awd_max_amd_letter_date": "2017-09-01",
 "awd_abstract_narration": "This project develops new technologies at the interface of computer vision and natural language processing to understand text-to-image relationships. For example, given a captioned image, the project develops techniques which determine which words (e.g. \"woman talking on phone\", \"The farther vehicle\") correspond to which image parts. From robotics to human-computer interaction, there are numerous real-world tasks that benefit from practical systems to identify objects in scenes based on language and understand language based on visual context. In particular, the project develops the first language-based image authoring tool which allows users to edit or synthesize realistic imagery using only natural language (e.g. \"delete the garbage truck from this photo\" or \"make an image with three boys chasing a shaggy dog\"). Beyond the immediate impact of creating new ways for users to access and author digital images, the broader impacts of this work include three focus areas: the development of new benchmarks for the vision and language communities, outreach and undergraduate research, and leadership in promoting diversity. \r\n\r\nAt the core of the project are new techniques for large-scale text-to-image reference resolution (TIRR) that enable systems to automatically identify the image regions that depict entities described in natural language sentences or commands. These techniques advance image interpretation by enabling systems to perform partial matching between images and sentences, referring expression understanding, and image-based question answering. They also advance image manipulation by enabling systems that can synthesize images starting from a textual description, or modify images based on natural language commands. The main technical contributions of the project are:  (1) benchmark datasets for TIRR with comprehensive large-scale gold standard annotations that will make TIRR a standard task for recognition; (2) principled new representations for text-to-image annotations that expose the compositional nature of language using the formalism of the denotation graph; (3) new models for TIRR that perform an explicit alignment (grounding) of words and phrases to image regions guided by the structure of the denotation graph; (4) applications of TIRR methods to referring expression understanding and visual question answering; and (5) applications of TIRR to image creation and manipulation based on natural language input.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Svetlana",
   "pi_last_name": "Lazebnik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Svetlana Lazebnik",
   "pi_email_addr": "slazebni@illinois.edu",
   "nsf_id": "000298493",
   "pi_start_date": "2016-03-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Julia",
   "pi_last_name": "Hockenmaier",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Julia C Hockenmaier",
   "pi_email_addr": "juliahmr@illinois.edu",
   "nsf_id": "000237005",
   "pi_start_date": "2016-03-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618207473",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 179270.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 370730.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>Visual recogntion research is moving from predicting discrete, categorical labels to generating rich descriptions of visual data. We are witnessing a surge of interest in tasks that involve&nbsp;cross-modal learning&nbsp;from image and text data, widely viewed as the &ldquo;next frontier&rdquo; of scene understanding. Accordingly, this project was focused on&nbsp;<span>developing new technologies at the interface of computer vision and natural language processing to understand the relationships between visual content and language grounded in that content.&nbsp;</span></span></p>\n<p><span><span>Intellectual merit: </span></span></p>\n<p><span><span>This project has resulted in several advances in image-language models, most notably, novel&nbsp;</span></span>neural network architectures for measuring the semantic similarity between visual data, i.e., images or regions, and text data, i.e., sentences or phrases. These architectures included two-branch embedding and similarity networks, and more advanced conditional embedding networks that can learn a representation specific to a subset of phrases while efficiently sharing parameters across phrases. In turn, these architectures were used as building blocks for designing systems for a variety of vision-language tasks, including visual phrase grounding, open-ended phrase detection, image captioning with diverse sentences, visual question answering, visual relationship detection, and text-driven image editing.</p>\n</div>\n</div>\n</div>\n<p>Broader impacts:</p>\n<p>From robotics to human-computer interaction, there are numerous real-world tasks that would benefit from practical, large-scale systems that can identify objects in scenes based on language and understand language based on visual context. The techniques developed in this project have the potential to serve as a stepping stone to artificial intelligence systems that can interact with people using freeform natural language, as well as more general embodied intelligent agents.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/20/2021<br>\n\t\t\t\t\tModified by: Svetlana&nbsp;Lazebnik</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\nVisual recogntion research is moving from predicting discrete, categorical labels to generating rich descriptions of visual data. We are witnessing a surge of interest in tasks that involve cross-modal learning from image and text data, widely viewed as the \"next frontier\" of scene understanding. Accordingly, this project was focused on developing new technologies at the interface of computer vision and natural language processing to understand the relationships between visual content and language grounded in that content. \n\nIntellectual merit: \n\nThis project has resulted in several advances in image-language models, most notably, novel neural network architectures for measuring the semantic similarity between visual data, i.e., images or regions, and text data, i.e., sentences or phrases. These architectures included two-branch embedding and similarity networks, and more advanced conditional embedding networks that can learn a representation specific to a subset of phrases while efficiently sharing parameters across phrases. In turn, these architectures were used as building blocks for designing systems for a variety of vision-language tasks, including visual phrase grounding, open-ended phrase detection, image captioning with diverse sentences, visual question answering, visual relationship detection, and text-driven image editing.\n\n\n\n\nBroader impacts:\n\nFrom robotics to human-computer interaction, there are numerous real-world tasks that would benefit from practical, large-scale systems that can identify objects in scenes based on language and understand language based on visual context. The techniques developed in this project have the potential to serve as a stepping stone to artificial intelligence systems that can interact with people using freeform natural language, as well as more general embodied intelligent agents.\n\n\t\t\t\t\tLast Modified: 07/20/2021\n\n\t\t\t\t\tSubmitted by: Svetlana Lazebnik"
 }
}