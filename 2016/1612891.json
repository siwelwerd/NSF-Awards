{
 "awd_id": "1612891",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:  New stochastically-motivated solutions to classical inverse problems",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 125645.0,
 "awd_amount": 125645.0,
 "awd_min_amd_letter_date": "2016-05-24",
 "awd_max_amd_letter_date": "2016-05-24",
 "awd_abstract_narration": "Numerous scientific questions assume the form of inverse problems, in which an unknown input to a system under study gives rise to an observed noisy output, and the goal is to estimate the input from the output. An example of such a problem is calculating the density of the Earth from measurements of the local gravitational field.  Only rarely can such inverse problems be solved analytically, and in general numerical approximations are required to find solutions. In this research project, the investigators aim to introduce a novel iterative algorithm for solving inverse problems, develop its theoretical and computational properties, and establish its performance in applications.  It is anticipated that the new algorithm will be adaptable to a range of problems currently under investigation in applied and numerical mathematics, for example in solving a sparse system of linear equations, currently of great interest in areas including tomography, archaeology, astrophysics, and other sciences.\r\n\r\nThis research project explores a novel iterative algorithm for the solution of a class inverse problems that includes Fredholm integral equations of the first kind, Laplace transform inversion, mixing distribution estimation in statistics, and solving sparse systems of linear equations. The investigators plan to (i) introduce a novel iterative algorithm for solving inverse problems of these types, and perhaps others, (ii) develop its theoretical and computational properties, and (iii) establish its performance in applications. A motivation for the research is the statistical problem of estimating a mixing density in a nonparametric mixture model. To date, there are no general algorithms that produce globally consistent estimators of the mixing density, in the sense of almost sure convergence with respect to a strong metric; only weak convergence results are available. An important feature of the algorithm under development is that, if it is initialized at a smooth density function, then the estimator is necessarily also a smooth density function. Other algorithms designed by numerical analysts for solving these inverse problems do not have this closure property. The form of the novel iterative algorithm, along with the fact that it yields smooth density estimators, suggests that this open problem can be solved; the investigators aim to establish a general global consistency result and demonstrate rates of convergence.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Walker",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen Walker",
   "pi_email_addr": "s.g.walker@math.utexas.edu",
   "nsf_id": "000658016",
   "pi_start_date": "2016-05-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121532",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 125645.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica; min-height: 14.0px} -->\n<p class=\"p1\">There is a class of problems within scientific endeavors known as &ldquo;inverse problems&rdquo;. The idea is to find a source, or causal factor, behind an observation. The most well known of these problems are the linear system of equations and the Laplace inversion formula, which is a special case of the class of Fredholm equations of the first kind. The most well known problem in statistics is the estimation of a mixing distribution. All of these problems are widely encountered in engineering, statistics and financial modeling, to name but a few.</p>\n<p class=\"p2\">&nbsp;Solutions to these inverse problems proceed via the use of iterative algorithms. Starting with a chosen value, the algorithm iterates this initial value to another value and then treats this as the new starting point and does another iteration of the algorithm, and so on. Eventually, the algorithm converges to a value which represents the solution to the problem. Issues to be considered include convergence, the the algorithm eventually pins down a single unique value. It is to be noted that some of the most popular algorithms fail to converge under certain conditions. Also important is the stability of the algorithm and computing time.&nbsp;</p>\n<p class=\"p2\">&nbsp;The aim of the project was to investigate a new algorithm for solving inverse problems which was motivated by an iterative method known as the EM algorithm; the EM standing for Expectation Maximization. This algorithm has traditionally been used for optimizing a likelihood function in which some of the pieces of observation are missing. The project was to adapt the basic EM algorithm to be used for solving inverse problems.&nbsp;</p>\n<p class=\"p2\">&nbsp;One of the key outcomes was proving the new algorithm converged appropriately. In particular, we achieved this for the nonparametric maximum likelihood estimator of a mixing distribution.&nbsp; In general we were able to prove convergence&nbsp; of a number of different versions of the algorithms in different settings and demonstrate&nbsp; competitiveness with current algorithms. We were also able to demonstrate the properties of the algorithm in new cases, such as with negative functions, when traditionally all functions were required to be non-negative.&nbsp;</p>\n<p class=\"p2\">Our findings and outcomes were published in the journals Statistics &amp; Computing, Methodology &amp; Computing in Applied Probability, Linear &amp; Multilinear Algebra, and Statistics &amp; Probability Letters. &nbsp;</p>\n<p class=\"p2\">&nbsp;Two PhD students working at UT Austin became involved with the project. One looked at the statistical problem of estimating mixing distributions associated with multivariate functions. In particular, a Laplace inversion algorithm for multivariate distributions. It appears that the new algorithm is better at handling the multivariate setting than the traditional algorithms. The other looked at the statistical problem of providing uncertainty quantification when the solution to the inverse problem is based on a finite amount of observations. Within a Bayesian framework this amounts to providing random solutions which are close in some way (not gone into here) to the original solution. We investigated this possibility by suitably randomizing the iterative algorithm. This work is ongoing.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2019<br>\n\t\t\t\t\tModified by: Stephen&nbsp;Walker</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThere is a class of problems within scientific endeavors known as \"inverse problems\". The idea is to find a source, or causal factor, behind an observation. The most well known of these problems are the linear system of equations and the Laplace inversion formula, which is a special case of the class of Fredholm equations of the first kind. The most well known problem in statistics is the estimation of a mixing distribution. All of these problems are widely encountered in engineering, statistics and financial modeling, to name but a few.\n Solutions to these inverse problems proceed via the use of iterative algorithms. Starting with a chosen value, the algorithm iterates this initial value to another value and then treats this as the new starting point and does another iteration of the algorithm, and so on. Eventually, the algorithm converges to a value which represents the solution to the problem. Issues to be considered include convergence, the the algorithm eventually pins down a single unique value. It is to be noted that some of the most popular algorithms fail to converge under certain conditions. Also important is the stability of the algorithm and computing time. \n The aim of the project was to investigate a new algorithm for solving inverse problems which was motivated by an iterative method known as the EM algorithm; the EM standing for Expectation Maximization. This algorithm has traditionally been used for optimizing a likelihood function in which some of the pieces of observation are missing. The project was to adapt the basic EM algorithm to be used for solving inverse problems. \n One of the key outcomes was proving the new algorithm converged appropriately. In particular, we achieved this for the nonparametric maximum likelihood estimator of a mixing distribution.  In general we were able to prove convergence  of a number of different versions of the algorithms in different settings and demonstrate  competitiveness with current algorithms. We were also able to demonstrate the properties of the algorithm in new cases, such as with negative functions, when traditionally all functions were required to be non-negative. \nOur findings and outcomes were published in the journals Statistics &amp; Computing, Methodology &amp; Computing in Applied Probability, Linear &amp; Multilinear Algebra, and Statistics &amp; Probability Letters.  \n Two PhD students working at UT Austin became involved with the project. One looked at the statistical problem of estimating mixing distributions associated with multivariate functions. In particular, a Laplace inversion algorithm for multivariate distributions. It appears that the new algorithm is better at handling the multivariate setting than the traditional algorithms. The other looked at the statistical problem of providing uncertainty quantification when the solution to the inverse problem is based on a finite amount of observations. Within a Bayesian framework this amounts to providing random solutions which are close in some way (not gone into here) to the original solution. We investigated this possibility by suitably randomizing the iterative algorithm. This work is ongoing. \n\n \n\n\t\t\t\t\tLast Modified: 10/29/2019\n\n\t\t\t\t\tSubmitted by: Stephen Walker"
 }
}