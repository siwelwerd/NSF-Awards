{
 "awd_id": "1553687",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Large Scale Learning for Complex Image-Omics Data Analytics",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 535763.0,
 "awd_amount": 535763.0,
 "awd_min_amd_letter_date": "2016-08-02",
 "awd_max_amd_letter_date": "2020-07-07",
 "awd_abstract_narration": "This proposal aims to develop computational tools for analyzing complex pathology and radiology image data as well genomics data. Recent technological innovations are enabling scientists to capture complex imaging and genomic data from different views. However, the major computational challenges are due to the unprecedented scale and complexity of heterogeneous data analytics. To solve the key and challenging problems in mining such comprehensive heterogeneous image and genomic data, the PI proposes to develop novel large scale learning tools and explore ways to integrate features from multiple data sources for clinical outcome prediction. It will greatly support the Precision Medicine Initiative, which has become a national goal and was unveiled by the U.S. government as a research effort designed to enable physicians to select individualized treatments. This project will facilitate the development of novel educational tools to enhance several current courses. \r\n\r\nThe PI proposes an integrated research and education plan based on the following three components: (1) big image analytics and feature extraction, in which novel sparse convolution kernels, sparse deformable models and quantitative topology measurements are proposed to extract local and global features to fully characterize whole  images; (2) large scale feature learning, in which domain knowledge guided sparse feature learning models and non-convex sparse feature learning models are proposed for large scale image marker discovery; and (3) multi-source image-omics data integration, in which sparse multi-view learning and large scale learning with bipartite graph are developed for big image-omics data integration, where the image-omics refers to both image data (pathology images or radiology images) and omics data (genomics, proteomics or metabolomics) captured from the same patient. This project will advance research in efficient feature learning from giga-pixel images, and in integrating heterogeneous image-omics data for outcome prediction and knowledge discovery. The success of this project will create a new paradigm for medical image informatics and big data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Junzhou",
   "pi_last_name": "Huang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Junzhou Huang",
   "pi_email_addr": "jzhuang@uta.edu",
   "nsf_id": "000608846",
   "pi_start_date": "2016-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Arlington",
  "inst_street_address": "701 S NEDDERMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "ARLINGTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "8172722105",
  "inst_zip_code": "760199800",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT ARLINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "LMLUKUPJJ9N3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Arlington",
  "perf_str_addr": "500 UTA Blvd, ERB 640",
  "perf_city_name": "Arlington",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "760190045",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 99665.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 103257.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 106998.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 110893.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 114950.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The investigation of this project produces several important outcomes.</p>\n<ol>\n<li>We developed a deep learning method to make survival prediction from whole slide histopathological images. The gigapixel resolution of Whole Slide Histopathological Images (WSIs) makes traditional survival models computationally impossible. Another challenge is that survival prediction usually comes with insufficient training patient samples. We develop an effective Whole Slide Histopathological Images Survival Analysis framework (WSISA) to overcome above challenges. Results on three large public datasets demonstrate the proposed framework can significantly improve the prediction performance compared with the existing state-of-the-arts survival methods. Our code is publicly available at <a href=\"https://github.com/uta-smile/WSISA\">https://github.com/uta-smile/WSISA</a></li>\n<li>We developed Adaptive Graph Convolutional Neural Networks and applied them to survival prediction from WSIs. Graph Convolutional Neural Networks (GCNs) are generalizations of classical CNNs to handle graph data such as molecular data, point could and social networks. However, for most real data, the graph structures vary in both size and connectivity. We propose a generalized and flexible graph CNN taking data of arbitrary graph structure as input. In that way a task-driven adaptive graph is learned for each graph data while training. Moreover, we applied them to survival prediction from WSIs by modeling a WSI as a graph and achieved significantly better performance. <a href=\"https://github.com/uta-smile/DeepGraphSurv\">https://github.com/uta-smile/DeepGraphSurv</a></li>\n<li>We developed Attention Guided Deep Multi-instance Learning methods and Graph Attention Multiple-instance Learning methods for Survival Prediction from WSIs. Different from the existing works on learning using key patches or clusters from WSIs, we take advantages of a deep multiple instance learning to encode all possible patterns from WSIs and consider the joint effects from different patterns for clinical outcomes prediction. The proposed framework can significantly improve the prediction performances compared with existing state-of-the-arts survival analysis approaches. Results also demonstrate the effectiveness of the proposed method as a recommender system to provide personalized recommendations based on an individual's calculated risk. Our code is publicly available at <a href=\"https://github.com/uta-smile/DeepAttnMISL\">https://github.com/uta-smile/DeepAttnMISL</a></li>\n<li>We developed Adversarial Domain Adaptation methods for Cell Nuclei Segmentation. To successfully train a cell segmentation network in fully-supervised manner for a particular type of organ or cancer, we need the dataset with ground-truth annotations. However, high unavailability of such annotated dataset and tedious labeling process enforces us to discover a way for training with unlabeled dataset. We developed a method called CellSegUDA for cell segmentation on the unlabeled dataset (target domain). It is achieved by applying unsupervised domain adaptation (UDA) technique with the help of another labeled dataset (source domain) that may come from other organs or sources. We also extended it to a semi-supervised domain adaptation (SSDA) based approach while few of training data is available. Our SSDA model also gives excellent results which are quite close to the fully-supervised upper bound in target domain. Our code is publicly available at <a href=\"https://github.com/uta-smile/CellSegDA\">https://github.com/uta-smile/CellSegDA</a></li>\n<li>We developed the first systematic study on how existing UDA methods in semantic segmentation are vulnerable to adversarial attacks. This investigation provides new insight into this area. We therefore introduce a new UDA method known as ASSUDA to robustly adapt domain knowledge in urban-scene semantic segmentation. The key insight of our method is to leverage the regularization power of adversarial examples in self-supervision. Specifically, we propose the adversarial self-supervision that maximizes the agreement between clean images and their adversarial examples by a contrastive loss in the output space. Comprehensive empirical studies demonstrate the robustness of our method against adversarial attacks on two benchmark settings. Our code is publicly available at <a href=\"https://github.com/uta-smile/ASSUDA\">https://github.com/uta-smile/ASSUDA</a></li>\n<li>We propose a Transformer-based Self-Supervised Learning (SSL) approach for pre-training so that the segmentation network implicitly acquires a better understanding of the nuclei and background using a large-scale unannotated histology image dataset extracted from Whole Slide Images (WSI). Our pre-trained SSL model learns to separate nuclei features from the background features in the embedding space. To the best of our knowledge, this is the first work focusing on Transformer-based SSL for semantic segmentation of nuclei. Extensive experimental results demonstrate the superiority of our proposed SSL incorporated transformer model over baseline methods. Our code is publicly available at <a href=\"https://github.com/uta-smile/TransNuSS\">https://github.com/uta-smile/TransNuSS</a>&nbsp;</li>\n</ol>\n<p>We published over 50 full-length papers related to this project in peer-reviewed conference proceedings and journals, such as CVPR, ICML, NeurIPS, AAAI, IJCAI, MICCAI, TPAMI and so on. This project provides a bed to train 18 Ph.D. students at the University of Texas at Arlington. 10 of them have graduated with PhD degrees and are working as research scientists now in Google, Meta, Amazon and so on. This project also provides a bed to train 9 master students at University of Texas at Arlington. Both of them have graduated with Master degrees and are working as algorithm engineers now in Meta, Samsung, Walmart, BMW and so on. The research materials produced in this project are used in teaching several graduate courses at University of Texas at Arlington.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/01/2023<br>\n\t\t\t\t\tModified by: Junzhou&nbsp;Huang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe investigation of this project produces several important outcomes.\n\nWe developed a deep learning method to make survival prediction from whole slide histopathological images. The gigapixel resolution of Whole Slide Histopathological Images (WSIs) makes traditional survival models computationally impossible. Another challenge is that survival prediction usually comes with insufficient training patient samples. We develop an effective Whole Slide Histopathological Images Survival Analysis framework (WSISA) to overcome above challenges. Results on three large public datasets demonstrate the proposed framework can significantly improve the prediction performance compared with the existing state-of-the-arts survival methods. Our code is publicly available at https://github.com/uta-smile/WSISA\nWe developed Adaptive Graph Convolutional Neural Networks and applied them to survival prediction from WSIs. Graph Convolutional Neural Networks (GCNs) are generalizations of classical CNNs to handle graph data such as molecular data, point could and social networks. However, for most real data, the graph structures vary in both size and connectivity. We propose a generalized and flexible graph CNN taking data of arbitrary graph structure as input. In that way a task-driven adaptive graph is learned for each graph data while training. Moreover, we applied them to survival prediction from WSIs by modeling a WSI as a graph and achieved significantly better performance. https://github.com/uta-smile/DeepGraphSurv\nWe developed Attention Guided Deep Multi-instance Learning methods and Graph Attention Multiple-instance Learning methods for Survival Prediction from WSIs. Different from the existing works on learning using key patches or clusters from WSIs, we take advantages of a deep multiple instance learning to encode all possible patterns from WSIs and consider the joint effects from different patterns for clinical outcomes prediction. The proposed framework can significantly improve the prediction performances compared with existing state-of-the-arts survival analysis approaches. Results also demonstrate the effectiveness of the proposed method as a recommender system to provide personalized recommendations based on an individual's calculated risk. Our code is publicly available at https://github.com/uta-smile/DeepAttnMISL\nWe developed Adversarial Domain Adaptation methods for Cell Nuclei Segmentation. To successfully train a cell segmentation network in fully-supervised manner for a particular type of organ or cancer, we need the dataset with ground-truth annotations. However, high unavailability of such annotated dataset and tedious labeling process enforces us to discover a way for training with unlabeled dataset. We developed a method called CellSegUDA for cell segmentation on the unlabeled dataset (target domain). It is achieved by applying unsupervised domain adaptation (UDA) technique with the help of another labeled dataset (source domain) that may come from other organs or sources. We also extended it to a semi-supervised domain adaptation (SSDA) based approach while few of training data is available. Our SSDA model also gives excellent results which are quite close to the fully-supervised upper bound in target domain. Our code is publicly available at https://github.com/uta-smile/CellSegDA\nWe developed the first systematic study on how existing UDA methods in semantic segmentation are vulnerable to adversarial attacks. This investigation provides new insight into this area. We therefore introduce a new UDA method known as ASSUDA to robustly adapt domain knowledge in urban-scene semantic segmentation. The key insight of our method is to leverage the regularization power of adversarial examples in self-supervision. Specifically, we propose the adversarial self-supervision that maximizes the agreement between clean images and their adversarial examples by a contrastive loss in the output space. Comprehensive empirical studies demonstrate the robustness of our method against adversarial attacks on two benchmark settings. Our code is publicly available at https://github.com/uta-smile/ASSUDA\nWe propose a Transformer-based Self-Supervised Learning (SSL) approach for pre-training so that the segmentation network implicitly acquires a better understanding of the nuclei and background using a large-scale unannotated histology image dataset extracted from Whole Slide Images (WSI). Our pre-trained SSL model learns to separate nuclei features from the background features in the embedding space. To the best of our knowledge, this is the first work focusing on Transformer-based SSL for semantic segmentation of nuclei. Extensive experimental results demonstrate the superiority of our proposed SSL incorporated transformer model over baseline methods. Our code is publicly available at https://github.com/uta-smile/TransNuSS \n\n\nWe published over 50 full-length papers related to this project in peer-reviewed conference proceedings and journals, such as CVPR, ICML, NeurIPS, AAAI, IJCAI, MICCAI, TPAMI and so on. This project provides a bed to train 18 Ph.D. students at the University of Texas at Arlington. 10 of them have graduated with PhD degrees and are working as research scientists now in Google, Meta, Amazon and so on. This project also provides a bed to train 9 master students at University of Texas at Arlington. Both of them have graduated with Master degrees and are working as algorithm engineers now in Meta, Samsung, Walmart, BMW and so on. The research materials produced in this project are used in teaching several graduate courses at University of Texas at Arlington.\n\n \n\n\t\t\t\t\tLast Modified: 09/01/2023\n\n\t\t\t\t\tSubmitted by: Junzhou Huang"
 }
}