{
 "awd_id": "1629395",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: EXPL: Hippogriff: Efficient Heterogeneous Servers for Data Centers and Cloud Services",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2020-09-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2016-09-08",
 "awd_max_amd_letter_date": "2016-09-08",
 "awd_abstract_narration": "The growing importance of artificial intelligence, network services, and cloud storage drives the demand of building powerful computer systems that can perform many operation at once.  Building computers with different kinds of computing processors (i.e., heterogeneous processing) is an effective way to achieve this goal. However, this approach also creates new problems that can negate some of the benefits it provides.  In particular, using different processors for different tasks requires moving data between those processors.  This movement takes time and can cancel out saving heterogeneous processing provides. This project is addressing this problem in heterogeneous computing systems by making the movement of data between different processors more efficient.  This improved efficiency leads directly to benefits for applications of scientific and commercial importance.\r\n\r\nMuch of the cost of data movement heterogeneous computing systems stems from the entrenched central processing unit (CPU)-centric programming model.  This project is revisiting the design of the application interface, system software and hardware components to remove CPUs and main memory from the critical path of moving data.  The project provides an efficient programming model that allows the system software stack to automatically and efficiently setup the data movements between heterogeneous processors. We are applying the system to large-scale database systems, massive parallel programming systems like Spark and MapReduce as well as scientific computing that power important daily applications and research projects.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Swanson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Steven Swanson",
   "pi_email_addr": "swanson@cs.ucsd.edu",
   "nsf_id": "000069307",
   "pi_start_date": "2016-09-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930404",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern computer systems are experiencing a revolution a range of new memory, networking, and compute technologies appear.&nbsp; These new technologies will exist along side existing technologies to create heterogeneous computing systems comprised of a multiplicity of technologies each providing strengths and weaknesses.&nbsp; One of the central challenge facing system designers in the coming decade is how to combine these technologies so that their complimentary strengths are available to applications.&nbsp; The \"front line\" in this challenge is in warehouse-scale datacenter or computing clouds that power the modern applications that we all rely on.&nbsp;</p>\n<p>Our work focuses on how to architect systems that includes this range of technologies.&nbsp; We have attacked this problem from several complimentary directions.&nbsp; For instance, the FileMR project provide a more efficient mechanism for transferring persistent program state across next-generation RDMA networks. &nbsp;The Hippogriff project developed software techniques to efficiently move data between processors, co-processors, and memory. &nbsp;KAML introduced a novel kind of SSD storage device that can store key-value pairs rather than just streams of bytes. The Corundum project provides a new programming framework that can prevent a broad class of bugs in persistent memory programs rather than relying on programmers to track them down after-the-fact. &nbsp;The SoftFlash project blurred the line between SSDs and network interfaces to bring compute, storage, and data movement together in a single device. &nbsp;Finally, we worked to make it faster and more efficient to develop and deploy custom silicon in the data center.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/09/2021<br>\n\t\t\t\t\tModified by: Steven&nbsp;Swanson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModern computer systems are experiencing a revolution a range of new memory, networking, and compute technologies appear.  These new technologies will exist along side existing technologies to create heterogeneous computing systems comprised of a multiplicity of technologies each providing strengths and weaknesses.  One of the central challenge facing system designers in the coming decade is how to combine these technologies so that their complimentary strengths are available to applications.  The \"front line\" in this challenge is in warehouse-scale datacenter or computing clouds that power the modern applications that we all rely on. \n\nOur work focuses on how to architect systems that includes this range of technologies.  We have attacked this problem from several complimentary directions.  For instance, the FileMR project provide a more efficient mechanism for transferring persistent program state across next-generation RDMA networks.  The Hippogriff project developed software techniques to efficiently move data between processors, co-processors, and memory.  KAML introduced a novel kind of SSD storage device that can store key-value pairs rather than just streams of bytes. The Corundum project provides a new programming framework that can prevent a broad class of bugs in persistent memory programs rather than relying on programmers to track them down after-the-fact.  The SoftFlash project blurred the line between SSDs and network interfaces to bring compute, storage, and data movement together in a single device.  Finally, we worked to make it faster and more efficient to develop and deploy custom silicon in the data center.\n\n \n\n\t\t\t\t\tLast Modified: 02/09/2021\n\n\t\t\t\t\tSubmitted by: Steven Swanson"
 }
}