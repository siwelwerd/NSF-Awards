{
 "awd_id": "1622536",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SCH: INT: Collaborative Research: Assistive Integrative Support Tool for Retinopathy of Prematurity",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 799999.0,
 "awd_amount": 799999.0,
 "awd_min_amd_letter_date": "2016-07-27",
 "awd_max_amd_letter_date": "2016-07-27",
 "awd_abstract_narration": "Retinopathy of prematurity (ROP) is a leading cause of childhood visual loss worldwide, and the social burdens of infancy-acquired blindness are enormous. Early diagnosis is critically important for successful treatment, and can prevent most cases of blindness. However, lack of access to expert medical diagnosis and care, especially in rural areas, remains a growing healthcare challenge. In addition, clinical expertise in ROP is lacking, and medical professionals are struggling to meet the increasing need for ROP care. As point-of-care technologies for diagnosis and intervention are rapidly expanding, the potential ability to assess ROP severity from any location with an internet connection and a camera, even without immediate ophthalmologic consultation available, could significantly improve delivery of ROP care by identifying infants who are in most urgent need for referral and treatment. This would dramatically reduce the incidence of blindness without a proportionate increase in the need for human resources, which take many years to develop. \r\n\r\nThis project develops a prototype assistive integrative support tool for ROP, featuring a modular design comprising: (a) image analysis, (b) information fusion of clinical, imaging, and diagnostic data, and (c) generative probabilistic and regression models with associated computationally efficient machine learning algorithms. The outcomes of the project include disease severity metrics and diagnostic estimates obtained through clinical evidence classifiers trained jointly over expert-generated labels. These labels consist of discrete diagnostic labels, as well as comparison outcomes of relative severity between pairs of images. Random process models for vessel tortuosity and diameter distributions over the retina, as well as patch-based vessel-free image analysis through the use of convolutional neural networks on the entire image, enhance and augment feature extraction. Moreover, incorporating severity comparison outcomes through novel hard and soft constraint methods force inferred severity to agree with ordinal information provided by experts and address inherent uncertainty in expert ground-truth labels. The above severity inference methods are evaluated and fine-tuned over a broad array of generative models, both through retrospective analysis, including cross-validation, longitudinal tests, and tests across multiple sites, as well as through prospective analysis, evaluating its real-world clinical impact.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stratis",
   "pi_last_name": "Ioannidis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stratis Ioannidis",
   "pi_email_addr": "IOANNIDIS@ECE.NEU.EDU",
   "nsf_id": "000711788",
   "pi_start_date": "2016-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jennifer",
   "pi_last_name": "Dy",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Jennifer G Dy",
   "pi_email_addr": "jdy@ece.neu.edu",
   "nsf_id": "000286912",
   "pi_start_date": "2016-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Deniz",
   "pi_last_name": "Erdogmus",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Deniz Erdogmus",
   "pi_email_addr": "erdogmus@ece.neu.edu",
   "nsf_id": "000483728",
   "pi_start_date": "2016-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Avenue",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801800",
   "pgm_ele_name": "Smart and Connected Health"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8018",
   "pgm_ref_txt": "Smart and Connected Health"
  },
  {
   "pgm_ref_code": "8062",
   "pgm_ref_txt": "SCH Type II: INT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 799999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-4b712395-7fff-5a43-e759-a22b44fcf1a3\"> </span></p>\n<p dir=\"ltr\"><strong>Intellectual Merit:</strong></p>\n<p dir=\"ltr\"><strong>Learning from comparisons. </strong><span>We propose a neural network architecture that is trained on both class and comparison labels. We extensively evaluate the predictive performance of our severity score with respect to a broad array of metrics, and show that comparisons can aid learning significantly: taking&nbsp; Retinopathy of Prematurity (ROP) classification as an example, we can train a 5.9M parameter neural network from pairwise comparisons on just 80 retina images, attaining very high predictive performance (0.92 AUC) on a holdout test set of 5 thousand images. As part of this project, we also studied sampling complexity questions in the context of learning from comparisons, characterizing the number of pairwise required so that a shallow model learned from this observed data is within a desirable precision bound.</span></p>\n<p dir=\"ltr\"><strong>Spectral methods for accelerating learning from rankings.</strong><span>&nbsp; Though comparisons between samples more generally, rankings of sets of samples, can significantly improve inference performance, the corresponding increase in the dataset size (by, e.g., comparing all possible pairs) creates a computational burden. We show that this can be significantly ameliorated by using spectral techniques. In particular, we propose the first approach to accelerate learning in ranking regression via spectral methods. Our resulting algorithms are up to 175 times faster and attain better predictions by up to 26% Top-1 Accuracy and 6% Kendall-Tau correlation over five real-life ranking datasets, including ROP, when compared to standard gradient-descent based algorithms.</span></p>\n<p dir=\"ltr\"><strong>Active learning from comparisons.</strong><span> Despite this success in inference, soliciting comparison labels from experts also poses a significant challenge, again, because the number of potential comparisons is quadratic in the dataset size. We have extensively investigated the following experimental design (i.e., batch active learning) problem: given a budget K, and a set of existing class labels, we identify the K comparison labels the expert should generate to improve inference quality. We studied several design objectives based on mutual information, the D-optimality criterion, and the Fisher information metric. Despite the NP-hardness of the corresponding optimization problems, several of these objectives are submodular, and thus a set of comparisons within 1 &minus; 1/e from the optimal can be found via the so-called greedy algorithm. We showed that by exploiting the inherent geometry of the dataset&ndash;namely, that it consists of pairwise comparisons&ndash;the greedy algorithm&rsquo;s complexity with a D-optimal objective can be reduced by orders of magnitude. We also apply our accelerations also to the so-called lazy greedy algorithm. When combined, the above improvements lead to an execution time of less than 1 hour for a dataset with 108 comparisons; the na&iuml;ve greedy algorithm requires more than 10 days to terminate on this dataset.</span></p>\n<p dir=\"ltr\"><strong>Guided attention networks.</strong><span> </span><span>Convolutional neural networks (CNNs) have shown great performance in medical diagnostic applications. However, because of their black-box nature, clinicians are reluctant to trust CNN diagnostic outcomes. Incorporating visual attention capabilities in CNNs enhances interpretability by highlighting regions in the images that CNNs utilize for prediction. Clinicians can often provide domain knowledge on relevant features: e.g., to diagnose ROP, structural information such as tortuosity of vessels aid clinicians in diagnosing ROP. We propose a Structural Visual Guidance Attention Networks (SVGA-Net) method that leverages structural domain knowledge to guide visual attention in CNNs.</span><span> </span><span>&nbsp;We demonstrate the efficacy of Structural Visual Guidance Attention Networks (SVGA-Net) on ROP; SVGA-Net consistently results in higher AUC compared to visual attention CNNs without guidance, baseline CNNs, and CNNs with structured masks. </span><span>We also propose Structurally Guided Channel Attention Networks (SGCA-Net), a principled way to guide the channel attention of CNNs. The convolution operator constructs feature maps by using both channel and spatial information within the receptive fields of its filters. We guide the channel attention of networks using feature vectors that contain clinically relevant information. We do so by attaching guided attention modules into a state-of-the-art network architecture, and guiding these attention modules with domain knowledge using feature vectors. We further experiment on SGCA-Net on the same ROP dataset; SGCA-Net achieves higher performance than CNNs without attention modules and CNNs with unguided attention modules.</span></p>\n<p><strong>Broader Impacts:</strong></p>\n<p dir=\"ltr\"><span>The project supported 9 Ph.D. students (4 women). Several have graduated and have gone on to have successful careers in the industry. Research was disseminated to the computer science and engineering community via publications&nbsp; to conferences such as KDD, AISTATS, IJCAI, and SDM, as well as a tutorial at the WWW 2021, but also to the medical community through publications in venues such as JAMA Ophthalmology and the annual meetings of the Association for Research in Vision and Ophthalmology. Software developed by the team was made publicly available and shared via github repositories. The principal investigators also gave several invite seminars about this research to several universities and companies.</span></p>\n<p><br /><br /></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/20/2023<br>\n\t\t\t\t\tModified by: Stratis&nbsp;Ioannidis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nIntellectual Merit:\nLearning from comparisons. We propose a neural network architecture that is trained on both class and comparison labels. We extensively evaluate the predictive performance of our severity score with respect to a broad array of metrics, and show that comparisons can aid learning significantly: taking  Retinopathy of Prematurity (ROP) classification as an example, we can train a 5.9M parameter neural network from pairwise comparisons on just 80 retina images, attaining very high predictive performance (0.92 AUC) on a holdout test set of 5 thousand images. As part of this project, we also studied sampling complexity questions in the context of learning from comparisons, characterizing the number of pairwise required so that a shallow model learned from this observed data is within a desirable precision bound.\nSpectral methods for accelerating learning from rankings.  Though comparisons between samples more generally, rankings of sets of samples, can significantly improve inference performance, the corresponding increase in the dataset size (by, e.g., comparing all possible pairs) creates a computational burden. We show that this can be significantly ameliorated by using spectral techniques. In particular, we propose the first approach to accelerate learning in ranking regression via spectral methods. Our resulting algorithms are up to 175 times faster and attain better predictions by up to 26% Top-1 Accuracy and 6% Kendall-Tau correlation over five real-life ranking datasets, including ROP, when compared to standard gradient-descent based algorithms.\nActive learning from comparisons. Despite this success in inference, soliciting comparison labels from experts also poses a significant challenge, again, because the number of potential comparisons is quadratic in the dataset size. We have extensively investigated the following experimental design (i.e., batch active learning) problem: given a budget K, and a set of existing class labels, we identify the K comparison labels the expert should generate to improve inference quality. We studied several design objectives based on mutual information, the D-optimality criterion, and the Fisher information metric. Despite the NP-hardness of the corresponding optimization problems, several of these objectives are submodular, and thus a set of comparisons within 1 &minus; 1/e from the optimal can be found via the so-called greedy algorithm. We showed that by exploiting the inherent geometry of the dataset&ndash;namely, that it consists of pairwise comparisons&ndash;the greedy algorithm\u2019s complexity with a D-optimal objective can be reduced by orders of magnitude. We also apply our accelerations also to the so-called lazy greedy algorithm. When combined, the above improvements lead to an execution time of less than 1 hour for a dataset with 108 comparisons; the na&iuml;ve greedy algorithm requires more than 10 days to terminate on this dataset.\nGuided attention networks. Convolutional neural networks (CNNs) have shown great performance in medical diagnostic applications. However, because of their black-box nature, clinicians are reluctant to trust CNN diagnostic outcomes. Incorporating visual attention capabilities in CNNs enhances interpretability by highlighting regions in the images that CNNs utilize for prediction. Clinicians can often provide domain knowledge on relevant features: e.g., to diagnose ROP, structural information such as tortuosity of vessels aid clinicians in diagnosing ROP. We propose a Structural Visual Guidance Attention Networks (SVGA-Net) method that leverages structural domain knowledge to guide visual attention in CNNs.  We demonstrate the efficacy of Structural Visual Guidance Attention Networks (SVGA-Net) on ROP; SVGA-Net consistently results in higher AUC compared to visual attention CNNs without guidance, baseline CNNs, and CNNs with structured masks. We also propose Structurally Guided Channel Attention Networks (SGCA-Net), a principled way to guide the channel attention of CNNs. The convolution operator constructs feature maps by using both channel and spatial information within the receptive fields of its filters. We guide the channel attention of networks using feature vectors that contain clinically relevant information. We do so by attaching guided attention modules into a state-of-the-art network architecture, and guiding these attention modules with domain knowledge using feature vectors. We further experiment on SGCA-Net on the same ROP dataset; SGCA-Net achieves higher performance than CNNs without attention modules and CNNs with unguided attention modules.\n\nBroader Impacts:\nThe project supported 9 Ph.D. students (4 women). Several have graduated and have gone on to have successful careers in the industry. Research was disseminated to the computer science and engineering community via publications  to conferences such as KDD, AISTATS, IJCAI, and SDM, as well as a tutorial at the WWW 2021, but also to the medical community through publications in venues such as JAMA Ophthalmology and the annual meetings of the Association for Research in Vision and Ophthalmology. Software developed by the team was made publicly available and shared via github repositories. The principal investigators also gave several invite seminars about this research to several universities and companies.\n\n\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/20/2023\n\n\t\t\t\t\tSubmitted by: Stratis Ioannidis"
 }
}