{
 "awd_id": "1621746",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: CDS&E-MSS: Local Approximation for Large Scale Spatial Modeling",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Christopher Stark",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2016-08-24",
 "awd_max_amd_letter_date": "2018-08-16",
 "awd_abstract_narration": "Computer simulation is growing as a means of studying complex dynamics in applied science.  Once a tool exclusive to industrial engineering and computational physics, it is increasingly common in biology, chemistry, and economics.  Gone are the days when equilibrium dynamics are appropriate and cute systems of equations can be solved by hand.  Computer experiments are becoming more diverse, they are becoming more complex and they are growing in size thanks to modern supercomputing.  We need a new vanguard of modeling tools that can cope with the needs of modern computer experiments, particularly their increasing size (big data) and rapidly evolving and refining nature as models become more sophisticated, and supercomputing environments approach the exa-scale.  This funded research targets extensions and applications of a new breed of flexible and fast response surface methods, the so-called local approximate Gaussian process (laGP).  Our motivating applications come primarily from problems in computer experiments and uncertainty quantification, and ideas are borrowed from -- and will represent an important extension to -- the related literatures of geo-statistics and machine learning.  The over-arching goal is a  modernization of the response surface and surrogate modeling toolkit to better serve future applications across applied science.\r\n\r\nGaussian process (GP) models are popular in spatial modeling contexts, like geostatistics or computer experiments, where response surfaces are reasonably smooth but little else can be assumed. GP models provide accurate predictors, but increasingly impose computational bottlenecks: large dense matrix decompositions impede efforts to keep pace with modern trends in data acquisition. A scramble is on for fast approximations. Two common themes are sparsity, allowing fast matrix decompositions, and supercomputing, allowing distributed calculation. But these inroads are at capacity. Rapidly expanding mobile device networks, high-resolution satellite imagery (and GPS), and supercomputer simulation generate data of ever-increasing size. This funded research centers on local approximate GP (laGP) models as a means of enabling the powerful GP spatial modeling framework to address modern big data problems. Initial implementations show promise, expanding data size capabilities by several orders of magnitude. However much work remains to ensure that laGP methods can supplant conventional GPs in diverse spatial modeling contexts. Here we propose several methodological enhancements, many involving shortcuts that have provably minimal impact on laGP performance. We are motivated by two big data computer model emulation applications: one involving satellite positioning and another on solar power generation. Yet we are mindful that for our efforts to have impact, the wider spatial modeling context must always be kept in view.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Gramacy",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Robert B Gramacy",
   "pi_email_addr": "rbg@vt.edu",
   "nsf_id": "000614645",
   "pi_start_date": "2016-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "Hutcheson Hall, 250 Drillfield Drive",
  "perf_city_name": "Blacksburg",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "240610001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "VA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 51198.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 48591.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 50211.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Gaussian processes (GP) are a powerful nonlinear and nonparametric regression tool that have revolutionized modeling across several statistical disciplines, e.g., machine learning, geostatistics and computer surrogate modeling.&nbsp; They make accurate predictors with excellent out-of-sample coverage properties (i.e., interpretable uncertainty quantification or UQ).&nbsp; When modeling deterministic computer model simulations they can interpolate while retaining the same good UQ properties away from training simulations.&nbsp; But they are not without their drawbacks.&nbsp; Since they involve working with multivariate normal (MVN) distributions whose dimensionality is commensurate with training data size, computation grows cubically owing to the need to decompose large covariance matrices.&nbsp;<br />A common remedy is to induce sparsity in the covariance structure and leverage sparse-matrix linear algebra libraries for decomposition.&nbsp; Unfortunately, that's hard to do well.&nbsp; One exception is sparsity induction by divide-and-conquer, for example with tree models or local, transductive learning -- i.e., tailoring inference to prediction tasks.&nbsp; The local approximate Gaussian process (laGP) is an example of the latter class of such schemes.&nbsp; A predictor is derived using a small data subset nearby to where prediction(s) is/are required.&nbsp; LaGP builds a training design in a greedy fashion to minimize predictive variance at the predictive location(s) of interest.&nbsp; One of the major advantages to this approach is that it doesn't require sparse-matrix libraries and at the same time is massively parallelizable.&nbsp; Calculations for each predictive location can transpire independently, both in statistical and computational senses.&nbsp; However, the setup is in it's infancy.<br />This funded research sought extensions to the laGP setup along a number of directions, as motivated by challenging applications involving computer simulation surrogate modeling.&nbsp; One example comes from satellite positioning.&nbsp; A key component in tracking and guiding satellites in low-Earth orbit (LEO) is measurements of atmospheric drag.&nbsp; Drag coefficients can be simulated, but at a computational expense that is too cumbersome for real-time application.&nbsp; However, accurate GP training of surrogate drag models required prohibitively large training sets: upwards of one-million runs in eight input dimensions.&nbsp; That's way too big for conventional GPs.&nbsp; At the same time, it was too small for state-of-the art laGPs, yielding predictions that were very fast but not accurate enough (up to 1% relative error) for practical use.&nbsp; In order to expand the fidelity of laGP models, we created a variation based on a separable covariance structure, and devised a pre-conditioning scheme that offered a multi-resolution effect -- stretching and compressing the space on a global scale before local transductive modeling commences.&nbsp; When combined, those two approaches led to emulations which were still very fast, and more importantly were accurate below the 1% level.&nbsp; We also developed a scheme that provided joint surrogate predictions along a path of trajectories in LEO for a more nuanced assessment of covarying uncertainty.<br />A second motivating project involved predicting solar irradiance across the continental United States.&nbsp; Our data come from three sources.&nbsp; One is field data measurements at approximately 1500 weather station sites spread non-uniformly across the country.&nbsp; (There are far more in national forest land and urban areas than elsewhere.) At those same sites we had access to irradiance simulations from two libraries (NAM and SREF) as provided by the IBM PAIRS database.&nbsp; Daily data are available for approximately 1.5 years.&nbsp; This led to a large database of multiple sources, which would ideally be combined in order to \"extrapolate\" predictions to parts of the continental US which are poorly covered by weather monitoring stations, such as in the Great Plains.&nbsp; We developed an inverse variance weighting (IVW) scheme that combined field data and computer surrogate model predictions (with bias corrected by field data) using local (laGP) and global GP predictors.&nbsp; We did this on time-aggregated data and on the daily scale using a hierarchical auto-regressive and seasonal smoothing approach, again with GPs.&nbsp; We augmented with an additional simulation campaign of 1000 new sites with the help of our IBM colleagues.&nbsp; The result was highly accurate predictions of solar irradiance in space and time, complete with well-covered UQ as might be useful for planning a harvest of solar energy.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/11/2019<br>\n\t\t\t\t\tModified by: Robert&nbsp;B&nbsp;Gramacy</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nGaussian processes (GP) are a powerful nonlinear and nonparametric regression tool that have revolutionized modeling across several statistical disciplines, e.g., machine learning, geostatistics and computer surrogate modeling.  They make accurate predictors with excellent out-of-sample coverage properties (i.e., interpretable uncertainty quantification or UQ).  When modeling deterministic computer model simulations they can interpolate while retaining the same good UQ properties away from training simulations.  But they are not without their drawbacks.  Since they involve working with multivariate normal (MVN) distributions whose dimensionality is commensurate with training data size, computation grows cubically owing to the need to decompose large covariance matrices. \nA common remedy is to induce sparsity in the covariance structure and leverage sparse-matrix linear algebra libraries for decomposition.  Unfortunately, that's hard to do well.  One exception is sparsity induction by divide-and-conquer, for example with tree models or local, transductive learning -- i.e., tailoring inference to prediction tasks.  The local approximate Gaussian process (laGP) is an example of the latter class of such schemes.  A predictor is derived using a small data subset nearby to where prediction(s) is/are required.  LaGP builds a training design in a greedy fashion to minimize predictive variance at the predictive location(s) of interest.  One of the major advantages to this approach is that it doesn't require sparse-matrix libraries and at the same time is massively parallelizable.  Calculations for each predictive location can transpire independently, both in statistical and computational senses.  However, the setup is in it's infancy.\nThis funded research sought extensions to the laGP setup along a number of directions, as motivated by challenging applications involving computer simulation surrogate modeling.  One example comes from satellite positioning.  A key component in tracking and guiding satellites in low-Earth orbit (LEO) is measurements of atmospheric drag.  Drag coefficients can be simulated, but at a computational expense that is too cumbersome for real-time application.  However, accurate GP training of surrogate drag models required prohibitively large training sets: upwards of one-million runs in eight input dimensions.  That's way too big for conventional GPs.  At the same time, it was too small for state-of-the art laGPs, yielding predictions that were very fast but not accurate enough (up to 1% relative error) for practical use.  In order to expand the fidelity of laGP models, we created a variation based on a separable covariance structure, and devised a pre-conditioning scheme that offered a multi-resolution effect -- stretching and compressing the space on a global scale before local transductive modeling commences.  When combined, those two approaches led to emulations which were still very fast, and more importantly were accurate below the 1% level.  We also developed a scheme that provided joint surrogate predictions along a path of trajectories in LEO for a more nuanced assessment of covarying uncertainty.\nA second motivating project involved predicting solar irradiance across the continental United States.  Our data come from three sources.  One is field data measurements at approximately 1500 weather station sites spread non-uniformly across the country.  (There are far more in national forest land and urban areas than elsewhere.) At those same sites we had access to irradiance simulations from two libraries (NAM and SREF) as provided by the IBM PAIRS database.  Daily data are available for approximately 1.5 years.  This led to a large database of multiple sources, which would ideally be combined in order to \"extrapolate\" predictions to parts of the continental US which are poorly covered by weather monitoring stations, such as in the Great Plains.  We developed an inverse variance weighting (IVW) scheme that combined field data and computer surrogate model predictions (with bias corrected by field data) using local (laGP) and global GP predictors.  We did this on time-aggregated data and on the daily scale using a hierarchical auto-regressive and seasonal smoothing approach, again with GPs.  We augmented with an additional simulation campaign of 1000 new sites with the help of our IBM colleagues.  The result was highly accurate predictions of solar irradiance in space and time, complete with well-covered UQ as might be useful for planning a harvest of solar energy.\n\n\t\t\t\t\tLast Modified: 09/11/2019\n\n\t\t\t\t\tSubmitted by: Robert B Gramacy"
 }
}