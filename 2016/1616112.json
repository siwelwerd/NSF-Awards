{
 "awd_id": "1616112",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Learning to Read, Ground, and Reason in Multimodal Text",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2016-06-10",
 "awd_max_amd_letter_date": "2016-06-10",
 "awd_abstract_narration": "Web data, news, and textbooks offer informative but unstructured multimodal text. The ability to translate multimodal text into a semantic representation that is amenable to further reasoning is a key step toward taming information overload, one of the fundamental problems in modern AI. Designing systems that can understand and use multimodal text requires multiple interconnected components: semantic interpretation, multimodal alignment, knowledge acquisition, and reasoning. Most previous work has focused on a single component in isolation and ignored the high-order crucial interdependencies between these tasks. This proposal aims at building a unified frame&#8232;work for learning to read, ground, and reason in multimodal textbooks.  This&#8232; framework will include three interconnected&#8232; components: context-aware visual and textual interpretation, acquiring and representing knowledge, and reasoning. This work is designed for significant social impact through a broad range of applications including educational and accessibility. The advances in understanding textbooks and question answering could be potentially helpful in designing an automatic personalized tutoring system to educate students about algebra, geometry, and science topics. Advancements in visual interpretation and multimodal knowledge could be beneficial to visually impaired individuals to make the diagrammatic information accessible to them. This project will be instrumental for education, research, and collaborative experience for undergraduate and graduate students including under-represented and minority groups.\r\n\r\nThe proposed framework is designed to iteratively read multimodal textbooks in context, acquire knowledge, interpret data, update and prune the acquired knowledge, and finally reason about the queries.  A core challenge is to do robust, scalable, context-aware semantic analysis and reasoning on multimodal text. The proposal is organized in three main thrusts that build upon each other toward the complete proposed framework. First, the project proposes a precise reasoning algorithm in narratives in learning to solve algebra word problems.  The proposed algorithm will learn to combine local contextual cues into a novel semantic structure using the global context of the narrative. Second, it proposes to build an automated system for interpreting and reasoning in multimodal text by learning to ground text and diagram into a formal representation and a new reasoning algorithm to solve those problems. Finally, it will construct a novel, principled machine learning framework for knowledge acquisition, interpretation, and reasoning in multimodal texts - science textbooks. The proposed framework will be applied in conversational dialogs and personalized tutoring systems. The key contributions will include a unified framework for learning to read, ground, and reason in multimodal textbooks, new algorithms for joint multi-modal text and diagram interpretation, precise understanding of narratives, gradual knowledge acquisition, and reasoning.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hanna",
   "pi_last_name": "Hajishirzi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hanna Hajishirzi",
   "pi_email_addr": "hannaneh@uw.edu",
   "nsf_id": "000634179",
   "pi_start_date": "2016-06-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn Ave. NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-44c59b6b-7fff-a3b1-7c65-1d1116e4eef1\">\n<p dir=\"ltr\"><span>Web data, news, and textbooks offer informative but unstructured multimodal text. The ability to translate multimodal text into a semantic representation that is amenable to further reasoning is a key step toward taming information overload, one of the fundamental problems in modern AI. Designing systems that can </span><span>understand </span><span>and </span><span>use </span><span>multimodal text requires multiple interconnected components: semantic interpretation, multimodal alignment, knowledge acquisition, and reasoning. Most previous work has focused on a single component in isolation and ignored the high-order crucial interdependencies between these tasks. This research is focused on building a unified framework for learning to read, ground, and reason, particularly in multimodal textbooks and technical documents, including three interconnected&#8232; components: context-aware visual and textual interpretation, acquiring and representing knowledge, and reasoning.&nbsp; First, the project introduces reasoning algorithms in narratives in learning to solve algebra word problems.&nbsp; Second, it introduces algorithms to build an automated system for interpreting and reasoning in multimodal text by learning to ground text and diagram into a formal representation and a new reasoning algorithm to solve those problems. Finally, it constructs a novel, principled machine learning framework for knowledge acquisition, interpretation, and question answering in general texts, science textbooks and technical documents. The key contributions include a unified framework for learning to read, ground, and reason in general domain, multimodal textbooks and technical documents, new algorithms for joint multi-modal text and diagram interpretation, precise understanding of narratives, gradual knowledge acquisition, and reasoning.</span></p>\n<br />\n<p dir=\"ltr\"><span>This work is designed for significant social impact through a broad range of applications including educational and accessibility. The project has been impactful to the AI and NLP communities by pioneering a new research direction in numerical reasoning and learning to solve math word problems, leading to many follow ups from the research community.&nbsp; Moreover, this research has pioneered an impactful research direction in knowledge acquisition and question answering about technical documents such as scientific articles. In addition, this work has been instrumental for building general-purpose question answering systems that are capable of reasoning. The advances in understanding textbooks and question answering could be potentially helpful in designing an automatic personalized tutoring system to educate students about algebra, geometry, and science topics. Advancements in visual interpretation and multimodal knowledge could be beneficial to visually impaired individuals to make the diagrammatic information accessible to them. This project will be instrumental for education, research, and collaborative experience for undergraduate and graduate students including under-represented and minority groups.</span></p>\n<div></div>\n</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/04/2020<br>\n\t\t\t\t\tModified by: Hanna&nbsp;Hajishirzi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nWeb data, news, and textbooks offer informative but unstructured multimodal text. The ability to translate multimodal text into a semantic representation that is amenable to further reasoning is a key step toward taming information overload, one of the fundamental problems in modern AI. Designing systems that can understand and use multimodal text requires multiple interconnected components: semantic interpretation, multimodal alignment, knowledge acquisition, and reasoning. Most previous work has focused on a single component in isolation and ignored the high-order crucial interdependencies between these tasks. This research is focused on building a unified framework for learning to read, ground, and reason, particularly in multimodal textbooks and technical documents, including three interconnected&#8232; components: context-aware visual and textual interpretation, acquiring and representing knowledge, and reasoning.  First, the project introduces reasoning algorithms in narratives in learning to solve algebra word problems.  Second, it introduces algorithms to build an automated system for interpreting and reasoning in multimodal text by learning to ground text and diagram into a formal representation and a new reasoning algorithm to solve those problems. Finally, it constructs a novel, principled machine learning framework for knowledge acquisition, interpretation, and question answering in general texts, science textbooks and technical documents. The key contributions include a unified framework for learning to read, ground, and reason in general domain, multimodal textbooks and technical documents, new algorithms for joint multi-modal text and diagram interpretation, precise understanding of narratives, gradual knowledge acquisition, and reasoning.\n\n\nThis work is designed for significant social impact through a broad range of applications including educational and accessibility. The project has been impactful to the AI and NLP communities by pioneering a new research direction in numerical reasoning and learning to solve math word problems, leading to many follow ups from the research community.  Moreover, this research has pioneered an impactful research direction in knowledge acquisition and question answering about technical documents such as scientific articles. In addition, this work has been instrumental for building general-purpose question answering systems that are capable of reasoning. The advances in understanding textbooks and question answering could be potentially helpful in designing an automatic personalized tutoring system to educate students about algebra, geometry, and science topics. Advancements in visual interpretation and multimodal knowledge could be beneficial to visually impaired individuals to make the diagrammatic information accessible to them. This project will be instrumental for education, research, and collaborative experience for undergraduate and graduate students including under-represented and minority groups.\n\n\n\n\t\t\t\t\tLast Modified: 12/04/2020\n\n\t\t\t\t\tSubmitted by: Hanna Hajishirzi"
 }
}