{
 "awd_id": "1638429",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Small: DA: Collaborative Research: Real Time Observation Analysis for Healthcare Applications via Automatic Adaptation to Hardware Limitations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-03-01",
 "awd_exp_date": "2017-06-30",
 "tot_intn_awd_amt": 97799.0,
 "awd_amount": 97799.0,
 "awd_min_amd_letter_date": "2016-04-07",
 "awd_max_amd_letter_date": "2016-04-18",
 "awd_abstract_narration": "This research seeks to develop novel machine learning algorithms that enable real-time video and sensor data analysis on large data streams given limited computational resources. The work focuses on healthcare as an application domain where real-time video analysis can prevent user-errors in operating medical devices or provide immediate alerts to caregivers about dangerous situations.  The research will develop algorithms to automatically adapt data analysis approaches to maximize accuracy of analysis within a short time period despite limited available computing resources. Today's healthcare environment is significantly more technologically sophisticated than ever before. Many medical devices are now frequently used in patient's homes, ranging from simple equipment such as canes and wheelchairs to sophisticated items such as glucose meters, ambulatory infusion pumps and laptop-sized ventilators. The rapidly growing home health industry raises new safety concerns about devices being used inappropriately in the home setting. The proposed research is designed to reduce medical device related use-errors by developing computational algorithms that perform real-time video analysis and alert the patient or caregiver when medical devices are not used appropriately. The real-time video and sensor data analysis is also critical to the healthcare systems that monitor the activities of the elderly or those with disabilities in order to allow a caregiver to react immediately to an incident. \r\n\r\nNew machine learning theories and algorithms will automatically adapt to hardware limitations, with the aim to learn from a large number of training examples, a prediction function that (i) is sufficiently accurate in making effective predictions and (ii) can be run efficiently on a specified computer system to deliver time critical results. Three types of prediction models are studied to address the problem of automatic hardware adaptation, including a vector-based model, a matrix-based model, and a prediction model based on a function from a Reproducing Kernel Hilbert Space (RKHS).  A general framework and multiple optimization techniques are being developed to learn accurate prediction models that match limited memory and computational capacity. The new learning algorithms will be evaluated in several medical scenarios through real-time prediction of a patient's activities from observations in the large video archives collected by several healthcare related projects.  The intellectual merit of the proposed work is in bridging the gap between the high complexity of a prediction model and limited computational resources, a scenario that is encountered in many application domains besides healthcare. The proposed research in machine learning algorithms and theories will make it possible to run complicated prediction algorithms on big data within the limitation of a given computing infrastructure. The developed techniques for automatic hardware adaptation will be applied to a large dataset of continuous video and sensor recordings for medically-critical activity recognition.  The project's broader impacts include providing medical experts with algorithms and tools supporting novel approaches to analyzing observational data in their quest to recognize and characterize human behavior. Surveillance systems with continuous observations will be able to categorize salient events with co-located, limited hardware. Researchers with complex data from continuous streams will be able to explore their domains with greater accuracy within constrained time using their available computing resources. Similarly, large archives can be exploited as rapidly as possible with limited hardware.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Hauptmann",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander G Hauptmann",
   "pi_email_addr": "alex@cs.cmu.edu",
   "nsf_id": "000228336",
   "pi_start_date": "2016-04-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Rong",
   "pi_last_name": "Jin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rong Jin",
   "pi_email_addr": "rongjin@cse.msu.edu",
   "nsf_id": "000217305",
   "pi_start_date": "2016-04-07",
   "pi_end_date": "2016-04-18"
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 97799.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research has developed novel machine learning algorithms that  enable real-time or almost real-time video and sensor data analysis on large data streams  when given limited computational resources. The work focused mainly on healthcare as  an application domain where real-time video analysis can prevent  user-errors in operating medical devices or provide immediate alerts to  caregivers about dangerous situations. Other domains related to video were also considered. The research developed  algorithms and adapted data analysis approaches to maximize  accuracy of analysis with limited time and computing resources.</p>\n<p>The intellectual merit of the proposed work is in bridging the gap  between the high complexity of a prediction model and limited  computational resources, a scenario that is encountered in many  application domains besides healthcare. The newly developed machine  learning algorithms&nbsp;&nbsp; make it possible to run  complicated prediction algorithms on big data such as video archives within the limitation of a  given computing infrastructure. The developed techniques can be applied to a large dataset of&nbsp;  video and sensor recordings for medically-relevant as well as generic activity recognition tasks.</p>\n<p>The project's broader impacts include tools for medical research in the form of  algorithms&nbsp; supporting novel approaches to analyzing  observational data in their quest. E,g. Observational systems in nursing homes with continuous observations arenow able  to categorize salient events with co-located, limited hardware.  Other researchers with complex data from continuous streams will be able to  explore their domains with greater accuracy within constrained time  using their available computing resources. Similarly, large archives can  be exploited as rapidly as possible with limited hardware.</p>\n<p>More specifically, we have focused on different tasks&nbsp; within the scope of the proposed research. The first work is video localization, for which we collected images with annotated bounding box from external sources, e.g., ImageNet Detection dataset and manually annotate bounding boxes for categories without any annotations. We trained frame-level detectors using ResNet-200 features and for action classes we also used Improved Dense Trajectories features. Finally, we fused bounding box score, frame score and shot score to get the best score for each bounding box.</p>\n<p>Second, we enhanced the attention-based neural machine translation by incorporating information in multiple modalities. We explored different encoder-decoder architectures including the LSTM with multiple sequential global/regional visual and textual features as states for attention and the parallel LSTM threads approach.</p>\n<p>Third, we worked on semantics assisted event detection by developing an algorithm established on a correlation vector that correlates semantic concepts to a target event. For this, we incorporated latent (learned) video attributes as extra information into the event detector learnt in a joint framework. Our approach does not train attribute detectors and apply them to event videos to get the confidence scores as the intermediate representation. This&nbsp; avoids two shortcomings. First, our method is not bound to an intermediate representation trained from attribute detectors, which is constrained by the number of attributes. Second, different attributes have a different value for a particular event. Our method jointly learns from video attributes and a target event. The additional information uncovered from those attributes is event specific, and this approach alleviates the difficulty of deciding what attributes to use for different events.</p>\n<p>We also worked on event reconstruction data generation for urban events such as shootings. We showed that the videos included in the dataset are quite diverse as they cover different moments of the event at different positions from different perspectives. We provided high precision ground truth labels for localization and also fine granularity ground truth labels for synchronization. We derived a number of metrics on the video level and frame level to evaluate the two tasks.</p>\n<p>Next, we continued our work on automatic video event localization. We formulated the event video localization task as a region-based joint saliency estimation and matching problem. Additionally, we derived an iterative solution to the non-convex optimization problem composed of closed form solution for saliency estimation and self-paced learning for region matching model. The resulted solution output is not only accurate but also interpretable by human beings.</p>\n<p>Finally, we developed a more robust PCA that is robust to outliers and also invariant to rotation. More importantly, the reformulated objective not only automatically avoids the calculation of optimal mean and makes the assumption of centered data unnecessary, but also theoretically connects to the minimization of reconstruction error.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/12/2017<br>\n\t\t\t\t\tModified by: Alexander&nbsp;G&nbsp;Hauptmann</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis research has developed novel machine learning algorithms that  enable real-time or almost real-time video and sensor data analysis on large data streams  when given limited computational resources. The work focused mainly on healthcare as  an application domain where real-time video analysis can prevent  user-errors in operating medical devices or provide immediate alerts to  caregivers about dangerous situations. Other domains related to video were also considered. The research developed  algorithms and adapted data analysis approaches to maximize  accuracy of analysis with limited time and computing resources.\n\nThe intellectual merit of the proposed work is in bridging the gap  between the high complexity of a prediction model and limited  computational resources, a scenario that is encountered in many  application domains besides healthcare. The newly developed machine  learning algorithms   make it possible to run  complicated prediction algorithms on big data such as video archives within the limitation of a  given computing infrastructure. The developed techniques can be applied to a large dataset of   video and sensor recordings for medically-relevant as well as generic activity recognition tasks.\n\nThe project's broader impacts include tools for medical research in the form of  algorithms  supporting novel approaches to analyzing  observational data in their quest. E,g. Observational systems in nursing homes with continuous observations arenow able  to categorize salient events with co-located, limited hardware.  Other researchers with complex data from continuous streams will be able to  explore their domains with greater accuracy within constrained time  using their available computing resources. Similarly, large archives can  be exploited as rapidly as possible with limited hardware.\n\nMore specifically, we have focused on different tasks  within the scope of the proposed research. The first work is video localization, for which we collected images with annotated bounding box from external sources, e.g., ImageNet Detection dataset and manually annotate bounding boxes for categories without any annotations. We trained frame-level detectors using ResNet-200 features and for action classes we also used Improved Dense Trajectories features. Finally, we fused bounding box score, frame score and shot score to get the best score for each bounding box.\n\nSecond, we enhanced the attention-based neural machine translation by incorporating information in multiple modalities. We explored different encoder-decoder architectures including the LSTM with multiple sequential global/regional visual and textual features as states for attention and the parallel LSTM threads approach.\n\nThird, we worked on semantics assisted event detection by developing an algorithm established on a correlation vector that correlates semantic concepts to a target event. For this, we incorporated latent (learned) video attributes as extra information into the event detector learnt in a joint framework. Our approach does not train attribute detectors and apply them to event videos to get the confidence scores as the intermediate representation. This  avoids two shortcomings. First, our method is not bound to an intermediate representation trained from attribute detectors, which is constrained by the number of attributes. Second, different attributes have a different value for a particular event. Our method jointly learns from video attributes and a target event. The additional information uncovered from those attributes is event specific, and this approach alleviates the difficulty of deciding what attributes to use for different events.\n\nWe also worked on event reconstruction data generation for urban events such as shootings. We showed that the videos included in the dataset are quite diverse as they cover different moments of the event at different positions from different perspectives. We provided high precision ground truth labels for localization and also fine granularity ground truth labels for synchronization. We derived a number of metrics on the video level and frame level to evaluate the two tasks.\n\nNext, we continued our work on automatic video event localization. We formulated the event video localization task as a region-based joint saliency estimation and matching problem. Additionally, we derived an iterative solution to the non-convex optimization problem composed of closed form solution for saliency estimation and self-paced learning for region matching model. The resulted solution output is not only accurate but also interpretable by human beings.\n\nFinally, we developed a more robust PCA that is robust to outliers and also invariant to rotation. More importantly, the reformulated objective not only automatically avoids the calculation of optimal mean and makes the assumption of centered data unnecessary, but also theoretically connects to the minimization of reconstruction error. \n\n \n\n\t\t\t\t\tLast Modified: 10/12/2017\n\n\t\t\t\t\tSubmitted by: Alexander G Hauptmann"
 }
}