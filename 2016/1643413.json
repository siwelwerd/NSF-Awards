{
 "awd_id": "1643413",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Training A Mobile Robot from Human Feedback via Income Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 70000.0,
 "awd_amount": 70000.0,
 "awd_min_amd_letter_date": "2016-08-09",
 "awd_max_amd_letter_date": "2016-08-09",
 "awd_abstract_narration": "As cyberphysical systems become more widespread, there is an increasing number of complex tasks that they can usefully perform to assist human users. Tasks are typically formalized in the sequential decision framework, where the learner perceives states, takes actions, and receives a reward feedback signal. In practice, there is a critical need to learn directly from human users if such machines are to accomplish tasks outside of those pre-specified by the original developers. This project will develop new algorithms that can learn more effectively from humans. We will evaluate these algorithms in both virtual agents and on robot platforms. We will investigate whether and how non-expert humans can construct sequences of tasks of increasing difficulty, similar to how expert animal trainers shape tasks. Insights from these user studies will be leveraged to further improve our algorithms' abilities to learn from human trainers. Once successful, this project will make critical progress towards allowing non-technical users to be able to teach virtual and physical agents to perform complex tasks in a natural setting, familiar to many from previous experience in training household pets.\r\n\r\nThis project is a part of a larger effort between Washington State University (WSU), North Carolina State University, and Brown University. The Brown effort will focus on deriving a well-motivated learning algorithm (tentatively called \"I-learning\") and understanding its theoretical properties. Of particular interest is the behavior of these algorithms in settings that are well studied in the reinforcement-learning community such as Markov decisions processes, k-armed bandit, and learning with function approximation. Algorithms will be implemented and tested on virtual and physical platforms (robots) and broader impacts on education and control will be pursued.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Littman",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Littman",
   "pi_email_addr": "mlittman@cs.brown.edu",
   "nsf_id": "000210482",
   "pi_start_date": "2016-08-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "115 Waterman Streete",
  "perf_city_name": "Providence",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 70000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As virtual agents and physical robots become more common, there is an increasing number of complex tasks they can usefully perform to assist humans. These tasks are typically formalized as sequential decision tasks, where robots and agents perceive states, take actions, and receive a reward feedback signal. While sequential decision tasks have been well-studied in a machine-learning context, a common assumption is that a stationary reward function has been fully specified and that learning can be completely autonomous; however, in practice, there is a critical need to learn directly from human users---the majority of human users will not be able to directly program or fully specify a useful reward function. On the other hand, they can likely train an agent to perform tasks unanticipated by the original designer. Machine reinforcement learning (RL), a paradigm often used for solving sequential decision making tasks, was originally developed with inspiration from animal learning research</p>\n<p>This project leveraged insights from animal training to reformulate the learning of sequential tasks from an agent learning alone in a fixed environment to an agent learning cooperatively with a competent, but not necessarily perfect, human teacher.</p>\n<p>Intellectual Merit: This project looked at the training of software agents through the lens of animal training, with a focus on dog training, to develop several new approaches. The COACH algorithm is novel because it formalizes because it makes use of an insight gleaned from experiments with human trainers---people provide feedback relative to their perception of the agent's current skill level. COACH can use this type of feedback to hone its behavior to be more in line with the expectations of human trainers. The project went beyond this insight to provide a new algorithm that can build up new behaviors in stages to make learning from limited human feedback maximally easy and effective. Development was guided and validated through experiments with both expert and novice human trainers, on both virtual agents and robotic platforms.</p>\n<p>Broader Impacts: The long-term outcome of this project is to enable non-technical users to better train and interact with both virtual and physical software agents. By allowing humans to train using a natural interface, the ability to automate tasks and cooperate with intelligent agents will no longer be limited to the technical elite, potentially impacting a wide-variety of end users of smart devices, computers, and personal robots. The project focused on outreach, dissemination, incorporation of undergraduate researchers, and the release of open-source software packages.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/06/2019<br>\n\t\t\t\t\tModified by: Michael&nbsp;L&nbsp;Littman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAs virtual agents and physical robots become more common, there is an increasing number of complex tasks they can usefully perform to assist humans. These tasks are typically formalized as sequential decision tasks, where robots and agents perceive states, take actions, and receive a reward feedback signal. While sequential decision tasks have been well-studied in a machine-learning context, a common assumption is that a stationary reward function has been fully specified and that learning can be completely autonomous; however, in practice, there is a critical need to learn directly from human users---the majority of human users will not be able to directly program or fully specify a useful reward function. On the other hand, they can likely train an agent to perform tasks unanticipated by the original designer. Machine reinforcement learning (RL), a paradigm often used for solving sequential decision making tasks, was originally developed with inspiration from animal learning research\n\nThis project leveraged insights from animal training to reformulate the learning of sequential tasks from an agent learning alone in a fixed environment to an agent learning cooperatively with a competent, but not necessarily perfect, human teacher.\n\nIntellectual Merit: This project looked at the training of software agents through the lens of animal training, with a focus on dog training, to develop several new approaches. The COACH algorithm is novel because it formalizes because it makes use of an insight gleaned from experiments with human trainers---people provide feedback relative to their perception of the agent's current skill level. COACH can use this type of feedback to hone its behavior to be more in line with the expectations of human trainers. The project went beyond this insight to provide a new algorithm that can build up new behaviors in stages to make learning from limited human feedback maximally easy and effective. Development was guided and validated through experiments with both expert and novice human trainers, on both virtual agents and robotic platforms.\n\nBroader Impacts: The long-term outcome of this project is to enable non-technical users to better train and interact with both virtual and physical software agents. By allowing humans to train using a natural interface, the ability to automate tasks and cooperate with intelligent agents will no longer be limited to the technical elite, potentially impacting a wide-variety of end users of smart devices, computers, and personal robots. The project focused on outreach, dissemination, incorporation of undergraduate researchers, and the release of open-source software packages.\n\n \n\n\t\t\t\t\tLast Modified: 01/06/2019\n\n\t\t\t\t\tSubmitted by: Michael L Littman"
 }
}