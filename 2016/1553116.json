{
 "awd_id": "1553116",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Learning and Using Models of Geo-Temporal Appearance",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2016-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 499426.0,
 "awd_amount": 523426.0,
 "awd_min_amd_letter_date": "2016-02-16",
 "awd_max_amd_letter_date": "2019-03-14",
 "awd_abstract_narration": "Billions of geotagged and time-stamped images are publicly available via the Internet, providing a rich record of the appearance of people, places, and things across the globe. These images are a largely untapped resource that could be used to improve our understanding of the world and how it changes over time. This project develops automated methods of extracting useful information from this imagery and fusing it into high-resolution global models that capture geo-temporal trends. Once the trends have been captured, these models are used to improve performance on computer vision tasks and make geotagged imagery a usable and navigable resource for education and research in other disciplines. The project includes an education and outreach component that brings real-world problems to computer science (CS) students, mentors students across the educational spectrum, and makes the research accessible to the public.\r\n\r\nThis project develops computer vision technologies to capture spatial and temporal appearance trends and is organized into four main research thrusts: (1) investigating novel methods for extracting information from Internet imagery using weakly supervised learning, (2) developing techniques that integrate ground-level imagery with aerial and satellite data to model the expected image appearance anywhere in the world at any time, (3) evaluating methods for using such models to improve the performance of computer vision algorithms, and (4) automatically creating visual representations that make it possible for novice users to explore the learned geo-temporal trends via the Internet.\r\n\r\nProject webpage: http://geotemporal.csr.uky.edu",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nathan",
   "pi_last_name": "Jacobs",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nathan Jacobs",
   "pi_email_addr": "jacobsn@wustl.edu",
   "nsf_id": "000581550",
   "pi_start_date": "2016-02-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Kentucky Research Foundation",
  "inst_street_address": "500 S LIMESTONE",
  "inst_street_address_2": "109 KINKEAD HALL",
  "inst_city_name": "LEXINGTON",
  "inst_state_code": "KY",
  "inst_state_name": "Kentucky",
  "inst_phone_num": "8592579420",
  "inst_zip_code": "405260001",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "KY06",
  "org_lgl_bus_name": "UNIVERSITY OF KENTUCKY RESEARCH FOUNDATION, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "H1HYA8Z1NTM5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Kentucky Research Foundation",
  "perf_str_addr": "500 South Limestone, 109 Kinkead",
  "perf_city_name": "Lexington",
  "perf_st_code": "KY",
  "perf_st_name": "Kentucky",
  "perf_zip_code": "405260001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "KY06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 507426.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explored ways to combine imagery from various sources, including social media, ground-based and aerial vehicles, outdoor webcams, and satellites, to improve our understanding of the world and to develop stronger computer vision algorithms. A significant portion of this work was at the intersection of computer vision and remote sensing. A central theme was exploiting the known capture location and time of the imagery to facilitate cross-modal and weakly supervised learning.</p>\n<p>We proposed novel computer-vision approaches to address a long-standing, challenging problem in remote sensing: fine-grained land use estimation. Our approach included a novel neural network architecture for combining overhead and ground-level imagery to make predictions that are simultaneously more accurate and of higher-spatial resolution than could be made without such a unified architecture.</p>\n<p>We also addressed one of the key challenges in remote sensing, the relative lack of high quality, large scale annotated datasets. While this is a general problem for learning-based methods, it is especially challenging in remote sensing giving the wide variety of unique sensor platforms. We proposed a general-purpose approach for overcoming this problem by using weakly supervised learning. The key idea is to extract semantic information from strong, pre-trained neural networks that were trained for ground-level image understanding. This is done by using these networks to extract the semantic information from a set of geotagged ground-level images and then learning to predict these attributes from satellite imagery of the same location. We proposed methods that predicted scene categories, transient attributes, object counts, and semantic segmentations. We extend this model to also be dependent on when the image was captured and its rough geographic location, thereby capturing long-term temporal trends and global spatial patterns. We further extended this to support the image forensics task of recognizing when the temporal metadata of a ground-level image has been tampered with. These approaches make it possible to map attributes that would be difficult, or impossible, to map using traditional supervised learning approaches. In addition to semantic attributes, we proposed novel algorithms for several image synthesis tasks, including estimating the ground-level view given only an overhead image and estimating how a panoramic image would change if the camera were moved. In addition to their immediate usefulness for mapping semantic attributes, these methods can serve as initialization for remote sensing tasks thereby reducing the amount of manual annotation needed to train a strong model.</p>\n<p>We also addressed the task of absolute pose regression, in which the location of the camera within a given scene is directly predicted from a single image using a neural network. Such approaches have traditionally been black-box neural networks that are trained to work in specific scenes, which means it is difficult to understand why they fail on a given image and that they must be completely retrained for new scenes. We proposed a novel approach that isolates the scene-dependent component in a single network layer, allowing for fast retraining on new scenes without losing significant performance on current scenes. We also proposed a novel non-black box architecture, which explicitly modularizes the network architecture, allowing for further improvements when generalizing to new scenes. This modular approach leads to significant improvements in accuracy and interpretability, all without sacrificing computational speed.</p>\n<p>In addition to these core research contributions, we focused significant effort on applications of the proposed techniques. One major area of work was in transportation infrastructure assessment, where we proposed approach for estimating roadway safety, average roadway speeds, estimating the spatial extent of sinkholes, and how roadway speeds are likely to vary over time. We also proposed solutions for deforestation detection and estimating populations in refugee camps. These approaches both benefit from and inspired the core research that we performed.</p>\n<p>To support this work, we developed several datasets that are made available for future research, including the Brooklyn and Queens datasets for fine-grained land use estimation, the Cross-View Time dataset, the Cross-View Sound dataset, the Cross-View ScenicOrNot dataset, and the Brooklyn Panorama Synthesis dataset. Several of these datasets have already been used by other research groups seeking to improve upon the methods we developed.</p>\n<p>Collectively, this project has proposed novel tasks, algorithms, and datasets that will be useful as a foundation for future research and applications in a broad range of disciplines.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/03/2021<br>\n\t\t\t\t\tModified by: Nathan&nbsp;Jacobs</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project explored ways to combine imagery from various sources, including social media, ground-based and aerial vehicles, outdoor webcams, and satellites, to improve our understanding of the world and to develop stronger computer vision algorithms. A significant portion of this work was at the intersection of computer vision and remote sensing. A central theme was exploiting the known capture location and time of the imagery to facilitate cross-modal and weakly supervised learning.\n\nWe proposed novel computer-vision approaches to address a long-standing, challenging problem in remote sensing: fine-grained land use estimation. Our approach included a novel neural network architecture for combining overhead and ground-level imagery to make predictions that are simultaneously more accurate and of higher-spatial resolution than could be made without such a unified architecture.\n\nWe also addressed one of the key challenges in remote sensing, the relative lack of high quality, large scale annotated datasets. While this is a general problem for learning-based methods, it is especially challenging in remote sensing giving the wide variety of unique sensor platforms. We proposed a general-purpose approach for overcoming this problem by using weakly supervised learning. The key idea is to extract semantic information from strong, pre-trained neural networks that were trained for ground-level image understanding. This is done by using these networks to extract the semantic information from a set of geotagged ground-level images and then learning to predict these attributes from satellite imagery of the same location. We proposed methods that predicted scene categories, transient attributes, object counts, and semantic segmentations. We extend this model to also be dependent on when the image was captured and its rough geographic location, thereby capturing long-term temporal trends and global spatial patterns. We further extended this to support the image forensics task of recognizing when the temporal metadata of a ground-level image has been tampered with. These approaches make it possible to map attributes that would be difficult, or impossible, to map using traditional supervised learning approaches. In addition to semantic attributes, we proposed novel algorithms for several image synthesis tasks, including estimating the ground-level view given only an overhead image and estimating how a panoramic image would change if the camera were moved. In addition to their immediate usefulness for mapping semantic attributes, these methods can serve as initialization for remote sensing tasks thereby reducing the amount of manual annotation needed to train a strong model.\n\nWe also addressed the task of absolute pose regression, in which the location of the camera within a given scene is directly predicted from a single image using a neural network. Such approaches have traditionally been black-box neural networks that are trained to work in specific scenes, which means it is difficult to understand why they fail on a given image and that they must be completely retrained for new scenes. We proposed a novel approach that isolates the scene-dependent component in a single network layer, allowing for fast retraining on new scenes without losing significant performance on current scenes. We also proposed a novel non-black box architecture, which explicitly modularizes the network architecture, allowing for further improvements when generalizing to new scenes. This modular approach leads to significant improvements in accuracy and interpretability, all without sacrificing computational speed.\n\nIn addition to these core research contributions, we focused significant effort on applications of the proposed techniques. One major area of work was in transportation infrastructure assessment, where we proposed approach for estimating roadway safety, average roadway speeds, estimating the spatial extent of sinkholes, and how roadway speeds are likely to vary over time. We also proposed solutions for deforestation detection and estimating populations in refugee camps. These approaches both benefit from and inspired the core research that we performed.\n\nTo support this work, we developed several datasets that are made available for future research, including the Brooklyn and Queens datasets for fine-grained land use estimation, the Cross-View Time dataset, the Cross-View Sound dataset, the Cross-View ScenicOrNot dataset, and the Brooklyn Panorama Synthesis dataset. Several of these datasets have already been used by other research groups seeking to improve upon the methods we developed.\n\nCollectively, this project has proposed novel tasks, algorithms, and datasets that will be useful as a foundation for future research and applications in a broad range of disciplines.\n\n \n\n\t\t\t\t\tLast Modified: 11/03/2021\n\n\t\t\t\t\tSubmitted by: Nathan Jacobs"
 }
}