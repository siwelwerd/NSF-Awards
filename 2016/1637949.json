{
 "awd_id": "1637949",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: Experiential Learning for Robots: From Physics to Actions to Tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 648000.0,
 "awd_amount": 648000.0,
 "awd_min_amd_letter_date": "2016-08-17",
 "awd_max_amd_letter_date": "2016-08-17",
 "awd_abstract_narration": "Recent advances in machine learning coupled with unprecedented archives of labeled data are advancing machine perception at a remarkable rate. However, applying these advances to robotics has not advanced as quickly because learning for robotics requires both active interaction with the physical world, and the ability to generalize over a variety of task contexts. This project addresses this knowledge gap through the development of new learning methods to produce experience-based models of physics. In this approach, an object or category specific model of physics is learned directly from perceptual data rather than deploying general-purpose physical simulation methods. These physical models will support both direct control of action - for example pouring a liquid into a container, and the learning of the physical effects of sequences of actions - for example planning to handle fluids in a laboratory. More generally, these methods will provide a means for robots to learn how to handle fluids, soft materials, and other complex physical phenomena.\r\n\r\n    The proposed experiential learning framework will build on recent advances in deep neural networks. The key problem is to learn the mappings between raw perceptual and control data via a low-dimensional implicit physics space representing a perception-based physical model of how an object acts in the environment. Three directions will be investigated: 1) the development of experiential physics models for object interaction and fluid flow that have strong predictive capabilities, 2) creating mappings directly from experiential models to control of actions such as pouring or moving an object, 3) the assembly of local experience-based controllers into complex tasks from interactive demonstration. Additionally, the project will develop unique data sets that include physical models, simulations, data components, and learned components that other groups can access and build on to enable comparative research similar to what has emerged in machine perception.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Hager",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory D Hager",
   "pi_email_addr": "hager@cs.jhu.edu",
   "nsf_id": "000385453",
   "pi_start_date": "2016-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Marin",
   "pi_last_name": "Kobilarov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Marin Kobilarov",
   "pi_email_addr": "marin@jhu.edu",
   "nsf_id": "000629795",
   "pi_start_date": "2016-08-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 648000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Recent advances in machine learning coupled with unprecedented archives of labeled data are advancing machine perception at a remarkable rate. However, applying these advances to robotics has not advanced as quickly because learning for robotics requires both active interaction with the physical world, and the ability to generalize over a variety of task contexts.</p>\n<p>This project addressed this knowledge gap through the development of new learning methods to produce experience-based models of physics. In this approach, an object or category specific model of physics is learned directly from perceptual data rather than deploying general-purpose physical simulation methods. These physical models will support both direct control of action - for example pouring a liquid into a container, and the learning of the physical effects of sequences of actions - for example planning to handle fluids in a laboratory. More generally, these methods will provide a means for robots to learn how to handle fluids, soft materials, and other complex physical phenomena.</p>\n<p>Over the course of this award, we were able to report significant progress along all of these lines. The group at the University of Washington developed significant advances in deep-learning based simulations of fluids, and was able to demonstrate new control methods making use these models. The group at Johns Hopkins was able to make sure of simulation to learn to perform complex multi-step tasks, and demonstrated the transfer of the learned models directly to a real robot without alteration. Additional, predictive models were learned from simulation and these models were used to develop perception-grounded planners for multi-step tasks. Another set of related methods were developed that enabled a mobile robot to learn to drive in simulation, and transfer those methods to a real robot navigating outdoors. Finally, simulation methods were employed to learn to navigate a surgical tool inside the human body, and the trained models were deployed on a simulated surgical robot. Taken together, these methods provide new and practical approaches to a multitude of real-world perception-driven robotic tasks.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/11/2021<br>\n\t\t\t\t\tModified by: Gregory&nbsp;D&nbsp;Hager</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nRecent advances in machine learning coupled with unprecedented archives of labeled data are advancing machine perception at a remarkable rate. However, applying these advances to robotics has not advanced as quickly because learning for robotics requires both active interaction with the physical world, and the ability to generalize over a variety of task contexts.\n\nThis project addressed this knowledge gap through the development of new learning methods to produce experience-based models of physics. In this approach, an object or category specific model of physics is learned directly from perceptual data rather than deploying general-purpose physical simulation methods. These physical models will support both direct control of action - for example pouring a liquid into a container, and the learning of the physical effects of sequences of actions - for example planning to handle fluids in a laboratory. More generally, these methods will provide a means for robots to learn how to handle fluids, soft materials, and other complex physical phenomena.\n\nOver the course of this award, we were able to report significant progress along all of these lines. The group at the University of Washington developed significant advances in deep-learning based simulations of fluids, and was able to demonstrate new control methods making use these models. The group at Johns Hopkins was able to make sure of simulation to learn to perform complex multi-step tasks, and demonstrated the transfer of the learned models directly to a real robot without alteration. Additional, predictive models were learned from simulation and these models were used to develop perception-grounded planners for multi-step tasks. Another set of related methods were developed that enabled a mobile robot to learn to drive in simulation, and transfer those methods to a real robot navigating outdoors. Finally, simulation methods were employed to learn to navigate a surgical tool inside the human body, and the trained models were deployed on a simulated surgical robot. Taken together, these methods provide new and practical approaches to a multitude of real-world perception-driven robotic tasks.\n\n\t\t\t\t\tLast Modified: 04/11/2021\n\n\t\t\t\t\tSubmitted by: Gregory D Hager"
 }
}