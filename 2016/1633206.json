{
 "awd_id": "1633206",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "BIGDATA: Collaborative Research: IA: BirdVox: Automating Acoustic Monitoring of Migrating Bird Species",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 947018.0,
 "awd_amount": 963018.0,
 "awd_min_amd_letter_date": "2016-09-13",
 "awd_max_amd_letter_date": "2021-02-17",
 "awd_abstract_narration": "Current bioacoustic monitoring of natural environments requires processing by humans to extract information content from recordings. Thus human processing creates a fundamental bottleneck in which data collection far outpaces capabilities to extract relevant and desired information. Bioacoustic research on automatic species classification in natural environments can be broadly divided into two groups: distinguishing a predefined set of known species from audio clips and extracting species as events that occur in a continuous audio stream. Both classification techniques have their specific problems--many of the data used distinguishing predefined species are recorded under \"studio\" conditions and not extensible to natural conditions, while processing of continuous audio streams generate many false positives. \r\n\r\nTo overcome these challenges we will take a multi-tiered approach:  Analyzing a data set consisting of full-night recordings from 10 recording units over 100 nights. Building a web-enabled software to engage citizen scientists to identify the flight calls, providing us with a large and extensive model training dataset.  Developing novel convolutional deep-learning networks, which are well suited for analysis of complex auditory scenes.  Visualizing patterns detected and classified flight calls in space and time to produce novel information about the bird migration.  Comparing model-generated acoustic data with radar, video, and direct visual citizen science datasets to produce the most comprehensive accounts of nocturnal bird migration possible.  The combination of domain knowledge in bird vocalizations, engaging citizen scientists to allow development of large well annotated training datasets, and taking a novel deep-learning approach, will finally resolve the machine classification of acoustic signals in natural environments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Kelling",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Steven T Kelling",
   "pi_email_addr": "stk2@cornell.edu",
   "nsf_id": "000203575",
   "pi_start_date": "2016-09-13",
   "pi_end_date": "2021-02-17"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrew",
   "pi_last_name": "Farnsworth",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andrew Farnsworth",
   "pi_email_addr": "af27@cornell.edu",
   "nsf_id": "000558151",
   "pi_start_date": "2021-02-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Andrew",
   "pi_last_name": "Farnsworth",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andrew Farnsworth",
   "pi_email_addr": "af27@cornell.edu",
   "nsf_id": "000558151",
   "pi_start_date": "2016-09-13",
   "pi_end_date": "2021-02-17"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Holger",
   "pi_last_name": "Klinck",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Holger Klinck",
   "pi_email_addr": "Holger.Klinck@cornell.edu",
   "nsf_id": "000690820",
   "pi_start_date": "2016-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "159 Sapsucker Wood Road",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148501999",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  },
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 947018.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>BirdVox developed an automated detection and classification pipeline for acoustic data containing signals of nocturnally migrating birds, producing peer-reviewed publications, diverse collaborations, features in numerous media outlets, and broader impacts in bioacoustic methods and machine learning.</p>\n<p>At project outset in 2016, bioacoustic monitoring of natural environments required, generally, an immense amount of human processing to extract information content from recordings, creating a fundamental bottleneck in which data collection far outpaced capabilities to extract relevant and desired information.&nbsp;Our primary goal was to develop computational approaches to automate collection and annotation of large datasets of vocalizations from migrating birds in diverse acoustic environments. We aimed to:</p>\n<p>(1) collect a large amount of audio data with human-annotated bird vocalizations for training and validation of our tools;</p>\n<p>(2) develop novel computational methods for flight call detection in continuous audio stream containing mostly non-relevant sonic events and variable background noise; and</p>\n<p>(3) develop novel computational methods for robustly classifying these calls correctly to species, in scenarios in which calls may come from a large number of different species and may contain significant variation among individuals within a species.&nbsp;</p>\n<p>We took a multi-tiered approach: analyzing a dataset consisting of full-night recordings from 10 recording units over 100 nights; building a web-enabled software to engage citizen scientists to identify flight calls, providing us with a large and extensive model training dataset; developing novel convolutional deep-learning networks, well suited for analysis of complex auditory scenes; visualizing patterns detected and classified flight calls in space and time to produce novel information about bird migration; and comparing model-generated acoustic data with radar and citizen science datasets to produce the most comprehensive accounts of nocturnal bird migration possible.&nbsp;</p>\n<p>In particular:</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;we implemented a state-of-the-art audio classification model based on unsupervised feature learning and evaluated it on three novel datasets, one for studying the N-class problem and two realistic datasets for studying the monitoring scenario comprising hundreds of thousands of audio clips that were compiled by means of remote acoustic sensors deployed in the field during two migration seasons;</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;we explored state-of-the-art classification techniques for large-vocabulary bird species classification from flight calls, in particular by contrasting a ?shallow learning? approach based on unsupervised dictionary learning with a deep convolutional neural network combined with data augmentation. We showed that the two models perform comparably, with both significantly outperforming a baseline model;</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;we collaborated with Google on the application of per channel energy normalization (PCEN) on various datasets of natural acoustic environments as a means of normalizing background noise while decorrelating frequency bands to enhance signal detection;</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;we developed and employed a convolutional neural network (CNN) yielding state-of-the-art accuracy, introducing into it two noise adaptation techniques, respectively integrating short-term (60 ms) and long-term (30 min) context: PCEN and a context-adaptive neural network (CA-NN) layer;</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;we performed unsupervised detection of bioacoustic events by pooling the magnitudes of spectrogram frames after PCEN, proving that PCEN generalizes logarithm-based spectral flux, yet with a tunable time scale for background noise estimation. In comparison with point- wise logarithm, PCEN reduced false alarm rate by 50x in the near field and 5x in the far field, both on avian and marine bioacoustic datasets;</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;we introduced TaxoNet, a deep neural network for structured classification of signals from living organisms. An experimental benchmark on two new bioacoustic datasets led to state-of-the-art results in bird species classification; and</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;we examined the ability of automated acoustic monitoring to provide information about two important facets of nocturnal bird migration: (1) number of migrating birds aloft and (2) migration phenology of individual species and validated these data with contemporaneous data gathered by Doppler weather surveillance radar and visual observations from a large community of community scientists, from which we derive independent measures of migration passage and timing. A model combining acoustic and weather data explained 75% of variation in radar-derived migration passage. Including acoustic data in the model decreased prediction error by 33%. Seasonal migration timing estimated by acoustic sensors was consistent with timing metrics derived from community science data at family and species levels.</p>\n<p>Numerous citations of BirdVox research now appear in peer-reviewed articles, including machine learning, musicology, and ecology. Results include analysis of ecological patterns, soundscapes, deep and shallow learning methodologies, network analyses, representational learning, statistical and computational transformations, denoising, and speech recognition. BirdVox provided direct opportunities for research and teaching; improved the performance, skills, or attitudes of high school, undergraduate, doctoral, and professional underserved communities that will improve their access to or retention in research, teaching, or other related professions; and provided exposure to science and technology across a broad array of academic and community scientists, developers and statisticians, ornithologists, musicologists, and members of the public that have contacted project leaders for collaboration in arts, science projects, and media projects.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/06/2021<br>\n\t\t\t\t\tModified by: Andrew&nbsp;Farnsworth</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638722899282_BirdVoxSampleFigure--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638722899282_BirdVoxSampleFigure--rgov-800width.jpg\" title=\"Visual representation of BirdVox Detection and Classification Processes\"><img src=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638722899282_BirdVoxSampleFigure--rgov-66x44.jpg\" alt=\"Visual representation of BirdVox Detection and Classification Processes\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Automated BirdVox pipeline to detect and classify flight calls of nocturnally migrating birds includes signal processing (a,b), error detection (c, d) and detection and classification outputs (e,f). Algorithmic approaches combine deep, shallow, active, passive learning with ornithological expertise.</div>\n<div class=\"imageCredit\">Vincent Lostanlen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Andrew&nbsp;Farnsworth</div>\n<div class=\"imageTitle\">Visual representation of BirdVox Detection and Classification Processes</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638818335013_ScreenShot2021-12-06at1.57.36PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638818335013_ScreenShot2021-12-06at1.57.36PM--rgov-800width.jpg\" title=\"Multi-modal data describing migration patterns and timing\"><img src=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638818335013_ScreenShot2021-12-06at1.57.36PM--rgov-66x44.jpg\" alt=\"Multi-modal data describing migration patterns and timing\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Using our fully automated migration monitoring pipeline in Fall 2015 in Ithaca, our acoustic system could successfully capture the number of birds migrating aloft (radar and acoustic estimates), as well as the different seasonal timing of different species of birds (eBird and acoustics timing).</div>\n<div class=\"imageCredit\">Benjamin Van Doren</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Andrew&nbsp;Farnsworth</div>\n<div class=\"imageTitle\">Multi-modal data describing migration patterns and timing</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638828523570_ScreenShot2021-12-06at5.05.09PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638828523570_ScreenShot2021-12-06at5.05.09PM--rgov-800width.jpg\" title=\"BirdVox flight call models and system over time\"><img src=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638828523570_ScreenShot2021-12-06at5.05.09PM--rgov-66x44.jpg\" alt=\"BirdVox flight call models and system over time\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Patterns of flight calls and insects in nocturnal audio streams vary over time. Our research investigated how our model recall rates also vary in time. The best model, in the lower right, beats the baseline, top right, during most hours of the night.</div>\n<div class=\"imageCredit\">BirdVox Team, Lostanlen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Andrew&nbsp;Farnsworth</div>\n<div class=\"imageTitle\">BirdVox flight call models and system over time</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638828965827_pone.0214168.g002--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638828965827_pone.0214168.g002--rgov-800width.jpg\" title=\"Figure 2 from Lostanlen et al. 2019\"><img src=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638828965827_pone.0214168.g002--rgov-66x44.jpg\" alt=\"Figure 2 from Lostanlen et al. 2019\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">After training a deep learning model to identify a flight call, we run this on a recording. We compare peaks in resulting event detection function with a fixed threshold?to obtain timestamps for possible flight calls: input representation is a mel-frequency spectrogram; deep learning model is a CNN.</div>\n<div class=\"imageCredit\">BirdVox Team, Lostanlen</div>\n<div class=\"imageSubmitted\">Andrew&nbsp;Farnsworth</div>\n<div class=\"imageTitle\">Figure 2 from Lostanlen et al. 2019</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638834623803_spp-fig-1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638834623803_spp-fig-1--rgov-800width.jpg\" title=\"eBird-Acoustic migration timing relationships\"><img src=\"/por/images/Reports/POR/2021/1633206/1633206_10461477_1638834623803_spp-fig-1--rgov-66x44.jpg\" alt=\"eBird-Acoustic migration timing relationships\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A comparison of the timing of migration as defined by eBird (community science observations) and acoustic (BirdVox automated pipeline) data, shown in terms of peak migration timing for Fall 2015.</div>\n<div class=\"imageCredit\">Van Doren et al. Submitted JAE</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Andrew&nbsp;Farnsworth</div>\n<div class=\"imageTitle\">eBird-Acoustic migration timing relationships</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nBirdVox developed an automated detection and classification pipeline for acoustic data containing signals of nocturnally migrating birds, producing peer-reviewed publications, diverse collaborations, features in numerous media outlets, and broader impacts in bioacoustic methods and machine learning.\n\nAt project outset in 2016, bioacoustic monitoring of natural environments required, generally, an immense amount of human processing to extract information content from recordings, creating a fundamental bottleneck in which data collection far outpaced capabilities to extract relevant and desired information. Our primary goal was to develop computational approaches to automate collection and annotation of large datasets of vocalizations from migrating birds in diverse acoustic environments. We aimed to:\n\n(1) collect a large amount of audio data with human-annotated bird vocalizations for training and validation of our tools;\n\n(2) develop novel computational methods for flight call detection in continuous audio stream containing mostly non-relevant sonic events and variable background noise; and\n\n(3) develop novel computational methods for robustly classifying these calls correctly to species, in scenarios in which calls may come from a large number of different species and may contain significant variation among individuals within a species. \n\nWe took a multi-tiered approach: analyzing a dataset consisting of full-night recordings from 10 recording units over 100 nights; building a web-enabled software to engage citizen scientists to identify flight calls, providing us with a large and extensive model training dataset; developing novel convolutional deep-learning networks, well suited for analysis of complex auditory scenes; visualizing patterns detected and classified flight calls in space and time to produce novel information about bird migration; and comparing model-generated acoustic data with radar and citizen science datasets to produce the most comprehensive accounts of nocturnal bird migration possible. \n\nIn particular:\n\n-       we implemented a state-of-the-art audio classification model based on unsupervised feature learning and evaluated it on three novel datasets, one for studying the N-class problem and two realistic datasets for studying the monitoring scenario comprising hundreds of thousands of audio clips that were compiled by means of remote acoustic sensors deployed in the field during two migration seasons;\n\n-       we explored state-of-the-art classification techniques for large-vocabulary bird species classification from flight calls, in particular by contrasting a ?shallow learning? approach based on unsupervised dictionary learning with a deep convolutional neural network combined with data augmentation. We showed that the two models perform comparably, with both significantly outperforming a baseline model;\n\n-       we collaborated with Google on the application of per channel energy normalization (PCEN) on various datasets of natural acoustic environments as a means of normalizing background noise while decorrelating frequency bands to enhance signal detection;\n\n-       we developed and employed a convolutional neural network (CNN) yielding state-of-the-art accuracy, introducing into it two noise adaptation techniques, respectively integrating short-term (60 ms) and long-term (30 min) context: PCEN and a context-adaptive neural network (CA-NN) layer;\n\n-       we performed unsupervised detection of bioacoustic events by pooling the magnitudes of spectrogram frames after PCEN, proving that PCEN generalizes logarithm-based spectral flux, yet with a tunable time scale for background noise estimation. In comparison with point- wise logarithm, PCEN reduced false alarm rate by 50x in the near field and 5x in the far field, both on avian and marine bioacoustic datasets;\n\n-       we introduced TaxoNet, a deep neural network for structured classification of signals from living organisms. An experimental benchmark on two new bioacoustic datasets led to state-of-the-art results in bird species classification; and\n\n-       we examined the ability of automated acoustic monitoring to provide information about two important facets of nocturnal bird migration: (1) number of migrating birds aloft and (2) migration phenology of individual species and validated these data with contemporaneous data gathered by Doppler weather surveillance radar and visual observations from a large community of community scientists, from which we derive independent measures of migration passage and timing. A model combining acoustic and weather data explained 75% of variation in radar-derived migration passage. Including acoustic data in the model decreased prediction error by 33%. Seasonal migration timing estimated by acoustic sensors was consistent with timing metrics derived from community science data at family and species levels.\n\nNumerous citations of BirdVox research now appear in peer-reviewed articles, including machine learning, musicology, and ecology. Results include analysis of ecological patterns, soundscapes, deep and shallow learning methodologies, network analyses, representational learning, statistical and computational transformations, denoising, and speech recognition. BirdVox provided direct opportunities for research and teaching; improved the performance, skills, or attitudes of high school, undergraduate, doctoral, and professional underserved communities that will improve their access to or retention in research, teaching, or other related professions; and provided exposure to science and technology across a broad array of academic and community scientists, developers and statisticians, ornithologists, musicologists, and members of the public that have contacted project leaders for collaboration in arts, science projects, and media projects.\n\n\t\t\t\t\tLast Modified: 12/06/2021\n\n\t\t\t\t\tSubmitted by: Andrew Farnsworth"
 }
}