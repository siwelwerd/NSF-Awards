{
 "awd_id": "1619376",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: To Ask or Not to Ask - A Foundation for the Optimization of Human-Robot Networks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 497554.0,
 "awd_amount": 497554.0,
 "awd_min_amd_letter_date": "2016-06-30",
 "awd_max_amd_letter_date": "2016-06-30",
 "awd_abstract_narration": "A team of unmanned vehicles can have tremendous impact in areas such as disaster relief, emergency response, and national security.  Human operators are an integral part of these missions. Given the complexity of the tasks, harshness of the operation environments, and resource limitations, a fundamental understanding of how humans and robots can best interact is crucial to a successful operation. This proposal introduces a new multidisciplinary approach for efficient human-robot collaboration on tasks that require a visual search in the presence of uncertainty.  The proposed methodology for characterizing human visual performance also contributes to the field of human perception and cognitive psychology. This proposal also has a significant educational component targeting under-represented students of the central California area and UC Santa Barbara.\r\n\r\nA new methodology is proposed for predicting whether people can make a correct visual decision for a given sensory input.  The approach is based on training convolutional neural networks using extensive input from Amazon Mechanical Turk workers. This foundational understanding of human visual performance will have significant implication for robotic field decision making. A new set of mathematical tools will be developed for robotic field operation, given a proper understanding of human visual capabilities. This enables a robot to properly decide when to seek human help, when to rely on itself, and when to sense more. Finally, the cost of communication is characterized, due to the space-varying nature of link quality, which can significantly affect seeking human help. The proposed theories and design paradigms will be validated using a robotic testbed.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yasamin",
   "pi_last_name": "Mostofi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yasamin Mostofi",
   "pi_email_addr": "ymostofi@ece.ucsb.edu",
   "nsf_id": "000488554",
   "pi_start_date": "2016-06-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Pradeep",
   "pi_last_name": "Sen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Pradeep Sen",
   "pi_email_addr": "psen@ece.ucsb.edu",
   "nsf_id": "000291374",
   "pi_start_date": "2016-06-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "ECE Department",
  "perf_city_name": "Santa Barbara",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931069560",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 497554.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Here are sample outcomes of our award:&nbsp;&nbsp;</p>\n<p>1) We have proposed a novel approach to predict human performance in robotic tasks that require human collaboration. Consider robotic surveillance tasks that involve visual perception. The robot has a limited access to a remote operator to ask for help. However, humans may not be able to accomplish the visual task in many scenarios (e.g., poor sensing quality). We have then proposed a machine learning-based approach that allows the robot to probabilistically predict human visual performance for any visual input. Based on this prediction, we then presented a methodology that allows the robot to properly optimize its field decisions in terms of when to ask for help, when to sense more, and when to rely on itself. The proposed approach enables the robot to ask the right questions, only querying the human operator with the sensory inputs for which humans have a high chance of success. We have experimentally validated the proposed approach extensively on our campus. The results showcase the efficacy of our approach, indicating a considerable increase in the success rate of human queries and the overall performance.&nbsp; We have released our data and codes so other researchers can utilize them for human-robot application.</p>\n<p>2) We have Shown how the robot can infer object similarity from output of its onboard Deep Neural Network for free.&nbsp; We have then developed a new efficient approach for joint scene understanding, path planning, and human query based on this finding.&nbsp; More specifically, we have shown that the correlation coefficient of the automatically-learned DNN features of two object images carries robust information on their similarity, even though the two objects are misclassified, and can thus be utilized to significantly improve the robot&rsquo;s classification accuracy, without additional training. In other words, while the robot may have difficulty labeling many objects, our framework allows it to figure out which objects are the same (while still unclassified). It can then use this information to improve its overall classification performance by properly allocating its given resources (e.g., motion budget, human query) to a smaller carefully-selected number of objects.</p>\n<p>3) Our human visual performance prediction models have significant implications beyond robotic applications, and in the general area of vision. For instance, the ability to estimate the perceptual error between images is an important problem in computer vision with many applications. Although it has been studied extensively, no existing method could robustly predict human visual preference when given two synthesized images. Building on our work on human visual performance prediction, we have then proposed a deep-learning model using a novel, pairwise-learning framework, to predict the human preference of one distorted image over the other. The perceptual error estimated by our new metric, PieAPP, is well-correlated with human opinion. Furthermore, it significantly outperforms existing algorithms, beating the state-of-the-art by almost 3 folds. We have released our codes and data so the research community can build on it.&nbsp;</p>\n<p class=\"Default\">4) Equipped with our human performance predictor, we have extensively studied human-robot collaborative sensing, site inspection and target classification. We considered the realistic case that human performance is not perfect and depends on the sensory input quality, and that the robot has constraints in communication with humans (e.g., limited chances for query, poor channel quality). The robot also has limited onboard motion and communication energy, and operates in realistic channel environments, experiencing path loss, shadowing, and multipath fading. We have then developed the theoretical foundation for how to co-optimize motion, sensing, and human queries in this challenging setting. We further comprehensively validated the proposed approach with extensive real human data (from Amazon MTurk) and real channel data (from downtown San Francisco), confirming that the proposed approach significantly outperforms benchmark methodologies.</p>\n<p class=\"Default\">5) A comprehensive repository of several existing work on different aspects of human-robot collaboration has been generated and shared with the research community.</p>\n<p class=\"Default\">6) In terms of outreach, the PIs have partnered with the UCSB Center for Science and Engineering Partnerships (CSEP) and have hosted a number of minority community college students in their labs over the course of this project. &nbsp;Each student then participated in research experiments and was mentored by a graduate student of the lab during the visit. High school students have also been hosted for 6 weeks in the summer, as part of the UCSB Summer Research Mentorship Program.&nbsp; These students got to work on various problems related to machine learning and vision.&nbsp; Minority graduate students have also been involved in this research in the PIs&rsquo; labs. Finally, one of the PIs has also been a faculty mentor for Dream Scholars, who are undocumented students at UCSB, and further participated in activities by National Society of Black Engineers (NSBE) and Los Ingenieros (for Latino engineering students).</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/08/2021<br>\n\t\t\t\t\tModified by: Yasamin&nbsp;Mostofi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHere are sample outcomes of our award:  \n\n1) We have proposed a novel approach to predict human performance in robotic tasks that require human collaboration. Consider robotic surveillance tasks that involve visual perception. The robot has a limited access to a remote operator to ask for help. However, humans may not be able to accomplish the visual task in many scenarios (e.g., poor sensing quality). We have then proposed a machine learning-based approach that allows the robot to probabilistically predict human visual performance for any visual input. Based on this prediction, we then presented a methodology that allows the robot to properly optimize its field decisions in terms of when to ask for help, when to sense more, and when to rely on itself. The proposed approach enables the robot to ask the right questions, only querying the human operator with the sensory inputs for which humans have a high chance of success. We have experimentally validated the proposed approach extensively on our campus. The results showcase the efficacy of our approach, indicating a considerable increase in the success rate of human queries and the overall performance.  We have released our data and codes so other researchers can utilize them for human-robot application.\n\n2) We have Shown how the robot can infer object similarity from output of its onboard Deep Neural Network for free.  We have then developed a new efficient approach for joint scene understanding, path planning, and human query based on this finding.  More specifically, we have shown that the correlation coefficient of the automatically-learned DNN features of two object images carries robust information on their similarity, even though the two objects are misclassified, and can thus be utilized to significantly improve the robot\u2019s classification accuracy, without additional training. In other words, while the robot may have difficulty labeling many objects, our framework allows it to figure out which objects are the same (while still unclassified). It can then use this information to improve its overall classification performance by properly allocating its given resources (e.g., motion budget, human query) to a smaller carefully-selected number of objects.\n\n3) Our human visual performance prediction models have significant implications beyond robotic applications, and in the general area of vision. For instance, the ability to estimate the perceptual error between images is an important problem in computer vision with many applications. Although it has been studied extensively, no existing method could robustly predict human visual preference when given two synthesized images. Building on our work on human visual performance prediction, we have then proposed a deep-learning model using a novel, pairwise-learning framework, to predict the human preference of one distorted image over the other. The perceptual error estimated by our new metric, PieAPP, is well-correlated with human opinion. Furthermore, it significantly outperforms existing algorithms, beating the state-of-the-art by almost 3 folds. We have released our codes and data so the research community can build on it. \n4) Equipped with our human performance predictor, we have extensively studied human-robot collaborative sensing, site inspection and target classification. We considered the realistic case that human performance is not perfect and depends on the sensory input quality, and that the robot has constraints in communication with humans (e.g., limited chances for query, poor channel quality). The robot also has limited onboard motion and communication energy, and operates in realistic channel environments, experiencing path loss, shadowing, and multipath fading. We have then developed the theoretical foundation for how to co-optimize motion, sensing, and human queries in this challenging setting. We further comprehensively validated the proposed approach with extensive real human data (from Amazon MTurk) and real channel data (from downtown San Francisco), confirming that the proposed approach significantly outperforms benchmark methodologies.\n5) A comprehensive repository of several existing work on different aspects of human-robot collaboration has been generated and shared with the research community.\n6) In terms of outreach, the PIs have partnered with the UCSB Center for Science and Engineering Partnerships (CSEP) and have hosted a number of minority community college students in their labs over the course of this project.  Each student then participated in research experiments and was mentored by a graduate student of the lab during the visit. High school students have also been hosted for 6 weeks in the summer, as part of the UCSB Summer Research Mentorship Program.  These students got to work on various problems related to machine learning and vision.  Minority graduate students have also been involved in this research in the PIs\u2019 labs. Finally, one of the PIs has also been a faculty mentor for Dream Scholars, who are undocumented students at UCSB, and further participated in activities by National Society of Black Engineers (NSBE) and Los Ingenieros (for Latino engineering students).\n\n \n\n \n\n\t\t\t\t\tLast Modified: 03/08/2021\n\n\t\t\t\t\tSubmitted by: Yasamin Mostofi"
 }
}