{
 "awd_id": "1613002",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Canonical Linear Methods and Hierarchical Non-Linear Methods in High-Dimensional Statistics",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Pena Edsel",
 "awd_eff_date": "2016-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2016-05-20",
 "awd_max_amd_letter_date": "2019-06-27",
 "awd_abstract_narration": "Statistics is at the heart of extracting meaningful information from big data. Its primary tasks include estimation and uncertainty assessment. The latter is crucial in big data analysis for sound decision making. For the former, the methods employed in deep learning machines, such as those behind Google's Brain and AlphaGo and Microsoft's Cortana, beg understanding. This research project is intended to bridge practice and theory of statistics in these areas. It aims to provide accessible uncertainty measures for linear modeling of big data and to derive insights into how deep learning works, based on mathematical analysis. \r\n\r\nThis research project develops and analyzes linear and non-linear high-dimensional statistical inferential methods that are easily accessible by practitioners in data science. In the linear case, it develops and analyzes inferential methods based on well-established bootstrap, lasso, partial ridge, and random projection methods. In the non-linear case, it takes the first steps to explain in a principled manner the impressive success of deep learning in practical problems such as image classification and speech recognition. In particular, statistical properties of these methods will be studied under linear and Neyman-Rubin high dimensional models, and via analytical and simulation means. A generative model of a two-layer neural network (or hierarchical non-linear model) will be explored to understand and compare deep learning with other methods, analytically and through simulation studies. Improvements over deep learning as a general supervised learning method are sought by enforcing biologically meaningful constraints from brain connectivity research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bin",
   "pi_last_name": "Yu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bin Yu",
   "pi_email_addr": "binyu@stat.berkeley.edu",
   "nsf_id": "000465148",
   "pi_start_date": "2016-05-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947045940",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 150000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 150000.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 150000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-6a5069d9-7fff-c372-96a8-5a728ccff03c\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The main focus of this research proposal was to develop and analyze linear and non-linear high-dimensional statistical inferential methods that are easily accessible by practitioners in data science. Broadly, the main goals were to: strengthen understanding, interpretability, and predictability of linear and nonlinear methods through theoretical investigations and designing new methods; and establish theoretical guarantees for stability-based model selection and interpretation methods.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The outcomes of this proposal include establishing theoretical properties of random forests (RFs) and deep learning (DL)&nbsp; and developing novel methods to interpret these approaches and improve predictability. Among other contributions, the PI studied and improved feature selection and importances of RFs. For example, the PI proposed a debiased MDI feature importance measure using out-of-bag samples called MDI-oob which achieved state-of-the-art performance in feature selection from RFs for deep and shallow trees. The PI also studied feature selection consistency theory for RFs, obtaining theoretical results on understanding how a theoretically tractable version of iterative RFs (iRF) can discover boolean interactions under the proposed Locally Spiky Sparse model that is inspired by thresholding behavior in many biological processes, including gene regulatory networks. iRF is an approach proposed by the PI that uses a tree ensemble from iteratively modified RFs to obtain predictive and stable non-linear or Boolean interactions of features. The general methodology of RFs was improved via hierarchical shrinkage (HS) and fast interpretable tree sums (FIGS). The former mitigates overfitting when building tree-based models through regularization by shrinking the prediction over each node towards the sample means of its ancestors. The latter mitigates the inefficiency of tree-based models for additive generative models by simultaneously growing a flexible number of trees in a summation.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Regarding DL, the PI made contributions to improve both interpretability and predictability. For example, a method was developed that generates hierarchical interpretations for any neural network predictions. Contextual decomposition explanation penalization (CDEP) was also proposed which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models. Furthermore, the PI extended interpretable methods that attribute importances to features in a transformed space, which is called TRIM (Transformation Importance). Another approach, Adaptive Wavelet Distillation (AWD), was developed which aims to distill information from a trained neural network into a wavelet transform. Specifically, AWD penalizes feature attributions of a neural network in the wavelet domain to learn an effective multi-resolution wavelet transform. The resulting model is highly predictive, concise, computationally efficient, and has properties (e.g., a multi-scale structure) which make it easy to interpret.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Additionally, the Expectation-Maximization (EM) algorithm, one of the widely popular tools for inference with missing data, was studied under model misspecification, i.e., the behavior of EM for parameter estimation in mixture models when the number of mixtures is over-specified. The PI established that slow algorithmic and statistical rates of EM do arise for a class of over-specified mixture models both in small and high-dimensions.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Throughout the duration of this project, the PI built and expanded principles of modern data science through the predictability, computability, and stability (PCS) framework for veridical (truthful) data science. For example, the PCS workflow and documentation was demonstrated in a genomics case study available on Zenodo. The PI is in the process of writing a textbook titled Veridical Data Science: The Practice of Responsible Data Analysis and Decision Making, which teaches Data Science through the lens of the PCS principles and workflow. Furthermore, a python package, VeridicalFlow, was developed to facilitate the practice of veridical data science. This framework also guided much of the methodology mentioned previously.</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Outcomes from this proposal included interdisciplinary projects solving domain problems. For instance, while deep neural networks have been shown to be effective in predicting the V4 neurons&rsquo; responses, it is hard to use those models to interpret neuron functionality. Thus, a stability based method to seek optimal stimuli, DeepTune images, was proposed and confirmed previous findings on the presence of diverse shape and texture tuning in area V4. It provided concrete and naturalistic visualization of predicted optimal stimuli of individual neurons. In addition, AWD was utilized to address challenges in two real-world settings: cosmological parameter inference and molecular-partner prediction. In both cases, AWD yielded a scientifically interpretable and concise model which gave better predictive performance than state-of-the-art neural networks. Moreover, AWD identified predictive features that are scientifically meaningful in the domain context.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project has provided opportunities for many students and postdocs coming from a variety of backgrounds. This included technical training (e.g., utilizing parallel computing to handle large volumes of data) as well as mentoring. Regarding educational impact, the PI was actively involved with the curriculum development of UC Berkeley&rsquo;s Data Science Education Program (DSEP), incorporated relevant research topics into her course content, and some of the high-dimensional analysis examples used in her book were motivated by achievements of this project.&nbsp;</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/01/2022<br>\n\t\t\t\t\tModified by: Bin&nbsp;Yu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The main focus of this research proposal was to develop and analyze linear and non-linear high-dimensional statistical inferential methods that are easily accessible by practitioners in data science. Broadly, the main goals were to: strengthen understanding, interpretability, and predictability of linear and nonlinear methods through theoretical investigations and designing new methods; and establish theoretical guarantees for stability-based model selection and interpretation methods.\n\n \nThe outcomes of this proposal include establishing theoretical properties of random forests (RFs) and deep learning (DL)  and developing novel methods to interpret these approaches and improve predictability. Among other contributions, the PI studied and improved feature selection and importances of RFs. For example, the PI proposed a debiased MDI feature importance measure using out-of-bag samples called MDI-oob which achieved state-of-the-art performance in feature selection from RFs for deep and shallow trees. The PI also studied feature selection consistency theory for RFs, obtaining theoretical results on understanding how a theoretically tractable version of iterative RFs (iRF) can discover boolean interactions under the proposed Locally Spiky Sparse model that is inspired by thresholding behavior in many biological processes, including gene regulatory networks. iRF is an approach proposed by the PI that uses a tree ensemble from iteratively modified RFs to obtain predictive and stable non-linear or Boolean interactions of features. The general methodology of RFs was improved via hierarchical shrinkage (HS) and fast interpretable tree sums (FIGS). The former mitigates overfitting when building tree-based models through regularization by shrinking the prediction over each node towards the sample means of its ancestors. The latter mitigates the inefficiency of tree-based models for additive generative models by simultaneously growing a flexible number of trees in a summation.\n\n \nRegarding DL, the PI made contributions to improve both interpretability and predictability. For example, a method was developed that generates hierarchical interpretations for any neural network predictions. Contextual decomposition explanation penalization (CDEP) was also proposed which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models. Furthermore, the PI extended interpretable methods that attribute importances to features in a transformed space, which is called TRIM (Transformation Importance). Another approach, Adaptive Wavelet Distillation (AWD), was developed which aims to distill information from a trained neural network into a wavelet transform. Specifically, AWD penalizes feature attributions of a neural network in the wavelet domain to learn an effective multi-resolution wavelet transform. The resulting model is highly predictive, concise, computationally efficient, and has properties (e.g., a multi-scale structure) which make it easy to interpret. \n\n \nAdditionally, the Expectation-Maximization (EM) algorithm, one of the widely popular tools for inference with missing data, was studied under model misspecification, i.e., the behavior of EM for parameter estimation in mixture models when the number of mixtures is over-specified. The PI established that slow algorithmic and statistical rates of EM do arise for a class of over-specified mixture models both in small and high-dimensions.\n\n \nThroughout the duration of this project, the PI built and expanded principles of modern data science through the predictability, computability, and stability (PCS) framework for veridical (truthful) data science. For example, the PCS workflow and documentation was demonstrated in a genomics case study available on Zenodo. The PI is in the process of writing a textbook titled Veridical Data Science: The Practice of Responsible Data Analysis and Decision Making, which teaches Data Science through the lens of the PCS principles and workflow. Furthermore, a python package, VeridicalFlow, was developed to facilitate the practice of veridical data science. This framework also guided much of the methodology mentioned previously.\n\n \nOutcomes from this proposal included interdisciplinary projects solving domain problems. For instance, while deep neural networks have been shown to be effective in predicting the V4 neurons\u2019 responses, it is hard to use those models to interpret neuron functionality. Thus, a stability based method to seek optimal stimuli, DeepTune images, was proposed and confirmed previous findings on the presence of diverse shape and texture tuning in area V4. It provided concrete and naturalistic visualization of predicted optimal stimuli of individual neurons. In addition, AWD was utilized to address challenges in two real-world settings: cosmological parameter inference and molecular-partner prediction. In both cases, AWD yielded a scientifically interpretable and concise model which gave better predictive performance than state-of-the-art neural networks. Moreover, AWD identified predictive features that are scientifically meaningful in the domain context. \n\n \nThis project has provided opportunities for many students and postdocs coming from a variety of backgrounds. This included technical training (e.g., utilizing parallel computing to handle large volumes of data) as well as mentoring. Regarding educational impact, the PI was actively involved with the curriculum development of UC Berkeley\u2019s Data Science Education Program (DSEP), incorporated relevant research topics into her course content, and some of the high-dimensional analysis examples used in her book were motivated by achievements of this project. \n\n\t\t\t\t\tLast Modified: 12/01/2022\n\n\t\t\t\t\tSubmitted by: Bin Yu"
 }
}