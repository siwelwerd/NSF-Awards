{
 "awd_id": "1646162",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Synergy: Collaborative Research: Cyber-Physical Sensing, Modeling, and Control with Augmented Reality for Smart Manufacturing Workforce Training and Operations Management",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Bruce Kramer",
 "awd_eff_date": "2017-02-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 505287.0,
 "awd_amount": 607287.0,
 "awd_min_amd_letter_date": "2016-08-07",
 "awd_max_amd_letter_date": "2020-08-26",
 "awd_abstract_narration": "Smart manufacturing integrates information, technology, and human ingenuity to inspire the next revolution in the manufacturing industry. Manufacturing has been identified as a key strategic investment area by the U.S. government, private sector, and university leaders to spur innovation and keep America competitive. However, the lack of new methodologies and tools is challenging continuous innovation in the smart manufacturing industry. This award supports fundamental research to develop a cyber-physical sensing, modeling, and control infrastructure, coupled with augmented reality, to significantly improve the efficiency of future workforce training, performance of operations management, safety and comfort of workers for smart manufacturing. Results from this research are expected to transform the practice of worker-machine-task coordination and provide a powerful tool for operations management. This research involves several disciplines including sensing, data analytics, modeling, control, augmented reality, and workforce training and will provide unique interdisciplinary training opportunities for students and future manufacturing engineers. \r\n\r\nAn effective way for manufacturers to tackle and outpace the increasing complexity of product designs and ever-shortening product lifecycles is to effectively develop and assist the workforce. Yet the current management of manufacturing workforce systems relies mostly on the traditional methods of data collection and modeling, such as subjective observations and after-the-fact statistics of workforce performance, which has reached a bottleneck in effectiveness. The goal of this project is to investigate an integrated set of cyber-physical system methods and tools to sense, understand, characterize, model, and optimize the learning and operation of manufacturing workers, so as to achieve significantly improved efficiency in worker training, effectiveness of behavioral operations management, and safety of front-line workers. The research team will instrument a suite of sensors to gather real-time data about individual workers, worker-machine interactions, and the working environment,develop advanced methods and tools to track and understand workers' actions and physiological status, and detect their knowledge and skill deficiencies or assistance needs in real time. The project will also establish mathematical models that encode the manufacturing process in the research sensing and analysis framework, characterize the efficiency of worker-machine-task coordination, model the learning curves of individual workers, investigate various multi-modal augmented reality-based visualization, guidance, control, and intervention schemes to improve task efficiency and worker safety, and deploy, test, and conduct comprehensive performance assessments of the Researched technologies.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ming",
   "pi_last_name": "Leu",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Ming C Leu",
   "pi_email_addr": "mleu@mst.edu",
   "nsf_id": "000205055",
   "pi_start_date": "2020-08-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Ruwen",
   "pi_last_name": "Qin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ruwen Qin",
   "pi_email_addr": "ruwen.qin@stonybrook.edu",
   "nsf_id": "000513600",
   "pi_start_date": "2019-06-24",
   "pi_end_date": "2020-08-14"
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Zhaozheng",
   "pi_last_name": "Yin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhaozheng Yin",
   "pi_email_addr": "zhaozheng.yin@stonybrook.edu",
   "nsf_id": "000605796",
   "pi_start_date": "2016-08-07",
   "pi_end_date": "2019-06-24"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Ming",
   "pi_last_name": "Leu",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Ming C Leu",
   "pi_email_addr": "mleu@mst.edu",
   "nsf_id": "000205055",
   "pi_start_date": "2016-08-07",
   "pi_end_date": "2020-08-14"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Ruwen",
   "pi_last_name": "Qin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ruwen Qin",
   "pi_email_addr": "ruwen.qin@stonybrook.edu",
   "nsf_id": "000513600",
   "pi_start_date": "2016-08-07",
   "pi_end_date": "2019-06-24"
  }
 ],
 "inst": {
  "inst_name": "Missouri University of Science and Technology",
  "inst_street_address": "300 W. 12TH STREET",
  "inst_street_address_2": "202 CENTENNIAL HALL",
  "inst_city_name": "ROLLA",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "5733414134",
  "inst_zip_code": "654091330",
  "inst_country_name": "United States",
  "cong_dist_code": "08",
  "st_cong_dist_code": "MO08",
  "org_lgl_bus_name": "UNIVERSITY OF MISSOURI SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "Y6MGH342N169"
 },
 "perf_inst": {
  "perf_inst_name": "Missouri University of Science and Technology",
  "perf_str_addr": "300 W 12th Street",
  "perf_city_name": "Rolla",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "654096506",
  "perf_ctry_code": "US",
  "perf_cong_dist": "08",
  "perf_st_cong_dist": "MO08",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "018Y00",
   "pgm_ele_name": "CM - Cybermanufacturing System"
  },
  {
   "pgm_ele_code": "088Y00",
   "pgm_ele_name": "AM-Advanced Manufacturing"
  },
  {
   "pgm_ele_code": "164200",
   "pgm_ele_name": "Special Initiatives"
  },
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "016Z",
   "pgm_ref_txt": "Cybermanufacturing Systems"
  },
  {
   "pgm_ref_code": "091Z",
   "pgm_ref_txt": "Data Initiatives"
  },
  {
   "pgm_ref_code": "116E",
   "pgm_ref_txt": "RESEARCH EXP FOR UNDERGRADS"
  },
  {
   "pgm_ref_code": "152E",
   "pgm_ref_txt": "Cyber-Physical Systems"
  },
  {
   "pgm_ref_code": "1654",
   "pgm_ref_txt": "HUMAN COMPUTER INTERFACE"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9231",
   "pgm_ref_txt": "SUPPL FOR UNDERGRAD RES ASSIST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "MANU",
   "pgm_ref_txt": "MANUFACTURING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 505287.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 70000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project is aimed at introducing, creating and developing an integrated set of cyber-physical methods and tools to sense, understand, characterize, model, and optimize the learning and operations of assembly workers, so as to achieve smart manufacturing with significantly improved efficiency of work training, effectiveness of operations management, and safety of front-line workers.</p>\n<p>The project's outcomes in terms of intellectual merit include the following:</p>\n<ul>\n<li> We created a foundation for building multimodal sensor-based action recognition systems by fusing and refining convolutional neural network models. Based on this foundation, we developed a prototype multimodal sensor-based action recognition system and demonstrated using this system for worker activity recognition in human-centered mechanical assembly using data from an inertial measurement unit and a video camera.</li>\n<li>We developed a smart instructional system incorporating augmented reality, with the support of a deep learning network for detection of tools, parts and worker activities in manual assembly. We have demonstrated and evaluated this smart instructional system in assembling a CNC carving machine performed by a human worker.</li>\n<li>We developed a fog computing approach to bring computing power closer to the data source than cloud computing in order to achieve real-time worker assembly action recognition, and based on this approach we demonstrated a transfer learning model?s ability to achieve high recognition accuracy.</li>\n<li>We created a novel video-based human action recognition network which integrates discriminative feature pooling with a video segment attention model. This action recognition network has been shown to outperform the state-of-the-art action recognition networks when evaluated on four widely benchmarked datasets.</li>\n<li>We created a method to develop an individualized system of convolutional neural networks for assembly action recognition using human skeletal data. The system comprises CNN classifiers adapted to any new worker through transfer learning and iterative boosting, followed by an individualized fusion method that integrates the adapted classifiers into a real-time action recognition system. This individualized system of skeletal data-based CNN classifiers for action recognition improves the accuracy of action recognition compared to the CNN classifiers directly built with the skeletal data.</li>\n<li>We introduced a context and structure mining network for video object detection. This network includes an encoding module to encode the spatial-temporal context information in video frames into object features, and an aggregation module to better aggregate structure-based features with temporal information in support frames.</li>\n<li>We introduced a class-aware feature aggregation network for video object detection by putting the video object detection to the edge, and showed this network achieving state-of-the-art performance on the commonly used ImageNet VID dataset without use of any post-processing methods.</li>\n<li>We introduced a convolutional neural network that embeds a novel discriminative feature pooling mechanism and a novel video segment attention model, for video-based human action recognition from both trimmed and untrimmed videos. Based on this method, we developed an action recognition network and demonstrated that this network can be trained using both trimmed videos in a fully supervised way and untrimmed videos in a weakly supervised way.</li>\n<li>We introduced a novel method of weakly-supervised Action Completeness Modeling with Background Aware Networks (ACM-BANets) to address two main challenges in smart manufacturing: (1) how to design and train a weakly-supervised network that can suppress both highly discriminative and ambiguous frames in order to remove the false positives? and (2) how to design a temporal action localization framework in order to discover action instances in both highly discriminative and ambiguous action frames for the complete localization?</li>\n<li>We explored the unique characteristics of human trajectories and introduced a new approach, called reciprocal network learning, for human trajectory prediction. Extensive experimental results obtained using this approach on public benchmark datasets showed that this method outperforms the state-of-the-art human trajectory prediction methods.</li>\n</ul>\n<p>The project's outcomes in terms of broader impacts include the following:</p>\n<ul>\n<li> This project has contributed to the smart manufacturing literature through the publication of 11 journal papers, 11 peer-reviewed conference papers, and 1 book chapter. The research results including the developed frameworks, methods, algorithms, and tools for multimodal sensing, data analytics, deep learning, predictive modeling, and augmented reality have contributed significantly to the field of smart manufacturing.</li>\n<li>Three senior investigators, including one female, were involved in this collaborative research project, which provided research training opportunities for 10 Ph.D. students, 1 M.S. student, and 14 undergraduate students over the project duration. The involved faculty and students were from multiple disciplines, and all the project personnel gained valuable experiences in teamwork and convergent research.</li>\n<li>The project has improved the research infrastructure of the participating universities, with laboratories built that enable further research on manufacturing cyber-physical systems involving multimodal sensor fusion, deep learning algorithms for data analytics, and development of augmented reality assistive systems for human-centered intelligent manufacturing.&nbsp;&nbsp;</li>\n</ul>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/03/2022<br>\n\t\t\t\t\tModified by: Ming&nbsp;C&nbsp;Leu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project is aimed at introducing, creating and developing an integrated set of cyber-physical methods and tools to sense, understand, characterize, model, and optimize the learning and operations of assembly workers, so as to achieve smart manufacturing with significantly improved efficiency of work training, effectiveness of operations management, and safety of front-line workers.\n\nThe project's outcomes in terms of intellectual merit include the following:\n\n We created a foundation for building multimodal sensor-based action recognition systems by fusing and refining convolutional neural network models. Based on this foundation, we developed a prototype multimodal sensor-based action recognition system and demonstrated using this system for worker activity recognition in human-centered mechanical assembly using data from an inertial measurement unit and a video camera.\nWe developed a smart instructional system incorporating augmented reality, with the support of a deep learning network for detection of tools, parts and worker activities in manual assembly. We have demonstrated and evaluated this smart instructional system in assembling a CNC carving machine performed by a human worker.\nWe developed a fog computing approach to bring computing power closer to the data source than cloud computing in order to achieve real-time worker assembly action recognition, and based on this approach we demonstrated a transfer learning model?s ability to achieve high recognition accuracy.\nWe created a novel video-based human action recognition network which integrates discriminative feature pooling with a video segment attention model. This action recognition network has been shown to outperform the state-of-the-art action recognition networks when evaluated on four widely benchmarked datasets.\nWe created a method to develop an individualized system of convolutional neural networks for assembly action recognition using human skeletal data. The system comprises CNN classifiers adapted to any new worker through transfer learning and iterative boosting, followed by an individualized fusion method that integrates the adapted classifiers into a real-time action recognition system. This individualized system of skeletal data-based CNN classifiers for action recognition improves the accuracy of action recognition compared to the CNN classifiers directly built with the skeletal data.\nWe introduced a context and structure mining network for video object detection. This network includes an encoding module to encode the spatial-temporal context information in video frames into object features, and an aggregation module to better aggregate structure-based features with temporal information in support frames.\nWe introduced a class-aware feature aggregation network for video object detection by putting the video object detection to the edge, and showed this network achieving state-of-the-art performance on the commonly used ImageNet VID dataset without use of any post-processing methods.\nWe introduced a convolutional neural network that embeds a novel discriminative feature pooling mechanism and a novel video segment attention model, for video-based human action recognition from both trimmed and untrimmed videos. Based on this method, we developed an action recognition network and demonstrated that this network can be trained using both trimmed videos in a fully supervised way and untrimmed videos in a weakly supervised way.\nWe introduced a novel method of weakly-supervised Action Completeness Modeling with Background Aware Networks (ACM-BANets) to address two main challenges in smart manufacturing: (1) how to design and train a weakly-supervised network that can suppress both highly discriminative and ambiguous frames in order to remove the false positives? and (2) how to design a temporal action localization framework in order to discover action instances in both highly discriminative and ambiguous action frames for the complete localization?\nWe explored the unique characteristics of human trajectories and introduced a new approach, called reciprocal network learning, for human trajectory prediction. Extensive experimental results obtained using this approach on public benchmark datasets showed that this method outperforms the state-of-the-art human trajectory prediction methods.\n\n\nThe project's outcomes in terms of broader impacts include the following:\n\n This project has contributed to the smart manufacturing literature through the publication of 11 journal papers, 11 peer-reviewed conference papers, and 1 book chapter. The research results including the developed frameworks, methods, algorithms, and tools for multimodal sensing, data analytics, deep learning, predictive modeling, and augmented reality have contributed significantly to the field of smart manufacturing.\nThree senior investigators, including one female, were involved in this collaborative research project, which provided research training opportunities for 10 Ph.D. students, 1 M.S. student, and 14 undergraduate students over the project duration. The involved faculty and students were from multiple disciplines, and all the project personnel gained valuable experiences in teamwork and convergent research.\nThe project has improved the research infrastructure of the participating universities, with laboratories built that enable further research on manufacturing cyber-physical systems involving multimodal sensor fusion, deep learning algorithms for data analytics, and development of augmented reality assistive systems for human-centered intelligent manufacturing.  \n\n\n \n\n\t\t\t\t\tLast Modified: 01/03/2022\n\n\t\t\t\t\tSubmitted by: Ming C Leu"
 }
}