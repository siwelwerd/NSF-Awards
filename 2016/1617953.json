{
 "awd_id": "1617953",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Supervised Descent Method and its Applications to Computer Vision (and Beyond)",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 449715.0,
 "awd_amount": 449715.0,
 "awd_min_amd_letter_date": "2016-06-07",
 "awd_max_amd_letter_date": "2021-05-24",
 "awd_abstract_narration": "This project develops a fast optimization strategy for continuous and possibly combinatorial optimization problems. Optimization is a fundamental problem in many scientific disciplines from biology, physics and statistics to computer graphics and computer vision. This project focuses on solving 2 dimensional (2D) and 3 dimensional (3D) image alignment problems in computer vision. Solving the correspondence between 2D and 3D images is an open research problem and it is a fundamental component in most computer vision systems for medical imaging, surveillance, advanced driver assistance systems, mobile robots, augmented reality, object recognition and aerial video exploration, among other applications. The project integrates the research with education, and involves undergraduate/graduate students in the research.\r\n \r\nThis research addresses two main issues with second order descent methods in optimization: (1) the objective function to optimize might not be analytically differentiable and numerical approximations are impractical, and (2) the Hessian might be large and not positive definite. The research team advocates the concept of learning generic descent maps (i.e., average \"descent directions\") in a supervised manner. Using generic descent maps, the research team derives a practical algorithm, Supervised Descent Method (SDM), for minimizing Nonlinear Least Squares (NLS) problems with continuous parameters. During training, SDM learns a sequence of decent maps that minimize the NLS objective. During testing, these learned descent maps are used to minimize the NLS objective without requiring computation of the expensive Jacobian or Hessian. Beyond NLS, the research team explores the use of SDM to optimize combinatorial optimization problems such as finding 2D and 3D correspondences in images and point-clouds.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Fernando",
   "pi_last_name": "De la Torre",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fernando De la Torre",
   "pi_email_addr": "ftorre@cs.cmu.edu",
   "nsf_id": "000235613",
   "pi_start_date": "2016-06-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie Mellon Universityc",
  "perf_str_addr": "5000 Forbes av.",
  "perf_city_name": "United States",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 449715.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"flex flex-grow flex-col max-w-full gap-3 gizmo:gap-0\">\n<div class=\"min-h-[20px] text-message flex flex-col items-start gap-3 whitespace-pre-wrap break-words [.text-message+&amp;]:mt-5 overflow-x-auto\">\n<div class=\"markdown prose w-full break-words dark:prose-invert light\">\n<p>Many computer vision problems are formulated as the optimization of a cost function.&nbsp;This approach faces two main challenges: (i) designing a cost function  with a local optimum at an acceptable solution, and (ii) developing an  efficient numerical method to search for one (or multiple) of these  local optima. While designing such functions is feasible in the  noiseless case, the stability and location of local optima are mostly  unknown under noise, occlusion, or missing data. In practice, this can  result in undesirable local optima or not having a local optimum in the  expected place.</p>\n<p>This research introduces Discriminative Optimization (DO), a methodology aimed at addressing parameter estimation challenges in computer vision through the learning descent directions from training examples. DO offers notable advantages compared to conventional first order and second order methods, including robustness against noise and perturbations, as well as enhanced efficiency. We have presented theoretical findings regarding convergence. Specifically, we have provided:</p>\n<ol>\n<li>An analysis of the conditions for DO's convergence.</li>\n<li>Exploration of DO extensions to locate favorable local minima in non-convex problems.</li>\n<li>Evaluation of DO's ability to escape local minima in comparison to traditional methods using synthetic problems.</li>\n</ol>\n<p>Furthermore, we have developed a practical algorithm that has demonstrated state-of-the-art performance across various computer vision tasks, such as facial feature tracking, 3D cloud alignment, 2D shape alignment, and image denoising. Notably, this technique excels in efficiency, primarily relying on straightforward matrix multiplications.</p>\n<p>Our work has resulted in multiple publications in major computer vision conferences and journals. The associated code is publicly available on the following link: <a href=\"https://github.com/jayakornv/discriminative-optimization\" target=\"_new\">GitHub - jayakornv/discriminative-optimization</a>.</p>\n</div>\n</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/28/2023<br>\nModified by: Fernando&nbsp;De La Torre</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nMany computer vision problems are formulated as the optimization of a cost function.This approach faces two main challenges: (i) designing a cost function  with a local optimum at an acceptable solution, and (ii) developing an  efficient numerical method to search for one (or multiple) of these  local optima. While designing such functions is feasible in the  noiseless case, the stability and location of local optima are mostly  unknown under noise, occlusion, or missing data. In practice, this can  result in undesirable local optima or not having a local optimum in the  expected place.\n\n\nThis research introduces Discriminative Optimization (DO), a methodology aimed at addressing parameter estimation challenges in computer vision through the learning descent directions from training examples. DO offers notable advantages compared to conventional first order and second order methods, including robustness against noise and perturbations, as well as enhanced efficiency. We have presented theoretical findings regarding convergence. Specifically, we have provided:\n\nAn analysis of the conditions for DO's convergence.\nExploration of DO extensions to locate favorable local minima in non-convex problems.\nEvaluation of DO's ability to escape local minima in comparison to traditional methods using synthetic problems.\n\n\n\nFurthermore, we have developed a practical algorithm that has demonstrated state-of-the-art performance across various computer vision tasks, such as facial feature tracking, 3D cloud alignment, 2D shape alignment, and image denoising. Notably, this technique excels in efficiency, primarily relying on straightforward matrix multiplications.\n\n\nOur work has resulted in multiple publications in major computer vision conferences and journals. The associated code is publicly available on the following link: GitHub - jayakornv/discriminative-optimization.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 11/28/2023\n\n\t\t\t\t\tSubmitted by: FernandoDe La Torre\n"
 }
}