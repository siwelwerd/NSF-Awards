{
 "awd_id": "1563744",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Medium: Collaborative Research: Toward Extreme Scale Fault-Tolerance: Exploration Methods, Comparative Studies and Decision Processes",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2016-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 530000.0,
 "awd_amount": 530000.0,
 "awd_min_amd_letter_date": "2016-07-21",
 "awd_max_amd_letter_date": "2016-07-21",
 "awd_abstract_narration": "Current high-performance computing (HPC) research target computer systems with exaflop (1018 or a quintillion floating point operations per second) capabilities. Such computational power will enable new, important discoveries across all basic science domains. Application resilience to computer faults and failures is a major challenge to the realization of extreme scale computing systems. This project, Simulation and Modeling for Understanding Resilience and Faults at Scale (SMURFS), addresses this challenge by developing methods to improve our predictive understanding of the complex interactions amongst a given application, a given real or hypothetical hardware and software system environment, and a given fault-tolerance strategy at extreme scale. Specifically, SMURFS develops:\r\n1.\tNew simulation and modeling capabilities for studying application resilience at scale;\r\n2.\tCapabilities to execute a comprehensive set of comparative fault-tolerance studies; and\r\n3.\tEffective prescriptions to guide application developers, hardware architects and system designers to realize efficient, resilient extreme scale capabilities.\r\n\r\nSMURFS explores the impact of faults and failures, fault mitigation strategies and emerging technologies by providing new analytical and component models for predicting fault-tolerant application behavior at scale. The Iron simulation framework integrates these models for validation and comprehensive performance studies over a wide range of representative applications, application proxies, fault-tolerance protocols and hardware configurations. These studies inform a rule-based system for prescribing best fault-tolerance practices and configurations for new candidate applications and scenarios.\r\n\r\nSMURFS renders (1) new simulation and analytical models that predict application performance at scale; (2) detailed understandings of how application features interplay with different fault-tolerance strategies and hardware technologies; (3) new knowledge about application behavior at scale; and (4) valuable insight and prescriptions for designing, developing and deploying future extreme scale HPC systems.\r\n\r\nMore broadly, artifacts like the Iron framework and the public suite of application traces will be valuable to the HPC research, engineering, development, procurement and administrative communities. Researchers can use these artifacts for their own research that can impact the HPC exploration and design space.  For example, this framework can be instrumental in the co-design of cohesive extreme scale applications, software environments and hardware platforms. Additionally, Iron-based research can inform and improve scientific computing practices, accelerating the rate of scientific discovery.  Finally, Iron will be useful as an instructional device to teach about HPC issues both in classroom and tutorial contexts and other programs that engage diverse populations of middle, high school and college students in New Mexico and Tennessee.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Herault",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas Herault",
   "pi_email_addr": "herault@icl.utk.edu",
   "nsf_id": "000595689",
   "pi_start_date": "2016-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Knoxville",
  "inst_street_address": "201 ANDY HOLT TOWER",
  "inst_street_address_2": "",
  "inst_city_name": "KNOXVILLE",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "8659743466",
  "inst_zip_code": "379960001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "TN02",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "LXG4F9K8YZK5",
  "org_uei_num": "FN2YCS2YAUW3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Knoxville",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "379960003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "TN02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 530000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Models and simulators, developed in the context of SMURFS, demonstrated that inter-application I/O contentions caused by frequent checkpointing can reduce the I/O availability to only a small portion of the bandwidth, while cooperative checkpointing has the potential to greatly reduce I/O contentions and still provide high platform efficiency.</p>\n<p><br />We extended existing models to integrate critical realistic parameters often ignored in existing studies. From these extended models, we derived optimal numbers of spare nodes &mdash; that should be included in a given rigid application allocation &mdash; to maximize the application yield. Instantiated on reasonable platforms, our models predict that only a few percent of spare nodes (relative to the total number of nodes on a given HPC machine) are sufficient to maintain a 90% efficiency of the application, when the time between two allocations is on the order of 10 hours (which is a reasonable value for a busy HPC platform). Under the same conditions, the traditional approach that would not allocate spare nodes preemptively would see its efficiency drop below 50%.</p>\n<p><br />We considered multiple realistic scenarios on the usage of spare nodes to maintain high performance after a failure strikes a parallel application in a modern high-performance computer. We have shown the important role network topology plays in deciding the strategy of substitution that should be followed when replacing a failed process with a spare process running on a different node.&nbsp;</p>\n<p><br />We extended traditional performance models of resilience to take into account the wait time of shared HPC platforms. We showed that the impact on performance is dependent upon the application type, and considered 4 classes of applications: three where the resilience is generic (checkpoint/restart) and one where it is application-specific (ABFT). We showed that including a small number of spare nodes (up to 1% of the initial allocation size) enabled the different techniques to keep close to failure-free performance, even with long wait times, while the no-spare approach performance would quickly degrade with the wait time.<br />In the context of replicated applications, we proposed a new approach to implement checkpoint/restart, providing a much longer optimal checkpoint interval, which reduces the amount of I/O operations (and thus the stress on the I/O subsystem).</p>\n<p><br />We have also introduced a new rollback/recovery strategy &mdash; the restart strategy &mdash; which is combined with replication, and consists of restarting all failed processes at the beginning of each period. Thanks to this rejuvenation, the system remains in the same conditions at the beginning of each checkpointing period, which allowed us to build an accurate performance model and to derive the optimal checkpointing period for this strategy. This period turns out to be much longer than the one used with the no-restart strategy, hence reducing significantly the I/O pressure introduced by checkpoints and improving the overall time-to-solution.</p>\n<p><br />Last, we have reviewed and compared algorithm-specific fault tolerant methods, namely Algorithm Based Fault Tolerance (ABFT) and Residual Checking (RC) for detecting and correcting floating-point errors in matrix multiplication. On the theoretical side, we have detailed both methods, their variants, their common characteristics, and their differences. On the practical side, we have implemented two variants for error correction in each method&mdash;one based on solving a small linear system, and one based on recomputing only corrupted elements&mdash;using coordinate checksumming to locate them. An extensive experimental comparison reveals similar execution times for the core of each method, but ABFT requires embedding the checksum in the user data in order to benefit from the high-performance kernel implementation, while RC does not. Also, the flexibility of RC becomes very important when error rates are high, because RC can adapt a posteriori to the number of errors encountered within each particular execution.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/22/2021<br>\n\t\t\t\t\tModified by: Thomas&nbsp;Herault</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModels and simulators, developed in the context of SMURFS, demonstrated that inter-application I/O contentions caused by frequent checkpointing can reduce the I/O availability to only a small portion of the bandwidth, while cooperative checkpointing has the potential to greatly reduce I/O contentions and still provide high platform efficiency.\n\n\nWe extended existing models to integrate critical realistic parameters often ignored in existing studies. From these extended models, we derived optimal numbers of spare nodes &mdash; that should be included in a given rigid application allocation &mdash; to maximize the application yield. Instantiated on reasonable platforms, our models predict that only a few percent of spare nodes (relative to the total number of nodes on a given HPC machine) are sufficient to maintain a 90% efficiency of the application, when the time between two allocations is on the order of 10 hours (which is a reasonable value for a busy HPC platform). Under the same conditions, the traditional approach that would not allocate spare nodes preemptively would see its efficiency drop below 50%.\n\n\nWe considered multiple realistic scenarios on the usage of spare nodes to maintain high performance after a failure strikes a parallel application in a modern high-performance computer. We have shown the important role network topology plays in deciding the strategy of substitution that should be followed when replacing a failed process with a spare process running on a different node. \n\n\nWe extended traditional performance models of resilience to take into account the wait time of shared HPC platforms. We showed that the impact on performance is dependent upon the application type, and considered 4 classes of applications: three where the resilience is generic (checkpoint/restart) and one where it is application-specific (ABFT). We showed that including a small number of spare nodes (up to 1% of the initial allocation size) enabled the different techniques to keep close to failure-free performance, even with long wait times, while the no-spare approach performance would quickly degrade with the wait time.\nIn the context of replicated applications, we proposed a new approach to implement checkpoint/restart, providing a much longer optimal checkpoint interval, which reduces the amount of I/O operations (and thus the stress on the I/O subsystem).\n\n\nWe have also introduced a new rollback/recovery strategy &mdash; the restart strategy &mdash; which is combined with replication, and consists of restarting all failed processes at the beginning of each period. Thanks to this rejuvenation, the system remains in the same conditions at the beginning of each checkpointing period, which allowed us to build an accurate performance model and to derive the optimal checkpointing period for this strategy. This period turns out to be much longer than the one used with the no-restart strategy, hence reducing significantly the I/O pressure introduced by checkpoints and improving the overall time-to-solution.\n\n\nLast, we have reviewed and compared algorithm-specific fault tolerant methods, namely Algorithm Based Fault Tolerance (ABFT) and Residual Checking (RC) for detecting and correcting floating-point errors in matrix multiplication. On the theoretical side, we have detailed both methods, their variants, their common characteristics, and their differences. On the practical side, we have implemented two variants for error correction in each method&mdash;one based on solving a small linear system, and one based on recomputing only corrupted elements&mdash;using coordinate checksumming to locate them. An extensive experimental comparison reveals similar execution times for the core of each method, but ABFT requires embedding the checksum in the user data in order to benefit from the high-performance kernel implementation, while RC does not. Also, the flexibility of RC becomes very important when error rates are high, because RC can adapt a posteriori to the number of errors encountered within each particular execution.\n\n \n\n\t\t\t\t\tLast Modified: 11/22/2021\n\n\t\t\t\t\tSubmitted by: Thomas Herault"
 }
}