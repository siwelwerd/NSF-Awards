{
 "awd_id": "1634050",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Computational neuroimaging of human auditory cortex",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Kurt Thoroughman",
 "awd_eff_date": "2016-07-15",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2016-07-19",
 "awd_max_amd_letter_date": "2016-07-19",
 "awd_abstract_narration": "Just by listening, humans can infer a vast array of things about the world around them: who is talking, whether a window in their house is open or shut, or what their child dropped on the floor in the next room. This ability to derive information from sound is a core component of human intelligence, and is enabled by many stages of neuronal processing extending from the ear into the brain. Although much is known about how the ears convert sound to electrical signals that are sent to the brain, the mechanisms by which the brain mediates our sound recognition abilities remains poorly understood. These gaps in knowledge limit our ability to develop machine systems that can replicate our listening skills (e.g. for use in robots) or to understand the basis of listening difficulties, as in disorders such as dyslexia or auditory processing disorder, or in age-related hearing loss. To gain insight into the neuronal processes that enable auditory recognition, the brain's processing of sound will be studied using fMRI, a technique to non-invasively measure brain activity. The responses measured in the brain will be compared to the numerical responses produced by state-of-the-art computer algorithms for sound recognition. The research will help reveal the principles of human auditory intelligence, with the long-term goals of enabling more effective machine algorithms and treatments for listening disorders. The research will also provide insight into the inner workings of computer audio algorithms, stimulating interaction between engineering, industry, and neuroscience. The project will facilitate other research efforts via the dissemination of new tools for manipulating sound and the creation of audio data sets, and will recruit and train women and underrepresented minorities in computational neuroscience.\r\n\r\nAspects of the structure and function of primary auditory cortex are well established, and there are a variety of proposals for pathways that might extend out of primary auditory cortex. However, we know little about the transformations within the auditory cortex that enable sound recognition, and there are few computational models of how such transformations might occur. The goal of the proposed research is to conduct fMRI experiments that reveal representational transformations within auditory cortex that might contribute to auditory recognition, to use fMRI responses to test existing models of auditory computation, and to develop new models that can account for human abilities and neuronal responses. Functional MRI will be used to characterize cortical responses because it allows measurements from the entire auditory cortex at once, making it possible to compare responses in different regions of the auditory cortex (including those far from the cortical surface), and thus to probe for representational transformations between regions. New models of auditory computation will be developed by leveraging the recent successes of \"deep learning\", and their relevance to the brain will be tested using new synthesis-based methods for model evaluation. The results will help reveal how the auditory cortex mediates robust sound recognition.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Joshua",
   "pi_last_name": "McDermott",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Joshua McDermott",
   "pi_email_addr": "jhm@MIT.EDU",
   "nsf_id": "000632976",
   "pi_start_date": "2016-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Just by listening, humans can infer a vast array of things about the world around them: who is talking, whether a window in their house is open or shut, or what their child dropped on the floor in the next room. This ability to derive information from sound is a core component of human intelligence, and is enabled by neuronal processing extending from the ear into the brain. Although much is known about how the ears convert sound to electrical signals that are sent to the brain, the mechanisms by which the brain mediates our sound recognition abilities have remained poorly understood. These gaps in knowledge have limited our ability to develop machine systems that can replicate our listening skills (e.g. for use in robots) or to understand the basis of listening difficulties, as in disorders such as dyslexia or auditory processing disorder, or in age-related hearing loss. To gain insight into the neuronal processes that enable auditory recognition, this project studied the brain&rsquo;s processing of sound using fMRI, a technique to non-invasively measure brain activity. The responses measured in the brain were compared to the numerical responses produced by state-of-the-art computer algorithms for sound recognition. The principle findings were that brain responses in the auditory cortex of humans could be predicted relatively well by the numerical responses in computer algorithms, but that different parts of the human cortex were best predicted by different algorithmic stages. This result suggests that processing in the human auditory cortex is divided into at least two stages. The later processing stage appears to enable sound recognition in the presence of noise. The research has advanced the long-term goals of enabling more effective machine algorithms and treatments for listening disorders, by showing that current algorithms replicate aspects of sound processing in the human brain. The research also clarified the inner workings of computer audio algorithms, and in doing so stimulated interaction between engineering and neuroscience. The project has resulted in the dissemination of new tools for manipulating sound that will hopefully aid future research in the field, and helped to recruit and train women and underrepresented minorities in computational neuroscience.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/11/2019<br>\n\t\t\t\t\tModified by: Joshua&nbsp;Mcdermott</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nJust by listening, humans can infer a vast array of things about the world around them: who is talking, whether a window in their house is open or shut, or what their child dropped on the floor in the next room. This ability to derive information from sound is a core component of human intelligence, and is enabled by neuronal processing extending from the ear into the brain. Although much is known about how the ears convert sound to electrical signals that are sent to the brain, the mechanisms by which the brain mediates our sound recognition abilities have remained poorly understood. These gaps in knowledge have limited our ability to develop machine systems that can replicate our listening skills (e.g. for use in robots) or to understand the basis of listening difficulties, as in disorders such as dyslexia or auditory processing disorder, or in age-related hearing loss. To gain insight into the neuronal processes that enable auditory recognition, this project studied the brain?s processing of sound using fMRI, a technique to non-invasively measure brain activity. The responses measured in the brain were compared to the numerical responses produced by state-of-the-art computer algorithms for sound recognition. The principle findings were that brain responses in the auditory cortex of humans could be predicted relatively well by the numerical responses in computer algorithms, but that different parts of the human cortex were best predicted by different algorithmic stages. This result suggests that processing in the human auditory cortex is divided into at least two stages. The later processing stage appears to enable sound recognition in the presence of noise. The research has advanced the long-term goals of enabling more effective machine algorithms and treatments for listening disorders, by showing that current algorithms replicate aspects of sound processing in the human brain. The research also clarified the inner workings of computer audio algorithms, and in doing so stimulated interaction between engineering and neuroscience. The project has resulted in the dissemination of new tools for manipulating sound that will hopefully aid future research in the field, and helped to recruit and train women and underrepresented minorities in computational neuroscience.\n\n\t\t\t\t\tLast Modified: 07/11/2019\n\n\t\t\t\t\tSubmitted by: Joshua Mcdermott"
 }
}