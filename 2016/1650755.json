{
 "awd_id": "1650755",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Lifecycle Management of Collaborative Analysis Workflows through Provenance Capture and Analysis",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 256913.0,
 "awd_amount": 256913.0,
 "awd_min_amd_letter_date": "2016-08-31",
 "awd_max_amd_letter_date": "2016-08-31",
 "awd_abstract_narration": "Data-driven methods and products have shown tremendous promise and are becoming increasingly common in a variety of communities, including science, education, economics, government, and social and web analytics. This trend, popularly referred to as \"big data\" or \"data science\", has resulted in a pressing need for sustainable and scalable tools that facilitate the end-to-end collaborative data analysis process; this process is often ad hoc, typically featuring highly unstructured datasets, an amalgamation of different tools and techniques, significant back-and-forth among the members of a team, and trial-and-error to identify the right analysis tools, algorithms, models, and parameters.  Although there is much prior and ongoing work on developing tools to perform specific data analysis tasks, there is no easy way to capture and reason about ad hoc data science pipelines, many of which are often spread across a collection of analysis scripts. Metadata or provenance information about how datasets were generated, including the programs or scripts used for generating them and/or values of any crucial parameters, is often lost. Similarly, it is hard to keep track of any dependencies between the datasets, or information about how they evolved over time. \r\n\r\nThis project is building a unified provenance and metadata management system to support end-to-end lifecycle management of complex collaborative \"data science workflows\" that arise in big data applications. The system features a flexible and intuitive data model that can capture a variety of different types of data and metadata, including versioning and provenance information, derivation information, parameters used during experiments or modeling, statistics gathered to make decisions, analysis scripts, notes or tags, etc. It provides novel mechanisms for making it easy to capture such information with minimal burden on the users. The system also features a rich, high-level domain-specific query language that enables unified querying over such data, as well as a web browser-based visualization tool for formulating queries, and for exploring the search results. By continuously analyzing and exploiting such provenance information, the system also enables a host of new features including: searching for relevant data science workflows or analysis scripts for a given task, comparing end results of multiple pipelines to identify key similarities and differences, and quickly and automatically detecting problems or anomalies during model development and/or deployment. The system will transform the way in which data scientists manage provenance information and metadata while performing data analysis, and will allow them to more quickly derive actionable and useful insights or knowledge from the data. By lowering the barrier to sharing and reusing the work done by others, the system will lead to new insights that may not have been achievable beforehand. This project provides research opportunities for graduate and undergraduate students, and is aligned with several undergraduate and graduate courses offered by the PI.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Amol",
   "pi_last_name": "Deshpande",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Amol V Deshpande",
   "pi_email_addr": "amol@cs.umd.edu",
   "nsf_id": "000486255",
   "pi_start_date": "2016-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425141",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 256913.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This NSF-funded project was motivated by a lack of sustainable and scalable tools that facilitate end-to-end collaborative data analysis process, and enable introspective reasoning over the overall process to ensure \"robust\" data analysis. Over the last decade, data-driven methods and products have shown tremendous promise and are becoming increasingly common in a variety of communities, including science, education, economics, government, and social and web analytics; however, the process of analyzing datasets and inferring actionable insights or knowledge from them is still ad hoc, typically featuring highly unstructured datasets, an amalgamation of different tools and techniques, significant back-and-forth among the members of a data analysis team, and trial-and-error to identify the right analysis tools, algorithms, models, and paramters. Although there has been much prior and ongoing work on developing tools to perform specific data analysis tasks, there is no easy way to capture and reason about ad hoc data science pipelines, many of which are often spread across a collection of analysis scripts. Metadata or provenance information about how datasets were generated, including the programs or scripts used for generating them and/or values of any crucial parameters, is often lost. Similarly, it is hard to keep track of any dependencies between the datasets, or information about how they evolved over time. All of these make it a challenge to understand whether the end product is robust, trustworthy and fair; and make it hard to \"explain\" specific results or insights.<br /><br />The goal of this project was to investigate how to address this gap by building a unified provenance and metadata management system that fundamentally supports end-to-end lifecycle management of complex and collaborative data analysis workflows. <br />&nbsp;<br />The key outcomes of this project can be summarized as follows. (a) To address the challenges faced by data scientists as described above, we designed and built a unified, stand-alone provenance and metadata management system, called ProvDB, to support lifecycle management of complex collaborative data science workflows. ProvDB is designed to capture a large amount of fine-grained information about the analysis processes and versioned data artifacts in a semi-passive manner using a flexible and extensible ingestion mechanism. It provides novel querying and analysis capabilities for simplifying bookkeeping and debugging tasks for data analysts, and it enables a rich new set of capabilities like identifying flaws in the data science process. (b) After an investigation into the main challenges faced by several teams of researchers who were using \"deep learning\" to solve a variety of problems, we developed a system called ModelHub to help those researchers manage the end-to-end deep learning lifecycle, to accelerate modeling and learning tasks, and to manage the rich set of lifecycle artifacts that get generated in the process. As part of developing this system, we developed novel declarative abstraction layers to help the researchers focus on the important tasks, algorithms for comparing models and for storing a large number of versions of models compactly, and a progressive, approximate query evaluation scheme. (c) We also developed a variety of new techniques for managing a large number of versions of different types of data in a compact manner, while allowing users to query them efficiently. In particular, we built a system called RStore, first of its kind, that enables storing a large number of versions of semi-structured documents in a key-value store, and we designed new algorithms for executing a variety of queries over a delta-oriented versioned storage system, often used for unstructured datasets like models or scripts.<br /><br />The major results from this project were published in leading conferences and journals in the field of Databases, formed major parts of three PhD Dissertations at the University of Maryland, and were the subject of several invited talks. The project was also instrumental in training three graduate students to conduct research, all of who have graduated with PhDs over the last year and are currently employed in industry and academic research labs.<br /><br />More details about the project, and the publications that resulted from it, can be found at: http://www.cs.umd.edu/~amol/DBGroup/ProvDB.html.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/31/2019<br>\n\t\t\t\t\tModified by: Amol&nbsp;V&nbsp;Deshpande</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis NSF-funded project was motivated by a lack of sustainable and scalable tools that facilitate end-to-end collaborative data analysis process, and enable introspective reasoning over the overall process to ensure \"robust\" data analysis. Over the last decade, data-driven methods and products have shown tremendous promise and are becoming increasingly common in a variety of communities, including science, education, economics, government, and social and web analytics; however, the process of analyzing datasets and inferring actionable insights or knowledge from them is still ad hoc, typically featuring highly unstructured datasets, an amalgamation of different tools and techniques, significant back-and-forth among the members of a data analysis team, and trial-and-error to identify the right analysis tools, algorithms, models, and paramters. Although there has been much prior and ongoing work on developing tools to perform specific data analysis tasks, there is no easy way to capture and reason about ad hoc data science pipelines, many of which are often spread across a collection of analysis scripts. Metadata or provenance information about how datasets were generated, including the programs or scripts used for generating them and/or values of any crucial parameters, is often lost. Similarly, it is hard to keep track of any dependencies between the datasets, or information about how they evolved over time. All of these make it a challenge to understand whether the end product is robust, trustworthy and fair; and make it hard to \"explain\" specific results or insights.\n\nThe goal of this project was to investigate how to address this gap by building a unified provenance and metadata management system that fundamentally supports end-to-end lifecycle management of complex and collaborative data analysis workflows. \n \nThe key outcomes of this project can be summarized as follows. (a) To address the challenges faced by data scientists as described above, we designed and built a unified, stand-alone provenance and metadata management system, called ProvDB, to support lifecycle management of complex collaborative data science workflows. ProvDB is designed to capture a large amount of fine-grained information about the analysis processes and versioned data artifacts in a semi-passive manner using a flexible and extensible ingestion mechanism. It provides novel querying and analysis capabilities for simplifying bookkeeping and debugging tasks for data analysts, and it enables a rich new set of capabilities like identifying flaws in the data science process. (b) After an investigation into the main challenges faced by several teams of researchers who were using \"deep learning\" to solve a variety of problems, we developed a system called ModelHub to help those researchers manage the end-to-end deep learning lifecycle, to accelerate modeling and learning tasks, and to manage the rich set of lifecycle artifacts that get generated in the process. As part of developing this system, we developed novel declarative abstraction layers to help the researchers focus on the important tasks, algorithms for comparing models and for storing a large number of versions of models compactly, and a progressive, approximate query evaluation scheme. (c) We also developed a variety of new techniques for managing a large number of versions of different types of data in a compact manner, while allowing users to query them efficiently. In particular, we built a system called RStore, first of its kind, that enables storing a large number of versions of semi-structured documents in a key-value store, and we designed new algorithms for executing a variety of queries over a delta-oriented versioned storage system, often used for unstructured datasets like models or scripts.\n\nThe major results from this project were published in leading conferences and journals in the field of Databases, formed major parts of three PhD Dissertations at the University of Maryland, and were the subject of several invited talks. The project was also instrumental in training three graduate students to conduct research, all of who have graduated with PhDs over the last year and are currently employed in industry and academic research labs.\n\nMore details about the project, and the publications that resulted from it, can be found at: http://www.cs.umd.edu/~amol/DBGroup/ProvDB.html.\n\n\t\t\t\t\tLast Modified: 01/31/2019\n\n\t\t\t\t\tSubmitted by: Amol V Deshpande"
 }
}