{
 "awd_id": "1600474",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "I-CORPS: First Person Visual Analytics",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Steven Konsek",
 "awd_eff_date": "2016-02-15",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2016-02-05",
 "awd_max_amd_letter_date": "2016-02-05",
 "awd_abstract_narration": "There is an urgent need for direct, continuous, and objective measures of social behavior in individuals with Autism Spectrum Disorder (ASD) that are sensitive to treatment-related change. Tracking progress by measuring changes in behavior is crucial to determine whether the treatment is working or needs to be modified. Direct observational measures, such as live or video-based scoring, are time and resource intensive, and all but impossible to implement in clinics and homes, where treatments are increasingly delivered. There are currently no standardized, objective, and reliable measures of key social behaviors that can be used in clinical settings to assess treatment outcomes in individuals with ASD. The goal of this project is to develop visual analytics tools to enable the automatic detection and quantification of social behaviors of interest from video captured by a wearable camera, a task this I-Corps team refers to as First Person Vision (FPV). In previous work, this team developed a method for automatically detecting moments when a child makes eye contact with an adult social partner by analyzing video recorded by a pair of commercially available glasses worn by the adult which had a camera embedded in the bridge over the nose. By identifying and localizing the child's head and face in the video and analyzing the image regions containing the eyes in conjunction with the head pose, the proposed algorithm can determine whether the child's gaze is directed towards the adult's eyes. The proposed approach can generate predictions for eye contact at the frame level, and can also be used to detect eye contact events from which measures of duration can be derived. \r\n\r\nIf properly developed, the proposed measurement approach may enable clinical providers to scale their intervention efforts, with the potential of improving the quality of life for thousands of children with autism and their families. More broadly, analytics tools for video captured from body-worn cameras (first person video) will provide new opportunities to model and analyze human behavior, create personalized records of visual experiences, and improve the treatment of a broad range of mental and physical health conditions. This team will conduct additional customer discovery in the areas of human resources training and product marketing, with the potential of expanding the focus of our analytics from gaze behavior to other social behaviors relevant to these contexts, such as facial expressions and gestures. The team also expects to develop a prototype of the hardware (wearable camera system with wide field of view) and incorporate analytic capabilities developed by our group (algorithms for automated detection of eye contact, attention to objects, gaze shifts) into the system. the proposed prototype will also include options for visualizing and reporting the results of the automated analysis back to the user.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Rehg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "James Rehg",
   "pi_email_addr": "jrehg@illinois.edu",
   "nsf_id": "000257071",
   "pi_start_date": "2016-02-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802300",
   "pgm_ele_name": "I-Corps"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>PROJECT OUTCOMES</p>\n<p>&nbsp;</p>\n<p>Wearable cameras such as GoPro and Pivothead are enabling people to capture interesting events in their lives from their own point of view (Figure 1). The current generation of these wearable video recorders can continuously capture for about two hours, but new technologies are poised to extend battery life by a factor of ten. It will soon be possible for an individual to easily and unobtrusively capture their day-to-day life experiences on video. Our belief is that if the proper tools for analysis and consumption of such video content were available, a passive record of visual experience can be converted into useful quantitative data with many applications.</p>\n<p>&nbsp;</p>\n<p>Our team has been pursuing research that aims to create computer algorithms that analyze such video footage to automatically detect and quantify behaviors of interest. One application that we envision is the automatic measurement of social behaviors during face-to-face interactions to support the treatment of childhood developmental disorders such as autism. Motivated by the autism use case, we previously developed a method to automatically detecting moments when a child makes eye contact with an adult play partner who is wearing a pair of glasses with a camera embedded in the bridge over the nose (Figure 2). In this I-Corps grant, we sought to explore whether this tool may be of use to clinics who treat children with autism to help them select and refine treatment approaches and track the progress of their clients.</p>\n<p>&nbsp;</p>\n<p>We conducted over 100 customer discovery interviews to understand the areas of clinical care delivery for individuals with autism that may benefit from automated measurement of gaze behavior. We spoke both with front-line clinical staff as well as program and clinic directors and CEOs at 32 clinical centers and specialized schools around the US, focusing on those that deliver behavior-based interventions to children with autism. Our interviews revealed that front-line therapists, case managers, and clinical/program directors saw value in being able to collect reliable and less noisy data on client progress, to quantify difficult-to-measure behaviors, and to make more efficient use of staff (especially by reducing the need for staff to do data collection/entry). However, the main pain point for Directors/CEOs reflected the new requirements around tracking and reporting client progress for purposes of reimbursement, especially staff time spent on insurance documentation. We further mapped out the ecosystem within which individuals with autism are provided with clinical and educational services, including sources of funding for these services, as illustrated in the Figure 3. Finally, a quantitative analysis of the interviews indicated that the majority of clinical service providers (77%) rely on paper and pencil for data collection on client progress. Only 23% reported using one of the available electronic data collection apps and &lt;8% incorporate video recording and analysis into their clinical practice.</p>\n<p>&nbsp;</p>\n<p>Upon completion of the I-Corps customer discovery activities, our team decided not to proceed with launching a company to commercialize our technology. This decision was largely driven by evidence that the market for our technology was not yet fully-established: our target customer segment primarily relies on paper and pencil record keeping, with little adoption of electronic data collection and minimal use of video recording capabilities. Our best strategy is to partner with a number of the early evangelists that we identified - organizations that are already using electronic data collection tools and have technology-minded Directors and CEOs - to gather additional data on the value-added of our technology, especially with respect to payers (insurance companies).</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/12/2017<br>\n\t\t\t\t\tModified by: James&nbsp;Rehg</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1600474/1600474_10409822_1494606909763_FPV-example--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1600474/1600474_10409822_1494606909763_FPV-example--rgov-800width.jpg\" title=\"Autism Services Ecosystem\"><img src=\"/por/images/Reports/POR/2017/1600474/1600474_10409822_1494606909763_FPV-example--rgov-66x44.jpg\" alt=\"Autism Services Ecosystem\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of the ecosystem within which individuals with autism are provided clinical and educational services, including sources of funding for these services.</div>\n<div class=\"imageCredit\">Copyrighted</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">James&nbsp;Rehg</div>\n<div class=\"imageTitle\">Autism Services Ecosystem</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1600474/1600474_10409822_1494607038212_eye-contact--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1600474/1600474_10409822_1494607038212_eye-contact--rgov-800width.jpg\" title=\"Eye Contact Detection Example\"><img src=\"/por/images/Reports/POR/2017/1600474/1600474_10409822_1494607038212_eye-contact--rgov-66x44.jpg\" alt=\"Eye Contact Detection Example\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of automatic detection of eye contact during a face-to-face social interaction from a wearable camera system</div>\n<div class=\"imageCredit\">Copyrighted</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">James&nbsp;Rehg</div>\n<div class=\"imageTitle\">Eye Contact Detection Example</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1600474/1600474_10409822_1494607155693_autism_services_ecosystem--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1600474/1600474_10409822_1494607155693_autism_services_ecosystem--rgov-800width.jpg\" title=\"Point-of-View Video Example\"><img src=\"/por/images/Reports/POR/2017/1600474/1600474_10409822_1494607155693_autism_services_ecosystem--rgov-66x44.jpg\" alt=\"Point-of-View Video Example\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Example frame from a video collected with a Point-of-View wearable camera system.</div>\n<div class=\"imageCredit\">Copyrighted</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">James&nbsp;Rehg</div>\n<div class=\"imageTitle\">Point-of-View Video Example</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nPROJECT OUTCOMES\n\n \n\nWearable cameras such as GoPro and Pivothead are enabling people to capture interesting events in their lives from their own point of view (Figure 1). The current generation of these wearable video recorders can continuously capture for about two hours, but new technologies are poised to extend battery life by a factor of ten. It will soon be possible for an individual to easily and unobtrusively capture their day-to-day life experiences on video. Our belief is that if the proper tools for analysis and consumption of such video content were available, a passive record of visual experience can be converted into useful quantitative data with many applications.\n\n \n\nOur team has been pursuing research that aims to create computer algorithms that analyze such video footage to automatically detect and quantify behaviors of interest. One application that we envision is the automatic measurement of social behaviors during face-to-face interactions to support the treatment of childhood developmental disorders such as autism. Motivated by the autism use case, we previously developed a method to automatically detecting moments when a child makes eye contact with an adult play partner who is wearing a pair of glasses with a camera embedded in the bridge over the nose (Figure 2). In this I-Corps grant, we sought to explore whether this tool may be of use to clinics who treat children with autism to help them select and refine treatment approaches and track the progress of their clients.\n\n \n\nWe conducted over 100 customer discovery interviews to understand the areas of clinical care delivery for individuals with autism that may benefit from automated measurement of gaze behavior. We spoke both with front-line clinical staff as well as program and clinic directors and CEOs at 32 clinical centers and specialized schools around the US, focusing on those that deliver behavior-based interventions to children with autism. Our interviews revealed that front-line therapists, case managers, and clinical/program directors saw value in being able to collect reliable and less noisy data on client progress, to quantify difficult-to-measure behaviors, and to make more efficient use of staff (especially by reducing the need for staff to do data collection/entry). However, the main pain point for Directors/CEOs reflected the new requirements around tracking and reporting client progress for purposes of reimbursement, especially staff time spent on insurance documentation. We further mapped out the ecosystem within which individuals with autism are provided with clinical and educational services, including sources of funding for these services, as illustrated in the Figure 3. Finally, a quantitative analysis of the interviews indicated that the majority of clinical service providers (77%) rely on paper and pencil for data collection on client progress. Only 23% reported using one of the available electronic data collection apps and &lt;8% incorporate video recording and analysis into their clinical practice.\n\n \n\nUpon completion of the I-Corps customer discovery activities, our team decided not to proceed with launching a company to commercialize our technology. This decision was largely driven by evidence that the market for our technology was not yet fully-established: our target customer segment primarily relies on paper and pencil record keeping, with little adoption of electronic data collection and minimal use of video recording capabilities. Our best strategy is to partner with a number of the early evangelists that we identified - organizations that are already using electronic data collection tools and have technology-minded Directors and CEOs - to gather additional data on the value-added of our technology, especially with respect to payers (insurance companies).\n\n \n\n\t\t\t\t\tLast Modified: 05/12/2017\n\n\t\t\t\t\tSubmitted by: James Rehg"
 }
}