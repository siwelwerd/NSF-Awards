{
 "awd_id": "1554810",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Optimizing Non-Native Speech Sound Learning",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2016-06-01",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 433995.0,
 "awd_amount": 433995.0,
 "awd_min_amd_letter_date": "2016-04-14",
 "awd_max_amd_letter_date": "2017-09-10",
 "awd_abstract_narration": "Learning to speak a new language in adulthood has become a common occurrence in our increasingly interconnected world. It is often very hard for learners to perceive and produce the sounds in the new language when they are different from the sounds in the native language. Most adult learners will never speak like a native, instead retaining a non-native accent. New evidence suggests that sleep may have a special role to play in solidifying learning of non-native sounds. Moreover, the timing of learning with respect to sleep may be a crucial factor. This project assesses the relationship between sleep and non-native speech sound learning 1) through analysis of a large database from an industry partner specializing in language learning software and 2) by imaging and analyzing the neural systems that underlie successful learning.  A greater understanding of the optimal conditions for learning that integrates knowledge from perception, sleep, and neural processing will suggest strategies for language instruction that best help learners acquire the sounds of a new language. The research program showcases an academic/industry partnership and sharing of online tools that will be developed for perceptual testing.  In addition, outreach to middle-school students will help introduce these questions and experimental methods into STEM education.\r\n\r\nThe overarching goal of this project is to build a model of non-native speech sound learning, the Sleep Consolidation Model for Speech (SCMS), that integrates research from literatures on sleep, perception, and neurobiology to discover facilitating and constraining conditions on speech sound acquisition. Three linked projects test the idea that consolidation during sleep serves to protect learned speech sound information from interference and also allows learners to generalize learning to new speech sound categories. Project 1 tests predictions of the SCMS using web-based training and sleep monitoring in a standard laboratory (college student) sample. Project 2 involves the analysis of a large database from users of the Rosetta Stone language learning software, enabling the research team to extend these predictions to a more diverse and ecologically valid subject sample and new target languages (e.g. Irish, Arabic). Project 3 evaluates the neural predictions of the SCMS using magnetic resonance imaging (MRI) to assess the relationship between brain structure and learning, and transcranial magnetic stimulation (TMS) to search for the neural locus of interference effects.  The results should help maximize conditions for learning, for example, by scheduling training in relation to sleep or by minimizing interfering stimuli and tasks between training and sleep. Results may also inform neural models of speech and learning, by elucidating the role of white matter connectivity in non-native speech learning. Although questions are couched in terms of a specific perceptual problem, learning a new language, the research also has implications for perceptual learning in other domains (e.g. novel visual category acquisition).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Emily",
   "pi_last_name": "Myers",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Emily Myers",
   "pi_email_addr": "emily.myers@uconn.edu",
   "nsf_id": "000688586",
   "pi_start_date": "2016-04-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Connecticut",
  "inst_street_address": "438 WHITNEY RD EXTENSION UNIT 1133",
  "inst_street_address_2": "",
  "inst_city_name": "STORRS",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "8604863622",
  "inst_zip_code": "062699018",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CT02",
  "org_lgl_bus_name": "UNIVERSITY OF CONNECTICUT",
  "org_prnt_uei_num": "",
  "org_uei_num": "WNTPS995QBM7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Connecticut",
  "perf_str_addr": "850 Bolton Road, Unit 1085",
  "perf_city_name": "Storrs",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "062691085",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CT02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 167837.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 266158.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Learning a new language in adulthood can be challenging. Prevailing views suggest that adults who learn a new language after puberty may struggle with many aspects of language acquisition, among them learning to hear differences between sounds that are not part of one&rsquo;s native language. Indeed, adults are incredibly variable in their ability learn to perceive non-native speech sounds even with sufficient motivation and opportunities to learn. In this project, we investigated differences in brain structure, in the types of language input people receive, and in the use of memory consolidation processes during sleep in an effort to account for some this variable success. In a set of projects, we showed that when listeners are learning new speech sounds, listening to those sounds spoken by multiple talkers and in acoustically variable contexts destabilizes learning, and that having listeners do anything at all distracting (for instance, trying to articulate the sound) also resulted in poorer learning. &nbsp;We also tested the idea that children and adolescents might be better able to hear differences between sounds that are not part of their language. Instead, we found that adults out-performed adolescents and children, but that when tested the next day, the younger participants were able to remember what they learned slightly better than adults. It may be that younger learners have more efficient memory consolidation processes, especially those associated with sleep. Another account of the difficulty of new speech-sound learning is that there is interference from what listeners have already learned about sounds in their native language. Specifically, some theories suggest that by adulthood, it is difficult to hear any kind of variability within a speech sound category (for instance, two slightly different versions of the &lsquo;d&rsquo; in &lsquo;dog&rsquo;), and that this results in problems when new speech sound categories fall within a native language category. We challenged this view, showing no difference in non-native learning ability for people who showed greater or lesser sensitivity to these within-category differences. Instead, we showed that listeners who showed very accurate native language sound perception were also somewhat better at learning new sounds. One potential source for variability in learning may come from differences in the structure of the brain. Specifically, we showed that people who were better learners of new speech sounds (and who showed more accurate or reliable perception of their native language sounds) showed differences in brain structure in the transverse temporal gyrus&mdash;a part of the brain that is responsible for processing sounds. This suggests that some of the variability we see in learning new sounds may be innate, since this particular type of brain variability would have arisen during prenatal development. Nonetheless, our results also show that by structuring the learning environment appropriately, and perhaps by also encouraging memory consolidation processes, most people can learn these difficult sound contrasts. Language learning is a high-stakes enterprise. In our increasingly multilingual world, the ability to successfully communicate in a new language has consequences for social integration, for employment, and for economic success. By better understanding the processes that help listeners become more successful learners, we have the opportunity to shape teaching strategies to optimize language learning.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/29/2022<br>\n\t\t\t\t\tModified by: Emily&nbsp;Myers</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nLearning a new language in adulthood can be challenging. Prevailing views suggest that adults who learn a new language after puberty may struggle with many aspects of language acquisition, among them learning to hear differences between sounds that are not part of one\u2019s native language. Indeed, adults are incredibly variable in their ability learn to perceive non-native speech sounds even with sufficient motivation and opportunities to learn. In this project, we investigated differences in brain structure, in the types of language input people receive, and in the use of memory consolidation processes during sleep in an effort to account for some this variable success. In a set of projects, we showed that when listeners are learning new speech sounds, listening to those sounds spoken by multiple talkers and in acoustically variable contexts destabilizes learning, and that having listeners do anything at all distracting (for instance, trying to articulate the sound) also resulted in poorer learning.  We also tested the idea that children and adolescents might be better able to hear differences between sounds that are not part of their language. Instead, we found that adults out-performed adolescents and children, but that when tested the next day, the younger participants were able to remember what they learned slightly better than adults. It may be that younger learners have more efficient memory consolidation processes, especially those associated with sleep. Another account of the difficulty of new speech-sound learning is that there is interference from what listeners have already learned about sounds in their native language. Specifically, some theories suggest that by adulthood, it is difficult to hear any kind of variability within a speech sound category (for instance, two slightly different versions of the \u2018d\u2019 in \u2018dog\u2019), and that this results in problems when new speech sound categories fall within a native language category. We challenged this view, showing no difference in non-native learning ability for people who showed greater or lesser sensitivity to these within-category differences. Instead, we showed that listeners who showed very accurate native language sound perception were also somewhat better at learning new sounds. One potential source for variability in learning may come from differences in the structure of the brain. Specifically, we showed that people who were better learners of new speech sounds (and who showed more accurate or reliable perception of their native language sounds) showed differences in brain structure in the transverse temporal gyrus&mdash;a part of the brain that is responsible for processing sounds. This suggests that some of the variability we see in learning new sounds may be innate, since this particular type of brain variability would have arisen during prenatal development. Nonetheless, our results also show that by structuring the learning environment appropriately, and perhaps by also encouraging memory consolidation processes, most people can learn these difficult sound contrasts. Language learning is a high-stakes enterprise. In our increasingly multilingual world, the ability to successfully communicate in a new language has consequences for social integration, for employment, and for economic success. By better understanding the processes that help listeners become more successful learners, we have the opportunity to shape teaching strategies to optimize language learning.\n\n \n\n\t\t\t\t\tLast Modified: 09/29/2022\n\n\t\t\t\t\tSubmitted by: Emily Myers"
 }
}