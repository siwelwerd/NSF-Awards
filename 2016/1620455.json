{
 "awd_id": "1620455",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Harmonic analysis, non-convex optimization, and large data sets",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Leland Jameson",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2019-09-30",
 "tot_intn_awd_amt": 179979.0,
 "awd_amount": 179979.0,
 "awd_min_amd_letter_date": "2016-09-13",
 "awd_max_amd_letter_date": "2016-09-13",
 "awd_abstract_narration": "Future scientific and technological progress will depend heavily on the generation of new information technology capabilities and novel methods from signal and image processing to deal with today's massive volumes of data. A research effort is proposed to create mathematical concepts and computational methods to address some of the key challenges in this important area. In particular, the PI will focus on the areas of imaging, high-dimensional data analysis, machine learning, and information theory. The project uses tools from computational harmonic analysis, operator theory, random matrix theory, and optimization yielding efficient numerical algorithms with rigorously-established properties under carefully stated conditions. The payoffs for society at large are many, including new information technology capabilities, improved methods for signal- and image processing, as well as better understanding of data mining tools for Big Data.\r\n\r\nTwo concrete topics of this research effort are:(i) Fast and reliable algorithms of non-convex problems: When dealing with massive data sets, many tasks involve the use of a heuristic algorithm to solve a non-convex optimization problem. Often these heuristic algorithms get stuck in local minima, that are far away from the global minimum. We will develop fast numerical algorithms that come with theoretical performance guarantees for a range of important data analysis tasks; (ii) Efficient algorithms for heterogenous and high-dimensional data: Existing methods for high-dimensional data are often computationally rather expensive and rely on stationarity and homogeneity of the data, thus limiting their use for massive, heterogenous data sets. The PI will derive a framework of computationally efficient methods for properly fusing and efficiently processing heterogeneous, high-dimensional data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Strohmer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas Strohmer",
   "pi_email_addr": "strohmer@math.ucdavis.edu",
   "nsf_id": "000487508",
   "pi_start_date": "2016-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956168633",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 179979.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Future scientic and technological progress will depend heavily on the generation of new information technology capabilities and novel methods from signal and image processing to deal with today's massive volumes of data. Advances in technology and the ever-growing role of digital sensors and computers in science have led to an exponential growth in the amount and complexity of data we collect. Uncertainty, scale, non-stationarity, noise, and heterogeneity are fundamental issues impeding progress at all phases of the pipeline that creates knowledge from data. This means that the amount of new mathematical challenges arising from the need of data analysis and information processing is enormous, with their solution requiring fundamentally new ideas and approaches, with signicant consequences in the practical applications.<br />The major goals of this project have been the development of mathematical concepts and computational methods to efficiently and mathematically rigorously deal with various non-convex problems arising in connection with the analysis of massive data sets. <br /><br />The project has seen several major advances. We will describe three of them in more detail below.</p>\n<p>Organizing data into meaningful groups is one of the most fundamental tasks<br />in data analysis and machine learning Spectral clustering has become one of the most widely used clustering techniques when the structure of the individual clusters is non-convex or highly anisotropic. Yet, despite its immense popularity, there exists fairly little theory about performance guarantees for spectral clustering. This issue is partly due to the fact that spectral clustering typically involves two steps which complicated its theoretical analysis. The PI and his collaborators have developed the first rigorous&nbsp; framework for spectral clustering. By combining advanced tools from graph theory, random matrix theory, and convex&nbsp; optimization they developed algorithms together with precise performance bounds for spectral clustering. This result also paves the way for major advances in other areas such as community detection.<br /><br />Machine learning at the edge offers great benefits such as increased privacy and security, low latency, and more autonomy. However, a major challenge is that many devices, in particular edge devices, have very limited memory, weak processors, and scarce energy supply. The PI and his team have developed a hybrid hardware-software framework that has the potential to significantly reduce the computational complexity and memory requirements of on-device machine learning. In the first step, inspired by compressive sensing, data is collected in compressed form simultaneously with the sensing process. Thus this compression happens already at the hardware level during data acquisition. But unlike in compressive sensing, this compression is achieved via a projection operator that is specifically tailored to the desired machine learning task. The second step consists of a specially designed and trained deep network.&nbsp; An additional benefit of our approach is that it can be easily combined with existing on-device techniques.This finding opens up a new direction in putting deep learning on edge devices. Moreover, the agriculture industry is getting increasingly in the concept developed by the PI, and is starting to investigate if it can be incorporated in drones used as methane sensing/inference platform to measure/infer the methane foot print in dairy farm rich agricultures.</p>\n<p>Blind deconvolution is an omnipresent problem that pervades many areas of science and technology, including astronomy, medical imaging, optics, and communications engineering.&nbsp; The quest for finding fast, robust, and reliable algorithms for blind deconvolution has confounded researchers for many decades. In the future Internet-of-Things we face the eve more complicated problem where we have to extract multiple convolved signals mixed together in one observation signal. The PI has developed the first rigorous and computationally effient algorithms to solve this important problem. These algorithms my find use in 5G wireless communications.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/18/2020<br>\n\t\t\t\t\tModified by: Thomas&nbsp;Strohmer</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1620455/1620455_10461371_1582056006103_Ondevice_ML--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1620455/1620455_10461371_1582056006103_Ondevice_ML--rgov-800width.jpg\" title=\"Compressive deep learning on the edge\"><img src=\"/por/images/Reports/POR/2020/1620455/1620455_10461371_1582056006103_Ondevice_ML--rgov-66x44.jpg\" alt=\"Compressive deep learning on the edge\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Schematic depiction of compressive deep learning: Data acquisition and compression arecarried out simultaneously. Compression is achieved at the hardware level via a mathematical projection operator that is specifically tailored to the desired machine learning task</div>\n<div class=\"imageCredit\">Created by the PI</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Thomas&nbsp;Strohmer</div>\n<div class=\"imageTitle\">Compressive deep learning on the edge</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1620455/1620455_10461371_1582054757540_Ondevice_ML--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1620455/1620455_10461371_1582054757540_Ondevice_ML--rgov-800width.jpg\" title=\"Compressive deep learning on the edge\"><img src=\"/por/images/Reports/POR/2020/1620455/1620455_10461371_1582054757540_Ondevice_ML--rgov-66x44.jpg\" alt=\"Compressive deep learning on the edge\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Schematic depiction of compressive deep learning: Data acquisition and compression arecarried out simultaneously. Compression is achieved at the hardware level,  that is specifically tailored to the desired machine learning task.</div>\n<div class=\"imageCredit\">Created by the PI</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Thomas&nbsp;Strohmer</div>\n<div class=\"imageTitle\">Compressive deep learning on the edge</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nFuture scientic and technological progress will depend heavily on the generation of new information technology capabilities and novel methods from signal and image processing to deal with today's massive volumes of data. Advances in technology and the ever-growing role of digital sensors and computers in science have led to an exponential growth in the amount and complexity of data we collect. Uncertainty, scale, non-stationarity, noise, and heterogeneity are fundamental issues impeding progress at all phases of the pipeline that creates knowledge from data. This means that the amount of new mathematical challenges arising from the need of data analysis and information processing is enormous, with their solution requiring fundamentally new ideas and approaches, with signicant consequences in the practical applications.\nThe major goals of this project have been the development of mathematical concepts and computational methods to efficiently and mathematically rigorously deal with various non-convex problems arising in connection with the analysis of massive data sets. \n\nThe project has seen several major advances. We will describe three of them in more detail below.\n\nOrganizing data into meaningful groups is one of the most fundamental tasks\nin data analysis and machine learning Spectral clustering has become one of the most widely used clustering techniques when the structure of the individual clusters is non-convex or highly anisotropic. Yet, despite its immense popularity, there exists fairly little theory about performance guarantees for spectral clustering. This issue is partly due to the fact that spectral clustering typically involves two steps which complicated its theoretical analysis. The PI and his collaborators have developed the first rigorous  framework for spectral clustering. By combining advanced tools from graph theory, random matrix theory, and convex  optimization they developed algorithms together with precise performance bounds for spectral clustering. This result also paves the way for major advances in other areas such as community detection.\n\nMachine learning at the edge offers great benefits such as increased privacy and security, low latency, and more autonomy. However, a major challenge is that many devices, in particular edge devices, have very limited memory, weak processors, and scarce energy supply. The PI and his team have developed a hybrid hardware-software framework that has the potential to significantly reduce the computational complexity and memory requirements of on-device machine learning. In the first step, inspired by compressive sensing, data is collected in compressed form simultaneously with the sensing process. Thus this compression happens already at the hardware level during data acquisition. But unlike in compressive sensing, this compression is achieved via a projection operator that is specifically tailored to the desired machine learning task. The second step consists of a specially designed and trained deep network.  An additional benefit of our approach is that it can be easily combined with existing on-device techniques.This finding opens up a new direction in putting deep learning on edge devices. Moreover, the agriculture industry is getting increasingly in the concept developed by the PI, and is starting to investigate if it can be incorporated in drones used as methane sensing/inference platform to measure/infer the methane foot print in dairy farm rich agricultures.\n\nBlind deconvolution is an omnipresent problem that pervades many areas of science and technology, including astronomy, medical imaging, optics, and communications engineering.  The quest for finding fast, robust, and reliable algorithms for blind deconvolution has confounded researchers for many decades. In the future Internet-of-Things we face the eve more complicated problem where we have to extract multiple convolved signals mixed together in one observation signal. The PI has developed the first rigorous and computationally effient algorithms to solve this important problem. These algorithms my find use in 5G wireless communications.\n\n\t\t\t\t\tLast Modified: 02/18/2020\n\n\t\t\t\t\tSubmitted by: Thomas Strohmer"
 }
}