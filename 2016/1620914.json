{
 "awd_id": "1620914",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "The Mathematical Knowledge for Teaching Measures: Refreshing the Item Pool",
 "cfda_num": "47.076",
 "org_code": "11090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Finbarr Sloane",
 "awd_eff_date": "2016-12-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 2998220.0,
 "awd_amount": 2998220.0,
 "awd_min_amd_letter_date": "2016-09-12",
 "awd_max_amd_letter_date": "2021-08-13",
 "awd_abstract_narration": "This project proposes an assessment study that focuses on improving existing measures of teachers' Mathematical Knowledge for Teaching (MKT). The research team will update existing measures, adding new items and aligning the instrument to new standards in school mathematics. In addition, the team will update the delivery system for the assessment to Qualtrics, a more flexible online system.\r\n\r\nThe research team will build an updated measure of teachers' Mathematical Knowledge for Teaching (MKT).  Project researchers will conduct item writing camps, develop new items, cognitively pilot and revise items, and factor analyze items.  The researchers will also determine item constructs and calibrate items (and constructs) through an innovative application of Item Response Theory (IRT) employing a variant of the standard 2-parameter IRT model.  Finally, the team will oversee the transition of the Teacher Knowledge Assessment System to the Qualtrics data collection environment to allow for more flexible item specification.\r\n\r\nThe Discovery Research PreK-12 program (DRK-12) seeks to significantly enhance the learning and teaching of science, technology, engineering and mathematics (STEM) by preK-12 students and teachers, through research and development of innovative resources, models and tools (RMTs). Projects in the DRK-12 program build on fundamental research in STEM education and prior research and development efforts that provide theoretical and empirical justification for proposed projects.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DRL",
 "org_div_long_name": "Division of Research on Learning in Formal and Informal Settings (DRL)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Heather",
   "pi_last_name": "Hill",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Heather Hill",
   "pi_email_addr": "heather_hill@gse.harvard.edu",
   "nsf_id": "000082212",
   "pi_start_date": "2016-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Schilling",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen Schilling",
   "pi_email_addr": "schillsg@umich.edu",
   "nsf_id": "000154458",
   "pi_start_date": "2016-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Merrie",
   "pi_last_name": "Blunk",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Merrie Blunk",
   "pi_email_addr": "mblunk@umich.edu",
   "nsf_id": "000496414",
   "pi_start_date": "2016-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Corinne",
   "pi_last_name": "Herlihy",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Corinne M Herlihy",
   "pi_email_addr": "corinne_herlihy@gse.harvard.edu",
   "nsf_id": "000612811",
   "pi_start_date": "2016-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "McGinn",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel McGinn",
   "pi_email_addr": "daniel_mcginn@gse.harvard.edu",
   "nsf_id": "000661153",
   "pi_start_date": "2016-09-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University Graduate School of Education",
  "perf_str_addr": "13 Appian Way, Gutman 445",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021383703",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "179500",
   "pgm_ele_name": "Robert Noyce Scholarship Pgm"
  },
  {
   "pgm_ele_code": "764500",
   "pgm_ele_name": "Discovery Research K-12"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0415",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001516DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "04XX",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "0400XXXXDB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 2998220.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"xmsonormal\">The Mathematical Knowledge for Teaching (MKT) assessment was first designed and fielded in 2002 as a way to measure the mathematical and pedagogical knowledge specific to teaching K - 8 math. Since then it has been used in numerous studies to measure teacher learning in response to professional learning opportunities, understand how teacher knowledge is related to instructional practice, and to relate teacher knowledge to student outcomes. Given the widespread use of the MKT assessment in a variety of settings, and the small bank of forms available to researchers and evaluators, the National Science Foundation funded an expansion of MKT items and forms. The Mathematical Knowledge for Teaching Measures: Refreshing the Item Pool project began in 2016 with a team of researchers, including members of the original MKT team, to start the process of updating the measures and to enhance the user experience with the online Teacher Knowledge Assessment System (TKAS).</p>\n<p class=\"xmsonormal\">We successfully transferred TKAS onto the Qualtrics survey platform and updated the training for TKAS administrators. These upgrades improve the user experience with easier navigation and additional tech support.</p>\n<p class=\"xmsonormal\">We also enhanced the quality of the assessments by adding new items and forms to the system.</p>\n<p class=\"pf0\">New MKT items were generated at several brainstorming sessions across the country. Participants included math teachers, math coaches, professional developers, district administrators, curriculum designers, math education researchers, and mathematicians. Items were then reviewed by the MKT team (which included teachers, researchers, and mathematicians) for clarity, appropriateness, and item structure. Lastly, items were sent out to math content experts to ensure the items were unambiguous and contained a single correct answer.</p>\n<p class=\"xmsonormal\">We used Qualtrics research panel, a pool of willing survey participants maintained by Qualtrics and organized by characteristics (e.g. grade level taught), to pilot forms of new items. Items typically passed through two stages of piloting. First, we used a mini-pilot, targeting roughly 150 respondents, to identify poorly performing items for revision or, if necessary, removal. Successful items then moved on to a full pilot, targeting roughly 500 respondents.&nbsp; We used Classical Test Theory and Item Response Theory to inform our analysis of item functioning. We then used Item Response Theory to create scoring parameters for each form (either 1 Parameter or 2 Parameter Logistic models, depending on the sample size).</p>\n<p class=\"xmsonormal\">In total, we deployed 5 new online forms (1 Grades 4 - 8 Geometry, 2 Elementary Number Concepts and Operations, 1 Middle School Algebra, and 2 Middle School Number Concept and Operations). Each of these new online assessments consists of two parallel and equated forms, with 30 - 40 items that passed both mini-pilot and full-pilot. &nbsp;The TKAS online platform will produce IRT scores for participants. Compared to raw score or percent correct, IRT scores provide a more robust way to compare samples, track growth over time, or correlate with other measures. Corresponding pilot data, tech report, and scoring guidance have also been developed. &nbsp;</p>\n<p class=\"pf0\">We also created 5 paper and pencil only (PPO) forms: two algebra, one elementary number concepts and operations, and two middle school number concepts and operations. Each of these is a single form, composed of MKT items that passed mini-pilot but did not get piloted with a full sample. While these forms are not available for online administration through the TKAS system, administrators will have access to pilot data and tools (i.e. scoring guidance) to produce IRT scores for their participants.</p>\n<p>And finally, we compiled 72 items that failed piloting for potential exploration and open-ended discussions in professional development and teacher education.&nbsp;</p>\n<p>Towards our goal of piloting new item formats, we presented teachers with twelve classroom scenarios and then asked them to record themselves speaking and responding to students' mathematical contributions in those scenarios as if they were in a real classroom. Half of the items (administered to 300 teachers) provided opportunities for teachers to take up students' mathematical contributions (\"uptake\") and the other half required teachers to provide a mathematical explanation for student work (n=300 teachers). Our goal was to determine whether each set of items measures a latent trait (inclination to uptake student responses; quality of mathematical explanation).</p>\n<p>Our findings suggest that the uptake items do not appear to measure a single latent trait, as the math content and specific student response in each item appears to drive variation among participants. The second set, however, do appear to measure an underlying latent trait. Teachers who provide a conceptual explanation to one item are more likely to do so for other items; however, their explanations tend to be more procedural than conceptual in nature. These items move the assessment of teachers' knowledge toward in-the-moment decision-making, a goal we have long held but that has been hard to achieve in the multiple choice format.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/30/2023<br>\n\t\t\t\t\tModified by: Heather&nbsp;Hill</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The Mathematical Knowledge for Teaching (MKT) assessment was first designed and fielded in 2002 as a way to measure the mathematical and pedagogical knowledge specific to teaching K - 8 math. Since then it has been used in numerous studies to measure teacher learning in response to professional learning opportunities, understand how teacher knowledge is related to instructional practice, and to relate teacher knowledge to student outcomes. Given the widespread use of the MKT assessment in a variety of settings, and the small bank of forms available to researchers and evaluators, the National Science Foundation funded an expansion of MKT items and forms. The Mathematical Knowledge for Teaching Measures: Refreshing the Item Pool project began in 2016 with a team of researchers, including members of the original MKT team, to start the process of updating the measures and to enhance the user experience with the online Teacher Knowledge Assessment System (TKAS).\nWe successfully transferred TKAS onto the Qualtrics survey platform and updated the training for TKAS administrators. These upgrades improve the user experience with easier navigation and additional tech support.\nWe also enhanced the quality of the assessments by adding new items and forms to the system.\nNew MKT items were generated at several brainstorming sessions across the country. Participants included math teachers, math coaches, professional developers, district administrators, curriculum designers, math education researchers, and mathematicians. Items were then reviewed by the MKT team (which included teachers, researchers, and mathematicians) for clarity, appropriateness, and item structure. Lastly, items were sent out to math content experts to ensure the items were unambiguous and contained a single correct answer.\nWe used Qualtrics research panel, a pool of willing survey participants maintained by Qualtrics and organized by characteristics (e.g. grade level taught), to pilot forms of new items. Items typically passed through two stages of piloting. First, we used a mini-pilot, targeting roughly 150 respondents, to identify poorly performing items for revision or, if necessary, removal. Successful items then moved on to a full pilot, targeting roughly 500 respondents.  We used Classical Test Theory and Item Response Theory to inform our analysis of item functioning. We then used Item Response Theory to create scoring parameters for each form (either 1 Parameter or 2 Parameter Logistic models, depending on the sample size).\nIn total, we deployed 5 new online forms (1 Grades 4 - 8 Geometry, 2 Elementary Number Concepts and Operations, 1 Middle School Algebra, and 2 Middle School Number Concept and Operations). Each of these new online assessments consists of two parallel and equated forms, with 30 - 40 items that passed both mini-pilot and full-pilot.  The TKAS online platform will produce IRT scores for participants. Compared to raw score or percent correct, IRT scores provide a more robust way to compare samples, track growth over time, or correlate with other measures. Corresponding pilot data, tech report, and scoring guidance have also been developed.  \nWe also created 5 paper and pencil only (PPO) forms: two algebra, one elementary number concepts and operations, and two middle school number concepts and operations. Each of these is a single form, composed of MKT items that passed mini-pilot but did not get piloted with a full sample. While these forms are not available for online administration through the TKAS system, administrators will have access to pilot data and tools (i.e. scoring guidance) to produce IRT scores for their participants.\n\nAnd finally, we compiled 72 items that failed piloting for potential exploration and open-ended discussions in professional development and teacher education. \n\nTowards our goal of piloting new item formats, we presented teachers with twelve classroom scenarios and then asked them to record themselves speaking and responding to students' mathematical contributions in those scenarios as if they were in a real classroom. Half of the items (administered to 300 teachers) provided opportunities for teachers to take up students' mathematical contributions (\"uptake\") and the other half required teachers to provide a mathematical explanation for student work (n=300 teachers). Our goal was to determine whether each set of items measures a latent trait (inclination to uptake student responses; quality of mathematical explanation).\n\nOur findings suggest that the uptake items do not appear to measure a single latent trait, as the math content and specific student response in each item appears to drive variation among participants. The second set, however, do appear to measure an underlying latent trait. Teachers who provide a conceptual explanation to one item are more likely to do so for other items; however, their explanations tend to be more procedural than conceptual in nature. These items move the assessment of teachers' knowledge toward in-the-moment decision-making, a goal we have long held but that has been hard to achieve in the multiple choice format. \n\n\t\t\t\t\tLast Modified: 01/30/2023\n\n\t\t\t\t\tSubmitted by: Heather Hill"
 }
}