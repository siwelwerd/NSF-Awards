{
 "awd_id": "1564074",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Medium: Collaborative Research: An Inspector/Executor Compilation Framework for Irregular Applications",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927885",
 "po_email": "abanerje@nsf.gov",
 "po_sign_block_name": "Anindya Banerjee",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 399993.0,
 "awd_amount": 399993.0,
 "awd_min_amd_letter_date": "2016-07-27",
 "awd_max_amd_letter_date": "2016-07-27",
 "awd_abstract_narration": "Computational science and engineering provides inexpensive exploration of physical phenomena and design spaces and helps direct experimentation and advise theory. Irregular applications such as molecular dynamics simulations, n-body simulations, finite element analysis, and big graph analytics constitute a critical and significant portion of scientific computing applications.  An irregular application is characterized by having indirect memory accesses that cannot be determined when the application is being compiled, therefore severely limiting the applicability of the large body of work on parallelizing compiler technology. Consequently, irregular applications, which are so important in pushing forward the frontiers of science, place a very large burden on computational and domain scientists in developing high-performance implementations for the ever-changing landscape of parallel architectures. The intellectual merit of this project is to develop a compiler and runtime framework for irregular applications, particularly well suited for sparse matrix and graph computations that underlie critical problems in computational science and data science. The broader impact is to provide domain scientists a powerful tool for optimizing and porting performance-critical, irregular computations to current and future multi-core processors and many-core accelerators. The PIs will also continue  efforts in outreach and diversity to increase the participation in STEM careers, particularly among women and underrepresented minorities.\r\n\r\nThe approach in this project is to extend the well-established inspector/executor paradigm where the computational dependence structure (based on the memory access pattern) is determined at runtime, and runtime information is passed to a compile-time generated executor. Specifically, an inspector can examine the memory access patterns early in the computation at runtime, and an executor leverages this information to perform data and computation reordering and scheduling to affect memory hierarchy and parallelism optimizations. The project is developing a compiler and runtime framework with new abstractions for expressing and manipulating inspectors; these inspectors may then be integrated nearly seamlessly with each other and with existing compiler optimizations (e.g., loop tiling) to optimize executors. The project is also extending prior work that supports non-affine input code and mixes compile-time and runtime optimization. The resulting system increases the productivity of expert programmers in achieving both high performance and portability on a wide variety of irregular applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mary",
   "pi_last_name": "Hall",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mary Hall",
   "pi_email_addr": "mhall@cs.utah.edu",
   "nsf_id": "000367228",
   "pi_start_date": "2016-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Utah",
  "inst_street_address": "201 PRESIDENTS CIR",
  "inst_street_address_2": "",
  "inst_city_name": "SALT LAKE CITY",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8015816903",
  "inst_zip_code": "841129049",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "UT01",
  "org_lgl_bus_name": "UNIVERSITY OF UTAH",
  "org_prnt_uei_num": "",
  "org_uei_num": "LL8GLEVH6MG3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Utah",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "841128930",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "UT01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "689200",
   "pgm_ele_name": "CI REUSE"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7943",
   "pgm_ref_txt": "PROGRAMMING LANGUAGES"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 399993.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Irregular computations involving sparse matrices, tensors and graphs (referred to as <em>sparse computations</em>) are found in a variety of important applications such as molecular dynamics, particle simulation, and deep learning.&nbsp; A common aspect of sparse computations is that not all points representing a 3-dimensional space have associated data -- as a result, the data are compressed to only describe nonzero values, and the data's organization, density, and nonzero patterns are only known when the data becomes available during program execution.&nbsp; Moreover, data must be decompressed during computation, which leads to unpredictable memory access patterns.&nbsp; These aspects of sparse computations lead to very low arithmetic intensity, where performance is dominated by moving the sparse data through the memory system.</p>\n<p>The CHiLL-I/E project developed compiler optimization techniques to optimize sparse computations for modern parallel platforms including CPUs and GPUs.&nbsp; Recognizing that the memory access patterns are not fully available until runtime, CHiLL-I/E employs an <em>inspector-executor approach</em>, where the compiler generates <em>inspector</em> code at compile time that collects runtime information used in the optimized <em>executor</em> code that performs the computation.&nbsp; This work combines the existing CHiLL autotuning compiler and the IEGenLib for generating inspector-executor code. The resulting system employs sparse polyhedral compilation technology and a configurable schedule of polyhedral transformations to compose inspector-executor code generation with other code transformations, thus increasing parallelism and improving memory access patterns for sparse computations.&nbsp;</p>\n<p>The CHiLL-I/E project focused on several critical issues associated with optimizing sparse computations using an inspector-executor approach:</p>\n<ul>\n<li><strong>Automated dependence testing for sparse computations and wavefront parallelism:&nbsp;</strong> We developed techniques for testing data dependences in the presence of indirect memory accesses through <em>index arrays</em>, which map from sparse data layouts to associated dense locations.&nbsp; The compiler derives a set of constraints, and a SAT solver determines whether the constraints are unsatisfiable, indicated there are no data dependences.&nbsp; Data dependences associated with remaining satisfiable constraints are evaluated at runtime using an inspector.&nbsp; This approach has been used to optimize common sparse linear algebra computations via wavefront parallelism,&nbsp;including privatizing index arrays to reduce dependences.</li>\n<li><strong>Optimizing inspectors:&nbsp;</strong>Inspectors may be generated to support multiple optimization goals, each potentially requiring a sweep over sparse data.&nbsp; We have developed a dataflowrepresentation of inspector computations, and graph-based abstractions for fusing computations and reducing unnecessary intermediate storage.</li>\n<li><strong>Generalizing sparse data layout conversion:&nbsp;</strong>Since the most performant sparse data format depends on architectural features, sparsity patterns and density, it is frequently necessary to&nbsp;transform a computation to work with different sparse data layouts. We have prototyped a general approach to layout conversion that is capable of&nbsp;transforming existing sparse tensor computations from the layout of their current specification to any new layout that can be expressed in a sparse&nbsp;polyhedral framework.</li>\n<li><strong>Co-optimizing data layout and tensor computation:&nbsp;</strong>We have developed a code generator in CHiLL-I/E that employs sparse polyhedral technology and code synthesis to generate code for co-iteration across&nbsp;multiple sparse tensors, limited to multiplication and addition operations between tensors. Our approach composes the iteration space of the&nbsp;computation with an iteration space over the data layout for one of&nbsp;the sparse tensors. It then performs a lookup in the other sparse tensor to see if that element appears in both tensors. Our approach combines&nbsp;polyhedral scanning over the composed iteration space with code synthesis using an SMT solver to implement the find operation.&nbsp; </li>\n</ul><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/24/2022<br>\n\t\t\t\t\tModified by: Mary&nbsp;Hall</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1564074/1564074_10444364_1643041777502_Slide1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1564074/1564074_10444364_1643041777502_Slide1--rgov-800width.jpg\" title=\"Project Overview\"><img src=\"/por/images/Reports/POR/2022/1564074/1564074_10444364_1643041777502_Slide1--rgov-66x44.jpg\" alt=\"Project Overview\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This figure illustrates optimization of sparse computations using an inspector-executor approach in a Sparse Polyhedral Framework.</div>\n<div class=\"imageCredit\">Michelle Strout</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Mary&nbsp;Hall</div>\n<div class=\"imageTitle\">Project Overview</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIrregular computations involving sparse matrices, tensors and graphs (referred to as sparse computations) are found in a variety of important applications such as molecular dynamics, particle simulation, and deep learning.  A common aspect of sparse computations is that not all points representing a 3-dimensional space have associated data -- as a result, the data are compressed to only describe nonzero values, and the data's organization, density, and nonzero patterns are only known when the data becomes available during program execution.  Moreover, data must be decompressed during computation, which leads to unpredictable memory access patterns.  These aspects of sparse computations lead to very low arithmetic intensity, where performance is dominated by moving the sparse data through the memory system.\n\nThe CHiLL-I/E project developed compiler optimization techniques to optimize sparse computations for modern parallel platforms including CPUs and GPUs.  Recognizing that the memory access patterns are not fully available until runtime, CHiLL-I/E employs an inspector-executor approach, where the compiler generates inspector code at compile time that collects runtime information used in the optimized executor code that performs the computation.  This work combines the existing CHiLL autotuning compiler and the IEGenLib for generating inspector-executor code. The resulting system employs sparse polyhedral compilation technology and a configurable schedule of polyhedral transformations to compose inspector-executor code generation with other code transformations, thus increasing parallelism and improving memory access patterns for sparse computations. \n\nThe CHiLL-I/E project focused on several critical issues associated with optimizing sparse computations using an inspector-executor approach:\n\nAutomated dependence testing for sparse computations and wavefront parallelism:  We developed techniques for testing data dependences in the presence of indirect memory accesses through index arrays, which map from sparse data layouts to associated dense locations.  The compiler derives a set of constraints, and a SAT solver determines whether the constraints are unsatisfiable, indicated there are no data dependences.  Data dependences associated with remaining satisfiable constraints are evaluated at runtime using an inspector.  This approach has been used to optimize common sparse linear algebra computations via wavefront parallelism, including privatizing index arrays to reduce dependences.\nOptimizing inspectors: Inspectors may be generated to support multiple optimization goals, each potentially requiring a sweep over sparse data.  We have developed a dataflowrepresentation of inspector computations, and graph-based abstractions for fusing computations and reducing unnecessary intermediate storage.\nGeneralizing sparse data layout conversion: Since the most performant sparse data format depends on architectural features, sparsity patterns and density, it is frequently necessary to transform a computation to work with different sparse data layouts. We have prototyped a general approach to layout conversion that is capable of transforming existing sparse tensor computations from the layout of their current specification to any new layout that can be expressed in a sparse polyhedral framework.\nCo-optimizing data layout and tensor computation: We have developed a code generator in CHiLL-I/E that employs sparse polyhedral technology and code synthesis to generate code for co-iteration across multiple sparse tensors, limited to multiplication and addition operations between tensors. Our approach composes the iteration space of the computation with an iteration space over the data layout for one of the sparse tensors. It then performs a lookup in the other sparse tensor to see if that element appears in both tensors. Our approach combines polyhedral scanning over the composed iteration space with code synthesis using an SMT solver to implement the find operation.  \n\n\n\t\t\t\t\tLast Modified: 01/24/2022\n\n\t\t\t\t\tSubmitted by: Mary Hall"
 }
}