{
 "awd_id": "1637473",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Reduced-order Methods for Big-Data Challenges in Nonlinear and Stochastic Optimization",
 "cfda_num": "47.041",
 "org_code": "07030008",
 "po_phone": "7032922443",
 "po_email": "gaklutke@nsf.gov",
 "po_sign_block_name": "Georgia-Ann Klutke",
 "awd_eff_date": "2016-02-15",
 "awd_exp_date": "2018-12-31",
 "tot_intn_awd_amt": 342803.0,
 "awd_amount": 342803.0,
 "awd_min_amd_letter_date": "2016-05-26",
 "awd_max_amd_letter_date": "2016-05-26",
 "awd_abstract_narration": "The objective of this Faculty Early Career Development (CAREER) Program project is to develop a set of new reduced-order algorithms to tackle the big-data challenges in optimization. The last several years have seen an unprecedented growth in the amount of available data. While nonlinear, especially convex programming (CP) models are important to extract useful knowledge from raw data, high problem dimensionality, large data volumes and inherent uncertainty present significant challenges to the design of optimization algorithms. This research aims to attack these challenges by investigating: (i) novel first-order methods for deterministic CP that converge faster, require little structural information and do not rely on line search, based on level methods; (ii) stochastic first-order methods that handle data uncertainty in an optimal manner, based on stochastic approximation; (iii) novel randomization schemes for solving certain challenging deterministic CP problems beyond the capability of first-order methods; and (iv) stochastic first- and zeroth-order methods for general, not necessarily convex, stochastic programs. The research focuses on two fundamental issues across these topics: (i) the study of complexity which provides guarantees on algorithmic performance; and (ii) the exploitation of structures that leads to the design of algorithms with stronger complexity and superior practical performance.\r\n\r\nIf successful, a set of new algorithmic schemes will advance the state-of-the-art in nonlinear and stochastic optimization, bringing many practically relevant data analysis problems within the range of tractability. Example applications include algorithms for faster and more accurate medical image reconstruction and classification, which will be beneficial to healthcare.  In addition, in seismology, effective stochastic programming methods will help to build predictive models by measuring thousands of earthquakes detected at seismic stations. The project will also support the PI's educational goals to improve students' learning in operations research, broaden the representation of underrepresented groups in the PhD program, and contribute to open research infrastructure through the development of optimization solvers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Guanghui",
   "pi_last_name": "Lan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Guanghui Lan",
   "pi_email_addr": "george.lan@isye.gatech.edu",
   "nsf_id": "000545078",
   "pi_start_date": "2016-05-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320420",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "551400",
   "pgm_ele_name": "OPERATIONS RESEARCH"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "072E",
   "pgm_ref_txt": "NETWORKS & QUEUING SYSTEMS"
  },
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "077E",
   "pgm_ref_txt": "SIMULATION MODELS"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 342803.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 15\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>NSF Award No. CMMI-1637473&nbsp;</span><span>&ldquo;CAREER: Reduced-order methods for big-data challenges in nonlinear and stochastic optimization&rdquo;&nbsp;</span></p>\n<p><span>Under this grant support, for the first time in the literature, the PI has developed stochastic gradient descent (SGD) for nonconvex optimization and established their iteration and sampling complexity. In addition, the PI showed how the gradient descent (GD) method and its stochastic counterpart can be accelerated in the nonconvex setting. These works had laid out the foundation for nonconvex stochastic optimization and its applications for solving nonconvex machine learning models, e.g., deep learing. &nbsp;With the support of this grant, the PI had also developed the first uniformly optimal first-order methods called the accelerated prox-level method, which is a problem-parameter free first-order method with superior practical performance. The PI also developed the first optimal randomized incremental gradient method for convex optimization, and the first optimal algorithms for decentralized stochastic convex optimization.</span></p>\n<p><span>&nbsp;</span><span>This project has involved a few Ph.D. students. Three of them already became professors in higher education institutions, and two of them joined a few leading research labs (e.g., IBM and Google research). </span><span>Under the support of this grant, the PI has completed one monograph entitled \"Lectures on Optimization Methods for Machine Learning\" that can serve as a textbook in this area for senior undergraduate, masters and Ph.D. students. This project has also resulted&nbsp; more than 20 research publications, most of which appear in leading journals such as Mathematical Programming and SIAM Journal on Optimizatoin, and top conference proceedings such as the one for International Conference on Machine Learning (ICML).</span></p>\n</div>\n</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/28/2019<br>\n\t\t\t\t\tModified by: Guanghui&nbsp;Lan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\nNSF Award No. CMMI-1637473 \"CAREER: Reduced-order methods for big-data challenges in nonlinear and stochastic optimization\" \n\nUnder this grant support, for the first time in the literature, the PI has developed stochastic gradient descent (SGD) for nonconvex optimization and established their iteration and sampling complexity. In addition, the PI showed how the gradient descent (GD) method and its stochastic counterpart can be accelerated in the nonconvex setting. These works had laid out the foundation for nonconvex stochastic optimization and its applications for solving nonconvex machine learning models, e.g., deep learing.  With the support of this grant, the PI had also developed the first uniformly optimal first-order methods called the accelerated prox-level method, which is a problem-parameter free first-order method with superior practical performance. The PI also developed the first optimal randomized incremental gradient method for convex optimization, and the first optimal algorithms for decentralized stochastic convex optimization.\n\n This project has involved a few Ph.D. students. Three of them already became professors in higher education institutions, and two of them joined a few leading research labs (e.g., IBM and Google research). Under the support of this grant, the PI has completed one monograph entitled \"Lectures on Optimization Methods for Machine Learning\" that can serve as a textbook in this area for senior undergraduate, masters and Ph.D. students. This project has also resulted  more than 20 research publications, most of which appear in leading journals such as Mathematical Programming and SIAM Journal on Optimizatoin, and top conference proceedings such as the one for International Conference on Machine Learning (ICML).\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 03/28/2019\n\n\t\t\t\t\tSubmitted by: Guanghui Lan"
 }
}