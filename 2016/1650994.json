{
 "awd_id": "1650994",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER:  Controlling a Robotic Third Hand - Exploring Use of Distributed Intelligence from Autonomy to Brain Machine Interfaces for Augmenting Human Capability",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2016-08-15",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2016-08-06",
 "awd_max_amd_letter_date": "2017-01-27",
 "awd_abstract_narration": "This project aims to determine if the human brain has sufficient self-adaptivity, called plasticity, to operate brain-controlled robotic devices in performing tasks that require continuous coordination and synchronization with the person's natural limbs.  These tasks are those where the action of one hand (or limb) depends on that of the other, here extended to include a robotic device, such as an unattached artificial \"third hand.\"  Besides advanced human prosthetics, application domains include co-robots as aides supporting emergency responders in hazardous environments performing complex manual operations in the field, in space and undersea, particularly where they need to perform independently without the assistance of other persons. The results are potentially transformative for human-computer interaction and cyber-human systems research and applications.\r\n\r\nThe research performs a set of non-invasive human subject experiments to determine whether the human brain may have sufficient neuroplasticity to enable \"asymmetric dependent\" operation of a detached robotic \"third hand\" (those in which the action of one hand depends on that of the other) through a brain-machine interface, or whether it may be limited in that capability by 2.5 million years of hominin evolution of our limbs and their extremities. This is a critical consideration as we explore the many issues of augmenting human physical capability with control implemented through brain-machine technology.  Designs combining varying degrees of robot autonomy with human-machine verbal and gesture communicated control are also considered as supplements or alternatives to direct brain-control.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Howard",
   "pi_last_name": "Wactlar",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Howard D Wactlar",
   "pi_email_addr": "wactlar@cmu.edu",
   "nsf_id": "000153479",
   "pi_start_date": "2016-08-06",
   "pi_end_date": "2017-01-27"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Hauptmann",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander G Hauptmann",
   "pi_email_addr": "alex@cs.cmu.edu",
   "nsf_id": "000228336",
   "pi_start_date": "2017-01-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Ave.",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<p>This project broad aims were to determine if the human brain has sufficient self-adaptivity, called plasticity, to operate brain-controlled, smart robotic devices in performing tasks that require continuous coordination and synchronization with the person's natural limbs.These tasks are those where the action of one hand (or limb) depends on that of the other, here extended to include a robotic device, such as an unattached artificial \"third hand.\" Besides advanced human prosthetics, application domains include co-robots as aides supporting emergency responders in hazardous environments performing complex manual operations in the field, in space and under sea, particularly where they need to perform independently without the assistance of other persons.The results are potentially transformative for human-computer interaction and cyber-human systems research and applications. Rather than performing a set of human subject experiments our research focused on determining what the robotic device (\"third hand\") needs to understand in order to provide useful assistance. This involves understending of the task, being aware of the steps in the process and being able to predict some of the necessary movements or actions based on the real world context requirements.We used existing collections of video and text datasets to serve a proxies for live human interaction.</p>\n<p>In the final year, we have put significant emphasis on considering the robotic side of the third hand, that needs to be aware ofthe context, have some understanding the current task and be able to predict some of the necessary movements or actionsbased on the real world requirements.&nbsp;</p>\n<p>In one thrust of this research we have proposed novel deep learning models that utilize enhanced contextual cues like scene semantics and human actions for trajectory prediction in urban traffic scenes. One of the research goals is to promote human safety in applications such as robotic devices or autonomously driving vehicles. We have explored multi-modal trajectory prediction and robust learning algorithms using simulation-augmentation. These are important directions as they provide better traffic safety if used in diverse environments. To push the boundary even further, we have proposed along-term trajectory prediction dataset and models for joint action and trajectory prediction in urban traffic scenes. In summary, we have answered the key research question of how to build a robust trajectory prediction system with enhanced semantic context understanding for urban traffic scenes.</p>\n<p>In another part of this work, we studied how intelligent robot devices might come to predict events using instructional videos as a proxy for real activities. While instructional videos usually have concrete steps for events, these future events typically have multiple hypotheses and unpredictable details. We incorporate a memory network to encode the video into a compressed set of memories contrastive analysis to explicitly model the temporal relations across events. Our experiments show the effectiveness of our model on the YouCookII dataset where it can efficiently predict the next possible events and outperform existing baselines work by a large margin.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/23/2021<br>\n\t\t\t\t\tModified by: Alexander&nbsp;G&nbsp;Hauptmann</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \n\nThis project broad aims were to determine if the human brain has sufficient self-adaptivity, called plasticity, to operate brain-controlled, smart robotic devices in performing tasks that require continuous coordination and synchronization with the person's natural limbs.These tasks are those where the action of one hand (or limb) depends on that of the other, here extended to include a robotic device, such as an unattached artificial \"third hand.\" Besides advanced human prosthetics, application domains include co-robots as aides supporting emergency responders in hazardous environments performing complex manual operations in the field, in space and under sea, particularly where they need to perform independently without the assistance of other persons.The results are potentially transformative for human-computer interaction and cyber-human systems research and applications. Rather than performing a set of human subject experiments our research focused on determining what the robotic device (\"third hand\") needs to understand in order to provide useful assistance. This involves understending of the task, being aware of the steps in the process and being able to predict some of the necessary movements or actions based on the real world context requirements.We used existing collections of video and text datasets to serve a proxies for live human interaction.\n\nIn the final year, we have put significant emphasis on considering the robotic side of the third hand, that needs to be aware ofthe context, have some understanding the current task and be able to predict some of the necessary movements or actionsbased on the real world requirements. \n\nIn one thrust of this research we have proposed novel deep learning models that utilize enhanced contextual cues like scene semantics and human actions for trajectory prediction in urban traffic scenes. One of the research goals is to promote human safety in applications such as robotic devices or autonomously driving vehicles. We have explored multi-modal trajectory prediction and robust learning algorithms using simulation-augmentation. These are important directions as they provide better traffic safety if used in diverse environments. To push the boundary even further, we have proposed along-term trajectory prediction dataset and models for joint action and trajectory prediction in urban traffic scenes. In summary, we have answered the key research question of how to build a robust trajectory prediction system with enhanced semantic context understanding for urban traffic scenes.\n\nIn another part of this work, we studied how intelligent robot devices might come to predict events using instructional videos as a proxy for real activities. While instructional videos usually have concrete steps for events, these future events typically have multiple hypotheses and unpredictable details. We incorporate a memory network to encode the video into a compressed set of memories contrastive analysis to explicitly model the temporal relations across events. Our experiments show the effectiveness of our model on the YouCookII dataset where it can efficiently predict the next possible events and outperform existing baselines work by a large margin.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 08/23/2021\n\n\t\t\t\t\tSubmitted by: Alexander G Hauptmann"
 }
}