{
 "awd_id": "1562098",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Text-to-Image Reference Resolution for Image Understanding and Manipulation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2016-06-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 275000.0,
 "awd_amount": 275000.0,
 "awd_min_amd_letter_date": "2016-03-23",
 "awd_max_amd_letter_date": "2020-05-08",
 "awd_abstract_narration": "This project develops new technologies at the interface of computer vision and natural language processing to understand text-to-image relationships. For example, given a captioned image, the project develops techniques which determine which words (e.g. \"woman talking on phone\", \"The farther vehicle\") correspond to which image parts. From robotics to human-computer interaction, there are numerous real-world tasks that benefit from practical systems to identify objects in scenes based on language and understand language based on visual context. In particular, the project develops the first language-based image authoring tool which allows users to edit or synthesize realistic imagery using only natural language (e.g. \"delete the garbage truck from this photo\" or \"make an image with three boys chasing a shaggy dog\"). Beyond the immediate impact of creating new ways for users to access and author digital images, the broader impacts of this work include three focus areas: the development of new benchmarks for the vision and language communities, outreach and undergraduate research, and leadership in promoting diversity. \r\n\r\nAt the core of the project are new techniques for large-scale text-to-image reference resolution (TIRR) that enable systems to automatically identify the image regions that depict entities described in natural language sentences or commands. These techniques advance image interpretation by enabling systems to perform partial matching between images and sentences, referring expression understanding, and image-based question answering. They also advance image manipulation by enabling systems that can synthesize images starting from a textual description, or modify images based on natural language commands. The main technical contributions of the project are:  (1) benchmark datasets for TIRR with comprehensive large-scale gold standard annotations that will make TIRR a standard task for recognition; (2) principled new representations for text-to-image annotations that expose the compositional nature of language using the formalism of the denotation graph; (3) new models for TIRR that perform an explicit alignment (grounding) of words and phrases to image regions guided by the structure of the denotation graph; (4) applications of TIRR methods to referring expression understanding and visual question answering; and (5) applications of TIRR to image creation and manipulation based on natural language input.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Tamara",
   "pi_last_name": "Berg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tamara Berg",
   "pi_email_addr": "tlberg@cs.unc.edu",
   "nsf_id": "000519059",
   "pi_start_date": "2016-03-23",
   "pi_end_date": "2019-12-20"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mohit",
   "pi_last_name": "Bansal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mohit Bansal",
   "pi_email_addr": "mbansal@cs.unc.edu",
   "nsf_id": "000689943",
   "pi_start_date": "2019-12-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Chapel Hill",
  "perf_str_addr": "201 S Columbia St",
  "perf_city_name": "Chapel Hill",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275993175",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 75950.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 199050.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-fb020c4b-7fff-231e-46bb-ae477cebb6e0\"> </span></p>\n<p dir=\"ltr\"><span>Understanding the interplay between vision (image or video) and language is essential for general-purpose AI systems. From image search and video search in online platforms to multimodal conversational agents like robots, there are many research areas that are highly concerned with the processing of such visual and textual multimodal information and with the understanding of text-to-image relationships, i.e., large-scale text-to-image reference resolution (TIRR) and large-scale text-to-video reference resolution (TVRR).</span></p>\n<p dir=\"ltr\"><span>A number of research projects have been initiated by the PI to address the challenges in the broader domain of vision and language, for both images and videos. Specific projects include: 1) Developing better measures of connecting referring phrases with referred objects in images by incorporating context information, 2) Studying an approach that localizes objects and people in images with an automatic categorization/modularization of the referring phrases, etc, 3) Collecting a dataset with spatio-temporally grounded questions, 4) Collecting dataset within questions based on embodied simulated environments, 5) Developing a system that generates coherent multi-sentence descriptions for a video, 6) Building a system that automatically retrieves relevant clips from a large corpus of videos, as well as curating a dataset for evaluating this system, 7) Collecting a dataset for predicting (by selecting) possible future event following a video and dialogue context.</span></p>\n<p dir=\"ltr\"><span>Concrete outcomes of this grant include collecting multiple new large-scale datasets: images with referring phrases and their referred objects (RefCOCO, RefCOCO+), question-answer pairs based on the frames in the embodied environment (MT-EQA), question-answer pairs that are grounded in both spatial and temporal domains of the videos (TVQA, TVQA+), aligned query and video pairs with multilinguality (TVR, mTVR), future event text descriptions along with their corresponding videos (VLEP). In addition, this grant produced multiple publicly released codebases, and over 10 publications in top conferences and journals in computer vision, natural language processing, and machine learning. Graduate students involved in this grant benefited by developing skills related to modeling, large-scale training, and data processing. The resulting publications and demos are also presented at conferences for academic researchers from all over the world, at outreach events for high school and middle school students. They are also integrated into the graduate courses at the University of North Carolina Chapel Hill, including topics like referring expression comprehension, video question answering, video retrieval.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/01/2021<br>\n\t\t\t\t\tModified by: Mohit&nbsp;Bansal</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nUnderstanding the interplay between vision (image or video) and language is essential for general-purpose AI systems. From image search and video search in online platforms to multimodal conversational agents like robots, there are many research areas that are highly concerned with the processing of such visual and textual multimodal information and with the understanding of text-to-image relationships, i.e., large-scale text-to-image reference resolution (TIRR) and large-scale text-to-video reference resolution (TVRR).\nA number of research projects have been initiated by the PI to address the challenges in the broader domain of vision and language, for both images and videos. Specific projects include: 1) Developing better measures of connecting referring phrases with referred objects in images by incorporating context information, 2) Studying an approach that localizes objects and people in images with an automatic categorization/modularization of the referring phrases, etc, 3) Collecting a dataset with spatio-temporally grounded questions, 4) Collecting dataset within questions based on embodied simulated environments, 5) Developing a system that generates coherent multi-sentence descriptions for a video, 6) Building a system that automatically retrieves relevant clips from a large corpus of videos, as well as curating a dataset for evaluating this system, 7) Collecting a dataset for predicting (by selecting) possible future event following a video and dialogue context.\nConcrete outcomes of this grant include collecting multiple new large-scale datasets: images with referring phrases and their referred objects (RefCOCO, RefCOCO+), question-answer pairs based on the frames in the embodied environment (MT-EQA), question-answer pairs that are grounded in both spatial and temporal domains of the videos (TVQA, TVQA+), aligned query and video pairs with multilinguality (TVR, mTVR), future event text descriptions along with their corresponding videos (VLEP). In addition, this grant produced multiple publicly released codebases, and over 10 publications in top conferences and journals in computer vision, natural language processing, and machine learning. Graduate students involved in this grant benefited by developing skills related to modeling, large-scale training, and data processing. The resulting publications and demos are also presented at conferences for academic researchers from all over the world, at outreach events for high school and middle school students. They are also integrated into the graduate courses at the University of North Carolina Chapel Hill, including topics like referring expression comprehension, video question answering, video retrieval.\n\n \n\n\t\t\t\t\tLast Modified: 10/01/2021\n\n\t\t\t\t\tSubmitted by: Mohit Bansal"
 }
}