{
 "awd_id": "1564080",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Medium: Leveraging Human Interaction to Efficiently Learn and Use Multimodal Object Affordances",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2016-06-15",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 1199831.0,
 "awd_amount": 1214231.0,
 "awd_min_amd_letter_date": "2016-06-03",
 "awd_max_amd_letter_date": "2020-05-28",
 "awd_abstract_narration": "The goal of this research is to enable robots to effectively identify, reason about and predict the affordances of common objects found in everyday human environments.  When a robot enters a new environment, it should not need to learn all domain knowledge from scratch.  Instead, it should be able to leverage general commonsense knowledge about the objects it sees, as well as domain-specific knowledge it acquires through situated interaction, to reason effectively about the attributes and affordances of objects in the surrounding environment.  The PIs' ultimate objective is to make robots more accessible to everyday people.  Project outcomes will contribute research infrastructure and novel data sources for the research community, as well as create an opportunity for broadening participation and STEM educational outreach.  Undergraduate and graduate education will be impacted because the research will supplement the material and projects covered in the PIs' AI, robotics and HRI courses.  The PIs have a track record of including undergraduate research assistants in their labs, and PI Thomaz serves as faculty advisor to the undergraduate AI Club.  Both PIs have an extensive history of mentoring and promoting women in science and technology.  And they will continue their tradition of open source software development in this project; all data deriving from this research will be made publicly available.\r\n\r\nThis project encompasses an end-to-end research agenda that explores how a domain-specific affordance knowledge base, which the PIs call a Situated Affordance Network (SAN), can be represented, acquired, and then used for reasoning about complex tasks.\r\n\r\n* SAN Representation: The PIs will use Markov logic networks to establish the SAN knowledge representation, which relates physical object properties to object attributes and affordances.  This representation will serve as the unifying foundation for the remainder of the work.\r\n* SAN construction from semantic knowledge sources: The PIs will develop automated techniques for leveraging existing, general purpose semantic knowledge resources to construct a domain-specific SAN based on object and location observations made by the robot.  The outcome will be a Markov logic network that represents abstract conceptual knowledge about the robot's environment, including categorical labels, object attributes and affordances.\r\n* SAN refinement through situated interaction: Next, the PIs will develop techniques for physically grounding the SAN's abstract affordance concepts in the environment in which the robot exists.   They will develop techniques for learning specific representations of objects, locations, attributes, and the controllers needed to achieve affordances through situated interaction with the environment and with the human user.\r\n* Affordance reasoning using SAN: In the final research thrust, the PIs will develop algorithmic techniques that leverage the unified SAN representation to enable the robot to perform high level task planning, adapt to changes in the environment and generalize domain-independent knowledge across multiple contexts.\r\n\r\nAt the completion of this work, a robot will be able to enter a novel environment, and 1) use objects that it recognizes in the scene to initialize a domain-specific SAN that contains abstract knowledge about the attributes and affordances of objects in the surrounding environment, 2) incrementally refine the resulting SAN through exploration of the environment and interaction with a human, and 3) leverage the resulting representation to perform complex tasks in the environment, including prediction of the affordances of novel objects, grounding of abstract task plans, and performing plan repair.  The main contribution of this research is not the specific SAN knowledge base that has been generated for a given domain and object set, but rather the domain-independent method by which a robot can construct a SAN for any new environment.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sonia",
   "pi_last_name": "Chernova",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sonia Chernova",
   "pi_email_addr": "chernova@cc.gatech.edu",
   "nsf_id": "000083882",
   "pi_start_date": "2016-06-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Andrea",
   "pi_last_name": "Thomaz",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Andrea L Thomaz",
   "pi_email_addr": "athomaz@ece.utexas.edu",
   "nsf_id": "000082310",
   "pi_start_date": "2016-06-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Ave., NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 1199831.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 14400.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This grant contributed to the completion of the following three research efforts focused on learning, modeling and utilizing multimodal object affordances in robotic systems:</p>\n<p><strong>Using Multi-Relational Embeddings as Knowledge Graph Representations for Robotics Applications</strong>: User demonstrations of robot tasks in everyday environments, such as households, can be brittle due in part to the dynamic, diverse, and complex properties of those environments. Humans can find solutions in ambiguous or unfamiliar situations by using a wealth of common-sense knowledge about their domains to make informed generalizations. For example, likely locations for food in a novel household. This project examined the use of multi-relational embeddings as knowledge graph representations within the context of robust task execution and develops methods to explain the inferences of and sequentially train multi-relational embeddings. This project contributed: (i) a survey of knowledge graph representations that model semantic domain knowledge in robotics, (ii) the development and evaluation of our knowledge graph representation based on multi-relational embeddings, (iii) the integration of our knowledge graph representation into a robot architecture to improve robust task execution, (iv) the development and evaluation of methods to sequentially update multi-relational embeddings, and (v) the development and evaluation of an inference reconciliation framework for multi-relational embeddings.<br /><br /><br /><strong>Grounded Semantic Reasoning for Robotic Interaction with Real-World Objects: </strong>Robots are increasingly transitioning from specialized, single-task machines to general-purpose systems that operate in unstructured environments, such as homes, offices, and warehouses. In these real-world domains, robots need to manipulate novel objects while adapting to changes in environments and goals. Semantic knowledge, which concisely describes target domains with symbols, can potentially reveal the meaningful patterns shared between problems and environments. However, existing robots are yet to effectively reason about semantic data encoding complex relational knowledge. This project developed semantic reasoning frameworks capable of modeling complex semantic knowledge grounded in robot perception and action. Specifically, this project made the following contributions: (1) a survey providing a unified view for the diversity of works in the field; (2) a method for predicting missing relations in large-scale knowledge graphs by leveraging type hierarchies of entities, effectively avoiding ambiguity while maintaining generalization of multi-hop reasoning patterns; (3) a method for predicting unknown properties of objects in various environmental contexts, outperforming prior knowledge graph and statistical relational learning methods due to the use of n-ary relations for modeling object properties; (4) a method for purposeful robotic grasping that accounts for a broad range of contexts (including object visual affordance, material, state, and task constraint), outperforming existing approaches in novel contexts and for unknown objects; (5) a systematic investigation into the generalization of task-oriented grasping that includes a benchmark dataset of 250k grasps, and a novel graph neural network that incorporates semantic relations into end-to-end learning of 6-DoF grasps; (6) a method for rearranging novel objects into semantically meaningful spatial structures based on high-level language instructions, more effectively capturing multi-object spatial constraints than existing pairwise spatial representations; (7) a novel planning-inspired approach that iteratively optimizes placements of partially observed objects subject to both physical constraints and semantic constraints inferred from language instructions.<br /><br /><strong>Robogyver: Autonomous Tool Macgyvering for Inventive Problem Solving</strong>: Robots that are situated in the real world are often faced with unforeseen situations that require them to adapt and improvise to be more useful. Particularly in the context of using tools, there may be situations where a robot does not have access to the tools it needs for completing a task. While humans show remarkable improvisation capabilities, similar skills are beyond the scope of robots today. In order to address these scenarios, a resourceful robot should be able to inventively use whatever objects are available to it, in order to replace the missing tool. We refer to this process as &ldquo;tool macgyvering&rdquo;. Tool macgyvering can be achieved by either substituting the missing tool with an object (tool substitution) or constructing a replacement tool by combining multiple objects (tool construction). This project examined the problem of tool macgyvering, to enable a robot to effectively use available objects to make up for missing tools that are necessary for completing a task. Specifically, this project contributed: (1) a formalization of three levels of tool macgyvering that highlights the levels of complexity involved in tool macgyvering problems; (2) novel algorithms for tool construction through shape, material and attachment reasoning, where attachment refers to the different ways in which objects can be combined; (3) a novel algorithm for tool substitution using shape and material reasoning; (4) a novel framework that performs tool macgyvering through arbitration of substitution and construction, to enable a robot to effectively decide the better of the two solutions for completing the task; and (5) a novel algorithm to perform tool macgyvering in task planning, to enable a robot to leverage existing planning algorithms to perform tool macgyvering efficiently.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/21/2023<br>\n\t\t\t\t\tModified by: Sonia&nbsp;Chernova</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis grant contributed to the completion of the following three research efforts focused on learning, modeling and utilizing multimodal object affordances in robotic systems:\n\nUsing Multi-Relational Embeddings as Knowledge Graph Representations for Robotics Applications: User demonstrations of robot tasks in everyday environments, such as households, can be brittle due in part to the dynamic, diverse, and complex properties of those environments. Humans can find solutions in ambiguous or unfamiliar situations by using a wealth of common-sense knowledge about their domains to make informed generalizations. For example, likely locations for food in a novel household. This project examined the use of multi-relational embeddings as knowledge graph representations within the context of robust task execution and develops methods to explain the inferences of and sequentially train multi-relational embeddings. This project contributed: (i) a survey of knowledge graph representations that model semantic domain knowledge in robotics, (ii) the development and evaluation of our knowledge graph representation based on multi-relational embeddings, (iii) the integration of our knowledge graph representation into a robot architecture to improve robust task execution, (iv) the development and evaluation of methods to sequentially update multi-relational embeddings, and (v) the development and evaluation of an inference reconciliation framework for multi-relational embeddings.\n\n\nGrounded Semantic Reasoning for Robotic Interaction with Real-World Objects: Robots are increasingly transitioning from specialized, single-task machines to general-purpose systems that operate in unstructured environments, such as homes, offices, and warehouses. In these real-world domains, robots need to manipulate novel objects while adapting to changes in environments and goals. Semantic knowledge, which concisely describes target domains with symbols, can potentially reveal the meaningful patterns shared between problems and environments. However, existing robots are yet to effectively reason about semantic data encoding complex relational knowledge. This project developed semantic reasoning frameworks capable of modeling complex semantic knowledge grounded in robot perception and action. Specifically, this project made the following contributions: (1) a survey providing a unified view for the diversity of works in the field; (2) a method for predicting missing relations in large-scale knowledge graphs by leveraging type hierarchies of entities, effectively avoiding ambiguity while maintaining generalization of multi-hop reasoning patterns; (3) a method for predicting unknown properties of objects in various environmental contexts, outperforming prior knowledge graph and statistical relational learning methods due to the use of n-ary relations for modeling object properties; (4) a method for purposeful robotic grasping that accounts for a broad range of contexts (including object visual affordance, material, state, and task constraint), outperforming existing approaches in novel contexts and for unknown objects; (5) a systematic investigation into the generalization of task-oriented grasping that includes a benchmark dataset of 250k grasps, and a novel graph neural network that incorporates semantic relations into end-to-end learning of 6-DoF grasps; (6) a method for rearranging novel objects into semantically meaningful spatial structures based on high-level language instructions, more effectively capturing multi-object spatial constraints than existing pairwise spatial representations; (7) a novel planning-inspired approach that iteratively optimizes placements of partially observed objects subject to both physical constraints and semantic constraints inferred from language instructions.\n\nRobogyver: Autonomous Tool Macgyvering for Inventive Problem Solving: Robots that are situated in the real world are often faced with unforeseen situations that require them to adapt and improvise to be more useful. Particularly in the context of using tools, there may be situations where a robot does not have access to the tools it needs for completing a task. While humans show remarkable improvisation capabilities, similar skills are beyond the scope of robots today. In order to address these scenarios, a resourceful robot should be able to inventively use whatever objects are available to it, in order to replace the missing tool. We refer to this process as \"tool macgyvering\". Tool macgyvering can be achieved by either substituting the missing tool with an object (tool substitution) or constructing a replacement tool by combining multiple objects (tool construction). This project examined the problem of tool macgyvering, to enable a robot to effectively use available objects to make up for missing tools that are necessary for completing a task. Specifically, this project contributed: (1) a formalization of three levels of tool macgyvering that highlights the levels of complexity involved in tool macgyvering problems; (2) novel algorithms for tool construction through shape, material and attachment reasoning, where attachment refers to the different ways in which objects can be combined; (3) a novel algorithm for tool substitution using shape and material reasoning; (4) a novel framework that performs tool macgyvering through arbitration of substitution and construction, to enable a robot to effectively decide the better of the two solutions for completing the task; and (5) a novel algorithm to perform tool macgyvering in task planning, to enable a robot to leverage existing planning algorithms to perform tool macgyvering efficiently.\n\n\t\t\t\t\tLast Modified: 03/21/2023\n\n\t\t\t\t\tSubmitted by: Sonia Chernova"
 }
}