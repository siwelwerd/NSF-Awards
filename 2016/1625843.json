{
 "awd_id": "1625843",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI: Acquisition of mobile robots to support indoor navigation and online 3D object detection",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2016-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 100450.0,
 "awd_amount": 112450.0,
 "awd_min_amd_letter_date": "2016-09-16",
 "awd_max_amd_letter_date": "2017-05-12",
 "awd_abstract_narration": "Mobile intelligent robots are used today in the domain of assisted living and health care, as they can accompany people, assist them, and be used to effectively support various applications, such as navigation. This project called SemaFORR, uses two robots equipped with multiple real-world sensors to provide cognitively plausible solution when traditional navigation assumptions cannot be used.  The robots are equipped to provide an autonomous navigation and real time classification from 3D range data. The project highlights the role of cognition in navigation, enhancing the image-understanding capabilities of mobile robots by pushing the state of the art in classification algorithms. \r\n\r\nAddressing fundamental issues in multi-method search, inference learning, representation, online quickest detection and classification that contribute in solving challenging problems, the project involves increasing the image-understanding capabilities of mobile robots. This goal is to be accomplished by developing real time classification and object detection to help computers and people solve challenging problems. Operating under the open-source standard ROS (Robot Operating System) serves as the base on which to build and test algorithms to: 1. Enhance navigation with cognitive plausible decision making by integrating a novel architectural method that enables the robot to learn a dynamic spatial model that diagrams special affordances; 2. Enhance online real-time object detection and classification by performing a stage-based 3D classification; and 3. Optimize algorithms by balancing the tradeoff between accuracy and speed of object classification.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ioannis",
   "pi_last_name": "Stamos",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ioannis Stamos",
   "pi_email_addr": "istamos@hunter.cuny.edu",
   "nsf_id": "000166434",
   "pi_start_date": "2016-09-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Susan",
   "pi_last_name": "Epstein",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Susan L Epstein",
   "pi_email_addr": "susan.epstein@hunter.cuny.edu",
   "nsf_id": "000173560",
   "pi_start_date": "2016-09-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Olympia",
   "pi_last_name": "Hadjiliadis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Olympia Hadjiliadis",
   "pi_email_addr": "olympia.hadjiliadis@gmail.com",
   "nsf_id": "000079351",
   "pi_start_date": "2016-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "CUNY Hunter College",
  "inst_street_address": "695 PARK AVE",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2127724020",
  "inst_zip_code": "100655024",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NY12",
  "org_lgl_bus_name": "RESEARCH FOUNDATION OF THE CITY UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "EK93EZLLBSC4"
 },
 "perf_inst": {
  "perf_inst_name": "CUNY Hunter College",
  "perf_str_addr": "695 Park Avenue",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100655024",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1189",
   "pgm_ref_txt": "MAJOR RESEARCH INSTRUMENTATION"
  },
  {
   "pgm_ref_code": "7484",
   "pgm_ref_txt": "IIS SPECIAL PROJECTS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 100450.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 12000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We have done research in three main aspect of a robotics system:</p>\n<p>a) We developed a novel object detection system from both RGB-D (i.e. color and depth) as well as Depth-only images. That system is crucial since it gives the ability to the robot to recognize common objects. It also allows the robot to know where in 3D world the objects are, and what it is their orientation. That allows the robot to navigate towards and object of interest, as well as grasp a particular object (for example go towards a table and locate a glass on top of it with the purpose of grasping it). One of our main consideration was to make the system, not only accurate, but also efficient.&nbsp;</p>\n<p>b) We developed an autonomous navigation system to be used in a large, complex indoor spaces that do not include a detailed map. We are able to perform robust hierarchical path planning. Navigation is a crucial aspect of any robotic system. Being able to do it in a largely unfamiliar environment is crucial. We also developed path planning algorithms that take into account position of crowds in the world.</p>\n<p>c) We developed methods based on mathematical statistics, that are able to disambiguate signal from noise. This is critical since some of our sensor measurements maybe noisy. This work is based on a robust estimators and can be used in other problems that do not involve robotic sensors.</p>\n<p>We integrated the object detection and aspects of the navigation system on the Fetch robotic platform that we purchased as part of the MRI grant. We performed experiments in which our robot navigates an indoor scene, detects 3D objects and moves towards objects of interest. Some of our navigation algorithms are performed in simulated environments.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/29/2022<br>\n\t\t\t\t\tModified by: Ioannis&nbsp;Stamos</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1625843/1625843_10462918_1643488232521_sensors-21-01213-g001--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1625843/1625843_10462918_1643488232521_sensors-21-01213-g001--rgov-800width.jpg\" title=\"Overview of 3D Classification.\"><img src=\"/por/images/Reports/POR/2022/1625843/1625843_10462918_1643488232521_sensors-21-01213-g001--rgov-66x44.jpg\" alt=\"Overview of 3D Classification.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Upper left: RGB image and detected 2D bounding boxes. Upper right: Depth image and detected 2D bounding boxes. Bottom: The final 3D detected objects.(SUN RGB-D Dataset)</div>\n<div class=\"imageCredit\">Xiaoke Shen and Ioannis Stamos</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Ioannis&nbsp;Stamos</div>\n<div class=\"imageTitle\">Overview of 3D Classification.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1625843/1625843_10462918_1643486960507_sensors-21-01213-g013--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1625843/1625843_10462918_1643486960507_sensors-21-01213-g013--rgov-800width.jpg\" title=\"3D Object Classification and Detection\"><img src=\"/por/images/Reports/POR/2022/1625843/1625843_10462918_1643486960507_sensors-21-01213-g013--rgov-66x44.jpg\" alt=\"3D Object Classification and Detection\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Visualizations of 2D and 3D object detection results. This visualization contains four RGB images and four Depth images. Next to each image we show the corresponding 3D detection results: light green ones are the 3D ground truth boxes and orange-color boxes are our predictions. SUN RGB-D dataset.</div>\n<div class=\"imageCredit\">Xiaoke Shen and Ioannis Stamos</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Ioannis&nbsp;Stamos</div>\n<div class=\"imageTitle\">3D Object Classification and Detection</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nWe have done research in three main aspect of a robotics system:\n\na) We developed a novel object detection system from both RGB-D (i.e. color and depth) as well as Depth-only images. That system is crucial since it gives the ability to the robot to recognize common objects. It also allows the robot to know where in 3D world the objects are, and what it is their orientation. That allows the robot to navigate towards and object of interest, as well as grasp a particular object (for example go towards a table and locate a glass on top of it with the purpose of grasping it). One of our main consideration was to make the system, not only accurate, but also efficient. \n\nb) We developed an autonomous navigation system to be used in a large, complex indoor spaces that do not include a detailed map. We are able to perform robust hierarchical path planning. Navigation is a crucial aspect of any robotic system. Being able to do it in a largely unfamiliar environment is crucial. We also developed path planning algorithms that take into account position of crowds in the world.\n\nc) We developed methods based on mathematical statistics, that are able to disambiguate signal from noise. This is critical since some of our sensor measurements maybe noisy. This work is based on a robust estimators and can be used in other problems that do not involve robotic sensors.\n\nWe integrated the object detection and aspects of the navigation system on the Fetch robotic platform that we purchased as part of the MRI grant. We performed experiments in which our robot navigates an indoor scene, detects 3D objects and moves towards objects of interest. Some of our navigation algorithms are performed in simulated environments.\n\n \n\n\t\t\t\t\tLast Modified: 01/29/2022\n\n\t\t\t\t\tSubmitted by: Ioannis Stamos"
 }
}