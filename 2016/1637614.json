{
 "awd_id": "1637614",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: A Framework for Hierarchical, Probabilistic Planning and Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Miller",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 542682.0,
 "awd_amount": 542682.0,
 "awd_min_amd_letter_date": "2016-08-17",
 "awd_max_amd_letter_date": "2016-08-17",
 "awd_abstract_narration": "This project is an effort to create a unified framework for solving very large problems with uncertain states and actions, such as manipulator robots acting in real-world environments.  The results may have especially great promise for assistive technologies, including autonomous robots that can be used by elderly and disabled populations to aid them in their daily activities.  The proposed integrated framework will represent, apply, and learn hierarchical domain knowledge, and will include the ability to transfer knowledge from simpler problems to more complex ones. The research will enable autonomous agents to develop a structured representation of complex domains based on experience. The agents will use learned representations to interpret natural language commands for both low-level and high-level requests.  \r\n\r\nThe technical focus is enabling tractable planning in large, uncertain domains by generating and leveraging probabilistic domain knowledge at multiple levels of abstraction. Agents will autonomously create layered representations in which the layers build on one another to produce complex behaviors. Agents will learn to perform useful behaviors, such as navigating using low-level sensor feedback or assembling complex objects such as a bridge or a table.  The key technical contributions will be methods for (1) planning in large state/action spaces using the abstract object-oriented Markov decision process (AMDP) model, a new formalism for representing probabilistic domain knowledge at multiple levels of abstraction; (2) learning hierarchical task knowledge in the form of AMDPs; and (3) interpreting natural language commands at multiple levels of abstraction by mapping to the learned hierarchical structure. The formalism will be demonstrated and validated in several domains, including a simulated \"cleanup\" toy domain, challenging and complex video games, and a robot manipulation task.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stefanie",
   "pi_last_name": "Tellex",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefanie Tellex",
   "pi_email_addr": "stefie10@cs.brown.edu",
   "nsf_id": "000651585",
   "pi_start_date": "2016-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Littman",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Littman",
   "pi_email_addr": "mlittman@cs.brown.edu",
   "nsf_id": "000210482",
   "pi_start_date": "2016-08-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University, Computer Science Dept.",
  "perf_str_addr": "115 Waterman Street",
  "perf_city_name": "Providence",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 542682.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This program resulted in a body of work that created abstraction and language understanding.&nbsp; We created a new framework for reasoning in domains with uncertain transition dynamics at different levels of abstraction.&nbsp; The robot could plan to perform low level tasks such as \"move north\" or high level tasks such as \"move the red block to the green room.\"&nbsp; &nbsp; This framework focused on planning, so the robot's goals were specified as a symbolic expression.&nbsp; We showed that our abstraction approach could plan more quickly and accurately than baselines.&nbsp; &nbsp;This work was published at the ICAPs conference in 2017.</p>\n<p>Next we created a system that learned to map from English sentences to a symbolic expression at different levels of abstraction.&nbsp; &nbsp;This framework enabled the robot to map between English and a symbolic expression representing a goal.&nbsp; Then using the previous framework we created as part of this award, the robot could efficiently find a plan to satisfy the natural language expression.&nbsp; As a result, the robot could map from an English sentence like \"MOve the red block to the green room\" and behavior that satisfies this command.&nbsp; &nbsp;This work was published at RSS in 2017.</p>\n<p>\n<div>To better incorporate symbolically  represented tasks into a reinforcement-learning framework, we developed a  representation we call \"GLTL\" or \"geometric LTL\". It generalizes the  idea of temporal discounting in RL to temporal tasks in linear temporal  logic (LTL). As a result, what would ordinarily be tasks that can only  be evaluated over infinitely long trajectories turns into tasks that can  be completed over unbounded but expected finite-length trajectories. We  used this representation to solve tasks across a wide variety of  environments from simple gridworlds, to Atari video games, to robot  navigation.&nbsp; We discovered a somewhat unexpected connection between abstraction  and the successor feature representation. Roughly speaking, a state  representation that supports predictions of future state features can be  used as an effective abstraction for decision making and transfer.\n<div>We examined the construction of state  abstractions in the lifelong RL setting where the idea is that each  environment that learner is exposed to provides some evidence for what  state abstractions it should be considering. Since state abstractions  are expensive to discover, the lifelong setting is a good fit as it  provides the opportunity to amortize this cost across multiple sessions  of learning.</div>\n</div>\n</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/25/2020<br>\n\t\t\t\t\tModified by: Stefanie&nbsp;Tellex</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis program resulted in a body of work that created abstraction and language understanding.  We created a new framework for reasoning in domains with uncertain transition dynamics at different levels of abstraction.  The robot could plan to perform low level tasks such as \"move north\" or high level tasks such as \"move the red block to the green room.\"    This framework focused on planning, so the robot's goals were specified as a symbolic expression.  We showed that our abstraction approach could plan more quickly and accurately than baselines.   This work was published at the ICAPs conference in 2017.\n\nNext we created a system that learned to map from English sentences to a symbolic expression at different levels of abstraction.   This framework enabled the robot to map between English and a symbolic expression representing a goal.  Then using the previous framework we created as part of this award, the robot could efficiently find a plan to satisfy the natural language expression.  As a result, the robot could map from an English sentence like \"MOve the red block to the green room\" and behavior that satisfies this command.   This work was published at RSS in 2017.\n\n\nTo better incorporate symbolically  represented tasks into a reinforcement-learning framework, we developed a  representation we call \"GLTL\" or \"geometric LTL\". It generalizes the  idea of temporal discounting in RL to temporal tasks in linear temporal  logic (LTL). As a result, what would ordinarily be tasks that can only  be evaluated over infinitely long trajectories turns into tasks that can  be completed over unbounded but expected finite-length trajectories. We  used this representation to solve tasks across a wide variety of  environments from simple gridworlds, to Atari video games, to robot  navigation.  We discovered a somewhat unexpected connection between abstraction  and the successor feature representation. Roughly speaking, a state  representation that supports predictions of future state features can be  used as an effective abstraction for decision making and transfer.\nWe examined the construction of state  abstractions in the lifelong RL setting where the idea is that each  environment that learner is exposed to provides some evidence for what  state abstractions it should be considering. Since state abstractions  are expensive to discover, the lifelong setting is a good fit as it  provides the opportunity to amortize this cost across multiple sessions  of learning.\n\n\n\n\t\t\t\t\tLast Modified: 04/25/2020\n\n\t\t\t\t\tSubmitted by: Stefanie Tellex"
 }
}