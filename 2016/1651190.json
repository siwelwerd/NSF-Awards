{
 "awd_id": "1651190",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Linguistic Event Extraction and Integration (LEXI): A New Approach to Speech Analysis",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "D.  Langendoen",
 "awd_eff_date": "2016-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 214449.0,
 "awd_amount": 238449.0,
 "awd_min_amd_letter_date": "2016-08-18",
 "awd_max_amd_letter_date": "2018-04-13",
 "awd_abstract_narration": "This exploratory project develops a new system for speech signal analysis that can be used to improve automatic speech recognition (ASR) systems, and provide a testable model of human speech perception. The system is based on finding important events in the speech signal, i.e. 'acoustic edges' where the signal changes suddenly because the mouth closes or opens during the formation of a consonant (like /p/ or /s/), or a vowel (like /a/ or /u/). These abrupt changes, called Landmarks, are especially informative, because they (and the parts of the signal near them) are richly informative about the speaker's intended words and their sounds. Focusing on these events results in greater computational efficiency, by identifying the linguistically relevant information in the speech signal, rather than measuring every part of the signal. This focus on individual cues to speech sounds also means that the system can deal with non-typical speech produced by children, older people, speakers with foreign accents, or those with clinical speech disabilities. As a result, this system will bring the benefits of ASR to speakers who are not well served by current recognition systems, making it possible for more people to use cell phones, tablets and laptops. While existing systems work well for typical speakers by using statistical analysis of large samples of typical speech, they leave many people underserved. The Landmark-based system will also provide a tool for testing whether human speech recognition depends on finding the individual cues to the sounds of words, even when those cues are very different in different contexts, and so can lead to the development of a new model of human speech perception.\r\n\r\nThe system works by extracting speech-related measurements from the signal, such as fundamental frequency, formant frequencies, spectral band energies and their derivatives, and interpreting these measures as acoustic cues for distinctive features. Innovative aspects of the system include the use of Landmarks, which are the most robust of the acoustic feature cues and are related to articulatory manner features. Once the landmark acoustic cues are found, other acoustic cues related to place and voicing features, and to prosodic structure, can also be found. The extraction of distinctive features and prosodic structure provides the first abstract linguistic units that can be extracted from the physical continuous signal, and this information is used to identify words, and to construct a representation of the entire utterance. To develop and evaluate the performance of this innovative system, speech databases consisting of isolated vowel-consonant-vowel sequences, read continuous speech, read radio-style speech, and spontaneous speech will be hand-labeled with Landmarks and other acoustic cues. Results of this basic speech research project will support the development of new approaches to ASR, will provide a testable computational model of human speech production, and will produce material suitable for development of a tutorial to train students in engineering, linguistics and cognitive science to label acoustic feature cues.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stefanie",
   "pi_last_name": "Shattuck-Hufnagel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefanie Shattuck-Hufnagel",
   "pi_email_addr": "sshuf@mit.edu",
   "nsf_id": "000410230",
   "pi_start_date": "2016-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jeung-Yoon",
   "pi_last_name": "Choi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jeung-Yoon Choi",
   "pi_email_addr": "jyechoi@mit.edu",
   "nsf_id": "000641298",
   "pi_start_date": "2016-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "116E",
   "pgm_ref_txt": "RESEARCH EXP FOR UNDERGRADS"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 214449.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project is focused on developing a speech recognition system that more closely models the way human listeners recognize spoken words.&nbsp;&nbsp; Existing automatic speech recognition systems, while effective in many contexts, do not attempt to model human performance, and provide little insight into the speech recognition process as it evolved to operate in human listeners.&nbsp; In contrast, the LEXI system (for&nbsp;<strong>L</strong>inguistic&nbsp;<strong>E</strong>vent E<strong>X</strong>traction and&nbsp;<strong>I</strong>nterpretation) takes advantage of ongoing advances in cognitive science, linguistics and acoustic phonetics indicating that listeners attend to&nbsp;<em>individual acoustic cues&nbsp;</em>to the differences among words and their sounds (Stevens 2002, Journal of the Acoustical Society of America 111, 1872-1891).&nbsp; This approach provides a solution to the perplexing problem of contextual variation in words and sounds, which is systematic (and therefore informative) at the cue level, but is challenging for most existing automatic recognizers (especially for new and unfamiliar words not in the training set).&nbsp; This is because current recognition systems operate at the level of whole speech sounds (phones), and these sounds vary substantially across different contexts, in ways that sometimes destroy information that a phone-based recognizer needs.&nbsp; However, these modifications preserve many residual acoustic cues (as when the word sequence&nbsp;<em>Why don?t you</em>&nbsp;is pronounced something like&nbsp;<em>whynchuh</em>).&nbsp; An understanding of these patterns of variation in terms of the selection/preservation of individual acoustic cues and their values provides a way forward toward modeling human speech recognition as it occurs in the human brain and sensory systems.</p>\n<p>Based on this approach, the project had four major goals, which were met in the following ways:&nbsp;</p>\n<p>1) Integrating the components of the LEXI system.&nbsp; This goal was realized by combining modules for extracting individual feature cues from the speech waveform and interpreting the cues as evidence for the features and phonemes of the intended utterance, into a transparent system which (unlike many current speech recognition system) can be evaluated at each step in the process (see Figure 1).</p>\n<p>2) Hand-labelling the individual acoustic cues in speech samples in a variety of speaking styles.&nbsp; This goal was realized by training more than 20 undergraduate labellers using a recently-developed on-line training tutorial, supplemented by an 8-session lab course, and making use of two in-house-generated software packages (see (4) below).&nbsp; &nbsp;The labellers annotated more than 100,000 cues in speech produced in a range of speech styles, including VCV nonsense words, read words, read sentences and task-directed spontaneous speech,&nbsp;produced by more than 60 speakers, providing training material for the cue-detection modules, and a means for testing hypotheses about constraints on cue modifications in context.&nbsp; &nbsp;</p>\n<p>3) Evaluating the LEXI system, using a unique feature-cue-based metric that provides more detailed information than most current phone- or word-based metrics.&nbsp; Combined with LEXI's transparent design, which permits scrutiny of the performance of each component separately, this evaluation method highlights which components in the system most need improvement, enabling efficient allocation of effort.&nbsp; For example, the Landmark detection algorithms exhibit a range of performance levels, from 80-85% correct for stop-closure and stop-release LMs, to 50-55% correct for nasality, highlighting the likely system-wide payoff for improvements in the nasal detection module.&nbsp; &nbsp;&nbsp;</p>\n<p>4) Developing materials for teaching acoustic-cue-based speech analysis.&nbsp; In addition to the on-line tutorial and the 8-session lab training course, two software packages have been developed 1)&nbsp;to enable&nbsp;more accurate/efficient cue labelling (by e.g. predicting all the cues that a speaker might produce for a given feature, so that the labeller simply needs to mark which ones were actually produced by the speaker of the target utterance) and 2)&nbsp; to facilitate analysis of cue modification patterns (by e.g. aligning labelled cues with predicted cues).&nbsp; These thoroughly-vetted pedagogical tools are now ready for dissemination.&nbsp; &nbsp;&nbsp;</p>\n<p>During the project, several unexpected opportunities arose for applying the cue-based analysis approach to speech from clinical populations.&nbsp; For example, preliminary analyses of speech samples from children with different types of speech processing challenges (arising from e.g. Specific Language Impairment, Autism, Dyslexia) suggest that these different conditions may be characterized by different cue production patterns.&nbsp; Theis observation raises the possibility of understanding more deeply the causes of these developmental difficulties. Similarly, preliminary analyses of the speech of patients with Parkinson?s Disease suggest that their acoustic cue production differs from that of typical age-matched controls at an early stage in the progression of the disease, raising the possibility of reaching a definitive diagnosis and starting treatment earlier than is currently possible, so that the disease is more responsive to clinical intervention.&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;</p>\n<p>In sum, creation of the LEXI system and its accompanying cue-labelled databases and cue analysis tools is an important step toward a testable working model of the human speech perception system, which will enable the development of more effective clinical intervention methods, and of techniques for teaching a second language that emphasize speaking without a foreign accent.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/08/2020<br>\n\t\t\t\t\tModified by: Stefanie&nbsp;Shattuck-Hufnagel</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1651190/1651190_10451840_1578491730331_cuelabelingprocess--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1651190/1651190_10451840_1578491730331_cuelabelingprocess--rgov-800width.jpg\" title=\"Labeling acoustic cues to distinctive features\"><img src=\"/por/images/Reports/POR/2020/1651190/1651190_10451840_1578491730331_cuelabelingprocess--rgov-66x44.jpg\" alt=\"Labeling acoustic cues to distinctive features\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Canonical acoustic cues can be predicted from a word sequence by finding a phoneme sequence from a dictionary pronunciation. Any modifications to canonically predicted acoustic cues can then be added to produce the final acoustic cue labels for the utterance.</div>\n<div class=\"imageCredit\">Jeung-Yoon Choi</div>\n<div class=\"imageSubmitted\">Jeung-Yoon&nbsp;Choi</div>\n<div class=\"imageTitle\">Labeling acoustic cues to distinctive features</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project is focused on developing a speech recognition system that more closely models the way human listeners recognize spoken words.   Existing automatic speech recognition systems, while effective in many contexts, do not attempt to model human performance, and provide little insight into the speech recognition process as it evolved to operate in human listeners.  In contrast, the LEXI system (for Linguistic Event EXtraction and Interpretation) takes advantage of ongoing advances in cognitive science, linguistics and acoustic phonetics indicating that listeners attend to individual acoustic cues to the differences among words and their sounds (Stevens 2002, Journal of the Acoustical Society of America 111, 1872-1891).  This approach provides a solution to the perplexing problem of contextual variation in words and sounds, which is systematic (and therefore informative) at the cue level, but is challenging for most existing automatic recognizers (especially for new and unfamiliar words not in the training set).  This is because current recognition systems operate at the level of whole speech sounds (phones), and these sounds vary substantially across different contexts, in ways that sometimes destroy information that a phone-based recognizer needs.  However, these modifications preserve many residual acoustic cues (as when the word sequence Why don?t you is pronounced something like whynchuh).  An understanding of these patterns of variation in terms of the selection/preservation of individual acoustic cues and their values provides a way forward toward modeling human speech recognition as it occurs in the human brain and sensory systems.\n\nBased on this approach, the project had four major goals, which were met in the following ways: \n\n1) Integrating the components of the LEXI system.  This goal was realized by combining modules for extracting individual feature cues from the speech waveform and interpreting the cues as evidence for the features and phonemes of the intended utterance, into a transparent system which (unlike many current speech recognition system) can be evaluated at each step in the process (see Figure 1).\n\n2) Hand-labelling the individual acoustic cues in speech samples in a variety of speaking styles.  This goal was realized by training more than 20 undergraduate labellers using a recently-developed on-line training tutorial, supplemented by an 8-session lab course, and making use of two in-house-generated software packages (see (4) below).   The labellers annotated more than 100,000 cues in speech produced in a range of speech styles, including VCV nonsense words, read words, read sentences and task-directed spontaneous speech, produced by more than 60 speakers, providing training material for the cue-detection modules, and a means for testing hypotheses about constraints on cue modifications in context.   \n\n3) Evaluating the LEXI system, using a unique feature-cue-based metric that provides more detailed information than most current phone- or word-based metrics.  Combined with LEXI's transparent design, which permits scrutiny of the performance of each component separately, this evaluation method highlights which components in the system most need improvement, enabling efficient allocation of effort.  For example, the Landmark detection algorithms exhibit a range of performance levels, from 80-85% correct for stop-closure and stop-release LMs, to 50-55% correct for nasality, highlighting the likely system-wide payoff for improvements in the nasal detection module.    \n\n4) Developing materials for teaching acoustic-cue-based speech analysis.  In addition to the on-line tutorial and the 8-session lab training course, two software packages have been developed 1) to enable more accurate/efficient cue labelling (by e.g. predicting all the cues that a speaker might produce for a given feature, so that the labeller simply needs to mark which ones were actually produced by the speaker of the target utterance) and 2)  to facilitate analysis of cue modification patterns (by e.g. aligning labelled cues with predicted cues).  These thoroughly-vetted pedagogical tools are now ready for dissemination.    \n\nDuring the project, several unexpected opportunities arose for applying the cue-based analysis approach to speech from clinical populations.  For example, preliminary analyses of speech samples from children with different types of speech processing challenges (arising from e.g. Specific Language Impairment, Autism, Dyslexia) suggest that these different conditions may be characterized by different cue production patterns.  Theis observation raises the possibility of understanding more deeply the causes of these developmental difficulties. Similarly, preliminary analyses of the speech of patients with Parkinson?s Disease suggest that their acoustic cue production differs from that of typical age-matched controls at an early stage in the progression of the disease, raising the possibility of reaching a definitive diagnosis and starting treatment earlier than is currently possible, so that the disease is more responsive to clinical intervention.                                 \n\nIn sum, creation of the LEXI system and its accompanying cue-labelled databases and cue analysis tools is an important step toward a testable working model of the human speech perception system, which will enable the development of more effective clinical intervention methods, and of techniques for teaching a second language that emphasize speaking without a foreign accent.\n\n\t\t\t\t\tLast Modified: 01/08/2020\n\n\t\t\t\t\tSubmitted by: Stefanie Shattuck-Hufnagel"
 }
}