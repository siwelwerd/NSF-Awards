{
 "awd_id": "1548867",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Providing Automatic Anomaly Prediction and Diagnosis Software as a Service for Cloud Infrastructures",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2016-01-01",
 "awd_exp_date": "2016-12-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 171250.0,
 "awd_min_amd_letter_date": "2015-12-17",
 "awd_max_amd_letter_date": "2016-08-09",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be to greatly improve the robustness and diagnosability of many real world cloud computing infrastructures. The proposed technology will significantly reduce the downtime of production cloud systems, which can attract more users to adopt cloud computing technology and thus benefit the expanding segment of society and the economy that depends on cloud technology. The project will also advance the state of the art of cloud system reliability research by putting research results into real world use.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project will transform system anomaly management for production cloud computing infrastructures. The novelty of the company's solution lies in three unique features: 1) it provides automatic multivariate anomaly detection that can enable high-fidelity anomaly alerts without imposing any configuration burden on the user; 2) it provides early anomaly alerts before big system problems occur; and 3) it provides anomaly diagnosis that can generate hints on why an anomaly occurs. The proposed research will produce novel and practical anomaly prediction and diagnosis solutions that will be validated in real world cloud infrastructures. Specifically, the project consists of two thrusts: 1) online multivariate anomaly prediction that explores new light-weight unsupervised learning algorithms for achieving high-fidelity anomaly alerts and providing time-to-failure estimations; and 2) automatic anomaly diagnosis that can identify possible causes of an anomaly to greatly expedite the anomaly troubleshooting process in the cloud. The company will implement the software products and carry out case studies with partners on real world cloud computing infrastructures.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Xiaohui",
   "pi_last_name": "Gu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaohui Gu",
   "pi_email_addr": "xgu1000@gmail.com",
   "nsf_id": "000677719",
   "pi_start_date": "2015-12-17",
   "pi_end_date": "2016-08-09"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chao",
   "pi_last_name": "Huang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chao Huang",
   "pi_email_addr": "chao@insightfinder.com",
   "nsf_id": "000720005",
   "pi_start_date": "2016-08-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "InsightFinder Inc.",
  "inst_street_address": "539 4TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "BROOKLYN",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "9196001004",
  "inst_zip_code": "112153070",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "INSIGHTFINDER INC.",
  "org_prnt_uei_num": "QAMAVY5MST46",
  "org_uei_num": "ZRJPHK6VBMU5"
 },
 "perf_inst": {
  "perf_inst_name": "Cloud Solutions LLC",
  "perf_str_addr": "805 Transom View Way",
  "perf_city_name": "Cary",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275196221",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "163E",
   "pgm_ref_txt": "SBIR Phase IB"
  },
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "8032",
   "pgm_ref_txt": "Software Services and Applications"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 171250.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">In January 2016, we received an NSF SBIR Phase I grant to study the feasibility of providing anomaly detection and diagnosis Software as a Service (SaaS) for cloud infrastructures. We first developed a scalable and resilient analysis engine to host our anomaly detection and root cause inference algorithms in public clouds and provide web interface for the user to access our anomaly detection and diagnosis services via Internet. We then developed a set of monitoring agents that connect our analysis engine to different metric data sources including Amazon CloudWatch Monitoring APIs, Google Cloud Monitoring APIs, Docker containers, Kubernetes Clusters, Vmware Hypervisors, Splunk, and ELK data repositories. We launched our beta product in March and made our products publicly available in May. We have been been doing Proof-of-Concept (POC) testing with a set of prospective customers since then.</p>\n<p class=\"p1\">Our research aims at answering a set of key commercialization feasibility questions: 1) can our multivariate anomaly detection algorithms raise advance alerts with high accuracy for real world system failures? 2) can we significantly reduce false alarms compared to traditional anomaly detection algorithms? 3) can we provide useful diagnostic hints about the anomaly root cause? and 4) can we provide real-time alerts for large-scale real world distributed applications? We carried out data-driven research study to answer those questions.</p>\n<p class=\"p1\">We evaluated our anomaly detection and diagnosis algorithms using real world failure data sets provided by our POC testing customers. The root causes of those system failures range from hardware failures, human errors, to software bugs, or a combination of those three factors. Those failure data sets are typically quite big consisting of several weeks of monitoring data samples and hundreds of or thousands of metrics. We also compared our algorithms with a set of common alternative approaches such as simple threshold-based alert and clustering (DBScan). Our anomaly detection solution not only achieves 100% detection rate (detecting all true anomalies) but also raises alerts hours or days earlier than customers&rsquo; existing tools by capturing early warning signs. Our false alarm rates are orders of magnitude lower than existing approaches, which can significantly reduce the alert processing cost. Moreover, our faulty metric inference algorithm can provide useful hints on the anomaly root cause, which can potentially&nbsp;reduce the incident triage time from hours or days to minutes. Our tool also detected some production system failures that are missed by existing alert tools.&nbsp;</p>\n<p class=\"p1\">We have also implemented an initial prototype of the system call analysis system that provides deep-dive debugging for applications running inside the public cloud infrastructures. Our system call monitoring agent only imposes less than 1% CPU overhead to the customer&rsquo;s environment. The monitoring agent transmits compressed system call trace to our server only after an anomaly alert is raised. The system call analysis module can estimate the fault impact scope (e.g., global vs local impact) and produce a rank list of root cause related functions. We tested our prototype using 10 real software bugs existed in 4 common open source software systems (Cassandra, Apache, Hadoop, MySQL). The results show that we can accurately estimate the fault impact scope and rank the root cause related functions within top 25 candidate faulty functions out of millions of application functions, which can greatly reduce the debugging time for the application developer.</p>\n<p class=\"p1\">We also tested with a range of distributed applications deployed on production cloud infrastructures. The distributed applications we tested include hundreds of computing nodes. Our system can complete anomaly detection within several seconds and train hundreds of models in parallel within tens of seconds. So we believe that our approach is scalable and practical for large-scale system monitoring and real-time analytics. During our Phase IB project, we also implemented a set of open source monitoring agents to integrate InsightFinder with different types of systems. We also implemented an initial prototype of log event classification and anomaly detection tool.&nbsp;</p>\n<p class=\"p1\">The project provides valuable internship opportunties to three graduate students and one female undergraduate student to obtain software development and data analytics skills. Techniques developed in this project have significant impact on improving the diagnosability and robustness of many real world cloud computing infrastructures. The commercial potential of the cloud anomaly prediction and diagnosis technology is big and has been demonstrated by our intial comericalization success. As the proposing technology increases the robustness of the cloud infrastructure, it allows more users to adopt the cloud computing technology and thus benefit the whole society that depends on the cloud technology. The project also advances the state of the art of the cloud reliability management research by putting the technology into real world use and enhance the technology to address real world challenges.&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/12/2016<br>\n\t\t\t\t\tModified by: Chao&nbsp;Huang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "In January 2016, we received an NSF SBIR Phase I grant to study the feasibility of providing anomaly detection and diagnosis Software as a Service (SaaS) for cloud infrastructures. We first developed a scalable and resilient analysis engine to host our anomaly detection and root cause inference algorithms in public clouds and provide web interface for the user to access our anomaly detection and diagnosis services via Internet. We then developed a set of monitoring agents that connect our analysis engine to different metric data sources including Amazon CloudWatch Monitoring APIs, Google Cloud Monitoring APIs, Docker containers, Kubernetes Clusters, Vmware Hypervisors, Splunk, and ELK data repositories. We launched our beta product in March and made our products publicly available in May. We have been been doing Proof-of-Concept (POC) testing with a set of prospective customers since then.\nOur research aims at answering a set of key commercialization feasibility questions: 1) can our multivariate anomaly detection algorithms raise advance alerts with high accuracy for real world system failures? 2) can we significantly reduce false alarms compared to traditional anomaly detection algorithms? 3) can we provide useful diagnostic hints about the anomaly root cause? and 4) can we provide real-time alerts for large-scale real world distributed applications? We carried out data-driven research study to answer those questions.\nWe evaluated our anomaly detection and diagnosis algorithms using real world failure data sets provided by our POC testing customers. The root causes of those system failures range from hardware failures, human errors, to software bugs, or a combination of those three factors. Those failure data sets are typically quite big consisting of several weeks of monitoring data samples and hundreds of or thousands of metrics. We also compared our algorithms with a set of common alternative approaches such as simple threshold-based alert and clustering (DBScan). Our anomaly detection solution not only achieves 100% detection rate (detecting all true anomalies) but also raises alerts hours or days earlier than customers? existing tools by capturing early warning signs. Our false alarm rates are orders of magnitude lower than existing approaches, which can significantly reduce the alert processing cost. Moreover, our faulty metric inference algorithm can provide useful hints on the anomaly root cause, which can potentially reduce the incident triage time from hours or days to minutes. Our tool also detected some production system failures that are missed by existing alert tools. \nWe have also implemented an initial prototype of the system call analysis system that provides deep-dive debugging for applications running inside the public cloud infrastructures. Our system call monitoring agent only imposes less than 1% CPU overhead to the customer?s environment. The monitoring agent transmits compressed system call trace to our server only after an anomaly alert is raised. The system call analysis module can estimate the fault impact scope (e.g., global vs local impact) and produce a rank list of root cause related functions. We tested our prototype using 10 real software bugs existed in 4 common open source software systems (Cassandra, Apache, Hadoop, MySQL). The results show that we can accurately estimate the fault impact scope and rank the root cause related functions within top 25 candidate faulty functions out of millions of application functions, which can greatly reduce the debugging time for the application developer.\nWe also tested with a range of distributed applications deployed on production cloud infrastructures. The distributed applications we tested include hundreds of computing nodes. Our system can complete anomaly detection within several seconds and train hundreds of models in parallel within tens of seconds. So we believe that our approach is scalable and practical for large-scale system monitoring and real-time analytics. During our Phase IB project, we also implemented a set of open source monitoring agents to integrate InsightFinder with different types of systems. We also implemented an initial prototype of log event classification and anomaly detection tool. \nThe project provides valuable internship opportunties to three graduate students and one female undergraduate student to obtain software development and data analytics skills. Techniques developed in this project have significant impact on improving the diagnosability and robustness of many real world cloud computing infrastructures. The commercial potential of the cloud anomaly prediction and diagnosis technology is big and has been demonstrated by our intial comericalization success. As the proposing technology increases the robustness of the cloud infrastructure, it allows more users to adopt the cloud computing technology and thus benefit the whole society that depends on the cloud technology. The project also advances the state of the art of the cloud reliability management research by putting the technology into real world use and enhance the technology to address real world challenges. \n \n\n \n\n\t\t\t\t\tLast Modified: 10/12/2016\n\n\t\t\t\t\tSubmitted by: Chao Huang"
 }
}