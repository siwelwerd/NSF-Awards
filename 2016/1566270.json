{
 "awd_id": "1566270",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: Automatically Understanding the Messages and Goals of Visual Media",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2016-06-01",
 "awd_exp_date": "2019-05-31",
 "tot_intn_awd_amt": 174590.0,
 "awd_amount": 182590.0,
 "awd_min_amd_letter_date": "2016-05-27",
 "awd_max_amd_letter_date": "2018-03-08",
 "awd_abstract_narration": "This project develops technologies to interpret the visual rhetoric of images. The project advances computer vision through novel solutions to the novel problem of decoding the visual messages in advertisements and artistic photographs, and thus brings computer vision closer to its goal of being able to automatically understand visual content. From a practical standpoint, understanding visual rhetoric can be used to produce image descriptions for the visually impaired that align with how a human would label these images, and thus give them access to the rich content shown in newspapers or on TV. This project is tightly integrated with education. The work is interdisciplinary and can attract undergraduate students to the research from different fields. \r\n\r\nThis research focuses on three media understanding tasks: (1) understanding the persuasive messages conveyed by artistic images and the strategies that those images use to convey their message; (2) exposing a photographer's bias towards their subject, e.g., determining whether a photograph portrays its subject in a positive or negative light; and (3) predicting what part of an artistic photograph a viewer might find most captivating or poignant. To enable decoding of artistic images, a large dataset is collected and annotated with a number of artistic properties and persuasion techniques that are intended for human understanding, then methods are developed to model visual symbolism in artistic images, as well as adapt positive/negative effect methods from sentiment analysis. To predict the photographer's bias towards a subject, a dataset of historical and modern portrayals of minorities and foreigners is collected, then an algorithm is created that reasons about body language and 3D layout and composition of the photo. To predict poignance, eyetracking data on a set of artistic images from famous photographers is collected, then semantic and connotation conflicts between the objects in the photographs are analyzed.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Adriana",
   "pi_last_name": "Kovashka",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Adriana Kovashka",
   "pi_email_addr": "kovashka@cs.pitt.edu",
   "nsf_id": "000690095",
   "pi_start_date": "2016-05-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pittsburgh",
  "inst_street_address": "4200 FIFTH AVENUE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4126247400",
  "inst_zip_code": "152600001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "MKAGLD59JRL1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pittsburgh",
  "perf_str_addr": "University Club",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152132303",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "026Y00",
   "pgm_ele_name": "CRII CISE Research Initiation"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 174590.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project is to develop resources and techniques for  understanding the \"visual rhetoric\" of the media. Unlike most work in  computer vision which analyses explicit content in photographs, we are  primarily interested in what can be \"read between the lines\" of a  photograph, i.e. persuasive messages that are encoded in a photo but  require careful analysis of the image content to discover. For example,  advertisements and public service announcements visually encode messages and stories to convince the audience to take a certain action. This  rhetoric depends on non-photorealistic portrayal of objects, symbolic  associations, commonsense reasoning, etc. In addition to understanding  advertisements specifically, we also learn to infer what photos in the news imply about their photo  subjects, and what their political bias is.  The project also aims to educate graduate and undergraduate students,  and inform the computer vision and AI community about the rich challenges that automatic visual media understanding poses.</p>\n<p>This award funded nine projects, which were published in CVPR, ICCV, ECCV, NeurIPS, BMVC and ACCV. The first of these projects contributed a large, richly annotated dataset and posed several concrete tasks  to measure how well the system understands the rhetoric of the ad,  ranging from relatively simple (classify topic and sentiment) to more  complex ones (answer questions about what the viewer should do, based on  the ad, and what arguments the ad provides for taking the suggested  action). In follow-up work, we proposed a novel suite of mechanisms for  retrieving action-reason statements, in a multiple-choice, multi-modal scenario, through novel metric learning techniques. We also developed more general techniques inspired by the challenges of predicting the rhetoric of ads. We proposed a weakly supervised object detection method, which was  inspired by the associations between object regions and properties in our learned feature space. This work relies on  captions at the image level, to learn to localize objects at the box  level. We also examined the relationship between image and text in  political articles, where the alignment between the two modalities is  very weak. We used the text as a privileged modality at training time,  to learn to predict the political bias in an image, better than a  variety of strong baselines. Finally, we developed a domain  adaptation approach for recognizing objects in atypical modalities (e.g.  sketches, paintings), as a step towards being able to recognize  non-photorealistic objects in advertisements.</p>\n<p>In addition to these research projects, we organized a workshop at CVPR  2018, which included invited talks, posters, and a challenge. We  discussed the challenges of ad understanding and brainstormed potential future directions in group exercises. We also showcased  our dataset. The dataset has 64,832 annotated images and 3,477 videos,  and about 730,000 annotations. It has been accessed in 6,010 unique  sessions: 2,425 times from the United States (40 different states), 997  from India, 402 from Japan, 306 from France, 301 from China, and more  from 73 other countries.</p>\n<p>The PI collaborated on the published projects above with thirteen students, four of whom were undergraduates, two are female, and one is Latin-American. Four of  these students have been funded by this project.</p>\n<div class=\"tinyMCEContent\">\n<p>In summary, this work funded contributions in the  domain of vision and language, metric learning, weak supervision,  transfer learning and learning with privileged information, domain  adaptation and generalization, content and style separation, and video  understanding. We identified several future research directions: how to best perform commonsense  reasoning, how to model the complementary information of image and text in weakly supervised fashion, and how to extract robust representations of objects for domain generalization. This project has also spurred collaborations of computer and information scientists, with social and political scientists.</p>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/16/2019<br>\n\t\t\t\t\tModified by: Adriana&nbsp;Kovashka</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project is to develop resources and techniques for  understanding the \"visual rhetoric\" of the media. Unlike most work in  computer vision which analyses explicit content in photographs, we are  primarily interested in what can be \"read between the lines\" of a  photograph, i.e. persuasive messages that are encoded in a photo but  require careful analysis of the image content to discover. For example,  advertisements and public service announcements visually encode messages and stories to convince the audience to take a certain action. This  rhetoric depends on non-photorealistic portrayal of objects, symbolic  associations, commonsense reasoning, etc. In addition to understanding  advertisements specifically, we also learn to infer what photos in the news imply about their photo  subjects, and what their political bias is.  The project also aims to educate graduate and undergraduate students,  and inform the computer vision and AI community about the rich challenges that automatic visual media understanding poses.\n\nThis award funded nine projects, which were published in CVPR, ICCV, ECCV, NeurIPS, BMVC and ACCV. The first of these projects contributed a large, richly annotated dataset and posed several concrete tasks  to measure how well the system understands the rhetoric of the ad,  ranging from relatively simple (classify topic and sentiment) to more  complex ones (answer questions about what the viewer should do, based on  the ad, and what arguments the ad provides for taking the suggested  action). In follow-up work, we proposed a novel suite of mechanisms for  retrieving action-reason statements, in a multiple-choice, multi-modal scenario, through novel metric learning techniques. We also developed more general techniques inspired by the challenges of predicting the rhetoric of ads. We proposed a weakly supervised object detection method, which was  inspired by the associations between object regions and properties in our learned feature space. This work relies on  captions at the image level, to learn to localize objects at the box  level. We also examined the relationship between image and text in  political articles, where the alignment between the two modalities is  very weak. We used the text as a privileged modality at training time,  to learn to predict the political bias in an image, better than a  variety of strong baselines. Finally, we developed a domain  adaptation approach for recognizing objects in atypical modalities (e.g.  sketches, paintings), as a step towards being able to recognize  non-photorealistic objects in advertisements.\n\nIn addition to these research projects, we organized a workshop at CVPR  2018, which included invited talks, posters, and a challenge. We  discussed the challenges of ad understanding and brainstormed potential future directions in group exercises. We also showcased  our dataset. The dataset has 64,832 annotated images and 3,477 videos,  and about 730,000 annotations. It has been accessed in 6,010 unique  sessions: 2,425 times from the United States (40 different states), 997  from India, 402 from Japan, 306 from France, 301 from China, and more  from 73 other countries.\n\nThe PI collaborated on the published projects above with thirteen students, four of whom were undergraduates, two are female, and one is Latin-American. Four of  these students have been funded by this project.\n\n\nIn summary, this work funded contributions in the  domain of vision and language, metric learning, weak supervision,  transfer learning and learning with privileged information, domain  adaptation and generalization, content and style separation, and video  understanding. We identified several future research directions: how to best perform commonsense  reasoning, how to model the complementary information of image and text in weakly supervised fashion, and how to extract robust representations of objects for domain generalization. This project has also spurred collaborations of computer and information scientists, with social and political scientists.\n\n\n\t\t\t\t\tLast Modified: 09/16/2019\n\n\t\t\t\t\tSubmitted by: Adriana Kovashka"
 }
}