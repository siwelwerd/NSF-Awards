{
 "awd_id": "1619189",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Effective bounds for distributed storage and data access",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2016-06-15",
 "awd_exp_date": "2020-05-31",
 "tot_intn_awd_amt": 306107.0,
 "awd_amount": 306107.0,
 "awd_min_amd_letter_date": "2016-06-10",
 "awd_max_amd_letter_date": "2016-06-10",
 "awd_abstract_narration": "Today's data centers rely on advanced methods to efficiently store increasing volumes of data. To protect against loss of data, data is stored redundantly on multiple disks. At the large scale of clusters of thousands of disks, simple replication of data is inefficient and not an option. To address the challenge of efficient, reliable and secure storage of data at a large scale, the project uses various combinations of algebraic and combinatorial methods. New constructions are given for the efficient recovery of data in case of disk failure. New methods are introduced to optimize bounds for storage capacity. New secure schemes are developed to ensure that information can only be obtained from the combined data of multiple disks. The research uses a novel algebraic approach to fundamental aspects of data storage and data access. Undergraduate and graduate students will be involved, working on projects with both a theoretical and a computational component.\r\n\r\nAlgorithms for data storage encode and divide data over several disks. The encoding challenge is to optimize the allocation of storage space between primary data and repair data. The optimization is analyzed for the general case in the setting of entropy inequalities and for the linear case in the setting of rank inequalities for matroids. The main focus is on three aspects. 1) Outer bounds: Optimize the use of available storage space under various combinations of constraints. 2) Multiple-access: Use coordinated encoding of data from different sources to add error-correction without sacrificing storage capacity. 3) Small alphabets: Using a novel approach, analyze nontraditional coset schemes that are defined over smaller fields.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Iwan",
   "pi_last_name": "Duursma",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Iwan M Duursma",
   "pi_email_addr": "duursma@math.uiuc.edu",
   "nsf_id": "000190767",
   "pi_start_date": "2016-06-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S. Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 306107.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Error correcting codes have been used for many decades to automatically recognize and correct certain types of errors that occur when data is stored or transmitted. Data centers store data in new and different ways, by employing clusters of thousands of disks and dividing large data into smaller pieces that are stored on different disks. Standard solutions to protect data against errors such as 3-replication (storing each piece of data in the same form on three different disks) do not scale well to the size of data centers. It requires advanced code constructions to store data reliably while at the same time limiting hardware cost, network traffic, and energy consumption.<br /><br />The research led to two new families of codes for large scale distributed data storage. Both families have a clear underlying structure that relies on combinatorics and multilinear algebra. The mathematical framework is used to prove the performance of the codes. The codes adapt to different storage scenarios and different parameters can be used to optimize the use of available resources and to minimize their cost.<br /><br />As part of the research the known Hadoop code was analyzed and then improved. Previous work had reduced the repair bandwidth (the number of bits that disks exchange to correct a single error) from 80 to 64 bits. A new repair scheme was formulated that uses 54 bits.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/08/2020<br>\n\t\t\t\t\tModified by: Iwan&nbsp;M&nbsp;Duursma</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nError correcting codes have been used for many decades to automatically recognize and correct certain types of errors that occur when data is stored or transmitted. Data centers store data in new and different ways, by employing clusters of thousands of disks and dividing large data into smaller pieces that are stored on different disks. Standard solutions to protect data against errors such as 3-replication (storing each piece of data in the same form on three different disks) do not scale well to the size of data centers. It requires advanced code constructions to store data reliably while at the same time limiting hardware cost, network traffic, and energy consumption.\n\nThe research led to two new families of codes for large scale distributed data storage. Both families have a clear underlying structure that relies on combinatorics and multilinear algebra. The mathematical framework is used to prove the performance of the codes. The codes adapt to different storage scenarios and different parameters can be used to optimize the use of available resources and to minimize their cost.\n\nAs part of the research the known Hadoop code was analyzed and then improved. Previous work had reduced the repair bandwidth (the number of bits that disks exchange to correct a single error) from 80 to 64 bits. A new repair scheme was formulated that uses 54 bits.\n\n\t\t\t\t\tLast Modified: 09/08/2020\n\n\t\t\t\t\tSubmitted by: Iwan M Duursma"
 }
}