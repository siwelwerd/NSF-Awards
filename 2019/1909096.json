{
 "awd_id": "1909096",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Partitioning Big Data for the High Performance Computation of Persistent Homology",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 499344.0,
 "awd_amount": 499344.0,
 "awd_min_amd_letter_date": "2019-09-03",
 "awd_max_amd_letter_date": "2019-09-03",
 "awd_abstract_narration": "New insights with machine learning exists across may domains, including, for example, medicine, social media, image processing, biology, and computer and network security. Machine learning is able to process large, high-dimensional data sets that are beyond human capabilities. One emerging method of machine learning is based on a branch of mathematics called topology that is sometimes able to discover knowledge that is not available using conventional methods. The field of topology is concerned with of the shape of an object and Persistent Homology is the critical method in topology used to extract the features of a shape. Persistent Homology will classify an object by the size and number of holes and voids in that object. Unfortunately, computing the Persistent Homology for an object requires significant amounts of memory and long run-times that increases exponentially in the number of points that forms that object. This project will treat the object formed by the data and subdivide it into smaller regions for the parallel computation of Persistent Homology on each region. The results from the regional analyses will then be assembled together and any duplicate or missing results will be identified and restored in a post analysis step. The computation on all of the regions will be completed in substantially less time and in much less total memory than a single computation on the entire data set. Testing of the methods developed will be performed using a variety of synthetic and real-world data. The synthetic data will permit controlled studies on performance and scalability. Realworld data from a variety of sources and especially data where the small topological features are significant (such as data from brain scans) will be used. This project will propel the application of topology based analysis to discover new insights and meaningful information from massive high-dimensional data. An expansion of student training in data mining through topological-based methods will be achieved with the addition of classes, projects (senior project, MS Theses, PhD Dissertations, and so on), seminars, and research co-op training experiences. Students at all levels will be impacted and special emphasis placed on minority and underrepresented student groups participation. This project will also participate in the Women in Science and Engineering programs at UC. The project investigators will engage local area K-12 students, international exchange students and researchers at UC's collaborative institutions, UC's Medical School, Cincinnati Children's Medical Center, the Air Force Research Lab, and local industries with information and seminars on this project investigations and results.\r\n\r\nThis project proposes to combine the fields of Approximate Computing with Topological Data Analysis to dramatically reduce the computational and memory requirements to use Topological Data Analysis on very large data sets.  In particular, this project will develop approximate methods for computing Persistent Homology that dramatically increase the sizes of data sets for which data mining methods based on topological data analysis can be applied. This project expects to increase the size of the input data set that can be analyzed by Topological Data Analysis methods by at least 3-5 orders of magnitude. While approximate methods can introduce error, the features identified by the approximate methods will identify regions of the point cloud where an upscaling steps and regional computations of Persistent Homology can be used (in parallel) to establish more precise boundaries of those features. The project will develop algorithmic improvements, formal statements on the correctness, error bounds, and complexities of the algorithms and approximation techniques. These techniques have important implications on the ability to apply topological data analysis techniques to much larger data sets than currently possible.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Philip",
   "pi_last_name": "Wilsey",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Philip A Wilsey",
   "pi_email_addr": "philip.wilsey@uc.edu",
   "nsf_id": "000299348",
   "pi_start_date": "2019-09-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Cincinnati Main Campus",
  "inst_street_address": "2600 CLIFTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CINCINNATI",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "5135564358",
  "inst_zip_code": "452202872",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "OH01",
  "org_lgl_bus_name": "CINCINNATI UNIV OF",
  "org_prnt_uei_num": "DZ4YCZ3QSPR5",
  "org_uei_num": "DZ4YCZ3QSPR5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Cincinnati Main Campus",
  "perf_str_addr": "Dept of EECS PO Box 210030",
  "perf_city_name": "Cincinnati",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "452210030",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "OH01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499344.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Topology is a field of mathematics that characterizes invariant properties of geometric objects under continuous deformation (without tearing). In the simplest terms, think about a coffee cup and a donut.&nbsp; In the field of topology, these two surfaces are equivalent.&nbsp; This is because the donut can be deformed into the shape of a coffee cup and vice-versa.&nbsp; Essentially they both have one hole.<br /><br />Topological Data Analysis (TDA) is a method to extract information from data based on the holes and voids in the data. Studies with PH have show it to provide insight in understanding data from domains such as: network analysis, images and movies, protein analysis, genomic sequencing protein modeling, cell evolution and differentiation, statistical data analysis, and other domains.<br /><br />One of the more important computations of TDA is called Persistent Homology (PH).&nbsp; PH examines data in n-dimensions and considers a family of graphs (each called a subcomplex) formed by the data at increasing connectivity distances.&nbsp; That is, the data is first considered as disconnected and then a second graph is formed from the data with a connection between the two points closed to each other.&nbsp; The system continues to build complexes at increasing distances until the data is fully connected. As connecting points are formed into triangles, tetrahedrons, (or higher dimensional triangular structures), the connected points are considered filled and treated as a single object.&nbsp; Adjacent triangular structures are joined together as a filled object.&nbsp; Holes are formed (in any dimension) when a set of points are connected about a space that is not filled with triangular structures (at that dimension).&nbsp; When the hold forms in a subcomplex, the connectivity distance for that subcomplex is called the birth of the hole.&nbsp; As the system continues to examine the set of subcomplexes, each hole will eventually be filed in and experience what is called its death.&nbsp; Thus, the data is characterized by all of the holes in the data and specifically when each hole is born and when it dies.&nbsp; This characterization permits one to compare different data sets to establish if and how they are related.<br /><br />Unfortunately, computing PH of data is costly in terms of total memory required to store the subcomplexes and runtime.&nbsp; While it is possible to compute PH on moderately sized data sets (100K-500K points) in low dimensions (2D or 3D), it fails to scale to high dimensions.&nbsp; In fact, computing PH on data in dimension 8 is possible only with a few hundred points.&nbsp; This project is investigating techniques to dramatically expand the size and dimension of data that PH can be computed on.&nbsp; There are several approaches that were studied and developed, namely: (i) sampling and partitioning the data for concurrent execution on multiple computers (to leverage the additional memory and CPU cycles available with multiple computers.&nbsp; The approach requires that the data be partitioned in overlapping regions of points that are near each other (this is done with a standard data mining technique called k-means clustering).&nbsp; The PH results from the overlapping partitions are merged together with duplicate entries merged; (ii) the second technique is to optimize the subcomplexes with planar graphs (graphs whose connecting lines do not cross) that help limit the size.&nbsp; One of the important planar graph constructions is called the Alpha complex.&nbsp; The reason why use of the Alpha complex is not the standard approach in the computation of PH is that constructing the planar graphs is expensive in runtime and increases dramatically with dimension.&nbsp; This project explored this problem and has developed a technique that constructs an approximate Alpha complex in time that is much faster than the standard method; (iii) the final significant contribution of this work is the development of a topological property complimentary to PH, namely the Euler-Poincare Characteristic (EC).&nbsp; EC is computed by counting and summing the vertices, edges, triangles, tetrahedrons, and their high dimensional counterparts.&nbsp; EC is an old concept, however, techniques to compute it in dimensions &gt; 3 were not previously know.&nbsp; This project found a solution to compute EC in any dimension.&nbsp; Since the computation of EC is simply counting and summing, the computation is significantly faster and more memory efficient than computing PH.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/10/2023<br>\n\t\t\t\t\tModified by: Philip&nbsp;A&nbsp;Wilsey</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nTopology is a field of mathematics that characterizes invariant properties of geometric objects under continuous deformation (without tearing). In the simplest terms, think about a coffee cup and a donut.  In the field of topology, these two surfaces are equivalent.  This is because the donut can be deformed into the shape of a coffee cup and vice-versa.  Essentially they both have one hole.\n\nTopological Data Analysis (TDA) is a method to extract information from data based on the holes and voids in the data. Studies with PH have show it to provide insight in understanding data from domains such as: network analysis, images and movies, protein analysis, genomic sequencing protein modeling, cell evolution and differentiation, statistical data analysis, and other domains.\n\nOne of the more important computations of TDA is called Persistent Homology (PH).  PH examines data in n-dimensions and considers a family of graphs (each called a subcomplex) formed by the data at increasing connectivity distances.  That is, the data is first considered as disconnected and then a second graph is formed from the data with a connection between the two points closed to each other.  The system continues to build complexes at increasing distances until the data is fully connected. As connecting points are formed into triangles, tetrahedrons, (or higher dimensional triangular structures), the connected points are considered filled and treated as a single object.  Adjacent triangular structures are joined together as a filled object.  Holes are formed (in any dimension) when a set of points are connected about a space that is not filled with triangular structures (at that dimension).  When the hold forms in a subcomplex, the connectivity distance for that subcomplex is called the birth of the hole.  As the system continues to examine the set of subcomplexes, each hole will eventually be filed in and experience what is called its death.  Thus, the data is characterized by all of the holes in the data and specifically when each hole is born and when it dies.  This characterization permits one to compare different data sets to establish if and how they are related.\n\nUnfortunately, computing PH of data is costly in terms of total memory required to store the subcomplexes and runtime.  While it is possible to compute PH on moderately sized data sets (100K-500K points) in low dimensions (2D or 3D), it fails to scale to high dimensions.  In fact, computing PH on data in dimension 8 is possible only with a few hundred points.  This project is investigating techniques to dramatically expand the size and dimension of data that PH can be computed on.  There are several approaches that were studied and developed, namely: (i) sampling and partitioning the data for concurrent execution on multiple computers (to leverage the additional memory and CPU cycles available with multiple computers.  The approach requires that the data be partitioned in overlapping regions of points that are near each other (this is done with a standard data mining technique called k-means clustering).  The PH results from the overlapping partitions are merged together with duplicate entries merged; (ii) the second technique is to optimize the subcomplexes with planar graphs (graphs whose connecting lines do not cross) that help limit the size.  One of the important planar graph constructions is called the Alpha complex.  The reason why use of the Alpha complex is not the standard approach in the computation of PH is that constructing the planar graphs is expensive in runtime and increases dramatically with dimension.  This project explored this problem and has developed a technique that constructs an approximate Alpha complex in time that is much faster than the standard method; (iii) the final significant contribution of this work is the development of a topological property complimentary to PH, namely the Euler-Poincare Characteristic (EC).  EC is computed by counting and summing the vertices, edges, triangles, tetrahedrons, and their high dimensional counterparts.  EC is an old concept, however, techniques to compute it in dimensions &gt; 3 were not previously know.  This project found a solution to compute EC in any dimension.  Since the computation of EC is simply counting and summing, the computation is significantly faster and more memory efficient than computing PH.\n\n\t\t\t\t\tLast Modified: 10/10/2023\n\n\t\t\t\t\tSubmitted by: Philip A Wilsey"
 }
}