{
 "awd_id": "1849822",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CHS: Novel technology for improving access to trainer-led aerobic exercise for people who are blind",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2019-04-01",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 174995.0,
 "awd_amount": 101295.0,
 "awd_min_amd_letter_date": "2019-03-25",
 "awd_max_amd_letter_date": "2022-06-03",
 "awd_abstract_narration": "People who are blind or have low vision are more likely to be obese than people with normal vision. One effective way to prevent obesity is by exercising regularly, but exercise instructors typically give verbal directions and feedback based on the premise that the trainee is sighted. To address this problem, this research will create a system that uses a sensing mat to track a trainee's feet during step aerobics and provide reorienting feedback if s/he falls out of sync with the class. Studies with people who are visually impaired will be used to determine how best to design verbal feedback to the trainee and when the verbal feedback should be provided.  System accuracy will also be measured.  This research will create knowledge that will help designers of future assistive technologies. The research will also help enhance the quality of life for people who are visually impaired, providing them with an opportunity to have an accessible experience in group-based aerobic exercise classes.  \r\n\r\nTo achieve the project's goals, an empirical user study will first be conducted where people with visual impairments perform aerobic exercises with video and audio workouts. Participants will be video recorded during exercise, and the research team will analyze the recordings to determine which verbal phrases help or confuse participants. Second, sensor mat technology and associated algorithms will be developed to track the trainee's feet with template matching to determine when the trainee falls out of sync during step aerobics. Two kinds of verbal feedback will be developed: proactive feedback if the system detects unhelpful verbal phrases, and reactive feedback if the person performs three missteps in a row. User studies will be conducted to determine the effectiveness of this feedback, as well as participant preferences with respect to type of verbal feedback.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kyle",
   "pi_last_name": "Rector",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Kyle K Rector",
   "pi_email_addr": "kyle-rector@uiowa.edu",
   "nsf_id": "000754390",
   "pi_start_date": "2019-03-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Iowa",
  "inst_street_address": "105 JESSUP HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IOWA CITY",
  "inst_state_code": "IA",
  "inst_state_name": "Iowa",
  "inst_phone_num": "3193352123",
  "inst_zip_code": "522421316",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IA01",
  "org_lgl_bus_name": "THE UNIVERSITY OF IOWA",
  "org_prnt_uei_num": "",
  "org_uei_num": "Z1H9VJS8NG16"
 },
 "perf_inst": {
  "perf_inst_name": "University of Iowa",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IA",
  "perf_st_name": "Iowa",
  "perf_zip_code": "522421320",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IA01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 101294.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 0.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to make aerobic exercises more accessible to people who are blind through a novel technological design. Specifically, the major goals of the project were to determine what phrases are helpful or confusing during aerobic exercises and develop an algorithm using computer vision and a pressure sensor mat to determine whether a person is in sync with a step aerobics class.</p>\n<p dir=\"ltr\">First, user studies were conducted with 10 people with visual impairments exercising along with audio and video aerobic workouts. Video footage of their exercise and audio interviews were analyzed to determine a preliminary set of phrases that were helpful or confusing. Then, an iterative qualitative analysis of six other exercise videos was conducted, followed by expert feedback to derive a taxonomy of accessible and inaccessible phrases. Phrases were accessible when they were describing what to do with one&rsquo;s body (e.g., specifying a limb or direction to move) and when to do an exercise (e.g., starting, stopping, pacing). Phrases not describing timing or body movement were less accessible (e.g., using locations that require sight, conversations between people in a video, encouraging phrases).</p>\n<p dir=\"ltr\">Then, a prototype was developed that determines whether a person is in sync with a workout. The prototype compares the current &ldquo;status&rdquo; of the user&rsquo;s feet with a &ldquo;workout specification.&rdquo; A workout specification includes the moves broken down to each &ldquo;step,&rdquo; the number of repetitions of each move, and beats per minute. To determine the current &ldquo;status,&rdquo; sensor mat readings are used - pressure and contour data in color. Computer Vision techniques along with a Convolutional Neural Network are used to determine whether each shoe print is a left shoe, right shoe, or indeterminable and its orientation to the nearest multiple of 45&deg;. The latency from receiving the mat reading to updating the status is 75-95 milliseconds.</p>\n<p dir=\"ltr\">The intellectual merit of this research is that it could inform design guidelines for technologies that give exercise or body movement instructions. These will specifically help people with visual impairments, but also people who cannot see a visual instruction for other reasons (e.g., head pointed away from screen, chose not to have visuals). These findings will help people who conduct research in Accessibility, Human-Computer Interaction, and more specifically, those who develop exercise games or spatial interfaces (e.g., mixed reality).</p>\n<p dir=\"ltr\">The broader impact of the research is that it could help practitioners involved in physical fitness for people with visual impairments. The guidelines have the potential to improve in-person exercise instruction or audio and video workouts done independently. They can provide more context at the beginning of an exercise and more frequent specific instructions throughout. Finally, female researchers led this work.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/23/2022<br>\n\t\t\t\t\tModified by: Kyle&nbsp;K&nbsp;Rector</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project aims to make aerobic exercises more accessible to people who are blind through a novel technological design. Specifically, the major goals of the project were to determine what phrases are helpful or confusing during aerobic exercises and develop an algorithm using computer vision and a pressure sensor mat to determine whether a person is in sync with a step aerobics class.\nFirst, user studies were conducted with 10 people with visual impairments exercising along with audio and video aerobic workouts. Video footage of their exercise and audio interviews were analyzed to determine a preliminary set of phrases that were helpful or confusing. Then, an iterative qualitative analysis of six other exercise videos was conducted, followed by expert feedback to derive a taxonomy of accessible and inaccessible phrases. Phrases were accessible when they were describing what to do with one\u2019s body (e.g., specifying a limb or direction to move) and when to do an exercise (e.g., starting, stopping, pacing). Phrases not describing timing or body movement were less accessible (e.g., using locations that require sight, conversations between people in a video, encouraging phrases).\nThen, a prototype was developed that determines whether a person is in sync with a workout. The prototype compares the current \"status\" of the user\u2019s feet with a \"workout specification.\" A workout specification includes the moves broken down to each \"step,\" the number of repetitions of each move, and beats per minute. To determine the current \"status,\" sensor mat readings are used - pressure and contour data in color. Computer Vision techniques along with a Convolutional Neural Network are used to determine whether each shoe print is a left shoe, right shoe, or indeterminable and its orientation to the nearest multiple of 45&deg;. The latency from receiving the mat reading to updating the status is 75-95 milliseconds.\nThe intellectual merit of this research is that it could inform design guidelines for technologies that give exercise or body movement instructions. These will specifically help people with visual impairments, but also people who cannot see a visual instruction for other reasons (e.g., head pointed away from screen, chose not to have visuals). These findings will help people who conduct research in Accessibility, Human-Computer Interaction, and more specifically, those who develop exercise games or spatial interfaces (e.g., mixed reality).\nThe broader impact of the research is that it could help practitioners involved in physical fitness for people with visual impairments. The guidelines have the potential to improve in-person exercise instruction or audio and video workouts done independently. They can provide more context at the beginning of an exercise and more frequent specific instructions throughout. Finally, female researchers led this work.\n\n\t\t\t\t\tLast Modified: 09/23/2022\n\n\t\t\t\t\tSubmitted by: Kyle K Rector"
 }
}