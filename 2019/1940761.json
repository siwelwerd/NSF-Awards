{
 "awd_id": "1940761",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "E2CDA: Type II: Self-Adaptive Reservoir Computing with Spiking Neurons: Learning Algorithms and Processor Architectures",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 215672.0,
 "awd_amount": 215672.0,
 "awd_min_amd_letter_date": "2019-08-15",
 "awd_max_amd_letter_date": "2019-08-15",
 "awd_abstract_narration": "While computing has become increasingly data centric across many disciplines, conventional computer architectures have limited potential in meeting the escalating performance and energy efficiency needs in this era of data-driven science and engineering. This project aims to develop brain-inspired neural models of computation and adaptive processor architectures to enable intelligent data processing and learning in a wide range of applications. While being strongly interdisciplinary, this work will bridge neuroscience, artificial neural networks, computer architecture, and hardware engineering. The planned research will provide rich training and educational opportunities to students, and produce new curriculum. Research participation from undergraduate and underrepresented students will be promoted. The outcomes of this project will be broadly disseminated. Research collaboration with the US industry will be actively pursued via interaction with the Semiconductor Research Corporation. \r\n\r\nThis work is aimed at attaining brain-like learning performance by imitating how the brain represents, processes, and learns from information, and more specifically, by developing models of computation based on the third-generation spiking neural networks, and efficient adaptive processor architectures.  Within the framework of so called reservoir computing, the proposed neural models mimic key characteristics of the brain such as information processing based on spike timing. Furthermore, this project will develop brain-inspired learning mechanisms to allow training of complex recurrent spiking neural networks. Self-adaptive processor architectures with integrated on-chip learning, light-weight runtime learning performance prediction, and energy management will be developed to maximize system energy efficiency while providing a guarantee of performance.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Peng",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Peng Li",
   "pi_email_addr": "lip@ece.ucsb.edu",
   "nsf_id": "000388188",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931062050",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "015Y00",
   "pgm_ele_name": "Energy Efficient Computing: fr"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 104758.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 110914.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With the slowing down of CMOS technology scaling, the mainstream von Neumann architecture has limited potential in meeting the escalating performance and energy efficiency needs in the era of data-driven science and engineering. Disruptive models and architectures of computation are highly desirable in order to serve the growing need in processing and learning from large amounts of data. To this end, the human brain learns, adapts to the changing environment, and generalizes past experience to new situations seemingly with little effort. Hence, it offers a remarkable reference for building novel computing paradigms to address the performance and energy crisis facing today&rsquo;s computing systems.&nbsp; &nbsp;</p>\n<p>The work performed under this award and its precursor award #1639995 at Texas A&amp;M University was motivated by the understanding that attaining brain-like performance may ultimately entail imitating how the brain represents, processes, and learns from data. The research team worked towards this objective by developing models of computation based on the third-generation of artificial neural networks, namely, spiking neural networks, and energy-efficient adaptive spiking neural hardware architectures and processors. &nbsp;One main research focus was on spiking neural models in the form of reservoir computing that mimic several key characteristics of the brain: computation is performed by a network of spiking neurons wired up in a recurrent manner, based on spikes encoded with timing information, and supported by spike-based learning mechanisms.</p>\n<p>The research team made a number of contributions including developing:</p>\n<ul>\n<li>Biologically-inspired information-theoretic learning rules based on intrinsic plasticity for online unsupervised training of spiking neural networks;</li>\n</ul>\n<ul>\n<li>Biologically-inspired supervised spike-timing-dependent plasticity learning rules for training the liquid state machine, a reservoir computing model based on spiking neurons;</li>\n</ul>\n<ul>\n<li>Biologically-inspired spiking neural networks and learning rules for mobile robot collision avoidance;</li>\n</ul>\n<ul>\n<li>Backpropagation algorithms for supervised training feedforward and recurrent spiking neural networks while achieving the state-of-the-art accuracy;</li>\n</ul>\n<ul>\n<li>Energy-efficient spiking neural processor designs with on-chip supervised and unsupervised spike-timing-dependent plasticity and other types of biologically-inspired learning algorithms on field programmable gate array (FPGA) hardware;</li>\n</ul>\n<ul>\n<li>Low-power hardware design techniques exploring sparsity of firing activities and online system adaptation for recurrent spiking neural processors;</li>\n</ul>\n<ul>\n<li>Hardware acceleration of spiking neural networks on general systolic-array architectures. </li>\n</ul>\n<p>Along the above lines, a coherent set of technical contributions have been made as summarized in four journal publications and eight conference papers. One conference paper received the best paper award (in the processor architecture track) from the IEEE International Conference on Computer Design in 2020.</p>\n<p>Broadly, this project developed algorithms, architectures and hardware systems for brain-inspired computing. The outcomes from this project may simulate the development of the general field of machine learning which proliferates into broad application domains rapidly. The brain-inspired approaches taken by this project may contribute to interdisciplinary research across neuroscience, cognitive science, and engineering.&nbsp; The results of this project have benefited the graduate-level education in the areas of brain-inspired computing and hardware engineering at Texas A&amp;M University and University of California at Santa Barbara. The results of this project were disseminated widely to academia, industry, and the general public through publications, and invited talks at conferences and universities. A number of Ph. D., M. S., and undergraduate student researchers were trained. Among these, three Ph. D. students and three M.S. student graduated, and another Ph. D. student is expected to finish his Ph.D. study in the near future.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/11/2020<br>\n\t\t\t\t\tModified by: Peng&nbsp;Li</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1940761/1940761_10455266_1607720222173_OutcomeImage1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1940761/1940761_10455266_1607720222173_OutcomeImage1--rgov-800width.jpg\" title=\"Online Unsupervised Spiking Neural Network Training using Intrinsic Plasticity\"><img src=\"/por/images/Reports/POR/2020/1940761/1940761_10455266_1607720222173_OutcomeImage1--rgov-66x44.jpg\" alt=\"Online Unsupervised Spiking Neural Network Training using Intrinsic Plasticity\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Online Unsupervised Spiking Neural Network Training using Intrinsic Plasticity</div>\n<div class=\"imageCredit\">Peng Li</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Peng&nbsp;Li</div>\n<div class=\"imageTitle\">Online Unsupervised Spiking Neural Network Training using Intrinsic Plasticity</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1940761/1940761_10455266_1607720334567_OutcomeImage2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1940761/1940761_10455266_1607720334567_OutcomeImage2--rgov-800width.jpg\" title=\"Liquid State Machine Processor on Xilinx Virtex 6 FPGA\"><img src=\"/por/images/Reports/POR/2020/1940761/1940761_10455266_1607720334567_OutcomeImage2--rgov-66x44.jpg\" alt=\"Liquid State Machine Processor on Xilinx Virtex 6 FPGA\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Liquid State Machine based Spiking Neural Processor on Xilinx Virtex 6 FPGA</div>\n<div class=\"imageCredit\">Peng Li</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Peng&nbsp;Li</div>\n<div class=\"imageTitle\">Liquid State Machine Processor on Xilinx Virtex 6 FPGA</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1940761/1940761_10455266_1607720400545_OutcomeImage3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1940761/1940761_10455266_1607720400545_OutcomeImage3--rgov-800width.jpg\" title=\"Hardware Architecture with Onchip Spike-Train Level Direct Feedback Alignment (ST-DFA) Training\"><img src=\"/por/images/Reports/POR/2020/1940761/1940761_10455266_1607720400545_OutcomeImage3--rgov-66x44.jpg\" alt=\"Hardware Architecture with Onchip Spike-Train Level Direct Feedback Alignment (ST-DFA) Training\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Hardware Architecture with Onchip Spike-Train Level Direct Feedback Alignment (ST-DFA) Training</div>\n<div class=\"imageCredit\">Peng Li</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Peng&nbsp;Li</div>\n<div class=\"imageTitle\">Hardware Architecture with Onchip Spike-Train Level Direct Feedback Alignment (ST-DFA) Training</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nWith the slowing down of CMOS technology scaling, the mainstream von Neumann architecture has limited potential in meeting the escalating performance and energy efficiency needs in the era of data-driven science and engineering. Disruptive models and architectures of computation are highly desirable in order to serve the growing need in processing and learning from large amounts of data. To this end, the human brain learns, adapts to the changing environment, and generalizes past experience to new situations seemingly with little effort. Hence, it offers a remarkable reference for building novel computing paradigms to address the performance and energy crisis facing today\u2019s computing systems.   \n\nThe work performed under this award and its precursor award #1639995 at Texas A&amp;M University was motivated by the understanding that attaining brain-like performance may ultimately entail imitating how the brain represents, processes, and learns from data. The research team worked towards this objective by developing models of computation based on the third-generation of artificial neural networks, namely, spiking neural networks, and energy-efficient adaptive spiking neural hardware architectures and processors.  One main research focus was on spiking neural models in the form of reservoir computing that mimic several key characteristics of the brain: computation is performed by a network of spiking neurons wired up in a recurrent manner, based on spikes encoded with timing information, and supported by spike-based learning mechanisms.\n\nThe research team made a number of contributions including developing:\n\nBiologically-inspired information-theoretic learning rules based on intrinsic plasticity for online unsupervised training of spiking neural networks;\n\n\nBiologically-inspired supervised spike-timing-dependent plasticity learning rules for training the liquid state machine, a reservoir computing model based on spiking neurons;\n\n\nBiologically-inspired spiking neural networks and learning rules for mobile robot collision avoidance;\n\n\nBackpropagation algorithms for supervised training feedforward and recurrent spiking neural networks while achieving the state-of-the-art accuracy;\n\n\nEnergy-efficient spiking neural processor designs with on-chip supervised and unsupervised spike-timing-dependent plasticity and other types of biologically-inspired learning algorithms on field programmable gate array (FPGA) hardware;\n\n\nLow-power hardware design techniques exploring sparsity of firing activities and online system adaptation for recurrent spiking neural processors;\n\n\nHardware acceleration of spiking neural networks on general systolic-array architectures. \n\n\nAlong the above lines, a coherent set of technical contributions have been made as summarized in four journal publications and eight conference papers. One conference paper received the best paper award (in the processor architecture track) from the IEEE International Conference on Computer Design in 2020.\n\nBroadly, this project developed algorithms, architectures and hardware systems for brain-inspired computing. The outcomes from this project may simulate the development of the general field of machine learning which proliferates into broad application domains rapidly. The brain-inspired approaches taken by this project may contribute to interdisciplinary research across neuroscience, cognitive science, and engineering.  The results of this project have benefited the graduate-level education in the areas of brain-inspired computing and hardware engineering at Texas A&amp;M University and University of California at Santa Barbara. The results of this project were disseminated widely to academia, industry, and the general public through publications, and invited talks at conferences and universities. A number of Ph. D., M. S., and undergraduate student researchers were trained. Among these, three Ph. D. students and three M.S. student graduated, and another Ph. D. student is expected to finish his Ph.D. study in the near future.\n\n\t\t\t\t\tLast Modified: 12/11/2020\n\n\t\t\t\t\tSubmitted by: Peng Li"
 }
}