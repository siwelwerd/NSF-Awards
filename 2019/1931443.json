{
 "awd_id": "1931443",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Medium: Collaborative: User-Centered Deployment of Differential Privacy",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "James Joshi",
 "awd_eff_date": "2020-01-01",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 343110.0,
 "awd_amount": 343110.0,
 "awd_min_amd_letter_date": "2019-08-16",
 "awd_max_amd_letter_date": "2019-08-16",
 "awd_abstract_narration": "Differential privacy (DP) has been accepted as the de facto standard for data privacy in the research community and beyond. Both companies and government agencies are trying to deploy DP technologies.  Broader deployments of DP technology, however, face challenges.  This project aims to understand the needs of different stakeholders in data  privacy, and to develop algorithms and software to enable broader deployment of private data sharing.  The project's novelty is combining the expertise of social science researchers with that of computer scientists who have both theoretical and system research experiences related to DP to develop a hybrid approach to private data sharing to achieve better privacy-utility tradeoff.  The project's impacts are in advancing the state-of-the-art with regard to DP deployment in particular and privacy protection in general.  More specifically the project identifies the workflow of DP data sharing, improve understanding of DP communication, and develop new algorithms, privacy concepts, and privacy mechanisms to support deployment of DP. \r\n \r\nThe project has four tasks that will advance the understanding of user-centered DP and lay a foundation for its deployment. (1) Examine individual human users' perception, comprehension and acceptance of the concept and guarantee of DP and the effect of privacy parameter, and to investigate effective ways to communicate those concepts.  (2) Implement methods from the domains of human factors and human-computer interaction to identify tasks, goals, and workflow in private data sharing.   (3) Develop key algorithms and software for a hybrid approach of private data sharing.  In the hybrid approach, one first publishes a private synopsis of dataset using carefully selected low-degree marginals.  From these marginals, one can either synthesize new datasets, or answer queries directly using inference under the maximum entropy principle.  The hybrid approach enhances this with interactive query answering, enabling extraction of information not covered by low-degree marginals.  (4) Develop techniques to further improve the privacy-utility tradeoff in private data sharing, including a theory of differential privacy under publishable information.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ninghui",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ninghui Li",
   "pi_email_addr": "ninghui@cs.purdue.edu",
   "nsf_id": "000166436",
   "pi_start_date": "2019-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Proctor",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Robert W Proctor",
   "pi_email_addr": "proctor@psych.purdue.edu",
   "nsf_id": "000375639",
   "pi_start_date": "2019-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jeremiah",
   "pi_last_name": "Blocki",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Jeremiah M Blocki",
   "pi_email_addr": "jblocki@purdue.edu",
   "nsf_id": "000681660",
   "pi_start_date": "2019-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "305 N. University Street",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072107",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 343110.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Differential privacy (DP) has been accepted as the de facto standard for data privacy in the research community and beyond. DP techniques have started to be deployed both in industry and in government agencies. Broader deployments of DP technology, however, face several challenges.&nbsp; This project focuses on understanding how human users perceive DP, how to generate synthetic datasets given private input datasets, and how to further improve the privacy-utility tradeoff in DP techniques.&nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>An interesting and important open question is whether users understand these DP techniques, trust them, and consequently, increase their data disclosure when these techniques are deployed. Through human subject studies, it was found that participants increased data sharing for the high-sensitive questions when they were informed of protection from differential privacy, indicating a positive effect of communicating privacy techniques. It was found that descriptions that focus on definition and/or data perturbation processes are less effective than descriptions that explain the implications, i.e., whether the privacy protection relies on the trustworthiness of the company or the server. A related question is whether researchers who use large data sets understand DP and the impact that it would have on their research. A survey conducted in the project among researchers who use data from the U.S. census or related American Community Survey found that most had concerns about implementation of DP on the 2020 data. They had no formal training in DP, showed little understanding of why it was needed, and thought it would negatively impact their research.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>For generating synthetic datasets under differential privacy, the main challenge is how to deal with high-dimensional data.&nbsp; It is known that answering too many queries accurately enables reconstruction of the dataset, violating privacy.&nbsp; Therefore, one has to decide what information to preserve (and implicitly what to give up) in a private dataset.&nbsp; The researchers developed the PrivSyn approach, which uses a large number of low-degree marginals to represent a dataset.&nbsp; For example, given around 100 attributes, PrivSyn uses all one-way marginals and around 500 two-way marginals. A two-way marginal (specified by two attributes) is a frequency distribution table, showing the number of records with each possible combination of values for the two attributes. Information that can be derived from these low-degree marginals is preserved, and high-dimensional correlation that cannot be derived from low-degree marginals is lost, as the utility cost to satisfy DP.&nbsp; PrivSyn has been shown to perform well in practice.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>This interdisciplinary project has helped support the education of several students, both in Computer Science and Psychology.&nbsp; These include several female students, including a Hispanic female student.&nbsp; One student supported by the project joined academia as a tenure-track faculty member.&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/29/2022<br>\n\t\t\t\t\tModified by: Ninghui&nbsp;Li</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDifferential privacy (DP) has been accepted as the de facto standard for data privacy in the research community and beyond. DP techniques have started to be deployed both in industry and in government agencies. Broader deployments of DP technology, however, face several challenges.  This project focuses on understanding how human users perceive DP, how to generate synthetic datasets given private input datasets, and how to further improve the privacy-utility tradeoff in DP techniques.  \n\n \nAn interesting and important open question is whether users understand these DP techniques, trust them, and consequently, increase their data disclosure when these techniques are deployed. Through human subject studies, it was found that participants increased data sharing for the high-sensitive questions when they were informed of protection from differential privacy, indicating a positive effect of communicating privacy techniques. It was found that descriptions that focus on definition and/or data perturbation processes are less effective than descriptions that explain the implications, i.e., whether the privacy protection relies on the trustworthiness of the company or the server. A related question is whether researchers who use large data sets understand DP and the impact that it would have on their research. A survey conducted in the project among researchers who use data from the U.S. census or related American Community Survey found that most had concerns about implementation of DP on the 2020 data. They had no formal training in DP, showed little understanding of why it was needed, and thought it would negatively impact their research.\n\n \nFor generating synthetic datasets under differential privacy, the main challenge is how to deal with high-dimensional data.  It is known that answering too many queries accurately enables reconstruction of the dataset, violating privacy.  Therefore, one has to decide what information to preserve (and implicitly what to give up) in a private dataset.  The researchers developed the PrivSyn approach, which uses a large number of low-degree marginals to represent a dataset.  For example, given around 100 attributes, PrivSyn uses all one-way marginals and around 500 two-way marginals. A two-way marginal (specified by two attributes) is a frequency distribution table, showing the number of records with each possible combination of values for the two attributes. Information that can be derived from these low-degree marginals is preserved, and high-dimensional correlation that cannot be derived from low-degree marginals is lost, as the utility cost to satisfy DP.  PrivSyn has been shown to perform well in practice. \n\n \nThis interdisciplinary project has helped support the education of several students, both in Computer Science and Psychology.  These include several female students, including a Hispanic female student.  One student supported by the project joined academia as a tenure-track faculty member. \n\n \n\n\t\t\t\t\tLast Modified: 04/29/2022\n\n\t\t\t\t\tSubmitted by: Ninghui Li"
 }
}