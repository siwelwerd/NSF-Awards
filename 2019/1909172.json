{
 "awd_id": "1909172",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CNS Core: Small: Collaborative: Content-Based Viewport Prediction Framework for Live Virtual Reality Streaming",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922935",
 "po_email": "dmedhi@nsf.gov",
 "po_sign_block_name": "Deepankar Medhi",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 171229.0,
 "awd_amount": 171229.0,
 "awd_min_amd_letter_date": "2019-08-15",
 "awd_max_amd_letter_date": "2019-08-15",
 "awd_abstract_narration": "Virtual reality (VR) video streaming has been gaining popularity recently with the rapid adoption of mobile head mounted display (HMD) devices in the consumer video market. As the cost for the immersive experience drops, VR video streaming introduces new bandwidth and performance challenges, especially in live streaming, due to the delivery of 360-degree views. This project develops a new content-based viewport prediction framework to improve the bandwidth and performance in live VR streaming, which predicts the user's viewport through a fusion of tracking the moving objects in the video, extracting the video semantics, and modeling the user's viewport of interest.\r\n\r\nThis project consists of three research thrusts. First, it develops a content-based viewport prediction framework for live VR streaming by tracking the motions and semantics of the objects. Second, it employs hardware and software techniques to facilitate real-time execution and scale the viewport prediction mechanism to a large number of users. Third, it develops evaluation frameworks to verify the functionality, performance, and scalability of the approach. The project uniquely considers the correlation between video content and user behavior, which leverages the deterministic nature of the former to conquer the randomness of the latter.\r\n\r\nWith the rapidly increasing popularity of VR systems in domain-specific immersive environments, the project will benefit several VR-related fields of studies with significant bandwidth savings and performance improvements, such as VR-based live broadcast, healthcare, and scientific visualization. Moreover, the interdisciplinary nature of the project will enhance the education and recruitment of underrepresented minorities in several science, technology, engineering, and mathematics (STEM) fields.\r\n\r\nThe project repository will be stored on a publicly accessible server (https://github.com/hwsel). All the project data will be maintained for at least five years following the end of the grant period.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yanzhi",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yanzhi Wang",
   "pi_email_addr": "yanzhiwang@northeastern.edu",
   "nsf_id": "000695637",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave.",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "736300",
   "pgm_ele_name": "Networking Technology and Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 171229.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Virtual reality (VR) video streaming has been gaining popularity recently with the rapid adoption of mobile head mounted display (HMD) devices in the consumer video market. As the cost for the immersive experience drops, VR video streaming introduces new bandwidth and performance challenges, especially in live streaming, due to the delivery of 360-degree views. This project develops a new content-based viewport prediction framework to improve the bandwidth and performance in live VR streaming, which predicts the user's viewport through a fusion of tracking the moving objects in the video, extracting the video semantics, and modeling the user's viewport of interest.&nbsp;</span><span>This project consists of three research thrusts. First, it develops a content-based viewport prediction framework for live VR streaming by tracking the motions and semantics of the objects. Second, it employs hardware and software techniques to facilitate real-time execution and scale the viewport prediction mechanism to a large number of users. Third, it develops evaluation frameworks to verify the functionality, performance, and scalability of the approach. The project uniquely considers the correlation between video content and user behavior, which leverages the deterministic nature of the former to conquer the randomness of the latter.</span></p>\r\n<p>The major research of this project of PI Wang's group include: (1) Developing a content-based viewport prediction framework for live VR streaming, using an effective DNN-based fusion of the motion of objects in the video and semantics extracted from the video. (2) Developing hardware/software techniques to facilitate real-time CPU/GPU/FPGA implementation and enhance scalability to a large number of users. (3) Developing an evaluation framework to verify the functionality, performance, and scalability of the proposed system.&nbsp;<span>PI Wang has over 20 presentations in academia and industry and has over 10 open-source projects with impact on research community.&nbsp;<span>The research activities of this project have been partially incorporated into</span><span>&nbsp;ECEE7398 Advances in Deep Learning and Data Science 5220 Supervised Machine Learning, created and taught by PI Wang in the ECE department at Northeastern University (enrolled by around 140 graduate/undergraduate students annually, respectively). The relevant course topics include deep learning acceleration techniques.</span></span></p><br>\n<p>\n Last Modified: 03/04/2025<br>\nModified by: Yanzhi&nbsp;Wang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nVirtual reality (VR) video streaming has been gaining popularity recently with the rapid adoption of mobile head mounted display (HMD) devices in the consumer video market. As the cost for the immersive experience drops, VR video streaming introduces new bandwidth and performance challenges, especially in live streaming, due to the delivery of 360-degree views. This project develops a new content-based viewport prediction framework to improve the bandwidth and performance in live VR streaming, which predicts the user's viewport through a fusion of tracking the moving objects in the video, extracting the video semantics, and modeling the user's viewport of interest.This project consists of three research thrusts. First, it develops a content-based viewport prediction framework for live VR streaming by tracking the motions and semantics of the objects. Second, it employs hardware and software techniques to facilitate real-time execution and scale the viewport prediction mechanism to a large number of users. Third, it develops evaluation frameworks to verify the functionality, performance, and scalability of the approach. The project uniquely considers the correlation between video content and user behavior, which leverages the deterministic nature of the former to conquer the randomness of the latter.\r\n\n\nThe major research of this project of PI Wang's group include: (1) Developing a content-based viewport prediction framework for live VR streaming, using an effective DNN-based fusion of the motion of objects in the video and semantics extracted from the video. (2) Developing hardware/software techniques to facilitate real-time CPU/GPU/FPGA implementation and enhance scalability to a large number of users. (3) Developing an evaluation framework to verify the functionality, performance, and scalability of the proposed system.PI Wang has over 20 presentations in academia and industry and has over 10 open-source projects with impact on research community.The research activities of this project have been partially incorporated intoECEE7398 Advances in Deep Learning and Data Science 5220 Supervised Machine Learning, created and taught by PI Wang in the ECE department at Northeastern University (enrolled by around 140 graduate/undergraduate students annually, respectively). The relevant course topics include deep learning acceleration techniques.\t\t\t\t\tLast Modified: 03/04/2025\n\n\t\t\t\t\tSubmitted by: YanzhiWang\n"
 }
}