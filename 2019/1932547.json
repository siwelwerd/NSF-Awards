{
 "awd_id": "1932547",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Small: Collaborative Research: RF Sensing for Sign Language Driven Smart Environments",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Corman",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-03-31",
 "tot_intn_awd_amt": 366252.0,
 "awd_amount": 416252.0,
 "awd_min_amd_letter_date": "2019-08-05",
 "awd_max_amd_letter_date": "2022-07-27",
 "awd_abstract_narration": "Deaf individuals who rely on American Sign Language (ASL) as their primary mode of communication heavily rely on technology as an assistive device. Yet, many technologies are designed for hearing individuals, which precludes the Deaf community from benefiting from advances, which, if designed to be compatible with ASL, could in fact generate tangible improvements in their quality of life. This proposal aims at transforming ubiquitous sensing technologies through the integration of a new sensing modality - radio frequency (RF) sensing - into smart environments designed to respond to the needs of ASL users. RF sensors are uniquely desirable for this application because they are non-contact, can operate in the dark or through-the-wall, protect privacy, and bring to bear a new type of information that will aid in ASL understanding: namely, the micro-Doppler signature, which is reflective of the time-varying velocity profiles of motion.  Thus, RF sensing is uniquely suited to capture the rapid progression of dynamic sign sequences that is characteristic of ASL usage.  This collaborative project not only brings to bear, for the first time, a linguistic perspective to RF-based motion recognition, but also a physics-based machine learning approach achieved through integration of kinematics with deep learning.  In this way, the project aims at 1) improving ASL recognition technologies and the design of smart environments for deaf individuals, 2) augmenting the tools linguists use to analyze language and related cognitive processes, and 3) advancing machine learning approaches specifically geared towards RF signal classification.\r\n\r\n     The project is focused on developing signal processing algorithms for leveraging the unique aspects of RF sensing towards understanding of ASL and related linguistic features.  More specifically, three aspects of ASL recognition are considered: classification of pre-defined ASL words and phrases, design of RF-sensing based dynamic sequence segmentation algorithms, and differentiation of daily activities from communicative sign language gestures. Novel ways of visualizing and representing RF data in one, two, and three dimensions will be investigated, both for extraction of linguistic features and as inputs to deep neural networks.   Novel techniques will be developed for classification of three-dimensional time-varying data streams, the generation of synthetic RF data samples that have improved kinematic fidelity and realism, sequential classification and segmentation, as well as discrimination of daily motion from communicative signing. The critical experiments conducted during this project will result in a one-of-a-kind dataset of multi-frequency RF sensor network and Kinect(tm) sensor measurements of ASL signs, which will be made publicly available.  The project directly engages the Deaf community through support and interaction of the Alabama Institute of Deaf and Blind (AIDB) and Gallaudet University as part of a needs-driven approach to communicative and assistive technology design, which will ultimately serve personal, professional, and educational needs of the Deaf community.\r\n\r\nThis project is jointly funded by the Cyber Physical Systems Program and the Established Program to Stimulate Competitive Research (EPSCoR).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sevgi",
   "pi_last_name": "Gurbuz",
   "pi_mid_init": "Z",
   "pi_sufx_name": "",
   "pi_full_name": "Sevgi Z Gurbuz",
   "pi_email_addr": "szgurbuz@ncsu.edu",
   "nsf_id": "000762665",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Evguenia",
   "pi_last_name": "Malaia",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Evguenia Malaia",
   "pi_email_addr": "eamalaia@ua.edu",
   "nsf_id": "000595605",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Chris",
   "pi_last_name": "Crawford",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Chris S Crawford",
   "pi_email_addr": "crawford@cs.ua.edu",
   "nsf_id": "000751344",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Darrin",
   "pi_last_name": "Griffin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Darrin Griffin",
   "pi_email_addr": "djgriffin1@ua.edu",
   "nsf_id": "000769663",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Alabama Tuscaloosa",
  "inst_street_address": "801 UNIVERSITY BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "TUSCALOOSA",
  "inst_state_code": "AL",
  "inst_state_name": "Alabama",
  "inst_phone_num": "2053485152",
  "inst_zip_code": "354012029",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "AL07",
  "org_lgl_bus_name": "UNIVERSITY OF ALABAMA",
  "org_prnt_uei_num": "TWJWHYEM8T63",
  "org_uei_num": "RCNJEHZ83EV6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Alabama",
  "perf_str_addr": "",
  "perf_city_name": "Tuscaloosa",
  "perf_st_code": "AL",
  "perf_st_name": "Alabama",
  "perf_zip_code": "354860005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "AL07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  },
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 366252.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Although there are over 1 million users of American Sign Language (ASL) in the U.S. and Canada, who rely on technology as an assistive device to navigate communication/language barriers that status quo society often creates, many emerging technologies are designed for hearing individuals via voice-controlled devices.&nbsp; This precludes the Deaf/Hard-of-Hearing (HoH) community from benefitting from advances in technology, which, if designed to be compatible with ASL, could in fact generate tangible improvements to their quality of life.&nbsp; This proposal aims at transforming ubiquitous sensing technologies through the integration of a new sensing modality - radio frequency (RF) sensing - into smart environments designed to respond to the needs of ASL users.&nbsp; RF sensors are uniquely desirable for this application because they are non-contact, can operate in the dark or through-the-wall, protect privacy, and bring to bear a new type of information that aides in ASL recognition:&nbsp; namely, the micro-Doppler signature, which is reflective of the time-varying velocity profiles of motion.&nbsp; Thus, RF sensing can capture the kinematics of the rapid progression of dynamic sign sequences that is characteristic of ASL usage.&nbsp; This collaborative project not only brings to bear, for the first time, a linguistic perspective to RF-based motion recognition, but also a physics-based machine learning approach achieved through the integration of kinematics with deep learning.&nbsp; In this way, the project aims at 1) designing effective RF-sensing based ASL-sensitive smart environments and user interfaces for human-computer interaction (HCI), 2) exploiting RF sensing data as a tool for linguistic analysis of ASL, and 3) advancing machine learning approaches specifically geared towards RF signal classification. &nbsp;</p>\n<p>This project pioneered unique machine learning approaches for leveraging the unique aspects of RF sensing towards understanding ASL.&nbsp; First, physics-aware generative adversarial networks (PhGANs) were designed that more effectively represent the kinematics of ASL in synthetically generated RF data that provide more effective training of deep neural networks (DNNs) when data is limited.&nbsp; The project showed that practice of utilizing imitation signing in DNN training and validation - the use of hearing non-signers who attempt to replicate ASL by watching videos - is not effective for model training and significant overestimates true classification accuracy.&nbsp; Moreover, Contrastive learning techniques designed specifically for RF signal classification are proposed that leverage multi-resolution signal processing to improve model training with fewer data. Second, novel ways of exploiting the 4D RF data tensor of range, velocity, angle, and time, is developed within a joint-domain multi-input multi-task learning (JD-MIMTL) framework to enable segmentation and sequential classification of ASL signs mixed with daily activities in continuous RF recordings.&nbsp; Techniques for boosting the signal-to-noise ratio of the RF signal from the user of interest are proposed for multi-person scenarios that are typical in daily life.&nbsp; Third, fractal complexity is proposed as a method for distinguishing signing from daily activities in RF data acquired in real-world scenarios.&nbsp; During the course of the project, a dictionary of 140 ASL signs and over 150 sentences were recorded.&nbsp; The single-word classification accuracy of video was found to be comparable to that of video for a 100-sign dataset, validating the benefit of RF as a modality that primarily leverages kinematic and temporal features of the data, complementing the spatial features extracted by video.&nbsp; Thus, this project not only demonstrates the feasibility of RF-only ASL recognition, but also the benefits of fusing RF and video data. &nbsp;</p>\n<p>Finally, the project developed and tested an interactive ASL-Chess game for collecting RF and video data of the natural expression of ASL words and connected discourse in an entertaining way that eliminates the need for a human study director and minimizes the burden of data collection on Deaf/HoH participants.&nbsp; Participants naturally sign via interaction in the game, allowing for the recording of ASL that is more comparable to real-life and reflects linguistic artifacts, such as regional dialects, individual articulation, and co-articulation.&nbsp; The ASL-Chess game can enable the acquisition of large amounts of natural ASL signing, which is essential to the advancement of AI/ML algorithms for sign language processing (SLP) technologies.&nbsp; RF data acquired during this project has been made publicly available.</p>\n<p>This project directly engaged the Deaf community through the support and collaboration of the Alabama Institute of the Deaf and Blind (AIDB) and Gallaudet University - the world's only university designed to be barrier-free for Deaf/HoH students, located in Washington D.C. - as part of a needs-driven, Deaf-centric approach to communicative and assistive technology design, which will ultimately serve personal, professional and educational needs of the Deaf community.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/26/2023<br>\n\t\t\t\t\tModified by: Sevgi&nbsp;Z&nbsp;Gurbuz</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687780872526_Figure2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687780872526_Figure2--rgov-800width.jpg\" title=\"Concept of RF-Enabled ASL Recognition for HCI\"><img src=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687780872526_Figure2--rgov-66x44.jpg\" alt=\"Concept of RF-Enabled ASL Recognition for HCI\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">RF-Sensing for ASL-Controlled Smart Environments & Personal Devices</div>\n<div class=\"imageCredit\">Sevgi Zubeyde Gurbuz</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Sevgi&nbsp;Z&nbsp;Gurbuz</div>\n<div class=\"imageTitle\">Concept of RF-Enabled ASL Recognition for HCI</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687781013609_Figure1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687781013609_Figure1--rgov-800width.jpg\" title=\"RF Sensing: A New Modality for ASL Recognition\"><img src=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687781013609_Figure1--rgov-66x44.jpg\" alt=\"RF Sensing: A New Modality for ASL Recognition\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This project proposes RF Sensing as a new modality for ASL recognition.  This chart summarizes advantages and disadvantages of various sensing modalities and shows the benefit RF sensing can bring for ASL recognition.</div>\n<div class=\"imageCredit\">Sevgi Zubeyde Gurbuz</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Sevgi&nbsp;Z&nbsp;Gurbuz</div>\n<div class=\"imageTitle\">RF Sensing: A New Modality for ASL Recognition</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687784037346_img_9311_v2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687784037346_img_9311_v2--rgov-800width.jpg\" title=\"ASL-Chess Game Close-Up\"><img src=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687784037346_img_9311_v2--rgov-66x44.jpg\" alt=\"ASL-Chess Game Close-Up\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A close-up picture of the laptop-based ASL-Chess game that records RF sensing and webcam data of ASL articulated to move chess pieces during the game.</div>\n<div class=\"imageCredit\">Emre Kurtoglu</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Sevgi&nbsp;Z&nbsp;Gurbuz</div>\n<div class=\"imageTitle\">ASL-Chess Game Close-Up</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687784420099_ASL_Chess_InAction--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687784420099_ASL_Chess_InAction--rgov-800width.jpg\" title=\"Deaf participant playing ASL-Chess\"><img src=\"/por/images/Reports/POR/2023/1932547/1932547_10630069_1687784420099_ASL_Chess_InAction--rgov-66x44.jpg\" alt=\"Deaf participant playing ASL-Chess\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Image of a Deaf participant articulating ASL to move a piece while playing ASL-Chess</div>\n<div class=\"imageCredit\">Emre Kurtoglu</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Sevgi&nbsp;Z&nbsp;Gurbuz</div>\n<div class=\"imageTitle\">Deaf participant playing ASL-Chess</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nAlthough there are over 1 million users of American Sign Language (ASL) in the U.S. and Canada, who rely on technology as an assistive device to navigate communication/language barriers that status quo society often creates, many emerging technologies are designed for hearing individuals via voice-controlled devices.  This precludes the Deaf/Hard-of-Hearing (HoH) community from benefitting from advances in technology, which, if designed to be compatible with ASL, could in fact generate tangible improvements to their quality of life.  This proposal aims at transforming ubiquitous sensing technologies through the integration of a new sensing modality - radio frequency (RF) sensing - into smart environments designed to respond to the needs of ASL users.  RF sensors are uniquely desirable for this application because they are non-contact, can operate in the dark or through-the-wall, protect privacy, and bring to bear a new type of information that aides in ASL recognition:  namely, the micro-Doppler signature, which is reflective of the time-varying velocity profiles of motion.  Thus, RF sensing can capture the kinematics of the rapid progression of dynamic sign sequences that is characteristic of ASL usage.  This collaborative project not only brings to bear, for the first time, a linguistic perspective to RF-based motion recognition, but also a physics-based machine learning approach achieved through the integration of kinematics with deep learning.  In this way, the project aims at 1) designing effective RF-sensing based ASL-sensitive smart environments and user interfaces for human-computer interaction (HCI), 2) exploiting RF sensing data as a tool for linguistic analysis of ASL, and 3) advancing machine learning approaches specifically geared towards RF signal classification.  \n\nThis project pioneered unique machine learning approaches for leveraging the unique aspects of RF sensing towards understanding ASL.  First, physics-aware generative adversarial networks (PhGANs) were designed that more effectively represent the kinematics of ASL in synthetically generated RF data that provide more effective training of deep neural networks (DNNs) when data is limited.  The project showed that practice of utilizing imitation signing in DNN training and validation - the use of hearing non-signers who attempt to replicate ASL by watching videos - is not effective for model training and significant overestimates true classification accuracy.  Moreover, Contrastive learning techniques designed specifically for RF signal classification are proposed that leverage multi-resolution signal processing to improve model training with fewer data. Second, novel ways of exploiting the 4D RF data tensor of range, velocity, angle, and time, is developed within a joint-domain multi-input multi-task learning (JD-MIMTL) framework to enable segmentation and sequential classification of ASL signs mixed with daily activities in continuous RF recordings.  Techniques for boosting the signal-to-noise ratio of the RF signal from the user of interest are proposed for multi-person scenarios that are typical in daily life.  Third, fractal complexity is proposed as a method for distinguishing signing from daily activities in RF data acquired in real-world scenarios.  During the course of the project, a dictionary of 140 ASL signs and over 150 sentences were recorded.  The single-word classification accuracy of video was found to be comparable to that of video for a 100-sign dataset, validating the benefit of RF as a modality that primarily leverages kinematic and temporal features of the data, complementing the spatial features extracted by video.  Thus, this project not only demonstrates the feasibility of RF-only ASL recognition, but also the benefits of fusing RF and video data.  \n\nFinally, the project developed and tested an interactive ASL-Chess game for collecting RF and video data of the natural expression of ASL words and connected discourse in an entertaining way that eliminates the need for a human study director and minimizes the burden of data collection on Deaf/HoH participants.  Participants naturally sign via interaction in the game, allowing for the recording of ASL that is more comparable to real-life and reflects linguistic artifacts, such as regional dialects, individual articulation, and co-articulation.  The ASL-Chess game can enable the acquisition of large amounts of natural ASL signing, which is essential to the advancement of AI/ML algorithms for sign language processing (SLP) technologies.  RF data acquired during this project has been made publicly available.\n\nThis project directly engaged the Deaf community through the support and collaboration of the Alabama Institute of the Deaf and Blind (AIDB) and Gallaudet University - the world's only university designed to be barrier-free for Deaf/HoH students, located in Washington D.C. - as part of a needs-driven, Deaf-centric approach to communicative and assistive technology design, which will ultimately serve personal, professional and educational needs of the Deaf community.\n\n \n\n\t\t\t\t\tLast Modified: 06/26/2023\n\n\t\t\t\t\tSubmitted by: Sevgi Z Gurbuz"
 }
}