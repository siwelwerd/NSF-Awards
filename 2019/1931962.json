{
 "awd_id": "1931962",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS:Medium:Interactive Human-Drone Partnerships in Emergency Response Scenarios",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927375",
 "po_email": "adubey@nsf.gov",
 "po_sign_block_name": "Abhishek Dubey",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2025-03-31",
 "tot_intn_awd_amt": 1186394.0,
 "awd_amount": 1233394.0,
 "awd_min_amd_letter_date": "2019-09-12",
 "awd_max_amd_letter_date": "2024-03-07",
 "awd_abstract_narration": "Small unmanned aerial, land, or submersible vehicles (drones) are increasingly used to support emergency response scenarios such as search-and-rescue, structural building fires, and medical deliveries. However, in current practice drones are typically controlled by a single operator thereby significantly limiting their potential. The proposed work will deliver a novel DroneResponse platform, representing the next generation of emergency response solutions in which semi-autonomous and self-coordinating cohorts of drones will serve as fully-fledged members of an emergency response team.   Drones will play diverse roles in each emergency response scenario - for example, using thermal imagery to map the structural integrity of a burning building, methodically searching an area for a child lost in a cornfield, or delivering a life-saving device to a person caught in a fast-flowing river.  The benefits of this project will be realized by urban and rural communities who will benefit from enhanced emergency response capabilities.  Practical lessons learned from this work will broadly contribute to the conversation around best practices for drone deployment in the community including issues related to privacy, safety, and equity.  \r\n \r\nAchieving the DroneResponse vision involves delivering novel scene recognition algorithms capable of recreating high-fidelity models of the environment under less than ideal environmental conditions.  The work addresses non-trivial cyber-physical systems (CPS) research challenges associated with (1) scene recognition, including image merging, dealing with uncertainty, and geolocating objects; (2) exploring, designing, and evaluating human-CPS interfaces that provide situational awareness and empower users to define missions and communicate current mission objectives and achievements, (3) developing algorithms to support drone autonomy and runtime adaptation with respect to mission goals established by humans, (4) developing a framework for coordinating image recognition algorithms with real-time drone command and control, and finally (5) evaluating DroneResponse in real-world scenarios.  Researchers will leverage user-centered design principles to develop human-CPS interfaces that support situational awareness designed to enable emergency responders to make informed decisions. The end goal is to empower human operators and drones to work collaboratively to save lives, minimize property damage,  gather critical information, and contribute to the success of a mission across diverse emergency scenarios.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jane",
   "pi_last_name": "Huang",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Jane L Huang",
   "pi_email_addr": "JaneClelandHuang@nd.edu",
   "nsf_id": "000304869",
   "pi_start_date": "2019-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Walter",
   "pi_last_name": "Scheirer",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Walter J Scheirer",
   "pi_email_addr": "wscheire@nd.edu",
   "nsf_id": "000590702",
   "pi_start_date": "2019-09-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Notre Dame",
  "inst_street_address": "940 GRACE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "NOTRE DAME",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "5746317432",
  "inst_zip_code": "465565708",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "IN02",
  "org_lgl_bus_name": "UNIVERSITY OF NOTRE DAME DU LAC",
  "org_prnt_uei_num": "FPU6XGFXMBE9",
  "org_uei_num": "FPU6XGFXMBE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Notre Dame",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "465565637",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "IN02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 1186394.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 11000.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 20000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-10978126-7fff-5790-7d12-d86af7cb67b5\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Small Unmanned Aerial, Land, or Submersible Vehicles (drones) are increasingly used to support time-critical tasks such as search-and-rescue missions, fire surveillance, and medical deliveries. While current deployments typically involve manual control of a single small Uncrewed Aerial System (sUAS), this project aimed to enable a swarm of semi-autonomous sUAS to partner with humans in emergency response missions. To realize this vision, the team developed the DroneResponse platform and pursued four integrated objectives: (1) enabling UAVs to dynamically construct semantically rich scenes using onboard computer vision; (2) creating mission models that allow sUAS to perform emergency tasks with minimal human input; (3) supporting effective human-drone teaming through interactive interfaces; and (4) deploying these capabilities in real-world settings.</span></p>\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The Smart Mission Planner (SMP) combines onboard computer vision with real-time mission planning. The planner segments aerial scenes from satellite imagery, into semantically labeled terrain maps, which are fused with data from terrain models to identify mission-critical environmental features. This contextual model supports downstream tasks such as geolocation, navigation, victim detection, and prioritized task assignments. The SMP operates within a distributed MAPE-K (Monitor, Analyze, Plan, Execute, Knowledge) architecture, in which each sUAS runs its own autonomy loop while contributing to shared mission goals. The system decomposes high-level objectives into executable subtasks, assigns them to available drones, and reallocates work in response to changing mission conditions. Airspace safety is supported by a custom air-leasing system that guarantees minimum separation between vehicles, and supports incremental leasing. The SMP was validated in simulation with up to 120 drones and deployed on hundreds of physical field tests with up to four sUAS per mission, demonstrating robust behavior for mid-mission task reprioritization and dynamic reconfiguration. Humans, supported by real-time situational awareness, supervise autonomous sUAS that perform independent yet coordinated tasks to achieve prioritized mission goals.</span></p>\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To support aerial person detection under real-world conditions, the team developed the NOMAD dataset, a curated collection of 42,825 aerial images. NOMAD captures 100 unique individuals from five aerial altitudes, with each image annotated with bounding boxes and occlusion level labels. The dataset addresses the challenges of aerial SAR scenarios, including small, distant figures, partial or full occlusions, lighting variability, and complex visual backgrounds.</span></p>\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Building on NOMAD, the team introduced Psych-Occlusion, a novel training technique that integrates human behavioral data into CV models. A large-scale crowdsourced study was conducted using over 5,000 NOMAD images, where participants were asked to find the person in the picture. The resulting Psych-ER dataset includes detailed behavioral data such as cursor paths, dwell times, fixations, accuracy, and response latency. This data was used to construct a psychophysical loss function for training detection models. Unlike traditional losses that optimize bounding box tightness, this loss encourages human-aligned localization, prioritizing detection of presence under occlusion rather than geometric precision. Integrated into a RetinaNet model and evaluated using NOMAD&rsquo;s occlusion and distance conditions, Psych-Occlusion significantly improved detection in high-occlusion and long-range scenarios, without degrading performance in clearer cases or increasing inference cost. This approach marks a substantial step toward more resilient and human-aligned computer vision systems for deployment on sUAS in real-world emergency response environments.</span></p>\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Deploying autonomous CV-enabled sUAS in the field required the development of a robust onboard compute solution. The team designed EmCUBE (Edge Matrix Cube), a modular, high-performance edge computing platform for UAVs. Each EmCUBE unit integrates a Jetson Orin NX for real-time AI inference and CV processing, a MeshRadio module for peer-to-peer swarm communication, and a layer for diverse navigation and environmental sensors. Onboard voltage modulation ensures power stability, while active cooling supports sustained performance under high computational loads. The EmCUBE platform is compatible with PX4 and Ardupilot-based flight stacks and enables plug-and-play deployment across different UAVs. EmCUBEs were integrated into Inspired Flight IF1200A and Aurelia X6 Max platforms, powering the Smart Mission Planner during live flights. The ability to execute AI-driven autonomy entirely at the edge makes EmCUBE a critical enabler for future field-ready, scalable UAV deployments.</span></p>\r\n<p><br /><br /></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/02/2025<br>\nModified by: Jane&nbsp;L&nbsp;Huang</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743625323125_emCUBE--rgov-214x142.PNG\" original=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743625323125_emCUBE--rgov-800width.PNG\" title=\"Edge Matrix Cube\"><img src=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743625323125_emCUBE--rgov-66x44.PNG\" alt=\"Edge Matrix Cube\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The EmCUBE (Edge Matrix Cube) is a modular edge computing unit designed for UAVs, featuring Jetson Orin NX for onboard AI, MeshRadio for swarm comms, sensor integration, and active cooling. It is deployed here on an Inspired Flight 1200A running Ardupilot.</div>\n<div class=\"imageCredit\">Jason Matthew Brauer</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jane&nbsp;L&nbsp;Huang\n<div class=\"imageTitle\">Edge Matrix Cube</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626332361_fig_main--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626332361_fig_main--rgov-800width.jpg\" title=\"PsychOcclusion\"><img src=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626332361_fig_main--rgov-66x44.jpg\" alt=\"PsychOcclusion\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Psych-ER is a behavioral dataset derived from NOMAD, capturing human search patterns across 5,000 aerial images in emergency response scenarios. Collected via MTurk, it includes gaze data, accuracy, and response time, enabling a psychophysical loss to enhance CV models like RetinaNet.</div>\n<div class=\"imageCredit\">Arturo Miguel Russell Bernal</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jane&nbsp;L&nbsp;Huang\n<div class=\"imageTitle\">PsychOcclusion</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626869287_IMG_7736--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626869287_IMG_7736--rgov-800width.png\" title=\"Air-Leasing\"><img src=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626869287_IMG_7736--rgov-66x44.png\" alt=\"Air-Leasing\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Two drones fly close to each other during a demo at Skyway-36 in Oklahoma. Safe separation distance was guaranteed through use of DroneResponse's Airleasing technology.</div>\n<div class=\"imageCredit\">DroneResponse</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jane&nbsp;L&nbsp;Huang\n<div class=\"imageTitle\">Air-Leasing</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626228546_fig_NOMAD--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626228546_fig_NOMAD--rgov-800width.jpg\" title=\"NOMAD\"><img src=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626228546_fig_NOMAD--rgov-66x44.jpg\" alt=\"NOMAD\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">NOMAD (Natural Occluded Multi-scale Aerial Dataset) provides annotated aerial imagery from emergency response scenarios, capturing diverse individuals under varied occlusion levels and altitudes. It addresses CV challenges in real-world sUAS deployments.</div>\n<div class=\"imageCredit\">Arturo Miguel Russell Bernal</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jane&nbsp;L&nbsp;Huang\n<div class=\"imageTitle\">NOMAD</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626070686_SMP--rgov-214x142.PNG\" original=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626070686_SMP--rgov-800width.PNG\" title=\"Smart Mission Planner\"><img src=\"/por/images/Reports/POR/2025/1931962/1931962_10641707_1743626070686_SMP--rgov-66x44.PNG\" alt=\"Smart Mission Planner\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Smart Mission Planner assigns color-coded sUAS to matching sectors, with each zone showing real-time drone coverage. EmCUBE edge AI enables onboard vision and adaptive responses to weather, terrain, or detected objects.</div>\n<div class=\"imageCredit\">Pedro Alarcon Granadeno</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jane&nbsp;L&nbsp;Huang\n<div class=\"imageTitle\">Smart Mission Planner</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nSmall Unmanned Aerial, Land, or Submersible Vehicles (drones) are increasingly used to support time-critical tasks such as search-and-rescue missions, fire surveillance, and medical deliveries. While current deployments typically involve manual control of a single small Uncrewed Aerial System (sUAS), this project aimed to enable a swarm of semi-autonomous sUAS to partner with humans in emergency response missions. To realize this vision, the team developed the DroneResponse platform and pursued four integrated objectives: (1) enabling UAVs to dynamically construct semantically rich scenes using onboard computer vision; (2) creating mission models that allow sUAS to perform emergency tasks with minimal human input; (3) supporting effective human-drone teaming through interactive interfaces; and (4) deploying these capabilities in real-world settings.\r\n\n\nThe Smart Mission Planner (SMP) combines onboard computer vision with real-time mission planning. The planner segments aerial scenes from satellite imagery, into semantically labeled terrain maps, which are fused with data from terrain models to identify mission-critical environmental features. This contextual model supports downstream tasks such as geolocation, navigation, victim detection, and prioritized task assignments. The SMP operates within a distributed MAPE-K (Monitor, Analyze, Plan, Execute, Knowledge) architecture, in which each sUAS runs its own autonomy loop while contributing to shared mission goals. The system decomposes high-level objectives into executable subtasks, assigns them to available drones, and reallocates work in response to changing mission conditions. Airspace safety is supported by a custom air-leasing system that guarantees minimum separation between vehicles, and supports incremental leasing. The SMP was validated in simulation with up to 120 drones and deployed on hundreds of physical field tests with up to four sUAS per mission, demonstrating robust behavior for mid-mission task reprioritization and dynamic reconfiguration. Humans, supported by real-time situational awareness, supervise autonomous sUAS that perform independent yet coordinated tasks to achieve prioritized mission goals.\r\n\n\nTo support aerial person detection under real-world conditions, the team developed the NOMAD dataset, a curated collection of 42,825 aerial images. NOMAD captures 100 unique individuals from five aerial altitudes, with each image annotated with bounding boxes and occlusion level labels. The dataset addresses the challenges of aerial SAR scenarios, including small, distant figures, partial or full occlusions, lighting variability, and complex visual backgrounds.\r\n\n\nBuilding on NOMAD, the team introduced Psych-Occlusion, a novel training technique that integrates human behavioral data into CV models. A large-scale crowdsourced study was conducted using over 5,000 NOMAD images, where participants were asked to find the person in the picture. The resulting Psych-ER dataset includes detailed behavioral data such as cursor paths, dwell times, fixations, accuracy, and response latency. This data was used to construct a psychophysical loss function for training detection models. Unlike traditional losses that optimize bounding box tightness, this loss encourages human-aligned localization, prioritizing detection of presence under occlusion rather than geometric precision. Integrated into a RetinaNet model and evaluated using NOMADs occlusion and distance conditions, Psych-Occlusion significantly improved detection in high-occlusion and long-range scenarios, without degrading performance in clearer cases or increasing inference cost. This approach marks a substantial step toward more resilient and human-aligned computer vision systems for deployment on sUAS in real-world emergency response environments.\r\n\n\nDeploying autonomous CV-enabled sUAS in the field required the development of a robust onboard compute solution. The team designed EmCUBE (Edge Matrix Cube), a modular, high-performance edge computing platform for UAVs. Each EmCUBE unit integrates a Jetson Orin NX for real-time AI inference and CV processing, a MeshRadio module for peer-to-peer swarm communication, and a layer for diverse navigation and environmental sensors. Onboard voltage modulation ensures power stability, while active cooling supports sustained performance under high computational loads. The EmCUBE platform is compatible with PX4 and Ardupilot-based flight stacks and enables plug-and-play deployment across different UAVs. EmCUBEs were integrated into Inspired Flight IF1200A and Aurelia X6 Max platforms, powering the Smart Mission Planner during live flights. The ability to execute AI-driven autonomy entirely at the edge makes EmCUBE a critical enabler for future field-ready, scalable UAV deployments.\r\n\n\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 04/02/2025\n\n\t\t\t\t\tSubmitted by: JaneLHuang\n"
 }
}