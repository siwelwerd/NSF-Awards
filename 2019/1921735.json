{
 "awd_id": "1921735",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CompCog: Template Contrast and Saliency (TCAS) Toolbox: a tool to visualize parallel attentive evaluation of scenes",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 656928.0,
 "awd_amount": 656928.0,
 "awd_min_amd_letter_date": "2019-07-22",
 "awd_max_amd_letter_date": "2019-07-22",
 "awd_abstract_narration": "One of the most common visual tasks humans do is use their eyes to find objects in the world around them. This task involves analyzing all the visual objects and backgrounds in the scene. This is a complicated task because the brain has to separate objects from the background. The brain also has to process the color, shape, and size of all objects. The aim of the research is to build a mathematical model that can find objects in scenes, despite the difficulty of the problem. The model is inspired by the visual system. It uses two ways to process information. First, it uses central vision to get a fine-grained analysis of the object it is looking at. Second, it also uses peripheral vision, which is the area around and away from central vision. Peripheral vision can analyze several objects at the same time but is less precise than central vision. The ultimate goal of the project is to develop a free, open-source software toolbox that anyone can use. The toolbox will visualize how the visual system processes complex scenes. It will determine which regions in a scene should be ignored and which regions the eyes should focus on. One strength of the proposal is that it makes specific predictions that can be tested in various fields of neuroscience. It might also lead to improvements in visual aids for visually impaired individuals because it can guide users toward areas in a scene that are likely to contain the target object.\r\n\r\nThe starting point for the proposed work is a mathematically explicit model of goal-directed visual processing. The model incorporates two components of visual complexity: a parameter that measures the visual difference between objects in the scene and the object the observer is looking for (the target) and a parameter that measures how similar objects in the scene are to one another. The preliminary work indicated that the model is very capable of predicting how long it will take observers to find targets in visually complex scenes. The first two goals of the present research aim at evaluating other components of visual complexity to improve the model and its ability to predict visual processing in more complex visual scenes. The experiments in Goals 1 and 2 will help determine how to combine the visual qualities of objects (such as color, shape and texture) as well as how to account for the contrast between objects and their background. Results from Goals 1 and 2 will directly guide the development of a computational toolbox. The toolbox will allow users to visualize visual processing of simple and complex scenes and make predictions about where observers are likely to move their eyes as a function of their current goals (freely inspect the scene or find a specific object within it). The proposed work combines behavioral psychophysics and computational simulations (Goals 1 and 2), toolbox implementation and eye-tracking validation (Goal 3). The merits of the toolbox include the fact that: 1) it combines different types of visual processing (visual conspicuity contrast and target template contrast), 2) it can predict eye movements over different time scales, and 3) it can evaluate the contribution of these two types of processing to performance. This implementation is important because the contribution of these two processes is known to vary as a function of search goals (free-view vs. goal-directed) and search strategy adopted by observers (active search vs. passive search). Finally, another innovation of the toolbox is that it will be able to make predictions when targets are only defined in abstract terms, that is, when observers only have vague descriptions about the item they are supposed to find in the scene, which is particularly challenging for current computer vision systems to achieve.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Simona",
   "pi_last_name": "Buetti",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Simona L Buetti",
   "pi_email_addr": "buetti@illinois.edu",
   "nsf_id": "000692274",
   "pi_start_date": "2019-07-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Hummel",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "John E Hummel",
   "pi_email_addr": "jehummel@uiuc.edu",
   "nsf_id": "000238256",
   "pi_start_date": "2019-07-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alejandro",
   "pi_last_name": "Lleras",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alejandro Lleras",
   "pi_email_addr": "alejandrolleras@gmail.com",
   "nsf_id": "000205178",
   "pi_start_date": "2019-07-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "HAB 506 S. Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 656928.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focused on the study of how humans search for specific objects (targets) within visual scenes. One of the primary objectives was to investigate how the complexity of visual information within a scene, as well as the complexity of the target itself, influences the speed at which humans can process all the information present in the scene. We developed mathematical models to accomplish two main goals:</p>\n<p>&nbsp;</p>\n<p>1.&nbsp;&nbsp;&nbsp; Predict the time required to complete visual inspections under increasingly complex scenes and backgrounds.</p>\n<p>2.&nbsp;&nbsp;&nbsp; Characterize how information from different visual dimensions, such as color, shape, and texture, is integrated by the visual system to guide attention within the scene.</p>\n<p>&nbsp;</p>\n<p>Additionally, the project introduced a novel tool for assessing the attention-grabbing properties of objects within a scene relative to their immediate surroundings. To evaluate the speed at which humans locate targets within scenes, we employed eye-movement monitoring, considering two key parameters:</p>\n<p>&nbsp;</p>\n<p>1.&nbsp;&nbsp;&nbsp; The extent to which the visual characteristics of objects in the scene attract attention.</p>\n<p>2.&nbsp;&nbsp;&nbsp; The degree of resemblance between those objects and the target.</p>\n<p>&nbsp;</p>\n<p>This work resulted in the publication of multiple peer-reviewed papers, including an invited review in the prestigious journal Nature Reviews: Psychology. All data and experimental code developed throughout the project are open-source and can be accessed on the Open Science Framework online repository, enabling other researchers to test and expand upon our methods and models. The Python code created as part of this project is also available on GitHub.</p>\n<p>&nbsp;</p>\n<p>Furthermore, this project served as an educational opportunity for 41 undergraduate students and provided training and professional development for five graduate students. In total, we conducted 54 experiments over the award period.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/03/2023<br>\n\t\t\t\t\tModified by: Simona&nbsp;L&nbsp;Buetti</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project focused on the study of how humans search for specific objects (targets) within visual scenes. One of the primary objectives was to investigate how the complexity of visual information within a scene, as well as the complexity of the target itself, influences the speed at which humans can process all the information present in the scene. We developed mathematical models to accomplish two main goals:\n\n \n\n1.    Predict the time required to complete visual inspections under increasingly complex scenes and backgrounds.\n\n2.    Characterize how information from different visual dimensions, such as color, shape, and texture, is integrated by the visual system to guide attention within the scene.\n\n \n\nAdditionally, the project introduced a novel tool for assessing the attention-grabbing properties of objects within a scene relative to their immediate surroundings. To evaluate the speed at which humans locate targets within scenes, we employed eye-movement monitoring, considering two key parameters:\n\n \n\n1.    The extent to which the visual characteristics of objects in the scene attract attention.\n\n2.    The degree of resemblance between those objects and the target.\n\n \n\nThis work resulted in the publication of multiple peer-reviewed papers, including an invited review in the prestigious journal Nature Reviews: Psychology. All data and experimental code developed throughout the project are open-source and can be accessed on the Open Science Framework online repository, enabling other researchers to test and expand upon our methods and models. The Python code created as part of this project is also available on GitHub.\n\n \n\nFurthermore, this project served as an educational opportunity for 41 undergraduate students and provided training and professional development for five graduate students. In total, we conducted 54 experiments over the award period.\n\n \n\n\t\t\t\t\tLast Modified: 11/03/2023\n\n\t\t\t\t\tSubmitted by: Simona L Buetti"
 }
}