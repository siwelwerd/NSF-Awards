{
 "awd_id": "1850975",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Doctoral Dissertation Research: An Ethnographic Study of the Making of Mental Health Care Artificial Intelligence",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927783",
 "po_email": "jmantz@nsf.gov",
 "po_sign_block_name": "Jeffrey Mantz",
 "awd_eff_date": "2019-03-01",
 "awd_exp_date": "2020-12-31",
 "tot_intn_awd_amt": 11378.0,
 "awd_amount": 11378.0,
 "awd_min_amd_letter_date": "2019-02-26",
 "awd_max_amd_letter_date": "2019-02-26",
 "awd_abstract_narration": "This multi-sited ethnographic investigation will examine how technology companies design artificial intelligence (AI) to provide mental health diagnostic and therapeutic care. What factors guide the many decisions that translate mental health care into algorithms, a set of instructions carried out by AI? There is tremendous variation in how these companies make mental health into an object that is measurable and/or treatable via algorithm. Facial expressions, vocal tone, word choice, or smartphone usage patterns become potential sources of data that communicate mental health states. In other words, companies do not simply collect data; they determine what counts as data sources and establish the correlation of this data to mental health states, and they are doing so in very different ways. In addition to providing funding for the training of a graduate student in anthropology in scientific methods of rigorous data collection and analysis, the project would broadly disseminate its findings to organizations invested in discovering more effective means of improving mental health diagnosis and treatment.\r\n\r\nValerie Black, under the supervision of Dr. Karen Nakamura at the University of California at Berkeley, will explore how technology companies design AI for mental health diagnosis and treatment. The algorithms developed by companies that gather and analyze mental health data are typically proprietary. Since these algorithms are being developed at a time when AI can determine matters ranging from bank loan eligibility to criminal sentencing, there is a growing call for AI decision-making transparency. This project asks how mental health technology companies in the US and Japan (two major, interconnected centers of AI development) are addressing this concern, and how that impacts the ways in which they use AI to provide care. Research will take place at three AI companies (two in the U.S. and one in Japan) in the understudied startup technology sector. Data collection includes interviews and participant observation with innovators and investors, psychological service leads, and engineers and developers. Data will be analyzed to ascertain what objectives, ethical and practical considerations, and relationships inform the mental health care algorithms. This ethnographic investigation aims to further scientific understanding of how different configurations of and approaches to AI make possible an array of different (and sometimes clashing) objectives, strategies, and core beliefs about what constitutes optimal ways of caring for mental health. The project will advance theoretical understanding at the intersections of anthropology, psychology, and STS around datafication in health care and how mental care, as experienced across different cultural terrains, is translated into algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Karen",
   "pi_last_name": "Nakamura",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Karen Nakamura",
   "pi_email_addr": "karen.nakamura@berkeley.edu",
   "nsf_id": "000597676",
   "pi_start_date": "2019-02-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Valerie",
   "pi_last_name": "Black",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Valerie E Black",
   "pi_email_addr": "vblack@berkeley.edu",
   "nsf_id": "000755132",
   "pi_start_date": "2019-02-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "232 Kroeber Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947203710",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760500",
   "pgm_ele_name": "Cult Anthro DDRI"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "1390",
   "pgm_ref_txt": "CULTURAL ANTHROPOLOGY"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 11378.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span class=\"s1\">The primary fieldsites for this project were two startup companies, both of which use artificial intelligence to deliver care: a Japanese mental health videogame startup and an American mental health chatbot startup. Participant-observation and interviews conducted with startup founders and workers at these research sites made it possible to investigate how non-clinical therapeutic AI care is designed and delivered within a startup setting.&nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">Through AI, startups are attempting to solve an ongoing global public health issue. Given tremendous caregiver scarcity alongside ongoing stigma and other barriers in accessing mental health care (such as cost, difficulty in navigating and applying health benefiting, and limited times/locations at which to access services), the argument for turning to AI caregivers is compelling. But this move also raises important questions about the role of startups in public health and managed care. Simply put, the success markers for a startup are not necessarily the best indicators for understanding mental health impacts and care outcomes. An important outcome of this research is a call to recognize the ways in which possibilities and expectations for AI in mental health care may be shaped, and perhaps even limited, by the requirements and business practices of startups.</span></p>\n<p class=\"p1\"><span class=\"s1\">The scope of AI within these (non-diagnostic, therapeutic) care tools/approaches is relatively limited. Still, both fieldsites were in the process of developing products that use machine learning (ML), indicating that the presently overall limited role of AI in this industry may change significantly and rapidly in the near future. <br /></span></p>\n<p class=\"p1\"><span class=\"s1\">For all that the scope of AI in these applications may be limited at present, the contexts in which this technology is called upon can be extremely serious.&nbsp;</span><span class=\"s1\">For example, observations at the therapeutic chatbot fieldsite indicated that detecting user-communicated suicide ideation is an ongoing priority.&nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">This mode of care walks a fascinating line between entertainment and medical treatment. This allows for a degree of slippage and fluctuation in the forms these services take. At both fieldsites, much effort was devoted towards establishing proof of concept&mdash;something that swiftly flourished in the unfolding COVID-19 crisis, with its simultaneous restrictions of access to conventional mental health care alongside the crisis&rsquo; imposition of greater mental health burdens. During the research period for this project, regulation (such as certification requirements with the US FDA or Japan&rsquo;s PMDA) was typically moot for most of the fieldsites&rsquo; products and services; however, increased interest in and use of such modes of care may lead to more stringent regulation in the future.</span></p>\n<p class=\"p1\"><span class=\"s1\">Ultimately, this research reveals that there is an ongoing (and growing) open-endedness around the question of what &ldquo;mental health care&rdquo; is, and who is best equipped to provide it.&nbsp;</span></p>\n<p id=\"docs-internal-guid-de69b35a-7fff-a598-4974-504286283c77\" dir=\"ltr\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/08/2021<br>\n\t\t\t\t\tModified by: Valerie&nbsp;E&nbsp;Black</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The primary fieldsites for this project were two startup companies, both of which use artificial intelligence to deliver care: a Japanese mental health videogame startup and an American mental health chatbot startup. Participant-observation and interviews conducted with startup founders and workers at these research sites made it possible to investigate how non-clinical therapeutic AI care is designed and delivered within a startup setting. \nThrough AI, startups are attempting to solve an ongoing global public health issue. Given tremendous caregiver scarcity alongside ongoing stigma and other barriers in accessing mental health care (such as cost, difficulty in navigating and applying health benefiting, and limited times/locations at which to access services), the argument for turning to AI caregivers is compelling. But this move also raises important questions about the role of startups in public health and managed care. Simply put, the success markers for a startup are not necessarily the best indicators for understanding mental health impacts and care outcomes. An important outcome of this research is a call to recognize the ways in which possibilities and expectations for AI in mental health care may be shaped, and perhaps even limited, by the requirements and business practices of startups.\nThe scope of AI within these (non-diagnostic, therapeutic) care tools/approaches is relatively limited. Still, both fieldsites were in the process of developing products that use machine learning (ML), indicating that the presently overall limited role of AI in this industry may change significantly and rapidly in the near future. \n\nFor all that the scope of AI in these applications may be limited at present, the contexts in which this technology is called upon can be extremely serious. For example, observations at the therapeutic chatbot fieldsite indicated that detecting user-communicated suicide ideation is an ongoing priority. \nThis mode of care walks a fascinating line between entertainment and medical treatment. This allows for a degree of slippage and fluctuation in the forms these services take. At both fieldsites, much effort was devoted towards establishing proof of concept&mdash;something that swiftly flourished in the unfolding COVID-19 crisis, with its simultaneous restrictions of access to conventional mental health care alongside the crisis\u2019 imposition of greater mental health burdens. During the research period for this project, regulation (such as certification requirements with the US FDA or Japan\u2019s PMDA) was typically moot for most of the fieldsites\u2019 products and services; however, increased interest in and use of such modes of care may lead to more stringent regulation in the future.\nUltimately, this research reveals that there is an ongoing (and growing) open-endedness around the question of what \"mental health care\" is, and who is best equipped to provide it. \n \n\n\t\t\t\t\tLast Modified: 04/08/2021\n\n\t\t\t\t\tSubmitted by: Valerie E Black"
 }
}