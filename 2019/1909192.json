{
 "awd_id": "1909192",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: Dynamic Light Transport Acquisition and Applications to Computational Illumination",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2019-08-06",
 "awd_max_amd_letter_date": "2019-08-06",
 "awd_abstract_narration": "Cameras that view a dynamic scene typically capture interactions of moving objects with light. Computer vision algorithms can use these measurements to infer properties of these objects, such as depth, motion and appearance. However, there is a subtler, richer visual back-story that occurs as an object moves in a scene, and usually these effects are ignored in traditional algorithms, sometimes causing errors. This project studies all the interactions of light with dynamic scenes, which we term as dynamic light transport, and the goal is to understand and recover effects such as multiple reflections and scattering as objects move in a scene. The innovations of the project include new computational cameras and projectors to capture light transport for dynamic scenes, and to explore new physics-based and data-driven algorithms to exploit this information for improved computer vision and graphics applications. The project further seeks to include broadening access to computing education and research through curriculum material and capstone experiences which emphasize the intersection of light transport and digital media as well as outreach to middle and high school students in summer programs to discover imaging and optics applications.\r\n\r\nThis research focuses on designing new light transport acquisition frameworks to capture dynamic scenes, characterization of dynamic light transport properties including sparsity and low-rank, and algorithms to exploit this information for computer vision applications. In particular, the project focuses on three main objectives.  The first is design of an MEMs-based optical scanner coupled with high frame rate cameras to capture the full set of light transport paths at extremely fast timescales.  The second contribution is new algorithms for adaptive light transport sampling using both physics-based and data-driven priors for light transport interpolation via generalized light transport flow.  Finally, the project will provide applications of dynamic light transport for 3D scanning of deformable, moving, and specular objects. These innovations are evaluated in an integrated testbed via the optical scanner and the collection of a dataset of dynamic light transport for real-world scenes.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Suren",
   "pi_last_name": "Jayasuriya",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Suren Jayasuriya",
   "pi_email_addr": "sjayasur@asu.edu",
   "nsf_id": "000753682",
   "pi_start_date": "2019-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona Board of Regents on behalf of Arizona State University",
  "perf_str_addr": "P.O. Box 876011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852876011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research project has developed new imaging systems and frameworks for the capture of light transport in dynamic, moving scenes. A novel MEMS mirror-based flying spot projector was developed along with a high-speed camera to capture dynamic light transport matrices which was published initially in a journal publication in the IEEE Transactions on Computational Imaging in 2020 and subsequently improved in a journal paper in Optics Express 2021. This dynamic light transport capture enabled various applications in computational relighting including relighting moving scenes, dual videography which allowed synthesizing videos from the perspective of the projector to look around corners, and finally performing separation of direct and indirect light paths in a scene. In addition, this project has developed a novel synchronized projector-camera known as slope disparity gating which is able to selectively capture certain light paths triangulated in the scene. This resulted in novel capabilities including visual-based touch sensing published in IEEE Access 2021 and depth-dependent light transport capture and relighting published in IEEE Transactions on Computational Imaging in 2022. Finally, the project developed a series of works on non-line-of-sight (NLOS) imaging, particularly tracking human pedestrians walking outside the field-of-view of the camera and/or illumination source. NLOS tracking and activity recognition was achieved by using intelligent illumination for complex, non-planar line-of-sight walls published at the IEEE Winter Conference on Applications of Computer Vision in 2024, and passive/no-illumination NLOS tracking with a camera mounted on an unmanned aerial vehicle was demonstrated in a paper published at the IEEE/RSJ International Conference on Intelligent Robots and Systems in 2024. The project has also spurred other research avenues in computational photography and physics-based computer vision including pinhole videography and event-based sensing. In addition to scholarly research, this project had broader impacts including programming at the Digital Culture Summer Institute that featured activities surrounding computational cameras and photography for middle school teachers and students and the development of a new course EEE 598: Physics-based Computer Vision that features topics surrounding light transport and computational imaging.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 08/30/2024<br>\nModified by: Suren&nbsp;Jayasuriya</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis research project has developed new imaging systems and frameworks for the capture of light transport in dynamic, moving scenes. A novel MEMS mirror-based flying spot projector was developed along with a high-speed camera to capture dynamic light transport matrices which was published initially in a journal publication in the IEEE Transactions on Computational Imaging in 2020 and subsequently improved in a journal paper in Optics Express 2021. This dynamic light transport capture enabled various applications in computational relighting including relighting moving scenes, dual videography which allowed synthesizing videos from the perspective of the projector to look around corners, and finally performing separation of direct and indirect light paths in a scene. In addition, this project has developed a novel synchronized projector-camera known as slope disparity gating which is able to selectively capture certain light paths triangulated in the scene. This resulted in novel capabilities including visual-based touch sensing published in IEEE Access 2021 and depth-dependent light transport capture and relighting published in IEEE Transactions on Computational Imaging in 2022. Finally, the project developed a series of works on non-line-of-sight (NLOS) imaging, particularly tracking human pedestrians walking outside the field-of-view of the camera and/or illumination source. NLOS tracking and activity recognition was achieved by using intelligent illumination for complex, non-planar line-of-sight walls published at the IEEE Winter Conference on Applications of Computer Vision in 2024, and passive/no-illumination NLOS tracking with a camera mounted on an unmanned aerial vehicle was demonstrated in a paper published at the IEEE/RSJ International Conference on Intelligent Robots and Systems in 2024. The project has also spurred other research avenues in computational photography and physics-based computer vision including pinhole videography and event-based sensing. In addition to scholarly research, this project had broader impacts including programming at the Digital Culture Summer Institute that featured activities surrounding computational cameras and photography for middle school teachers and students and the development of a new course EEE 598: Physics-based Computer Vision that features topics surrounding light transport and computational imaging.\n\n\n\t\t\t\t\tLast Modified: 08/30/2024\n\n\t\t\t\t\tSubmitted by: SurenJayasuriya\n"
 }
}