{
 "awd_id": "1940931",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Learning Language in Simulation for Real Robot Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2019-12-01",
 "awd_exp_date": "2021-11-30",
 "tot_intn_awd_amt": 219516.0,
 "awd_amount": 219516.0,
 "awd_min_amd_letter_date": "2019-08-15",
 "awd_max_amd_letter_date": "2019-08-15",
 "awd_abstract_narration": "While robots are rapidly becoming more capable and ubiquitous, their\r\nutility is still severely limited by the inability of regular users to\r\ncustomize their behaviors. This EArly Grant for Exploratory Research (EAGER) \r\nwill explore how examples of language, gaze, and other communications can be collected from a\r\nvirtual interaction with a robot in order to learn how robots can\r\ninteract better with end users. Current robots' difficulty of use and\r\ninflexibility are major factors preventing them from being more\r\nbroadly available to populations that might benefit, such as\r\naging-in-place seniors. One promising solution is to let users control\r\nand teach robots with natural language, an intuitive and comfortable\r\nmechanism. This has led to active research in the area of grounded\r\nlanguage acquisition: learning language that refers to and is informed\r\nby the physical world. Given the complexity of robotic systems, there\r\nis growing interest in approaches that take advantage of the latest in\r\nvirtual reality technology, which can lower the barrier of entry to\r\nthis research.\r\n\r\nThis EAGER project develops infrastructure that will lay the necessary\r\ngroundwork for applying simulation-to-reality approaches to natural\r\nlanguage interactions with robots. This project aims to bootstrap\r\nrobots' learning to understand language, using a combination of data\r\ncollected in a high-fidelity virtual reality environment with\r\nsimulated robots and real-world testing on physical robots. A person\r\nwill interact with simulated robots in virtual reality, and his or her\r\nactions and language will be recorded. By integrating with existing\r\nrobotics technology, this project will model the connection between\r\nthe language people use and the robot's perceptions and actions.\r\nNatural language descriptions of what is happening in simulation will\r\nbe obtained and used to train a joint model of language and simulated\r\npercepts as a way to learn grounded language. The effectiveness of the\r\nframework and algorithms will be measured on automatic\r\nprediction/generation tasks and transferability of learned models to a\r\nreal, physical robot. This work will serve as a proof of concept for\r\nthe value of combining robotics simulation with human interaction, as\r\nwell as providing interested researchers with resources to bootstrap\r\ntheir own work.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Cynthia",
   "pi_last_name": "Matuszek",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Cynthia Matuszek",
   "pi_email_addr": "cmat@umbc.edu",
   "nsf_id": "000690099",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Donald",
   "pi_last_name": "Engel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Donald Engel",
   "pi_email_addr": "donengel@umbc.edu",
   "nsf_id": "000668900",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Francis",
   "pi_last_name": "Ferraro",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Francis Ferraro",
   "pi_email_addr": "ferraro@umbc.edu",
   "nsf_id": "000763127",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland Baltimore County",
  "inst_street_address": "1000 HILLTOP CIR",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4104553140",
  "inst_zip_code": "212500001",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND BALTIMORE COUNTY",
  "org_prnt_uei_num": "",
  "org_uei_num": "RNKYWXURFRL5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland, Baltimore County",
  "perf_str_addr": "1000 Hilltop Circle",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212500002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 219516.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-ed84a1fa-7fff-9e1b-596d-6f5e6fbb983c\" style=\"line-height: 1.38; text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">While robots are rapidly becoming more capable and ubiquitous, their utility is still severely limited by the inability of regular users to customize robots' behaviors. In this project, we have explored how examples of language, gaze, and other communication can be collected from virtual interaction with a robot in order to learn how robots can interact better with end users. Current robots' difficulty of use and inflexibility are major factors preventing them from being more broadly available to populations that might benefit from the technology.&nbsp; One promising solution is to let users control and teach robots with natural spoken language, an intuitive and comfortable mechanism. Our work explores how users can use language with robots to describe objects in the world around them and explain how to do tasks such as helping someone pack a lunch. In order to accomplish this in a reproducible, shareable way, we take advantage of virtual reality to explore human-robot interactions and lower the barrier of entry to this research. We have built a simulation environment in which people can engage in these language-based interactions, and explored using it to understand how people talk about the world around them.</span></p>\n<p style=\"line-height: 1.38; text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span></p>\n<p style=\"line-height: 1.38; text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This infrastructure lays the necessary groundwork for applying simulation-to-reality approaches to natural language interactions with robots. We have studied bootstrapping robots' learning to understand language, using a combination of data collected in a high-fidelity virtual reality environment with simulated robots and real-world testing on physical robots. A person's interactions (actions, language) with simulated robots in virtual reality are recorded. By integrating with existing robotics technology, this project models the connection between the language people use and the robot's perceptions and actions.&nbsp; Natural language descriptions of what is happening in simulation are used to train a joint model of language and simulated percepts as a way to learn grounded language. The simulator we have developed is being used to explore learning from language labels of objects and tasks, and is being deployed to other sites to support further robot learning from language.&nbsp; This work serves as a proof of concept for the value of combining robotics simulation with human interaction, as well as providing interested researchers with resources to bootstrap their own work.&nbsp; We hope the simulation environment and scenarios will spark the interest of others in pursuing these questions. The research advances the field by improving the ability of robotic systems to learn from users, interact with users, and improve their own abilities to communicate with users.</span></p>\n<p style=\"line-height: 1.38; text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span></p>\n<p style=\"line-height: 1.38; text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Robotics as a research area requires significant resources to pursue, but language-based interaction has significant potential to improve the usability and usefulness of current robots. Demonstrating that meaningful robotics research involving people can be performed partially in simulation lowers this bar and makes these avenues of research more available to interested scientists.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/11/2022<br>\n\t\t\t\t\tModified by: Cynthia&nbsp;Matuszek</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "While robots are rapidly becoming more capable and ubiquitous, their utility is still severely limited by the inability of regular users to customize robots' behaviors. In this project, we have explored how examples of language, gaze, and other communication can be collected from virtual interaction with a robot in order to learn how robots can interact better with end users. Current robots' difficulty of use and inflexibility are major factors preventing them from being more broadly available to populations that might benefit from the technology.  One promising solution is to let users control and teach robots with natural spoken language, an intuitive and comfortable mechanism. Our work explores how users can use language with robots to describe objects in the world around them and explain how to do tasks such as helping someone pack a lunch. In order to accomplish this in a reproducible, shareable way, we take advantage of virtual reality to explore human-robot interactions and lower the barrier of entry to this research. We have built a simulation environment in which people can engage in these language-based interactions, and explored using it to understand how people talk about the world around them.\n \nThis infrastructure lays the necessary groundwork for applying simulation-to-reality approaches to natural language interactions with robots. We have studied bootstrapping robots' learning to understand language, using a combination of data collected in a high-fidelity virtual reality environment with simulated robots and real-world testing on physical robots. A person's interactions (actions, language) with simulated robots in virtual reality are recorded. By integrating with existing robotics technology, this project models the connection between the language people use and the robot's perceptions and actions.  Natural language descriptions of what is happening in simulation are used to train a joint model of language and simulated percepts as a way to learn grounded language. The simulator we have developed is being used to explore learning from language labels of objects and tasks, and is being deployed to other sites to support further robot learning from language.  This work serves as a proof of concept for the value of combining robotics simulation with human interaction, as well as providing interested researchers with resources to bootstrap their own work.  We hope the simulation environment and scenarios will spark the interest of others in pursuing these questions. The research advances the field by improving the ability of robotic systems to learn from users, interact with users, and improve their own abilities to communicate with users.\n \nRobotics as a research area requires significant resources to pursue, but language-based interaction has significant potential to improve the usability and usefulness of current robots. Demonstrating that meaningful robotics research involving people can be performed partially in simulation lowers this bar and makes these avenues of research more available to interested scientists.\n\n\t\t\t\t\tLast Modified: 05/11/2022\n\n\t\t\t\t\tSubmitted by: Cynthia Matuszek"
 }
}