{
 "awd_id": "1908347",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Physics-Based Methods for Sampling from Continuous Distributions",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2019-06-17",
 "awd_max_amd_letter_date": "2019-06-17",
 "awd_abstract_narration": "The aim of this project is to develop scalable algorithms with provable guarantees for sampling from probability distributions. Sampling is a fundamental primitive that is deployed in a variety of summarization, inference, training, and simulation related tasks in several areas including machine learning, statistics, optimization, theoretical computer science, and molecular dynamics. The approach of this project would be to study certain widely deployed heuristics for sampling that have their roots in classical physics, but for whom a rigorous understanding of the conditions under which they work is lacking. This project will contribute to speeding up core tasks in emergent machine-learning-based technologies that rely on optimization and sampling, advance the technical core of computer science by infusing it with ideas from physics, and prepare a new generation of graduate and undergraduate students that are adept at these tasks.\r\n\r\nAn important example is the class of Hamiltonian Monte Carlo (HMC) Markov chain-based methods that possess the remarkable ability to take long steps in the domain and come with the promise of dimension-independent running times for sampling from regular-enough distributions. This ability of HMC to take long steps is derived from certain laws of physics - Hamiltonian dynamics - that underlie this method. This project expects to leverage this physics viewpoint to enable both the design of algorithms and their analysis by providing the right equations of motion, potential functions, a principled way of tuning parameters, and arriving at regularity conditions on the distribution. Making progress on the mathematical aspects of these physics-based methods requires a synthesis of algorithms, optimization, probability, geometry, dynamical systems, and intuition from physics. A key aspect of this project is to train the students involved in the relevant technical areas and inculcate a physics-based approach to thinking about algorithms in a wide audience.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nisheeth",
   "pi_last_name": "Vishnoi",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Nisheeth K Vishnoi",
   "pi_email_addr": "nisheeth.vishnoi@yale.edu",
   "nsf_id": "000791655",
   "pi_start_date": "2019-06-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "AK Watson Hall",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208285",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-e768190a-7fff-3281-5723-ddf5eb0c1527\"> </span></p>\n<p dir=\"ltr\"><span>Many fundamental tasks in disciplines such as machine learning, optimization, statistics, and theoretical computer science, rely on the ability to&nbsp; sample from a&nbsp; probability distributions.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The focus of this project was to develop new models and and efficient algorithms for sampling problems arising in a variety of areas. Major findings include:</span></p>\n<ul>\n<li>&nbsp;A fundamental problem in the area of sampling is to develop fast algorithms to sample from logconcave densities supported on convex bodies. One set of works&nbsp; improves upon the state of the art algorithms for approximately sampling from logconcave densities supported on polytopes. These results rely on new variants of the ball walk and the Dikin walk, two well-studied Markov chains to sample from such distributions. Moreover, a new method to convert samples that are approximate in the total variation distance to the stronger infinity distance was developed. This latter measure of approximation is important for the applicability of sampling algorithms to the area of differentially private optimization.</li>\n<li>Another line of findings culminates in efficient algorithms to sample from exponential densities on nonconvex manifolds such as the space of matrices with specified eigenvalues. The realization is that these manifolds are orbits under certain actions of Lie groups such as the unitary group. Applications include an efficient algorithm to sample from the Harish-Chandra&ndash;Itzykson&ndash;Zuber densities important in mathematics and physics, an efficient algorithm to sample from complex versions of matrix Langevin distributions studied in statistics,&nbsp; an efficient algorithm to sample from continuous max-entropy distributions on unitary orbits, which implies an efficient algorithm to sample a pure quantum state from the entropy-maximizing ensemble representing a given density matrix, and) an efficient algorithm for differentially private low-rank approximation.</li>\n<li>Generative Adversarial Networks,&nbsp; discovered in the context of unsupervised learning, have had far reaching implications to science, engineering, and society. The goal is to learn a probability distribution from given samples.&nbsp; However, training GANs remained challenging (in part) due to the lack of convergent algorithms for nonconvex-nonconcave min-max optimization. A set of outcomes include developing a computationally efficient alternative to the min-max optimization approach to learn the probability distribution &ndash; the greedy adversarialy equilibrium. This new notion is based on the realization that the max player, in most applications, is a greedy algorithm. The new algorithms include a new first-order algorithm which is particularly suited to train GANs. This algorithm is guaranteed to converge to an equilibrium, is competitive in terms of time and memory with gradient descent-ascent and, most importantly, GANs trained using it seem to be stable.</li>\n<li>&nbsp;A line of work has also focussed on developed efficient sampling algorithms for summarizing time-series data important in the area of econometrics.</li>\n</ul>\n<ol> </ol>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/19/2023<br>\n\t\t\t\t\tModified by: Nisheeth&nbsp;K&nbsp;Vishnoi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nMany fundamental tasks in disciplines such as machine learning, optimization, statistics, and theoretical computer science, rely on the ability to  sample from a  probability distributions. \nThe focus of this project was to develop new models and and efficient algorithms for sampling problems arising in a variety of areas. Major findings include:\n\n A fundamental problem in the area of sampling is to develop fast algorithms to sample from logconcave densities supported on convex bodies. One set of works  improves upon the state of the art algorithms for approximately sampling from logconcave densities supported on polytopes. These results rely on new variants of the ball walk and the Dikin walk, two well-studied Markov chains to sample from such distributions. Moreover, a new method to convert samples that are approximate in the total variation distance to the stronger infinity distance was developed. This latter measure of approximation is important for the applicability of sampling algorithms to the area of differentially private optimization.\nAnother line of findings culminates in efficient algorithms to sample from exponential densities on nonconvex manifolds such as the space of matrices with specified eigenvalues. The realization is that these manifolds are orbits under certain actions of Lie groups such as the unitary group. Applications include an efficient algorithm to sample from the Harish-Chandra&ndash;Itzykson&ndash;Zuber densities important in mathematics and physics, an efficient algorithm to sample from complex versions of matrix Langevin distributions studied in statistics,  an efficient algorithm to sample from continuous max-entropy distributions on unitary orbits, which implies an efficient algorithm to sample a pure quantum state from the entropy-maximizing ensemble representing a given density matrix, and) an efficient algorithm for differentially private low-rank approximation.\nGenerative Adversarial Networks,  discovered in the context of unsupervised learning, have had far reaching implications to science, engineering, and society. The goal is to learn a probability distribution from given samples.  However, training GANs remained challenging (in part) due to the lack of convergent algorithms for nonconvex-nonconcave min-max optimization. A set of outcomes include developing a computationally efficient alternative to the min-max optimization approach to learn the probability distribution &ndash; the greedy adversarialy equilibrium. This new notion is based on the realization that the max player, in most applications, is a greedy algorithm. The new algorithms include a new first-order algorithm which is particularly suited to train GANs. This algorithm is guaranteed to converge to an equilibrium, is competitive in terms of time and memory with gradient descent-ascent and, most importantly, GANs trained using it seem to be stable.\n A line of work has also focussed on developed efficient sampling algorithms for summarizing time-series data important in the area of econometrics.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 07/19/2023\n\n\t\t\t\t\tSubmitted by: Nisheeth K Vishnoi"
 }
}