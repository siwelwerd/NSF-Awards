{
 "awd_id": "1850029",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CIF: Unifying Scheduling and Optimization Techniques to Speed-up Distributed Stochastic Gradient Descent",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2019-03-01",
 "awd_exp_date": "2022-02-28",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2019-01-31",
 "awd_max_amd_letter_date": "2019-01-31",
 "awd_abstract_narration": "Stochastic gradient descent (SGD) is at the core of state-of-the-art supervised learning, which is revolutionizing inference and decision-making in many diverse applications such as self-driving cars, robotics, personalized search and recommendations, and medical diagnosis. Thus, improving the speed of stochastic gradient descent is a timely and important research problem. Due to the massive scale of neural network models and training data sets used today, it has become advantageous to parallelize SGD across multiple computing nodes. Although parallelizing SGD boosts the amount of data processed per iteration, it exposes the algorithm to unpredictable node slowdown and communication delays stemming from variability in the computing infrastructure. The goal of this project is to design provably fast SGD algorithms that easily lend themselves to distributed implementations, and are robust to fluctuations in computation and network delays as well as unpredictable node failures. This project can assist in making machine learning universally accessible, without requiring access to expensive high-performance computing infrastructure. An open-source implementation of the resulting adaptive distributed SGD algorithms will be released. The research outcomes will also be incorporated into two new machine learning classes at Carnegie Mellon University, and into curriculum development and research sampler workshops for K-12 teachers and students.\r\n\r\nThe speed of single-node SGD is typically measured in terms of the convergence of training error with respect to the number of iterations. In distributed SGD, the runtime per iteration depends on system-level factors such as the computation delays at worker nodes and the gradient aggregation mechanism. Thus, there is a critical need to understand the error convergence with respect to the wall-clock time rather than the number of iterations. This project will improve the true convergence of distributed SGD with respect to wall-clock time by jointly optimizing the runtime-per-iteration and error-versus-iterations. It will consider two popular distributed SGD frameworks, the parameter server model and the communication-efficient SGD model. The research is expected to provide novel runtime and error analyses of distributed SGD in these frameworks and design the first adaptive distributed SGD algorithms that strike the best error-runtime trade-off.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gauri",
   "pi_last_name": "Joshi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gauri Joshi",
   "pi_email_addr": "gaurij@andrew.cmu.edu",
   "nsf_id": "000732900",
   "pi_start_date": "2019-01-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7797",
   "pgm_ref_txt": "COMM & INFORMATION FOUNDATIONS"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Stochastic gradient descent (SGD) is at the core optimization algorithm used to train machine learning models, which are transforming many diverse fields including technology, healthcare, manufacturing, transportation. Thus, improving the speed of stochastic gradient descent is a timely and important research problem. Due to the massive scale of neural network models and training data sets used today, SGD is typically implemented in a distributed manner. Distributing the computation of gradients across different nodes&nbsp;exposes the algorithm to unpredictable node slowdown and communication delays stemming from variability in the computing infrastructure. The goal of this project was to design provably fast SGD algorithms that are robust to computation and communication delays. During the course of this project, we studied two canonical distributed SGD frameworks:</p>\n<p>1) Synchronous and Asynchronous Distributed SGD:&nbsp;In the parameter server model, in every SGD iteration, a set of worker nodes compute gradients based on partitions of the dataset and send them to the parameter server, which aggregates them to update the model. In standard synchronous implementations, one or more slow workers can significantly slowdown the algorithm. Asynchronous implementations alleviate the straggling worker issue, but cause gradient staleness, which adversely affects the error convergence. In this project, we provided a fundamental understanding of these trade-offs and proposed adaptive synchronization algorithms that strike the best balance between error convergence and the runtime per iteration.</p>\n<p>2) Communication-Efficient Local-update SGD: The synchronous and asynchronous distributed SGD algorithms described above require constant communication between the parameter server and the worker nodes. This can be expensive and slow, especially when worker nodes are edge devices, which is the standard in the emerging paradigm of federated learning. Therefore, we designed and analyzed local-update SGD algorithms where the worker nodes perform multiple local SGD updates and the resulting models are only periodically aggregated by the parameter server. We were the first to provide a tight convergence analysis of local-update SGD using a novel framework called the Cooperative SGD. In this project, we also designed an adaptive communication strategy to strike the best error-runtime trade-off.&nbsp;</p>\n<p><span>The speed of SGD is typically measured in terms of the convergence of training error with respect to the number of iterations. In distributed SGD, the runtime per iteration depends on system-level factors such as the computation delays at worker nodes and the gradient aggregation mechanism. Thus, it is critical to understand the error convergence with respect to the wall-clock time rather than the number of iterations. The work done during this project was one of the first to analyze and improve the true convergence of distributed SGD with respect to wall-clock time by jointly optimizing the runtime-per-iteration and error-versus-iterations. It produced novel runtime and error analyses and several adaptive distributed SGD algorithms that strike the best error-runtime trade-off.&nbsp;</span></p>\n<p>The broader societal impact of this project is that it makes machine learning universally accessible, without requiring access to expensive high-performance computing infrastructure. The research outcomes were incorporated into two new machine learning classes at Carnegie Mellon University: an introductory machine learning class and an advanced distributed and federated learning class. The PI is currently working on a book on distributed ML algorithms, which will serve as teaching material for future classes. She is also actively involved in outreach activities, especially for women pursuing careers in data science.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/02/2022<br>\n\t\t\t\t\tModified by: Gauri&nbsp;Joshi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nStochastic gradient descent (SGD) is at the core optimization algorithm used to train machine learning models, which are transforming many diverse fields including technology, healthcare, manufacturing, transportation. Thus, improving the speed of stochastic gradient descent is a timely and important research problem. Due to the massive scale of neural network models and training data sets used today, SGD is typically implemented in a distributed manner. Distributing the computation of gradients across different nodes exposes the algorithm to unpredictable node slowdown and communication delays stemming from variability in the computing infrastructure. The goal of this project was to design provably fast SGD algorithms that are robust to computation and communication delays. During the course of this project, we studied two canonical distributed SGD frameworks:\n\n1) Synchronous and Asynchronous Distributed SGD: In the parameter server model, in every SGD iteration, a set of worker nodes compute gradients based on partitions of the dataset and send them to the parameter server, which aggregates them to update the model. In standard synchronous implementations, one or more slow workers can significantly slowdown the algorithm. Asynchronous implementations alleviate the straggling worker issue, but cause gradient staleness, which adversely affects the error convergence. In this project, we provided a fundamental understanding of these trade-offs and proposed adaptive synchronization algorithms that strike the best balance between error convergence and the runtime per iteration.\n\n2) Communication-Efficient Local-update SGD: The synchronous and asynchronous distributed SGD algorithms described above require constant communication between the parameter server and the worker nodes. This can be expensive and slow, especially when worker nodes are edge devices, which is the standard in the emerging paradigm of federated learning. Therefore, we designed and analyzed local-update SGD algorithms where the worker nodes perform multiple local SGD updates and the resulting models are only periodically aggregated by the parameter server. We were the first to provide a tight convergence analysis of local-update SGD using a novel framework called the Cooperative SGD. In this project, we also designed an adaptive communication strategy to strike the best error-runtime trade-off. \n\nThe speed of SGD is typically measured in terms of the convergence of training error with respect to the number of iterations. In distributed SGD, the runtime per iteration depends on system-level factors such as the computation delays at worker nodes and the gradient aggregation mechanism. Thus, it is critical to understand the error convergence with respect to the wall-clock time rather than the number of iterations. The work done during this project was one of the first to analyze and improve the true convergence of distributed SGD with respect to wall-clock time by jointly optimizing the runtime-per-iteration and error-versus-iterations. It produced novel runtime and error analyses and several adaptive distributed SGD algorithms that strike the best error-runtime trade-off. \n\nThe broader societal impact of this project is that it makes machine learning universally accessible, without requiring access to expensive high-performance computing infrastructure. The research outcomes were incorporated into two new machine learning classes at Carnegie Mellon University: an introductory machine learning class and an advanced distributed and federated learning class. The PI is currently working on a book on distributed ML algorithms, which will serve as teaching material for future classes. She is also actively involved in outreach activities, especially for women pursuing careers in data science.\n\n\t\t\t\t\tLast Modified: 06/02/2022\n\n\t\t\t\t\tSubmitted by: Gauri Joshi"
 }
}