{
 "awd_id": "1928627",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FW-HTF-RM: Collaborative Research: Augmenting Social Media Content Moderation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 849024.0,
 "awd_amount": 849024.0,
 "awd_min_amd_letter_date": "2019-09-04",
 "awd_max_amd_letter_date": "2022-08-24",
 "awd_abstract_narration": "Around the world, users of social media platforms generate millions of comments, videos, and photos per day. Within this content is dangerous material such as child pornography, sex trafficking, and terrorist propaganda. Though platforms leverage algorithmic systems to facilitate detection and removal of problematic content, decisions about whether to remove content, whether it's as benign as an off-topic comment or as dangerous as self-harm or abuse videos, are often made by humans. Companies are hiring moderators by the thousands and tens of thousands work as volunteer moderators. This work involves economic, emotional, and often physical safety risks. With social media content moderation as the focus of work and the content moderators as the workers, this project facilitates the human-technology partnership by designing new technologies to augment moderator performance. The project will improve moderators' quality of life, augment their capabilities, and help society understand how moderation decisions are made and how to support the workers who help keep the internet open and enjoyable. These advances will enable moderation efforts to keep pace with user-generated content and ensure that problematic content does not overwhelm internet users. The project includes outreach and engagement activities with academic, industry, policy-makers, and the public that ensure the project's findings and tools support broad stakeholders impacted by user-generated content and its moderation.\r\n\r\nSpecifically, the project involves five main research objectives that will be met through qualitative, historical, experimental, and computational research approaches. First, the project will improve understanding of human-in-the-loop decision making practices and mental models of moderation by conducting interviews and observations with moderators across different content domains. Second, it will assess the socioeconomic impact of technology-augmented moderation through industry personnel interviews. Third, the project will test interventions to decrease the emotional toll on human moderators and optimize their performance through a series of experiments utilizing theories of stress alleviation. Fourth, the project will design, develop, and test a suite of cognitive assistance tools for live streaming moderators. These tools will focus on removing easy decisions and helping moderators dynamically manage their emotional and cognitive capabilities. Finally, the project will employ a historical perspective to analyze companies' content moderation policies to inform legal and platform policies.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Donghee Yvette",
   "pi_last_name": "Wohn",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Donghee Yvette Wohn",
   "pi_email_addr": "wohn@njit.edu",
   "nsf_id": "000679141",
   "pi_start_date": "2019-09-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Elizabeth",
   "pi_last_name": "Petrick",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Elizabeth R Petrick",
   "pi_email_addr": "epetrick@njit.edu",
   "nsf_id": "000780541",
   "pi_start_date": "2019-09-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Aritra",
   "pi_last_name": "Dasgupta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aritra Dasgupta",
   "pi_email_addr": "aritra.dasgupta@njit.edu",
   "nsf_id": "000790917",
   "pi_start_date": "2019-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New Jersey Institute of Technology",
  "inst_street_address": "323 DR MARTIN LUTHER KING JR BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "NEWARK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "9735965275",
  "inst_zip_code": "071021824",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NJ10",
  "org_lgl_bus_name": "NEW JERSEY INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "SGBMHQ7VXNH5"
 },
 "perf_inst": {
  "perf_inst_name": "New Jersey Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "071021982",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NJ10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "103Y00",
   "pgm_ele_name": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 849024.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>By the time you have finished reading this sentence, thousands of hours of videos will have been uploaded to YouTube, Snapchat users will have shared 528,000 photos, 456,000 tweets will have been posted on X, and 510,000 comments will have been made on Facebook posts. Mixed amongst cat memes and makeup tutorials will be things most people will not want to see; child pornography, sex trafficking, terrorist propaganda, and sexism are just the start of the list.</p>\r\n<p>Negative, illegal, and/or harmful online content does not magically disappear. Behind the scenes of any user-generated content platform is a massive workforce of people who make the bad go away. Thus whether it is deleting a racist comment, or removing a self-harm video, often it is ultimately a human who has to make decisions about how to handle the risky situation. In particular, human moderators are needed for moderation scenarios where artificial intelligence has limited efficacy. Despite advancements in technology that both prevent people from saying rude things, or are able to automatically flag potentially harmful content, technologies still have limitations. Algorithms are unable to understand context or discern between a photograph of a naked child being used for child trafficking versus a famed photograph of a naked Vietnamese child running away from attackers during war, which has historical significance.</p>\r\n<p>Unfortunately, the number of human moderators is vastly outnumbered by the content that is generated, which is a concern when content creation is growing at an exponential speed and a core element of many of the major informational and social platforms today. Companies like Meta and Google are well aware of this problem, and have been on a hiring streak to deal with these issues; some other companies such as Reddit and Twitch (Amazon) have implemented systems that rely on volunteer moderators. However, as user-generated content is at the heart of many contemporary information services, content moderation is a job that is poised to have increased demand, representing a labor-intensive type of information work that is unique and essential to our information ecology.</p>\r\n<p>These sanitation workers of the information age, however, face distinct challenges. First, they spend all their time looking at the worst of the worst. This constant exposure to negativity comes with significant psychological tolls and heightened stress. Second, they have to go through large amounts of content and make rapid decisions about what to do with the content; this can be very repetitive, but also extremely stressful because moderation is not just physical labor&mdash;the decision-making aspect calls for a specific type of constant cognitive activity that prevents it from being a &ldquo;mindless&rdquo; task. Third, despite their importance in keeping the industry afloat, the sheer number of people required to fill these roles means that much of the work is outsourced, so workers are often in situations where they are under-compensated, have little access to mental healthcare, and lack benefits that direct, full-time employees would have. Volunteer moderators face similar challenges, as well as unique problems that are associated with the ad-hoc nature of how volunteers are recruited and put to work. Moreover, moderators themselves are often targets of harassment from people who are upset about their content being removed, adding emotional labor to their already taxing physical labor.&nbsp;</p>\r\n<p>This multi-year project conducted a systematic investigation of this rapidly growing workforce from a sociotechnical perspective to understand the nature of the work of human content moderators, how it is changing the industry, and explore how technology such as AI can assist and augment their abilities. Engaging experts in different fields ranging from computer science to history, we looked at labor aspect of moderation from a sociological perspective, studied the current practices of moderators to develop tools for their work and find ways to alleviate their emotional tolls, and examined how technology can augment human abilities from a historical perspective. The problem of \"bad\" content on social media has not been solved, but major breakthroughs have been achieved in terms of identifying and mitigating the challenges associated with content moderation.</p>\r\n<p>This project has enabled the training of students at both the undergraduate and graduate level, and supported research activities seven senior scholars, resulting in dozens of peer-reviewed academic publications, and development of multiple technical systems to support content moderation. The funding supported the hosting of two academic workshops on content moderation and a one-day conference, This conference invited experts from different domains to discuss the challenges and opportunities of moderation through two panels, hosted two design workshops to brainstorm how to better design systems, and held a student research competition in which graduate students presented their work.</p><br>\n<p>\n Last Modified: 02/28/2025<br>\nModified by: Donghee Yvette&nbsp;Wohn</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nBy the time you have finished reading this sentence, thousands of hours of videos will have been uploaded to YouTube, Snapchat users will have shared 528,000 photos, 456,000 tweets will have been posted on X, and 510,000 comments will have been made on Facebook posts. Mixed amongst cat memes and makeup tutorials will be things most people will not want to see; child pornography, sex trafficking, terrorist propaganda, and sexism are just the start of the list.\r\n\n\nNegative, illegal, and/or harmful online content does not magically disappear. Behind the scenes of any user-generated content platform is a massive workforce of people who make the bad go away. Thus whether it is deleting a racist comment, or removing a self-harm video, often it is ultimately a human who has to make decisions about how to handle the risky situation. In particular, human moderators are needed for moderation scenarios where artificial intelligence has limited efficacy. Despite advancements in technology that both prevent people from saying rude things, or are able to automatically flag potentially harmful content, technologies still have limitations. Algorithms are unable to understand context or discern between a photograph of a naked child being used for child trafficking versus a famed photograph of a naked Vietnamese child running away from attackers during war, which has historical significance.\r\n\n\nUnfortunately, the number of human moderators is vastly outnumbered by the content that is generated, which is a concern when content creation is growing at an exponential speed and a core element of many of the major informational and social platforms today. Companies like Meta and Google are well aware of this problem, and have been on a hiring streak to deal with these issues; some other companies such as Reddit and Twitch (Amazon) have implemented systems that rely on volunteer moderators. However, as user-generated content is at the heart of many contemporary information services, content moderation is a job that is poised to have increased demand, representing a labor-intensive type of information work that is unique and essential to our information ecology.\r\n\n\nThese sanitation workers of the information age, however, face distinct challenges. First, they spend all their time looking at the worst of the worst. This constant exposure to negativity comes with significant psychological tolls and heightened stress. Second, they have to go through large amounts of content and make rapid decisions about what to do with the content; this can be very repetitive, but also extremely stressful because moderation is not just physical laborthe decision-making aspect calls for a specific type of constant cognitive activity that prevents it from being a mindless task. Third, despite their importance in keeping the industry afloat, the sheer number of people required to fill these roles means that much of the work is outsourced, so workers are often in situations where they are under-compensated, have little access to mental healthcare, and lack benefits that direct, full-time employees would have. Volunteer moderators face similar challenges, as well as unique problems that are associated with the ad-hoc nature of how volunteers are recruited and put to work. Moreover, moderators themselves are often targets of harassment from people who are upset about their content being removed, adding emotional labor to their already taxing physical labor.\r\n\n\nThis multi-year project conducted a systematic investigation of this rapidly growing workforce from a sociotechnical perspective to understand the nature of the work of human content moderators, how it is changing the industry, and explore how technology such as AI can assist and augment their abilities. Engaging experts in different fields ranging from computer science to history, we looked at labor aspect of moderation from a sociological perspective, studied the current practices of moderators to develop tools for their work and find ways to alleviate their emotional tolls, and examined how technology can augment human abilities from a historical perspective. The problem of \"bad\" content on social media has not been solved, but major breakthroughs have been achieved in terms of identifying and mitigating the challenges associated with content moderation.\r\n\n\nThis project has enabled the training of students at both the undergraduate and graduate level, and supported research activities seven senior scholars, resulting in dozens of peer-reviewed academic publications, and development of multiple technical systems to support content moderation. The funding supported the hosting of two academic workshops on content moderation and a one-day conference, This conference invited experts from different domains to discuss the challenges and opportunities of moderation through two panels, hosted two design workshops to brainstorm how to better design systems, and held a student research competition in which graduate students presented their work.\t\t\t\t\tLast Modified: 02/28/2025\n\n\t\t\t\t\tSubmitted by: Donghee YvetteWohn\n"
 }
}