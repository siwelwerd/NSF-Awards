{
 "awd_id": "1912077",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:  Expanding a National Network for Automated Analysis of Constructed Response Assessments to Reveal Student Thinking in STEM",
 "cfda_num": "47.076",
 "org_code": "11040200",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ellen Carpenter",
 "awd_eff_date": "2018-11-14",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 127866.0,
 "awd_amount": 127866.0,
 "awd_min_amd_letter_date": "2019-01-31",
 "awd_max_amd_letter_date": "2019-01-31",
 "awd_abstract_narration": "This project is being conducted by a large team across 6 institutions that is building on already developed open-ended constructed response versions of well-established concept inventories that can be accurately assessed with already created computer automated analysis resources.  The computer-automated analyses are able to predict human ratings of students' work on these topics and have demonstrated higher inter-rater reliability than a group of trained expert human graders. Constructed response assessments reveal more about student thinking and the persistence of misconceptions than do multiple-choice questions, but require more analysis on the part of the educator. In past work, items designed to identify important disciplinary constructs were created based on prior research. The items were then administered via online course management systems where students entered responses. Lexical and statistical analysis software was used to predict expert ratings of student responses. To date, the work has focused primarily in the fields of biology and chemistry in biological contexts. \r\n\r\nThe current project is leveraging the previous research on Automated Assessment of Constructed Response (AACR), and extending the work to other institutions and other STEM disciplines. The specific goals of this project are to: 1. Create a community web portal for the Automated Assessment of Constructed Response (AACR) assessments to expand and deepen collaborations among STEM education researchers, thus providing the infrastructure for expanding the community of researchers and supporting the adoption and implementation of the innovative instructional materials by instructors at other institutions. 2. Propagate the innovations by providing instructors with professional development and long-term, ongoing support to use the assessments. This includes information about common student conceptions revealed by the questions, instructional materials for addressing conceptual barriers, and the opportunity to join a community of practitioners who are using the AACR questions and exchanging materials. 3. Expand the basic research to create and validate AACR questions in introductory chemistry, chemical engineering, and statistics. 4. Engage in ongoing project evaluation for continuous quality improvement and to document the challenges and successes the project encounters. 5. Lay the foundation for sustainability by providing interfaces for e-text publishers, Learning Management System vendors, and Massively Open Online Courses as potential revenue streams to operate and maintain the online infrastructure.\r\n\r\nIntellectual Merit:\r\nImproving STEM education requires valid and reliable instruments that provide insight into student thinking. The automated analysis of constructed response assessments have the potential to assess \"big ideas\" in STEM in a richer, more multi-faceted manner than multiple-choice instruments. This project is extending the number of these items and provide an online community where instructors may obtain, score, and contribute to the library of items and resources necessary for their analyses. \r\n\r\nBroader Impacts:\r\nThe web portal is extending the use of the products created in this project to instructors nationwide. In addition it is providing the foundation for a national collaboration of science and engineering educators interested in developing deeper conceptual assessment tools and supports and mentors postdoctoral research fellows, and graduate research assistants in STEM education research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michelle",
   "pi_last_name": "Smith",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michelle Smith",
   "pi_email_addr": "mks274@cornell.edu",
   "nsf_id": "000608105",
   "pi_start_date": "2019-01-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "373 Pine Tree Road",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148502820",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "199800",
   "pgm_ele_name": "IUSE"
  },
  {
   "pgm_ele_code": "751200",
   "pgm_ele_name": "TUES-Type 3 Project"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0413",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001314DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0416",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001617DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0417",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001718DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 46338.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 38824.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 42704.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-303702ad-7fff-296c-0a09-35703ce90f8e\">&nbsp;</span></p>\n<p dir=\"ltr\"><span>Many national calls for science, technology and mathematics (STEM) education reform over the last decade have encouraged focusing on the teaching and learning of key disciplinary concepts (e.g. evolution).&nbsp; Instructors need more information about the ideas students struggle to learn in order to adapt their teaching and support student learning. Another issue instructors face when teaching for conceptual understanding is efficiently evaluating written responses, especially for large-enrollment introductory level college courses.</span></p>\n<p dir=\"ltr\"><span>The Automated Analysis of Constructed Response project has supported and extended a national network of science education researchers beginning with five universities: Michigan State University, State University of New York - Stony Brook, University of Colorado, University of Georgia, and the University of Maine. The work expanded to include teams at the University of South Florida, South Dakota State University, and Cornell University. This project focused on: 1) exploring student thinking about key ideas in STEM using student writing; 2) developing written college-level assessment items that address key ideas within multiple STEM disciplines which can be evaluated using computer-assisted scoring models; 3) making the assessments and computer-assisted scoring models freely and publicly available on project websites; and 4) supporting a network of college faculty to use the items and the scored student writing.&nbsp; Since faculty are often hesitant to adopt new technology, and require support to implement such technology in teaching, this project (along with NSF DUE 1347740) supported professional development efforts using faculty learning communities (FLCs) at seven research-intensive universities. Participants in these FLCs used the developed items and computerized-scoring reports as part of their courses, then discussed how to use this information to better  engage students and  improve student learning.&nbsp;&nbsp;</span></p>\n<p dir=\"ltr\"><span>The intellectual merit of this project addresses the application of new technology, research into student thinking, and science faculty engagement in professional development. First, the research team applied methods from artificial intelligence to help identify and categorize ideas in student writing in STEM.&nbsp; These tools can evaluate student responses with the same level of accuracy as scientific experts, evaluate any number of new student responses in near real time, and return analysis reports to faculty.  These reports can help guide faculty in evaluating student work and adjusting instruction in the classroom to address identified student difficulties.&nbsp; Second, the team advanced knowledge about student difficulties in learning key STEM disciplinary concepts by developing a set of conceptual assessment items on key ideas.  The answers to these items showed that students often have a mix of both scientific and non-scientific ideas about scientific phenomenon, which can be identified using the computer-assisted scoring tools.&nbsp; Third, faculty participants in our FLCs were motivated to persist in multi-year teaching professional development, in part, because it connected them to a national effort targeting college science teaching. We also found that our participants varied in their level of student-centered teaching.  Some faculty used traditional lectures, while other faculty engaged students in in-class exercises that were designed to help students master and apply difficult scientific concepts. Even though the faculty all value student thinking, most of their teaching remained stable over time.&nbsp; This outcome is significant in that science faculty at research-intensive universities were engaged in a long-term effort to employ technology and improve instruction in science courses. This outcome also shows the additional work that is needed to help faculty connect their engagement in student thinking and technology with changes in their classrooms.</span></p>\n<p><span>The broader impacts of this project are demonstrated through the number of faculty users and students assessed, as well as impacts on related science education initiatives. First, over the life of the project a total of 35 instructors at seven institutions participated in in-person professional development programs in FLCs. </span><span>&nbsp;Second, </span><span>the project extended the network of users outside of the FLCs by making these tools freely available on project websites, currently reaching an additional 46 users.&nbsp; Over the life of the project, faculty have administered the developed questions to over 21,000 undergraduate students and collected more than 150,000 written responses.&nbsp; Third, web portals supported during this project make the computer-assisted scoring models available to any interested instructor, help connect a community of interested users, and will be sustained after the end of the grant (www.beyondmultiplechoice.org). Fourth, </span><span>the findings from our research on professional development are guiding the design of future college-level professional development initiatives. For example, our findings suggested the importance of building partnerships with K-12 professional development experts to create programs for higher education that focus instructors on learning from student thinking and supporting it in their teaching. </span><span>&nbsp;Finally, the methods from this project are being used in new efforts in science education assessment aligned with the Next Generation Science Standards for K-12 education and for assessing undergraduate learning in STEM majors over 2- and 4- year college programs. </span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/25/2019<br>\n\t\t\t\t\tModified by: Michelle&nbsp;Smith</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2019/1912077/1912077_10276537_1574449165872_Fig1_TUESIII_projectoutcomes_FLCs--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1912077/1912077_10276537_1574449165872_Fig1_TUESIII_projectoutcomes_FLCs--rgov-800width.jpg\" title=\"Supported network of faculty learning communities (FLCs).\"><img src=\"/por/images/Reports/POR/2019/1912077/1912077_10276537_1574449165872_Fig1_TUESIII_projectoutcomes_FLCs--rgov-66x44.jpg\" alt=\"Supported network of faculty learning communities (FLCs).\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Each institution, denoted by large blue circles, had a group of faculty participants (small red dots) led by a project team member (small blue dots) as a facilitator.  This project attempted to connect local FLCs, at a total of 7 institutions, to a larger community of practice.</div>\n<div class=\"imageCredit\">McCourt, J., Andrews, T. C., Knight, J. K., Merrill, J., Nehm, R., Prevost, L. B., Smith, M. K., Urban-Lurain, M., and Lemons, P.P. (2017) CBE-Life Sciences Education. 16: ar54 doi: 10.1187/cbe.16-08-0241.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Michelle&nbsp;Smith</div>\n<div class=\"imageTitle\">Supported network of faculty learning communities (FLCs).</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1912077/1912077_10276537_1574449241197_Fig2_TUESIII_projectoutcomes_webpage--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1912077/1912077_10276537_1574449241197_Fig2_TUESIII_projectoutcomes_webpage--rgov-800width.jpg\" title=\"A screenshot of the project webpage.\"><img src=\"/por/images/Reports/POR/2019/1912077/1912077_10276537_1574449241197_Fig2_TUESIII_projectoutcomes_webpage--rgov-66x44.jpg\" alt=\"A screenshot of the project webpage.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The project webpage delivers basic information about the research conducted by the group.  Registered users can also browse assessment questions created by this project and upload student responses to be analyzed by computerized assisted scoring models.</div>\n<div class=\"imageCredit\">Automated Analysis of Constructed Response research project</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Michelle&nbsp;Smith</div>\n<div class=\"imageTitle\">A screenshot of the project webpage.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1912077/1912077_10276537_1574449329540_Fig3_TUESIII_projectoutcomes_report--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1912077/1912077_10276537_1574449329540_Fig3_TUESIII_projectoutcomes_report--rgov-800width.jpg\" title=\"A sample screenshot from an instructor feedback report.\"><img src=\"/por/images/Reports/POR/2019/1912077/1912077_10276537_1574449329540_Fig3_TUESIII_projectoutcomes_report--rgov-66x44.jpg\" alt=\"A sample screenshot from an instructor feedback report.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Interactive reports for instructors contain a variety of different representations of predicted scores of student responses.  Reports also allow instructors to view individual student responses and connections between phrases and predicted scores.</div>\n<div class=\"imageCredit\">Automated Analysis of Constructed Response research project</div>\n<div class=\"imageSubmitted\">Michelle&nbsp;Smith</div>\n<div class=\"imageTitle\">A sample screenshot from an instructor feedback report.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nMany national calls for science, technology and mathematics (STEM) education reform over the last decade have encouraged focusing on the teaching and learning of key disciplinary concepts (e.g. evolution).  Instructors need more information about the ideas students struggle to learn in order to adapt their teaching and support student learning. Another issue instructors face when teaching for conceptual understanding is efficiently evaluating written responses, especially for large-enrollment introductory level college courses.\nThe Automated Analysis of Constructed Response project has supported and extended a national network of science education researchers beginning with five universities: Michigan State University, State University of New York - Stony Brook, University of Colorado, University of Georgia, and the University of Maine. The work expanded to include teams at the University of South Florida, South Dakota State University, and Cornell University. This project focused on: 1) exploring student thinking about key ideas in STEM using student writing; 2) developing written college-level assessment items that address key ideas within multiple STEM disciplines which can be evaluated using computer-assisted scoring models; 3) making the assessments and computer-assisted scoring models freely and publicly available on project websites; and 4) supporting a network of college faculty to use the items and the scored student writing.  Since faculty are often hesitant to adopt new technology, and require support to implement such technology in teaching, this project (along with NSF DUE 1347740) supported professional development efforts using faculty learning communities (FLCs) at seven research-intensive universities. Participants in these FLCs used the developed items and computerized-scoring reports as part of their courses, then discussed how to use this information to better  engage students and  improve student learning.  \nThe intellectual merit of this project addresses the application of new technology, research into student thinking, and science faculty engagement in professional development. First, the research team applied methods from artificial intelligence to help identify and categorize ideas in student writing in STEM.  These tools can evaluate student responses with the same level of accuracy as scientific experts, evaluate any number of new student responses in near real time, and return analysis reports to faculty.  These reports can help guide faculty in evaluating student work and adjusting instruction in the classroom to address identified student difficulties.  Second, the team advanced knowledge about student difficulties in learning key STEM disciplinary concepts by developing a set of conceptual assessment items on key ideas.  The answers to these items showed that students often have a mix of both scientific and non-scientific ideas about scientific phenomenon, which can be identified using the computer-assisted scoring tools.  Third, faculty participants in our FLCs were motivated to persist in multi-year teaching professional development, in part, because it connected them to a national effort targeting college science teaching. We also found that our participants varied in their level of student-centered teaching.  Some faculty used traditional lectures, while other faculty engaged students in in-class exercises that were designed to help students master and apply difficult scientific concepts. Even though the faculty all value student thinking, most of their teaching remained stable over time.  This outcome is significant in that science faculty at research-intensive universities were engaged in a long-term effort to employ technology and improve instruction in science courses. This outcome also shows the additional work that is needed to help faculty connect their engagement in student thinking and technology with changes in their classrooms.\n\nThe broader impacts of this project are demonstrated through the number of faculty users and students assessed, as well as impacts on related science education initiatives. First, over the life of the project a total of 35 instructors at seven institutions participated in in-person professional development programs in FLCs.  Second, the project extended the network of users outside of the FLCs by making these tools freely available on project websites, currently reaching an additional 46 users.  Over the life of the project, faculty have administered the developed questions to over 21,000 undergraduate students and collected more than 150,000 written responses.  Third, web portals supported during this project make the computer-assisted scoring models available to any interested instructor, help connect a community of interested users, and will be sustained after the end of the grant (www.beyondmultiplechoice.org). Fourth, the findings from our research on professional development are guiding the design of future college-level professional development initiatives. For example, our findings suggested the importance of building partnerships with K-12 professional development experts to create programs for higher education that focus instructors on learning from student thinking and supporting it in their teaching.  Finally, the methods from this project are being used in new efforts in science education assessment aligned with the Next Generation Science Standards for K-12 education and for assessing undergraduate learning in STEM majors over 2- and 4- year college programs. \n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/25/2019\n\n\t\t\t\t\tSubmitted by: Michelle Smith"
 }
}