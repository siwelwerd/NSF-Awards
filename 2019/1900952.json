{
 "awd_id": "1900952",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Towards Practical Encoderless Robotics Through Vision-Based Training and Adaptation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 424911.0,
 "awd_amount": 424911.0,
 "awd_min_amd_letter_date": "2019-08-02",
 "awd_max_amd_letter_date": "2019-08-02",
 "awd_abstract_narration": "As robots branch out into unstructured and dynamic human environments (such as homes, offices, and hospitals), they require a new design methodology. These robots need to be safe to operate next to humans; they are expected to handle frequent changes and uncertainties that are inherent in human environments; and they should be as inexpensive as possible to enable wide-spread dissemination. Such criteria have lead to the emergence of compliant/soft robots, 3D printed robots, and inexpensive consumer-grade hardware, all of which constitute a major shift from heavy and rigid robots with tight tolerances used in industry. Traditional measurement devices that are suitable for sensing and controlling the motion of the rigid robots, i.e. joint encoders, are incompatible or impractical for many of these new types of robot. Alternative approaches that do not rely on encoders are largely missing from robotics technology and must be developed for these novel design models. This project investigates ways of using only cameras for sensing and controlling the robot's motion. Vision-based algorithms for robotic walking, object grasping and manipulation will be derived. Such algorithms will not only enable the use of the new-wave robots in unstructured environments but will also significantly lower the cost of traditional robotic systems, and therefore, boost their dissemination for industry and educational purposes.\r\n\r\nThe project will focus on utilizing vision-based estimation schemes and learning methods for acquiring both robot configuration information and task models within a framework where modeling inaccuracies and environment uncertainties are dealt with by robust visual servoing approaches. Visual observations will be used to model the relationship between actuator inputs, manipulator configuration, and task states, and they will be combined with adaptive vision-based control schemes that are robust to modeling uncertainties and disturbances. The framework will fundamentally rely on using convolutional neural networks (CNNs) to build the models from observation alone, both for a low-dimensional representation of configuration and for an image segmentation of the manipulator. Reinforcement learning methods will also be applied to assess the practicality of a modular combination of such methods with the offline learned representations to perform complex positioning and control tasks. These approaches will be evaluated in the context of within-hand manipulation, compliant surgical tool control, locomotion of a 3D-printed multi-legged robot, and force-controlled grasping and peg-insertion using a soft continuum manipulator. The contributions of our proposed work are that no prior model of a robot's configuration is needed because it is explicitly observed and inferred up-front (system identification); uncertainty affecting task performance is addressed by adapting the robot dynamics on-the-fly (model-through-confirmation); and the broad applicability of our methods will be demonstrated through application to a wide variety of platforms. Work done on this project will help to enable lower cost robotic and mechatronic hardware across a range of domains and will particularly impact the ability to control compliant and under-actuated structures.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Hager",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory D Hager",
   "pi_email_addr": "hager@cs.jhu.edu",
   "nsf_id": "000385453",
   "pi_start_date": "2019-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182686",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 424911.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In order to deploy robotics in real-world situations, they require an ability to perceive and interpret the physical world relative to a task to be accomplished. This project has focused on developing and understanding methods that make it practical to use machine learning to train robots to perform vision-based tasks. With the recent rapid advances in machine learning methods for computer vision, the potential for computer vision to support robotics has advanced greatly. However, there are still many key barriers before vision is a truly practical sensor for robotics, including the need for large amounts of training data, and sensitivity to changes in the data available between training and deployment. Our project has explored two general paths toward practical vision-enabled robotics that address these issues: 1) using new machine learning techniques to create simple, low-dimensional representations of the world that robots can be trained to use, and 2) using reinforcement learning (RL) methods to learn control policies that directly map images to actions.&nbsp;</p>\n<p>The&nbsp;<strong>Intellectual Merit</strong>&nbsp;of our work is the development of new control and machine learning methods that support a range of robotic tasks, including walking, running, object rearrangement, and manipulation. Many of these methods build on existing foundations such as PID control and mutual-information RL, thus providing a mathematical basis for performance. We combine those methods with advances in deep neural networks and machine learning to gain the flexibility and power necessary to extract signal from computer vision.</p>\n<p>Our results have shown that it is possible to achieve impressive gains in robot performance. In particular, we have had particular success in creating techniques that are able to train in simulation, but which can be adapted to work on real-world data. For example, we were able to show that, by learning a dynamic a keypoint-conditioned Neural Radiance Field (KP-NeRF) to capture and model scenes, we could separate the visual modeling into object appearance and object pose &ndash; the latter represented by learned keypoints. We could then learn a forward prediction model of the encoded keypoints, constructed over the keypoint representation space, and perform model-predictive control (MPC) for challenging manipulation tasks including block rearrangement and door closing. We also explored interactive approaches to adapting from simulation to real-world settings. In particular, we developed a framework that utilizes a short demonstration trajectory within the deployment environment to bridge the domain gap between simulation and the real world. We make use of the transformer architecture&rsquo;s ability to model sequential data to condition the system on this single demonstration trajectory. We were able to train the transformer to use the demonstration to adapt a controller to the target domain, and evaluated our framework on a set of environments in both simulation and on a physical UR5 robot under a sim-to-real setup. Finally, we also explored a second approach to addressing domain gap when an explicit demonstration is not possible. Our approach relies on the observation that rewards are often more resilient to domain shift than decision policies.&nbsp;&nbsp;That is, due to the nature of the sequential decision-making problem, errors made by the policy model can accumulate over the trajectory, leading to significantly worse episodic performance. However, we have found that a learned reward prediction function is able to reliably predict rewards while preserving ordering in the testing environment. Using this predicted reward, it is possible to fine-tune the policy in the new domain. Our method, Predicted Reward Fine-tuning (PRFT), improves generalization across various tasks in benchmarks, including the DeepMind Control Generalization Benchmark and the Distracting Control Suite, and outperforms baseline methods on a majority of environments.</p>\n<p>&nbsp;</p>\n<p>The&nbsp;<strong>Broader Impact</strong>&nbsp;of our work is primarily through two avenues. First, we have had the opportunity to train both graduate and master&rsquo;s students during the course of this project. These students have done industry internships and, in some cases, have gone on to industry careers, building on the work they did in this project. Second, the impact of vision-based robotics broadly is growing rapidly in the form of automated driving, the increasing use of vision-based robotics in manufacturing and in other warehouse applications. Our methods have been published in leading conferences and will be available to support those use-cases, as well as providing a basis for advances in other future research efforts.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/29/2023<br>\nModified by: Gregory&nbsp;D&nbsp;Hager</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn order to deploy robotics in real-world situations, they require an ability to perceive and interpret the physical world relative to a task to be accomplished. This project has focused on developing and understanding methods that make it practical to use machine learning to train robots to perform vision-based tasks. With the recent rapid advances in machine learning methods for computer vision, the potential for computer vision to support robotics has advanced greatly. However, there are still many key barriers before vision is a truly practical sensor for robotics, including the need for large amounts of training data, and sensitivity to changes in the data available between training and deployment. Our project has explored two general paths toward practical vision-enabled robotics that address these issues: 1) using new machine learning techniques to create simple, low-dimensional representations of the world that robots can be trained to use, and 2) using reinforcement learning (RL) methods to learn control policies that directly map images to actions.\n\n\nTheIntellectual Meritof our work is the development of new control and machine learning methods that support a range of robotic tasks, including walking, running, object rearrangement, and manipulation. Many of these methods build on existing foundations such as PID control and mutual-information RL, thus providing a mathematical basis for performance. We combine those methods with advances in deep neural networks and machine learning to gain the flexibility and power necessary to extract signal from computer vision.\n\n\nOur results have shown that it is possible to achieve impressive gains in robot performance. In particular, we have had particular success in creating techniques that are able to train in simulation, but which can be adapted to work on real-world data. For example, we were able to show that, by learning a dynamic a keypoint-conditioned Neural Radiance Field (KP-NeRF) to capture and model scenes, we could separate the visual modeling into object appearance and object pose  the latter represented by learned keypoints. We could then learn a forward prediction model of the encoded keypoints, constructed over the keypoint representation space, and perform model-predictive control (MPC) for challenging manipulation tasks including block rearrangement and door closing. We also explored interactive approaches to adapting from simulation to real-world settings. In particular, we developed a framework that utilizes a short demonstration trajectory within the deployment environment to bridge the domain gap between simulation and the real world. We make use of the transformer architectures ability to model sequential data to condition the system on this single demonstration trajectory. We were able to train the transformer to use the demonstration to adapt a controller to the target domain, and evaluated our framework on a set of environments in both simulation and on a physical UR5 robot under a sim-to-real setup. Finally, we also explored a second approach to addressing domain gap when an explicit demonstration is not possible. Our approach relies on the observation that rewards are often more resilient to domain shift than decision policies.That is, due to the nature of the sequential decision-making problem, errors made by the policy model can accumulate over the trajectory, leading to significantly worse episodic performance. However, we have found that a learned reward prediction function is able to reliably predict rewards while preserving ordering in the testing environment. Using this predicted reward, it is possible to fine-tune the policy in the new domain. Our method, Predicted Reward Fine-tuning (PRFT), improves generalization across various tasks in benchmarks, including the DeepMind Control Generalization Benchmark and the Distracting Control Suite, and outperforms baseline methods on a majority of environments.\n\n\n\n\n\nTheBroader Impactof our work is primarily through two avenues. First, we have had the opportunity to train both graduate and masters students during the course of this project. These students have done industry internships and, in some cases, have gone on to industry careers, building on the work they did in this project. Second, the impact of vision-based robotics broadly is growing rapidly in the form of automated driving, the increasing use of vision-based robotics in manufacturing and in other warehouse applications. Our methods have been published in leading conferences and will be available to support those use-cases, as well as providing a basis for advances in other future research efforts.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 12/29/2023\n\n\t\t\t\t\tSubmitted by: GregoryDHager\n"
 }
}