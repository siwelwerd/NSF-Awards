{
 "awd_id": "1849796",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: AF: Guarantees for Training Neural Networks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 174613.0,
 "awd_amount": 174613.0,
 "awd_min_amd_letter_date": "2019-06-27",
 "awd_max_amd_letter_date": "2019-06-27",
 "awd_abstract_narration": "The past decade has seen explosive progress in artificial intelligence, with conspicuous improvements in autonomous vehicles, intelligent virtual assistants, and recommendation systems, to name just a few applications. These advances are all powered by a broad family of techniques known as deep learning. Despite its observed successes, many basic questions about deep learning remain unanswered. To ensure predictable artificial-intelligence outcomes, is it possible to give useful formal guarantees for the performance of deep-learning algorithms? In order to deploy artificial intelligence on less powerful hardware (such as smartphones), is there a practical way to compress a massive deep-learning model into a smaller and more efficient model? Can deep-learning models be made provably robust against maliciously-crafted inputs? This project will search for rigorous answers to such questions in order to expand the theoretical foundations of deep learning. Integral to this project's success is the mentoring of graduate students. Innovations in education will furthermore broaden the community of researchers equipped with tools to solve problems at the intersection of computer science and mathematics.\r\n\r\nThis project begins its development of rigorous algorithmic foundations for training neural networks by seeking useful upper bounds on the generalization error and time- and sample-complexity for supervised learning. However, lower bounds can serve as helpful guideposts by suggesting structural assumptions that are necessary for interesting algorithmic guarantees. For example, existing lower bounds rule out efficiently learning many simple concepts over high-dimensional Gaussian inputs. Hence, this project will explore the complexity of learning data labeled by deep neural networks with low-rank weight matrices over low-dimensional inputs. Additionally, this project will seek theoretically rigorous algorithms for compressing trained deep-neural-network models to much shallower approximations. Lower bounds against local or distribution-free approaches to the compression problem will also be developed to guide algorithmic intuition. Furthermore, this project will seek simple hypotheses under which gradient descent provably trains neural-network models that are susceptible to adversarial inputs. Understanding the weaknesses of existing training algorithms will guide the development of algorithms that are robust to adversarial inputs.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Wilmes",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "John Wilmes",
   "pi_email_addr": "wilmes@brandeis.edu",
   "nsf_id": "000784632",
   "pi_start_date": "2019-06-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brandeis University",
  "inst_street_address": "415 SOUTH ST",
  "inst_street_address_2": "",
  "inst_city_name": "WALTHAM",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "7817362121",
  "inst_zip_code": "024532728",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "BRANDEIS UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MXLZGAMFEKN5"
 },
 "perf_inst": {
  "perf_inst_name": "Brandeis University",
  "perf_str_addr": "415 South Street, MS-116",
  "perf_city_name": "Waltham",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "024532728",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7796",
   "pgm_ref_txt": "ALGORITHMIC FOUNDATIONS"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 174613.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The past decade has seen explosive progress in artificial intelligence, with conspicuous improvements in autonomous vehicles, intelligent virtual assistants, and recommendation systems, to name just a few applications. These advances are all powered by a broad family of techniques known as deep learning. Despite its observed successes, many basic questions about deep learning remain unanswered. To ensure predictable artificial-intelligence outcomes, is it possible to give useful formal guarantees for the performance of deep-learning algorithms? In order to deploy artificial intelligence on less powerful hardware (such as smartphones), is there a practical way to compress a massive deep-learning model into a smaller and more efficient model? This project has searched for rigorous answers to such questions in order to expand the theoretical foundations of deep learning.</p>\n<p><br />In particular, this project produced the some of the first rigorous theory for neural networks designed to perform community-detection tasks. Assuming that inputs are structured according to a ``stochastic block model,'' a simple randomized model for networks withcommunity structures, this project has proved that a commonly-used family of deep learning algorithms succeeds in identifying stochastic block model communities.</p>\n<p><br />This project also initiated the theoretical study of depth compression of neural networks. Usually, extremely large, multi-layer neural network models are preferred because they empirically allow for more accurate models. However, large neural network models are computationally expensive to evaluate, so compressing a very deep neural network to a shallower one is of practical importance. This project has shown that existing algorithms cannot succeed at depth compression in the absense of strong assumptions about the input distribution, and also rule out a family of divide-and-conquer algorithms.</p>\n<p><br />Integral to this project's success was the mentoring of graduate students. Innovations in education, in particular the development of new applied mathematics curriculum, furthermore broadened the community of researchers equipped with tools to solve problems at the intersection of computer science and mathematics.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/16/2022<br>\n\t\t\t\t\tModified by: John&nbsp;Wilmes</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe past decade has seen explosive progress in artificial intelligence, with conspicuous improvements in autonomous vehicles, intelligent virtual assistants, and recommendation systems, to name just a few applications. These advances are all powered by a broad family of techniques known as deep learning. Despite its observed successes, many basic questions about deep learning remain unanswered. To ensure predictable artificial-intelligence outcomes, is it possible to give useful formal guarantees for the performance of deep-learning algorithms? In order to deploy artificial intelligence on less powerful hardware (such as smartphones), is there a practical way to compress a massive deep-learning model into a smaller and more efficient model? This project has searched for rigorous answers to such questions in order to expand the theoretical foundations of deep learning.\n\n\nIn particular, this project produced the some of the first rigorous theory for neural networks designed to perform community-detection tasks. Assuming that inputs are structured according to a ``stochastic block model,'' a simple randomized model for networks withcommunity structures, this project has proved that a commonly-used family of deep learning algorithms succeeds in identifying stochastic block model communities.\n\n\nThis project also initiated the theoretical study of depth compression of neural networks. Usually, extremely large, multi-layer neural network models are preferred because they empirically allow for more accurate models. However, large neural network models are computationally expensive to evaluate, so compressing a very deep neural network to a shallower one is of practical importance. This project has shown that existing algorithms cannot succeed at depth compression in the absense of strong assumptions about the input distribution, and also rule out a family of divide-and-conquer algorithms.\n\n\nIntegral to this project's success was the mentoring of graduate students. Innovations in education, in particular the development of new applied mathematics curriculum, furthermore broadened the community of researchers equipped with tools to solve problems at the intersection of computer science and mathematics.\n\n\t\t\t\t\tLast Modified: 02/16/2022\n\n\t\t\t\t\tSubmitted by: John Wilmes"
 }
}