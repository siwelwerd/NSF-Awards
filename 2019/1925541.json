{
 "awd_id": "1925541",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CC* Compute: Integrating Georgia Tech into the Open Science Grid for Multi-Messenger Astrophysics",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924220",
 "po_email": "kthompso@nsf.gov",
 "po_sign_block_name": "Kevin Thompson",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2021-06-30",
 "tot_intn_awd_amt": 399883.0,
 "awd_amount": 399883.0,
 "awd_min_amd_letter_date": "2019-06-10",
 "awd_max_amd_letter_date": "2019-06-10",
 "awd_abstract_narration": "Studying the Universe with all possible sources of information is the objective of multi-messenger astrophysics. Groundbreaking gravitational wave observations with LIGO, high-energy neutrino observations with IceCube, and very high-energy gamma-ray observations with VERITAS enable multi-messenger astrophysics. These and future instruments like CTA enable a complete view of the most violent and energetic phenomena in the Universe, such as the merger of black holes and neutron stars or the processes near the supermassive black holes at the center of large galaxies.  All these efforts are computationally intensive, both in data analysis and in simulations. The Open Science Grid (OSG) infrastructure and services are an ideal platform to meet the computational requirements of Multi-messenger Astrophysics. This project acquires cyber infrastructure resources to connect Georgia Tech to the OSG and integrate these resources into computational efforts of the previously-mentioned NSF funded facilities.\r\n\r\n The High Throughput Computing Cluster acquired under this award includes 12 compute nodes with 40-core Intel Skylakes with 192 GB memory to support LIGO project requirements and others. IceCube and LIGO are supported with 4 GPU nodes, each equipped with 16-core Intel Skylakes and 4 NVIDIA TeslaV100 GPUs with 96GB memory. A special OSG StashCache has 432TB of storage in direct support of projects such as CTA. The system is 10Gbps connected in the Georgia Tech data center which in turn has 100Gbps+ to Southern Crossroads R&E exchange point. This project's resources serve as a catalyst for Georgia Tech's long-term integration into the OSG, as a standard service offered to all researchers on campus. An important component of this proposal is a significant number of Graphical Processing Units, used to accelerate simulations. This project makes Atlanta the first StashCache provider in the Southeast, as a service that enables fast access to distributed OSG datasets by regional institutions that participate in this national grid.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mehmet",
   "pi_last_name": "Belgin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mehmet Belgin",
   "pi_email_addr": "mehmet.belgin@oit.gatech.edu",
   "nsf_id": "000668412",
   "pi_start_date": "2019-06-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Laura",
   "pi_last_name": "Cadonati",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Laura Cadonati",
   "pi_email_addr": "cadonati@gatech.edu",
   "nsf_id": "000069965",
   "pi_start_date": "2019-06-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ignacio",
   "pi_last_name": "Taboada",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ignacio Taboada",
   "pi_email_addr": "itaboada@gatech.edu",
   "nsf_id": "000512339",
   "pi_start_date": "2019-06-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Nepomuk",
   "pi_last_name": "Otte",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nepomuk Otte",
   "pi_email_addr": "nepomuk@physics.gatech.edu",
   "nsf_id": "000612494",
   "pi_start_date": "2019-06-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Semir",
   "pi_last_name": "Sarajlic",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Semir Sarajlic",
   "pi_email_addr": "semir.sarajlic@vanderbilt.edu",
   "nsf_id": "000685138",
   "pi_start_date": "2019-06-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808000",
   "pgm_ele_name": "Campus Cyberinfrastructure"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 399883.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We are witnessing drastic changes in scientific computing practices in response to increasingly collaborative multi-disciplinary projects at national and international scale. Many modern scientific analysis involve running large numbers of short computations, which is often referred to as High Throughput Computing (HTC). Open Science Grid (OSG) is a unique consortium that provides shared infrastructure and services to unify access to supercomputing sites across the nation, making a vast array of HTC resources available to US-based researchers. OSG has been instrumental in many ground-breaking scientific advancements, including the Nobel-winning Gravitational-Wave research (LIGO).&nbsp; &nbsp;<br /><br />A careful analysis of job characteristics reveals an increasing demand for HTC workloads at Georgia Tech (GT) also. At the time of the proposal, the number of HTC-eligible jobs with less than 4 cores constituted 88% percent of the total. In addition, some&nbsp; GT faculty members specifically rely on the OSG software and data ecosystem and computational power.&nbsp; &nbsp;<br /><br />In response, GT&rsquo;s &ldquo;Partnership for an Advanced Computing Environment (PACE)&rdquo; team and faculty members representing LIGO, IceCube and CTA/VERITAS Astrophysics projects collaboratively set to advance GT&rsquo;s OSG capabilities. While our proposal predominantly focused on these projects, it acted as a catalyst for a PACE-managed, centralized OSG service for the entire GT campus.&nbsp; <br /><br />This project funded a new cluster, &ldquo;Buzzard&rdquo;, which is at the heart of PACE&rsquo;s centralized OSG service. Buzzard is now serving the OSG community with its 12x 24-core 384GB CPU nodes, 11x 24-core 384GB 4x RTX6000 GPUs nodes, and a storage server with 200+TB of usable storage. Using this storage, we deployed the first StashCache service in the Southeast US. Due to its geographic proximity, our cache also serves several international OSG sites. Buzzard already received its first externally funded expansion with an addition of 25 CPU nodes by Co-PI Cadonati (LIGO) and 400TB storage by Co-PI Otte (CTA/VERITAS).&nbsp;&nbsp;&nbsp; <br /><br />It&rsquo;s important to note that 100% of the resources acquired by this award are shared by the OSG community, making no distinction between local (GT) and external researchers, as long as they are members of the same OSG project. 10% of the CPU and 5% of GPU cycles are dedicated to the Open Science Pool (OSPool), for the benefit of researchers from US-based institutions from a vast variety of scientific domains.&nbsp; &nbsp;<br /><br />This cluster is the first of its kind in the way that it supports multiple OSG projects using a single pool of CPU and GPU nodes. This created some configuration challenges and required a close collaboration between GT and its main partner, the Maniac Lab in the University of Chicago, as well OSG leadership and technical experts. This effort led to a unique fairshare configuration that allocates available resources to different OSG projects, without needing to physically isolate nodes. Our model required some changes on the externally managed OSG and project-specific services, providing a roadmap to other institutions that may deploy a similar service in the future.&nbsp; &nbsp;<br /><br />Bringing Buzzard into OSG opens up great opportunities with significant scientific impact. IceCube simulation of neutrinos and cosmic rays make substantial use of its GPUs. IceCube PhD Students at GT are able to use standard data analysis pipelines via OSG. Buzzard is also used to execute simulations of particle cascades in the atmosphere initiated by very-high-energy gamma rays. These simulations are crucial for the analysis of data recorded with VERITAS and, in the future, with CTA. Buzzard already serves as a storage facility for billions of air showers simulated for VERITAS on the OSG, allowing members of these projects worldwide to access and analyze the same data set without having to transfer it to their local institutions. Buzzard is now contributing significant&nbsp; resources to the International Gravitational-Wave Network (IGWN) pool, accessible to all LIGO-affiliated researchers through shared submit nodes. Both local and remote LIGO users are receiving a large, fair share allocation on the cluster.&nbsp; This has proven beneficial for debugging with direct access to local administrators and for prototyping new analyses that are expected to contribute to the analysis of LIGO data from the next observational data. <br /><br />Buzzard&rsquo;s OSPool integration serves a large number of domains including but not limited to Physics, Biological Sciences, Astronomy, Chemistry, Engineering, Mathematics, Computer Sciences, Education, Health, Economics, Integrative Activities, Earth and Ocean Sciences and other social sciences for users from up to 70 different campuses. &nbsp;<br /><br />As part of centralized OSG support, PACE started an OSG orientation class coupled with our existing weekly consulting sessions. Our communication ramp-up features Buzzard and OSG in articles, blog posts and symposiums; hence, we are optimistic that the interest in OSG service will further grow at GT. Overall, the Buzzard cluster acts as a catalyst for PACE's centralized OSG support that includes campus level efforts to educate the GT researchers about OSG and HTC.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2021<br>\n\t\t\t\t\tModified by: Mehmet&nbsp;Belgin</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWe are witnessing drastic changes in scientific computing practices in response to increasingly collaborative multi-disciplinary projects at national and international scale. Many modern scientific analysis involve running large numbers of short computations, which is often referred to as High Throughput Computing (HTC). Open Science Grid (OSG) is a unique consortium that provides shared infrastructure and services to unify access to supercomputing sites across the nation, making a vast array of HTC resources available to US-based researchers. OSG has been instrumental in many ground-breaking scientific advancements, including the Nobel-winning Gravitational-Wave research (LIGO).   \n\nA careful analysis of job characteristics reveals an increasing demand for HTC workloads at Georgia Tech (GT) also. At the time of the proposal, the number of HTC-eligible jobs with less than 4 cores constituted 88% percent of the total. In addition, some  GT faculty members specifically rely on the OSG software and data ecosystem and computational power.   \n\nIn response, GT\u2019s \"Partnership for an Advanced Computing Environment (PACE)\" team and faculty members representing LIGO, IceCube and CTA/VERITAS Astrophysics projects collaboratively set to advance GT\u2019s OSG capabilities. While our proposal predominantly focused on these projects, it acted as a catalyst for a PACE-managed, centralized OSG service for the entire GT campus.  \n\nThis project funded a new cluster, \"Buzzard\", which is at the heart of PACE\u2019s centralized OSG service. Buzzard is now serving the OSG community with its 12x 24-core 384GB CPU nodes, 11x 24-core 384GB 4x RTX6000 GPUs nodes, and a storage server with 200+TB of usable storage. Using this storage, we deployed the first StashCache service in the Southeast US. Due to its geographic proximity, our cache also serves several international OSG sites. Buzzard already received its first externally funded expansion with an addition of 25 CPU nodes by Co-PI Cadonati (LIGO) and 400TB storage by Co-PI Otte (CTA/VERITAS).    \n\nIt\u2019s important to note that 100% of the resources acquired by this award are shared by the OSG community, making no distinction between local (GT) and external researchers, as long as they are members of the same OSG project. 10% of the CPU and 5% of GPU cycles are dedicated to the Open Science Pool (OSPool), for the benefit of researchers from US-based institutions from a vast variety of scientific domains.   \n\nThis cluster is the first of its kind in the way that it supports multiple OSG projects using a single pool of CPU and GPU nodes. This created some configuration challenges and required a close collaboration between GT and its main partner, the Maniac Lab in the University of Chicago, as well OSG leadership and technical experts. This effort led to a unique fairshare configuration that allocates available resources to different OSG projects, without needing to physically isolate nodes. Our model required some changes on the externally managed OSG and project-specific services, providing a roadmap to other institutions that may deploy a similar service in the future.   \n\nBringing Buzzard into OSG opens up great opportunities with significant scientific impact. IceCube simulation of neutrinos and cosmic rays make substantial use of its GPUs. IceCube PhD Students at GT are able to use standard data analysis pipelines via OSG. Buzzard is also used to execute simulations of particle cascades in the atmosphere initiated by very-high-energy gamma rays. These simulations are crucial for the analysis of data recorded with VERITAS and, in the future, with CTA. Buzzard already serves as a storage facility for billions of air showers simulated for VERITAS on the OSG, allowing members of these projects worldwide to access and analyze the same data set without having to transfer it to their local institutions. Buzzard is now contributing significant  resources to the International Gravitational-Wave Network (IGWN) pool, accessible to all LIGO-affiliated researchers through shared submit nodes. Both local and remote LIGO users are receiving a large, fair share allocation on the cluster.  This has proven beneficial for debugging with direct access to local administrators and for prototyping new analyses that are expected to contribute to the analysis of LIGO data from the next observational data. \n\nBuzzard\u2019s OSPool integration serves a large number of domains including but not limited to Physics, Biological Sciences, Astronomy, Chemistry, Engineering, Mathematics, Computer Sciences, Education, Health, Economics, Integrative Activities, Earth and Ocean Sciences and other social sciences for users from up to 70 different campuses.  \n\nAs part of centralized OSG support, PACE started an OSG orientation class coupled with our existing weekly consulting sessions. Our communication ramp-up features Buzzard and OSG in articles, blog posts and symposiums; hence, we are optimistic that the interest in OSG service will further grow at GT. Overall, the Buzzard cluster acts as a catalyst for PACE's centralized OSG support that includes campus level efforts to educate the GT researchers about OSG and HTC. \n\n\t\t\t\t\tLast Modified: 10/29/2021\n\n\t\t\t\t\tSubmitted by: Mehmet Belgin"
 }
}