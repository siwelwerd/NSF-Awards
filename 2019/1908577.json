{
 "awd_id": "1908577",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Frontiers in Monte Carlo and Variational Inference",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 449623.0,
 "awd_amount": 449623.0,
 "awd_min_amd_letter_date": "2019-08-19",
 "awd_max_amd_letter_date": "2019-08-19",
 "awd_abstract_narration": "Probabilistic inference allows humans to gain insight and make predictions from data. There is an ever-growing need in business, government, and science to answer questions using data. Often, these questions are best answered by phrasing them as probability calculations. As data sets grow larger and more complex, probability calculations are increasingly difficult and often cannot be performed exactly within reasonable time budgets. This project will promote science and technology by providing new theoretical results, algorithms, and empirical knowledge about how to compute approximate answers to probabilistic queries in a way that achieves good tradeoffs between accuracy and efficiency. In particular, the project will study how to best combine the strengths of two different strategies for calculating probabilities. This work will provide new techniques that are practical, have tunable accuracy, and scale to very large data sets.\r\n\r\nTo meet these goals, this project will combine two different approaches to probabilistic inference: variational inference (VI), and Monte Carlo (MC). MC algorithms are general-purpose and are asymptotically exact, but may fail to give good answers in reasonable time or scale large data sets. In contrast, VI is a way to get a \"pretty good answer, quickly\" by restricting the approximate posterior to tractable family. This project will combine these in a principled way to derive algorithms that are general-purpose, practical, have tunable accuracy, and scale to very large data sets. The new algorithms are expected to achieve time-accuracy tradeoffs that dominate Monte Carlo methods for a wide range of problems and time budgets. The proposed methods will 1)  incorporate strengths of Monte Carlo methods into variational inference by designing approximating families based on Monte Carlo estimators; and 2)  improve the usefulness of variational inference for downstream tasks by adapting divergences and approximating families to the needs of a downstream Monte Carlo estimator.  The project will result in a comprehensive evaluation benchmark as well as a set of practical techniques to make the method more effective. A novel application in ecology will demonstrate the project's real-world potential.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Justin",
   "pi_last_name": "Domke",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Justin Domke",
   "pi_email_addr": "jdomke@umass.edu",
   "nsf_id": "000733746",
   "pi_start_date": "2019-08-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Sheldon",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel R Sheldon",
   "pi_email_addr": "sheldon@cs.umass.edu",
   "nsf_id": "000515483",
   "pi_start_date": "2019-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "140 Governors Drive Computer Sci",
  "perf_city_name": "Amherst",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039264",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 449623.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The goal of probabilistic inference is to combine a user's assumptions and some observed data to predict the &ldquo;posterior&rdquo; distribution of some unknown variables. While widely used in many fields (political science, psychology, ecology, astrophysics) this problem remains difficult in general and typically relies on approximate approaches. The two most common of these are Monte Carlo methods (which approximate the answer using random sampling) and variational inference methods (which convert the inference problem into an optimization). The goal of this project was to investigate methods for integrating Monte Carlo methods into variational inference, yielding hybrid methods.</span></p>\n<p><span>Our primary results are:</span></p>\n<ol>\n<li><span>A general framework for how to integrate Monte Carlo methods into variational inference methods. This extends previous work that had done this for the simplest Monte Carlo method (importance sampling) and extends it to other standard methods (stratified sampling, quasi-Monte Carlo, etc.)</span></li>\n<li><span>An existing technique known as &ldquo;U-statistics&rdquo; can be adapted used to speed up some of these methods.</span></li>\n<li><span>That framework can be used to integrate Monte Carlo methods commonly used for time-series (Sequential Monte Carlo) into variational inference without a bias that appears in previous methods.</span></li>\n<li><span>In practice, the most popular Monte Carlo method for probabilistic inference is probably Hamiltonian Monte Carlo. This too can be integrated into variational inference by constructing a certain augmented space.</span></li>\n<li><span>Many probabilistic models are &ldquo;hierarchical models&rdquo; where the model forms a sort of tree. These present a computational difficulty since the number of variables typically grows with the dataset. Yet, neural networks can be used to share variables, increasing the scale that can be addressed by several orders of magnitude. The methods mentioned above can also be used in this setting.</span></li>\n<li><span>Many probabilistic models can be reformulated into equivalent models to make inference easier by changing the order of variables. While typically done manually, this can be automated in many cases.</span></li>\n<li><span>One challenge with variational inference is dealing with stochastic optimization, which is difficult to automate and make robust and reliable. But by essentially &ldquo;fixing&rdquo; the values of the random variables, this can be converted into a deterministic problem, making optimization more reliable.</span></li>\n</ol><br>\n<p>\n Last Modified: 01/31/2024<br>\nModified by: Justin&nbsp;Domke</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of probabilistic inference is to combine a user's assumptions and some observed data to predict the posterior distribution of some unknown variables. While widely used in many fields (political science, psychology, ecology, astrophysics) this problem remains difficult in general and typically relies on approximate approaches. The two most common of these are Monte Carlo methods (which approximate the answer using random sampling) and variational inference methods (which convert the inference problem into an optimization). The goal of this project was to investigate methods for integrating Monte Carlo methods into variational inference, yielding hybrid methods.\n\n\nOur primary results are:\n\nA general framework for how to integrate Monte Carlo methods into variational inference methods. This extends previous work that had done this for the simplest Monte Carlo method (importance sampling) and extends it to other standard methods (stratified sampling, quasi-Monte Carlo, etc.)\nAn existing technique known as U-statistics can be adapted used to speed up some of these methods.\nThat framework can be used to integrate Monte Carlo methods commonly used for time-series (Sequential Monte Carlo) into variational inference without a bias that appears in previous methods.\nIn practice, the most popular Monte Carlo method for probabilistic inference is probably Hamiltonian Monte Carlo. This too can be integrated into variational inference by constructing a certain augmented space.\nMany probabilistic models are hierarchical models where the model forms a sort of tree. These present a computational difficulty since the number of variables typically grows with the dataset. Yet, neural networks can be used to share variables, increasing the scale that can be addressed by several orders of magnitude. The methods mentioned above can also be used in this setting.\nMany probabilistic models can be reformulated into equivalent models to make inference easier by changing the order of variables. While typically done manually, this can be automated in many cases.\nOne challenge with variational inference is dealing with stochastic optimization, which is difficult to automate and make robust and reliable. But by essentially fixing the values of the random variables, this can be converted into a deterministic problem, making optimization more reliable.\n\t\t\t\t\tLast Modified: 01/31/2024\n\n\t\t\t\t\tSubmitted by: JustinDomke\n"
 }
}