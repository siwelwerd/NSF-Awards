{
 "awd_id": "1910146",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small:Learning Generalized Invariant Representations in Banach Space for Transfer Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2019-08-01",
 "awd_max_amd_letter_date": "2019-08-01",
 "awd_abstract_narration": "Humans often reason about their observations abstractly to prevent themselves from drawing incorrect conclusions based on unimportant differences. For example, a person tries to avoid on-coming traffic regardless of the lighting conditions; in this case, illumination is said to be an invariance to the problem of traffic avoidance. The goal of this project is to create methods and algorithms to make decisions by learning representations of data with invariances. Beyond simple invariances such as \"rotating an image does not change whether it shows a cat,\" this project seeks to learn more flexible forms of invariances. Some examples include: more general transformations such as changes in pose and facial expressions, semantic or logic relationships between classes (e.g., an image cannot be determined as both having and not having a cat), and structured relationships between entities (e.g., adding or removing an edge in a user's social network does not change that user's preferences). The result will benefit a wide range of social and real-world applications including computer vision, natural language processing, and graph-structured data analysis.\r\n\r\nThis project will use tools from functional analysis and optimization theory to achieve these goals while retaining the scalability, modularity, reliability, and flexibility of existing methods without these invariances. Specifically, it will apply linear and sublinear regularizations on a reproducing kernel Hilbert space to introduce invariant representations in the resulting Hilbert or Banach spaces. Three thrusts will be pursued. First, generalized invariances will be incorporated into distance and similarity measures between multiple domains, allowing transferrable feature representations to be inferred across domains. The result will be used for transfer learning such as few-shot prediction and multi-way relationship modeling. Second, logical relationships between classes will be modeled by kernels on labels, which, when applied in conjunction with adversarial training, can significantly improve learning under a shifting distribution of input and output. Third, invariances will be built into convex neural networks, allowing invariant features to be learned across tasks through the intermediate layers. The data and algorithm implementations resulting from the project will be disseminated publicly, under permissive open-source licenses.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xinhua",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xinhua Zhang",
   "pi_email_addr": "zhangx@uic.edu",
   "nsf_id": "000728327",
   "pi_start_date": "2019-08-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Brian",
   "pi_last_name": "Ziebart",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brian Ziebart",
   "pi_email_addr": "bziebart@uic.edu",
   "nsf_id": "000607805",
   "pi_start_date": "2019-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Chicago",
  "inst_street_address": "809 S MARSHFIELD AVE M/C 551",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3129962862",
  "inst_zip_code": "606124305",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "IL07",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "W8XEAJDKMXH3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Chicago",
  "perf_str_addr": "851 S Morgan Street",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606077053",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "IL07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-3c5b4af2-7fff-53a9-a850-e722be26f0c7\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project used tools from functional analysis and optimization theory to learn data representations with invariances.&nbsp; At the fundamental level, we created convex representation learning models for generalized invariance using kernel representers in a semi-inner-product (s.i.p) space.&nbsp; The existing convex methods are restricted to linear invariances, such as invariance to transformations of an image.&nbsp; We significantly expanded it to semi-norm based invariances, along with a computationally efficient algorithm to compute the representations.&nbsp; We next highlight some examples.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our first example of invariance is mixup smoothing, where any smooth interpolation in the input/image space is postulated to induce smooth interpolation in the output/label space.&nbsp; It has been very commonly used in data augmentation to boost accuracy, robustness, or calibration of classification.&nbsp; We showed that by using our new s.i.p based technique, such a prior can be effectively incorporated into data representation, allowing even a simple linear downstream classifier to achieve as good performance as a sophisticated and nonlinear classifier.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Such a representation learning technique provides an important modularity in enforcing the invariances.&nbsp; Specifically, one can use multiple layers of neural networks, where each layer enforces one invariance.&nbsp; This inspired our follow-up work on warping layers, which also permits end-to-end training.&nbsp; We applied it to weakly supervised learning such as semi-supervised learning and few-shot learning, where only weak supervision is available, consistent with many realistic learning tasks.&nbsp; With limited labeled data, prior structures become especially important, and prominent example priors include hierarchies and mutual exclusions in the class space. However, most existing approaches only learn the representations separately in the feature space and the label space, and do not explicitly enforce the logical relationships (e.g., an animal cannot be recognized as being a tiger and not being a mammal).&nbsp; By using our novel warping layer, representations in both spaces can be learned jointly, and thanks to the modularity and differentiability, it can be directly embedded into generative models to leverage the prior hierarchical structure (e.g., from WordNet) and unlabeled data. The effectiveness of the warping layer is demonstrated on both few-shot and semi-supervised learning, outperforming the state of the art in practice.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We additionally developed invariant representation learning algorithms for covariate shift, unsupervised domain adaptation (UDA), and imitation learning.&nbsp; For example, images can be collected from distinct domains such as Amazon website, Webcam, and DSLR.&nbsp; Some domains may have higher-resolution images and more labeled data.&nbsp; The goal is to transfer the invariant dependency between image and label across different domains, even when they have different styles, illumination, etc.&nbsp; Most existing methods address this task by finding data representations that yield similar distributions across domains, as measured by some probability discrepancy metrics.&nbsp; We made an important observation that such invariances are needed only in the context of the downstream predictor.&nbsp; We therefore proposed warping the measure towards the end tasks, which significantly improved the model capacities while still enforcing the useful invariances.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Furthermore, we applied our method to networked relationships between entities.&nbsp; For example, adding or removing an edge in some chemical networks does not change certain properties.&nbsp; In social networks, certain properties (e.g., community identification) remain invariant to permutations of their nodes/users and the corresponding edges/connections.&nbsp; We applied our method to graph representation learning to account for such invariances, leveraging the Gromov-Wasserstein (GW) discrepancy which seeks the optimal transportation-based alignment between the structured data.&nbsp; To tackle the nonconvexity in computation, we further developed an orthogonal GW discrepancy as a tight lower bound that can be efficiently computed in practice.&nbsp; Extensive experiments on both the synthetic and real-world datasets show the tightness of our lower bounds, and it efficiently delivers accurate predictions and satisfactory barycenters for graph sets (which serve as a representation of the graph set).</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 12pt; font-family: Georgia,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In terms of broader impact, the source code of all the publications has been freely released, and the papers have been presented at top-tier conferences such as ICML, NeurIPS, and UAI.&nbsp; The project also generated a lot of impact on teaching and educational experiences.&nbsp; Both PIs introduced new courses that covered the discoveries from the project.&nbsp; Over 10 Ph.D. students have been supported during the project, and many of them are now being employed by top industrial players in machine learning, e.g., Amazon, Meta, and Microsoft.&nbsp; In terms of educating minority (female) students, 2 Ph.D. students and 1 M.S. student have been trained.&nbsp; PI Zhang also advised three female undergraduate students through the Capstone project.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/08/2024<br>\nModified by: Xinhua&nbsp;Zhang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project used tools from functional analysis and optimization theory to learn data representations with invariances. At the fundamental level, we created convex representation learning models for generalized invariance using kernel representers in a semi-inner-product (s.i.p) space. The existing convex methods are restricted to linear invariances, such as invariance to transformations of an image. We significantly expanded it to semi-norm based invariances, along with a computationally efficient algorithm to compute the representations. We next highlight some examples.\n\n\n\n\n\nOur first example of invariance is mixup smoothing, where any smooth interpolation in the input/image space is postulated to induce smooth interpolation in the output/label space. It has been very commonly used in data augmentation to boost accuracy, robustness, or calibration of classification. We showed that by using our new s.i.p based technique, such a prior can be effectively incorporated into data representation, allowing even a simple linear downstream classifier to achieve as good performance as a sophisticated and nonlinear classifier.\n\n\n\n\n\nSuch a representation learning technique provides an important modularity in enforcing the invariances. Specifically, one can use multiple layers of neural networks, where each layer enforces one invariance. This inspired our follow-up work on warping layers, which also permits end-to-end training. We applied it to weakly supervised learning such as semi-supervised learning and few-shot learning, where only weak supervision is available, consistent with many realistic learning tasks. With limited labeled data, prior structures become especially important, and prominent example priors include hierarchies and mutual exclusions in the class space. However, most existing approaches only learn the representations separately in the feature space and the label space, and do not explicitly enforce the logical relationships (e.g., an animal cannot be recognized as being a tiger and not being a mammal). By using our novel warping layer, representations in both spaces can be learned jointly, and thanks to the modularity and differentiability, it can be directly embedded into generative models to leverage the prior hierarchical structure (e.g., from WordNet) and unlabeled data. The effectiveness of the warping layer is demonstrated on both few-shot and semi-supervised learning, outperforming the state of the art in practice.\n\n\n\n\n\nWe additionally developed invariant representation learning algorithms for covariate shift, unsupervised domain adaptation (UDA), and imitation learning. For example, images can be collected from distinct domains such as Amazon website, Webcam, and DSLR. Some domains may have higher-resolution images and more labeled data. The goal is to transfer the invariant dependency between image and label across different domains, even when they have different styles, illumination, etc. Most existing methods address this task by finding data representations that yield similar distributions across domains, as measured by some probability discrepancy metrics. We made an important observation that such invariances are needed only in the context of the downstream predictor. We therefore proposed warping the measure towards the end tasks, which significantly improved the model capacities while still enforcing the useful invariances.\n\n\n\n\n\nFurthermore, we applied our method to networked relationships between entities. For example, adding or removing an edge in some chemical networks does not change certain properties. In social networks, certain properties (e.g., community identification) remain invariant to permutations of their nodes/users and the corresponding edges/connections. We applied our method to graph representation learning to account for such invariances, leveraging the Gromov-Wasserstein (GW) discrepancy which seeks the optimal transportation-based alignment between the structured data. To tackle the nonconvexity in computation, we further developed an orthogonal GW discrepancy as a tight lower bound that can be efficiently computed in practice. Extensive experiments on both the synthetic and real-world datasets show the tightness of our lower bounds, and it efficiently delivers accurate predictions and satisfactory barycenters for graph sets (which serve as a representation of the graph set).\n\n\n\n\n\nIn terms of broader impact, the source code of all the publications has been freely released, and the papers have been presented at top-tier conferences such as ICML, NeurIPS, and UAI. The project also generated a lot of impact on teaching and educational experiences. Both PIs introduced new courses that covered the discoveries from the project. Over 10 Ph.D. students have been supported during the project, and many of them are now being employed by top industrial players in machine learning, e.g., Amazon, Meta, and Microsoft. In terms of educating minority (female) students, 2 Ph.D. students and 1 M.S. student have been trained. PI Zhang also advised three female undergraduate students through the Capstone project.\n\n\n\t\t\t\t\tLast Modified: 07/08/2024\n\n\t\t\t\t\tSubmitted by: XinhuaZhang\n"
 }
}