{
 "awd_id": "1911025",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Realizing Fairness in Recommender Systems: Intersectionality, Tools, Explanation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 497900.0,
 "awd_amount": 513900.0,
 "awd_min_amd_letter_date": "2019-09-09",
 "awd_max_amd_letter_date": "2020-05-27",
 "awd_abstract_narration": "Recommender systems provide personalized suggestions to users of e-commerce, social media, and many other types of applications. As they have become more prevalent, recommender systems have moved from areas of consumer taste to areas with greater social impact and sensitivity, such as financial services, employment, and housing. Concern has grown that personalized recommendations may exhibit bias, produce unfair results, and entrench problems of inequity. This limits the potential utility of recommender systems in environments such as employment, where fair treatment of users is legally mandated. Lack of attention to fairness has also meant that recommender systems have tended to reinforce biases and to limit users' exposure to diverse items. In spite of the importance of this issue to the public and the recent work of researchers, there is little progress on key aspects of fairness-aware recommendation. Companies whose sites depend heavily on personalized recommendation therefore have little guidance from the research community about how to apply fairness-aware recommendation and how to evaluate their efforts relative to the state of the art. At the same time, recommender systems researchers have difficulty making progress in the field because of the lack of established datasets and metrics. This project will make advances in fairness-aware recommendation that make it suitable for real-world applications. \r\n\r\nTo meet these needs, the project will develop recommendation models and algorithms that can achieve high accuracy, while preserving fairness in multiple inter-sectional dimensions, and explore their effectiveness in three fairness-critical domains: philanthropy, employment, and news. Existing fairness-aware recommendation algorithms have, with few exceptions, been developed and evaluated in contexts where a single dimension of fairness, defining a single protected group, is considered. The research team will extend these algorithms to be sensitive to multiple protected features, and to incorporate multiple sides of the recommendation transaction. It is well known that explanations support users in their use of recommender systems, engendering greater trust. However, the greater complexity of fairness-aware recommendation makes it difficult to produce explanations, and the introduction of fairness objectives may actually decrease trust in some users who may perceive the system as insufficiently responsive to their interests. The project will therefore develop explanation mechanisms for fairness-aware recommendation that support transparency in the application of fairness criteria. Finally, in order to put fairness-aware recommendation research on a firmer foundation, this project will develop techniques for generating synthetic datasets that can be used in developing and evaluating recommendation algorithms. The project will use latent factor methods to represent patterns of user-item associations, including associations with users of different types, and then apply sampling to these factors to generate synthetic data containing realistic rating patterns. The software developed throughout the project will be incorporated into open-source platforms for the benefit of other researchers.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robin",
   "pi_last_name": "Burke",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Robin D Burke",
   "pi_email_addr": "robin.burke@colorado.edu",
   "nsf_id": "000444545",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Colorado at Boulder",
  "inst_street_address": "3100 MARINE ST",
  "inst_street_address_2": "STE 481 572 UCB",
  "inst_city_name": "Boulder",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3034926221",
  "inst_zip_code": "803090001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "THE REGENTS OF THE UNIVERSITY OF COLORADO",
  "org_prnt_uei_num": "",
  "org_uei_num": "SPVKK1RC2MZ3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Colorado at Boulder",
  "perf_str_addr": "3100 Marine Street, Room 481",
  "perf_city_name": "Boulder",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "803031058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 497900.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Recommender systems are pervasive, providing personalized suggestions to help users&#8192;find items of interest in a wide variety of online systems. As they have become more prevalent, recommender systems have moved from areas of consumer taste to areas with greater social impact and sensitivity, such as&#8192;financial services, employment, and housing. In these area, issues of fairness and bias are particularly important. Among other important outcomes, this research project developed new algorithms for enhancing the fairness of recommendations and studied differences across recommendation algorithms in the fairness of their outputs. <br /><br />Fairness is a complex topic and simple formulations, such as those looking only at equality of outcomes, do not capture the nuances of real-world fairness and are not well suited to personalized outputs such as recommendations. As part of this work, we developed and validated new metrics to measure the fairness of recommendations and to measure how and how well fairness-enhancing interventions were working. We also found that there were significant practical barriers to researchers working on fairness topics in recommender systems. We took a number of actions reduce some of these barriers. In collaboration with our non-profit partner, we developed and released a dataset based on financial services data to support research in fairness-aware recommendation in crowdfunding, adding to the relatively few extant datasets based on real-world fairness concerns. A single dataset is of course insufficient to explore the complexities of fairness across different settings. So, we developed techniques for generating synthetic data with sensitive user and item attributes, which reduces researchers' dependence on obtaining private data for conducting experiments. We also developed open-source software for conducting experiments with fairness-aware recommendation and made it available to the research community, presenting the tool at a tutorial at the ACM Fairness, Accountability and Transparency conference. <br /><br />Since the start of this project in 2019, the area of fairness and machine learning has become ever more important with legislative and regulatory agenies examining how recommender systems in areas such as social media, news and streaming platforms may be harming users, creators and others. The work of this project has contributed to the growing body of research seeking to understand and address these harms and has contributed to building the cohort of researchers exploring the topic, including through the organization of research workshops and tutorials. <br /><br />Much machine learning fairness research is theoretical and divorced from the practical considerations that arise in applied settings and as a result the techniques and findings are less likely to be useful in practice. Key to this project's success was a collaboration between two researchers, one in machine learning and one in socio-technical systems. That collaboration has continued past the lifetime of this project into additional in-depth study of machine learning fairness as embedded in institutions and deployed systems.</p><br>\n<p>\n Last Modified: 11/17/2023<br>\nModified by: Robin&nbsp;D&nbsp;Burke</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nRecommender systems are pervasive, providing personalized suggestions to help users&#8192;find items of interest in a wide variety of online systems. As they have become more prevalent, recommender systems have moved from areas of consumer taste to areas with greater social impact and sensitivity, such as&#8192;financial services, employment, and housing. In these area, issues of fairness and bias are particularly important. Among other important outcomes, this research project developed new algorithms for enhancing the fairness of recommendations and studied differences across recommendation algorithms in the fairness of their outputs. \n\nFairness is a complex topic and simple formulations, such as those looking only at equality of outcomes, do not capture the nuances of real-world fairness and are not well suited to personalized outputs such as recommendations. As part of this work, we developed and validated new metrics to measure the fairness of recommendations and to measure how and how well fairness-enhancing interventions were working. We also found that there were significant practical barriers to researchers working on fairness topics in recommender systems. We took a number of actions reduce some of these barriers. In collaboration with our non-profit partner, we developed and released a dataset based on financial services data to support research in fairness-aware recommendation in crowdfunding, adding to the relatively few extant datasets based on real-world fairness concerns. A single dataset is of course insufficient to explore the complexities of fairness across different settings. So, we developed techniques for generating synthetic data with sensitive user and item attributes, which reduces researchers' dependence on obtaining private data for conducting experiments. We also developed open-source software for conducting experiments with fairness-aware recommendation and made it available to the research community, presenting the tool at a tutorial at the ACM Fairness, Accountability and Transparency conference. \n\nSince the start of this project in 2019, the area of fairness and machine learning has become ever more important with legislative and regulatory agenies examining how recommender systems in areas such as social media, news and streaming platforms may be harming users, creators and others. The work of this project has contributed to the growing body of research seeking to understand and address these harms and has contributed to building the cohort of researchers exploring the topic, including through the organization of research workshops and tutorials. \n\nMuch machine learning fairness research is theoretical and divorced from the practical considerations that arise in applied settings and as a result the techniques and findings are less likely to be useful in practice. Key to this project's success was a collaboration between two researchers, one in machine learning and one in socio-technical systems. That collaboration has continued past the lifetime of this project into additional in-depth study of machine learning fairness as embedded in institutions and deployed systems.\t\t\t\t\tLast Modified: 11/17/2023\n\n\t\t\t\t\tSubmitted by: RobinDBurke\n"
 }
}