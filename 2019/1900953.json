{
 "awd_id": "1900953",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Towards Practical Encoderless Robotics Through Vision-Based Training and Adaptation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 317833.0,
 "awd_amount": 317833.0,
 "awd_min_amd_letter_date": "2019-08-02",
 "awd_max_amd_letter_date": "2019-08-02",
 "awd_abstract_narration": "As robots branch out into unstructured and dynamic human environments (such as homes, offices, and hospitals), they require a new design methodology. These robots need to be safe to operate next to humans; they are expected to handle frequent changes and uncertainties that are inherent in human environments; and they should be as inexpensive as possible to enable wide-spread dissemination. Such criteria have lead to the emergence of compliant/soft robots, 3D printed robots, and inexpensive consumer-grade hardware, all of which constitute a major shift from heavy and rigid robots with tight tolerances used in industry. Traditional measurement devices that are suitable for sensing and controlling the motion of the rigid robots, i.e. joint encoders, are incompatible or impractical for many of these new types of robot. Alternative approaches that do not rely on encoders are largely missing from robotics technology and must be developed for these novel design models. This project investigates ways of using only cameras for sensing and controlling the robot's motion. Vision-based algorithms for robotic walking, object grasping and manipulation will be derived. Such algorithms will not only enable the use of the new-wave robots in unstructured environments but will also significantly lower the cost of traditional robotic systems, and therefore, boost their dissemination for industry and educational purposes.\r\n\r\nThe project will focus on utilizing vision-based estimation schemes and learning methods for acquiring both robot configuration information and task models within a framework where modeling inaccuracies and environment uncertainties are dealt with by robust visual servoing approaches. Visual observations will be used to model the relationship between actuator inputs, manipulator configuration, and task states, and they will be combined with adaptive vision-based control schemes that are robust to modeling uncertainties and disturbances. The framework will fundamentally rely on using convolutional neural networks (CNNs) to build the models from observation alone, both for a low-dimensional representation of configuration and for an image segmentation of the manipulator. Reinforcement learning methods will also be applied to assess the practicality of a modular combination of such methods with the offline learned representations to perform complex positioning and control tasks. These approaches will be evaluated in the context of within-hand manipulation, compliant surgical tool control, locomotion of a 3D-printed multi-legged robot, and force-controlled grasping and peg-insertion using a soft continuum manipulator. The contributions of our proposed work are that no prior model of a robot's configuration is needed because it is explicitly observed and inferred up-front (system identification); uncertainty affecting task performance is addressed by adapting the robot dynamics on-the-fly (model-through-confirmation); and the broad applicability of our methods will be demonstrated through application to a wide variety of platforms. Work done on this project will help to enable lower cost robotic and mechatronic hardware across a range of domains and will particularly impact the ability to control compliant and under-actuated structures.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Berk",
   "pi_last_name": "Calli",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Berk Calli",
   "pi_email_addr": "bcalli@wpi.edu",
   "nsf_id": "000788245",
   "pi_start_date": "2019-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Worcester Polytechnic Institute",
  "inst_street_address": "100 INSTITUTE RD",
  "inst_street_address_2": "",
  "inst_city_name": "WORCESTER",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "5088315000",
  "inst_zip_code": "016092280",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "WORCESTER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJNQME41NBU4"
 },
 "perf_inst": {
  "perf_inst_name": "Worcester Polytechnic Institute",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "016092280",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 317833.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Producing safe, inexpensive, and capable robots is the key for disseminating the benefits of the robotics technology to public. Reducing the costs requires the use of cheaper materials and fewer sensors, which, in turn, necessitates new intelligence capabilities to safely and reliably operate the robots. In this project, a new family of algorithms was developed to control the robots via camera images without requiring to install on-board sensors, like encoders and force sensors. One of the important capabilities that these algorithms enable is to control the shape of soft and continuum robots. These robots are intrinsically safe since they are made up of soft and compliant materials. However, their control is more difficult due to their compliance and lack of on-board sensors. The algorithms developed in this project is a significant step towards reliably controlling these robots using camera images, so that they can execute tasks while avoiding obstacles in the scene. Another capability is to control (soft or rigid) robots using the natural visual features on their body without the need of specialized (fiducial) markers. As such, it becomes more practical to utilize the robots without the need of altering their visual appearance. The developed algorithms also enable new in-hand manipulation capabilities for robots. Such capabilities are crucial for robots to operate in home/workspace environments while utilizing objects designed for humans.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/25/2023<br>\nModified by: Berk&nbsp;Calli</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1900953/1900953_10629156_1703507124572_in_hand_manipulation--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1900953/1900953_10629156_1703507124572_in_hand_manipulation--rgov-800width.png\" title=\"The robotic manipulator executes various in-hand manipulation skills to get the held object into the desired in-hand pose.\"><img src=\"/por/images/Reports/POR/2023/1900953/1900953_10629156_1703507124572_in_hand_manipulation--rgov-66x44.png\" alt=\"The robotic manipulator executes various in-hand manipulation skills to get the held object into the desired in-hand pose.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The robotic manipulator executes various in-hand manipulation skills to get the held object into a desired in-hand pose.</div>\n<div class=\"imageCredit\">Sahin, A., Spiers, A.J. and Calli, B., 2021, May. Region-Based Planning for 3D Within-Hand-Manipulation via Variable Friction Robot Fingers and Extrinsic Contacts. In 2021 IEEE International Conference on Robotics and Automation (ICRA) (pp. 6549-6555)</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Berk&nbsp;Calli\n<div class=\"imageTitle\">The robotic manipulator executes various in-hand manipulation skills to get the held object into the desired in-hand pose.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1900953/1900953_10629156_1703506778511_natural_features_example--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1900953/1900953_10629156_1703506778511_natural_features_example--rgov-800width.png\" title=\"The natural features on the robot is tracked and used to control the robots pose\"><img src=\"/por/images/Reports/POR/2023/1900953/1900953_10629156_1703506778511_natural_features_example--rgov-66x44.png\" alt=\"The natural features on the robot is tracked and used to control the robots pose\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An example of utilizing natural visual features on the robot's body to control its pose.</div>\n<div class=\"imageCredit\">Chatterjee, S., Karade, A.C., Gandhi, A. and Calli, B., 2023, October. Keypoints-Based Adaptive Visual Servoing for Control of Robotic Manipulators in Configuration Space. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 6387-6394).</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Berk&nbsp;Calli\n<div class=\"imageTitle\">The natural features on the robot is tracked and used to control the robots pose</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1900953/1900953_10629156_1703506931079_soft_robots_example--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1900953/1900953_10629156_1703506931079_soft_robots_example--rgov-800width.png\" title=\"A continuum soft robot getting into a target shape from a different initial shape\"><img src=\"/por/images/Reports/POR/2023/1900953/1900953_10629156_1703506931079_soft_robots_example--rgov-66x44.png\" alt=\"A continuum soft robot getting into a target shape from a different initial shape\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Initial shape of the continuum manipulator is represented by the purple curve and the target curve is shown in pink. The final shape of the origami manipulator is represented by the dashed purple curve. The reference shape is denoted by the pink curve.</div>\n<div class=\"imageCredit\">Gandhi, A., Chiang, S.S., Onal, C.D. and Calli, B., 2023, October. Shape Control of Variable Length Continuum Robots Using Clothoid-Based Visual Servoing. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 6379-6386)</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Berk&nbsp;Calli\n<div class=\"imageTitle\">A continuum soft robot getting into a target shape from a different initial shape</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nProducing safe, inexpensive, and capable robots is the key for disseminating the benefits of the robotics technology to public. Reducing the costs requires the use of cheaper materials and fewer sensors, which, in turn, necessitates new intelligence capabilities to safely and reliably operate the robots. In this project, a new family of algorithms was developed to control the robots via camera images without requiring to install on-board sensors, like encoders and force sensors. One of the important capabilities that these algorithms enable is to control the shape of soft and continuum robots. These robots are intrinsically safe since they are made up of soft and compliant materials. However, their control is more difficult due to their compliance and lack of on-board sensors. The algorithms developed in this project is a significant step towards reliably controlling these robots using camera images, so that they can execute tasks while avoiding obstacles in the scene. Another capability is to control (soft or rigid) robots using the natural visual features on their body without the need of specialized (fiducial) markers. As such, it becomes more practical to utilize the robots without the need of altering their visual appearance. The developed algorithms also enable new in-hand manipulation capabilities for robots. Such capabilities are crucial for robots to operate in home/workspace environments while utilizing objects designed for humans.\n\n\n\t\t\t\t\tLast Modified: 12/25/2023\n\n\t\t\t\t\tSubmitted by: BerkCalli\n"
 }
}