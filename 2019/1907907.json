{
 "awd_id": "1907907",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: SMALL: Robust Inference and Influence in Dynamic Environments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 365958.0,
 "awd_amount": 365958.0,
 "awd_min_amd_letter_date": "2019-08-31",
 "awd_max_amd_letter_date": "2019-08-31",
 "awd_abstract_narration": "Many application domains, such as online content recommendation, medical trials, and logistics and operations in intelligent infrastructure require the practitioner to sequentially make decisions given little information about the environment and the optimality of selected actions.On a limited measurement budget, adaptive data collection can make the difference between measuring a phenomenon, or missing it. Recent advances in machine learning have provided rich insights into ways of using past observations to guide planning of future measurements. Most state-of-the-art approaches assume observations arise from an environment with fixed probabilistic characteristics. Yet, in practice, environments such as these tend to be the exception and not the rule. To handle this, practitioners supplement existing algorithms with ad-hoc exceptions and rule of thumb mechanisms, but relying on heuristics to account for brittle assumptions may itself be brittle and call into question the entire experiment. The objective of this project is to develop algorithms with performance guarantees for collecting data adaptively in dynamic and unpredictable environments with the goal of making robust inferences, faster and cheaper.\r\n\r\nThe technical agenda of this project will advance the state-of-the-art by introducing a contextual non-stochastic pure exploration framework that extends recent advances in context-free non-stochastic best-arm identification, and exploits available contextual information when possible while simultaneously giving robust guarantees in non-stochastic environments.  By taking a best-of-both-worlds approach, the project will also introduce resilience into the design of algorithms for adaptive inference and influence by building model misspecification into the framework. This will allow for simple models to be used as an approximate model for inference and influence when the model is accurate, but not mislead when it is not. The framework will also extend to hyperparameter tuning of machine learning models in the more challenging setting of contextual non-stationary or streaming-data environments (e.g., federated learning on mobile devices for ``edge computing'') thereby providing theoretical advancements with utility in several application domains. The project will conduct experiments informed by real world data in its development of a rigorous theoretical framework. Accompanying the technical agenda is an integrated research and education plan that includes cross-disciplinary undergrad and grad course development leveraging the experimental platform as well as student opportunities to engage with industry.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lillian",
   "pi_last_name": "Ratliff",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lillian Ratliff",
   "pi_email_addr": "ratliffl@uw.edu",
   "nsf_id": "000717231",
   "pi_start_date": "2019-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Jamieson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kevin Jamieson",
   "pi_email_addr": "jamieson@cs.washington.edu",
   "nsf_id": "000776726",
   "pi_start_date": "2019-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn Ave NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981950001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 365958.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">Faced with having to make a sequence of decisions given only partial information about the environment, a learning agent will inevitably make mistakes, leading to sub-optimal decisions. However, through experience we expect an agent to improve. Decades of research have been dedicated to designing automated learning agents that are guaranteed to converge to an optimal strategy given enough experience, even under adversarial conditions and environments. As these algorithms were designed for the worst-case, they are very robust, but also frequently too conservative and are slow to learn in easy environments. Like a tortoise moves steady on all terrains, this worst-case objective leads to uniformly slow completion times. Yet, the fox adjusts its pace, swift on easy paths, cautious in thickets, mirroring how the best methods adapt, quick for the simple, deliberate for the complex. Learning algorithms that provably adapt to the particular difficulty of an environment are said to be instance-dependent optimal and are never worse than worst-case optimal algorithms, but can be substantially better on easy instances.<span>&nbsp;</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">The research carried out by this award resulted in foundational contributions to instance-dependent optimal algorithms. We studied three distinct paradigms. First, we considered stationary stochastic environments where the outcomes of an algorithm&rsquo;s actions were random and hence unpredictable, but whose behavior was consistent over time to enable learning. Second, we considered dynamic stochastic environments where the environment changed in response to the algorithm&rsquo;s actions in an unknown way and must be learned in order to control the environment. And third, stochastic environments where the learning algorithm is not playing against Nature, but a strategic adversary with her own incentives in a competitive game. In all three settings, the major insight we provided was characterizing what optimal exploration and experimental design looked like, and then we developed practical, provably optimal algorithms that harnessed these insights. <span>&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/29/2023<br>\nModified by: Kevin&nbsp;Jamieson</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nFaced with having to make a sequence of decisions given only partial information about the environment, a learning agent will inevitably make mistakes, leading to sub-optimal decisions. However, through experience we expect an agent to improve. Decades of research have been dedicated to designing automated learning agents that are guaranteed to converge to an optimal strategy given enough experience, even under adversarial conditions and environments. As these algorithms were designed for the worst-case, they are very robust, but also frequently too conservative and are slow to learn in easy environments. Like a tortoise moves steady on all terrains, this worst-case objective leads to uniformly slow completion times. Yet, the fox adjusts its pace, swift on easy paths, cautious in thickets, mirroring how the best methods adapt, quick for the simple, deliberate for the complex. Learning algorithms that provably adapt to the particular difficulty of an environment are said to be instance-dependent optimal and are never worse than worst-case optimal algorithms, but can be substantially better on easy instances.\n\n\n\n\n\nThe research carried out by this award resulted in foundational contributions to instance-dependent optimal algorithms. We studied three distinct paradigms. First, we considered stationary stochastic environments where the outcomes of an algorithms actions were random and hence unpredictable, but whose behavior was consistent over time to enable learning. Second, we considered dynamic stochastic environments where the environment changed in response to the algorithms actions in an unknown way and must be learned in order to control the environment. And third, stochastic environments where the learning algorithm is not playing against Nature, but a strategic adversary with her own incentives in a competitive game. In all three settings, the major insight we provided was characterizing what optimal exploration and experimental design looked like, and then we developed practical, provably optimal algorithms that harnessed these insights. \n\n\n\t\t\t\t\tLast Modified: 11/29/2023\n\n\t\t\t\t\tSubmitted by: KevinJamieson\n"
 }
}