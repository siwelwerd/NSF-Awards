{
 "awd_id": "1909845",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Manipulating Text in Screenless Environments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 499876.0,
 "awd_amount": 499876.0,
 "awd_min_amd_letter_date": "2019-08-14",
 "awd_max_amd_letter_date": "2019-08-14",
 "awd_abstract_narration": "Keyboards on mobile devices display characters visually, and in so doing they present challenges for people who are blind as well as for all users in situations where it is inconvenient or unsafe to hold a phone in order to communicate. Existing approaches to mobile accessibility for eyes-free text entry do not fully solve this problem, because they remain screen-centric; manipulating text generally requires interacting with visual keypads that at most read aloud keys upon touching. A major unsolved challenge is how to manipulate text aurally and silently, in a way that unbinds users from a visual display. The goal of this project is to expand our understanding of text interaction from a screen-centric paradigm to screenless, aural environments. To this end, the work will establish and evaluate principles for aural text manipulation that do not rely on a reference screen but rather operate entirely over the auditory channel. This research is significant because it is expected to contribute novel strategies to augment the ability of over seven million blind people in the United States to perform text-entry by circumventing direct interaction with screen-based devices. By breaking free from the constraints of mobile visual displays, project outcomes will have broader applicability to support aural text manipulation for sighted users in situations where it is inconvenient or unsafe to stay glued to the screen while typing, or where text manipulation needs to be discreet, silent, and concealed from view. The project will directly engage people who are blind and visually impaired from three partner organizations in Indiana as well as students at Indiana University from underrepresented groups to co-create and evaluate new options to interact with text while bypassing the screen.\r\n\r\nThe project will pursue two technical thrusts. The first thrust will identify strategies for the auditory arrangement of the character set that operate over time rather than within the visual-spatial constraints of the screen. The conceptual advances will be designed to work with current and future wearable input devices (hand/finger gestures supported by armband or smart rings) and will inform the foundation for a new class of auditory keyboards untethered to a visual display. The second thrust will focus on conducting iterative and comparative evaluation studies with sighted and blind participants on system prototypes of auditory keyboards, in order to examine how people can aurally manipulate text in screenless contexts.  By exploring and validating novel principles to re-imagine the concept of keyboard in the aural modality, the project will contribute to shifting the way people think about accessible typing in eyes-free scenarios and will play a fundamental role in the understanding of how people respond to screenless environments for text manipulation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Davide",
   "pi_last_name": "Bolchini",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Davide P Bolchini",
   "pi_email_addr": "dbolchin@iupui.edu",
   "nsf_id": "000511635",
   "pi_start_date": "2019-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University Purdue Univ",
  "perf_str_addr": "535 W Michigan St., IT 475",
  "perf_city_name": "Indianapolis",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "462023103",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "IN07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499876.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"Default\">The development of novel, entirely auditory keyboards are advancing our understanding of how it is possible to perform text-entry without relying on a reference screen. For the first time, we are demonstrating how text interaction might look like for users who are not able to look at or touch a screen and may keep the phone in their pocket. We engaged blind individuals in designing and validating novel, early examples of techniques that demonstrate discreet and silent text editing and communication. The advances are significant because they represent the next step in a continuum of research that will ultimately lead to unbind our understanding of text manipulation from a visual, screen-centric paradigm and open the space for accessible experiences that fit current and future generations of wearable input like hand bands, armbands, and smart rings.</p>\n<p class=\"Default\">The results of the project show how it is possible to depart from the status quo of spatial-visual keypads to investigate the entirely time-based, auditory keyboards amenable to be controlled by current and future forms of discreet and screenless input. Whereas conventional wisdom in HCI assumes that typing needs to happen by selecting keys on a screen-based surface, our results challenge this notion and demonstrate a radical reorganization of the elements that make up our traditional concept of keyboard. &nbsp;Screenless auditory keyboards may be practical and useful in a variety of contexts, not only for people who are blind and visually impaired, but also in situations where traditional mobile keyboards fail, or when taking out and holding the phone may not possible, socially acceptable, or convenient. By developing methods for selecting and editing text in aural mode while circumventing the reliance on a screen, the project has contributed to shift the way we think about accessible typing using entirely auditory user interfaces.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/16/2023<br>\n\t\t\t\t\tModified by: Davide&nbsp;Bolchini</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The development of novel, entirely auditory keyboards are advancing our understanding of how it is possible to perform text-entry without relying on a reference screen. For the first time, we are demonstrating how text interaction might look like for users who are not able to look at or touch a screen and may keep the phone in their pocket. We engaged blind individuals in designing and validating novel, early examples of techniques that demonstrate discreet and silent text editing and communication. The advances are significant because they represent the next step in a continuum of research that will ultimately lead to unbind our understanding of text manipulation from a visual, screen-centric paradigm and open the space for accessible experiences that fit current and future generations of wearable input like hand bands, armbands, and smart rings.\nThe results of the project show how it is possible to depart from the status quo of spatial-visual keypads to investigate the entirely time-based, auditory keyboards amenable to be controlled by current and future forms of discreet and screenless input. Whereas conventional wisdom in HCI assumes that typing needs to happen by selecting keys on a screen-based surface, our results challenge this notion and demonstrate a radical reorganization of the elements that make up our traditional concept of keyboard.  Screenless auditory keyboards may be practical and useful in a variety of contexts, not only for people who are blind and visually impaired, but also in situations where traditional mobile keyboards fail, or when taking out and holding the phone may not possible, socially acceptable, or convenient. By developing methods for selecting and editing text in aural mode while circumventing the reliance on a screen, the project has contributed to shift the way we think about accessible typing using entirely auditory user interfaces.\n\n\t\t\t\t\tLast Modified: 10/16/2023\n\n\t\t\t\t\tSubmitted by: Davide Bolchini"
 }
}