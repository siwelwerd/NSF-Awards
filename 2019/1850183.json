{
 "awd_id": "1850183",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CHS: Techniques for Helping Domain Experts Understand and Improve Models Underlying Intelligent Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-02-15",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 200460.0,
 "awd_min_amd_letter_date": "2019-02-08",
 "awd_max_amd_letter_date": "2020-05-21",
 "awd_abstract_narration": "A number of computing systems include intelligent components, employing algorithms or computational models that process data and make decisions or recommendations based on the data.  These models often function as 'black boxes': their internal mechanisms are often not visible to their users, and when they are, they are usually in a form that requires training in computer science to understand and modify them.  They underlie many socially useful functions, ranging from risk scoring to autonomous vehicles, which makes it important for their users -- who are often experts in the specific domain of use, but not in computing -- to be able to  use and improve the models.  This project's goal is to develop techniques that will help human users who are not trained computer scientists interact with these models using concepts and interaction techniques based on the specific domain and problem at hand rather than the underlying algorithms.  This will lead to scientific advances in the broader area of explainable artificial intelligence, as well as increasing the transparency and usefulness of intelligent systems that affect everyday life.  The project will also support a number of course development and outreach activities and develop freely available toolkits for other researchers and developers to use. \r\n\r\nThe long-term goal of this project is to computationally generate visualizations to reveal how intelligent systems work to non-computing knowledge workers. To progress toward this goal, the project team will focus on three major research activities.  First, they will conduct a bottom-up, four-pass analysis of literature spanning multiple research communities (Artificial Intelligence, Machine Learning, Computer Science Education, Human-Computer Interaction, and Visualization) to summarize key design dimensions of visualizing intelligent systems.  Second, they will collect empirical evidence of how these systems are currently used and understood in the medical domain by observing how medical professionals interact with data and make sense of domain-specific intelligent systems.  Third, based on the first two activities, they will develop computational methods to illustrate how these systems work by visualizing how user-sampled data is transformed into final results, while providing controls for domain experts to interact with the model.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiang",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiang Chen",
   "pi_email_addr": "xiangchen@acm.org",
   "nsf_id": "000779682",
   "pi_start_date": "2019-02-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "420 Westwood Plz, BH 6730B",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 186730.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 13730.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-8a0955d8-7fff-715a-e0de-920f1815e9c1\"> </span></p>\n<h1 dir=\"ltr\"><span>Project Overview</span></h1>\n<p dir=\"ltr\"><span>Intelligent systems often function as 'black boxes': their internal mechanisms are often not visible to their users or in a form that requires training in computer science to understand and modify them. They underlie many socially useful functions, ranging from risk scoring to autonomous vehicles, which makes it important for their users -- who are often experts in the specific domain of use, but not in computing -- to be able to use and improve the models. This project's goal is to develop techniques that will help human users who are not trained computer scientists interact with these models using concepts and interaction techniques based on the specific domain and problem at hand rather than the underlying algorithms.&nbsp;</span></p>\n<h1 dir=\"ltr\"><span>Intellectual Merit Outcome</span></h1>\n<p dir=\"ltr\"><span>The project team focused on medical professionals as representatives of knowledge workers that need to process large amounts of data. The findings include the following:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>A paired-survey on both referring physicians (N=39) and radiologists (N=38) about how radiologists currently explain their analysis to referring physicians and how referring physicians expect explanations from both human and (hypothetical) AI radiologists, which reveals whether, when, and what kinds of explanations are needed between referring physicians and radiologists. By juxtaposing referring physicians&rsquo; response to these questions with respect to human vs. AI radiologists, the project team summarizes system requirements that encompass the current practices and a future of AI-enabled diagnosis.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>A co-design with physicians further developed the aforementioned system requirements into eight specific system features: augmenting input CXR images with specific inquiries, mediating system complexity by the level of urgency, presenting explanations hierarchically, calibrating regional findings with contrastive examples, communicating results probabilistically contextualizing impressions with additional information, and comparing with images in the past or from other patients.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>A functional system front-end that allows physician users to explore real results generated by an AI while all the other explanatory information either comes from real clinical reports or is manually generated by a radiologist collaborator. An evaluation with medical professionals provides summative insights on each feature. Participants provided more detailed and accurate explanations of the underlying AI after interacting with the system, which is summarized as design and implementation recommendations for future development of explainable medical AI.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Lessons learned distilled from studying how pathologists interact with AI in their examination process: 1) To explain AI&rsquo;s guidance, suggestions and recommendations, the system should go beyond a one-size-fits-all concept and provide instance-specific details that allow a medical user to see evidence that leads to a recommendation; 2) Medical diagnosis is seldom a one-shot task, thus AI&rsquo;s recommendations need to continuously direct a medical user to filter and prioritize a large task space, taking into account new information extracted from a user&rsquo;s up-to-date input; 3) Medical tasks are often time-critical, thus the benefits of AI&rsquo;s guidance, suggestions and recommendations need to be weighed by the amount of extra efforts incurred and the actionability of the provided information; 4) To guide the examination process with prioritization, AI should help a medical user narrow in small regions of a large task space, as well as helping them filter out information within specific regions; 5) It is possible for medical users to provide labels during their workflow with acceptable extra effort. However, the system should provide explicit feedback on how the model improves as a result, as a way to motivate and guide medical users&rsquo; future inputs; and 6) Tasks treated equally by an AI might carry different weights to a medical user. Thus for medically high-staked tasks, AI should provide information to validate its confidence level.</span></p>\n</li>\n</ul>\n<h1 dir=\"ltr\"><span>Broader Impacts Outcome</span></h1>\n<p dir=\"ltr\"><span>Education. The systems and study findings in this project have become case study materials for the PI's graduate course (ECE 209AS) on Human-AI Interaction.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Broadening the participation of computing. This project has successfully involved two REU students who closely worked with graduate students, learning new technical skills (e.g., training medical AI models and developing user interface prototypes), and became co-authors of academic publications. The theme of enabling domain users to understand AI was appropriated as a theme for the Learning-by-Research program---a program founded by the PI to engage non-computing undergraduate students in participating in HCI projects. The same explainable AI theme was also employed on the team&rsquo;s collaboration with UCLA Center for Engineering Excellence and Diversity to introduce three freshmen students from under-represented groups to AI following a user-centered design process, thus lowering the barrier that might otherwise have prevented them from &ldquo;getting their feet wet&rdquo; with such a highly technical topic.</span></p>\n<p dir=\"ltr\"><span>Other disciplines. By introducing AI to medical professionals, we demystified AI that had previously appeared as a black-box and democratized the concepts of human-centered computing to the medical field. In collaborating with us, physicians learned more about how AI works at a high-level and how AI can be understandable and useful.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/18/2022<br>\n\t\t\t\t\tModified by: Xiang&nbsp;Chen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nProject Overview\nIntelligent systems often function as 'black boxes': their internal mechanisms are often not visible to their users or in a form that requires training in computer science to understand and modify them. They underlie many socially useful functions, ranging from risk scoring to autonomous vehicles, which makes it important for their users -- who are often experts in the specific domain of use, but not in computing -- to be able to use and improve the models. This project's goal is to develop techniques that will help human users who are not trained computer scientists interact with these models using concepts and interaction techniques based on the specific domain and problem at hand rather than the underlying algorithms. \nIntellectual Merit Outcome\nThe project team focused on medical professionals as representatives of knowledge workers that need to process large amounts of data. The findings include the following:\n\n\nA paired-survey on both referring physicians (N=39) and radiologists (N=38) about how radiologists currently explain their analysis to referring physicians and how referring physicians expect explanations from both human and (hypothetical) AI radiologists, which reveals whether, when, and what kinds of explanations are needed between referring physicians and radiologists. By juxtaposing referring physicians\u2019 response to these questions with respect to human vs. AI radiologists, the project team summarizes system requirements that encompass the current practices and a future of AI-enabled diagnosis.\n\n\nA co-design with physicians further developed the aforementioned system requirements into eight specific system features: augmenting input CXR images with specific inquiries, mediating system complexity by the level of urgency, presenting explanations hierarchically, calibrating regional findings with contrastive examples, communicating results probabilistically contextualizing impressions with additional information, and comparing with images in the past or from other patients.\n\n\nA functional system front-end that allows physician users to explore real results generated by an AI while all the other explanatory information either comes from real clinical reports or is manually generated by a radiologist collaborator. An evaluation with medical professionals provides summative insights on each feature. Participants provided more detailed and accurate explanations of the underlying AI after interacting with the system, which is summarized as design and implementation recommendations for future development of explainable medical AI.\n\n\nLessons learned distilled from studying how pathologists interact with AI in their examination process: 1) To explain AI\u2019s guidance, suggestions and recommendations, the system should go beyond a one-size-fits-all concept and provide instance-specific details that allow a medical user to see evidence that leads to a recommendation; 2) Medical diagnosis is seldom a one-shot task, thus AI\u2019s recommendations need to continuously direct a medical user to filter and prioritize a large task space, taking into account new information extracted from a user\u2019s up-to-date input; 3) Medical tasks are often time-critical, thus the benefits of AI\u2019s guidance, suggestions and recommendations need to be weighed by the amount of extra efforts incurred and the actionability of the provided information; 4) To guide the examination process with prioritization, AI should help a medical user narrow in small regions of a large task space, as well as helping them filter out information within specific regions; 5) It is possible for medical users to provide labels during their workflow with acceptable extra effort. However, the system should provide explicit feedback on how the model improves as a result, as a way to motivate and guide medical users\u2019 future inputs; and 6) Tasks treated equally by an AI might carry different weights to a medical user. Thus for medically high-staked tasks, AI should provide information to validate its confidence level.\n\n\nBroader Impacts Outcome\nEducation. The systems and study findings in this project have become case study materials for the PI's graduate course (ECE 209AS) on Human-AI Interaction. \nBroadening the participation of computing. This project has successfully involved two REU students who closely worked with graduate students, learning new technical skills (e.g., training medical AI models and developing user interface prototypes), and became co-authors of academic publications. The theme of enabling domain users to understand AI was appropriated as a theme for the Learning-by-Research program---a program founded by the PI to engage non-computing undergraduate students in participating in HCI projects. The same explainable AI theme was also employed on the team\u2019s collaboration with UCLA Center for Engineering Excellence and Diversity to introduce three freshmen students from under-represented groups to AI following a user-centered design process, thus lowering the barrier that might otherwise have prevented them from \"getting their feet wet\" with such a highly technical topic.\nOther disciplines. By introducing AI to medical professionals, we demystified AI that had previously appeared as a black-box and democratized the concepts of human-centered computing to the medical field. In collaborating with us, physicians learned more about how AI works at a high-level and how AI can be understandable and useful.\n\n \n\n\t\t\t\t\tLast Modified: 01/18/2022\n\n\t\t\t\t\tSubmitted by: Xiang Chen"
 }
}