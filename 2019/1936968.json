{
 "awd_id": "1936968",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Convergence Accelerator Phase I (RAISE): Toward Fair, Ethical, Efficient, and Trustworthy Crowdsourcing Platforms to Support Crowdworkers in Jobs of the Future",
 "cfda_num": "47.084",
 "org_code": "15020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Linda Molnar",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 999992.0,
 "awd_amount": 999992.0,
 "awd_min_amd_letter_date": "2019-09-10",
 "awd_max_amd_letter_date": "2019-09-10",
 "awd_abstract_narration": "The NSF Convergence Accelerator supports team-based, multidisciplinary efforts that address challenges of national importance and show potential for deliverables in the near future.  \r\n\r\nThe broader impact/potential benefit of this Convergence Accelerator Phase I project is multifaceted.  Crowdsourcing has created a vast and rapidly growing online labor market.  However, today's crowdsourcing platforms cannot well support crowdworkers, job requesters, and the healthy growth of this important online labor market due to four major problems: fairness, ethics, efficiency, and trustworthiness.  This project is a convergence of the research and development from multiple intellectually distinct disciplines including Computer Science, Economics & Business, and Humanities & Social Sciences.  By performing fundamental research with rapid development advances through partnerships with crowdsourcing platform providers, this project will deliver techniques that can be used to create fair, ethical, efficient, and trustworthy crowdsourcing platforms to support American crowdworkers.  It will also enable job requesters including researchers, companies, and government or humanitarian aid organizations to receive high-quality and trustworthy task submissions for them to confidently conduct their important studies and make important decisions.  This project will actively involve students from underrepresented groups including female and minority students.  It will train students on research and on producing high-quality deliverables.  It will widely disseminate its results via activities such as publishing research papers and promoting the wide use of the deliverables.\r\n\r\nThis Convergence Accelerator Phase I project has significant intellectual merit.  It addresses the critical interdisciplinary challenges of creating a healthy crowdsourcing labor market that is crucial to the important studies, computations, and decisions of researchers, companies, as well as government and humanitarian aid organizations. This labor market is vast and rapidly growing, but has four major problems intertwined from the fairness, ethics, efficiency, and trustworthiness perspectives in a very complicated manner.  This project addresses the four major problems by performing fundamental research with rapid development advances through partnerships with crowdsourcing platform providers.  It will (1) design incentive structures based on economic theory to incentivize fairness in crowdsourcing, (2) design research, training, and assessment mechanisms to incorporate ethics into crowdsourcing, (3) design machine learning models to improve the efficiency of crowdworkers, and (4) design machine learning models to securely protect both crowdworkers and job requesters.  It will integrate the designed techniques at the client-side into a web browser extension, and at the server-side into some industrial partner's crowdsourcing platform.  Overall, it takes a convergence approach to advance the scientific knowledge and understanding of crowdsourcing and its closely related disciplines including economics, business, humanities, social sciences, and computer science.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "ITE",
 "org_div_long_name": "Innovation and Technology Ecosystems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chuan",
   "pi_last_name": "Yue",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chuan Yue",
   "pi_email_addr": "chuanyue@mines.edu",
   "nsf_id": "000581299",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Qin",
   "pi_last_name": "Zhu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qin Zhu",
   "pi_email_addr": "qinzhu@vt.edu",
   "nsf_id": "000763167",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Gilbert",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin T Gilbert",
   "pi_email_addr": "bgilbert@mines.edu",
   "nsf_id": "000802953",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Colorado School of Mines",
  "inst_street_address": "1500 ILLINOIS ST",
  "inst_street_address_2": "",
  "inst_city_name": "GOLDEN",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3032733000",
  "inst_zip_code": "804011887",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "CO07",
  "org_lgl_bus_name": "TRUSTEES OF THE COLORADO SCHOOL OF MINES",
  "org_prnt_uei_num": "JW2NGMP4NMA3",
  "org_uei_num": "JW2NGMP4NMA3"
 },
 "perf_inst": {
  "perf_inst_name": "Colorado School of Mines",
  "perf_str_addr": "1500 Illinois St.",
  "perf_city_name": "Golden",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "804011887",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "CO07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "096Y00",
   "pgm_ele_name": "CA-FW-HTF: Convergence Acceler"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "049Z",
   "pgm_ref_txt": "RAISE-Research Advanced by Interdiscipli"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 999992.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Crowdsourcing has created a vast and rapidly growing online labor market. However, today's crowdsourcing platforms cannot well support crowdworkers, job requesters, and the healthy growth of this important online labor market due to four major problems: fairness, ethics, efficiency, and trustworthiness. This project is a convergence of the research and development from multiple intellectually distinct disciplines including Computer Science, Economics &amp; Business, and Humanities &amp; Social Sciences.<br /><br />In this project, we actively participated in all Phase I in-person and online meetings, webinars, training activities, and discussions; we interviewed 32 crowdworkers to learn more about their unmet needs and pain points; we performed fundamental research, built prototypes, and partnered with a major crowdsourcing platform provider to develop techniques that can be used to create fair, ethical, efficient, and trustworthy crowdsourcing platforms to support both crowdworkers and job requesters.&nbsp; The following are some major outcomes from this project.<br /><br />We investigated the vulnerabilities the attention check mechanism, which is a dominant quality control mechanism in crowdsourcing. We developed AI models to demonstrate how this widely used quality control mechanism can be easily manipulated by attackers or irresponsible crowdworkers to compromise the data quality, and we proposed corresponding defense methods.&nbsp; As the result, we prompted the research community towards developing more robust attention check mechanisms, and more broadly, we prompted the research community to seriously consider the emerging risks posed by the malicious use of machine learning techniques to the quality, validity, and trustworthiness of crowdsourcing and social computing.<br /><br />We designed and implemented a fine-grained behavior-based quality control framework that specifically extracts fine-grained behavioral features to provide three quality control mechanisms in crowdsourcing: (a) quality prediction for objective tasks, (b) suspicious behavior detection for subjective tasks, and (c) unsupervised worker categorization.&nbsp; Using this framework, we conducted two real-world crowdsourcing experiments and demonstrated that using fine-grained behavioral features is feasible and beneficial in all three quality control mechanisms.&nbsp; We also analyzed our framework from multiple perspectives including workers' tool usages, generalizability, deployability, scalability, limitations, and future work. Our work provides clues and implications for helping job requesters or crowdsourcing platforms to further achieve better quality control.<br /><br />Crowdsourcing platforms are powerful tools for academic researchers. Proponents claim that crowdsourcing helps researchers quickly and affordably recruit enough human subjects with diverse backgrounds to generate significant statistical power, while critics raise concerns about unreliable data quality, labor exploitation, and unequal power dynamics between researchers and workers. We examined these concerns along three dimensions: methods, fairness, and politics. We found that researchers offer vastly different compensation rates for crowdsourced tasks, and they address potential concerns about data validity by using platform-specific tools and user verification methods. Additionally, workers depend upon crowdsourcing platforms for a significant portion of their income, are motivated more by fear of losing access to work than by specific compensation rates, and are frustrated by a lack of transparency and occasional unfair treatment from job requesters. We analyzed critical computing scholars' proposals to address crowdsourcing's problems, challenges with implementing these resolutions, and potential avenues for future research.<br /><br />We completed two large scale crowdwork incentive experiments on Amazon's Mechanical Turk (MTurk). The purpose of these experiments was to identify incentive structures that job requesters can offer to crowdworkers that will improve outcomes for both parties: improve data quality for job requesters, and improve pay for crowdworkers. Experimental treatments included differences in the level of pay and in the risk of \"work rejection\", or lost pay and lost reputational scores which is a common risk for crowdworkers on MTurk.<br /><br />Overall, our outcomes address the critical interdisciplinary challenges of creating a healthy crowdsourcing labor market that is crucial to crowdworkers and to the important studies, computations, and decisions of researchers, companies, as well as government and humanitarian aid organizations.&nbsp; Our outcomes advance the scientific knowledge and understanding of crowdsourcing and its closely related disciplines including economics, business, humanities, social sciences, and computer science.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/22/2021<br>\n\t\t\t\t\tModified by: Chuan&nbsp;Yue</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nCrowdsourcing has created a vast and rapidly growing online labor market. However, today's crowdsourcing platforms cannot well support crowdworkers, job requesters, and the healthy growth of this important online labor market due to four major problems: fairness, ethics, efficiency, and trustworthiness. This project is a convergence of the research and development from multiple intellectually distinct disciplines including Computer Science, Economics &amp; Business, and Humanities &amp; Social Sciences.\n\nIn this project, we actively participated in all Phase I in-person and online meetings, webinars, training activities, and discussions; we interviewed 32 crowdworkers to learn more about their unmet needs and pain points; we performed fundamental research, built prototypes, and partnered with a major crowdsourcing platform provider to develop techniques that can be used to create fair, ethical, efficient, and trustworthy crowdsourcing platforms to support both crowdworkers and job requesters.  The following are some major outcomes from this project.\n\nWe investigated the vulnerabilities the attention check mechanism, which is a dominant quality control mechanism in crowdsourcing. We developed AI models to demonstrate how this widely used quality control mechanism can be easily manipulated by attackers or irresponsible crowdworkers to compromise the data quality, and we proposed corresponding defense methods.  As the result, we prompted the research community towards developing more robust attention check mechanisms, and more broadly, we prompted the research community to seriously consider the emerging risks posed by the malicious use of machine learning techniques to the quality, validity, and trustworthiness of crowdsourcing and social computing.\n\nWe designed and implemented a fine-grained behavior-based quality control framework that specifically extracts fine-grained behavioral features to provide three quality control mechanisms in crowdsourcing: (a) quality prediction for objective tasks, (b) suspicious behavior detection for subjective tasks, and (c) unsupervised worker categorization.  Using this framework, we conducted two real-world crowdsourcing experiments and demonstrated that using fine-grained behavioral features is feasible and beneficial in all three quality control mechanisms.  We also analyzed our framework from multiple perspectives including workers' tool usages, generalizability, deployability, scalability, limitations, and future work. Our work provides clues and implications for helping job requesters or crowdsourcing platforms to further achieve better quality control.\n\nCrowdsourcing platforms are powerful tools for academic researchers. Proponents claim that crowdsourcing helps researchers quickly and affordably recruit enough human subjects with diverse backgrounds to generate significant statistical power, while critics raise concerns about unreliable data quality, labor exploitation, and unequal power dynamics between researchers and workers. We examined these concerns along three dimensions: methods, fairness, and politics. We found that researchers offer vastly different compensation rates for crowdsourced tasks, and they address potential concerns about data validity by using platform-specific tools and user verification methods. Additionally, workers depend upon crowdsourcing platforms for a significant portion of their income, are motivated more by fear of losing access to work than by specific compensation rates, and are frustrated by a lack of transparency and occasional unfair treatment from job requesters. We analyzed critical computing scholars' proposals to address crowdsourcing's problems, challenges with implementing these resolutions, and potential avenues for future research.\n\nWe completed two large scale crowdwork incentive experiments on Amazon's Mechanical Turk (MTurk). The purpose of these experiments was to identify incentive structures that job requesters can offer to crowdworkers that will improve outcomes for both parties: improve data quality for job requesters, and improve pay for crowdworkers. Experimental treatments included differences in the level of pay and in the risk of \"work rejection\", or lost pay and lost reputational scores which is a common risk for crowdworkers on MTurk.\n\nOverall, our outcomes address the critical interdisciplinary challenges of creating a healthy crowdsourcing labor market that is crucial to crowdworkers and to the important studies, computations, and decisions of researchers, companies, as well as government and humanitarian aid organizations.  Our outcomes advance the scientific knowledge and understanding of crowdsourcing and its closely related disciplines including economics, business, humanities, social sciences, and computer science.\n\n\t\t\t\t\tLast Modified: 12/22/2021\n\n\t\t\t\t\tSubmitted by: Chuan Yue"
 }
}