{
 "awd_id": "1916163",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Learning Decision Rules with Observational Data",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Pena Edsel",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 140000.0,
 "awd_amount": 140000.0,
 "awd_min_amd_letter_date": "2019-08-08",
 "awd_max_amd_letter_date": "2019-08-08",
 "awd_abstract_narration": "The analysis of large-scale and complex data plays an increasingly central role in society, and innovations in machine learning are yielding ever more powerful predictive technologies. However, when we use such data to guide decision making, it is important to recognize that the majority of datasets in these domains are observational rather than randomized in nature, and require careful analysis in order to draw correct conclusions about the causal effect of deploying a potential policy. The research aims to develop new methods for data-driven decision making that can harness the power and expressiveness of machine learning, all while rigorously building on best practices for causal inference from non-randomized data.\r\n\r\n\r\nThis project is centered around the following three statistical tasks: (1) Examine the problem of heterogeneous treatment effect estimation in observational studies, and develop a flexible framework that can be used with, e.g., boosting or neural networks. The accuracy of the proposed method depends on the complexity of the causal signal that we can intervene on, not on other merely associational signals. (2) Consider welfare maximizing structured policy learning, and study an approach whose regret decays as the inverse square root of the sample size in a non-parametric setting. (3) Consider the problem of learning optimal stopping rules from sequentially randomized data, and propose a new robust yet computationally feasible approach to policy learning in this setting. A unifying theme underlying all these results is that they highlight how classical ideas from semiparametric statistics can be used to rigorously leverage accurate machine learning predictors in decision-making problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stefan",
   "pi_last_name": "Wager",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefan Wager",
   "pi_email_addr": "swager@stanford.edu",
   "nsf_id": "000784369",
   "pi_start_date": "2019-08-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "3160 Porter Drive",
  "perf_city_name": "Palo Alto",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943041212",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 140000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Rigorous data-driven decision making requires understanding how different actions affect outcomes we care about. Standard machine learning methods are not adequate for this task because they are focused on predicting baseline outcomes, as opposed to understanding how we might change them. For example, in the setting of cardiovascular health, standard machine learning methods are good at predicting which patients have high risk of adverse outcomes, but are not designed to find which patients would benefit the most from specific therapies, e.g., aggressive blood pressure medication.</p>\n<p>The focus of our research was on new machine learning methods that are directly focused on solving causal tasks. To this end, we developed a number of methods, including the following:</p>\n<p>- We proposed a loss function that enables machine learning methods to directly and robustly target heterogeneous treatment effects, without being distracted by variation in baseline effects. These methods can be used of flexible treatment personalization using complex data in a number of settings, including medicine. On a technical level, we showed that -- under flexible conditions -- our method enables learning causal effects at an optimal rate that only depends on the complexity of the causal signal. In contrast, standard machine learning methods do not achieve this benchmark, and their rate of convergence can also be adversely affected by the complexity of irrelevant problem components.&nbsp;</p>\n<p>- We advanced methods for policy learning, i.e., learning optimal (structured) treatment assignment rules from observational data. Our methods can handle a number of complex statistical settings, including ones where treatments need to be identified using instrumental variables. Our main technical result bounds the worst-case regret of a policymaker using our method relative to a policymaker who knew a-priori what the best decision rule was.</p>\n<p>- We developed methods for optimal decision-making in problems that have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment. For example, a medical doctor may choose between postponing treatment (watchful waiting) and prescribing one of several available treatments during the many visits from a patient. Our approach is practical for policy optimization, and does not rely on any Markovian assumptions.</p>\n<p>In addition to new methods, we have also developed and released open-source statistical software that enables applied researchers to readily add our tools to their causal inference toolkit.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2021<br>\n\t\t\t\t\tModified by: Stefan&nbsp;Wager</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nRigorous data-driven decision making requires understanding how different actions affect outcomes we care about. Standard machine learning methods are not adequate for this task because they are focused on predicting baseline outcomes, as opposed to understanding how we might change them. For example, in the setting of cardiovascular health, standard machine learning methods are good at predicting which patients have high risk of adverse outcomes, but are not designed to find which patients would benefit the most from specific therapies, e.g., aggressive blood pressure medication.\n\nThe focus of our research was on new machine learning methods that are directly focused on solving causal tasks. To this end, we developed a number of methods, including the following:\n\n- We proposed a loss function that enables machine learning methods to directly and robustly target heterogeneous treatment effects, without being distracted by variation in baseline effects. These methods can be used of flexible treatment personalization using complex data in a number of settings, including medicine. On a technical level, we showed that -- under flexible conditions -- our method enables learning causal effects at an optimal rate that only depends on the complexity of the causal signal. In contrast, standard machine learning methods do not achieve this benchmark, and their rate of convergence can also be adversely affected by the complexity of irrelevant problem components. \n\n- We advanced methods for policy learning, i.e., learning optimal (structured) treatment assignment rules from observational data. Our methods can handle a number of complex statistical settings, including ones where treatments need to be identified using instrumental variables. Our main technical result bounds the worst-case regret of a policymaker using our method relative to a policymaker who knew a-priori what the best decision rule was.\n\n- We developed methods for optimal decision-making in problems that have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment. For example, a medical doctor may choose between postponing treatment (watchful waiting) and prescribing one of several available treatments during the many visits from a patient. Our approach is practical for policy optimization, and does not rely on any Markovian assumptions.\n\nIn addition to new methods, we have also developed and released open-source statistical software that enables applied researchers to readily add our tools to their causal inference toolkit.\n\n\t\t\t\t\tLast Modified: 12/30/2021\n\n\t\t\t\t\tSubmitted by: Stefan Wager"
 }
}