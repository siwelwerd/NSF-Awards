{
 "awd_id": "1902440",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Fellowship Award",
 "awd_titl_txt": "PostDoctoral Research Fellowship",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924878",
 "po_email": "adpollin@nsf.gov",
 "po_sign_block_name": "Andrew Pollington",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2019-03-01",
 "awd_max_amd_letter_date": "2019-03-01",
 "awd_abstract_narration": "This award is made as part of the FY 2019 Mathematical Sciences Postdoctoral Research Fellowships Program. Each of the fellowships supports a research and training project at a host institution in the mathematical sciences, including applications to other disciplines, under the mentorship of a sponsoring scientist. The title of the project for this fellowship to Iain Carmichael is \"Modeling, integration, and analysis of complex, hierarchical, and multi-view data.\" The host institution for the fellowship is University of Washington, and the sponsoring scientist is Daniela Witten.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Iain",
   "pi_last_name": "Carmichael",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Iain Carmichael",
   "pi_email_addr": "",
   "nsf_id": "000788833",
   "pi_start_date": "2019-03-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carmichael, Iain",
  "inst_street_address": "",
  "inst_street_address_2": "",
  "inst_city_name": "Seattle",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "",
  "inst_zip_code": "981150000",
  "inst_country_name": "United States",
  "cong_dist_code": null,
  "st_cong_dist_code": "WA",
  "org_lgl_bus_name": "",
  "org_prnt_uei_num": "",
  "org_uei_num": ""
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": null,
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981954322",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "060Y00",
   "pgm_ele_name": "Workforce (MSPRF) MathSciPDFel"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9219",
   "pgm_ref_txt": "POSTDOCTORAL FELLOWSHIPS IN MATH SCIENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-dbcdb7da-7fff-c26d-5377-a7b29af606f0\"> </span></p>\n<p dir=\"ltr\"><span>A major focus of my postdoctoral work was the development of statistical methods to better understand and analyze networks. The analysis of network data &ndash; from social networks to brain networks &ndash; has received significant attention in the past decade. Typically a data scientist observes a network (e.g. a social network) and tries to understand properties of the network. I studied a slightly different scenario where the parameters of a statistical model can be viewed as a network and we want to understand the structure of these model parameters. For example, consider a neuroscience study where we have a brain scan of each patient measuring the blood flow between every pair of brain regions and want to understand which brain region connections are associated with a disease. Here the regression model parameter takes the shape of a network where there is one coefficient for each brain region connection. My work brought together ideas from sparse learning in statistics and network community detection to develop models for a type of structured sparsity called &ldquo;block diagonal&rdquo; sparsity.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>My first project on network structured parameters focused on the development of a parametric mixture model for multi-view data. In this model the connections between two data views can be viewed as a bipartite graph where the edges represent the probability of a sample being in a given cluster in the first view and a given cluster in the second view. Learning the structure of this graph informs us about the connections between the two data views. I developed two approaches to learn the graph structure &ndash; one based on a sparsity inducing penalized likelihood approach and another based on a block diagonally constrained likelihood approach. Both of these methods presented computational challenges that I developed algorithms to address.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>My second project on network structured data developed a novel sparsity inducing penalty that encourages a connected component/community structure in a network shaped model parameter (equivalently a block diagonal structure in a matrix shaped parameter). To use this non-convex penalty I developed a majorization-minimization algorithm inspired by the local linear approximation algorithm from the non-convex sparsity literature. I was able to prove strong computational and theoretical guarantees (the so-called &ldquo;two step convergence to the oracle&rdquo; property) for statistical models that use the penalty.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Another focus of my postdoctoral work was the development of a new generalized linear model python package called yaglm. The yaglm package aims to make the broader ecosystem of modern generalized linear models accessible to data analysts and researchers. This ecosystem encompasses a range of loss functions (e.g. linear, logistic, quantile regression), constraints (e.g. positive, isotonic) and penalties. Beyond the basic lasso/ridge, the package supports structured penalties such as the nuclear norm as well as the group, exclusive, fused, and generalized lasso. It also supports more accurate adaptive and non-convex (e.g. SCAD) versions of these penalties that often come with strong statistical guarantees at limited additional computational expense. yaglm comes with a variety of tuning parameter selection methods including: cross-validation, information criteria that have favorable model selection properties, and degrees of freedom estimators. While several solvers are built in (e.g. FISTA), a key design choice allows users to employ their favorite state of the art optimization algorithms. Designed to be user friendly, the package automatically creates tuning parameter grids, supports tuning with fast path algorithms along with parallelization, and follows a unified scikit-learn compatible API.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/25/2023<br>\n\t\t\t\t\tModified by: Iain&nbsp;Carmichael</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nA major focus of my postdoctoral work was the development of statistical methods to better understand and analyze networks. The analysis of network data &ndash; from social networks to brain networks &ndash; has received significant attention in the past decade. Typically a data scientist observes a network (e.g. a social network) and tries to understand properties of the network. I studied a slightly different scenario where the parameters of a statistical model can be viewed as a network and we want to understand the structure of these model parameters. For example, consider a neuroscience study where we have a brain scan of each patient measuring the blood flow between every pair of brain regions and want to understand which brain region connections are associated with a disease. Here the regression model parameter takes the shape of a network where there is one coefficient for each brain region connection. My work brought together ideas from sparse learning in statistics and network community detection to develop models for a type of structured sparsity called \"block diagonal\" sparsity.\n\n \nMy first project on network structured parameters focused on the development of a parametric mixture model for multi-view data. In this model the connections between two data views can be viewed as a bipartite graph where the edges represent the probability of a sample being in a given cluster in the first view and a given cluster in the second view. Learning the structure of this graph informs us about the connections between the two data views. I developed two approaches to learn the graph structure &ndash; one based on a sparsity inducing penalized likelihood approach and another based on a block diagonally constrained likelihood approach. Both of these methods presented computational challenges that I developed algorithms to address.\n\n \nMy second project on network structured data developed a novel sparsity inducing penalty that encourages a connected component/community structure in a network shaped model parameter (equivalently a block diagonal structure in a matrix shaped parameter). To use this non-convex penalty I developed a majorization-minimization algorithm inspired by the local linear approximation algorithm from the non-convex sparsity literature. I was able to prove strong computational and theoretical guarantees (the so-called \"two step convergence to the oracle\" property) for statistical models that use the penalty.\n\n \nAnother focus of my postdoctoral work was the development of a new generalized linear model python package called yaglm. The yaglm package aims to make the broader ecosystem of modern generalized linear models accessible to data analysts and researchers. This ecosystem encompasses a range of loss functions (e.g. linear, logistic, quantile regression), constraints (e.g. positive, isotonic) and penalties. Beyond the basic lasso/ridge, the package supports structured penalties such as the nuclear norm as well as the group, exclusive, fused, and generalized lasso. It also supports more accurate adaptive and non-convex (e.g. SCAD) versions of these penalties that often come with strong statistical guarantees at limited additional computational expense. yaglm comes with a variety of tuning parameter selection methods including: cross-validation, information criteria that have favorable model selection properties, and degrees of freedom estimators. While several solvers are built in (e.g. FISTA), a key design choice allows users to employ their favorite state of the art optimization algorithms. Designed to be user friendly, the package automatically creates tuning parameter grids, supports tuning with fast path algorithms along with parallelization, and follows a unified scikit-learn compatible API.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 08/25/2023\n\n\t\t\t\t\tSubmitted by: Iain Carmichael"
 }
}