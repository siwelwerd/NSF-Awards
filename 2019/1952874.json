{
 "awd_id": "1952874",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Learning with Model Uncertainty and Misspecification",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032927466",
 "po_email": "kgyimahb@nsf.gov",
 "po_sign_block_name": "Kwabena Gyimah-Brempong",
 "awd_eff_date": "2019-08-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 62552.0,
 "awd_amount": 62552.0,
 "awd_min_amd_letter_date": "2019-09-16",
 "awd_max_amd_letter_date": "2019-09-16",
 "awd_abstract_narration": "Conventional economic theory and equilibrium concepts do not permit agents to experience doubts and ambiguity that would lead them to test the way their models of behavior are specified. Nor do they encompass alternative (sample) paths to escape from the status quo and switch among different models. These aepcets are crucial for explaining the dynamics of many types economic phenomena, particularly hwo we get to equilibrium outcomes. We propose an alternative approach to investigate the uncertainty and doubt in a dynamic context, without changing the preference of the decision maker.  We aim to make theoretical and practical contributions, by developing models in which agents and economists are on an equal footing, in the sense that agents within the model confront the same doubts and ambiguity about their environment that confront an outside observer or econometrician anlyzing the data.\r\n\r\nThis proposal contains three distinct projects designed to serve as laboratories for exploring the implications of endowing agents with model uncertainty and misspecification in a dynamic environment.  First, the forecast combination is a way for a policy maker to aggregate dispersed information, and to hedge against model uncertainty, under the assumption that the data generating process is exogenous.  We demonstrate the potential shortcomings of averaging forecasts, if the data generating process is endogenous, and develop a sensible way of combining different forecasts.  Second, we investigate how how doubts can sow the seeds of financial crises.  We first endogenize the heterogeneous beliefs by showing that doubts about the process generating fundamentals can produce heterogeneous beliefs and asset market bubbles.  We then impose discipline on the degree of belief heterogeneity, to understand the source of recurrent bubbles.  Third, a formal investigation of events generated by the presence of model uncertainty calls for a new solution concept.  We plan to extend and then refine the notion of self-confirming equilibrium by focusing on the set of stochastic processes that satisfy a given bound on detection error probability rates.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "In-Koo",
   "pi_last_name": "Cho",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "In-Koo Cho",
   "pi_email_addr": "in-koo.cho@emory.edu",
   "nsf_id": "000198663",
   "pi_start_date": "2019-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Emory University",
  "inst_street_address": "201 DOWMAN DR NE",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4047272503",
  "inst_zip_code": "303221061",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "EMORY UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "S352L5PJLMP8"
 },
 "perf_inst": {
  "perf_inst_name": "Emory University",
  "perf_str_addr": "",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303224250",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "132000",
   "pgm_ele_name": "Economics"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 62552.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Agents within the model confront the doubts and ambiguity about their environment.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Instead of rational expectations, agents within the model are endowed with limited computational capability but are trying to identify and build the best possible model.&nbsp;&nbsp;&nbsp; The conventional assumption in statistics and econometrics models is that the data generating process is exogenous but unknown.&nbsp; In an economic problem, however, the data generating process and the decision of the agents typically interact with each other.&nbsp;&nbsp;&nbsp;</p>\n<p>We aim to understand the dynamics generated by the learning behavior of the agent, who is assumed to use well established econometric method to estimate unknown parameters.&nbsp;&nbsp; As the econometric techniques are built on the assumption of exogenous data generating process, the agent overlooks the feedback effect of his decision on the data generating process.&nbsp;&nbsp;&nbsp; Because the feedback effect is highly non-linear and non-stationary, it is exceptionally challenging for the agents within the model to estimate the feedback effect accurately.&nbsp;&nbsp; The agent ends up using a misspecified model to learn the environment. &nbsp; A fundamental question is whether and how a decision-maker endowed with misspecified models can learn to behave as if he knows the actual data generating process, as we assume in the equilibrium model.&nbsp; &nbsp;</p>\n<p>We analyze the problem in the context of the macroeconomic policy.&nbsp;&nbsp; A policymaker is subject to ambiguity and model uncertainty.&nbsp;&nbsp; A policymaker relies on a manageable model with a handful of variables to capture only essential aspects of a complex economy to design and implement policy. &nbsp;In building a manageable model, we need to drop variables in the real world intentionally or unintentionally.&nbsp; Model misspecification is a norm rather than an exception.&nbsp;&nbsp;&nbsp; A policymaker is aware of the potential flaws of the model in hand.&nbsp;&nbsp;&nbsp; Instead of relying on a single model, a policymaker combines multiple forecasts from different sources to produce his forecast.&nbsp; The forecast combination is a widely used way for a policymaker to aggregate dispersed information among professional forecasters.&nbsp; This practice has been justified for generating a more accurate, stable, and robust forecast than a forecast based on a single model.&nbsp; We challenge this widely accepted view of econometricians when the data generating process is endogenous.</p>\n<p>Our findings are surprising and pessimistic.&nbsp;&nbsp; The model averaging process can produce volatile forecast than a forecast based on a single model.&nbsp;&nbsp; &nbsp;A misspecified model can drive out a correctly specified model in the long run with probability 1.&nbsp;&nbsp; &nbsp;Suppose that a policymaker is initially endowed with a &ldquo;correct&rdquo; model: if he follows the prediction of the model, the economy will reach an equilibrium outcome.&nbsp;&nbsp; The policymaker wants to hedge against model misspecification and captures structural change, which the model in hand might have overlooked.&nbsp; The policymaker chooses another model with an additional source of volatility and generates a forecast by averaging out the two forecasts according to the posterior belief computed by Bayes rule.&nbsp;&nbsp;&nbsp; The additional source of volatility has no foundation for the real economy.&nbsp;&nbsp;&nbsp; If the data generating process is exogenous, the policymaker can realize that the second model is wrong and believes in the first model in the long run.&nbsp;&nbsp; If the feedback from the data to the belief of the policymaker is strong, then the policymaker assigns in the long run probability 1 to the forecast by the second model, which is better for capturing the volatility.&nbsp;&nbsp; His belief in the second model contributes to market volatility, which favors the second model.&nbsp;&nbsp; A wrong model can drive out the correct model in the long run.&nbsp;&nbsp;&nbsp; A corollary of this result is that if two forecasters have different models, it may take an exceedingly long time until the two forecasts converge.&nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p>A natural response would be whether we have an alternative way to respond to the model uncertain that does better than the model averaging process.&nbsp; We provide a partial answer.&nbsp; In the same way, as the model averaging is compared to the model selection in econometrics, the model validation process is an alternative way for the policymakers to respond to model uncertainty.&nbsp;&nbsp;Instead of a horse race among different models as in the model averaging process, the policymaker selects a primary model and continues to use the model until the model fails the specification test.&nbsp;&nbsp; The central banks around the world follow the validation process in selecting the primary model to guide their monetary policy.&nbsp;&nbsp;We show that the model validation process does better as it selects the correct model with a higher probability, if not probability 1.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/04/2020<br>\n\t\t\t\t\tModified by: In-Koo&nbsp;Cho</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAgents within the model confront the doubts and ambiguity about their environment.     Instead of rational expectations, agents within the model are endowed with limited computational capability but are trying to identify and build the best possible model.    The conventional assumption in statistics and econometrics models is that the data generating process is exogenous but unknown.  In an economic problem, however, the data generating process and the decision of the agents typically interact with each other.   \n\nWe aim to understand the dynamics generated by the learning behavior of the agent, who is assumed to use well established econometric method to estimate unknown parameters.   As the econometric techniques are built on the assumption of exogenous data generating process, the agent overlooks the feedback effect of his decision on the data generating process.    Because the feedback effect is highly non-linear and non-stationary, it is exceptionally challenging for the agents within the model to estimate the feedback effect accurately.   The agent ends up using a misspecified model to learn the environment.   A fundamental question is whether and how a decision-maker endowed with misspecified models can learn to behave as if he knows the actual data generating process, as we assume in the equilibrium model.   \n\nWe analyze the problem in the context of the macroeconomic policy.   A policymaker is subject to ambiguity and model uncertainty.   A policymaker relies on a manageable model with a handful of variables to capture only essential aspects of a complex economy to design and implement policy.  In building a manageable model, we need to drop variables in the real world intentionally or unintentionally.  Model misspecification is a norm rather than an exception.    A policymaker is aware of the potential flaws of the model in hand.    Instead of relying on a single model, a policymaker combines multiple forecasts from different sources to produce his forecast.  The forecast combination is a widely used way for a policymaker to aggregate dispersed information among professional forecasters.  This practice has been justified for generating a more accurate, stable, and robust forecast than a forecast based on a single model.  We challenge this widely accepted view of econometricians when the data generating process is endogenous.\n\nOur findings are surprising and pessimistic.   The model averaging process can produce volatile forecast than a forecast based on a single model.    A misspecified model can drive out a correctly specified model in the long run with probability 1.    Suppose that a policymaker is initially endowed with a \"correct\" model: if he follows the prediction of the model, the economy will reach an equilibrium outcome.   The policymaker wants to hedge against model misspecification and captures structural change, which the model in hand might have overlooked.  The policymaker chooses another model with an additional source of volatility and generates a forecast by averaging out the two forecasts according to the posterior belief computed by Bayes rule.    The additional source of volatility has no foundation for the real economy.    If the data generating process is exogenous, the policymaker can realize that the second model is wrong and believes in the first model in the long run.   If the feedback from the data to the belief of the policymaker is strong, then the policymaker assigns in the long run probability 1 to the forecast by the second model, which is better for capturing the volatility.   His belief in the second model contributes to market volatility, which favors the second model.   A wrong model can drive out the correct model in the long run.    A corollary of this result is that if two forecasters have different models, it may take an exceedingly long time until the two forecasts converge.    \n\nA natural response would be whether we have an alternative way to respond to the model uncertain that does better than the model averaging process.  We provide a partial answer.  In the same way, as the model averaging is compared to the model selection in econometrics, the model validation process is an alternative way for the policymakers to respond to model uncertainty.  Instead of a horse race among different models as in the model averaging process, the policymaker selects a primary model and continues to use the model until the model fails the specification test.   The central banks around the world follow the validation process in selecting the primary model to guide their monetary policy.  We show that the model validation process does better as it selects the correct model with a higher probability, if not probability 1.\n\n\t\t\t\t\tLast Modified: 09/04/2020\n\n\t\t\t\t\tSubmitted by: In-Koo Cho"
 }
}