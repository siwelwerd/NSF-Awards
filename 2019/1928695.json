{
 "awd_id": "1928695",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FW-HTF-RM: Augmenting Spatial Cognition Capabilities of Future Workforce to Enhance Work Performance in Altered Environments Using Virtual Reality",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Corman",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-01-31",
 "tot_intn_awd_amt": 1201560.0,
 "awd_amount": 1201560.0,
 "awd_min_amd_letter_date": "2019-09-05",
 "awd_max_amd_letter_date": "2019-09-05",
 "awd_abstract_narration": "The goal of this research is to enable the future workforce to work in unfamiliar environments, including desolate hard to reach places such as: deep space, low Earth orbit, deep ocean, and polar regions. The research team will introduce a new cost-effective educational platform combining Virtual Reality (VR), Eye Tracking and electroencephalography (EEG). This platform will inform design principles for scenario-based simulations and games to train the future workforce to adapt to and work in altered environments. The main goal is to understand how spatial cognitive processing differs in altered gravitational and visual environments, and how VR-based simulation can accelerate training the nation's future workforce to adapt to such environments. The research team integrates the principles of information modeling, VR, fixation analysis, EEG, and aerospace engineering to conduct the proposed research on spatial cognitive processing in altered conditions.\r\n\r\nSpecifically, the research team will address how, and to what extent, the non-alignment of visual and idiotropic frames of reference (FOR) and a lack of visuospatial cues offered by familiar landmarks influence spatial abilities, fixation patterns, and brain functions. This study will: (1) measure spatial abilities through behavioral tests, and contrast scores and reaction time in simulated normal and altered environments; (2) measure and analyze cognitive strategies and mental workload using eye tracking and electroencephalography (EEG) and contrast results in simulated normal and altered environments; and (3) apply the results of spatial abilities, attentional allocation, and mental workload to create the framework of a simulation or game to train the future workforce to work in altered conditions.\r\n\r\nThe proposed research will lead to inventing, evaluating, and applying innovative methods and tools that use VR, eye tracking and EEG to design scenario-based simulations and games for workforce training.  This study will create new knowledge in the behavioral and physiological domains of cognitive science leading to a better understanding of spatial cognitive processing in altered environments. The broader impacts of this work include developing a unique, safer, and cost-effective approach to train workers using virtual analogs and augment their spatial abilities to enhance their safety, quality of work life, productivity, and potential for more people to participate in the future workforce. Broader impacts also include developing educational course content and increased mentoring of underrepresented student groups. Educational activities include a strong outreach program for K-12 and college students to increase their participation in careers in science, technology, engineering and math (STEM), inform them about the future of work in altered conditions, and explain how a human-technology frontier can bolster spatial cognitive performance.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Manish Kumar",
   "pi_last_name": "Dixit",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Manish Kumar Dixit",
   "pi_email_addr": "mdixit@tamu.edu",
   "nsf_id": "000666232",
   "pi_start_date": "2019-09-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jyotsna",
   "pi_last_name": "Vaid",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jyotsna Vaid",
   "pi_email_addr": "jvaid@tamu.edu",
   "nsf_id": "000186505",
   "pi_start_date": "2019-09-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Joseph",
   "pi_last_name": "Orr",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Joseph M Orr",
   "pi_email_addr": "joseph.orr@tamu.edu",
   "nsf_id": "000081493",
   "pi_start_date": "2019-09-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ann",
   "pi_last_name": "McNamara",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ann McNamara",
   "pi_email_addr": "ann@viz.tamu.edu",
   "nsf_id": "000521199",
   "pi_start_date": "2019-09-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Chamitoff",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory E Chamitoff",
   "pi_email_addr": "chamitoff@tamu.edu",
   "nsf_id": "000690060",
   "pi_start_date": "2019-09-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M University",
  "inst_street_address": "400 HARVEY MITCHELL PKY S STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778454375",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A & M UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JF6XLNB4CDJ5"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M University Main Campus",
  "perf_str_addr": "",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778454375",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "103Y00",
   "pgm_ele_name": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 1201560.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With emerging technologies, the future of work is fast evolving and expanding to hard to reach and desolate places like Polar regions, deep oceans, deep space, and other planets, which have altered visuospatial conditions that may influence human spatial cognition. Spatial cognitive processing is essential to complete routine personal, professional, and recreational tasks safely and more productively. If spatial cognition is impaired by altered visuospatial conditions, it may adversely impact human ability to live and work in such extreme conditions. <strong>Intellectual merit</strong>: The main goal of this integrative and interdisciplinary research was to explore how altered visuospatial conditions may influence human spatial cognition. To reach this goal, we examined: (1) how spatial ability is impacted by static and dynamic misalignment of visual and idiotropic (body) axes; (2) how spatial perception is influenced by lacking visual landmarks and cues; (3) if human gaze patterns differ under these altered conditions and whether these patterns correlate to spatial cognition test performance; and (4) if measured alpha frequency over posterior cortex is reduced in the experiment vs. control group participants, and if their reduction correlates with their differences in spatial task performance.&nbsp; <strong>Broader Impacts</strong>: By understanding spatial cognitive impacts of altered conditions, this research helps augment the safety and productivity of future workforce essential for the United States prosperity, defense, and global economic dominance. We hired and trained 14 doctoral, 5 master&rsquo;s, and 25 undergraduate students over the course of this project, including 23 female students. We organized three workshops for high school teachers and conducted a lab visit from school students to disseminate the knowledge of space science and exploration.</p>\n<p>For spatial ability, we simulated, in Virtual Reality (VR), one control condition representing earth conditions and two experiment conditions denoting microgravity environments with statically and dynamically misaligned visual and idiotropic axes. For spatial perception, the control environment simulated a city condition in which familiar visual landmarks such as buildings, trees, roads, and vehicles are available to help determine spatial relationships. In the two experiment conditions, a lunar or Martian condition with ground plane but no familiar visual landmarks and a deep space environment with neither the ground plane nor familiar landmarks were simulated in VR. Spatial ability was measured through spatial visualization, spatial relations, and spatial orientation abilities using Purdue Spatial Visualization Test: Rotation (PSVT:R), Mental Cutting Test (MCT), and Perspective Taking Ability (PTA), respectively. For spatial perception, we created and utilized size (SP) and distance perception (DP) tests. All five tests were digitalized and integrated into the two control and four experiment conditions simulated in VR.&nbsp; We also collected participants&rsquo; gaze data using VR-integrated eye-tracker and brain activity using electroencephalography (EEG).</p>\n<p>A total of 233 participants were recruited, each of whom participated in two 45-60-minute study sessions. They sat in a swivel chair and wore HTC Vive Pro Eye Head-mounted Displays (HMD) integrated with Tobii eye-tracking technology and EEG caps with electrodes to record brain activity. To collect time-synchronized behavioral test data, gaze data, and EEG data, we utilized Unity 3D as an integrated development environment (IDE) that immersed participants in these simulated conditions and collected their test response, eye movement, and EEG data. The analysis of collected data showed that the spatial visualization, spatial relations, and spatial orientation test accuracy is adversely impacted by the static and dynamic misalignment of the visual and idiotropic axes. For spatial perception tests, the results indicated that the lack of familiar visual cues adversely influenced participants&rsquo; distance perception but not size perception. The patterns of gaze fixations were different between the control and experiment conditions for both spatial ability and perception tasks indicating different mental workload needs in altered conditions. These results were also supported by a reduction of the brain&rsquo;s alpha band, which has long been postulated to reflect re-allocation of the brain&rsquo;s attentional processes and support ongoing increases in task demand. Overall, evidence for alpha-band suppression was more convincing in PSVTR and MCT tasks, whereas DP and SP task results showed momentary reductions in narrow ranges of the alpha frequency band. Importantly, the present findings show that the more demanding environmental conditions contribute to added mental workload above and beyond the task itself.</p>\n<p>In summary, the research yielded essential insights into spatial cognitive processing suggesting that altered visuospatial conditions may pose challenges to spatial ability and spatial perception of future workers who may be living and working in such conditions for long time. Using these results, we also designed a framework for a potential game-based training tool that can be employed at the school and college level to reinforce spatial ability and spatial perception in extreme conditions.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 05/05/2024<br>\nModified by: Manish Kumar&nbsp;Dixit</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1928695/1928695_10640236_1714866348197_Spatial_perception--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/1928695/1928695_10640236_1714866348197_Spatial_perception--rgov-800width.jpg\" title=\"Spatial perception accuracy\"><img src=\"/por/images/Reports/POR/2024/1928695/1928695_10640236_1714866348197_Spatial_perception--rgov-66x44.jpg\" alt=\"Spatial perception accuracy\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Response accuracy for spatial perception tasks. CG, EG1 and EG2 denote availability of most, some and no visual cues/landmarks, respectively</div>\n<div class=\"imageCredit\">Manish Dixit</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Manish Kumar&nbsp;Dixit\n<div class=\"imageTitle\">Spatial perception accuracy</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1928695/1928695_10640236_1714866236208_Spatial_ability--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/1928695/1928695_10640236_1714866236208_Spatial_ability--rgov-800width.jpg\" title=\"Spatial ability accuracy\"><img src=\"/por/images/Reports/POR/2024/1928695/1928695_10640236_1714866236208_Spatial_ability--rgov-66x44.jpg\" alt=\"Spatial ability accuracy\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Response accuracy for spatial ability tasks. CG, EG1 and EG2 denote no misalignment, static misalignment and dynamic misalignment of idiotropic and visual axes, respectively</div>\n<div class=\"imageCredit\">Manish Dixit</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Manish Kumar&nbsp;Dixit\n<div class=\"imageTitle\">Spatial ability accuracy</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nWith emerging technologies, the future of work is fast evolving and expanding to hard to reach and desolate places like Polar regions, deep oceans, deep space, and other planets, which have altered visuospatial conditions that may influence human spatial cognition. Spatial cognitive processing is essential to complete routine personal, professional, and recreational tasks safely and more productively. If spatial cognition is impaired by altered visuospatial conditions, it may adversely impact human ability to live and work in such extreme conditions. Intellectual merit: The main goal of this integrative and interdisciplinary research was to explore how altered visuospatial conditions may influence human spatial cognition. To reach this goal, we examined: (1) how spatial ability is impacted by static and dynamic misalignment of visual and idiotropic (body) axes; (2) how spatial perception is influenced by lacking visual landmarks and cues; (3) if human gaze patterns differ under these altered conditions and whether these patterns correlate to spatial cognition test performance; and (4) if measured alpha frequency over posterior cortex is reduced in the experiment vs. control group participants, and if their reduction correlates with their differences in spatial task performance. Broader Impacts: By understanding spatial cognitive impacts of altered conditions, this research helps augment the safety and productivity of future workforce essential for the United States prosperity, defense, and global economic dominance. We hired and trained 14 doctoral, 5 masters, and 25 undergraduate students over the course of this project, including 23 female students. We organized three workshops for high school teachers and conducted a lab visit from school students to disseminate the knowledge of space science and exploration.\n\n\nFor spatial ability, we simulated, in Virtual Reality (VR), one control condition representing earth conditions and two experiment conditions denoting microgravity environments with statically and dynamically misaligned visual and idiotropic axes. For spatial perception, the control environment simulated a city condition in which familiar visual landmarks such as buildings, trees, roads, and vehicles are available to help determine spatial relationships. In the two experiment conditions, a lunar or Martian condition with ground plane but no familiar visual landmarks and a deep space environment with neither the ground plane nor familiar landmarks were simulated in VR. Spatial ability was measured through spatial visualization, spatial relations, and spatial orientation abilities using Purdue Spatial Visualization Test: Rotation (PSVT:R), Mental Cutting Test (MCT), and Perspective Taking Ability (PTA), respectively. For spatial perception, we created and utilized size (SP) and distance perception (DP) tests. All five tests were digitalized and integrated into the two control and four experiment conditions simulated in VR. We also collected participants gaze data using VR-integrated eye-tracker and brain activity using electroencephalography (EEG).\n\n\nA total of 233 participants were recruited, each of whom participated in two 45-60-minute study sessions. They sat in a swivel chair and wore HTC Vive Pro Eye Head-mounted Displays (HMD) integrated with Tobii eye-tracking technology and EEG caps with electrodes to record brain activity. To collect time-synchronized behavioral test data, gaze data, and EEG data, we utilized Unity 3D as an integrated development environment (IDE) that immersed participants in these simulated conditions and collected their test response, eye movement, and EEG data. The analysis of collected data showed that the spatial visualization, spatial relations, and spatial orientation test accuracy is adversely impacted by the static and dynamic misalignment of the visual and idiotropic axes. For spatial perception tests, the results indicated that the lack of familiar visual cues adversely influenced participants distance perception but not size perception. The patterns of gaze fixations were different between the control and experiment conditions for both spatial ability and perception tasks indicating different mental workload needs in altered conditions. These results were also supported by a reduction of the brains alpha band, which has long been postulated to reflect re-allocation of the brains attentional processes and support ongoing increases in task demand. Overall, evidence for alpha-band suppression was more convincing in PSVTR and MCT tasks, whereas DP and SP task results showed momentary reductions in narrow ranges of the alpha frequency band. Importantly, the present findings show that the more demanding environmental conditions contribute to added mental workload above and beyond the task itself.\n\n\nIn summary, the research yielded essential insights into spatial cognitive processing suggesting that altered visuospatial conditions may pose challenges to spatial ability and spatial perception of future workers who may be living and working in such conditions for long time. Using these results, we also designed a framework for a potential game-based training tool that can be employed at the school and college level to reinforce spatial ability and spatial perception in extreme conditions.\n\n\n\t\t\t\t\tLast Modified: 05/05/2024\n\n\t\t\t\t\tSubmitted by: Manish KumarDixit\n"
 }
}