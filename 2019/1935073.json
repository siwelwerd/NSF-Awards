{
 "awd_id": "1935073",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Addressing neuron-to-network energy-efficiency gap by investigating neuromorphic processors as a unified dynamical system",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Anthony Kuh",
 "awd_eff_date": "2019-09-15",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 380000.0,
 "awd_amount": 380000.0,
 "awd_min_amd_letter_date": "2019-08-12",
 "awd_max_amd_letter_date": "2019-08-12",
 "awd_abstract_narration": "This project investigates a fully-coupled, analog neuromorphic architecture where the entire learning network is designed as a unified dynamical system encoding information using short-term and long-term network dynamics. At the fundamental level, a single action potential generated by a biological neuron is not optimized for energy and consumes significantly more power than an equivalent floating-point operation in a Graphical Processing Unit (GPU) or a Tensor Processing Unit (TPU). Yet a population of coupled neurons in the human brain, using ~100 Giga coarse neural operations (or spikes) can learn and implement diverse functions compared to an application-specific deep-learning platform that typically use ~1 Peta 8-bit/16-bit floating-point operations or more. \r\n\r\nThe intellectual merit of this proposal is addressing this neuron-to-network energy-efficiency gap by investigating a growth-transform neural network (GTNN) based dynamical systems framework for designing energy-efficient, real-time neuromorphic processors. First, the project is investigating how a GTNN can exploit population dynamics to improve system energy-efficiency, while optimizing a learning or task objective in real-time. Second, the project is investigating how short-term and long-term network dynamics can enable scaling the proposed GTNN to billions of neurons without the need for explicit spike-routing and by exploiting network's limit-cycle fixed-points as analog memory. Third, the project is investigating a continuous-time, analog GTNN processor that can be used to demonstrate the energy-efficiency of proposed approach compared to other benchmark neuromorphic and deep-learning processors. \r\n\r\nThe project is also supporting open-source development of a GTNN simulator which will be disseminated to the neural network, neuromorphic engineering and neuroscience communities. The open-source tool will also form the basis for organizing tutorials and special sessions at IEEE conferences. The demonstration platforms developed through this project is being be used to connect with other NSF sponsored outreach programs at Washington University, which includes outreach to students belonging to underrepresented groups.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shantanu",
   "pi_last_name": "Chakrabartty",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shantanu Chakrabartty",
   "pi_email_addr": "shantanu@wustl.edu",
   "nsf_id": "000491083",
   "pi_start_date": "2019-08-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington University",
  "inst_street_address": "1 BROOKINGS DR",
  "inst_street_address_2": "",
  "inst_city_name": "SAINT LOUIS",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "3147474134",
  "inst_zip_code": "631304862",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MO01",
  "org_lgl_bus_name": "WASHINGTON UNIVERSITY, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "L6NFUM28LQM5"
 },
 "perf_inst": {
  "perf_inst_name": "Washington University",
  "perf_str_addr": "715 WESTWOOD DR APT 2W",
  "perf_city_name": "SAINT LOUIS",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "631052794",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760700",
   "pgm_ele_name": "EPCN-Energy-Power-Ctrl-Netwrks"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1653",
   "pgm_ref_txt": "Adaptive & intelligent systems"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 380000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project investigated an analog neuromorphic architecture where the entire learning network is designed as a unified dynamical system encoding information using short-term and long-term network dynamics. At the fundamental level, a single action potential generated by a biological neuron is not optimized for energy and consumes significantly more power than an equivalent floating-point operation in a Graphical Processing Unit (GPU) or a Tensor Processing Unit (TPU). Yet a population of coupled neurons in the human brain, using ~100 Giga coarse neural operations (or spikes) can learn and implement diverse functions compared to an application-specific deep-learning platform that typically use ~1 Peta 8-bit/16-bit floating-point operations or more. The intellectual merit of this project addressed this neuron-to-network energy-efficiency gap by investigating a growth-transform neural network (GTNN) based dynamical systems framework for designing energy-efficient, real-time neuromorphic processors. We reported how a GTNN can exploit population dynamics to improve system energy-efficiency, while optimizing a learning or task objective in real-time. We have reported how short-term and long-term network dynamics can enable scaling the proposed GTNN to solve learning and combinatorial optimization problems. We have also developed GTNN circuits and synaptic devices that can be used to implement a continuous-time, analog GTNN processor. Thus, the project has not only led to theoretical advances in continuous-time neural network architectures, where we have reported formulations that unify electrical resonance and learning, unify energy-harvesting and learning, and unify quantum mechanical tunneling and long-term learning. The energy-efficiency of the GTNN approach has been demonstrated for associative memory applications, for synthetic olfaction, and for memory consolidation. The broader impact of this project is the development of open-source GTNN simulator which has been disseminated to the neural network, the neuromorphic engineering and the neuroscience communities through a public web-portal. The open-source tool has also been the basis for hands-on tutorials at the Telluride Neuromorphic and Cognition Engineering (TNCE) workshops. The project has resulted in 5 journal publications and has supported the doctoral research of 5 PhD students which includes two female students. The project has also resulted in 3 US patent applications.</p><br>\n<p>\n Last Modified: 11/19/2024<br>\nModified by: Shantanu&nbsp;Chakrabartty</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1935073/1935073_10632487_1732041184417_GTNNraster--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/1935073/1935073_10632487_1732041184417_GTNNraster--rgov-800width.jpg\" title=\"GTNN Dynamics\"><img src=\"/por/images/Reports/POR/2024/1935073/1935073_10632487_1732041184417_GTNNraster--rgov-66x44.jpg\" alt=\"GTNN Dynamics\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Spatial and temporal short-term and long-term spiking dynamics exhibited by a GTNN network trained to recognize different odorants. With time the network self-learns to become more energy efficient without any degradation in the recognition accuracy.</div>\n<div class=\"imageCredit\">shantanu chakrabartty</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Shantanu&nbsp;Chakrabartty\n<div class=\"imageTitle\">GTNN Dynamics</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project investigated an analog neuromorphic architecture where the entire learning network is designed as a unified dynamical system encoding information using short-term and long-term network dynamics. At the fundamental level, a single action potential generated by a biological neuron is not optimized for energy and consumes significantly more power than an equivalent floating-point operation in a Graphical Processing Unit (GPU) or a Tensor Processing Unit (TPU). Yet a population of coupled neurons in the human brain, using ~100 Giga coarse neural operations (or spikes) can learn and implement diverse functions compared to an application-specific deep-learning platform that typically use ~1 Peta 8-bit/16-bit floating-point operations or more. The intellectual merit of this project addressed this neuron-to-network energy-efficiency gap by investigating a growth-transform neural network (GTNN) based dynamical systems framework for designing energy-efficient, real-time neuromorphic processors. We reported how a GTNN can exploit population dynamics to improve system energy-efficiency, while optimizing a learning or task objective in real-time. We have reported how short-term and long-term network dynamics can enable scaling the proposed GTNN to solve learning and combinatorial optimization problems. We have also developed GTNN circuits and synaptic devices that can be used to implement a continuous-time, analog GTNN processor. Thus, the project has not only led to theoretical advances in continuous-time neural network architectures, where we have reported formulations that unify electrical resonance and learning, unify energy-harvesting and learning, and unify quantum mechanical tunneling and long-term learning. The energy-efficiency of the GTNN approach has been demonstrated for associative memory applications, for synthetic olfaction, and for memory consolidation. The broader impact of this project is the development of open-source GTNN simulator which has been disseminated to the neural network, the neuromorphic engineering and the neuroscience communities through a public web-portal. The open-source tool has also been the basis for hands-on tutorials at the Telluride Neuromorphic and Cognition Engineering (TNCE) workshops. The project has resulted in 5 journal publications and has supported the doctoral research of 5 PhD students which includes two female students. The project has also resulted in 3 US patent applications.\t\t\t\t\tLast Modified: 11/19/2024\n\n\t\t\t\t\tSubmitted by: ShantanuChakrabartty\n"
 }
}