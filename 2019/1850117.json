{
 "awd_id": "1850117",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: Testing and Interpreting Image-based Computer Vision Models in 3D Space",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2019-06-01",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2019-05-17",
 "awd_max_amd_letter_date": "2019-05-17",
 "awd_abstract_narration": "From autonomous vehicles to cancer detection to speech recognition, artificial intelligence (AI) is transforming many economic sectors. While being increasingly ubiquitous, AI algorithms have been shown to easily misbehave when encountering natural, unexpected, never-seen inputs in the real world. For example, when a car on autopilot failed to recognize a white truck against a bright-lit sky, it crashed into the truck, killing the driver. To avoid such costly and unsafe failures, this project develops a framework for rigorously and automatically testing AI algorithms, specifically computer vision systems, in a 3D environment. In addition, via the framework, the project attempts to uncover why an algorithm makes a given decision. Providing explanations understandable by humans for decisions made by machines is crucial in gaining users' trust, advancing AI algorithms, and complying with the current and future legal regulations on the use of AI with sensitive human data.\r\n\r\nResearchers previously attempted to achieve the two main goals of (1) testing and (2) interpreting computer vision systems by synthesizing a 2D input image that fails a target image recognition model. However, the existing methods operate at the pixel level, generating special patterns that (a) are hard to explain; (b) might not transfer well to the physical world; and (c) may rarely be encountered in reality. Instead of optimizing in the 2D image space, the research objective of this project is to harness 3D graphics engines to create a 3D scene where the factors of variations (e.g. lighting, object geometry and appearances, background images) can be controlled and optimized to cause a target computer vision system to misbehave. This research effort will (1) reveal systematic defects via automatically testing the target model across many controlled, disentangled settings; and (2) improve the existing interpretability methods by incorporating 3D information. The developed methods attempt to provide explanations for the decisions made by computer vision models and create new insights into their inner functions. The project will improve the safety, reliability, and transparency of AI algorithms. This project is jointly funded by the Robust Intelligence (RI) and the Established Program to Stimulate Competitive Research (EPSCoR) programs.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anh",
   "pi_last_name": "Nguyen",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Anh M Nguyen",
   "pi_email_addr": "anhnguyen@auburn.edu",
   "nsf_id": "000784532",
   "pi_start_date": "2019-05-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Auburn University",
  "inst_street_address": "321-A INGRAM HALL",
  "inst_street_address_2": "",
  "inst_city_name": "AUBURN",
  "inst_state_code": "AL",
  "inst_state_name": "Alabama",
  "inst_phone_num": "3348444438",
  "inst_zip_code": "368490001",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "AL03",
  "org_lgl_bus_name": "AUBURN UNIVERSITY",
  "org_prnt_uei_num": "DMQNDJDHTDG4",
  "org_uei_num": "DMQNDJDHTDG4"
 },
 "perf_inst": {
  "perf_inst_name": "Auburn University",
  "perf_str_addr": "345 W. Magnolia St.",
  "perf_city_name": "Auburn",
  "perf_st_code": "AL",
  "perf_st_name": "Alabama",
  "perf_zip_code": "368490001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "AL03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>intellectual Merit.</strong>&nbsp;The support of NSF has enabled the PI and his team to make the following main scientific findings:</p>\n<ol>\n<li> Existing computer vision systems can easily misbehave when presented with images of objects in unusual poses. That is, state-of-the-art machines can easily mislabel a school bus lying on its side or a motorcycle with one wheel up in the air.</li>\n<li>Without (almost) any physical constraints, if we place an object visible inside the view of the camera randomly and randomly rotate it, there is only 3% chance that state-of-the-art machines can recognize the object.&nbsp;</li>\n<li>Currently, a common and better approach to addressing this issue is to collect more data (e.g. photos of objects in unsual angles) and teach machines on them. Yet, this approach is not guaranteed to work on new unusual poses or new objects.</li>\n<li>While current machines are not accurate in edge cases, it is also not unknown how to make them reliably and accurately explain their own decisions to users.</li>\n</ol>\n<p><strong>Outcomes covering the entire 2.5 years of the award:</strong></p>\n<ul>\n<li>The award has resulted in 14 publications (both published and pre-prints). Formal publication venues included top-tier computer vision or machine learning conferences (CVPR 19, 20, 22, NeurIPS 21, ACL 2021) and journals (Vision Research journal).</li>\n<li>The PI has created a new Deep Learning course at Auburn University as part of the Broader Impacts. The course started out as a Special Topics elective but now is part of the official curriculum of Computer Science program and attracts students from many disciplines including Biosystems Engineering, Agriculture, Aerospace Engineering, Mechanical Engineering, and Business Analytics, etc.</li>\n<li>The award has enabled the PI to help 6 Ph.D. students at Auburn University to publish their first, first-authored top-tier computer vision papers.</li>\n<li>The PI and his team have published 8 open-source repositories (which include a tool or code) to the community.</li>\n<li>We have made 5 public-oriented educational research videos covering our papers.</li>\n</ul>\n<ul>\n</ul><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/26/2022<br>\n\t\t\t\t\tModified by: Anh&nbsp;M&nbsp;Nguyen</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1850117/1850117_10605909_1666793398140_181130__pose_teaser-1024x922--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1850117/1850117_10605909_1666793398140_181130__pose_teaser-1024x922--rgov-800width.jpg\" title=\"Computer Vision systems are easily fooled by objects in unusual angles\"><img src=\"/por/images/Reports/POR/2022/1850117/1850117_10605909_1666793398140_181130__pose_teaser-1024x922--rgov-66x44.jpg\" alt=\"Computer Vision systems are easily fooled by objects in unusual angles\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Google Inception-v3 classifier correctly labels the canonical poses of objects (a), but fails to recognize out-of-distribution images of objects in unusual poses (b\ufffdd), including real photographs retrieved from the Internet (d).</div>\n<div class=\"imageCredit\">Alcorn et al. CVPR 2019</div>\n<div class=\"imageSubmitted\">Anh&nbsp;M&nbsp;Nguyen</div>\n<div class=\"imageTitle\">Computer Vision systems are easily fooled by objects in unusual angles</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nintellectual Merit. The support of NSF has enabled the PI and his team to make the following main scientific findings:\n\n Existing computer vision systems can easily misbehave when presented with images of objects in unusual poses. That is, state-of-the-art machines can easily mislabel a school bus lying on its side or a motorcycle with one wheel up in the air.\nWithout (almost) any physical constraints, if we place an object visible inside the view of the camera randomly and randomly rotate it, there is only 3% chance that state-of-the-art machines can recognize the object. \nCurrently, a common and better approach to addressing this issue is to collect more data (e.g. photos of objects in unsual angles) and teach machines on them. Yet, this approach is not guaranteed to work on new unusual poses or new objects.\nWhile current machines are not accurate in edge cases, it is also not unknown how to make them reliably and accurately explain their own decisions to users.\n\n\nOutcomes covering the entire 2.5 years of the award:\n\nThe award has resulted in 14 publications (both published and pre-prints). Formal publication venues included top-tier computer vision or machine learning conferences (CVPR 19, 20, 22, NeurIPS 21, ACL 2021) and journals (Vision Research journal).\nThe PI has created a new Deep Learning course at Auburn University as part of the Broader Impacts. The course started out as a Special Topics elective but now is part of the official curriculum of Computer Science program and attracts students from many disciplines including Biosystems Engineering, Agriculture, Aerospace Engineering, Mechanical Engineering, and Business Analytics, etc.\nThe award has enabled the PI to help 6 Ph.D. students at Auburn University to publish their first, first-authored top-tier computer vision papers.\nThe PI and his team have published 8 open-source repositories (which include a tool or code) to the community.\nWe have made 5 public-oriented educational research videos covering our papers.\n\n\n\n\n\t\t\t\t\tLast Modified: 10/26/2022\n\n\t\t\t\t\tSubmitted by: Anh M Nguyen"
 }
}