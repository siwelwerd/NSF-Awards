{
 "awd_id": "1909121",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Rethinking Haptic-Based Remote Communication Leveraging the DeafBlind Community's Tactile Intuitions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2020-01-01",
 "awd_exp_date": "2024-12-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2019-09-04",
 "awd_max_amd_letter_date": "2019-09-04",
 "awd_abstract_narration": "How would persons with 2 sensory losses - vision and hearing - communicate? They have to rely on another sense for that - touch. In contrast, the current environment is saturated with communication technologies that are skewed toward the visual and auditory channels. Individuals talk on mobile phones, chat via video conferencing, or message through various texting applications. Without being physically present with one another, individuals can connect in a variety of ways. Despite advances in new technologies, rich touch feedback is largely missing in remote communication tools. Most often, touch feedback is used as a cueing mechanism such as through vibrations that alert the user about the arrival of a message but convey little of its content. The omission of touch feedback prevents many new technologies from capturing the social and emotional dimensions of the experience and leaves new technologies inaccessible to those who rely on touch as a primary means of communication. This project rethinks touch-based, remote communication by leveraging the tactile intuitions of the protactile DeafBlind (DB) community. From this innovative community, a new language is emerging that enables humans to communicate with one another solely through touch. This project, in partnership with the DB community, aims to understand the core functions of intuitive, touch-based communication signals. Findings will inform the design of new wearable devices that attempt to replicate these touch-based profiles for richer remote communication experiences for all individuals. \r\n\r\nThe research goals of this project are to (a) understand how core components of local and remote interaction can be re-routed through the tactile sensory channel and (b) design technology which will support such re-routings by expanding and reinforcing the tactile capacities of its users. To accomplish these aims, anthropological approaches to communication are synthesized with engineering to drive the design of a wearable haptic device that enables core functions of communication via tactile feedback. Using rapid ethnography coupled with haptic systems design, this work will establish guidelines for tactile-based communication schemes while creating a platform from which independent remote communication among DB individuals can evolve and richer communication experiences for all individuals are possible. This project will increase the general understanding of the tactile channel as a primary mode of communication. It will also advance understanding of human abilities to communicate in multimodal ways, promoting optimal information transfer when all senses are available and promoting higher standards for access of information when sensory differences exist.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jenna",
   "pi_last_name": "Gorlewicz",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Jenna L Gorlewicz",
   "pi_email_addr": "jenna.gorlewicz@slu.edu",
   "nsf_id": "000656195",
   "pi_start_date": "2019-09-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Terra",
   "pi_last_name": "Edwards",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Terra Edwards",
   "pi_email_addr": "terraedwards@uchicago.edu",
   "nsf_id": "000655910",
   "pi_start_date": "2019-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Saint Louis University",
  "inst_street_address": "221 N GRAND BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "SAINT LOUIS",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "3149773925",
  "inst_zip_code": "631032006",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MO01",
  "org_lgl_bus_name": "SAINT LOUIS UNIVERSITY",
  "org_prnt_uei_num": "JNBLLTBTLLD8",
  "org_uei_num": "JNBLLTBTLLD8"
 },
 "perf_inst": {
  "perf_inst_name": "Saint Louis University",
  "perf_str_addr": "221 N. Grand Blvd",
  "perf_city_name": "St Louis",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "631032006",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our environment is saturated with communication technologies that heavily rely on sight and hearing. Individuals talk on mobile phones, chat via video conferencing, or message through various texting applications - all without being physically present with one another. Despite advances in new technologies, rich touch feedback is largely missing in remote communication tools. Most often, touch feedback is used as a cueing mechanism such as through vibrations that alert the user about the arrival of a message but convey little of its content. This is in spite of the fact that touch plays an integral role in face-to-face communication, where individuals can interpret emotions, gestures, and other factors through touch cues. The omission of touch feedback prevents many new technologies from capturing the social and emotional dimensions of the experience and leaves new technologies inaccessible to those who rely on touch as a primary means of communication. The objective of this project was to understand how the key building blocks of communication can be routed through touch to improve remote communication technologies for all. Specifically, we developed two wearable touch devices: one for a remote communication setting and one for an educational setting. The Conversational Haptic Technology (CHAT) system is a wearable device that enables two-way, remote communication through touch with a focus on conveying presence, attention getting, emotion, and referencing. The HapConnect system is a wearable device that brings early touch design experiences into the classroom for teaching and learning in engineering design. Both systems employed a human-centered, iterative design approach, working with collaborators in the Protactile DeafBlind and Blind and Low Vision communities. We completed human user studies with over 100 individuals, demonstrating that the key building blocks of communication can be created and interpreted via touch, foreshadowing a future where remote communication tools can become more effective and inclusive by incorporating touch feedback.<br /><br />Intellectual Merit: This project has produced (1) comprehensive, published overviews of the system hardware and electronics used to create two touch-based systems for remote communication and educational settings; (2) two system prototypes supported by human user studies with over 100 individuals illustrating the potential for touch-based systems to enhance numerous settings; (3) a rich data set on touch cues employed by users from diverse groups in communication contexts informing how to design touch-based feedback for future technologies; and (4) a set of lessons and learning modules for teaching about touch technology design in a classroom context. The two systems created in this work along with the data collected from diverse user groups provide a framework for touch to be elevated as a primary mode of communication.&nbsp; Learning from the protactile language, a purely touch-based language, this work establishes guidelines for touch-based communication schemes and the design of touch-based technologies. At the same time, this work demonstrates how touch-based technologies can enhance the teaching and learning of engineering students. Taken together, this work combines advances in wearable devices, haptics, and linguistic anthropology, to reroute key building blocks of communication through touch - opening up pathways to advance remote communication technologies broadly. <br /><br />Broader Impacts: This project works towards solutions to address two growing gaps in remote communication - the visual fatigue and lack of connection that&rsquo;s often experienced for users and the lack of access for individuals who rely on touch as a primary mode of communication.&nbsp; This research provides a framework for elevating touch as a primary mode of communication in the the design of new technologies, illustrating new possibilities for remote work, learning, and living, rooted in human intuition. This work has resulted in 3 peer-reviewed publications, several presentations and demonstrations to academic, industry, and community audiences. This project also directly involved over 100 individuals in the research studies, stretching across communities including those who are DeafBlind, blind, or low vision. The HapConnect system designed in this work was deployed in an undergraduate engineering course to teach principles of haptics and design thinking. This work has also been disseminated across national and regional events, including technical conferences, demonstrations, community workshops, youth summer camps, and classroom settings. The outcomes of this project have laid the foundation for elevating remote communication to increase its effectiveness and access, with a focus on bringing back the very human element of touch.</p><br>\n<p>\n Last Modified: 02/07/2025<br>\nModified by: Jenna&nbsp;L&nbsp;Gorlewicz</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nOur environment is saturated with communication technologies that heavily rely on sight and hearing. Individuals talk on mobile phones, chat via video conferencing, or message through various texting applications - all without being physically present with one another. Despite advances in new technologies, rich touch feedback is largely missing in remote communication tools. Most often, touch feedback is used as a cueing mechanism such as through vibrations that alert the user about the arrival of a message but convey little of its content. This is in spite of the fact that touch plays an integral role in face-to-face communication, where individuals can interpret emotions, gestures, and other factors through touch cues. The omission of touch feedback prevents many new technologies from capturing the social and emotional dimensions of the experience and leaves new technologies inaccessible to those who rely on touch as a primary means of communication. The objective of this project was to understand how the key building blocks of communication can be routed through touch to improve remote communication technologies for all. Specifically, we developed two wearable touch devices: one for a remote communication setting and one for an educational setting. The Conversational Haptic Technology (CHAT) system is a wearable device that enables two-way, remote communication through touch with a focus on conveying presence, attention getting, emotion, and referencing. The HapConnect system is a wearable device that brings early touch design experiences into the classroom for teaching and learning in engineering design. Both systems employed a human-centered, iterative design approach, working with collaborators in the Protactile DeafBlind and Blind and Low Vision communities. We completed human user studies with over 100 individuals, demonstrating that the key building blocks of communication can be created and interpreted via touch, foreshadowing a future where remote communication tools can become more effective and inclusive by incorporating touch feedback.\n\nIntellectual Merit: This project has produced (1) comprehensive, published overviews of the system hardware and electronics used to create two touch-based systems for remote communication and educational settings; (2) two system prototypes supported by human user studies with over 100 individuals illustrating the potential for touch-based systems to enhance numerous settings; (3) a rich data set on touch cues employed by users from diverse groups in communication contexts informing how to design touch-based feedback for future technologies; and (4) a set of lessons and learning modules for teaching about touch technology design in a classroom context. The two systems created in this work along with the data collected from diverse user groups provide a framework for touch to be elevated as a primary mode of communication. Learning from the protactile language, a purely touch-based language, this work establishes guidelines for touch-based communication schemes and the design of touch-based technologies. At the same time, this work demonstrates how touch-based technologies can enhance the teaching and learning of engineering students. Taken together, this work combines advances in wearable devices, haptics, and linguistic anthropology, to reroute key building blocks of communication through touch - opening up pathways to advance remote communication technologies broadly. \n\nBroader Impacts: This project works towards solutions to address two growing gaps in remote communication - the visual fatigue and lack of connection thats often experienced for users and the lack of access for individuals who rely on touch as a primary mode of communication. This research provides a framework for elevating touch as a primary mode of communication in the the design of new technologies, illustrating new possibilities for remote work, learning, and living, rooted in human intuition. This work has resulted in 3 peer-reviewed publications, several presentations and demonstrations to academic, industry, and community audiences. This project also directly involved over 100 individuals in the research studies, stretching across communities including those who are DeafBlind, blind, or low vision. The HapConnect system designed in this work was deployed in an undergraduate engineering course to teach principles of haptics and design thinking. This work has also been disseminated across national and regional events, including technical conferences, demonstrations, community workshops, youth summer camps, and classroom settings. The outcomes of this project have laid the foundation for elevating remote communication to increase its effectiveness and access, with a focus on bringing back the very human element of touch.\t\t\t\t\tLast Modified: 02/07/2025\n\n\t\t\t\t\tSubmitted by: JennaLGorlewicz\n"
 }
}