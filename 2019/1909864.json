{
 "awd_id": "1909864",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: APERTURE: Augmented Reality based Perception-Sensitive Robotic Gesture",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 257349.0,
 "awd_amount": 273349.0,
 "awd_min_amd_letter_date": "2019-08-31",
 "awd_max_amd_letter_date": "2020-05-05",
 "awd_abstract_narration": "How can a robot choose the \"best\" modality for drawing the attention of a human and communicating the needed information? This is the central theme of the project with the assumption that the human team mates of the robots use augmented reality (AR) based visualization. Specifically, this project plans the following three major research activities for enabling robots to communicate with human teammates in a way that is tailored to their teammates' current mental states: exploring how augmented reality technologies (such as the Microsoft Hololens) can be used to provide robots with new ways to communicate about objects, locations, and people in their environments, especially when used together with communication through spoken language; examining how technologies can non-invasively measure different aspects of teammates' mental states, including how much mental workload, perceptual workload, stress, and frustration they are experiencing; and determining how robots can choose the best way to communicate with their human teammates when the two technologies are combined, (for example, through language alone, AR visualizations alone, or both used together), based on those teammates' individual mental states. This system will then be used to test how it might improve the safety and productivity of underground workers, by allowing robots to communicate in a way that is more effective and less cognitively demanding. While the researchers will be investigating the effectiveness of these integrated technologies specifically within underground work environments, the research will also be applicable to a wide variety of areas, including eldercare, urban search-and-rescue, and space robotics, and will have broad scientific impact across both computer science and cognitive science.\r\n\r\nThe above goals will be achieved through APERTURE, a novel framework integrating head-mounted augmented reality displays, a multimodal suite of noninvasive, lightweight, and field-ready physiological sensors (such as  functional near-infrared spectroscopy (fNIRS), Electroencaphalography (EEG), Electrodermal Activity, Electrocardiogram (ECG), and Respiration sensors), and unmanned ground robots, within a cognitive robotic architecture. APERTURE will be built by integrating the Distributed Integrated Affect Reflection Cognition (DIARC) architecture with these robotic, augmented reality, and physiological hardware elements.  The project will design and evaluate physiological sensing models, augmented reality gestural cues, and machine learning models for selecting between AR gestural cues based on neurophysiological data. The designed machine learning models will classify users' cognitive and affective states from this sensor data, and help the robots understand when and how to communicate based on users' cognitive and affective states. The novel AR approach to deictic gesture will help robots pick out the objects they are referring to through the use of visualizations displayed in their teammates' augmented reality headsets.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Williams",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas Williams",
   "pi_email_addr": "twilliams@mines.edu",
   "nsf_id": "000762830",
   "pi_start_date": "2019-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Colorado School of Mines",
  "inst_street_address": "1500 ILLINOIS ST",
  "inst_street_address_2": "",
  "inst_city_name": "GOLDEN",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3032733000",
  "inst_zip_code": "804011887",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "CO07",
  "org_lgl_bus_name": "TRUSTEES OF THE COLORADO SCHOOL OF MINES",
  "org_prnt_uei_num": "JW2NGMP4NMA3",
  "org_uei_num": "JW2NGMP4NMA3"
 },
 "perf_inst": {
  "perf_inst_name": "Colorado School of Mines",
  "perf_str_addr": "1500 Illinois Street",
  "perf_city_name": "Golden",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "804011887",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "CO07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 257349.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-d2278d78-7fff-4abe-8d3a-2d9ce9b4474e\">\n<p dir=\"ltr\"><span>Augmented reality head-mounted displays are being or will be deployed in many domains of national interest, from advanced and collaborative manufacturing, to warehousing and logistics, to urban and alpine search and rescue. Moreover, these displays are often being used in the context of human-robot interactions, providing a novel channel that can be used to communicate information about robots&rsquo; perceptions and intentions that would otherwise be obscured from human interactants. The APERTURE project sought to answer a fundamental question: whether within or beyond human-robot interaction contexts, how should augmented reality devices communicate with humans about objects of interest, to effectively pick out relevant objects without cognitively overloading interactants?</span></p>\n<br />\n<p dir=\"ltr\"><span>To answer these questions, the project team pursued two strands of research.&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>First, the team designed and evaluated nonverbal cues that could be rendered in mixed reality interactions, ranging from circles and arrows directly picking out target objects, to virtual arms reaching out to indicate objects, to combinations thereof. Moreover, the project team explored the effectiveness of these augmented reality cues on their own, or paired with natural language of different levels of complexity. These efforts demonstrated the effectivness of the designed augmented reality cues for achieving different results, showed how these cues could be just as effective as those generated by physical robot arms, and demonstrated how combining different types of cues together enabled the &ldquo;best of both worlds&rdquo;. Overall, this thrust produced a range of design guidelines that can guide the deployment of mixed reality gestures across the domains of national interest listed above.</span></p>\n<br />\n<p dir=\"ltr\"><span>Second, the team explored how to measure different levels and types of cognitive load through brain-computer interfaces. Specifically, the project team developed new approaches to cognitive load estimated grounded in multiple resource theory. Our results demonstrate that functional Near Infrared Spectroscopy (fNIRS) can be used not only to measure overall cognitive load as shown in previous work, but moreover to measure different </span><span>types</span><span> of cognitive load -- auditory, visual, and working memory, and that fNIRS is overall both sensitive and diagnostic to load in complex tasks. Moreover, we have used the software testbed developed for this experiment to explore how different types of mixed-reality communication might perform under different levels and types of cognitive load, providing promising initial findings findings that may guide future research and enhance the design of human-technology interactions in mixed reality environments.</span></p>\n<br />\n<p dir=\"ltr\"><span>Finally, in addition to these research activities, the project team has used this project to do significant community building, growing the community of &ldquo;Virtual, Augmented, and Mixed Reality for Human Robot Interaction&rdquo; researchers into a vibrant subcommunity.</span></p>\n<br /></span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/30/2023<br>\nModified by: Thomas&nbsp;Williams</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nAugmented reality head-mounted displays are being or will be deployed in many domains of national interest, from advanced and collaborative manufacturing, to warehousing and logistics, to urban and alpine search and rescue. Moreover, these displays are often being used in the context of human-robot interactions, providing a novel channel that can be used to communicate information about robots perceptions and intentions that would otherwise be obscured from human interactants. The APERTURE project sought to answer a fundamental question: whether within or beyond human-robot interaction contexts, how should augmented reality devices communicate with humans about objects of interest, to effectively pick out relevant objects without cognitively overloading interactants?\n\n\n\n\nTo answer these questions, the project team pursued two strands of research.\n\n\n\n\nFirst, the team designed and evaluated nonverbal cues that could be rendered in mixed reality interactions, ranging from circles and arrows directly picking out target objects, to virtual arms reaching out to indicate objects, to combinations thereof. Moreover, the project team explored the effectiveness of these augmented reality cues on their own, or paired with natural language of different levels of complexity. These efforts demonstrated the effectivness of the designed augmented reality cues for achieving different results, showed how these cues could be just as effective as those generated by physical robot arms, and demonstrated how combining different types of cues together enabled the best of both worlds. Overall, this thrust produced a range of design guidelines that can guide the deployment of mixed reality gestures across the domains of national interest listed above.\n\n\n\n\nSecond, the team explored how to measure different levels and types of cognitive load through brain-computer interfaces. Specifically, the project team developed new approaches to cognitive load estimated grounded in multiple resource theory. Our results demonstrate that functional Near Infrared Spectroscopy (fNIRS) can be used not only to measure overall cognitive load as shown in previous work, but moreover to measure different types of cognitive load -- auditory, visual, and working memory, and that fNIRS is overall both sensitive and diagnostic to load in complex tasks. Moreover, we have used the software testbed developed for this experiment to explore how different types of mixed-reality communication might perform under different levels and types of cognitive load, providing promising initial findings findings that may guide future research and enhance the design of human-technology interactions in mixed reality environments.\n\n\n\n\nFinally, in addition to these research activities, the project team has used this project to do significant community building, growing the community of Virtual, Augmented, and Mixed Reality for Human Robot Interaction researchers into a vibrant subcommunity.\n\n\n\n\n\t\t\t\t\tLast Modified: 11/30/2023\n\n\t\t\t\t\tSubmitted by: ThomasWilliams\n"
 }
}