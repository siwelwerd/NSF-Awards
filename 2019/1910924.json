{
 "awd_id": "1910924",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Addressing Challenges for the Next Decade of Massively Parallel NUMA Accelerators",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 495380.0,
 "awd_amount": 495380.0,
 "awd_min_amd_letter_date": "2019-07-19",
 "awd_max_amd_letter_date": "2019-07-19",
 "awd_abstract_narration": "The physical and economic principles that enabled Dennard scaling and Moore's law in the semiconductor industry have reached their breaking point. However, as the number of transistors economically fabricated on a single chip plateaus, the processor industry has pivoted to create single-package computing systems, composed of multiple sub-components known as chiplets. Chiplets, which communicate via high-bandwidth on-package networks, offer the potential for transparent performance scaling into the next decade. However, chiplets introduce challenging non-uniform memory access characteristics into single-package systems that have traditionally not been subject to these effects. This project develops techniques to overcome the challenges of non-uniform memory accesses on high-performance single- and multi-package systems without programmer intervention. Exploring programmer-transparent scaling mechanisms improves the portability and lifetime of programs, decreasing the cost and complexity of software. Through the creation of course content and undergraduate summer internships, the project fosters an understanding of how to program machines in a post-Moore world and how compute accelerators should be designed to minimize the impact on the end-programmer as system complexity increases.\r\n\r\nThis project develops coordinated data placement and thread scheduling algorithms that leverage static information from the compiler and dynamic information from the runtime system to inform data placement and hardware-based thread scheduling. It advances the state-of-the-art by developing an open-source Graphic Processing Unit (GPU) simulator with a hierarchical interconnect that can be used to model both chiplet-based GPUs and multi-GPU systems. The researchers are exploring compiler informed data placement and thread scheduling in GPUs. Initial results demonstrate that a static analysis of the code can predict the data accessed by GPU threadblocks. Analysis shows that it is possible to determine which threads in a grid share memory pages, and the manner of that sharing, by building new static techniques that add an additional dimension to decades of work on compilers for sequential code. Using static information, in combination with runtime information provided by GPU drivers, the researchers are developing advanced data placement, prefetching, and thread scheduling algorithms. Both future chiplet-based designs and existing multi-GPU systems benefit from the development of these algorithms. Looking beyond the high-bandwidth memory used in GPUs today the project explores the system-level implications of heterogeneous memory in a chiplet-based system. Data placement and thread scheduling have even more importance in GPU systems of the future that make use of high bandwidth memory, traditional dynamic random-access memory, and non-volatile memory. The problem sizes in such systems are anticipated to be so large that opportunistic data placement and thread scheduling are even more critical than in conventional systems. The project uses sharing patterns based on the inter-kernel producer-consumer nature of machine learning workloads to change the program's code layout, runtime data placement, and threadblock scheduling algorithm to maximize locality in multi-node systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Timothy",
   "pi_last_name": "Rogers",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Timothy G Rogers",
   "pi_email_addr": "timrogers@purdue.edu",
   "nsf_id": "000723981",
   "pi_start_date": "2019-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072035",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 495380.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project studied the effects of scaling programmable acceleration hardware as Moore's law ends. The project produced the most accurate open-source model for contemporary Graphics Processing Units (GPUs), which can simulate the industry's most advanced machine-learning workloads. Evaluating the performance and energy consumption of such workloads on future systems is challenging due to their scale and the inherent slowdown of simulation. A key outcome of this project was a credible sampling mechanism to evaluate future hardware architecture changes in a tractable manner. The simulation and sampling infrastructure is open source, freely available to the public and industry, and has seen significant use from academics and product teams.</p>\n<p><br />At the macro level, this project used the infrastructure it created to evaluate novel mechanisms to place data and schedule work in massive GPU systems, providing insights into how vital workloads can be scaled today and in the future. Similarly, at the micro-level, the project developed mechanisms to help mitigate the effects of disaggregating massive cores into smaller sub-cores for energy efficiency gains and proposed new techniques to provide deterministic execution on GPUs, making the development of new machine learning algorithms more predictable.</p>\n<p><br />Looking beyond the current GPU landscape, the project explored a new and different type of programmable accelerator, with many of the energy-efficiency qualities of the GPU combined with carefully selected latency-optimizing mechanisms from contemporary Central Processing Units (CPUs). This new accelerator targets production microservices, significantly increasing their energy efficiency compared to execution on CPUs while still maintaining acceptable latency such that production web services meet their strict quality of service demands.</p>\n<p><br />In total, this project has made intellectual contributions to the simulation space, macro-level accelerator scaling, and micro-level accelerator scaling and has proposed programmable acceleration for the microservices space.</p><br>\n<p>\n Last Modified: 02/21/2024<br>\nModified by: Timothy&nbsp;G&nbsp;Rogers</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project studied the effects of scaling programmable acceleration hardware as Moore's law ends. The project produced the most accurate open-source model for contemporary Graphics Processing Units (GPUs), which can simulate the industry's most advanced machine-learning workloads. Evaluating the performance and energy consumption of such workloads on future systems is challenging due to their scale and the inherent slowdown of simulation. A key outcome of this project was a credible sampling mechanism to evaluate future hardware architecture changes in a tractable manner. The simulation and sampling infrastructure is open source, freely available to the public and industry, and has seen significant use from academics and product teams.\n\n\n\nAt the macro level, this project used the infrastructure it created to evaluate novel mechanisms to place data and schedule work in massive GPU systems, providing insights into how vital workloads can be scaled today and in the future. Similarly, at the micro-level, the project developed mechanisms to help mitigate the effects of disaggregating massive cores into smaller sub-cores for energy efficiency gains and proposed new techniques to provide deterministic execution on GPUs, making the development of new machine learning algorithms more predictable.\n\n\n\nLooking beyond the current GPU landscape, the project explored a new and different type of programmable accelerator, with many of the energy-efficiency qualities of the GPU combined with carefully selected latency-optimizing mechanisms from contemporary Central Processing Units (CPUs). This new accelerator targets production microservices, significantly increasing their energy efficiency compared to execution on CPUs while still maintaining acceptable latency such that production web services meet their strict quality of service demands.\n\n\n\nIn total, this project has made intellectual contributions to the simulation space, macro-level accelerator scaling, and micro-level accelerator scaling and has proposed programmable acceleration for the microservices space.\t\t\t\t\tLast Modified: 02/21/2024\n\n\t\t\t\t\tSubmitted by: TimothyGRogers\n"
 }
}