{
 "awd_id": "1926949",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase II:  The Data Context Map",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2019-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 749380.0,
 "awd_amount": 759343.0,
 "awd_min_amd_letter_date": "2019-07-31",
 "awd_max_amd_letter_date": "2021-10-06",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is that it will help scientists gain a better understanding of the underlying relationships in their data and as a result help humans make better decisions from data. The advantage of big data is that it allows for the possibility of identifying truly meaningful relationships. Exposing these relationships can provide great benefits for many different problem domains, and therefore has high commercial appeal in applications such as fraud detection in credit card or insurance data, drug discovery, and identifying threats to national security in homeland security data. Identifying previously unseen relationships also plays a big part in scientific research and discovery. Great scientific discoveries come from a deep understanding of the world around us. Data can capture these relationships, but this is only useful if a researcher can identify them. This motivates the need for sophisticated tools that can present these relationships in a comprehensible way. \r\n\r\nThis Small Business Innovation Research (SBIR) Phase II project will address the problem of mining and visualizing very high dimensional and time varying data sets. Modern data sets can have many thousands of attributes which can introduce a significant amount of noise and conflicting relationships. This problem is compounded in time varying datasets as some relationships can be inconsistent and disappear over time. There is hence a great need to find and explain temporally consistent and reliable relationships in large data sets. This Phase II project will accomplish this by developing a set of novel interactive visualizations that will find these consistent relationships and explain how they change over time. It will use causal analysis and also extend it to the temporal domain in which causes with delayed effects will be identified (e.g. smoking causes cancer after many decades). Finally, as these data mining techniques can be very computationally expensive, this Phase II project will develop useful optimizations to make them more efficient. The company expects that the results of this research will enable scientists and consequently end users to identify previously unseen relationships, leading to new discoveries.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Papenhausen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eric Papenhausen",
   "pi_email_addr": "epapenha@akaikaeru.com",
   "nsf_id": "000787419",
   "pi_start_date": "2019-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Akai Kaeru, LLC",
  "inst_street_address": "302 E 88TH ST APT 4F",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6466442034",
  "inst_zip_code": "101284931",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NY12",
  "org_lgl_bus_name": "AKAI KAERU, LLC",
  "org_prnt_uei_num": "X8FNW12ZSCB3",
  "org_uei_num": "X8FNW12ZSCB3"
 },
 "perf_inst": {
  "perf_inst_name": "Akai Kaeru, LLC",
  "perf_str_addr": "302 E. 88 Str, # 4F",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "101284931",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5373",
   "pgm_ref_txt": "SMALL BUSINESS PHASE II"
  },
  {
   "pgm_ref_code": "8032",
   "pgm_ref_txt": "Software Services and Applications"
  },
  {
   "pgm_ref_code": "8240",
   "pgm_ref_txt": "SBIR/STTR CAP"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 749380.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 9963.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Making predictions based on data has become common practice. All one requires is a dataset with features that are relevant to the predicted quantity and a sufficient number of past observations. The datasets are typically organized as a two-dimensional sheet where the columns are the features and the rows are the observations. For example, one might want to predict whether a certain house will increase in price or not, or alternatively, whether a certain house is a good deal or not. There are many features such as zip code, size, age, condition, number of bedrooms, swimming pool, garage, heating source, type of roof, proximity to shopping, schools quality, night life choices, and a myriad of others that can describe a house at an abundance of detail. Next, to make faithful predictions you also need many examples of describable houses where the price is known. Finally, you throw all this into a machine learning model and train it, and once the model is trained it?s ready to make predictions. Sounds easy, right?</p>\n<p>At first glance it seems fairly easy. There is no shortage of packages and tools that can help you build a machine learning model. But is it a good machine learning model? Will it predict the house price accurately? And can it be trained in a reasonable amount of time?&nbsp; You may think that the more features you have gathered, the better the predictions will be. Unfortunately, this is a fallacy and a common misconception. While most features are important, they are just not all important at the same time. To predict a reliable price it might already be sufficient to know that a house has solar panels for heating and good access to high quality schools. So we have essentially two features with associated levels, type of heating = solar and neighborhood school quality = good, which as a pair can predict that a house will have high value. It is a very simple machine learning model with just two features. The levels of all other features no longer matter because the grand majority of all those houses in the dataset have favorable values in them. So there?s no need to drag them along.</p>\n<p>You will likely think that it can?t be that only houses with solar panels and access to good schools are valuable. Indeed, there are typically many such small sets of features -- call them patterns -- that can point to valuable houses. Likewise, there are many such patterns that can indicate houses that are poor deals. &nbsp;So now the million dollar question is -- how can one find these relevant patterns and their associated value ranges. Say you have 100 features which is not uncommon and actually on the low end of most data science applications, then there are nearly 5,000 2-feature patterns and over 160,000 3-feature patterns. Only a very small fraction of these are patterns that matter for predicting whether a house is valuable or a bad deal. &nbsp;</p>\n<p>To address this challenge we devised a suite of sophisticated statistical analysis and machine learning techniques that can identify highly predictive patterns in large datasets with 100s, 1,000s and even 10,000s of features in an efficient manner. But this is not all. We also developed a software package that allows anyone with a modern computer to extract these patterns from their data. No coding skills are required. Just throw in your dataset, use our data transformer to clean your data from missing values, outliers, etc., perform aggregations, derive new features, specify targets of interest, then mine the transformed data for patterns, and finally inspect them with our interactive visual pattern browser. Our #1 mission is to help you gain a better understanding of your data and foster a better relationship with them. Knowing which features matter and when, will empower you to build better, lighter, and more efficient machine learning models, and do so with confidence in little time. &nbsp;</p>\n<p>We end by sharing one of our many success stories. Back in May 2020, at the early stages of COVID we downloaded a dataset made available by the CDC. It listed about 300 social vulnerability indicators for each US county and combined it with the May 2020 COVID-19 death rate statistics. Using our software we identified a set of patterns, each composed of 1-3 features, that were semantically meaningful feature combinations the counties needed to have to be at elevated risk for higher-than-average COVID-19 death rates. It was surprising (and reassuring) to us that 95% of these patterns were able to steadfastly predict the counties that saw their death toll rise well above normal not long after. Unfortunately we were too unknown to gain the attention of policy makers, but maybe our warnings could have saved some lives.&nbsp;&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/11/2022<br>\n\t\t\t\t\tModified by: Eric&nbsp;Papenhausen</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660267808703_patternbrowser--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660267808703_patternbrowser--rgov-800width.jpg\" title=\"Pattern Browser Interface\"><img src=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660267808703_patternbrowser--rgov-66x44.jpg\" alt=\"Pattern Browser Interface\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Shown here is an analysis of a stock dataset. Each bubble represents a group of data points (a pattern) that are similar in some way. The y-axis is the target variable (Returns) and the x-axis is one of the features (Volatility) chosen by the user based on the feature importance plot (top left).</div>\n<div class=\"imageCredit\">Akai Kaeru LLC</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Eric&nbsp;Papenhausen</div>\n<div class=\"imageTitle\">Pattern Browser Interface</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660268124907_pipeline--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660268124907_pipeline--rgov-800width.jpg\" title=\"Visual data analysis pipeline\"><img src=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660268124907_pipeline--rgov-66x44.jpg\" alt=\"Visual data analysis pipeline\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Shows the data processing pipeline the user has constructed by drag and drop operations for the stock data analysis. From left to right, there is the file loading module, the data transformer module, the pattern mining module, and the pattern browser module. The data flows from left to right.</div>\n<div class=\"imageCredit\">Akai Kaeru LLC</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Eric&nbsp;Papenhausen</div>\n<div class=\"imageTitle\">Visual data analysis pipeline</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660269396831_pipeline2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660269396831_pipeline2--rgov-800width.jpg\" title=\"Another, more complex execution pipeline\"><img src=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660269396831_pipeline2--rgov-66x44.jpg\" alt=\"Another, more complex execution pipeline\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Shows a more involved execution pipeline that implements four data analysis pipelines with different target variable conditions. All build on a common initial data preparation pipeline (left part).</div>\n<div class=\"imageCredit\">Akai Kaeru LLC</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Eric&nbsp;Papenhausen</div>\n<div class=\"imageTitle\">Another, more complex execution pipeline</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660269798194_transformer--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660269798194_transformer--rgov-800width.jpg\" title=\"Data transformer interface\"><img src=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660269798194_transformer--rgov-66x44.jpg\" alt=\"Data transformer interface\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Preparing the data before analysis and model building can take place is usually a very laborious and tedious process. Our software offers a comprehensive visual interface for this that makes this work much more efficient. Here we see the Returns variable where the user is alerted to a correlation.</div>\n<div class=\"imageCredit\">Akai Kaeru LLC</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Eric&nbsp;Papenhausen</div>\n<div class=\"imageTitle\">Data transformer interface</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660270725845_fig1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660270725845_fig1--rgov-800width.jpg\" title=\"COVID-19 death rate analysis\"><img src=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660270725845_fig1--rgov-66x44.jpg\" alt=\"COVID-19 death rate analysis\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Shows a study we conducted in May 2020 to see why certain US counties were more prone to high COVID-19 deaths. Example pattern is defined by 3 features: (a) county poverty rate; (b) age of county population; (c) county population density; (d) map with the pattern\ufffds counties, shading = death rate.</div>\n<div class=\"imageCredit\">Akai Kaeru LLC</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Eric&nbsp;Papenhausen</div>\n<div class=\"imageTitle\">COVID-19 death rate analysis</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660271012357_fig2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660271012357_fig2--rgov-800width.jpg\" title=\"Shows the predictive power of our patterns\"><img src=\"/por/images/Reports/POR/2022/1926949/1926949_10628392_1660271012357_fig2--rgov-66x44.jpg\" alt=\"Shows the predictive power of our patterns\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Predictive power of the pattern in  the other figure. This figure compares June vs. May 2020: (a) COVID-19 death rate by county, adjusted for US-wide county death rate average; (b) death growth rate by county, adjusted for US-wide growth; (c) county map comparison.</div>\n<div class=\"imageCredit\">Akai Kaeru LLC</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Eric&nbsp;Papenhausen</div>\n<div class=\"imageTitle\">Shows the predictive power of our patterns</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nMaking predictions based on data has become common practice. All one requires is a dataset with features that are relevant to the predicted quantity and a sufficient number of past observations. The datasets are typically organized as a two-dimensional sheet where the columns are the features and the rows are the observations. For example, one might want to predict whether a certain house will increase in price or not, or alternatively, whether a certain house is a good deal or not. There are many features such as zip code, size, age, condition, number of bedrooms, swimming pool, garage, heating source, type of roof, proximity to shopping, schools quality, night life choices, and a myriad of others that can describe a house at an abundance of detail. Next, to make faithful predictions you also need many examples of describable houses where the price is known. Finally, you throw all this into a machine learning model and train it, and once the model is trained it?s ready to make predictions. Sounds easy, right?\n\nAt first glance it seems fairly easy. There is no shortage of packages and tools that can help you build a machine learning model. But is it a good machine learning model? Will it predict the house price accurately? And can it be trained in a reasonable amount of time?  You may think that the more features you have gathered, the better the predictions will be. Unfortunately, this is a fallacy and a common misconception. While most features are important, they are just not all important at the same time. To predict a reliable price it might already be sufficient to know that a house has solar panels for heating and good access to high quality schools. So we have essentially two features with associated levels, type of heating = solar and neighborhood school quality = good, which as a pair can predict that a house will have high value. It is a very simple machine learning model with just two features. The levels of all other features no longer matter because the grand majority of all those houses in the dataset have favorable values in them. So there?s no need to drag them along.\n\nYou will likely think that it can?t be that only houses with solar panels and access to good schools are valuable. Indeed, there are typically many such small sets of features -- call them patterns -- that can point to valuable houses. Likewise, there are many such patterns that can indicate houses that are poor deals.  So now the million dollar question is -- how can one find these relevant patterns and their associated value ranges. Say you have 100 features which is not uncommon and actually on the low end of most data science applications, then there are nearly 5,000 2-feature patterns and over 160,000 3-feature patterns. Only a very small fraction of these are patterns that matter for predicting whether a house is valuable or a bad deal.  \n\nTo address this challenge we devised a suite of sophisticated statistical analysis and machine learning techniques that can identify highly predictive patterns in large datasets with 100s, 1,000s and even 10,000s of features in an efficient manner. But this is not all. We also developed a software package that allows anyone with a modern computer to extract these patterns from their data. No coding skills are required. Just throw in your dataset, use our data transformer to clean your data from missing values, outliers, etc., perform aggregations, derive new features, specify targets of interest, then mine the transformed data for patterns, and finally inspect them with our interactive visual pattern browser. Our #1 mission is to help you gain a better understanding of your data and foster a better relationship with them. Knowing which features matter and when, will empower you to build better, lighter, and more efficient machine learning models, and do so with confidence in little time.  \n\nWe end by sharing one of our many success stories. Back in May 2020, at the early stages of COVID we downloaded a dataset made available by the CDC. It listed about 300 social vulnerability indicators for each US county and combined it with the May 2020 COVID-19 death rate statistics. Using our software we identified a set of patterns, each composed of 1-3 features, that were semantically meaningful feature combinations the counties needed to have to be at elevated risk for higher-than-average COVID-19 death rates. It was surprising (and reassuring) to us that 95% of these patterns were able to steadfastly predict the counties that saw their death toll rise well above normal not long after. Unfortunately we were too unknown to gain the attention of policy makers, but maybe our warnings could have saved some lives.  \n\n\t\t\t\t\tLast Modified: 08/11/2022\n\n\t\t\t\t\tSubmitted by: Eric Papenhausen"
 }
}