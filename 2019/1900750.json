{
 "awd_id": "1900750",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CIF: Medium: Collaborative Research: Information-theoretic Guarantees on Privacy in the Age of Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2019-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 383000.0,
 "awd_amount": 383000.0,
 "awd_min_amd_letter_date": "2019-05-30",
 "awd_max_amd_letter_date": "2022-07-21",
 "awd_abstract_narration": "Armed with powerful advances in machine learning, the ability of an interested party to gather personal information from an individual's expanding digital footprint is outstripping anyone's capability to keep their information private. While this aggregated data can have tremendous benefit for consumers and data scientists via technologies built on machine learning and artificial intelligence, this benefit must be tempered with meaningful assurances of privacy for the very people who provided the data in the first place. This project adopts a rigorous information-theoretic approach to give meaningful privacy guarantees while still providing statistical utility. By combining theoretical and data-driven research, this project can inform public policy as well as best-practices for industry. The overall goal is to provide any data scientist with a set of tools to guarantee meaningful privacy in practice. To do so, this project explores meaningful measures of privacy leakage in the learning context, characterizes the fundamental tradeoffs between privacy and utility, develops techniques to ensure privacy in realistic settings, and tests these algorithms on publicly available datasets. The project is also committed to broadening participation in computing via two outreach efforts: (i) interactive demonstrations of privacy issues that stem from using social media to middle and high school students via ASU's annual STEM event, Open Door, and (ii) teaching modules on machine learning (ML) and artificial intelligence (AI), and short courses (\"data jams\") at ASU via the Young Engineers Shape the World (YESW) summer program and at Harvard; these modules, targeted at female, financially disadvantaged, and Latino and Hispanic students, aim to make a meaningful contribution to increasing a diverse STEM workforce by providing students hands-on experience on basic concepts of coding, manipulating datasets, and producing simple visualizations collectively. Outreach efforts will be evaluated using well understood metrics for assessment of student interest, engagement, and knowledge via ASU?s College Research and Evaluation Services Team (CREST).\r\n\r\nThis project aims to derive a foundational, statistical theory of privacy that builds upon and contributes to modern theoretical advances in information theory and machine learning. The statistical nature of inference (both for legitimate and illegitimate ends) requires a statistical approach to measuring and ensuring privacy and utility. A significant novel element derived from this view is the maximal alpha leakage, a new, tunable measure for information leakage which quantifies the ability of an adversary to learn any function of private data via a parametric class of loss functions. This tunable measure is derived from a rich information-theoretic framework based on Renyi divergence, thereby uniting disparate existing measures under a single framework. Moreover, its operational significance and computational flexibility allow for natural application in machine learning. In the context of these measures, this project studies privacy-utility tradeoffs both theoretically and in a data-driven manner in two distinct settings: (i) releasing datasets in a similar form as the original, with privacy and strict utility guarantees for arbitrary statistical analysis, and (ii) releasing privacy-guaranteed data representations for specific learning tasks. Broader dissemination of the work will go beyond conferences to organizing a privacy workshop in the latter half of the project to enable inter-disciplinary interactions and application.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Flavio",
   "pi_last_name": "Calmon",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Flavio Calmon",
   "pi_email_addr": "flavio@seas.harvard.edu",
   "nsf_id": "000733796",
   "pi_start_date": "2019-05-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "33 Oxford Street, MD 347",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382933",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 32534.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 116800.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 120154.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 113512.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-e75b65ff-7fff-bd4d-1fdc-4388bbec7ed9\">\r\n<p dir=\"ltr\"><span>Over the past decade, the widespread collection of personal data has fueled the rapid development of machine learning and artificial intelligence models. Services based on analyzing and learning from individual-level data are now everywhere, from recommender systems trained on millions of users' preferences to personalized healthcare platforms predicting treatment outcomes. The rapid increase in data-driven services has also exposed new privacy risks. Data-driven systems must provide meaningful privacy assurances to protect the very individuals whose data enables their functionality.&nbsp;</span></p>\r\n<br />\r\n<p dir=\"ltr\"><span>This project developed novel methods to quantify privacy risks and to balance privacy and utility in systems that learn from data. The work demonstrated that information theory -- the mathematical framework that underlies modern digital communications -- provides a powerful theoretical foundation for analyzing and ensuring privacy. A key contribution was the development of \"maximal alpha-leakage,\" a new information-theoretic metric that quantifies how much private information can be inferred from disclosed data. Maximal alpha-leakage quantifies and generalizes multiple privacy risk metrics under a single theoretical umbrella. The project delineated fundamental limits on what can be achieved when trying to balance utility and privacy under the maximal alpha-leakage metric.</span></p>\r\n<br />\r\n<p dir=\"ltr\"><span>The research also created new methods for ensuring differential privacy (DP). DP is a widely used privacy metric in both Industry and Government. Intuitively, DP quantifies how much a function computed over data depends on individual data points, providing a rigorous framework for privacy. The project yielded new DP-ensuring privacy mechanisms with (provably) minimal data distortion on disclosed data, including the discovery of two new DP mechanisms called &ldquo;Cactus Mechanism&rdquo; and &ldquo;Schrodinger Mechanisms.&rdquo; The research also produced new methods for tracking differential privacy guarantees when data is accessed multiple times.</span></p>\r\n<br />\r\n<p dir=\"ltr\"><span>The project's findings were disseminated through publications in information theory and machine learning venues and extensively communicated via invited lectures and presentations at universities and conferences. This project also supported several graduate students and post-doctoral fellows at Harvard University and Arizona State University.</span></p>\r\n</span></p><br>\n<p>\n Last Modified: 01/14/2025<br>\nModified by: Flavio&nbsp;Calmon</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\r\n\n\nOver the past decade, the widespread collection of personal data has fueled the rapid development of machine learning and artificial intelligence models. Services based on analyzing and learning from individual-level data are now everywhere, from recommender systems trained on millions of users' preferences to personalized healthcare platforms predicting treatment outcomes. The rapid increase in data-driven services has also exposed new privacy risks. Data-driven systems must provide meaningful privacy assurances to protect the very individuals whose data enables their functionality.\r\n\n\r\n\n\nThis project developed novel methods to quantify privacy risks and to balance privacy and utility in systems that learn from data. The work demonstrated that information theory -- the mathematical framework that underlies modern digital communications -- provides a powerful theoretical foundation for analyzing and ensuring privacy. A key contribution was the development of \"maximal alpha-leakage,\" a new information-theoretic metric that quantifies how much private information can be inferred from disclosed data. Maximal alpha-leakage quantifies and generalizes multiple privacy risk metrics under a single theoretical umbrella. The project delineated fundamental limits on what can be achieved when trying to balance utility and privacy under the maximal alpha-leakage metric.\r\n\n\r\n\n\nThe research also created new methods for ensuring differential privacy (DP). DP is a widely used privacy metric in both Industry and Government. Intuitively, DP quantifies how much a function computed over data depends on individual data points, providing a rigorous framework for privacy. The project yielded new DP-ensuring privacy mechanisms with (provably) minimal data distortion on disclosed data, including the discovery of two new DP mechanisms called Cactus Mechanism and Schrodinger Mechanisms. The research also produced new methods for tracking differential privacy guarantees when data is accessed multiple times.\r\n\n\r\n\n\nThe project's findings were disseminated through publications in information theory and machine learning venues and extensively communicated via invited lectures and presentations at universities and conferences. This project also supported several graduate students and post-doctoral fellows at Harvard University and Arizona State University.\r\n\t\t\t\t\tLast Modified: 01/14/2025\n\n\t\t\t\t\tSubmitted by: FlavioCalmon\n"
 }
}