{
 "awd_id": "1926929",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER AI-DCL Collaborative Research: Understanding and Overcoming Biases in STEM Education Using Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 23998.0,
 "awd_amount": 23998.0,
 "awd_min_amd_letter_date": "2019-08-19",
 "awd_max_amd_letter_date": "2019-08-19",
 "awd_abstract_narration": "Diversity is the cornerstone of innovation and essential for the progress of science. However, the number of female students in engineering, computing, and physical sciences in the United States remains strikingly low. The lack of diversity in science, technology, engineering, and mathematics (STEM) education is, to a significant extent, due to biases at different stages of schooling (e.g., different perceptions of math achievements by male and female students, lack of encouragement for female student enrollment in advance placement classes, stereotypes influencing college course selection). These biases appear as early as middle school: a critical period when student's educational experience can significantly influence their academic choices in high school and, ultimately, in deciding whether or not to enroll in STEM majors in college. In order to broaden the participation of women in STEM, it is critical to identify factors and practices in middle school learning environments that may attract (or repel) students into science. This award will use machine learning (ML) to develop new, automated, and data-driven methods for discovering and monitoring biases in STEM classrooms, focusing on middle school and early adolescence science and mathematics education.\r\n\r\nThe project combines methods from social psychology, machine learning, and information theory to create algorithmic tools that monitor middle school student, teacher, and school-level data for factors that impact students' engagement in STEM. These tools will (i) help identify pedagogical or socio-economic factors that have a disparate impact on the decisions made by female students, (ii) predict which students are most vulnerable to being discouraged from pursuing STEM fields, and (iii) inform effective interventions that help close the gender gap. Despite its potential, the use of ML in education is a double-edged sword: while ML algorithms may be able to flag discriminatory patterns, they can also propagate biases and have an unwarranted disparate impact if left unchecked.  Thus, in parallel, this project also aims to characterize the fairness challenges involved in deploying ML in education settings. The proposed approach will be validated on a dataset collected during a five year period from middle school students from across the United States.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nilanjana",
   "pi_last_name": "Dasgupta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nilanjana Dasgupta",
   "pi_email_addr": "dasgupta@psych.umass.edu",
   "nsf_id": "000100209",
   "pi_start_date": "2019-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010359450",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 23998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The major goal of this project to characterize the discrimination and fairness challenges involved in deploying machine learning (ML) and artificial intelligence (AI) in education settings. ML algorithms are at risk of nheriting and, ultimately, propagating social biases that exist in training data. This risk is particularly acute in algorithms trained on data surrounding STEM education where biases are widespread. Despite algorithmic fairness being a burgeoning research area within the computer science community, the specific challenges involved in applying ML to education data are far from understood. The project developed both new theoretical schemes that are parricularly tailored to STEM education, and experimental recommendations based on those schemes and on the empirical data.</p>\n<p>&nbsp;</p>\n<p>In this work, we demonstrate concrete examples of how racial inequities emerge when ML algorithms are used to predict students' future math performance in secondary education. Our analysis is based on training popular ML models on largescale datasets collected from middle schools and high schools across the United States. We find that the standard pipeline for training and deploying ML models--- collecting a representative dataset, then fitting a ML model to maximize predictive accuracy---can systematically fail Black, Hispanic, and Native American (BHN) students compared to White and Asian (WA) students when applied to predict future math performance.</p>\n<p>&nbsp;</p>\n<p>Accuracy, the metric most often used to choose a ML model, can be deceptive when it comes to assessing whether a ML model is fair in predicting student performance. Accuracy measures the rate of misclassification, but not all errors committed by a ML model are equal. False-positives give the benefit of the doubt to students and provide more opportunities, whereas false-negatives undercut students' future potential . Our experiments show that standard ML algorithms achieve comparably high accuracy for both WA and BHN students when predicting future math performance, but the patterns of misclassifcation can be strikingly different between these two groups: WA students receive substantially more benefit of the doubt while BHN students receive more pessimistic predictions. The harm is silent: the usual procedure of optimizing ML models for accuracy may mask predictions that deprive BHN students of educational opportunities. Our findings suggest that, when predicting student future performance, false-positive rates and false-negative rates across student populations must be closely monitored. When predicting student performance, unequal error rates have real-world consequences. Consider a scenario where we use a random forest model trained on the dataset provided by co-PI Dasgupta's group for 9th grade math placement. Students who are predicted to be in top 50% will be placed in the advanced-level math class and students who receive bottom 50% prediction will be placed in the basic math class. The false postiive rate (FPR) of 0.30 for WA students means that 30% of the students who would not perform well in the 9th grade will be placed in the advanced class. They are given the benefit of the doubt and the opportunity to learn more advanced math. On the other hand,&nbsp;only 18% of the BHN students get the same benefit of the doubt (FPR=0.18). The false negative rate (FNR) of 0.21 in WA students indicates that 21% of WA students who would in fact perform well in the future will be placed in the basic class by the ML algorithm. For BHN students, a startlingly high 37% will be incorrectly placed in the basic class, their academic potential ignored by the algorithm.</p>\n<p>&nbsp;</p>\n<p>If we consider achieving equal accuracy as a fairness goal, the results of our work suggest that balancing the dataset to have an equal number of BHN and WA students would be the right solution. However, our results echo that accuracy should not be the sole metric of focus as FPR and FNR gaps show severe discriminatory effects. If we choose a fairness criterion to be equal FPR and FNR, Our work indicates that simply changing the racial mixture of the training set cannot achieve this. If we relax the condition and choose a training set that has the smallest gap in FPR and FNR, meaning the training set is 100% WA. This can benefit BHN students as it not only has the smallest FPR/FNR gaps, but also has the smallest FNR for all groups. Reducing false negative predictions can be especially beneficial for minority students, who are at greater risk of losing interest in STEM subjects by receiving negative predictions. Contrary to the most intuitive choices of fair training set---one that contains an equal number of WA and BHN students or one that follows the national demographics---we show that using homogeneous samples of WA students to train a model can at times produce the fairest model in terms of the FPR ad FNR gap.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/13/2023<br>\n\t\t\t\t\tModified by: Nilanjana&nbsp;Dasgupta</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe major goal of this project to characterize the discrimination and fairness challenges involved in deploying machine learning (ML) and artificial intelligence (AI) in education settings. ML algorithms are at risk of nheriting and, ultimately, propagating social biases that exist in training data. This risk is particularly acute in algorithms trained on data surrounding STEM education where biases are widespread. Despite algorithmic fairness being a burgeoning research area within the computer science community, the specific challenges involved in applying ML to education data are far from understood. The project developed both new theoretical schemes that are parricularly tailored to STEM education, and experimental recommendations based on those schemes and on the empirical data.\n\n \n\nIn this work, we demonstrate concrete examples of how racial inequities emerge when ML algorithms are used to predict students' future math performance in secondary education. Our analysis is based on training popular ML models on largescale datasets collected from middle schools and high schools across the United States. We find that the standard pipeline for training and deploying ML models--- collecting a representative dataset, then fitting a ML model to maximize predictive accuracy---can systematically fail Black, Hispanic, and Native American (BHN) students compared to White and Asian (WA) students when applied to predict future math performance.\n\n \n\nAccuracy, the metric most often used to choose a ML model, can be deceptive when it comes to assessing whether a ML model is fair in predicting student performance. Accuracy measures the rate of misclassification, but not all errors committed by a ML model are equal. False-positives give the benefit of the doubt to students and provide more opportunities, whereas false-negatives undercut students' future potential . Our experiments show that standard ML algorithms achieve comparably high accuracy for both WA and BHN students when predicting future math performance, but the patterns of misclassifcation can be strikingly different between these two groups: WA students receive substantially more benefit of the doubt while BHN students receive more pessimistic predictions. The harm is silent: the usual procedure of optimizing ML models for accuracy may mask predictions that deprive BHN students of educational opportunities. Our findings suggest that, when predicting student future performance, false-positive rates and false-negative rates across student populations must be closely monitored. When predicting student performance, unequal error rates have real-world consequences. Consider a scenario where we use a random forest model trained on the dataset provided by co-PI Dasgupta's group for 9th grade math placement. Students who are predicted to be in top 50% will be placed in the advanced-level math class and students who receive bottom 50% prediction will be placed in the basic math class. The false postiive rate (FPR) of 0.30 for WA students means that 30% of the students who would not perform well in the 9th grade will be placed in the advanced class. They are given the benefit of the doubt and the opportunity to learn more advanced math. On the other hand, only 18% of the BHN students get the same benefit of the doubt (FPR=0.18). The false negative rate (FNR) of 0.21 in WA students indicates that 21% of WA students who would in fact perform well in the future will be placed in the basic class by the ML algorithm. For BHN students, a startlingly high 37% will be incorrectly placed in the basic class, their academic potential ignored by the algorithm.\n\n \n\nIf we consider achieving equal accuracy as a fairness goal, the results of our work suggest that balancing the dataset to have an equal number of BHN and WA students would be the right solution. However, our results echo that accuracy should not be the sole metric of focus as FPR and FNR gaps show severe discriminatory effects. If we choose a fairness criterion to be equal FPR and FNR, Our work indicates that simply changing the racial mixture of the training set cannot achieve this. If we relax the condition and choose a training set that has the smallest gap in FPR and FNR, meaning the training set is 100% WA. This can benefit BHN students as it not only has the smallest FPR/FNR gaps, but also has the smallest FNR for all groups. Reducing false negative predictions can be especially beneficial for minority students, who are at greater risk of losing interest in STEM subjects by receiving negative predictions. Contrary to the most intuitive choices of fair training set---one that contains an equal number of WA and BHN students or one that follows the national demographics---we show that using homogeneous samples of WA students to train a model can at times produce the fairest model in terms of the FPR ad FNR gap.\n\n\t\t\t\t\tLast Modified: 03/13/2023\n\n\t\t\t\t\tSubmitted by: Nilanjana Dasgupta"
 }
}