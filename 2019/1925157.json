{
 "awd_id": "1925157",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Learning Visual Dynamics from Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924702",
 "po_email": "cye@nsf.gov",
 "po_sign_block_name": "Cang Ye",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 750000.0,
 "awd_min_amd_letter_date": "2019-09-09",
 "awd_max_amd_letter_date": "2019-09-09",
 "awd_abstract_narration": "This project studies robots that utilize the nearby and available physical objects to perform tasks, such as building a bridge out of miscellaneous rubble in a disaster area. Resourceful robots have the potential to enable many new applications in emergency response, healthcare, and manufacturing, which will improve the welfare, security, and efficiency of the overall population. The research investigates how the patterns between multiple senses, such as vision, sound, and touch, will help teach the robot to solve interaction tasks without needing a human teacher, which is expected to improve the flexibility and versatility of autonomous robots. The project will provide research and educational opportunities for both graduate and undergraduate students in computer science and mechanical engineering. Outcomes from this research will translate into new educational materials in computer vision, machine learning, and robotics. \r\n\r\nThis research investigates robots that interact with realistic environments in order to learn reusable representations for navigation and manipulation tasks. While there has been significant advancements leveraging machine learning for computer vision and robotics problems, a central challenge in both fields is generalizing  to the realistic complexity and diversity of the physical world. Although simulation has proved instrumental in developing platforms for machine interaction, the unconstrained world is vast, making it computationally difficult to simulate. Instead, the investigators aim to capitalize on the inherent structure of physical environments through the natural synchronization of modalities and context to efficiently learn self-supervised representations and policies for interaction with unconstrained environments. The investigators also plan several evaluations to analyze the generalization capabilities of such algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Carl",
   "pi_last_name": "Vondrick",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Carl M Vondrick",
   "pi_email_addr": "cv2428@columbia.edu",
   "nsf_id": "000755733",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Hod",
   "pi_last_name": "Lipson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hod Lipson",
   "pi_email_addr": "hod.lipson@columbia.edu",
   "nsf_id": "000097015",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "530 West 120th Street",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277922",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 750000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project demonstrated how robots can learn about their environment through direct physical interaction rather than relying on pre-programmed simulations. The project developed new approaches that allow robots to gain understanding of physical properties and dynamics through hands-on experimentation, similar to how humans learn by interacting with the world around them. A major achievement of this research was the development of PaperBot, an innovative system that learns to design and optimize tools made from paper through real-world trial and error. Unlike traditional approaches that depend heavily on computer simulation, PaperBot learns directly from physical experiments using vision systems and force sensors to evaluate its designs. This represents an important advance in robotics, showing that robots can effectively learn complex physical properties through direct experience rather than theoretical models. The research produced results in two challenging test cases. First, PaperBot learned to design and fold paper airplanes that could fly further than human-designed versions after just 100 trials, mastering complex aerodynamic principles through experimentation. Second, the system created paper-based grippers capable of carefully handling delicate objects like fruit, demonstrating practical applications for fields like food processing and medical device handling.</p>\r\n<p>The broader impacts of this research extend well beyond robotics. The project advances sustainable manufacturing by showing how recyclable materials like paper can be transformed into functional tools. The development of low-cost, customizable paper-based tools could improve healthcare accessibility, particularly in resource-limited settings. The research also promotes more accessible technological development by using readily available materials and sharing findings openly. These advances lay the groundwork for more adaptable and resourceful robotic systems that can learn from their environment and create custom solutions for specific tasks. This capability could transform various fields, from manufacturing and healthcare to environmental sustainability, while promoting more accessible and sustainable technological development.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/03/2025<br>\nModified by: Carl&nbsp;M&nbsp;Vondrick</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project demonstrated how robots can learn about their environment through direct physical interaction rather than relying on pre-programmed simulations. The project developed new approaches that allow robots to gain understanding of physical properties and dynamics through hands-on experimentation, similar to how humans learn by interacting with the world around them. A major achievement of this research was the development of PaperBot, an innovative system that learns to design and optimize tools made from paper through real-world trial and error. Unlike traditional approaches that depend heavily on computer simulation, PaperBot learns directly from physical experiments using vision systems and force sensors to evaluate its designs. This represents an important advance in robotics, showing that robots can effectively learn complex physical properties through direct experience rather than theoretical models. The research produced results in two challenging test cases. First, PaperBot learned to design and fold paper airplanes that could fly further than human-designed versions after just 100 trials, mastering complex aerodynamic principles through experimentation. Second, the system created paper-based grippers capable of carefully handling delicate objects like fruit, demonstrating practical applications for fields like food processing and medical device handling.\r\n\n\nThe broader impacts of this research extend well beyond robotics. The project advances sustainable manufacturing by showing how recyclable materials like paper can be transformed into functional tools. The development of low-cost, customizable paper-based tools could improve healthcare accessibility, particularly in resource-limited settings. The research also promotes more accessible technological development by using readily available materials and sharing findings openly. These advances lay the groundwork for more adaptable and resourceful robotic systems that can learn from their environment and create custom solutions for specific tasks. This capability could transform various fields, from manufacturing and healthcare to environmental sustainability, while promoting more accessible and sustainable technological development.\r\n\n\n\t\t\t\t\tLast Modified: 02/03/2025\n\n\t\t\t\t\tSubmitted by: CarlMVondrick\n"
 }
}