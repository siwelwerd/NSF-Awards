{
 "awd_id": "1855096",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "PREEVENTS Track 2: Collaborative Research:  A Dynamic Unified Framework for Hurricane Storm Surge Analysis and Prediction Spanning across the Coastal Floodplain and Ocean",
 "cfda_num": "47.050",
 "org_code": "06010000",
 "po_phone": "7032927575",
 "po_email": "skennan@nsf.gov",
 "po_sign_block_name": "Sean Kennan",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2019-06-17",
 "awd_max_amd_letter_date": "2019-07-15",
 "awd_abstract_narration": "Storm-driven coastal flooding is influenced by many physical processes including riverine discharges, regional rainfall, wind, atmospheric pressure, wave-induced set up, wave runup, tides, and fluctuating baseline ocean water levels. Operational storm surge models such as those used by NOAA's Ocean Prediction Center (Extratropical Surge and Tide Operational Forecast System) incorporate a variety of these processes including riverine discharges, atmospheric winds and pressure, waves, and tides. However, coastal surge models do not typically incorporate the impact of rainfall across the coastal floodplain nor fluctuations in background water levels due to the oceanic density structure. Nonetheless, the floodplain hydrology and ocean baseline water levels provide vital controls in riverine and estuarine environments (e.g., the dramatic effect seen in the Houston metropolitan region during Hurricane Harvey in 2017 and in North Carolina during Hurricane Florence in 2018). \r\n\r\nRecent events have shown that a unified approach that incorporates all the relevant physical processes is critical for accurate predictive simulations of coastal flooding due to extreme events. This project will tackle this challenge by melding hydrology, hydraulics, and waves into a dynamic unified computational framework that uses unstructured meshes spanning from the deep ocean to upland areas and across the coastal floodplain. Improved capacity for flood risk managers, the insurance industry, and city planners to evaluate flood risk across the entire coastal floodplain. Improved models will lead to better guidance on development and construction practices, will help make cities more resilient and will reduce risk for coastal populations and infrastructure. In addition, this work will improve coastal flood forecasting enabling federal, state, and local disaster managers, to optimize issuing warnings for evacuation and emergency planning. The collaboration between the ocean circulation, coastal hydrodynamics, and hydrology modeling communities fostered by this project will help support ambitious projects such as NOAA's National Water Center's National Integrated Water Model, which is at the preliminary stages of integration of hydrology and coastal hydrodynamics. Training of students at the intersection of hydrology, coastal hydrodynamics, physical oceanography, and computational mathematics, to help develop and apply ever-more complex and advanced models in academia, government and industry.\r\n\r\nThe proposed unified framework will improve the predicted water level gradient and flows throughout the coastal floodplain by integrally considering the rainfall-driven hydrology within the coastal floodplain as well as improving the background open ocean water level. Well-developed but coarse global ocean models will be heterogeneously coupled to high-resolution 2D shallow water equation models in order to account for large-scale baroclinic ocean processes that impact coastal water levels. Interface strategies and conditions between heterogeneous physics will be developed that allow the interfaces to move in time and space for the range of physics from dry to surface runoff to pressurized flow. Applying the right physics and associated mathematical models as the storms evolve will result in more robust and accurate models, as well as much more efficient models. This will dynamically account for the hydrologic - hydrodynamic interaction of water across the floodplain. Dynamic load balancing will account for widely varying computational (CPU) costs for each set of physics and the dynamic migration of the physics will be implemented within the heterogeneous parallel computing environment.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "GEO",
 "org_dir_long_name": "Directorate for Geosciences",
 "div_abbr": "RISE",
 "org_div_long_name": "Integrative and Collaborative Education and Research (ICER)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Laxmikant",
   "pi_last_name": "Kale",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Laxmikant V Kale",
   "pi_email_addr": "kale@uiuc.edu",
   "nsf_id": "000123469",
   "pi_start_date": "2019-06-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S. Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "034Y00",
   "pgm_ele_name": "PREEVENTS - Prediction of and"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-3cac6fd4-7fff-251f-9749-528eb2202082\"> </span></p>\r\n<p dir=\"ltr\"><span>Storm-driven coastal flooding is influenced by many physical processes including riverine discharges, regional rainfall, wind, atmospheric pressure, wave-induced set up, wave runup, and tides.&nbsp; </span><span>Modeling multiple physical phenomena with numerical models and simulating them on powerful parallel computers is essential for accurate prediction of water levels in coastal plains and cities. This collaborative project dealt with and created such a simulation capability and incorporated it in a production code called ADCIRC. Work that was carried out mainly at University of Illinois involved developing techniques to support efficient parallel execution of these codes on parallel computers including supercomputers and to help incorporate these techniques into production codes and simulations. Parallel efficiency is especially challenging for these ADCIRC simulations because dynamic behaviors of the computational model create obstacles to efficiency. For example, numerical models for&nbsp; water bodies partition the area into triangles that tessellate or fill the space occupied by the water bodies. Since during floods, the area occupied by water increases, it is necessary to include dry triangles, which may become wet as computation progresses. Computational processes and actions are different in dry and wet domains, creating&nbsp; significant load imbalances:&nbsp; some processors are overloaded while others are underloaded. Each simulation step, which mimics the behavior of the physical system for one time step, needs to wait for the most overloaded processor before completing. This leads to many processors being idle and thus wasting valuable computational capability as well as delaying simulation prediction time, which can be critical to enable timely actions.&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>The main vehicle used for this project was &ldquo;Adaptive MPI&rdquo;, a parallel programming system being developed at University of Illinois, aimed at mitigating load imbalance issues, while reducing programming complexity.. Message Passing Interface (MPI), is a widely used standard parallel programming model. Although most applications in various domains of computational science and engineering are developed with MPI, it does not address capabilities such as dynamic load balancing, resource elasticity (i.e. changing the number of computers used by the program while the code is executing) or automatic checkpointing. Adaptive MPI is an alternative implementation of MPI, which virtualizes the notion of a processor. Instead of using P processes (called ranks in MPI) for P processors, it uses many more virtual processes (Figure 1). This allows it to respond to load variations by adaptively and intelligently migrating these logical processes across computers to restore load balance without significantly increasing communication costs.&nbsp;&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>Other models, such as Charm++, support these capabilities as well, but require significant redesign of the application to make it fit the abstraction supported by Charm++ rather than MPI. With Adaptive MPI, since the programming model remains unchanged, user code changes are small. However, the small&nbsp; changes that are needed are challenging, especially for long running codes such as ADCIRC, and written in FORTRAN, which is an older but still widely used language not directly supported by Charm++.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>The project focused on improving the capabilities of Adaptive MPI and to develop abstractions and techniques that not only help convert ADCIRC to use Adaptive MPI, but make it easy for many other CSE applications developed by NSF researchers as well.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>One of the capabilities was new dynamic load balancing techniques. The computational domains (partitions of space) in many applications need to communicate with nearby domains. Balancing loads has to be done carefully, because it may lead to a large number of domains communicating to non-local domains, as well as migration of a large number of domains. This project developed a &ldquo;graph-refinement&rdquo; load balancing scheme that optimizes all these metrics in addition to balancing the computational load. As shown in Figure 2, domains are allowed to migrate to only neighboring processors while a global fixpoint algorithm ensures the shifting produces global balance.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>An example of the abstractions needed to convert the code is presented by use of file input/output. In fortran codes, this is mediated through the use of logical unit numbers (LUNs). Yet, with overdecomposition, multiple virtual ranks within the same OS process need to output to their corresponding files via corresponding LUNs. This requires a complete revamp the values all active LUNs and how the fortran library for disk (file) output uses them. We developed automatic transformations and libraries to facilitate this process.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>An example of the flexibility and the efficiency afforded by the model is shown in Figure 3.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>Using such overdecompositon can have overhead, but we demonstrated, as shown in Figure 4, that&nbsp; (a) the overhead in Adaptive MPI is small even without overdecomposition, and (b) it is more than made up by adaptive load balancing, yielding speedups close to 2 on the same number of cores.</span></p>\r\n<p dir=\"ltr\"><span>The overall improvements in Adaptive MPI framework achieved in this project has the potential of helping improve the efficiency of many science and engineering codes.&nbsp;</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/11/2025<br>\nModified by: Laxmikant&nbsp;V&nbsp;Kale</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739273609672_Screenshot_2025_02_11_at_4.45.26__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739273609672_Screenshot_2025_02_11_at_4.45.26__8239_PM--rgov-800width.png\" title=\"Graph refinement load balancing\"><img src=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739273609672_Screenshot_2025_02_11_at_4.45.26__8239_PM--rgov-66x44.png\" alt=\"Graph refinement load balancing\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 2: The load balancer in action: load is shifted from overloaded processor (lightest blue) to underloaded processor (darkest purple) via only shifting objects through neighboring processors, preserving communication locality. Objects on other processors (gray) are not shown for clarity.</div>\n<div class=\"imageCredit\">Maya Taylor</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Laxmikant&nbsp;V&nbsp;Kale\n<div class=\"imageTitle\">Graph refinement load balancing</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739273129368_Screenshot_2025_02_11_at_4.45.02__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739273129368_Screenshot_2025_02_11_at_4.45.02__8239_PM--rgov-800width.png\" title=\"Adaptive MPI\"><img src=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739273129368_Screenshot_2025_02_11_at_4.45.02__8239_PM--rgov-66x44.png\" alt=\"Adaptive MPI\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 1: Adaptive MPI virtualizes the notion of processor or process, by implementing an MPI rank as a migratable user-level thread.</div>\n<div class=\"imageCredit\">Laxmikant Kale</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Laxmikant&nbsp;V&nbsp;Kale\n<div class=\"imageTitle\">Adaptive MPI</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739273785167_Screenshot_2025_02_11_at_4.45.40__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739273785167_Screenshot_2025_02_11_at_4.45.40__8239_PM--rgov-800width.png\" title=\"Overdecomposition in an ADCIRC Simulation\"><img src=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739273785167_Screenshot_2025_02_11_at_4.45.40__8239_PM--rgov-66x44.png\" alt=\"Overdecomposition in an ADCIRC Simulation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 3: Overdecomposed domains are assigned to processors using Adaptive MPI. The left half domains are well above sea level and are initially dry. This scheme involves pairing domains from left and right and assigning pairs to processors to ensure load balance, as facilitated by Adaptive MPI.</div>\n<div class=\"imageCredit\">Dylan Wood</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Laxmikant&nbsp;V&nbsp;Kale\n<div class=\"imageTitle\">Overdecomposition in an ADCIRC Simulation</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739274640813_Screenshot_2025_02_11_at_4.45.56__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739274640813_Screenshot_2025_02_11_at_4.45.56__8239_PM--rgov-800width.png\" title=\"Execution Time Improvement with Adaptive MPI\"><img src=\"/por/images/Reports/POR/2025/1855096/1855096_10611622_1739274640813_Screenshot_2025_02_11_at_4.45.56__8239_PM--rgov-66x44.png\" alt=\"Execution Time Improvement with Adaptive MPI\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 4: Execution time per step with Plain MPI, Adaptive MPI without overdecomposition, and Adaptive MPI with overdecomposition and load balancing. The same code with Adaptive MPI and  overdecomposition take a little more than half the time, reducing duration as well as the cost of computation.</div>\n<div class=\"imageCredit\">Dylan Wood</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Laxmikant&nbsp;V&nbsp;Kale\n<div class=\"imageTitle\">Execution Time Improvement with Adaptive MPI</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nStorm-driven coastal flooding is influenced by many physical processes including riverine discharges, regional rainfall, wind, atmospheric pressure, wave-induced set up, wave runup, and tides. Modeling multiple physical phenomena with numerical models and simulating them on powerful parallel computers is essential for accurate prediction of water levels in coastal plains and cities. This collaborative project dealt with and created such a simulation capability and incorporated it in a production code called ADCIRC. Work that was carried out mainly at University of Illinois involved developing techniques to support efficient parallel execution of these codes on parallel computers including supercomputers and to help incorporate these techniques into production codes and simulations. Parallel efficiency is especially challenging for these ADCIRC simulations because dynamic behaviors of the computational model create obstacles to efficiency. For example, numerical models for water bodies partition the area into triangles that tessellate or fill the space occupied by the water bodies. Since during floods, the area occupied by water increases, it is necessary to include dry triangles, which may become wet as computation progresses. Computational processes and actions are different in dry and wet domains, creating significant load imbalances: some processors are overloaded while others are underloaded. Each simulation step, which mimics the behavior of the physical system for one time step, needs to wait for the most overloaded processor before completing. This leads to many processors being idle and thus wasting valuable computational capability as well as delaying simulation prediction time, which can be critical to enable timely actions.\r\n\n\n\r\n\n\nThe main vehicle used for this project was Adaptive MPI, a parallel programming system being developed at University of Illinois, aimed at mitigating load imbalance issues, while reducing programming complexity.. Message Passing Interface (MPI), is a widely used standard parallel programming model. Although most applications in various domains of computational science and engineering are developed with MPI, it does not address capabilities such as dynamic load balancing, resource elasticity (i.e. changing the number of computers used by the program while the code is executing) or automatic checkpointing. Adaptive MPI is an alternative implementation of MPI, which virtualizes the notion of a processor. Instead of using P processes (called ranks in MPI) for P processors, it uses many more virtual processes (Figure 1). This allows it to respond to load variations by adaptively and intelligently migrating these logical processes across computers to restore load balance without significantly increasing communication costs.\r\n\n\n\r\n\n\nOther models, such as Charm++, support these capabilities as well, but require significant redesign of the application to make it fit the abstraction supported by Charm++ rather than MPI. With Adaptive MPI, since the programming model remains unchanged, user code changes are small. However, the small changes that are needed are challenging, especially for long running codes such as ADCIRC, and written in FORTRAN, which is an older but still widely used language not directly supported by Charm++.\r\n\n\nThe project focused on improving the capabilities of Adaptive MPI and to develop abstractions and techniques that not only help convert ADCIRC to use Adaptive MPI, but make it easy for many other CSE applications developed by NSF researchers as well.\r\n\n\nOne of the capabilities was new dynamic load balancing techniques. The computational domains (partitions of space) in many applications need to communicate with nearby domains. Balancing loads has to be done carefully, because it may lead to a large number of domains communicating to non-local domains, as well as migration of a large number of domains. This project developed a graph-refinement load balancing scheme that optimizes all these metrics in addition to balancing the computational load. As shown in Figure 2, domains are allowed to migrate to only neighboring processors while a global fixpoint algorithm ensures the shifting produces global balance.\r\n\n\nAn example of the abstractions needed to convert the code is presented by use of file input/output. In fortran codes, this is mediated through the use of logical unit numbers (LUNs). Yet, with overdecomposition, multiple virtual ranks within the same OS process need to output to their corresponding files via corresponding LUNs. This requires a complete revamp the values all active LUNs and how the fortran library for disk (file) output uses them. We developed automatic transformations and libraries to facilitate this process.\r\n\n\nAn example of the flexibility and the efficiency afforded by the model is shown in Figure 3.\r\n\n\nUsing such overdecompositon can have overhead, but we demonstrated, as shown in Figure 4, that (a) the overhead in Adaptive MPI is small even without overdecomposition, and (b) it is more than made up by adaptive load balancing, yielding speedups close to 2 on the same number of cores.\r\n\n\nThe overall improvements in Adaptive MPI framework achieved in this project has the potential of helping improve the efficiency of many science and engineering codes.\r\n\n\n\t\t\t\t\tLast Modified: 02/11/2025\n\n\t\t\t\t\tSubmitted by: LaxmikantVKale\n"
 }
}