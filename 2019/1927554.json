{
 "awd_id": "1927554",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AI-DCL: Governing bias in AI system with humans in the decision loop",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2019-09-03",
 "awd_max_amd_letter_date": "2019-09-03",
 "awd_abstract_narration": "Artificial intelligence (AI) systems have advanced dramatically in recent years and they have been applied in many real-world applications that touch our daily lives. Despite their remarkable performance, these technologies inherently carry the risk of aggravating the societal biases present in their training data. This can lead to unfair decisions based on sensitive demographic attributes (e.g., gender), as well as the unintentional generation of insulting outputs (e.g., tagging a person as an animal). This project develops a hybrid AI system that includes humans in the decision process (human-in-the-loop) in order to ensure decisions that are robust, unbiased, and fair. The results will enable intelligent machines to seamlessly integrate with human experts to: 1) identify various types of biases in the model predictions and 2) learn to mimic the behavior of human experts and take implicit societal factors into consideration when making automatic decisions.\r\n\r\nThe technical approach is based on developing a human-machine hybrid intelligence framework allowing human experts to censor and guide an AI agent in order to identify harmful decisions and correct biases. Specifically, the team will build a bias diagnosis module with a censor model, to predict if a decision is fair or not. When the censor model is uncertain, it will request that a human expert make judgments under an active imitation learning framework. The feedback from the bias diagnosis module will be used to improve the AI system and to correct the bias exhibited in its prediction. The approaches will be to various natural language processing and computer vision applications, including entity co-reference resolution (e.g., AI thinks a female pronoun is less likely to refer to a leader) and object detection in images (e.g., AI cannot identify a tie worn by a woman).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kai-Wei",
   "pi_last_name": "Chang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kai-Wei Chang",
   "pi_email_addr": "kw@kwchang.net",
   "nsf_id": "000726790",
   "pi_start_date": "2019-09-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Akihiro",
   "pi_last_name": "Nishi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Akihiro Nishi",
   "pi_email_addr": "akihironishi@ucla.edu",
   "nsf_id": "000799237",
   "pi_start_date": "2019-09-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "UCLA Computer Science Dept.",
  "perf_str_addr": "404 Westwood Plaza, 374 E6",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951596",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Natural language processing (NLP) has been widely adopted in various fields such as information technology, education, and healthcare. However, recent works have revealed the presence of social bias in NLP models. In this project, we aimed to detect and improve the fairness and inclusion of NLP models through the use of algorithmic methods and expert guidance. Overall, we were able to not only meet the objectives of the project but also expand its scope.&nbsp;</p>\n<p>Key outcomes of the project include:</p>\n<p>(1)&nbsp; &nbsp;We conducted a comprehensive survey on the measures of bias and harms in NLP and analyzed how existing bias measures in NLP -- both intrinsic measures of representation bias and extrinsic measures of downstream application bias -- can be aligned with different harms. Additionally, we studied the harms of gender exclusivity and surveyed non-binary individuals to gain insight into the harms associated with the treatment of gender as binary in English language technologies.</p>\n<p>&nbsp;</p>\n<p>(2)&nbsp; &nbsp;We defined &ldquo;local group bias&rdquo; as the bias exhibited in a local input region where the model produces the most biased outputs as determined by a fairness metric &nbsp;(e.g., performance gap across groups). Identifying this region is essential as it can help experts trace the source of biases and gain a deeper understanding of the models' behavior. To this end, we proposed a clustering algorithm that groups instances based on their features while maximizing thw bias metric within each cluster.</p>\n<p>&nbsp;</p>\n<p>(3)&nbsp; &nbsp;We investigated the use of ethical natural language intervention, such as statements promoting equitable judgment (e.g., 'if all individuals can be a lawyer irrespective of their gender'), to alter the model&rsquo;s prediction and reduce bias. In particular, we included the ethical natural language intervention as part of the input to the model and studied its effectiveness in reducing the bias of reading-comprehension models and in increasing the diversity of outputs from text-to-image systems. Our results indicate that while the interventions improve model fairness, further work is needed to achieve the desired level of bias reduction.</p>\n<p>&nbsp;</p>\n<p>(4)&nbsp; &nbsp;We investigated toxic dialogue responses, such as ad hominem attacks, generated by language generation models. These types of responses are harmful as they propagate implicit biases and diminish the credibility of individuals. To address this issue, we further proposed methods for controlling the generation of these responses and moderating the generated outputs to mitigate their negative effects.</p>\n<p>&nbsp;</p>\n<p>Additionally, we made contributions beyond the technical goals of the project. We created a new seminar course on &ldquo;Special Topic in AI: Fairness, Accountability, and Transparency in Natural Language Processing&rdquo; at UCLA and delivered two related tutorials in major NLP venues. Software and datasets have been released for reproducing the experiment results and for use by other researchers.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/26/2023<br>\n\t\t\t\t\tModified by: Kai-Wei&nbsp;Chang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nNatural language processing (NLP) has been widely adopted in various fields such as information technology, education, and healthcare. However, recent works have revealed the presence of social bias in NLP models. In this project, we aimed to detect and improve the fairness and inclusion of NLP models through the use of algorithmic methods and expert guidance. Overall, we were able to not only meet the objectives of the project but also expand its scope. \n\nKey outcomes of the project include:\n\n(1)   We conducted a comprehensive survey on the measures of bias and harms in NLP and analyzed how existing bias measures in NLP -- both intrinsic measures of representation bias and extrinsic measures of downstream application bias -- can be aligned with different harms. Additionally, we studied the harms of gender exclusivity and surveyed non-binary individuals to gain insight into the harms associated with the treatment of gender as binary in English language technologies.\n\n \n\n(2)   We defined \"local group bias\" as the bias exhibited in a local input region where the model produces the most biased outputs as determined by a fairness metric  (e.g., performance gap across groups). Identifying this region is essential as it can help experts trace the source of biases and gain a deeper understanding of the models' behavior. To this end, we proposed a clustering algorithm that groups instances based on their features while maximizing thw bias metric within each cluster.\n\n \n\n(3)   We investigated the use of ethical natural language intervention, such as statements promoting equitable judgment (e.g., 'if all individuals can be a lawyer irrespective of their gender'), to alter the model\u2019s prediction and reduce bias. In particular, we included the ethical natural language intervention as part of the input to the model and studied its effectiveness in reducing the bias of reading-comprehension models and in increasing the diversity of outputs from text-to-image systems. Our results indicate that while the interventions improve model fairness, further work is needed to achieve the desired level of bias reduction.\n\n \n\n(4)   We investigated toxic dialogue responses, such as ad hominem attacks, generated by language generation models. These types of responses are harmful as they propagate implicit biases and diminish the credibility of individuals. To address this issue, we further proposed methods for controlling the generation of these responses and moderating the generated outputs to mitigate their negative effects.\n\n \n\nAdditionally, we made contributions beyond the technical goals of the project. We created a new seminar course on \"Special Topic in AI: Fairness, Accountability, and Transparency in Natural Language Processing\" at UCLA and delivered two related tutorials in major NLP venues. Software and datasets have been released for reproducing the experiment results and for use by other researchers.\n\n\t\t\t\t\tLast Modified: 01/26/2023\n\n\t\t\t\t\tSubmitted by: Kai-Wei Chang"
 }
}