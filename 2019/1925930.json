{
 "awd_id": "1925930",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:  TRIPODS Institute for Optimization and Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2019-01-01",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 253228.0,
 "awd_amount": 253228.0,
 "awd_min_amd_letter_date": "2019-04-01",
 "awd_max_amd_letter_date": "2019-09-04",
 "awd_abstract_narration": "This Phase I project forms an NSF TRIPODS Institute, based at Lehigh University and in collaboration with Stony Brook and Northwestern Universities, with a focus on new advances in tools for machine learning applications.  A critical component for machine learning is mathematical optimization, where one uses historical data to train tools for making future predictions and decisions.  Traditionally, optimization techniques for machine learning have focused on simplified models and algorithms.  However, recent revolutionary leaps in the successes of machine learning tools---e.g., for image and speech recognition---have in many cases been made possible by a shift toward using more complicated techniques, often involving deep neural networks.  Continued advances in the use of such techniques require combined efforts between statisticians, computer scientists, and applied mathematicians to develop more sophisticated models and algorithms along with more comprehensive theoretical guarantees that support their use. In addition to its research goals, the institute trains Ph.D. students and postdoctoral fellows in statistics, computer science, and applied mathematics,  and hosts interdisciplinary workshops and Winter/Summer schools.  \r\n\r\nThe research efforts in Phase I are on the analysis of nonconvex machine learning models, the design of optimization algorithms for training them, and on the development of nonparametric models and associated algorithms.  The focus is on deep neural networks (DNNs), mostly in general, but also with respect to specific architectures of interest.  The institute's research efforts emphasize the need to develop connections between state-of-the-art approaches for training DNNs and statistical performance guarantees (e.g., on generalization errors), which are currently not well understood. Optimization algorithms development centers on second-order-derivative-type techniques, including (Hessian-free) Newton, quasi-Newton, Gauss-Newton, and their limited memory variants.  Recent advances have been made in the design of such methods; the PIs' work builds upon these efforts with their broad expertise in the design and implementation (including in parallel and distributed computing environments) of such methods.  The development of nonparametric models promises to free machine learning approaches from restrictions imposed by large numbers of user-defined parameters (e.g., defining a network structure or learning rate of an optimization algorithm).  Such models could lead to great advances in machine learning, and the institute's work in this area also draws on the PIs expertise in derivative-free optimization methods, which are needed for training in nonparametric settings.\r\n\r\nIn this TRIPODS institute, the PIs approach all of these research directions with a unified perspective in the three disciplines of statistics, computer science, and applied mathematics.  Indeed, as machine learning draws so heavily from these areas, future progress requires close collaborations between optimization experts, learning theorists, and statisticians---communities of researchers that, as yet, have tended to operate separately with differing terminology and publication venues.  With an emphasis on deep learning, this institute aims to foster intercollegiate and interdisciplinary collaborations that overcome these hindrances.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Francesco",
   "pi_last_name": "Orabona",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Francesco Orabona",
   "pi_email_addr": "fo@bu.edu",
   "nsf_id": "000747281",
   "pi_start_date": "2019-04-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Boston University",
  "perf_str_addr": "",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151703",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "041Y00",
   "pgm_ele_name": "TRIPODS Transdisciplinary Rese"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "047Z",
   "pgm_ref_txt": "TRIPODS Phase 1"
  },
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 58386.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 96231.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 98610.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>At the core of all training processes of modern machine learning algorithms there is always an optimization procedure. A better, faster, more reliable optimization procedure will give a better, faster, more reliable learning algorithm. However, often the optimization procedures are designed for general tasks, that might be suboptimal for the specific tasks in machine learning. Other times, practitioners use heuristic optimization procedures with unclear convergence properties. Hence, in this project we focused specifically on designing and analyzing optimization algorithms specialized for modern machine learning procedures, in particular deep learning algorithms.</p>\n<p>We have designed and analyzed new optimization algorithms that automatically adapts to some of the characteristics of the learning problem, reducing the need for a human expert to babysit the learning process. Moreover, we have also analyzed popular optimization heuristics, showing in which situations they are safe to use and characterizing their convergence speed.</p>\n<p>During the course of this project, the  investigators and their collaborators have published many articles  on new optimization algorithms for machine learning in top  journal and conference proceedings venues, in both the stochastic and online framework.</p>\n<p>The principal investigator has trained two  postdoctoral researchers in their lab over the course of the project  duration, both of whom have started academic positions at top  research universities. He has also trained two graduate students who will soon join the workforce as experts at the intersection between optimization and machine learning.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/30/2022<br>\n\t\t\t\t\tModified by: Francesco&nbsp;Orabona</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAt the core of all training processes of modern machine learning algorithms there is always an optimization procedure. A better, faster, more reliable optimization procedure will give a better, faster, more reliable learning algorithm. However, often the optimization procedures are designed for general tasks, that might be suboptimal for the specific tasks in machine learning. Other times, practitioners use heuristic optimization procedures with unclear convergence properties. Hence, in this project we focused specifically on designing and analyzing optimization algorithms specialized for modern machine learning procedures, in particular deep learning algorithms.\n\nWe have designed and analyzed new optimization algorithms that automatically adapts to some of the characteristics of the learning problem, reducing the need for a human expert to babysit the learning process. Moreover, we have also analyzed popular optimization heuristics, showing in which situations they are safe to use and characterizing their convergence speed.\n\nDuring the course of this project, the  investigators and their collaborators have published many articles  on new optimization algorithms for machine learning in top  journal and conference proceedings venues, in both the stochastic and online framework.\n\nThe principal investigator has trained two  postdoctoral researchers in their lab over the course of the project  duration, both of whom have started academic positions at top  research universities. He has also trained two graduate students who will soon join the workforce as experts at the intersection between optimization and machine learning.\n\n\t\t\t\t\tLast Modified: 05/30/2022\n\n\t\t\t\t\tSubmitted by: Francesco Orabona"
 }
}