{
 "awd_id": "1908291",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Collaborative Research: Generative Adversarial Networks: From Art to Science",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032928910",
 "po_email": "jafowler@nsf.gov",
 "po_sign_block_name": "James Fowler",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2019-06-10",
 "awd_max_amd_letter_date": "2019-06-10",
 "awd_abstract_narration": "In the modern era of big data, although the cost of labeling and analyzing a single data sample has been decreasing rapidly, it is usually outpaced by the unrivaled fast growth of dataset size, which makes it particularly timely to design unsupervised learning algorithms that are able to discover meaningful structures of data without extensive human efforts. Recently, Generative Adversarial Networks (GANs) have emerged as a thriving unsupervised machine learning technique that has led to significant advances in various fields such as computer vision, natural language processing, and others. GANs can generate high-quality realistic images based on unlabeled natural images and perform sophisticated tasks such as synthesizing photos from sketches and coloring images. However, there also exist challenges that need timely solutions. The training of GANs has been reportedly observed to be challenging, unstable, and not easily reproducible. This project seeks to conduct a systematic study of GANs through the fundamental formulation, generalization and optimization issues. The transformative potential of the project is in the development of foundational tools and practical guidelines through novel combinations of optimal transport, information theory, convex geometry, and empirical process theory.  \r\n\r\nThe goal of this project is four-fold: (1) develop a theoretical framework for analyzing the generalization properties of GANs in high-dimensions; (2) suggest principled approaches to design GANs to achieve optimal statistical properties; (3) diagnose GANs when issues such as mode collapse or discriminator winning occur; (4) develop computationally efficient algorithms that can attain the statistical limits of well-designed GANs. The theory and algorithms developed within this projection will have impact on various engineering and scientific applications and provide insights for the proper usage of GANs in the real world.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Tse",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "David N Tse",
   "pi_email_addr": "dntse@stanford.edu",
   "nsf_id": "000093631",
   "pi_start_date": "2019-06-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "3160 Porter Drive, Suite 100",
  "perf_city_name": "Palo Alto",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943041222",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Generative adversarial network (GAN) is a minimax game between two  players:  a generator whose goal is to produce real-like samples and a  discriminator whose task is to distinguish between the fake samples  generated by the generator and the real training samples. While GANs  result in state-of-the-art generative models in several benchmark  computer vision tasks, their theoretical aspects are still poorly  understood. Theoretical studies of GANs aim to address three main  questions: 1) Approximation: Which probability distributions can be  expressed by a generator function? 2) Generalization: Do the generative  models learned by GANs generalize properly to the true distribution of  data? 3) Optimization: Are standard gradient methods used to train GANs  globally stable? The outcome of this project is a comprehensive analysis of all the three  aspects for a simplistic GAN formulation with linear generator and  quadratic discriminator architectures. We also develop a convex analysis  framework for specifically analyzing GANs&rsquo; approximation properties,  which is applicable to a broad class of GAN formulations.</p>\n<p>The general problem of unsupervised learning is that of learning  something about an underlying distribution from which points are drawn.&nbsp;  In many problems, the points themselves are noisy and by resampling a  point, more independent samples can be obtained from that point. The  problem of active learning is that of deciding at each time whether to  sample a new point, or resample a previously sampled point, and if the  latter, which of the previously sampled points should be resampled.  Instances of this problem appear in applications such as adaptive Monte  Carlo computation.&nbsp; Moreover, one is interested in how coarse or  fine-grained the adaptivity has to be to get near-optimal sample  complexity.</p>\n<p>We formulate this as an infinite-armed bandit problem, each arm's average reward is  sampled from an unknown distribution, and each arm can be sampled  further to obtain noisy estimates of the average reward of that arm.  Prior work focuses on identifying the best arm, i.e., estimating the  maximum of the average reward distribution. We consider a general class  of distribution functionals beyond the maximum, and propose unified meta  algorithms for both the offline and online settings, achieving optimal  sample complexities. We show that online estimation, where the learner  can sequentially choose whether to sample a new or existing arm, offers  no advantage over the offline setting for estimating the mean  functional, but significantly reduces the sample complexity for other  functionals such as the median, maximum, and trimmed mean. The matching  lower bounds utilize several different Wasserstein distances. For the  special case of median estimation, we identify a curious thresholding  phenomenon on the indistinguishability between Gaussian convolutions  with respect to the noise level, which may be of independent interest.</p>\n<dl class=\"clearing\"><dd> </dd></dl>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 05/26/2024<br>\nModified by: David&nbsp;N&nbsp;Tse</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nGenerative adversarial network (GAN) is a minimax game between two  players:  a generator whose goal is to produce real-like samples and a  discriminator whose task is to distinguish between the fake samples  generated by the generator and the real training samples. While GANs  result in state-of-the-art generative models in several benchmark  computer vision tasks, their theoretical aspects are still poorly  understood. Theoretical studies of GANs aim to address three main  questions: 1) Approximation: Which probability distributions can be  expressed by a generator function? 2) Generalization: Do the generative  models learned by GANs generalize properly to the true distribution of  data? 3) Optimization: Are standard gradient methods used to train GANs  globally stable? The outcome of this project is a comprehensive analysis of all the three  aspects for a simplistic GAN formulation with linear generator and  quadratic discriminator architectures. We also develop a convex analysis  framework for specifically analyzing GANs approximation properties,  which is applicable to a broad class of GAN formulations.\n\n\nThe general problem of unsupervised learning is that of learning  something about an underlying distribution from which points are drawn.  In many problems, the points themselves are noisy and by resampling a  point, more independent samples can be obtained from that point. The  problem of active learning is that of deciding at each time whether to  sample a new point, or resample a previously sampled point, and if the  latter, which of the previously sampled points should be resampled.  Instances of this problem appear in applications such as adaptive Monte  Carlo computation. Moreover, one is interested in how coarse or  fine-grained the adaptivity has to be to get near-optimal sample  complexity.\n\n\nWe formulate this as an infinite-armed bandit problem, each arm's average reward is  sampled from an unknown distribution, and each arm can be sampled  further to obtain noisy estimates of the average reward of that arm.  Prior work focuses on identifying the best arm, i.e., estimating the  maximum of the average reward distribution. We consider a general class  of distribution functionals beyond the maximum, and propose unified meta  algorithms for both the offline and online settings, achieving optimal  sample complexities. We show that online estimation, where the learner  can sequentially choose whether to sample a new or existing arm, offers  no advantage over the offline setting for estimating the mean  functional, but significantly reduces the sample complexity for other  functionals such as the median, maximum, and trimmed mean. The matching  lower bounds utilize several different Wasserstein distances. For the  special case of median estimation, we identify a curious thresholding  phenomenon on the indistinguishability between Gaussian convolutions  with respect to the noise level, which may be of independent interest.\n \n\n\n\t\t\t\t\tLast Modified: 05/26/2024\n\n\t\t\t\t\tSubmitted by: DavidNTse\n"
 }
}