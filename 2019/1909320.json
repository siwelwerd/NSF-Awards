{
 "awd_id": "1909320",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: A Systematic Approach to Adversarial Machine Learning: Sparsity-based Defenses and Locally Linear Attacks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032920000",
 "po_email": "ahero@nsf.gov",
 "po_sign_block_name": "Alfred Hero",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 499856.0,
 "awd_amount": 515856.0,
 "awd_min_amd_letter_date": "2019-07-16",
 "awd_max_amd_letter_date": "2020-04-17",
 "awd_abstract_narration": "Machine learning has made tremendous advances in the past decade, and is rapidly becoming embedded in our daily lives. We experience its power directly when we interact with voice assistants and automated translation engines, which are improving rapidly every year.  Machine learning tools also enable many of the functionalities underlying search engines, e-commerce sites and social media. Thus, machine learning has become an essential component of cyberspace and our interactions with it, and is now poised to enter our physical space, for example, as a core component of perception for autonomous vehicles and drones. Much of the recent progress in machine learning has been in the area of multilayer, or deep, neural networks, which can be trained to learn complex relationships by leveraging the availability of large amounts of data and massive computing power.  However, before we rely on such capabilities for safety-critical applications such as vehicular autonomy, we must ensure the robustness and security of deep networks. Recent research shows, for example, that deep networks can be induced to make errors (e.g., to misclassify images) by an adversary by adding tiny perturbations which would be imperceptible to humans.  This project develops a systematic framework for defending against such adversarial perturbations, blending classical model-based techniques with the modern data-driven approach that characterizes machine learning practice today. The project will be validated through two key applications of deep learning: image classification and speech recognition.\r\n\r\nWhen the vulnerability of deep networks to adversarial perturbations was discovered a few years back, it was initially conjectured that this vulnerability is due to the complex and nonlinear nature of the neural networks.  However, there is now general agreement that this vulnerability is actually due to the excessive linearity of deep networks. Motivated by this observation, this project aims to develop a systematic approach to study adversarial machine learning by utilizing the sparsity inherent in natural data for defense, and locally linear models of the network for attack. The proposed approach is based on exploiting signal sparsity to develop provably efficient defense mechanisms. In particular, the project first investigates a sparsifying frontend, designed to preserve desired input information while attenuating perturbations before they enter the neural network. This then leads to a defense mechanism based on sparsifying the neural network, with the goal of mitigating the impact of an adversarial perturbation as it flows up the network. The methodology brings together ideas from sparse signal processing, optimization, and machine learning, and aims to bridge the gap between systematic theoretical understanding and machine learning practice. The proposal has an extensive evaluation plan that focuses on two important real-world applications of adversarial machine learning: image classification and speech recognition.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ramtin",
   "pi_last_name": "Pedarsani",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ramtin Pedarsani",
   "pi_email_addr": "ramtin@ece.ucsb.edu",
   "nsf_id": "000741613",
   "pi_start_date": "2019-07-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Upamanyu",
   "pi_last_name": "Madhow",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Upamanyu Madhow",
   "pi_email_addr": "madhow@ece.ucsb.edu",
   "nsf_id": "000296458",
   "pi_start_date": "2019-07-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "Harold Frank Hall",
  "perf_city_name": "Santa Barbara",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931069560",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499856.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p>The explosive growth of artificial intelligence (AI) and machine learning (ML) technologies over the past decade has been driven by advances in deep neural networks (DNNs).&nbsp; While DNNs are widely deployed, they have well-known limitations, including vulnerability to adversarial attacks, which are tiny (typically bounded in an appropriately chosen norm) perturbations to the input data that are carefully designed to create errors at the output of the DNN. &nbsp;Adversarial attacks represent an important abstraction for understanding security threats to AI technologies, and machine learning models which are robust to adversarial attacks are also potentially robust to non-adversarial impairments that occur ``in the wild,&rsquo;&rsquo; such as noise and various forms of distortion.&nbsp; The goal of this project is to design robust machine learning algorithms utilizing the sparsity of natural data, and to characterize the fundamental limits of robust learning and inference for different models for attacks and distortions.&nbsp;</p>\n<p>On the theoretical front, we have developed fundamental insight into robust classification by studying simplified models for the data, such as Gaussian mixture models, and a diversity of models for attacks and distortion, such as non-sparse perturbations that are bounded in magnitude, sparse perturbations which are not bounded in magnitude, and general noise models.&nbsp; We obtain a variety of results, including theoretical insight into defense mechanisms leveraging a sparsifying front-end that can provably improve the classification error in the presence of magnitude-bounded adversarial attacks, fundamental limits on robust classification under general noise models for binary Gaussian mixture models, detection-theoretic approaches to robust classification treating the adversarial perturbation as a nuisance parameter to be estimated, and understanding the generalization properties of adversarial training for sparse perturbations.&nbsp;&nbsp;</p>\n<p>We have developed a sequence of increasingly effective practical strategies for robust DNNs, leveraging the preceding theoretical insights along with extensive experiments on well-known image datasets used for benchmarking machine learning algorithms.&nbsp; The state of the art defense against adversarial attacks is adversarial training, in which data is subjected to adversarial perturbations during training.&nbsp; However, adversarial training against a particular type of attack may not lead to robustness against other distortions and attacks. We therefore seek to develop complementary strategies for enhancing general-purpose robustness.&nbsp; Early in the project, we showed the robustness benefits of nonlinear, neuro-inspired front-end signal processing in attenuating input perturbations.&nbsp; In the later stages of the project, we have developed a more general approach to modifying DNN training and inference based on ideas from neuroscience and communication theory.&nbsp; Standard DNN training is based on optimizing a cost function based on the DNN output, and cannot provide explicit control of the features extracted at the intermediate layers.&nbsp; We have proposed adding layerwise costs at intermediate layers (starting from the input layer) promoting sparse, strong activations which are more difficult to distort.&nbsp; Experiments with image datasets in the final year of the project show significant enhancements in general-purpose robustness, and represent a promising foundation for future work.</p>\n<p>Results from the project have been widely disseminated, with more than 20 publications in reputable conferences and journals, and invited presentations at conferences, universities and industry research labs.&nbsp; Ongoing collaborations with industry researchers increase the potential for successful technology transfer.</p>\n<p>The project includes major education and undergraduate research activities. Two graduate courses have been developed at UCSB with modules on adversarial machine learning. Moreover, REU (Research Experience for Undergraduates) supplement funding has been utilized to train 5 undergraduate students on this project.&nbsp;</p>\n<p>&nbsp;</p>\n</div>\n</div>\n</div>\n</div><br>\n<p>\n Last Modified: 01/16/2024<br>\nModified by: Upamanyu&nbsp;Madhow</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\n\nThe explosive growth of artificial intelligence (AI) and machine learning (ML) technologies over the past decade has been driven by advances in deep neural networks (DNNs). While DNNs are widely deployed, they have well-known limitations, including vulnerability to adversarial attacks, which are tiny (typically bounded in an appropriately chosen norm) perturbations to the input data that are carefully designed to create errors at the output of the DNN. Adversarial attacks represent an important abstraction for understanding security threats to AI technologies, and machine learning models which are robust to adversarial attacks are also potentially robust to non-adversarial impairments that occur ``in the wild, such as noise and various forms of distortion. The goal of this project is to design robust machine learning algorithms utilizing the sparsity of natural data, and to characterize the fundamental limits of robust learning and inference for different models for attacks and distortions.\n\n\nOn the theoretical front, we have developed fundamental insight into robust classification by studying simplified models for the data, such as Gaussian mixture models, and a diversity of models for attacks and distortion, such as non-sparse perturbations that are bounded in magnitude, sparse perturbations which are not bounded in magnitude, and general noise models. We obtain a variety of results, including theoretical insight into defense mechanisms leveraging a sparsifying front-end that can provably improve the classification error in the presence of magnitude-bounded adversarial attacks, fundamental limits on robust classification under general noise models for binary Gaussian mixture models, detection-theoretic approaches to robust classification treating the adversarial perturbation as a nuisance parameter to be estimated, and understanding the generalization properties of adversarial training for sparse perturbations.\n\n\nWe have developed a sequence of increasingly effective practical strategies for robust DNNs, leveraging the preceding theoretical insights along with extensive experiments on well-known image datasets used for benchmarking machine learning algorithms. The state of the art defense against adversarial attacks is adversarial training, in which data is subjected to adversarial perturbations during training. However, adversarial training against a particular type of attack may not lead to robustness against other distortions and attacks. We therefore seek to develop complementary strategies for enhancing general-purpose robustness. Early in the project, we showed the robustness benefits of nonlinear, neuro-inspired front-end signal processing in attenuating input perturbations. In the later stages of the project, we have developed a more general approach to modifying DNN training and inference based on ideas from neuroscience and communication theory. Standard DNN training is based on optimizing a cost function based on the DNN output, and cannot provide explicit control of the features extracted at the intermediate layers. We have proposed adding layerwise costs at intermediate layers (starting from the input layer) promoting sparse, strong activations which are more difficult to distort. Experiments with image datasets in the final year of the project show significant enhancements in general-purpose robustness, and represent a promising foundation for future work.\n\n\nResults from the project have been widely disseminated, with more than 20 publications in reputable conferences and journals, and invited presentations at conferences, universities and industry research labs. Ongoing collaborations with industry researchers increase the potential for successful technology transfer.\n\n\nThe project includes major education and undergraduate research activities. Two graduate courses have been developed at UCSB with modules on adversarial machine learning. Moreover, REU (Research Experience for Undergraduates) supplement funding has been utilized to train 5 undergraduate students on this project.\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 01/16/2024\n\n\t\t\t\t\tSubmitted by: UpamanyuMadhow\n"
 }
}