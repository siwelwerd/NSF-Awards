{
 "awd_id": "1909089",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Small: Rich Surface Interaction for Augmented Environments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 499552.0,
 "awd_amount": 515392.0,
 "awd_min_amd_letter_date": "2019-08-14",
 "awd_max_amd_letter_date": "2020-07-14",
 "awd_abstract_narration": "Virtual Reality (VR) and Augmented Reality (AR) head-mounted displays are increasingly being used in different computing related activities such as data visualization, education, and training. Currently, VR and AR devices lack efficient and ergonomic ways to perform common desktop interactions such as pointing-and-clicking and entering text. The goal of this project is to transform flat, everyday surfaces into a rich interactive surface. For example, a desk or a wall could be transformed into a virtual keyboard. Flat surfaces afford not only haptic feedback, but also provide ergonomic advantages by providing a place to rest your arms. This project will develop a system where microphones are placed on surfaces to enable the sensing of when and where a tap has occurred. Further, the system aims to differentiate different types of touch interactions such as tapping with a fingernail, tapping with a finger pad, or making short swipe gestures. \r\n\r\nThis project will investigate different machine learning algorithms for producing a continuous coordinate for taps on a surface along with associated error bars. Using the confidence of sensed taps, the project will investigate ways to intelligently inform aspects of the user interface, e.g. guiding the autocorrection algorithm of a virtual keyboard decoder. Initially, the project will investigate sensing via an array of surface-mounted microphones and design \"surface algorithms\" to determine and compare the location accuracy of the finger taps on the virtual keyboard. These algorithms will experiment with different models including existing time-of-flight model, a new model based on Gaussian Process Regression, and a baseline of classification using support vector machines. For all models, the project will investigate the impact of the amount of training data from other users, and varying the amount of adaptation data from the target user. The project will compare surface microphones with approaches utilizing cameras and wrist-based inertial sensors. The project will generate human-factors results on the accuracy, user preference, and ergonomics of interacting midair versus on a rigid surface. By examining different sensors, input surfaces, and interface designs, the project will map the design space for future AR and VR interactive systems. The project will disseminate software and data allowing others to outfit tables or walls with microphones to enable rich interactive experiences.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Keith",
   "pi_last_name": "Vertanen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Keith Vertanen",
   "pi_email_addr": "vertanen@mtu.edu",
   "nsf_id": "000620484",
   "pi_start_date": "2019-08-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Scott",
   "pi_last_name": "Kuhl",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Scott Kuhl",
   "pi_email_addr": "kuhl@mtu.edu",
   "nsf_id": "000558928",
   "pi_start_date": "2019-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Michigan Technological University",
  "inst_street_address": "1400 TOWNSEND DR",
  "inst_street_address_2": "",
  "inst_city_name": "HOUGHTON",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "9064871885",
  "inst_zip_code": "499311200",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MI01",
  "org_lgl_bus_name": "MICHIGAN TECHNOLOGICAL UNIVERSITY",
  "org_prnt_uei_num": "GKMSN3DA6P91",
  "org_uei_num": "GKMSN3DA6P91"
 },
 "perf_inst": {
  "perf_inst_name": "Michigan Technological University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "499311295",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499552.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 15840.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-68ef745b-7fff-7e8f-d940-10455137849a\"> </span></p>\n<p><span id=\"docs-internal-guid-28f73fb6-7fff-3106-9af8-3a61974917ea\"> </span></p>\n<p dir=\"ltr\"><span>In the future, augmentative reality (AR) glasses that overlay virtual content onto the real-world may offer an alternative to touchscreen devices for performing everyday computing tasks such as text entry. This project investigated algorithms and interfaces for detecting when and where people tapped on an everyday surface (e.g. a table or wall).</span></p>\n<p dir=\"ltr\"><span>In the project's first study, we had participants tap on a wall while we recorded video from an egocentric camera. Due to COVID restrictions, we conducted this study remotely and collected data via a participant's phone. Our results showed that tap or swipe gestures can plausibly be detected via a remote microphone located near a user's head.</span></p>\n<p dir=\"ltr\"><span>To investigate if the tactile feedback afforded by co-locating an AR virtual keyboard on a table or wall improved typing, we conducted a study with a HoloLens AR headset. Our study showed that despite the tactile feedback offered by a table or wall, participants typed faster and preferred a midair keyboard. In a follow-up study, we explored using a novel eye-tracking feature to improve midair ten-finger typing. Our eye-tracking technique halved how often users needed to backspace errors.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Our HoloLens studies showed that current AR headsets with hand tracking based on egocentric cameras may not be accurate enough to support efficient typing. A second thrust of the project was to sense a user's typing on a surface instrumented with a small number of contact microphones. Surfaces in a user's environment could be pre-instrumented with microphones to create an interactive surface, or a surface could be temporarily instrumented by deploying a small number of wireless devices.</span></p>\n<p dir=\"ltr\"><span>We developed a suite of algorithms for detecting when and where a tap occurred based on the acoustic signal from an array of microphones. We collected a dataset of 16 people tapping on an ordinary table augmented with microphones. Our dataset includes the audio recorded by eight microphones (four on the top of the table and four on the underside). Additionally, we recorded ground truth information in the center of the table via an infrared sensor. Using the four top microphones, we were able to correctly identify tap events 95% of the time. Using an algorithm based on the time difference of arrival, we located a user's tap location with an average error of 24 millimeters.&nbsp;</span></p>\n<p dir=\"ltr\"><span>In addition to our physics-based algorithm, we also explored a machine learning classification approach. We adapted a transformer neural network pretrained on audio to classify which key a user tapped based on audio from the microphone array. Our best system had an accuracy of 95% over 34 key targets and significantly outperformed other machine learning baseline models.</span></p>\n<p dir=\"ltr\"><span><span id=\"docs-internal-guid-a7fce7dd-7fff-9dc9-ecf3-da2b2b62d929\"><span><span id=\"docs-internal-guid-8beefc9c-7fff-b71b-5a34-67ddeee018f3\"><span>This project added substantially to our knowledge about how everyday surfaces can be used to provide an interactive surface for computing tasks such as text entry. We showed that both a physics and a machine learning model could resolve a large number of small targets based on an array of surface microphones. Our results show that efficient typing on a surface is possible, especially if it were coupled with auto-correct. The project has created four unique public datasets and three public code repositories to aid future AR research. Finally, this project provided research opportunities for four graduate students (one of whom became a research assistant professor) and four undergraduate students (two of whom continued to graduate studies).</span></span></span></span></span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/05/2024<br>\nModified by: Keith&nbsp;Vertanen</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1909089/1909089_10633749_1729005678147_table_with_surface_microphones--rgov-214x142.jpeg\" original=\"/por/images/Reports/POR/2024/1909089/1909089_10633749_1729005678147_table_with_surface_microphones--rgov-800width.jpeg\" title=\"Table with four microphones and a printed keyboard and grid.\"><img src=\"/por/images/Reports/POR/2024/1909089/1909089_10633749_1729005678147_table_with_surface_microphones--rgov-66x44.jpeg\" alt=\"Table with four microphones and a printed keyboard and grid.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A table instrumented with four contact microphones arranged in a diamond configuration. The table has a grid and as well as a QWERTY keyboard printed on it. The grid lines are two centimeters apart. The keyboard is the same size as a standard desktop keyboard.</div>\n<div class=\"imageCredit\">Keith Vertanen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Keith&nbsp;Vertanen\n<div class=\"imageTitle\">Table with four microphones and a printed keyboard and grid.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1909089/1909089_10633749_1729006606124_hololens_midair_keyboard--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/1909089/1909089_10633749_1729006606124_hololens_midair_keyboard--rgov-800width.jpg\" title=\"HoloLens virtual keyboard located in midair.\"><img src=\"/por/images/Reports/POR/2024/1909089/1909089_10633749_1729006606124_hololens_midair_keyboard--rgov-66x44.jpg\" alt=\"HoloLens virtual keyboard located in midair.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The condition in our HoloLens typing study in which participants typed in midair using all ten fingers with the help of an eye-tracking feature to prevent accidental key presses. Currently  the participant is looking at the G key and only the six adjacent keys are active.</div>\n<div class=\"imageCredit\">Keith Vertanen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Keith&nbsp;Vertanen\n<div class=\"imageTitle\">HoloLens virtual keyboard located in midair.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1909089/1909089_10633749_1729006317728_hololens_table_keyboard--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/1909089/1909089_10633749_1729006317728_hololens_table_keyboard--rgov-800width.jpg\" title=\"HoloLens virtual keyboard located on a table.\"><img src=\"/por/images/Reports/POR/2024/1909089/1909089_10633749_1729006317728_hololens_table_keyboard--rgov-66x44.jpg\" alt=\"HoloLens virtual keyboard located on a table.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The condition in our HoloLens typing study in which we located a keyboard right above the surface of a table. Participants typed sentences by tapping the keys on a deterministic virtual keyboard using one or both index fingers.</div>\n<div class=\"imageCredit\">Keith Vertanen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Keith&nbsp;Vertanen\n<div class=\"imageTitle\">HoloLens virtual keyboard located on a table.</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\n \n\n\nIn the future, augmentative reality (AR) glasses that overlay virtual content onto the real-world may offer an alternative to touchscreen devices for performing everyday computing tasks such as text entry. This project investigated algorithms and interfaces for detecting when and where people tapped on an everyday surface (e.g. a table or wall).\n\n\nIn the project's first study, we had participants tap on a wall while we recorded video from an egocentric camera. Due to COVID restrictions, we conducted this study remotely and collected data via a participant's phone. Our results showed that tap or swipe gestures can plausibly be detected via a remote microphone located near a user's head.\n\n\nTo investigate if the tactile feedback afforded by co-locating an AR virtual keyboard on a table or wall improved typing, we conducted a study with a HoloLens AR headset. Our study showed that despite the tactile feedback offered by a table or wall, participants typed faster and preferred a midair keyboard. In a follow-up study, we explored using a novel eye-tracking feature to improve midair ten-finger typing. Our eye-tracking technique halved how often users needed to backspace errors.\n\n\nOur HoloLens studies showed that current AR headsets with hand tracking based on egocentric cameras may not be accurate enough to support efficient typing. A second thrust of the project was to sense a user's typing on a surface instrumented with a small number of contact microphones. Surfaces in a user's environment could be pre-instrumented with microphones to create an interactive surface, or a surface could be temporarily instrumented by deploying a small number of wireless devices.\n\n\nWe developed a suite of algorithms for detecting when and where a tap occurred based on the acoustic signal from an array of microphones. We collected a dataset of 16 people tapping on an ordinary table augmented with microphones. Our dataset includes the audio recorded by eight microphones (four on the top of the table and four on the underside). Additionally, we recorded ground truth information in the center of the table via an infrared sensor. Using the four top microphones, we were able to correctly identify tap events 95% of the time. Using an algorithm based on the time difference of arrival, we located a user's tap location with an average error of 24 millimeters.\n\n\nIn addition to our physics-based algorithm, we also explored a machine learning classification approach. We adapted a transformer neural network pretrained on audio to classify which key a user tapped based on audio from the microphone array. Our best system had an accuracy of 95% over 34 key targets and significantly outperformed other machine learning baseline models.\n\n\nThis project added substantially to our knowledge about how everyday surfaces can be used to provide an interactive surface for computing tasks such as text entry. We showed that both a physics and a machine learning model could resolve a large number of small targets based on an array of surface microphones. Our results show that efficient typing on a surface is possible, especially if it were coupled with auto-correct. The project has created four unique public datasets and three public code repositories to aid future AR research. Finally, this project provided research opportunities for four graduate students (one of whom became a research assistant professor) and four undergraduate students (two of whom continued to graduate studies).\n\n\n\t\t\t\t\tLast Modified: 11/05/2024\n\n\t\t\t\t\tSubmitted by: KeithVertanen\n"
 }
}