{
 "awd_id": "1918692",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Robust Diagnosis in Electronic Health Records Integrating Physics-based Missing Data Multiple Imputation, Fast Inference for Hemodynamic Models, and Differential Privacy.",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Christopher Stark",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 880203.0,
 "awd_amount": 880203.0,
 "awd_min_amd_letter_date": "2019-05-24",
 "awd_max_amd_letter_date": "2019-05-24",
 "awd_abstract_narration": "This project will introduce new paradigms for dealing with missing values in electronic health record (EHR) data, with the objective of developing novel approaches for early diagnosis of diastolic ventricular dysfunction, a silent disease responsible for one-third of the total heart failure-related deaths worldwide. EHR are often messy and suffer from missing data problem for various reasons, for example more frequent clinical exams after the manifestation of the first symptoms of a certain disease and less frequent exams during routine screening. Missing data often limits the ability to extract useful information from these sources (e.g., early diagnosis). The goal of this project is to leverage the fact that missing information sometimes satisfies mathematical or physical principles to develop innovative model-based imputation approaches, combining models and efficient privacy-preserving learning techniques in large EHR datasets. Computationally efficient algorithms will be developed to train numerical models while preserving patient privacy, and the feasibility and practical usefulness of these approaches will be demonstrated at a scale that has not yet been addressed in the literature. The approaches for predictive numerical models developed for this project can be applied broadly in various fields. Additional project goals include development of infrastructure for research and education through freely available, open-source software libraries. This project will also provide invaluable multi-disciplinary skills to undergraduate and graduate students. Both research and outreach efforts focus on increasing the participation of women, people with disabilities, and of underrepresented groups.\r\n\r\nThe team will develop novel regularization approaches through numerical models, i.e., optimally trained models able to suggest distributions of missing data based on the underlying physics. For EHRs characterizing cardiovascular function, lumped parameter hemodynamic models offer an ideal regularizer. Parameter estimation for these models using Markov chain Monte Carlo is computationally expensive and therefore incompatible with fast application to large EHR collections. Additionally, optimally trained numerical models of the cardiovascular system can be thought as a type of query, rising issues of patient privacy. The proposed research tackles these issues through: (1) Acquisition and analysis of a large heart failure EHR dataset. (2) Development of privacy-preserving variational inference for hemodynamic models, enhanced using homotopy-based optimization. (3) Implementation and extensive testing of novel imputation approaches for missing data, combining uncertainty quantification and numerical models. (4) Demonstration on a large patient cohort.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniele",
   "pi_last_name": "Schiavazzi",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Daniele E Schiavazzi",
   "pi_email_addr": "dschiavazzi@nd.edu",
   "nsf_id": "000727147",
   "pi_start_date": "2019-05-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alison",
   "pi_last_name": "Marsden",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Alison L Marsden",
   "pi_email_addr": "amarsden@stanford.edu",
   "nsf_id": "000520439",
   "pi_start_date": "2019-05-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jonathan",
   "pi_last_name": "Hauenstein",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Jonathan D Hauenstein",
   "pi_email_addr": "hauenstein@nd.edu",
   "nsf_id": "000549793",
   "pi_start_date": "2019-05-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Fang",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fang Liu",
   "pi_email_addr": "Fang.Liu.131@nd.edu",
   "nsf_id": "000606444",
   "pi_start_date": "2019-05-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "andrew",
   "pi_last_name": "kahn",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "andrew kahn",
   "pi_email_addr": "akahn@ucsd.edu",
   "nsf_id": "000796403",
   "pi_start_date": "2019-05-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Notre Dame",
  "inst_street_address": "940 GRACE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "NOTRE DAME",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "5746317432",
  "inst_zip_code": "465565708",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "IN02",
  "org_lgl_bus_name": "UNIVERSITY OF NOTRE DAME DU LAC",
  "org_prnt_uei_num": "FPU6XGFXMBE9",
  "org_uei_num": "FPU6XGFXMBE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Notre Dame",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "465565708",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "IN02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 880203.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Applications of Artificial Intelligence (AI) in healthcare generally require a significant amount of data. However, the amount of patient-specific information typically available from Electronic Health Records (EHR) is scarce as collected under routine screening, and more abundant after the manifestation of symptoms. In addition, hemodynamic indicators (such as blood pressures, flow rates and volumes) can be indicative of cardiovascular disease, and hence contain sensitive information about the patient's health.<br /><br />Regularization plays a crucial role in addressing ill-posed inverse problems by introducing additional structure. Its primary objective is to reduce the size of the training dataset while maintaining the accuracy of a given predictor. This project focuses on a special type of regularization which occurs when the available data satisfy physical constraints, for example the Navier-Stokes (NS) equations governing fluid motion. For EHRs characterizing cardiovascular function, lumped parameter hemodynamic models (LPN, i.e., circuit models simulating the evolution of flow and pressure in the human cardiovascular system) offer an apt form of regularization. Thus, this project focuses on developing computational strategies to estimate the parameter of physics-based models, as a generative process for missing EHR data. Our specific interest lies in predicting elevated pulmonary pressures as an early indicator of cardiovascular disease, anticipating anomalies that are only detectable through invasive catheterization.<br /><br />Currently, parameter estimation for LPN models is primarily conducted using Markov Chain Monte Carlo sampling when data is scarce, or employing sequential Monte Carlo methods based on time-resolved clinical measurements when such data is available. These approaches are computationally expensive and convergence may be difficult to assess, particularly when multiple combinations of parameters lead to similar model outputs.<br /><br />During the initial phase of this project, our research team collaborated with the Regenstief Institute Data Core at the Indiana University School of Medicine to generate a dataset comprising ~1.6 million entries, extracting data from clinical notes with natural language processing. Each entry contains the measurement of a specific hemodynamic quantity (out of a total of 23 indicators considered) for an individual patient. Subsequently, a thorough statistical analysis of the dataset was conducted and a report was issued.<br /><br />In the second part we developed a general suite of tools for improving the robustness and accuracy of variational inference for estimating parameters of computationally expensive physics-based models. We started by performing extensive testing on state-of-the-art formulations for discrete autoregressive normalizing flow discussed in the literature. To reduce the cost of inference we developed NoFAS, where a neural surrogate is adaptively trained based on samples generated from an iteratively improved approximations of the posterior distribution. Such a surrogate is trained based on a memory-aware loss function, designed to progressively forget older samples, while remembering a \"pre-grid\" of initial samples assuring global accuracy. NoFAS leads to accurate posterior distributions at a fraction of the computational cost of competing sample-based methods. <br /><br />In addition, when inferred model parameters are increasingly correlated, sampling from the posterior distribution becomes more and more difficult. As a possible remedy, the posterior distribution can be \"annealed\", transforming a difficult inference problem in a sequence of simplified&nbsp; tasks. The team proposed AdaAnn, a new adaptive annealing annealing algorithm where the increments in inverse temperature are automatically selected based on the accuracy in the variational approximation. AdaAnn leads to faster inference and the ability to correctly approximate multimodal posterior distributions as verified through extensive numerical experimentation.<br /><br />Next the research team has focused on the important problem of safeguarding privacy in the inference process. We first integrated privacy preserving stochastic gradient descent into VI. We then compared inference results obtained from private data (through privacy preserving density estimation from a subset of the EHR dataset generated for this project), with privacy preserving inference based on the original sensitive dataset. This analysis was conducted employing both a nonlinear regression model and a lumped parameter hemodynamic model. In the latter, numerous parameters were held constant at their nominal values for healthy adults. Direct VI from the original data was found to significantly overestimate parameter correlations, an issue that was attributed to the inability of a Kullback-Leibler (KL)-based loss function to correctly estimate the divergence between two correlated distributions. This also suggested possible remedies that are the subject of ongoing work. <br /><br />Finally, we tackled higher dimensional inference problems by progressively reducing the number of fixed LPN parameters. However, convergence was hindered by lack of identifiability. To overcome this challenge, the team initiated the development of a data-driven dimensionality reduction approach for VI.<br /><br />The source code developed for this project has been incorporated into the LINFA library, freely accessible on GitHub (https://github.com/desResLab/LINFA) along with documentation and validation benchmarks.<br /><br />The project generated 6 publications and one conference paper. In addition, the four students supported by this award presented their work at international conference or institutional seminars on 12 occasions. They also participated in three Summer internships at National Laboratories and in the industry.</p><br>\n<p>\n Last Modified: 12/28/2023<br>\nModified by: Daniele&nbsp;E&nbsp;Schiavazzi</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nApplications of Artificial Intelligence (AI) in healthcare generally require a significant amount of data. However, the amount of patient-specific information typically available from Electronic Health Records (EHR) is scarce as collected under routine screening, and more abundant after the manifestation of symptoms. In addition, hemodynamic indicators (such as blood pressures, flow rates and volumes) can be indicative of cardiovascular disease, and hence contain sensitive information about the patient's health.\n\nRegularization plays a crucial role in addressing ill-posed inverse problems by introducing additional structure. Its primary objective is to reduce the size of the training dataset while maintaining the accuracy of a given predictor. This project focuses on a special type of regularization which occurs when the available data satisfy physical constraints, for example the Navier-Stokes (NS) equations governing fluid motion. For EHRs characterizing cardiovascular function, lumped parameter hemodynamic models (LPN, i.e., circuit models simulating the evolution of flow and pressure in the human cardiovascular system) offer an apt form of regularization. Thus, this project focuses on developing computational strategies to estimate the parameter of physics-based models, as a generative process for missing EHR data. Our specific interest lies in predicting elevated pulmonary pressures as an early indicator of cardiovascular disease, anticipating anomalies that are only detectable through invasive catheterization.\n\nCurrently, parameter estimation for LPN models is primarily conducted using Markov Chain Monte Carlo sampling when data is scarce, or employing sequential Monte Carlo methods based on time-resolved clinical measurements when such data is available. These approaches are computationally expensive and convergence may be difficult to assess, particularly when multiple combinations of parameters lead to similar model outputs.\n\nDuring the initial phase of this project, our research team collaborated with the Regenstief Institute Data Core at the Indiana University School of Medicine to generate a dataset comprising ~1.6 million entries, extracting data from clinical notes with natural language processing. Each entry contains the measurement of a specific hemodynamic quantity (out of a total of 23 indicators considered) for an individual patient. Subsequently, a thorough statistical analysis of the dataset was conducted and a report was issued.\n\nIn the second part we developed a general suite of tools for improving the robustness and accuracy of variational inference for estimating parameters of computationally expensive physics-based models. We started by performing extensive testing on state-of-the-art formulations for discrete autoregressive normalizing flow discussed in the literature. To reduce the cost of inference we developed NoFAS, where a neural surrogate is adaptively trained based on samples generated from an iteratively improved approximations of the posterior distribution. Such a surrogate is trained based on a memory-aware loss function, designed to progressively forget older samples, while remembering a \"pre-grid\" of initial samples assuring global accuracy. NoFAS leads to accurate posterior distributions at a fraction of the computational cost of competing sample-based methods. \n\nIn addition, when inferred model parameters are increasingly correlated, sampling from the posterior distribution becomes more and more difficult. As a possible remedy, the posterior distribution can be \"annealed\", transforming a difficult inference problem in a sequence of simplified tasks. The team proposed AdaAnn, a new adaptive annealing annealing algorithm where the increments in inverse temperature are automatically selected based on the accuracy in the variational approximation. AdaAnn leads to faster inference and the ability to correctly approximate multimodal posterior distributions as verified through extensive numerical experimentation.\n\nNext the research team has focused on the important problem of safeguarding privacy in the inference process. We first integrated privacy preserving stochastic gradient descent into VI. We then compared inference results obtained from private data (through privacy preserving density estimation from a subset of the EHR dataset generated for this project), with privacy preserving inference based on the original sensitive dataset. This analysis was conducted employing both a nonlinear regression model and a lumped parameter hemodynamic model. In the latter, numerous parameters were held constant at their nominal values for healthy adults. Direct VI from the original data was found to significantly overestimate parameter correlations, an issue that was attributed to the inability of a Kullback-Leibler (KL)-based loss function to correctly estimate the divergence between two correlated distributions. This also suggested possible remedies that are the subject of ongoing work. \n\nFinally, we tackled higher dimensional inference problems by progressively reducing the number of fixed LPN parameters. However, convergence was hindered by lack of identifiability. To overcome this challenge, the team initiated the development of a data-driven dimensionality reduction approach for VI.\n\nThe source code developed for this project has been incorporated into the LINFA library, freely accessible on GitHub (https://github.com/desResLab/LINFA) along with documentation and validation benchmarks.\n\nThe project generated 6 publications and one conference paper. In addition, the four students supported by this award presented their work at international conference or institutional seminars on 12 occasions. They also participated in three Summer internships at National Laboratories and in the industry.\t\t\t\t\tLast Modified: 12/28/2023\n\n\t\t\t\t\tSubmitted by: DanieleESchiavazzi\n"
 }
}