{
 "awd_id": "1909854",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Collaborative Research: Retraining-free Concurrent Test and Diagnosis in Emerging Neural Network Accelerators",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 264998.0,
 "awd_amount": 264998.0,
 "awd_min_amd_letter_date": "2019-08-15",
 "awd_max_amd_letter_date": "2019-08-15",
 "awd_abstract_narration": "Neural networks have become the go-to tool for solving many real-world recognition and classification problems in computer vision, language processing, life sciences and finance. While promising, smart and intelligent data interpretation via deep learning is extremely power hungry. To conduct power-efficient deep learning on battery-constrained edge platforms, one promising solution is to use hardware accelerators built with emerging non-volatile memory (NVM) devices, which offer high density, extremely low power consumption, as well as in-situ and parallelized data processing. While these advances are enticing, NVM devices also impose extra challenges, as their design and manufacturing technology are far less mature than CMOS. Furthermore, NVM technologies are likely to exhibit new types of errors, such as read/write disturbance, values drifting over time, and short data retention time. These errors can accumulate while the accelerator is running a deep learning application, and without careful mitigation could lead to significant accuracy degradation. To assuage these concerns, this project will develop a self-healing framework for NVM-based neural network accelerators integrating a test, diagnosis, and recovery loop that monitors and maintains the health of the accelerator. Results of this project will (1) deepen the understanding of interactions among hardware defects and errors, NVM-based accelerators, and machine learning, (2) increase community awareness of post-fabrication error debugging and fixing techniques, (3) enrich the computer engineering course curriculum, and (4) train and promote students of diverse backgrounds for both the workforce and research.\r\n \r\nThis project will investigate, characterize, and mitigate errors that will affect the adoption of NVM-based neural network accelerators. While existing solutions focus on fixing errors observed at fabrication time, this project targets the NVM-specific errors that will occur over the life of the accelerator, not just at the time of manufacturing. The project will lead to four outcomes, namely, (1) measurement and characterization of the error resilience capability of neural networks with different topologies and data types, (2) cost-effective approaches for deploying neural networks alongside NVM-based accelerators which exhibit new and diverse error patterns without involving costly retraining, (3) methods for generating neural network inputs as test vectors which will be tuned to be sensitive to different levels of error accumulation and accuracy loss and will provide real-time accelerator health statistics, and (4) an algorithm and device level co-diagnosis procedure which identifies and protects the most critical and vulnerable components of the neural network and the accelerator.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chengmo",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chengmo Yang",
   "pi_email_addr": "chengmo@udel.edu",
   "nsf_id": "000577737",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Delaware",
  "inst_street_address": "550 S COLLEGE AVE",
  "inst_street_address_2": "",
  "inst_city_name": "NEWARK",
  "inst_state_code": "DE",
  "inst_state_name": "Delaware",
  "inst_phone_num": "3028312136",
  "inst_zip_code": "197131324",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DE00",
  "org_lgl_bus_name": "UNIVERSITY OF DELAWARE",
  "org_prnt_uei_num": "",
  "org_uei_num": "T72NHKM259N3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Delaware",
  "perf_str_addr": "210 Hullihen Hall",
  "perf_city_name": "Newark",
  "perf_st_code": "DE",
  "perf_st_name": "Delaware",
  "perf_zip_code": "197162553",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DE00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 264998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project is to develop a reliable and cost-efficient framework to overcome the accuracy drop caused by errors in emerging Non-Volatile Memory (NVM)-based Process-In-Memory (PIM) neural network accelerators. The key idea is to integrate defect-aware deployment as well as self-testing and self-healing processes into a reliable framework, so that it can monitor and maintain the health of the accelerator in the face of a variety of fault types and at affordable hardware, performance, throughput, and power costs.&nbsp;</p>\r\n<p>At the University of Delaware, we measured and characterized the resilience of popular neural network workloads to various fault rates under different network topologies and quantization-levels. Our study indicates that: (1) Small rates of random faults do not have a noticeable impact on accuracy, their impact becomes significant as faults accumulate in weight memory; (2) Reliability attacks can crack NNs by injecting a small number of worst-case faults. We also conducted fault injection in real hardware. Specifically, we implemented a Deep-learning Processing Unit (DPU) on an AMD KRIA KV260 FPGA board, and injected faults to the inference stage through clock glitching. Our hardware-based experiments revealed that: (1) The fully connected (FC) layer is much more vulnerable to faults than the other layers; and (2) It is possible to perform a controlled fault injection attack that derails DNN inputs to a targeted range of classes.&nbsp;</p>\r\n<p>To tolerate permanent defects in neural network accelerators, our solution exploits the flexibility in setting the fault-free bits in weight memory to effectively approximate weight values. It was motivated by the key observation that small deviations in weight values only have trivial impact on model accuracy. Therefore, it is unnecessary to precisely restore weight values back to the original. Instead, our solution adopts an imprecise recovery policy that aims to mitigate large deviations in weight values. we have developed three recovery algorithms: Add/Sub Approximation, LSB Approximation, and Count-One (C-1) Approximation. These algorithms effectively mitigate the large deviations caused by defects in higher-order bits of DNN weights. They can be applied as a one-step solution when loading the weights to the accelerator, and they only require trivial hardware support and impose negligible run-time overhead.&nbsp;</p>\r\n<p>Targeting transient errors that may affect the accelerator hardware, we proposed a comprehensive self-test framework to monitor the health of NN accelerators at run-time. This framework adopts a two-stage test and diagnosis process and offers three functions: fault detection, fault type identification, and fault severeness prediction. Each function performs testing with a small set of images down selected from the test dataset. We have developed multiple test image selection strategies that rely on either numerical standards or statistical information collected from comprehensive fault injection experiments. We also developed an approach that generates ``white noise'' style test images to fulfill two criteria: the original fault-free model is extremely confused about a test image, while a faulty model is very confident about its class. Experimental results showed that these images achieve higher detection accuracy.</p>\r\n<p>In addition to random faults, our research also investigated maliciously injected faults. Specifically, we developed a light-weight countermeasure which protects the integrity of neural network models against Bit-Flip Attacks (BFAs), by embedding Light-weight Integrity MArks (LIMA) into the sum of a group of weights. LIMA can be easily embedded into a trained and quantized DNN model without demanding any extra hardware or storage support and can be verified through standard Multiply-Accumulate (MAC) operations during the inference stage to capture bit-flip attacks. LIMA provides comprehensive protection of all the layers of a DNN model. It is capable of detecting every single chain of bit-flips chosen by BFAs, while causing near-zero accuracy drop and non-observable change to the weight distribution.</p>\r\n<p>Targeting malicious errors that may be injected into the fully connected (FC) layer, we developed a selective and permuted recomputation scheme. It selects a subset of critical FC outputs for recomputation while at the same time permuting the computation of the FC layer to prevent an adversary from pinpointing the exact time of executing the target class. The proposed defense mechanism only requires basic arithmetic operations and control logic, which can be easily implemented in hardware. Experimental results show that under fault injection attacks, it can successfully recover 90-95% of the models&rsquo; original accuracy, achieved with less than 1.61% runtime overhead and no storage overhead.</p>\r\n<p>The project has offered extensive training opportunities to both undergraduate and graduate students. The project has supported four graduate students towards their PhD degrees. Ten undergraduates have been involved in the project. The PI has incorporated project results into the graduate course she taught in the University of Delaware, offeringcomprehensive training to students and bringing students up to the research frontier.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/07/2025<br>\nModified by: Chengmo&nbsp;Yang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of this project is to develop a reliable and cost-efficient framework to overcome the accuracy drop caused by errors in emerging Non-Volatile Memory (NVM)-based Process-In-Memory (PIM) neural network accelerators. The key idea is to integrate defect-aware deployment as well as self-testing and self-healing processes into a reliable framework, so that it can monitor and maintain the health of the accelerator in the face of a variety of fault types and at affordable hardware, performance, throughput, and power costs.\r\n\n\nAt the University of Delaware, we measured and characterized the resilience of popular neural network workloads to various fault rates under different network topologies and quantization-levels. Our study indicates that: (1) Small rates of random faults do not have a noticeable impact on accuracy, their impact becomes significant as faults accumulate in weight memory; (2) Reliability attacks can crack NNs by injecting a small number of worst-case faults. We also conducted fault injection in real hardware. Specifically, we implemented a Deep-learning Processing Unit (DPU) on an AMD KRIA KV260 FPGA board, and injected faults to the inference stage through clock glitching. Our hardware-based experiments revealed that: (1) The fully connected (FC) layer is much more vulnerable to faults than the other layers; and (2) It is possible to perform a controlled fault injection attack that derails DNN inputs to a targeted range of classes.\r\n\n\nTo tolerate permanent defects in neural network accelerators, our solution exploits the flexibility in setting the fault-free bits in weight memory to effectively approximate weight values. It was motivated by the key observation that small deviations in weight values only have trivial impact on model accuracy. Therefore, it is unnecessary to precisely restore weight values back to the original. Instead, our solution adopts an imprecise recovery policy that aims to mitigate large deviations in weight values. we have developed three recovery algorithms: Add/Sub Approximation, LSB Approximation, and Count-One (C-1) Approximation. These algorithms effectively mitigate the large deviations caused by defects in higher-order bits of DNN weights. They can be applied as a one-step solution when loading the weights to the accelerator, and they only require trivial hardware support and impose negligible run-time overhead.\r\n\n\nTargeting transient errors that may affect the accelerator hardware, we proposed a comprehensive self-test framework to monitor the health of NN accelerators at run-time. This framework adopts a two-stage test and diagnosis process and offers three functions: fault detection, fault type identification, and fault severeness prediction. Each function performs testing with a small set of images down selected from the test dataset. We have developed multiple test image selection strategies that rely on either numerical standards or statistical information collected from comprehensive fault injection experiments. We also developed an approach that generates ``white noise'' style test images to fulfill two criteria: the original fault-free model is extremely confused about a test image, while a faulty model is very confident about its class. Experimental results showed that these images achieve higher detection accuracy.\r\n\n\nIn addition to random faults, our research also investigated maliciously injected faults. Specifically, we developed a light-weight countermeasure which protects the integrity of neural network models against Bit-Flip Attacks (BFAs), by embedding Light-weight Integrity MArks (LIMA) into the sum of a group of weights. LIMA can be easily embedded into a trained and quantized DNN model without demanding any extra hardware or storage support and can be verified through standard Multiply-Accumulate (MAC) operations during the inference stage to capture bit-flip attacks. LIMA provides comprehensive protection of all the layers of a DNN model. It is capable of detecting every single chain of bit-flips chosen by BFAs, while causing near-zero accuracy drop and non-observable change to the weight distribution.\r\n\n\nTargeting malicious errors that may be injected into the fully connected (FC) layer, we developed a selective and permuted recomputation scheme. It selects a subset of critical FC outputs for recomputation while at the same time permuting the computation of the FC layer to prevent an adversary from pinpointing the exact time of executing the target class. The proposed defense mechanism only requires basic arithmetic operations and control logic, which can be easily implemented in hardware. Experimental results show that under fault injection attacks, it can successfully recover 90-95% of the models original accuracy, achieved with less than 1.61% runtime overhead and no storage overhead.\r\n\n\nThe project has offered extensive training opportunities to both undergraduate and graduate students. The project has supported four graduate students towards their PhD degrees. Ten undergraduates have been involved in the project. The PI has incorporated project results into the graduate course she taught in the University of Delaware, offeringcomprehensive training to students and bringing students up to the research frontier.\r\n\n\n\t\t\t\t\tLast Modified: 03/07/2025\n\n\t\t\t\t\tSubmitted by: ChengmoYang\n"
 }
}