{
 "awd_id": "1845171",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Optimization Landscape for Non-convex Functions - Towards Provable Algorithms for Neural Networks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 399999.0,
 "awd_amount": 399999.0,
 "awd_min_amd_letter_date": "2019-04-26",
 "awd_max_amd_letter_date": "2023-05-03",
 "awd_abstract_narration": "Deep learning, a machine learning method that is based on artificial neural networks, has greatly improved the performance of learning algorithms for many tasks that are related to understanding complicated data such as natural images, videos and language. Products based on deep learning have already made real-life impact in face recognition, machine translation, and shown promise for more applications such as self-driving cars. However, despite the practical success of deep learning, theoretical understanding for why these algorithms work has been scarce. One of the main difficulties in understanding deep learning algorithms is that these algorithms need to solve very complicated optimization problems that try to find out what are the best ways for the neurons to be connected. In the most general form, these optimization problems are known to be intractable. This research project will identify properties of the real-world problems that make these problems special and tractable, and provide new optimization algorithms with theoretical guarantees that are applicable to deep learning. The materials developed in the project will be disseminated through conferences and workshops that try to connect different research communities, and used to create new machine learning courses. The algorithms designed in the project will also be implemented in standard deep learning frameworks and made publicly available.\r\n\r\nThe specific approach of this project revolves around the new concept of optimization landscape. For an optimization problem, its optimization landscape includes clear understanding of the location and values of its local and global optimal solutions. The research goals are divided into three categories. First, the research project will focus on a class of locally optimizable functions for which local minima are all globally optimal. The research project will develop simple and efficient algorithms for optimizing such functions, as well as a new framework to prove several problems of practical interest are locally optimizable. Second, the project will develop stronger optimization algorithms that can work even when the optimization landscape is not as ideal. Finally, the research will focus on optimization problems that arise in deep learning and show how the techniques developed in the previous two parts can be applied. These projects will bring more theoretical insights into the heuristics for training neural networks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rong",
   "pi_last_name": "Ge",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rong Ge",
   "pi_email_addr": "rongge@cs.duke.edu",
   "nsf_id": "000699594",
   "pi_start_date": "2019-04-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "308 Research Dr, LSRC D226",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277080001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 89270.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 84086.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 153430.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 73213.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project focuses on the theoretical understanding of optimization for deep neural networks. Research has been performed in several closely related directions:</p>\n<ol>\n<li><strong>Training      dynamics of feature learning within neural networks</strong>. The ability to automatically      learn features from data has long been regarded as a main advantage of      deep learning. This project investigates how feature learning can occur      from the simple optimization process, which goes beyond the neural tangent      kernel approach. Results show that neural networks with appropriate      initialization exhibit interesting feature learning dynamics, learning      features incrementally. Additionally, deeper networks can learn functions      that are not representable by shallower networks.</li>\n<li><strong>Robust      nonconvex optimization</strong>. Many modern machine learning techniques rely      on nonconvex optimization, which often fails when the input data is      adversarially perturbed. Research in this project designed several      algorithms that enable robust optimization to be efficient and easily implementable      in the nonconvex setting.</li>\n<li><strong>Understanding      special phenomena in deep neural network training</strong>. The training      process of deep neural networks exhibits several surprising phenomena that      were not observed in more traditional approaches. This project provided      explanations for why many heuristics used in practice work (such as      masking, mix-up, and label-smoothing). The results also improved our      understanding of phenomena such as mode connectivity, monotonic linear      interpolation, and edge-of-stability.</li>\n<li><strong>Early      results for transformers and large language models</strong>. The theoretical      understanding of these new complex models has been sparse. The project      offers some of the early results in understanding why these models learn      useful features for downstream tasks.</li>\n</ol>\n<p><strong>Broader impact</strong>: The project supported PhD students and post-docs who have contributed to machine learning in both academia and industry. It also supported undergraduate researchers (some of the results mentioned above were primarily achieved by undergraduate students) who are now PhD students in the theory of machine learning.</p><br>\n<p>\n Last Modified: 08/06/2024<br>\nModified by: Rong&nbsp;Ge</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project focuses on the theoretical understanding of optimization for deep neural networks. Research has been performed in several closely related directions:\n\nTraining      dynamics of feature learning within neural networks. The ability to automatically      learn features from data has long been regarded as a main advantage of      deep learning. This project investigates how feature learning can occur      from the simple optimization process, which goes beyond the neural tangent      kernel approach. Results show that neural networks with appropriate      initialization exhibit interesting feature learning dynamics, learning      features incrementally. Additionally, deeper networks can learn functions      that are not representable by shallower networks.\nRobust      nonconvex optimization. Many modern machine learning techniques rely      on nonconvex optimization, which often fails when the input data is      adversarially perturbed. Research in this project designed several      algorithms that enable robust optimization to be efficient and easily implementable      in the nonconvex setting.\nUnderstanding      special phenomena in deep neural network training. The training      process of deep neural networks exhibits several surprising phenomena that      were not observed in more traditional approaches. This project provided      explanations for why many heuristics used in practice work (such as      masking, mix-up, and label-smoothing). The results also improved our      understanding of phenomena such as mode connectivity, monotonic linear      interpolation, and edge-of-stability.\nEarly      results for transformers and large language models. The theoretical      understanding of these new complex models has been sparse. The project      offers some of the early results in understanding why these models learn      useful features for downstream tasks.\n\n\n\nBroader impact: The project supported PhD students and post-docs who have contributed to machine learning in both academia and industry. It also supported undergraduate researchers (some of the results mentioned above were primarily achieved by undergraduate students) who are now PhD students in the theory of machine learning.\t\t\t\t\tLast Modified: 08/06/2024\n\n\t\t\t\t\tSubmitted by: RongGe\n"
 }
}