{
 "awd_id": "1938167",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Toward Interpretation of Pairwise Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2019-08-15",
 "awd_max_amd_letter_date": "2019-08-15",
 "awd_abstract_narration": "To exhibit machine intelligence, it is critical for the system to not only make intelligent decisions, but also be able to explain how it arrives at those decisions. Explainable machine learning is an emerging research area aiming to develop new or modified machine learning techniques that will produce more explainable models. This project will develop novel methods to interpret the predictions of a class of learning models known as \"pairwise learning models,\" which predicts relationships between instances rather than specific properties of an individual instance. For example, a consumer might want to know why the system recommends product A as similar to product B. The results of this research may benefit many real-world applications that are involved in pairwise learning, such as face recognition, visual tracking, information retrieval and bioinformatics. The development of the proposed approaches will contribute to the exploration of explainable machine learning as well as machine learning in general. \r\n\r\nTwo exploratory research tasks are carried out to generate explanations on both individual predictions (local interpretations) and the entire model behaviors (global interpretation). The proposed local interpretation method adapts the concept of Shapley-value to explain prediction decisions about an arbitrary pair of input instances. The proposed global explanation method is a Bayesian non-parametric pairwise interpretation method with the elastic nets, which will explain feature importance across a population. Such global interpretability can facilitate the understanding of the sensitivity levels of a target pairwise learning model to specific input dimensions.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aidong",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aidong Zhang",
   "pi_email_addr": "aidong@virginia.edu",
   "nsf_id": "000341960",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia Main Campus",
  "perf_str_addr": "85 Engineer's Way, Rice 509",
  "perf_city_name": "Charlottesville",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229044259",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This EAGER project tackles a challenging issue on interpretability of pairwise learning models which has been realized as a critical step toward our ultimate goal of machine intelligence. Two exploratory research tasks are carried out to generate explanations on both individual predictions and the entire model behaviors: both local and global interpretation strategies are designed to interpret individual predictions and the pairwise model behaviors. Specifically, we designed a novel adaptive Shapley-value-based interpretation method, based on which a vector of importance scores associated with the underlying features of a testing instance pair can be adaptively calculated with the consideration of feature correlations, and these scores can be used to indicate which features make key contributions to the final prediction. Considering that Shapley-value-based methods are usually computationally challenging, we further designed a novel robust approximation interpretation method for pairwise models. This method is not only much more efficient but also robust to data noise. We also developed several differentially private pairwise learning algorithms for both online and offline settings. For the online setting, we introduced a differentially private algorithm for strongly convex loss functions. Then, we extended this algorithm to general convex loss functions and give another differentially private algorithm. For the offline setting, we developed two differentially private algorithms for strongly and general convex loss functions, respectively. These algorithms can not only learn the model effectively from the data but also provide strong privacy protection guarantee for sensitive information in the training set. In addition, we designed a global interpretation method for patient similarity learning. Based on this global interpretation method, we can identify a minimal sufficient subset of data features that are sufficient in themselves to justify the global predictions made by the well-trained patient similarity model. The identified minimal sufficient feature subset can help us to better understand the overall behaviors of the learned model across different subpopulations. The global explanation method can explain feature importance across a population. Such global interpretability can facilitate the understanding of the sensitivity levels of a target pairwise learning model to specific input dimensions. The research results in this project will help us move one step forward toward achieving explainable machine learning.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/10/2022<br>\n\t\t\t\t\tModified by: Aidong&nbsp;Zhang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis EAGER project tackles a challenging issue on interpretability of pairwise learning models which has been realized as a critical step toward our ultimate goal of machine intelligence. Two exploratory research tasks are carried out to generate explanations on both individual predictions and the entire model behaviors: both local and global interpretation strategies are designed to interpret individual predictions and the pairwise model behaviors. Specifically, we designed a novel adaptive Shapley-value-based interpretation method, based on which a vector of importance scores associated with the underlying features of a testing instance pair can be adaptively calculated with the consideration of feature correlations, and these scores can be used to indicate which features make key contributions to the final prediction. Considering that Shapley-value-based methods are usually computationally challenging, we further designed a novel robust approximation interpretation method for pairwise models. This method is not only much more efficient but also robust to data noise. We also developed several differentially private pairwise learning algorithms for both online and offline settings. For the online setting, we introduced a differentially private algorithm for strongly convex loss functions. Then, we extended this algorithm to general convex loss functions and give another differentially private algorithm. For the offline setting, we developed two differentially private algorithms for strongly and general convex loss functions, respectively. These algorithms can not only learn the model effectively from the data but also provide strong privacy protection guarantee for sensitive information in the training set. In addition, we designed a global interpretation method for patient similarity learning. Based on this global interpretation method, we can identify a minimal sufficient subset of data features that are sufficient in themselves to justify the global predictions made by the well-trained patient similarity model. The identified minimal sufficient feature subset can help us to better understand the overall behaviors of the learned model across different subpopulations. The global explanation method can explain feature importance across a population. Such global interpretability can facilitate the understanding of the sensitivity levels of a target pairwise learning model to specific input dimensions. The research results in this project will help us move one step forward toward achieving explainable machine learning.\n\n\t\t\t\t\tLast Modified: 12/10/2022\n\n\t\t\t\t\tSubmitted by: Aidong Zhang"
 }
}