{
 "awd_id": "1909423",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NSF-BSF: CIF: Small: Self-adapting Code Generation in Rate-distortion Theory, Machine Learning, and Channel Coding",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 499869.0,
 "awd_amount": 499869.0,
 "awd_min_amd_letter_date": "2019-06-27",
 "awd_max_amd_letter_date": "2019-06-27",
 "awd_abstract_narration": "This project builds on the investigators' early information-theoretic results, establishing a mechanism called 'natural type selection' for source coding, which adapts a randomly generated code and is shown to be asymptotically optimal. Fundamental expansions of this framework will be pursued to develop universally applicable methodologies, and thereby yield contributions to information theory itself alongside powerful learning techniques in other application fields, including wireless communications, content delivery, social media, artificial intelligence, and others. From the educational perspective, the project offers a training opportunity for graduate students to experience, first-hand, an international and interdisciplinary research collaboration, which combines theoretical depth with practical impact. It further offers opportunities for extensive curriculum enrichment, and to produce accomplished researchers and practitioners with capacities and skills that are in high demand.\r\n\r\nThis project will develop novel approaches to learning, which employ universal self-adapting mechanisms for random code generation, designed to asymptotically achieve optimality for unknown source distributions. Research will be pursued, in terms of both theoretical analysis of performance bounds and powerful optimization approaches, along three main thrusts: i) Extension of the natural type selection framework to encompass continuous spaces and sources with memory, leveraging the concept of \"parametric type\" for continuous alphabets, which would expand applicability to virtually all practical scenarios of interest. ii) Applications in machine learning, where supervised learning (e.g., classification, regression) is reformulated as the rate-distortion problem of seeking the minimal amount of information to be learned from a source such that a desired output at the prescribed fidelity can be read from a random codebook; and, on the unsupervised learning side, where the \"information bottleneck\" method is reformulated universally in a self-adapting codebook generation setting. Both will leverage the optimization framework of deterministic annealing. iii) Applications in communications where stochastic mechanisms are developed for optimal channel input adaptation, including an important extension to multi-user communications which requires the development of a \"distributed natural type selection\" framework.  This project is a collaborative effort between researchers in the US and Israel, with funding for Israeli researchers provided by the Bi-National Science Foundation (BSF).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kenneth",
   "pi_last_name": "Rose",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kenneth Rose",
   "pi_email_addr": "rose@ece.ucsb.edu",
   "nsf_id": "000486690",
   "pi_start_date": "2019-06-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California Santa Barbara",
  "perf_str_addr": "ECE Department",
  "perf_city_name": "Santa Barbara",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931069560",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "014Z",
   "pgm_ref_txt": "NSF and US-Israel Binational Science Fou"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499869.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project's premise was that considerable advances can be achieved in fields revolving around information processing, by the development of self-adapting stochastic mechanisms that naturally learn from examples, and are guaranteed to ultimately achieve optimal performance. The project built on the investigators' information-theoretic results, establishing a mechanism called \"natural type selection\" (NTS) for source coding, which adapts a randomly generated code and is shown to be asymptotically optimal. Fundamental expansions of this framework were pursued to develop universally applicable methodologies, and thereby yield, beside contributions to information theory itself, new insights and powerful learning techniques that can be applied in a variety of application fields. Applications pursued within the project were in machine learning, source coding and communications, all fields of notable importance to society and to leading high-tech industry sectors, including wireless communications, content delivery, social media, artificial intelligence, and more.</p>\n<p>The overarching project objective was the development of novel approaches to learning, which employ universal self-adapting mechanisms for random code generation, designed to asymptotically achieve optimality for unknown source distributions. Research was pursued, in terms of both theoretical analysis of performance bounds and powerful optimization approaches, along three main thrusts: i) Development of a practically effective NTS framework in terms of algorithmic complexity and ability to encompass continuous spaces and sources with memory; ii) Applications of NTS in machine learning in terms of both fundamental performance limits on, and development of new universal self adapting stochastic mechanisms for, supervised and unsupervised learning; and iii) Applications of NTS in channel coding, with the development of stochastic mechanisms for optimal channel input adaptation, extension to continuous alphabets and multi-user communications. Main outcomes of the project include:</p>\n<p>i) Theoretical foundation:</p>\n<p>A main outcome is the establishment of the asymptotic optimality of the Natural Type Selection (NTS) approach to stochastic code generation from available source examples, in the sense that the optimal performance promised by rate-distortion bounds is attained by self adaptation without prior knowledge of the source distribution. Importantly, and specifically, optimality is guaranteed for a tractable variant of NTS where the string length is the last parameter to be increased. Additionally, convergence rates and performance guarantees have been established for practical (finite) values of the various parameters. These complement the asymptotic performance guarantees. The theoretical results were then extended to continuous alphabet sources, and to sources with memory, and later specialized to the most important class of Markov sources, which encompasses most sources encountered in practice, such as video or audio signals. In the channel coding thrust, a main outcome was the establishment of an NTS framework for channel input adaptation, as well as the analysis of its theoretical performance guarantees. Another main theoretical outcome was the rate-distortion based&nbsp;reformulation of the fundamental supervised learning problem as consisting of two steps: a) extracting the minimum amount of relevant information, from source examples, necessary to enable b) mapping the extracted information to a codebook entry providing the desired output at the required accuracy. The avoidance of local optima in the deep neural network used for information extraction in (a) was achieved by the deterministic annealing framework, and optimality of the codebook in (b) was established within the NTS framework.&nbsp;</p>\n<p>ii) Practical and algorithmic outcomes:&nbsp;</p>\n<p><span>Algorithms developed and tested include: A tractable NTS algorithm for stochastic generation of random codebooks in lossy coding discrete-alphabet settings, whose drastically reduced complexity opens the door to its deployment in practical applications;&nbsp;</span>A major extension of the algorithm to handle discrete sources with memory; A specialized NTS algorithm tailored to Markov sources (the most prevalent type of sources), which exploits the Markov property to further reduce substantially the computational complexity; An NTS algorithm extending applicability to continuous alphabet sources with memory; Speed of convergence and performance analysis of the NTS algorithm at finite parameter regimes, which are unavoidable in practice, and establishment of convergence to the optimal performance achievable at finite word-length and/or system memory depth, as well as the rate of convergence in terms of accuracy; Supervised learning algorithms combining NTS and deterministic annealing for codebook and deep neural network design, including establishment of experimental and theoretical evidence for the approach's substantial performance gains in terms of learning system accuracy and reduced complexity, wherein these experimental results provide evidence of practical performance gains, consistent with the asymptotically optimal performance achieving the rate-distortion bound, as has been theoretically established.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2023<br>\n\t\t\t\t\tModified by: Kenneth&nbsp;Rose</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project's premise was that considerable advances can be achieved in fields revolving around information processing, by the development of self-adapting stochastic mechanisms that naturally learn from examples, and are guaranteed to ultimately achieve optimal performance. The project built on the investigators' information-theoretic results, establishing a mechanism called \"natural type selection\" (NTS) for source coding, which adapts a randomly generated code and is shown to be asymptotically optimal. Fundamental expansions of this framework were pursued to develop universally applicable methodologies, and thereby yield, beside contributions to information theory itself, new insights and powerful learning techniques that can be applied in a variety of application fields. Applications pursued within the project were in machine learning, source coding and communications, all fields of notable importance to society and to leading high-tech industry sectors, including wireless communications, content delivery, social media, artificial intelligence, and more.\n\nThe overarching project objective was the development of novel approaches to learning, which employ universal self-adapting mechanisms for random code generation, designed to asymptotically achieve optimality for unknown source distributions. Research was pursued, in terms of both theoretical analysis of performance bounds and powerful optimization approaches, along three main thrusts: i) Development of a practically effective NTS framework in terms of algorithmic complexity and ability to encompass continuous spaces and sources with memory; ii) Applications of NTS in machine learning in terms of both fundamental performance limits on, and development of new universal self adapting stochastic mechanisms for, supervised and unsupervised learning; and iii) Applications of NTS in channel coding, with the development of stochastic mechanisms for optimal channel input adaptation, extension to continuous alphabets and multi-user communications. Main outcomes of the project include:\n\ni) Theoretical foundation:\n\nA main outcome is the establishment of the asymptotic optimality of the Natural Type Selection (NTS) approach to stochastic code generation from available source examples, in the sense that the optimal performance promised by rate-distortion bounds is attained by self adaptation without prior knowledge of the source distribution. Importantly, and specifically, optimality is guaranteed for a tractable variant of NTS where the string length is the last parameter to be increased. Additionally, convergence rates and performance guarantees have been established for practical (finite) values of the various parameters. These complement the asymptotic performance guarantees. The theoretical results were then extended to continuous alphabet sources, and to sources with memory, and later specialized to the most important class of Markov sources, which encompasses most sources encountered in practice, such as video or audio signals. In the channel coding thrust, a main outcome was the establishment of an NTS framework for channel input adaptation, as well as the analysis of its theoretical performance guarantees. Another main theoretical outcome was the rate-distortion based reformulation of the fundamental supervised learning problem as consisting of two steps: a) extracting the minimum amount of relevant information, from source examples, necessary to enable b) mapping the extracted information to a codebook entry providing the desired output at the required accuracy. The avoidance of local optima in the deep neural network used for information extraction in (a) was achieved by the deterministic annealing framework, and optimality of the codebook in (b) was established within the NTS framework. \n\nii) Practical and algorithmic outcomes: \n\nAlgorithms developed and tested include: A tractable NTS algorithm for stochastic generation of random codebooks in lossy coding discrete-alphabet settings, whose drastically reduced complexity opens the door to its deployment in practical applications; A major extension of the algorithm to handle discrete sources with memory; A specialized NTS algorithm tailored to Markov sources (the most prevalent type of sources), which exploits the Markov property to further reduce substantially the computational complexity; An NTS algorithm extending applicability to continuous alphabet sources with memory; Speed of convergence and performance analysis of the NTS algorithm at finite parameter regimes, which are unavoidable in practice, and establishment of convergence to the optimal performance achievable at finite word-length and/or system memory depth, as well as the rate of convergence in terms of accuracy; Supervised learning algorithms combining NTS and deterministic annealing for codebook and deep neural network design, including establishment of experimental and theoretical evidence for the approach's substantial performance gains in terms of learning system accuracy and reduced complexity, wherein these experimental results provide evidence of practical performance gains, consistent with the asymptotically optimal performance achieving the rate-distortion bound, as has been theoretically established.\n\n\t\t\t\t\tLast Modified: 10/29/2023\n\n\t\t\t\t\tSubmitted by: Kenneth Rose"
 }
}