{
 "awd_id": "1901378",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium: Collaborative Research: ADMM-NN: A Unified Software/Hardware Framework of DNN Computation and Storage Reduction Using ADMM",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2019-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 750000.0,
 "awd_min_amd_letter_date": "2019-05-31",
 "awd_max_amd_letter_date": "2020-07-28",
 "awd_abstract_narration": "Deep neural networks (DNNs) have been employed in wide application domains thanks to their extraordinary performance. Hardware implementations of DNNs are of critical importance for the ubiquitous embedded and Internet of Things (IoT) devices, which call for high performance in energy and resource constrained systems. This project aims to address the challenges when mapping complicated DNN models into hardware for energy-efficient and performance-driven implementations. The proposed techniques will promote wider adoptions of deep learning into both high-performance and low-power computing systems. The project will also enhance economic opportunities and have significant societal benefits via solutions that support broader adoption of intelligent systems for big data analytics, weather modeling and forecasting, disease diagnosis and drug delivery, and medical image processing. The research advances will be incorporated into coursework taught by the investigators. Activities on engaging underrepresented, undergraduate, and K12 students will be designed in collaboration with the Northeastern University Center of STEM Education and University of Southern California's Viterbi Center for Engineering Diversity. All software code from the project will be released via GitHub and educational modules and tutorials will be make available to the research community, industry, and government.\r\n \r\nExploring the inherent model redundancy of DNNs, this project will develop an algorithm-hardware co-optimization framework for greatly reducing DNN computation and storage requirements by leveraging ADMM (alternating direction method of multipliers), a powerful optimization technique. This project first solves the challenge in the application of ADMM due to the non-convex objective function in DNN training, and thereby lack of guarantees on solution feasibility, solution quality, and low runtime. Therefore, an integrated framework of ADMM regularization and masked mapping and retraining will be developed and further improvements on solution quality, performance-driven computation/storage reduction, and hardware feasibility will be investigated. Next, the project proposes a unified weight and intermediate result pruning and quantization technique that explores all four redundancy sources of DNN models. Due to the impact on energy efficiency of hardware implementations of DNNs, nearly all DNN models, or at least the most computationally intensive convolutional layers can be then placed on a single chip. Finally, design-time parameterization and algorithm-hardware co-design solutions will be developed for efficient utilization of available hardware resources, achieving high performance, energy efficiency, and adaptation capability. Extensive experimentation and evaluation will be performed to validate and tune the proposed technique with prototype systems using FPGA devices.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xue",
   "pi_last_name": "Lin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xue Lin",
   "pi_email_addr": "xuelin@coe.neu.edu",
   "nsf_id": "000724544",
   "pi_start_date": "2019-05-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Miriam",
   "pi_last_name": "Leeser",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Miriam E Leeser",
   "pi_email_addr": "mel@ece.neu.edu",
   "nsf_id": "000194950",
   "pi_start_date": "2019-05-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 248943.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 501057.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 23\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>Hardware implementation of deep neural networks (DNNs) with emphasis on performance and energy efficiency has been the focus of extensive investigations. Many applications require large DNNs to achieve high quality results. When such large DNNs are mapped to hardware as an inference engine, the resulting hardware suffers from expensive hardware computations and frequent accesses to off-chip DRAM memory, which in turn result in significant performance and energy overheads. To overcome this hurdle, previous research has proposed model compression techniques, with two separate and distinct approaches: weight pruning and quantization. We start from these prior work, but make the additional observation that the sources of redundancy in DNNs are more than simply the weight count or the bit-level representation. In fact there are two additional sources of redundancy in the number of intermediate results of a DNN and in the bit-level representation of the said intermediate results. Next we develop a rigorous and holistic optimization framework to achieve ultra-high DNN compression exploiting redundancies in the counts and bit-level representations of both weights and intermediate results. In addition, we design mapping approaches that can realize a compressed DNN on a target hardware platform in a resource-aware and performance-driven manner.</span></p>\n<p><span>&nbsp;</span>This project develops ADMM-NN, an algorithm-hardware co-optimization framework for greatly reducing DNN computation and storage requirements by incorporating Alternating Direction Method of Multipliers (ADMM) and utilizing all redundancy sources in a DNN. Our research can be divided into the following key areas: (i) development of a framework of joint pruning and quantization of DNN weights, (ii) development of a unified framework for utilizing all sources of redundancy, enabling joint pruning and quantization of both weights and intermediate results, and (iii) a co-design framework for mapping DNNs to FPGA hardware with focus on resource utilization, adaptivity, performance, and energy efficiency. We show that ADMM-NN can achieve the highest degree of model compression on representative DNNs.</p>\n<div class=\"page\" title=\"Page 23\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>This research results in ultra-high reduction of computational and storage requirements of a DNN inference engine without much loss in its accuracy. This enables efficient mapping of the compressed DNN to hardware, resulting in performance gain and energy efficiency. Having a light foot-print DNN inference engine with high performance and energy efficiency enables wide use in many energy-constrained platforms, including \"edge devices\". The research advances state-of-the-art in: it (i) enhances current ADMM technique to non-convex object function (in DNN training) by integrating ADMM regularization with masked mapping/retraining, thereby guaranteeing feasibility and improving quality; (ii) incorporates ADMM with Bregman updates to capture the nonlinear relationship between weights and intermediate results with reduced solution complexity; (iii) designs automatic hyperparameter determination using deep reinforcement learning; and (iv) combines load-aware scheduler, approximate computing, and memory control techniques to provide adaptativity to different sparsity and quantization schemes.</span></p>\n</div>\n</div>\n</div>\n<p><span><br /></span></p>\n</div>\n</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/29/2024<br>\nModified by: Xue&nbsp;Lin</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nHardware implementation of deep neural networks (DNNs) with emphasis on performance and energy efficiency has been the focus of extensive investigations. Many applications require large DNNs to achieve high quality results. When such large DNNs are mapped to hardware as an inference engine, the resulting hardware suffers from expensive hardware computations and frequent accesses to off-chip DRAM memory, which in turn result in significant performance and energy overheads. To overcome this hurdle, previous research has proposed model compression techniques, with two separate and distinct approaches: weight pruning and quantization. We start from these prior work, but make the additional observation that the sources of redundancy in DNNs are more than simply the weight count or the bit-level representation. In fact there are two additional sources of redundancy in the number of intermediate results of a DNN and in the bit-level representation of the said intermediate results. Next we develop a rigorous and holistic optimization framework to achieve ultra-high DNN compression exploiting redundancies in the counts and bit-level representations of both weights and intermediate results. In addition, we design mapping approaches that can realize a compressed DNN on a target hardware platform in a resource-aware and performance-driven manner.\n\n\nThis project develops ADMM-NN, an algorithm-hardware co-optimization framework for greatly reducing DNN computation and storage requirements by incorporating Alternating Direction Method of Multipliers (ADMM) and utilizing all redundancy sources in a DNN. Our research can be divided into the following key areas: (i) development of a framework of joint pruning and quantization of DNN weights, (ii) development of a unified framework for utilizing all sources of redundancy, enabling joint pruning and quantization of both weights and intermediate results, and (iii) a co-design framework for mapping DNNs to FPGA hardware with focus on resource utilization, adaptivity, performance, and energy efficiency. We show that ADMM-NN can achieve the highest degree of model compression on representative DNNs.\n\n\n\n\n\nThis research results in ultra-high reduction of computational and storage requirements of a DNN inference engine without much loss in its accuracy. This enables efficient mapping of the compressed DNN to hardware, resulting in performance gain and energy efficiency. Having a light foot-print DNN inference engine with high performance and energy efficiency enables wide use in many energy-constrained platforms, including \"edge devices\". The research advances state-of-the-art in: it (i) enhances current ADMM technique to non-convex object function (in DNN training) by integrating ADMM regularization with masked mapping/retraining, thereby guaranteeing feasibility and improving quality; (ii) incorporates ADMM with Bregman updates to capture the nonlinear relationship between weights and intermediate results with reduced solution complexity; (iii) designs automatic hyperparameter determination using deep reinforcement learning; and (iv) combines load-aware scheduler, approximate computing, and memory control techniques to provide adaptativity to different sparsity and quantization schemes.\n\n\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 09/29/2024\n\n\t\t\t\t\tSubmitted by: XueLin\n"
 }
}