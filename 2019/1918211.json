{
 "awd_id": "1918211",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FMitF: Track I: Formal Methods for Explainable Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922585",
 "po_email": "pprabhak@nsf.gov",
 "po_sign_block_name": "Pavithra Prabhakar",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 750000.0,
 "awd_min_amd_letter_date": "2019-07-12",
 "awd_max_amd_letter_date": "2019-07-12",
 "awd_abstract_narration": "Artificial intelligence, in the form of machine learning (ML), is rapidly transforming the world. Today, ML is responsible for an ever-growing spectrum of sensitive decisions from loan decisions, to diagnosing diseases, to autonomous driving. With ML spreading across many industries, the issue of explainability, i.e., explaining the decisions of opaque models in ML, has taken center stage. Despite much interest and progress in the explainability question, the research in the area is still nascent and does not capture the full spectrum of ML models used in practice and the forms of explanation that are of interest to users and subjects of those models. This project explores a range of explanation tasks can be enabled by (and benefit from) program-synthesis technology as developed by the formal-methods community. The project's impact is to lay logical foundations for explainability of AI decisions, and thus has the potential to ensure transparency in our increasingly autonomous world. \r\n\r\nThe project's novelty is to use program synthesis to automatically construct simple, coherent, human-readable explanations, in the form of high-level programs, of a ML model or its decisions.  The project investigates techniques for synthesizing actionable explanations of the decisions made by a traditional (non-sequence) ML models as well as recurrent models. From a technical viewpoint, this project develops new program-synthesis techniques that leverage optimization technologies and apply them to the unique problem setup presented by explainable machine learning. Second, the project develops algorithms for automata learning, synthesis of regular expressions, and synthesis of temporal-logic formulae and uses them to explain the predictions of sequence models.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Loris",
   "pi_last_name": "DAntoni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Loris DAntoni",
   "pi_email_addr": "ldantoni@ucsd.edu",
   "nsf_id": "000701818",
   "pi_start_date": "2019-07-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vikas",
   "pi_last_name": "Singh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vikas Singh",
   "pi_email_addr": "VSINGH@CS.WISC.EDU",
   "nsf_id": "000515836",
   "pi_start_date": "2019-07-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Aws",
   "pi_last_name": "Albarghouthi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aws Albarghouthi",
   "pi_email_addr": "aws@cs.wisc.edu",
   "nsf_id": "000682630",
   "pi_start_date": "2019-07-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin - Madison",
  "perf_str_addr": "1210 West Dayton Street, Office",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061685",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "094Y00",
   "pgm_ele_name": "FMitF: Formal Methods in the F"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "8206",
   "pgm_ref_txt": "Formal Methods and Verification"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 750000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-d646f902-7fff-3b1e-dfea-8b2cb9fe863f\"> </span></p>\n<p dir=\"ltr\"><span>The project focused on improving the explainability of machine learning (ML) models, which are increasingly being used in high-stakes decision-making scenarios. The research leveraged program synthesis techniques to generate actionable and understandable explanations for various types of models, such as decision trees, recurrent neural networks (RNNs), and deep neural networks, and introduced tools that help build trust in ML models by proving their robustness and fairness.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Key Accomplishments</span></p>\n<p dir=\"ltr\"><span>Antidote, Robustness Verification: We developed a tool called Antidote, designed to verify that the predictions made by decision tree models are resistant to data poisoning attacks. Data poisoning is a tactic where attackers introduce malicious data into the training set to influence the model's predictions. Antidote uses abstract interpretation techniques to ensure that a given model's prediction would remain unchanged even if the training set had been tampered with. The tool's effectiveness was demonstrated on popular datasets, providing a new way to secure models in high-stakes applications.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Actionable Explanations: Machine learning models must not only make accurate predictions but also offer users a way to understand and modify their decisions. Our team used program synthesis techniques to create actionable feedback for cases where individuals might want to change a model&rsquo;s decision&mdash;for example, how someone could modify their profile to receive a loan approval. This method allows users to see a step-by-step plan for improving their classification, offering transparency and guidance in scenarios where ML models are often opaque.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Interpretable Saliency Maps: Our team developed new methods for generating interpretable saliency maps from convolutional neural networks (CNNs), which are widely used in image classification tasks. Saliency maps show which parts of an image are most relevant to the model&rsquo;s decision. Our work provided more localized and meaningful explanations without compromising the model's accuracy, allowing users to better understand what features the model relies on when making its decisions.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Programmable Robustness for LSTMs: We extended our work on robust neural networks to LSTMs, showing how specific string transformations could be used to protect models from adversarial inputs, such as misspelled words or subtle changes in text data. This advancement is crucial for building reliable language models, as it helps ensure that models do not make erratic predictions based on small, irrelevant changes in input data.</span></p>\n<p dir=\"ltr\"><span>BagFlip: Defending Against Data Poisoning: Our BagFlip tool introduced a new way to protect ML models from data poisoning and backdoor attacks. Unlike many existing defenses, BagFlip is model-agnostic and can be applied to a wide range of scenarios, making it an important step forward in securing machine learning applications.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Broader Impacts</span></p>\n<p dir=\"ltr\"><span>This project addresses key societal challenges by improving the reliability, transparency, and fairness of machine learning models. The tools and techniques developed have direct applications in areas like healthcare, finance, and hiring, where ML models are often used to make critical decisions. The open-source nature of our work ensures that these advances will be accessible to a broad range of practitioners, from academic researchers to industry professionals. Furthermore, the project contributes to ongoing debates around algorithmic fairness and accountability, providing concrete solutions to improve trust in automated systems.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/01/2024<br>\nModified by: Loris&nbsp;Dantoni</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThe project focused on improving the explainability of machine learning (ML) models, which are increasingly being used in high-stakes decision-making scenarios. The research leveraged program synthesis techniques to generate actionable and understandable explanations for various types of models, such as decision trees, recurrent neural networks (RNNs), and deep neural networks, and introduced tools that help build trust in ML models by proving their robustness and fairness.\n\n\n\n\n\nKey Accomplishments\n\n\nAntidote, Robustness Verification: We developed a tool called Antidote, designed to verify that the predictions made by decision tree models are resistant to data poisoning attacks. Data poisoning is a tactic where attackers introduce malicious data into the training set to influence the model's predictions. Antidote uses abstract interpretation techniques to ensure that a given model's prediction would remain unchanged even if the training set had been tampered with. The tool's effectiveness was demonstrated on popular datasets, providing a new way to secure models in high-stakes applications.\n\n\n\n\n\nActionable Explanations: Machine learning models must not only make accurate predictions but also offer users a way to understand and modify their decisions. Our team used program synthesis techniques to create actionable feedback for cases where individuals might want to change a models decisionfor example, how someone could modify their profile to receive a loan approval. This method allows users to see a step-by-step plan for improving their classification, offering transparency and guidance in scenarios where ML models are often opaque.\n\n\n\n\n\nInterpretable Saliency Maps: Our team developed new methods for generating interpretable saliency maps from convolutional neural networks (CNNs), which are widely used in image classification tasks. Saliency maps show which parts of an image are most relevant to the models decision. Our work provided more localized and meaningful explanations without compromising the model's accuracy, allowing users to better understand what features the model relies on when making its decisions.\n\n\n\n\n\nProgrammable Robustness for LSTMs: We extended our work on robust neural networks to LSTMs, showing how specific string transformations could be used to protect models from adversarial inputs, such as misspelled words or subtle changes in text data. This advancement is crucial for building reliable language models, as it helps ensure that models do not make erratic predictions based on small, irrelevant changes in input data.\n\n\nBagFlip: Defending Against Data Poisoning: Our BagFlip tool introduced a new way to protect ML models from data poisoning and backdoor attacks. Unlike many existing defenses, BagFlip is model-agnostic and can be applied to a wide range of scenarios, making it an important step forward in securing machine learning applications.\n\n\n\n\n\nBroader Impacts\n\n\nThis project addresses key societal challenges by improving the reliability, transparency, and fairness of machine learning models. The tools and techniques developed have direct applications in areas like healthcare, finance, and hiring, where ML models are often used to make critical decisions. The open-source nature of our work ensures that these advances will be accessible to a broad range of practitioners, from academic researchers to industry professionals. Furthermore, the project contributes to ongoing debates around algorithmic fairness and accountability, providing concrete solutions to improve trust in automated systems.\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 11/01/2024\n\n\t\t\t\t\tSubmitted by: LorisDantoni\n"
 }
}