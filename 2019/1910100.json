{
 "awd_id": "1910100",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Collaborative Research: Rigorous Approaches for Scalable Privacy-preserving Deep Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 208000.0,
 "awd_amount": 208000.0,
 "awd_min_amd_letter_date": "2019-07-12",
 "awd_max_amd_letter_date": "2019-07-12",
 "awd_abstract_narration": "One of the most salient features of this time is the dissemination of massive amounts of personal and sensitive data. Despite their enormous societal benefits, the powerful tools of modern machine learning, especially deep learning, can pose real threats to personal privacy. For example, over the last few years, it has become evident that deep neural networks have a remarkable power in learning even the finest details from large complex data sets. With such powerful tools, the need for robust and rigorous guarantees for privacy protection has become more crucial. The last decade has witnessed the rise of a sound mathematical theory, known as differential privacy, that enables designing data-analysis algorithms with rigorous privacy guarantees for their input data sets. Despite the noticeable success of this theory, existing tools from differential privacy are severely limited in offering acceptable utility guarantees when dealing with complex models like those arising in deep learning. This project will address those limitations by offering new principled approaches for designing differentially-private deep-learning algorithms that can scale to industrial workloads. The project will also involve collaboration with industry, which will facilitate the evaluation of the developed algorithms on real-world datasets and the development of open-source software tools. The products of this project have the potential to transform the way massive sets of sensitive data are used in modern machine-learning systems, which will impact the way these systems are designed and implemented in practice. The activities of this project will also aim at promoting diversity in computing by recruiting women and members of underrepresented groups.\r\n\r\nThe investigators will develop a rigorous, multi-faceted design paradigm for scalable, practical, differentially private algorithms for modern machine learning. This paradigm is based on two general strategies: (i) exploiting realistic and useful properties of the data and the machine-learning models to circumvent existing limitations in the literature on differential privacy, and (ii) leveraging a limited amount of public data (that has no privacy constraints) to boost the utility of the algorithms. Based on these strategies, the project will pursue following directions: (1) developing a new, generic framework for utilizing public data in privacy-preserving machine learning, (2) designing improved iterative training algorithms that can bypass the standard use of the so-called \"composition theorem\" of differential privacy, and (3) designing new differentially private stochastic-gradient methods tuned specifically to non-convex and over-parameterized machine-learning problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bo",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bo Li",
   "pi_email_addr": "bol@uchicago.edu",
   "nsf_id": "000763478",
   "pi_start_date": "2019-07-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S. Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 208000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-3ed70c12-7fff-9b26-0ed4-3ba680bbebbf\">&nbsp;</span></p>\n<p dir=\"ltr\">Based on this project, we have published more than 20 papers at the top machine learning and security conferences. In particular, one of the papers, which provides the first comprehensive trustworthiness evaluation for large language models such as privacy and fairness, has won the Best Paper award at NeurIPS 2023.&nbsp;</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\">In addition, we have provided several privacy-preserving data generative frameworks for different domains, including autonomous driving, large language models, and image recognition tasks. We later provided a unified version of such privacy-preserving data generative model for multimodality data in the distributed setting, which has significantly improved the efficiency and has been adopted by industries from different domains such as medical analysis and finance.</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\">To further understand the tradeoff and underlying connections between privacy and other perspectives of machine learning models, we have conducted a series of works to explore the relationships among privacy, robustness, generation, and fairness. We have provided certified fairness and robustness frameworks, and we have represented these certifications as functions of the parameters in a differential private machine-learning pipeline, in both centralized and decentralized settings. Such analysis has provided theoretical foundations of the tradeoffs among different trustworthiness perspectives for machine learning models. In addition, our analysis provides practical principles for choosing different parameters and settings to balance these trustworthiness perspectives based on concrete use cases and scenarios.</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\">Given the fact that training a privacy-preserving machine learning model is expensive, we have also&nbsp;provided a series of approaches to improve the training efficiency of machine learning models with privacy guarantees. For instance, we have taken the model size into account to optimize the model trustworthiness together with model size pruning. Although some studies find that larger smoothed models would provide higher robustness certification, we have explored weakening such a tradeoff by training small models via optimized pruning. We have designed different quantitative criteria such as neuron Shapley, to evaluate the neuron weight/filter importance within DNNs, leading to effective unstructured/structured pruning strategies to improve the certified robustness of the pruned models. We have also proposed a series of novel training techniques for quantized models via gradient-based approximation and optimized sample-size to improve training efficiency while maintaining model performance. We have provided theoretical analysis on the impact of different model properties and sample complexity of our approaches. Our developed algorithms have been so far adopted in several privacy-preserving training frameworks including those on large language models.&nbsp;</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\">We have provided a sophisticated privacy analysis and evaluation on large language models considering models with different sizes, training epochs, and training life cycles. We provide interesting findings and fundamental understandings of privacy leakage and protection for large language models. We believe it will encourage the community to further explore the privacy vulnerabilities of foundation models and provide feasible protection approaches accordingly.&nbsp;</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\">We have provided several research talks in academia and industries and built wide collaborations. For instance, our developed platforms and toolboxes have been widely adopted by industries, such as Meta, Amazon, eBay, Microsoft, and Google. We have also provided several tutorials regarding privacy-preserving machine learning and data generation for the broad community.</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\">Through this project, there are three PhD students trained in machine learning, privacy, and data generation techniques, and one of them will graduate this year based on his research supported by this project.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/10/2024<br>\nModified by: Bo&nbsp;Li</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nBased on this project, we have published more than 20 papers at the top machine learning and security conferences. In particular, one of the papers, which provides the first comprehensive trustworthiness evaluation for large language models such as privacy and fairness, has won the Best Paper award at NeurIPS 2023.\n\n\n\n\n\nIn addition, we have provided several privacy-preserving data generative frameworks for different domains, including autonomous driving, large language models, and image recognition tasks. We later provided a unified version of such privacy-preserving data generative model for multimodality data in the distributed setting, which has significantly improved the efficiency and has been adopted by industries from different domains such as medical analysis and finance.\n\n\n\n\n\nTo further understand the tradeoff and underlying connections between privacy and other perspectives of machine learning models, we have conducted a series of works to explore the relationships among privacy, robustness, generation, and fairness. We have provided certified fairness and robustness frameworks, and we have represented these certifications as functions of the parameters in a differential private machine-learning pipeline, in both centralized and decentralized settings. Such analysis has provided theoretical foundations of the tradeoffs among different trustworthiness perspectives for machine learning models. In addition, our analysis provides practical principles for choosing different parameters and settings to balance these trustworthiness perspectives based on concrete use cases and scenarios.\n\n\n\n\n\nGiven the fact that training a privacy-preserving machine learning model is expensive, we have alsoprovided a series of approaches to improve the training efficiency of machine learning models with privacy guarantees. For instance, we have taken the model size into account to optimize the model trustworthiness together with model size pruning. Although some studies find that larger smoothed models would provide higher robustness certification, we have explored weakening such a tradeoff by training small models via optimized pruning. We have designed different quantitative criteria such as neuron Shapley, to evaluate the neuron weight/filter importance within DNNs, leading to effective unstructured/structured pruning strategies to improve the certified robustness of the pruned models. We have also proposed a series of novel training techniques for quantized models via gradient-based approximation and optimized sample-size to improve training efficiency while maintaining model performance. We have provided theoretical analysis on the impact of different model properties and sample complexity of our approaches. Our developed algorithms have been so far adopted in several privacy-preserving training frameworks including those on large language models.\n\n\n\n\n\nWe have provided a sophisticated privacy analysis and evaluation on large language models considering models with different sizes, training epochs, and training life cycles. We provide interesting findings and fundamental understandings of privacy leakage and protection for large language models. We believe it will encourage the community to further explore the privacy vulnerabilities of foundation models and provide feasible protection approaches accordingly.\n\n\n\n\n\nWe have provided several research talks in academia and industries and built wide collaborations. For instance, our developed platforms and toolboxes have been widely adopted by industries, such as Meta, Amazon, eBay, Microsoft, and Google. We have also provided several tutorials regarding privacy-preserving machine learning and data generation for the broad community.\n\n\n\n\n\nThrough this project, there are three PhD students trained in machine learning, privacy, and data generation techniques, and one of them will graduate this year based on his research supported by this project.\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 01/10/2024\n\n\t\t\t\t\tSubmitted by: BoLi\n"
 }
}