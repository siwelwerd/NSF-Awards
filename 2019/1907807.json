{
 "awd_id": "1907807",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: A data-driven computational model of dyadic rapport: Learning and transforming nonverbal behavior in shared virtual environments.",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2019-09-09",
 "awd_max_amd_letter_date": "2019-09-09",
 "awd_abstract_narration": "Social interaction is a complex and highly demanding task -- it can unfold in a harmonious and effortless way, yet sometimes also fail catastrophically. A critical determinant of success is whether or not the partners are able to establish rapport. Rapport impacts all communication contexts, from initial contact formation to private conflict resolution and business negotiations. While humans are sensitive to flaws in rapport, they frequently fail to identify the reasons or take counter measures. This project is motivated by the idea that new media technologies, such as social virtual reality (VR), can augment people's social-cognitive capacities in this regard and improve our communication skills in daily life interactions.  In relevant respects, machine capabilities can be superior to, and less biased than, human social perception. On the one hand, interactions taking place in VR allow the system to register behavioral details affecting rapport (such as movement, eye gaze, and facial expressions). Moreover, mobile sensor technologies can be seamlessly integrated into VR devices, such as headsets or controllers to measure the neurophysiological correlates of emotional, motivational, and attentional attunement. On the other hand computational power now allows us to run highly complex machine learning algorithms on standard personal computers. This project will leverage VR capture technologies, mobile neurophysiological sensing and deep learning methods to develop a bio-behavioral model of rapport. Based on this the project will develop and evaluate tools to monitor rapport in ongoing interactions and administer feedback to enable corrective actions that improve rapport. \r\n\r\nThe investigators will meet the two objectives. First, they will accumulate an annotated interaction database consisting of 150 dyads (pairs of subjects in conversation) performing three different interaction tasks, with an overall duration of 30 minutes. The database will include speech, movement, gaze, EEG measures of concurrent brain activity, and cardiovascular measures. The interaction protocols will be annotated for rapport by groups of observers, and the subjects themselves will evaluate interaction quality and outcomes. Second, they will develop and validate machine learning algorithms that identify bio-behavioral rapport signatures in the annotated multichannel database, and predict perceived rapport and physiological responses from nonverbal behavior. This development will lead to a bio-behavioral model of rapport, which provides the basis for social AI components, capable of monitoring and facilitating rapport in ongoing avatar interactions. Long term goals are to integrate these tools in communication media beyond social VR or in real life interactions, depending on more advanced, portable and unobtrusive sensing devices.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gary",
   "pi_last_name": "Bente",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Gary M Bente",
   "pi_email_addr": "gabente@msu.edu",
   "nsf_id": "000678750",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Reimers",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mark Reimers",
   "pi_email_addr": "reimersm@msu.edu",
   "nsf_id": "000521394",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vishnu",
   "pi_last_name": "Boddeti",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vishnu Boddeti",
   "pi_email_addr": "vishnu@msu.edu",
   "nsf_id": "000711404",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ralf",
   "pi_last_name": "Schmaelzle",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ralf Schmaelzle",
   "pi_email_addr": "schmaelz@msu.edu",
   "nsf_id": "000762186",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Michigan State University",
  "inst_street_address": "426 AUDITORIUM RD RM 2",
  "inst_street_address_2": "",
  "inst_city_name": "EAST LANSING",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "5173555040",
  "inst_zip_code": "488242600",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MI07",
  "org_lgl_bus_name": "MICHIGAN STATE UNIVERSITY",
  "org_prnt_uei_num": "VJKZC4D1JN36",
  "org_uei_num": "R28EKN92ZTZ9"
 },
 "perf_inst": {
  "perf_inst_name": "Michigan State University",
  "perf_str_addr": "404 Wilson Road",
  "perf_city_name": "East Lansing",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "488246402",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MI07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"TitlePage\"><a name=\"_Hlk152504576\">A substantial share of future human interactions will take place in the metaverse. Avatar-based communication of the future will provide unprecedented possibilities for strategic cue modification in the digital streams of human behavior - leading to many opportunities and threats to enhance or manipulate communication. Research has demonstrated that interpersonal coordination and movement synchrony are key factors in social influence, rapport, and collaboration, but our understanding of their underlying mechanisms remains incomplete. The project is rooted in the fundamental premise that a thorough comprehension of conversational synchrony necessitates the availability of expansive datasets that encapsulate the multifaceted nature of human conversations and encompassing their dynamic evolution across diverse modalities, social contexts, and timeframes. To address this imperative, this project furnishes an open-access interaction database distinguished by its substantial size, varied situational scenarios, and comprehensive behavioral scope.</a></p>\n<p class=\"TitlePage\">The introduced database comprises data from 132 short dyadic interactions, recorded in two different social settings: a get-to-know situation and a managerial conflict scenario. The recordings include full-body motion capture data, speech recordings, gaze tracking, as well as neurophysiological measures of arousal and attention. Furthermore, the dataset provides subjective ratings of the interactors and of independent observers regarding basic person characteristics as well as relational quality. Figure 1 serves as an illustrative guide, offering a visual depiction of the database components and the production line.</p>\n<p class=\"TitlePage\">A crucial aspect of the current methodology is its reliance on a full-body motion capture system that issues comprehensive 6-degree-of-freedom movement data for all joints of the human body with high temporal and spatial resolution and accuracy. This level of detail facilitates the in-depth analysis of various nonverbal subsystems, such as body postures, hand gestures, or head movements, and also enables post-hoc aggregation in terms of global activity measures. Importantly, the current database extends on existing motion capture datasets that are limited in the scope of settings and sample sizes. The approach also goes a step further compared to existing datasets by incorporating both verbal and visual communication (gaze) channels. To capture these aspects, unobtrusive eye-tracking glasses equipped with a scene camera and microphone (PupilLabs Invisible) were employed. A customized machine learning routine was used to identify the partners&rsquo; face in the video recordings of the scene camera and to automatically determine directed and averted gaze relative to this face. Speech protocols of both interactors were extracted and automatically transcribed applying python programs for diarization and speech to text conversion. Last not least, the dataset also comprises neurophysiological data (heart rate and EEG). All bio-behavioral protocols were synchronized using a common audio sync signal. This process data is complemented by subjective evaluations of the interaction partners and independent observers including social impressions and judgements of relational quality and interaction outcome. Importantly, the motion capture data enabled the generation of avatar animations that reproduced the original movements of the dyadic interaction partners in every detail. Thus, observer judgments were determined solely by the nonverbal variations avoiding stereotype activation by physical appearance features (see Figure 2).</p>\n<p class=\"TitlePage\">All raw datasets and software tools for data management and analysis are available online. The repository also contains the avatar animations of the first minute of each interaction provided as mp4 files. The unique combination of behavioral, neurophysiological, and subjective data will provide unprecedented possibilities for the study of coordination dynamics in dyadic interactions and their hidden bodily antecedents and consequences. This offers transformational potential for communication research and technology development in a broad range of science and engineering disciplines. The standardized assessment procedures allow to extend the database in size or scope, by adding systematical variations of situations and person characteristics. Furthermore, the provided avatar animation can be used for a broad variety of person perception studies, for instance by systematically varying physical appearance features of the avatars. A major intellectual merit of the project is expected from the systematic application of non-linear methods and Deep Learning approaches to the database and the respective contribution to an integrated theory of interpersonal synchrony and relational communication. The expected knowledge gain will also be essential for the development and evaluation of near-future technologies, such as meta humans and avatar-mediated communication. The gained knowledge can be applied to foster the smooth flow of mediated interactions through technology assistance, and it can be foundational for the detection of manipulative nonverbal cue transformations in avatar-mediated human interactions and in human-agent interactions.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/06/2023<br>\nModified by: Ralf&nbsp;Schmaelzle</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1907807/1907807_10640803_1701875714022_image001--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1907807/1907807_10640803_1701875714022_image001--rgov-800width.png\" title=\"Database schematic\"><img src=\"/por/images/Reports/POR/2023/1907807/1907807_10640803_1701875714022_image001--rgov-66x44.png\" alt=\"Database schematic\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Shown is the dyadic setup and components of the database and production procedure.</div>\n<div class=\"imageCredit\">Gary Bente</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Ralf&nbsp;Schmaelzle\n<div class=\"imageTitle\">Database schematic</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1907807/1907807_10640803_1701875569137_image002--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1907807/1907807_10640803_1701875569137_image002--rgov-800width.png\" title=\"Avatar Animation Sequence\"><img src=\"/por/images/Reports/POR/2023/1907807/1907807_10640803_1701875569137_image002--rgov-66x44.png\" alt=\"Avatar Animation Sequence\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Shown are three screenshot from the avatar animations used for rapport annotation. Animations are based on the recorded motion capture data from 132 real-life dyadic interactions.</div>\n<div class=\"imageCredit\">Gary Bente</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Ralf&nbsp;Schmaelzle\n<div class=\"imageTitle\">Avatar Animation Sequence</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nA substantial share of future human interactions will take place in the metaverse. Avatar-based communication of the future will provide unprecedented possibilities for strategic cue modification in the digital streams of human behavior - leading to many opportunities and threats to enhance or manipulate communication. Research has demonstrated that interpersonal coordination and movement synchrony are key factors in social influence, rapport, and collaboration, but our understanding of their underlying mechanisms remains incomplete. The project is rooted in the fundamental premise that a thorough comprehension of conversational synchrony necessitates the availability of expansive datasets that encapsulate the multifaceted nature of human conversations and encompassing their dynamic evolution across diverse modalities, social contexts, and timeframes. To address this imperative, this project furnishes an open-access interaction database distinguished by its substantial size, varied situational scenarios, and comprehensive behavioral scope.\n\n\nThe introduced database comprises data from 132 short dyadic interactions, recorded in two different social settings: a get-to-know situation and a managerial conflict scenario. The recordings include full-body motion capture data, speech recordings, gaze tracking, as well as neurophysiological measures of arousal and attention. Furthermore, the dataset provides subjective ratings of the interactors and of independent observers regarding basic person characteristics as well as relational quality. Figure 1 serves as an illustrative guide, offering a visual depiction of the database components and the production line.\n\n\nA crucial aspect of the current methodology is its reliance on a full-body motion capture system that issues comprehensive 6-degree-of-freedom movement data for all joints of the human body with high temporal and spatial resolution and accuracy. This level of detail facilitates the in-depth analysis of various nonverbal subsystems, such as body postures, hand gestures, or head movements, and also enables post-hoc aggregation in terms of global activity measures. Importantly, the current database extends on existing motion capture datasets that are limited in the scope of settings and sample sizes. The approach also goes a step further compared to existing datasets by incorporating both verbal and visual communication (gaze) channels. To capture these aspects, unobtrusive eye-tracking glasses equipped with a scene camera and microphone (PupilLabs Invisible) were employed. A customized machine learning routine was used to identify the partners face in the video recordings of the scene camera and to automatically determine directed and averted gaze relative to this face. Speech protocols of both interactors were extracted and automatically transcribed applying python programs for diarization and speech to text conversion. Last not least, the dataset also comprises neurophysiological data (heart rate and EEG). All bio-behavioral protocols were synchronized using a common audio sync signal. This process data is complemented by subjective evaluations of the interaction partners and independent observers including social impressions and judgements of relational quality and interaction outcome. Importantly, the motion capture data enabled the generation of avatar animations that reproduced the original movements of the dyadic interaction partners in every detail. Thus, observer judgments were determined solely by the nonverbal variations avoiding stereotype activation by physical appearance features (see Figure 2).\n\n\nAll raw datasets and software tools for data management and analysis are available online. The repository also contains the avatar animations of the first minute of each interaction provided as mp4 files. The unique combination of behavioral, neurophysiological, and subjective data will provide unprecedented possibilities for the study of coordination dynamics in dyadic interactions and their hidden bodily antecedents and consequences. This offers transformational potential for communication research and technology development in a broad range of science and engineering disciplines. The standardized assessment procedures allow to extend the database in size or scope, by adding systematical variations of situations and person characteristics. Furthermore, the provided avatar animation can be used for a broad variety of person perception studies, for instance by systematically varying physical appearance features of the avatars. A major intellectual merit of the project is expected from the systematic application of non-linear methods and Deep Learning approaches to the database and the respective contribution to an integrated theory of interpersonal synchrony and relational communication. The expected knowledge gain will also be essential for the development and evaluation of near-future technologies, such as meta humans and avatar-mediated communication. The gained knowledge can be applied to foster the smooth flow of mediated interactions through technology assistance, and it can be foundational for the detection of manipulative nonverbal cue transformations in avatar-mediated human interactions and in human-agent interactions.\n\n\n\t\t\t\t\tLast Modified: 12/06/2023\n\n\t\t\t\t\tSubmitted by: RalfSchmaelzle\n"
 }
}