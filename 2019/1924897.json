{
 "awd_id": "1924897",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Consistent distributed visual-inertial estimation and perception for cooperative unmanned aerial vehicles",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Juan Wachs",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 380049.0,
 "awd_amount": 380049.0,
 "awd_min_amd_letter_date": "2019-09-04",
 "awd_max_amd_letter_date": "2019-09-04",
 "awd_abstract_narration": "This project is in response to the emerging demand for ubiquitous deployment of autonomous robots in real-world applications. In particular, thanks to their small size, agile maneuverability, and low-altitude flight ability even in complex environments, unmanned aerial vehicles (UAVs) have witnessed significant progress over the last decade. The ubiquitous availability of small and inexpensive UAVs that are equipped with sensing, processing, and communication capabilities, will make it possible to deploy them in teams that can collaborate to accomplish missions more efficiently and robustly than a single vehicle. Assisted by technological advances in sensing, computing, communication, and hardware design and manufacturing, in the coming years, cooperative UAVs will become valuable tools in critical applications ranging from environmental monitoring and emergency response to precision agriculture. However, when developing cooperative UAV systems, many challenges remain, among which, one of the biggest is the stringent resource limitations (such as limited computation power, communication bandwidth, and energy) that UAVs are faced with. Performing cooperative estimation and perception under resource constraints, incurs many challenges that must be addressed during the UAV operations as well as during the design of UAV systems. \r\n\r\nIn this project, the investigators will design scalable, robust and distributed state estimation and 3D perception for cooperative UAVs using visual and inertial measurements under computation and communication constraints, thus providing 3D scene understanding and spatial cognition to support intelligent decision making. To this end, resource-adaptive consistent visual-inertial estimation will be formulated as constrained optimization to optimally utilize available resources. Leveraging deep learning/AI techniques, the project team will design deep neural networks to power visual-inertial 3D perception in order to semantically and spatially understand environments. To achieve optimal performance for given resources or determine cost-effective system design for desired performance, the project team will develop formal tools for characterization and co-design of UAV hardware and software systems. By technologically enabling ubiquitous deployment of UAVs, the results of this project will foster innovative applications in robotics such as aerial transportation during humanitarian aid and disaster relief, thus boosting economic development. Moreover, this project will promote hands-on learning in undergraduate education in mechanical engineering and enrich graduate curriculum in robotics, as well as create opportunities for students to perform meaningful research.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Guoquan",
   "pi_last_name": "Huang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Guoquan Huang",
   "pi_email_addr": "ghuang@udel.edu",
   "nsf_id": "000677461",
   "pi_start_date": "2019-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Delaware",
  "inst_street_address": "550 S COLLEGE AVE",
  "inst_street_address_2": "",
  "inst_city_name": "NEWARK",
  "inst_state_code": "DE",
  "inst_state_name": "Delaware",
  "inst_phone_num": "3028312136",
  "inst_zip_code": "197131324",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DE00",
  "org_lgl_bus_name": "UNIVERSITY OF DELAWARE",
  "org_prnt_uei_num": "",
  "org_uei_num": "T72NHKM259N3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Delaware",
  "perf_str_addr": "210 Hullihen Hall",
  "perf_city_name": "Newark",
  "perf_st_code": "DE",
  "perf_st_name": "Delaware",
  "perf_zip_code": "197162553",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DE00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 380049.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-11d63a70-7fff-438f-a92f-d1589ea4e26a\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In this project, we have gained in-depth understanding of visual-inertial systems by performing complete observability and degeneracy analysis of visual-inertial estimation and calibration. Based on this analysis, we have developed a set of state-of-the-art consistent visual-inertial navigation system (VINS) algorithms, many of which have been open sourced and greatly promoted the VINS research. In particular, our open platform for visual-inertial estimation algorithm (OpenVINS: </span><a style=\"text-decoration: none;\" href=\"https://github.com/rpng/open_vins\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #1155cc; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;\">https://github.com/rpng/open_vins</span></a><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">) has become one of the most popular open-source VINS projects, being widely used in both academia and industry. Building upon the OpenVINS, we have introduced the FEJ2 design methodology, which addresses the discrepancy between the best Jacobian evaluated at the latest state estimate and the first-estimates Jacobian evaluated at the first-time-ever state estimate to improve estimator consistency and accuracy. We have also incorporated the structural (plane) constraints into VINS (i.e., OV-Plane) to further improve estimation performance. Moreover, we have developed a novel square-root robocentric sliding-window visual-inertial estimation (i.e., Robocentric VIO), where online calibration is used to deal with unknown errors in both spatial and temporal sensor calibration parameters.&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">As sensor calibration plays an important role in visual-inertial sensor fusion, we have performed an in-depth investigation of online self-calibration for robust and accurate visual-inertial state estimation. We have conducted complete observability analysis for VINS with full calibration of sensing parameters, including IMU/camera intrinsics and IMU-camera spatial-temporal extrinsic calibration, along with readout time of rolling shutter (RS) cameras (if used). We have studied different inertial model variants containing intrinsic parameters that encompass most commonly used models for low-cost inertial sensors. With these models, the observability analysis of linearized VINS with full sensor calibration is performed. Our analysis theoretically proves that VINS with full sensor calibration has four unobservable directions, corresponding to the system&rsquo;s global yaw and position, while all sensor calibration parameters are observable given fully-excited motions. Moreover, we, for the first time, have identified degenerate motion primitives for IMU and camera intrinsic calibration, which, when combined, may produce complex degenerate motions. Extending from single-IMU single-camera systems to multi-visual-inertial systems (MVIS), we have thoroughly studied the MVIS full calibration of the associated visual-inertial sensors, including the IMU or camera intrinsics and the IMU-IMU(or camera) spatiotemporal extrinsics as well as the image readout time of rolling-shutter cameras (if used). We performed MVIS observability analysis, showing that the standard four unobservable directions remain -- no matter how many inertial sensors are used, and also identified the degenerate motions for IMU-IMU spatiotemporal extrinsics and auxiliary inertial intrinsics.&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rather than the standard VINS paradigm using a minimal sensing suite of a single camera and IMU, we have designed a real-time consistent multi-IMU multi-camera (MIMC)-VINS estimator that is able to seamlessly fuse multi-modal information from an arbitrary number of uncalibrated cameras and IMUs. Within an efficient multi-state constraint Kalman filter (MSCKF) framework, our MIMC-VINS optimally fuses asynchronous measurements from all sensors, while providing smooth, uninterrupted, and accurate 3D motion tracking even if some sensors fail. </span><span style=\"font-size: 10.5pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The key idea is to perform high-order on-manifold state interpolation to efficiently process all available visual measurements without increasing the computational burden due to estimating additional sensors&rsquo; poses at asynchronous imaging times. In order to fuse the information from multiple IMUs, we propagate a joint system consisting of all IMU states while enforcing rigid-body constraints between the IMUs during the filter update stage. We estimate online both spatiotemporal extrinsic and visual intrinsic parameters to make our system robust to errors in prior sensor calibration.&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 10.5pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Cambria,serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Extending from a single robot, we have designed a consistent and distributed visual-inertial state estimator for multi-robot cooperative localization (i.e., cooperative VINS),&nbsp; which efficiently fuses environmental features and loop-closure constraints across time and robots. In particular, we leveraged covariance intersection (CI) to allow each robot to only estimate its own state and autocovariance and compensate for the unknown correlations between robots. Two novel multi-robot methods for utilizing common environmental SLAM features are introduced and evaluated in terms of accuracy and efficiency. Moreover, we adapted CI to enable drift-free estimation through the use of loop-closure measurement constraints to other robots' historical poses without a significant increase in computational cost. </span></p><br>\n<p>\n Last Modified: 12/12/2023<br>\nModified by: Guoquan&nbsp;Huang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project, we have gained in-depth understanding of visual-inertial systems by performing complete observability and degeneracy analysis of visual-inertial estimation and calibration. Based on this analysis, we have developed a set of state-of-the-art consistent visual-inertial navigation system (VINS) algorithms, many of which have been open sourced and greatly promoted the VINS research. In particular, our open platform for visual-inertial estimation algorithm (OpenVINS: https://github.com/rpng/open_vins) has become one of the most popular open-source VINS projects, being widely used in both academia and industry. Building upon the OpenVINS, we have introduced the FEJ2 design methodology, which addresses the discrepancy between the best Jacobian evaluated at the latest state estimate and the first-estimates Jacobian evaluated at the first-time-ever state estimate to improve estimator consistency and accuracy. We have also incorporated the structural (plane) constraints into VINS (i.e., OV-Plane) to further improve estimation performance. Moreover, we have developed a novel square-root robocentric sliding-window visual-inertial estimation (i.e., Robocentric VIO), where online calibration is used to deal with unknown errors in both spatial and temporal sensor calibration parameters.\n\n\n\n\n\n\nAs sensor calibration plays an important role in visual-inertial sensor fusion, we have performed an in-depth investigation of online self-calibration for robust and accurate visual-inertial state estimation. We have conducted complete observability analysis for VINS with full calibration of sensing parameters, including IMU/camera intrinsics and IMU-camera spatial-temporal extrinsic calibration, along with readout time of rolling shutter (RS) cameras (if used). We have studied different inertial model variants containing intrinsic parameters that encompass most commonly used models for low-cost inertial sensors. With these models, the observability analysis of linearized VINS with full sensor calibration is performed. Our analysis theoretically proves that VINS with full sensor calibration has four unobservable directions, corresponding to the systems global yaw and position, while all sensor calibration parameters are observable given fully-excited motions. Moreover, we, for the first time, have identified degenerate motion primitives for IMU and camera intrinsic calibration, which, when combined, may produce complex degenerate motions. Extending from single-IMU single-camera systems to multi-visual-inertial systems (MVIS), we have thoroughly studied the MVIS full calibration of the associated visual-inertial sensors, including the IMU or camera intrinsics and the IMU-IMU(or camera) spatiotemporal extrinsics as well as the image readout time of rolling-shutter cameras (if used). We performed MVIS observability analysis, showing that the standard four unobservable directions remain -- no matter how many inertial sensors are used, and also identified the degenerate motions for IMU-IMU spatiotemporal extrinsics and auxiliary inertial intrinsics.\n\n\n\n\n\n\nRather than the standard VINS paradigm using a minimal sensing suite of a single camera and IMU, we have designed a real-time consistent multi-IMU multi-camera (MIMC)-VINS estimator that is able to seamlessly fuse multi-modal information from an arbitrary number of uncalibrated cameras and IMUs. Within an efficient multi-state constraint Kalman filter (MSCKF) framework, our MIMC-VINS optimally fuses asynchronous measurements from all sensors, while providing smooth, uninterrupted, and accurate 3D motion tracking even if some sensors fail. The key idea is to perform high-order on-manifold state interpolation to efficiently process all available visual measurements without increasing the computational burden due to estimating additional sensors poses at asynchronous imaging times. In order to fuse the information from multiple IMUs, we propagate a joint system consisting of all IMU states while enforcing rigid-body constraints between the IMUs during the filter update stage. We estimate online both spatiotemporal extrinsic and visual intrinsic parameters to make our system robust to errors in prior sensor calibration.\n\n\n\n\n\n\nExtending from a single robot, we have designed a consistent and distributed visual-inertial state estimator for multi-robot cooperative localization (i.e., cooperative VINS), which efficiently fuses environmental features and loop-closure constraints across time and robots. In particular, we leveraged covariance intersection (CI) to allow each robot to only estimate its own state and autocovariance and compensate for the unknown correlations between robots. Two novel multi-robot methods for utilizing common environmental SLAM features are introduced and evaluated in terms of accuracy and efficiency. Moreover, we adapted CI to enable drift-free estimation through the use of loop-closure measurement constraints to other robots' historical poses without a significant increase in computational cost. \t\t\t\t\tLast Modified: 12/12/2023\n\n\t\t\t\t\tSubmitted by: GuoquanHuang\n"
 }
}