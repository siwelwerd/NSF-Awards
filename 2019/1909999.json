{
 "awd_id": "1909999",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CNS Core: Small: Harnessing Weight Repetition for Efficient Deep Neural Network Inference on General-Purpose Platforms",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jason Hallstrom",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2019-07-11",
 "awd_max_amd_letter_date": "2019-07-11",
 "awd_abstract_narration": "Society is witnessing an explosion in the use of Deep Neural Networks (DNNs) across all facets of daily life including health, finances, entertainment and transportation.  DNNs are used by performing DNN inference, which queries the DNN with an input (for example, an image) to get an answer (for example, a classification).  Society relies on inference every day, where it is run on devices ranging from cloud servers to personal computers.  The goal of this project is to develop new ways to make inference efficient (fast, low power) on these devices.\r\n\r\nThe technical approach is to explore how a new phenomenon, called weight repetition, can be applied to general-purpose devices such as Central Processing Units (CPUs) and Graphical Processing Units (GPUs).  The idea is, when a DNN weight is repeated, DNN inference operations can be simplified.  The first project thrust will develop high-efficiency weight repetition-aware software kernels that can run on un-modified hardware.  The second thrust will develop novel training techniques to co-design the DNN with the weight repetition-aware kernels.  Finally, the third thrust will explore what point hardware modifications can be made to further improve efficiency in the first two thrusts.\r\n\r\nBy proving weight repetition's effectiveness on general-purpose devices, this project will unlock innovation in software, algorithms and hardware.  The project will also amplify the improvement possible from related, but orthogonal, techniques such as weight quantization and weight sparsity.  To support the cross-stack approach, the project will train a new class of students and researchers who can work across high-performance software, hardware and DNN training algorithms to build co-designed Machine Learning stacks and, in the future, apply the lessons learned to other high-impact problems that require cross-layer solutions.\r\n\r\nThe project will store all publications, code and data-sets on public-facing websites, hosted at the University of Illinois for at least 3 years after the end of the project.  This information will be made available via commercial websites.  Links to these websites will be mirrored at http://cwfletcher.net/weightrepetition.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Fletcher",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher Fletcher",
   "pi_email_addr": "cwfletch@illinois.edu",
   "nsf_id": "000743178",
   "pi_start_date": "2019-07-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S. Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project studied the phenomenon of &ldquo;weight repetition&rdquo; in neural networks and how it can be used to improve inference efficiency.&nbsp; A weight is a value in a matrix that encodes the neural network.&nbsp; Generally, neural network operations are unaware if the same weight appears multiple times in or across a matrix.&nbsp; The thesis of this project is that weight repetition-awareness can significantly improve neural network operation (inference) efficiency.&nbsp; For example, if a neural network has weights a and b and is given inputs x and y, it computes a*x + b*y.&nbsp; If its model weights are _repeated_, i.e., a and a, this operation can be optimized to a * (x + y).&nbsp;</p>\n<p>Intellectual Merit: The project made a number of contributions related to weight repetition.&nbsp; First, it proposed the first implementation of a weight-repetition aware program for inference that works on today&rsquo;s programmable devices (CPUs) that shows speedup over an industry-tuned kernel that is unaware of weight repetition (Intel&rsquo;s MKL library).&nbsp; Second, it developed a new method for searching for an efficient accelerator design for neural network inference.&nbsp; Third, it developed a new way to organize (&ldquo;tile&rdquo;) data for a neural network computation that takes advantage of the data&rsquo;s sparsity (a special case of weight repetition where the weight features repeated zero values).&nbsp; Fourth, it enabled the start of the TeAAL project.&nbsp; TeAAL is a language/compiler infrastructure for describing and modeling sparse tensor algebra and weight repetition-aware accelerators.&nbsp;</p>\n<p>Broader Impacts: Multiple students interned in industry as a result of the project; one has graduated (so far) and achieved a full-time position.&nbsp; Another is now a PhD student at a top program.&nbsp; The project has enabled significant undergraduate participation in research and has led to special topics lectures at MIT and UIUC on topics related to application-specific acceleration.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/29/2024<br>\nModified by: Christopher&nbsp;Fletcher</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project studied the phenomenon of weight repetition in neural networks and how it can be used to improve inference efficiency. A weight is a value in a matrix that encodes the neural network. Generally, neural network operations are unaware if the same weight appears multiple times in or across a matrix. The thesis of this project is that weight repetition-awareness can significantly improve neural network operation (inference) efficiency. For example, if a neural network has weights a and b and is given inputs x and y, it computes a*x + b*y. If its model weights are _repeated_, i.e., a and a, this operation can be optimized to a * (x + y).\n\n\nIntellectual Merit: The project made a number of contributions related to weight repetition. First, it proposed the first implementation of a weight-repetition aware program for inference that works on todays programmable devices (CPUs) that shows speedup over an industry-tuned kernel that is unaware of weight repetition (Intels MKL library). Second, it developed a new method for searching for an efficient accelerator design for neural network inference. Third, it developed a new way to organize (tile) data for a neural network computation that takes advantage of the datas sparsity (a special case of weight repetition where the weight features repeated zero values). Fourth, it enabled the start of the TeAAL project. TeAAL is a language/compiler infrastructure for describing and modeling sparse tensor algebra and weight repetition-aware accelerators.\n\n\nBroader Impacts: Multiple students interned in industry as a result of the project; one has graduated (so far) and achieved a full-time position. Another is now a PhD student at a top program. The project has enabled significant undergraduate participation in research and has led to special topics lectures at MIT and UIUC on topics related to application-specific acceleration.\n\n\n\t\t\t\t\tLast Modified: 01/29/2024\n\n\t\t\t\t\tSubmitted by: ChristopherFletcher\n"
 }
}