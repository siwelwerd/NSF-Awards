{
 "awd_id": "1914636",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Uncertainty Quantification, Optimal Designs and Calibration in Computer Experiments",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 142517.0,
 "awd_amount": 142517.0,
 "awd_min_amd_letter_date": "2019-06-19",
 "awd_max_amd_letter_date": "2019-06-19",
 "awd_abstract_narration": "From aerospace designs to material science to biomedical studies, today's practice in engineering and physical sciences has made increasing use of computer simulations. How to design the computer simulations, analyze the computer output data, as well as to enhance the accuracy of the computer models are fundamental challenges in computer simulations. This project will focus on the development of a statistical framework for computer experiments, with the goal of developing both theoretical and methodological tools that cover the typical computer simulation pipeline from data collection to modeling and analysis to verification and validation. Specifically, the team plans to establish a new uncertainty quantification theory and foster novel methodologies for data mining, interpretation and decision making. The project will provide accurate, efficient and robust approaches that would make an impact on contemporary computer simulation practice. \r\n\r\nThe project has three major objectives: (i) establish a statistically and computationally efficient uncertainty quantification framework for Gaussian process regression, (ii) propose a general experimental design scheme for multi-fidelity computer experiments, (iii) study the statistical properties and suggest efficient algorithms for novel calibration methods for computer models. The proposed work should lead to methodological development of a generic nature in the design, uncertainty quantification and calibration in computer experiments. The improved uniform error bounds can potentially lead to the use of fewer experimental runs for the same precision. Their significance can go beyond computer experiments such as in spatial statistics, which heavily uses kriging method. The optimal designs for nonstationary Gaussian Process models can help stimulate further development of experimental design theory in more complex situations. Standard approaches in experimental design do not pay much attention to the nonstationary situations. The proposed algorithm can substantially enhance the value of the projected kernel calibration (PKC) method. Although PKC is known to be theoretically superior, there is no known algorithm that can effectively calculate the PKC estimates. Because calibration is used to bridge the gap between computer simulations and physical experiments, this work can be potentially significant. Theoretical and technical advances made in this project can help facilitate further interactions between statistics, applied mathematics and probability theory, through journal publications, student exchange visits and presentations in interdisciplinary conferences, etc.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rui",
   "pi_last_name": "Tuo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rui Tuo",
   "pi_email_addr": "ruituo@tamu.edu",
   "nsf_id": "000793809",
   "pi_start_date": "2019-06-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433131",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 142517.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Statistics and data science help make computer simulations more like real life by fine-tuning the models, accounting for uncertainties, and identifying what factors matter most. These tools also check how well the simulation matches actual data and can even pool results from different simulations for better insights. This project has advanced the knowledge and methodologies in improving the accuracy, confidence guarantees, and speed of relevant statistical methods. Accuracy is crucial for gaining trust and making good decisions, while having trustworthy or \"confident\" results lets us understand the risks involved in those decisions. Speed is key for tackling big, complex problems in a timely manner and keeping costs down. Together, these elements make simulations a powerful tool for understanding and solving real-world problems.</p>\n<p>To improve the accuracy of the statistical tools for computer simulations, we studied a more flexible calibration framework for computer experiments. Calibration is the process of determining a collection of parameters for simulation in order to make the computer output match the physical observations. We examined the modeling framework that allows the computer parameter to depend on the physical input of the system. We provided sufficient conditions under which the problem is well-posed and developed accurate statistical method to find the solution.</p>\n<p>We conducted theoretical investigations into an important probabilistic model for simulation data: the Gaussian process regression models. Gaussian process models produce random curves, which is commonly used to model the response curves of computer models. This project has provided new insight into the mathematical properties of these models. In particular, it leads to new theory and methodologies to quantify the uncertainty of the prevailing statistical methods. These research outcomes can fortify confidence guarantees and improve risk management for simulation-based decision-making.</p>\n<p>In this project, we discovered an algebraic structure in Gaussian process regression. Leveraging this structure can significantly boost the computational efficiency, addressing a long-standing scalability issue that hindered the application of such models in big data scenarios. We introduced an algorithm which is easy to implement, highly accurate, and scales remarkably well, both theoretically and practically. We also demonstrated the potential of this approach in more complicated models and problems.</p>\n<p>To date, the project has produced 21 peer-reviewed publications, with further papers in development. Software packages for the developed methodologies have been made publicly available. Several graduate students were trained in data science for computer simulation and Gaussian process models. A graduate course on &ldquo;Computer Experiments&rdquo; is developed and delivered. The research outcomes were presented at INFORMS Annual Meetings.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/19/2023<br>\n\t\t\t\t\tModified by: Rui&nbsp;Tuo</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nStatistics and data science help make computer simulations more like real life by fine-tuning the models, accounting for uncertainties, and identifying what factors matter most. These tools also check how well the simulation matches actual data and can even pool results from different simulations for better insights. This project has advanced the knowledge and methodologies in improving the accuracy, confidence guarantees, and speed of relevant statistical methods. Accuracy is crucial for gaining trust and making good decisions, while having trustworthy or \"confident\" results lets us understand the risks involved in those decisions. Speed is key for tackling big, complex problems in a timely manner and keeping costs down. Together, these elements make simulations a powerful tool for understanding and solving real-world problems.\n\nTo improve the accuracy of the statistical tools for computer simulations, we studied a more flexible calibration framework for computer experiments. Calibration is the process of determining a collection of parameters for simulation in order to make the computer output match the physical observations. We examined the modeling framework that allows the computer parameter to depend on the physical input of the system. We provided sufficient conditions under which the problem is well-posed and developed accurate statistical method to find the solution.\n\nWe conducted theoretical investigations into an important probabilistic model for simulation data: the Gaussian process regression models. Gaussian process models produce random curves, which is commonly used to model the response curves of computer models. This project has provided new insight into the mathematical properties of these models. In particular, it leads to new theory and methodologies to quantify the uncertainty of the prevailing statistical methods. These research outcomes can fortify confidence guarantees and improve risk management for simulation-based decision-making.\n\nIn this project, we discovered an algebraic structure in Gaussian process regression. Leveraging this structure can significantly boost the computational efficiency, addressing a long-standing scalability issue that hindered the application of such models in big data scenarios. We introduced an algorithm which is easy to implement, highly accurate, and scales remarkably well, both theoretically and practically. We also demonstrated the potential of this approach in more complicated models and problems.\n\nTo date, the project has produced 21 peer-reviewed publications, with further papers in development. Software packages for the developed methodologies have been made publicly available. Several graduate students were trained in data science for computer simulation and Gaussian process models. A graduate course on \"Computer Experiments\" is developed and delivered. The research outcomes were presented at INFORMS Annual Meetings.\n\n \n\n\t\t\t\t\tLast Modified: 09/19/2023\n\n\t\t\t\t\tSubmitted by: Rui Tuo"
 }
}