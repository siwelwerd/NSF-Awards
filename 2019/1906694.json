{
 "awd_id": "1906694",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Small: Algorithms and Theoretical Foundations for Approximate Bayesian Inference in Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Wei Ding",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 376314.0,
 "awd_amount": 376314.0,
 "awd_min_amd_letter_date": "2018-12-14",
 "awd_max_amd_letter_date": "2018-12-14",
 "awd_abstract_narration": "Over the last two decades Bayesian models have become central in machine learning. Bayesian models often hypothesize latent (non-observed) variables with explanatory or predictive power toward observed phenomena. The challenge is to infer the state of these variables or a belief over that state from observed data. For example, one might try to infer a user's preferences from observations about their own behavior and the behavior of other users. The goal of this project is to develop general approximate inference algorithms that work across large families of Bayesian models so that solutions can be widely reused. The algorithmic work will be complemented by developing a learning theory for approximate Bayesian inference in machine learning. The theoretical framework will aim to prove performance guarantees for Bayesian prediction algorithms and inform the design of algorithms with desirable properties. The project will contribute to basic scientific research, advancing core goals in machine learning. The project will support training and research of PhD students and therefore will directly support human development. Through classroom teaching and outreach the project will expose a larger population of students to machine learning and its potential in applications.\r\n\r\nMore concretely, the project will investigate non-conjugate Bayesian latent variable models, i.e., it will avoid the often used but limiting simplifying assumption of conjugacy. On the algorithmic side the project will aim to generalize the paradigm of variational message passing for non-conjugate graphical models, and to develop stochastic variational inference algorithms using optimal structured approximations for large sub-families of such models. The proposed sub-families will capture the properties of many important problems in the literature. Exploratory research in several specific applications further motivates the work and will be used to test the algorithms. The project will develop a new angle for theoretical analysis of Bayesian algorithms, deriving performance guarantees on their expected error. A core idea is to view variational inference algorithms through the so-called agnostic learning framework where guarantees sought are relative to the best that can be done within a specific limited class of approximations. This will provide a fresh outlook that informs the design of algorithms with desired performance guarantees. The expected scientific impact of the project is having better algorithms with well understood performance characteristics and applicable for a larger class of machine learning problems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Roni",
   "pi_last_name": "Khardon",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Roni Khardon",
   "pi_email_addr": "rkhardon@iu.edu",
   "nsf_id": "000096076",
   "pi_start_date": "2018-12-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "474013654",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IN09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 376314.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Bayesian models in machine learning are increasingly popular and effective. Given prior beliefs over model parameters and given data, one computes the posterior belief over parameters, and uses that belief to calculate optimal predictions. However, with expressive models, these calculations are not feasible and approximate solutions are used. Previous work introduced multiple variations regarding what approximate objective is being optimized and how it is optimized. However, there is no general theory about the quality of predictions that is guaranteed by different approximations and how to choose among them.&nbsp;<br />In this context, the project advanced state of the art along four dimensions: developed new approximations and inference algorithms for machine learning and planning problems, developed analysis and risk bounds (provable bounds on the expected error) for some of these approaches, developed implementations and validated the performance of these algorithms in practice, and developed applications of some of these algorithms in biology and robotic information gathering. These results provide key insights for the design of machine learning algorithms that are successful in producing well-calibrated probabilistic predictions. Project funds were largely used for training and research of graduate students. The results of the research were widely disseminated through publications and technical reports. The publications and several software systems are freely available through the web.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/20/2022<br>\n\t\t\t\t\tModified by: Roni&nbsp;Khardon</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nBayesian models in machine learning are increasingly popular and effective. Given prior beliefs over model parameters and given data, one computes the posterior belief over parameters, and uses that belief to calculate optimal predictions. However, with expressive models, these calculations are not feasible and approximate solutions are used. Previous work introduced multiple variations regarding what approximate objective is being optimized and how it is optimized. However, there is no general theory about the quality of predictions that is guaranteed by different approximations and how to choose among them. \nIn this context, the project advanced state of the art along four dimensions: developed new approximations and inference algorithms for machine learning and planning problems, developed analysis and risk bounds (provable bounds on the expected error) for some of these approaches, developed implementations and validated the performance of these algorithms in practice, and developed applications of some of these algorithms in biology and robotic information gathering. These results provide key insights for the design of machine learning algorithms that are successful in producing well-calibrated probabilistic predictions. Project funds were largely used for training and research of graduate students. The results of the research were widely disseminated through publications and technical reports. The publications and several software systems are freely available through the web. \n\n\t\t\t\t\tLast Modified: 10/20/2022\n\n\t\t\t\t\tSubmitted by: Roni Khardon"
 }
}