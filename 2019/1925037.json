{
 "awd_id": "1925037",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Optoacoustic Material and Structure Pretouch Sensing at Robot Fingertip",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924702",
 "po_email": "cye@nsf.gov",
 "po_sign_block_name": "Cang Ye",
 "awd_eff_date": "2020-01-01",
 "awd_exp_date": "2024-12-31",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 750000.0,
 "awd_min_amd_letter_date": "2019-08-22",
 "awd_max_amd_letter_date": "2023-08-16",
 "awd_abstract_narration": "When robots move from factory floors to a wider service market, it is imperative to enable robots to grasp objects with no prior knowledge. Contactless detection of object: material type; shape; and close-to-surface interior structure, can provide vital information such as friction coefficient and applicable grasping force for planning for successful grasps. Unfortunately, no existing sensors can achieve this. Imaging and ranging devices such as cameras or lidars can neither see through surface nor distinguish material type. Tactile sensing requires physical contacts between the robot finger and the object surface which may risk damaging the object or changing the position of the object. Either case may lead to a grasping failure.  Microelectromechanical systems and robot perception experts will develop systems and algorithms to create a new type of miniature fingertip-mounted sensor that can detect and map object material type, shape, and close-to-surface interior structure without physical contact. The project will benefit a wide range of robotic applications that require grasping and manipulation such as manufacturing, service robots, search & rescue, etc.\r\n\r\nBuilding on the working principle of optoacoustic effect which refers to the formation of acoustic waves following light absorption in a solid material, investigators propose to send modulated laser pulse signals to probe material type and structure based on the acoustic spectrum, time-of-flight, and intensity analyses of the received ultrasound signals. The proposed sensor will be enabled by new and efficient material recognition and surface/interior structure mapping algorithms so that the recommended grasping points and force range will be available before robot fingers are closed. The integrated new research and educational effort is named as the Optoacoustic Material And Structure Sensor (OMASS) project which focuses three main tasks, 1) Development of OMASS devices: an iterative study on design, fabrication, packaging, calibration, testing, and device control, 2)  Pretouch perception algorithms to enable the core functions of OMASS devices: material type recognition, surface shape & interior structure mapping, and grasping point planning, and 3) Building a material database with raw signals and signatures for common household items. The OMASS project will share development and educational efforts via journal and conference publications, seminars, research experience for undergraduates and teachers, open-house activities, and the Internet to scientists, students, underrepresented groups, and the public worldwide. The OMASS project will demonstrate the state-of-the-art robotics to the public. The research team will distribute hardware designs, source codes (e.g. ROS stacks), application programming interfaces, experimental data, and documentation via the project website so that other groups can learn from the project team's experience.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dezhen",
   "pi_last_name": "Song",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dezhen Song",
   "pi_email_addr": "dzsong@cs.tamu.edu",
   "nsf_id": "000354064",
   "pi_start_date": "2019-08-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jun",
   "pi_last_name": "Zou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jun Zou",
   "pi_email_addr": "junzou@tamu.edu",
   "nsf_id": "000299557",
   "pi_start_date": "2019-08-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "HRBB 311B, CSE Department, TAMU",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433112",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 750000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Reliable grasping of unknown objects is a grand challenge for robotics. When robots move from factory floors to a wider service market, it is imperative to enable robots to grasp objects with no prior knowledge. Contactless detection of object material type, shape, close-to-surface interior structure can provide vital information such as friction coefficient and applicable grasping force for planning for successful grasps. Unfortunately, no existing sensors can achieve this. Imaging and ranging devices such as cameras/lidars can neither see through surface nor distinguish material type. Tactile sensing requires physical contact between the robot finger and the object&rsquo;s surface which may risk damaging the object or changing configuration of the object. Either case may lead to grasping failure. </span></p>\r\n<p><span>In this project, we have created a new type of miniature fingertip-mounted sensor that can contactlessly detect and map object material type, shape, close-to-surface interior structure at close proximity. Building on the working principle of optoacoustic effect which refers to the formation of acoustic waves following light absorption in a solid material, our design sends modulated laser pulse signals to probe material type and structure based on the acoustic spectrum, time-of-flight, and intensity analyses of the received ultrasound signals. In the five-year development, we have developed four generations of the new sensor with each iteration aims at improving sensing performance while reducing cost and physical footprint. At the same time, we have developed new and efficient material recognition and surface/interior structure mapping algorithms. We have demonstrated that the new sensor can help detect different physical medium types and even reconstruct the geometric shape of transparent objects in a common household. These newfound finger-tip contactless sensing capabilities enabled by our new sensor and its perception algorithm can significantly increase robot&rsquo;s adaptability in unknown environment which will extend the robotic application from limited factory floor to broader applications such as service robots or flexible manufacturing. &nbsp;</span></p>\r\n<p><span>Intellectually, the project has developed new sensors with efficient models and algorithms that enable robotic grasping for tasks with real world constraints. We have evaluated our developments using our testbed and common household items. The development of new hardware, sensing models, systems, architecture, data structures, and algorithms has advanced understandings in the emerging field of MEMS sensor design, sensor fusion and perception-facilitated grasping.</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/01/2025<br>\nModified by: Dezhen&nbsp;Song</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/1925037/1925037_10637118_1743498989564_g1_g4_prototypes--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1925037/1925037_10637118_1743498989564_g1_g4_prototypes--rgov-800width.png\" title=\"Prototypes of the new sensor design\"><img src=\"/por/images/Reports/POR/2025/1925037/1925037_10637118_1743498989564_g1_g4_prototypes--rgov-66x44.png\" alt=\"Prototypes of the new sensor design\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Four generations of sensor prototypes are shown here.</div>\n<div class=\"imageCredit\">Cheng Fang, Shuangliang Li, Jun Zou, and Dezhen Song</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Dezhen&nbsp;Song\n<div class=\"imageTitle\">Prototypes of the new sensor design</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1925037/1925037_10637118_1743498887222_g1_g4_design--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1925037/1925037_10637118_1743498887222_g1_g4_design--rgov-800width.png\" title=\"Designs of new sensor: Generations 1-4\"><img src=\"/por/images/Reports/POR/2025/1925037/1925037_10637118_1743498887222_g1_g4_design--rgov-66x44.png\" alt=\"Designs of new sensor: Generations 1-4\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This figure illustrates the four generation of new sensor design.</div>\n<div class=\"imageCredit\">Cheng Fang, Jun Zou, and Dezhen Song</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Dezhen&nbsp;Song\n<div class=\"imageTitle\">Designs of new sensor: Generations 1-4</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nReliable grasping of unknown objects is a grand challenge for robotics. When robots move from factory floors to a wider service market, it is imperative to enable robots to grasp objects with no prior knowledge. Contactless detection of object material type, shape, close-to-surface interior structure can provide vital information such as friction coefficient and applicable grasping force for planning for successful grasps. Unfortunately, no existing sensors can achieve this. Imaging and ranging devices such as cameras/lidars can neither see through surface nor distinguish material type. Tactile sensing requires physical contact between the robot finger and the objects surface which may risk damaging the object or changing configuration of the object. Either case may lead to grasping failure. \r\n\n\nIn this project, we have created a new type of miniature fingertip-mounted sensor that can contactlessly detect and map object material type, shape, close-to-surface interior structure at close proximity. Building on the working principle of optoacoustic effect which refers to the formation of acoustic waves following light absorption in a solid material, our design sends modulated laser pulse signals to probe material type and structure based on the acoustic spectrum, time-of-flight, and intensity analyses of the received ultrasound signals. In the five-year development, we have developed four generations of the new sensor with each iteration aims at improving sensing performance while reducing cost and physical footprint. At the same time, we have developed new and efficient material recognition and surface/interior structure mapping algorithms. We have demonstrated that the new sensor can help detect different physical medium types and even reconstruct the geometric shape of transparent objects in a common household. These newfound finger-tip contactless sensing capabilities enabled by our new sensor and its perception algorithm can significantly increase robots adaptability in unknown environment which will extend the robotic application from limited factory floor to broader applications such as service robots or flexible manufacturing. \r\n\n\nIntellectually, the project has developed new sensors with efficient models and algorithms that enable robotic grasping for tasks with real world constraints. We have evaluated our developments using our testbed and common household items. The development of new hardware, sensing models, systems, architecture, data structures, and algorithms has advanced understandings in the emerging field of MEMS sensor design, sensor fusion and perception-facilitated grasping.\r\n\n\n\t\t\t\t\tLast Modified: 04/01/2025\n\n\t\t\t\t\tSubmitted by: DezhenSong\n"
 }
}