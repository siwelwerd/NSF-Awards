{
 "awd_id": "1946115",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Development of Selective Attention to Multisensory Information in Human Infants",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Leher Singh",
 "awd_eff_date": "2019-05-01",
 "awd_exp_date": "2023-04-30",
 "tot_intn_awd_amt": 697000.0,
 "awd_amount": 697000.0,
 "awd_min_amd_letter_date": "2019-08-27",
 "awd_max_amd_letter_date": "2019-08-27",
 "awd_abstract_narration": "Studies have found that infants perceive, learn, and remember better when they can simultaneously see and hear the objects and events in their everyday life as opposed to when they can only see or hear them. This project tests the hypothesis that the reason for this is because redundant (i.e., matching) multisensory information elicits greater attention than does either mis-matching multisensory information or unisensory information. The results from this project will provide key insights into infant perception, attention, learning, and memory and, as a result, they will suggest better ways for helping infants acquire critical cognitive and social skills. In addition, the results will provide new insights into the root causes of some developmental disabilities (e.g., autism) which are known to involve impaired perception and attention to multisensory information. Finally, the results from this project will provide a scientific foundation for the design of  tools that will permit clinicians to more effectively detect and ameliorate impaired attention to multisensory information in young children with developmental disabilities, thereby improving their cognitive and social development.\r\n\r\nBy around 6 months of age, a previously rudimentary, reflex-like selective attention response system becomes transformed into an endogenously driven selective attention system. This new functionality permits infants to deploy their attention to sources of multisensory redundancy that are of specific interest to them. The project seeks to answer three main questions. First, what mechanisms underlie developmental changes in infant selective attention to sources of multisensory redundancy? Second, how does early experience contribute to developmental changes in selective attention to multisensory redundancy in infancy? Third, how does improving selective attention to multisensory redundancy contribute to information processing across different psychological domains? Infants residing in monolingual and bilingual households will be tested with eye tracking methods to assess attention to multisensory redundancy inherent in talking faces. In some cases, eye tracking will be used  with an intersensory matching procedure to examine infant matching of auditory and visual attributes of speech. In other cases, eye tracking will be used with a habituation/test procedure to investigate learning of multisensory events. The investigators will analyze the temporal distribution of attention as well as the latency of responses. The findings from this project will provide novel insights into the emergence of attention and learning in infancy, clues to maximizing attention and learning in typically developing infants, and clues to improving attention and learning in infants at risk for developmental disabilities.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Lewkowicz",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "David J Lewkowicz",
   "pi_email_addr": "david.lewkowicz@yale.edu",
   "nsf_id": "000229053",
   "pi_start_date": "2019-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Haskins Laboratories, Inc.",
  "inst_street_address": "300 GEORGE ST STE 900",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2038656163",
  "inst_zip_code": "065116624",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "HASKINS LABORATORIES, INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "HKDXUVASHNE4"
 },
 "perf_inst": {
  "perf_inst_name": "Haskins Laboratories",
  "perf_str_addr": "300 George St.",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065116610",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169800",
   "pgm_ele_name": "DS -Developmental Sciences"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1698",
   "pgm_ref_txt": "DS-Developmental Sciences"
  },
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 696999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our everyday world is specified by a continuously varying&nbsp;m&eacute;lange&nbsp;of concurrent multisensory inputs. In principle, this can be confusing. For example, whenever we find ourselves at a social event such as a party or a busy restaurant, we are confronted with multiple people, represented by unique pairs of audible (A) and visible (V) perceptual attributes, speaking at the same time. If we wish to meaningfully communicate with any one of these people, we must first solve the multisensory cocktail party problem (MCPP). To do so, we must: (a) perceptually segregate the target person&rsquo;s A and V attributes, including their audiovisual (AV) speech utterances, from those of all the other people, (b) correctly integrate the specific A &amp; V attributes representing the target person, and (c) ignore (i.e., filter out) the A &amp; V attributes representing all the other people who can be heard and seen talking at the same time.</p>\n<p>&nbsp;</p>\n<p>This project investigated the developmental emergence of the solution of the MCPP. Because its solution requires the operation of relatively sophisticated perceptual, attentional, and cognitive mechanisms, we expected that the MCPP would be challenging for infants and young children. To test this prediction, we created a novel experimental paradigm during which&nbsp;we presented composite AV events, either consisting of multiple moving/sounding objects or of multiple simultaneously talking faces, where the A event attribute corresponded to only one of the V event attributes. The participants&rsquo; task was to find the bouncing object or to find the talking face with the assumption being that they would spontaneously selectively attend to the object or face that corresponded to the concurrent auditory stimulus. Using an eye tracker, we measured selective attention to each object or to each face as well as the eyes and mouth of each respective face. By identifying which object or face captured maximum attention, we were able to determine which specific attributes enabled participants to solve the MCPP. Furthermore, in the talking face experiments, measures of selective attention to the eyes and mouth enabled us to determine whether the social/deictic cues inherent in a talker&rsquo;s eyes and/or the integrated AV speech cues inherent in a talker&rsquo;s mouth drove the solution of the MCPP.&nbsp;</p>\n<p>&nbsp;</p>\n<p>AV temporal synchrony is arguably the most basic and fundamental multisensory perceptual attribute that enables us to integrate our multisensory world and, in the process, perceive unitary objects/events that we can segregate from all others when challenged with the MCPP. Therefore, we focused on when and how A and V temporal synchrony affects infants&rsquo;, children&rsquo;s, and adults&rsquo; ability to perceptually segregate and integrate multiple moving/sounding objects and multiple talking faces, with a primary focus on the latter. We found that temporal A and V synchrony drives attention and responsiveness and, thus, that it is a powerful perceptual segregation and integration cue. As predicted, however, its power to drive attention and responsiveness only emerged gradually during development. Specifically, we found that even though infants are known to prefer temporally synchronized AV inputs in relatively simple AV displays, the infants in this project did not exhibit any preferences when they were confronted with the more complex and challenging AV event that more closely resembles the MCPP in the real world. We did find, however, that children did begin to exhibit a small preference for the temporally synchronized face by 3 years of age and that this preference grew considerably by 7 years of age. Despite this, the preference found by 7 years of age was significantly lower than that found in adults who exhibited a massive preference for the audiovisually integrated talking face (&gt;80%). Overall, these results indicate that the mechanisms essential for the solution of the MCPP emerge gradually during development. Furthermore, they suggest that efficient and correct identification of an AV target in a complex and dynamic AV scene emerges gradually over many years. Presumably, this critical component of an efficiently operating social communication system depends on neural maturation and extended experience with the everyday, multisensory world.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Broadly speaking, the results from this project demonstrate that the mechanisms that enable successful social communication emerge slowly and gradually during childhood. This has important implications for understanding how neurotypical children learn from their interlocutors and social partners and suggests how we might maximize children&rsquo;s learning opportunities at home and in school. Our findings also provide novel insights into the reasons why neuroatypical infants and children (e.g., those on the autism spectrum) fail to acquire appropriate communication skills. Furthermore, they suggest ways to ameliorate these children&rsquo;s disabilities through specific intervention strategies that may maximize their selective attention skills, that may improve their impaired multisensory integration skills, and that ultimately may improve their ability to interact with and learn from their social partners.&nbsp;&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/21/2023<br>\n\t\t\t\t\tModified by: David&nbsp;J&nbsp;Lewkowicz</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur everyday world is specified by a continuously varying m&eacute;lange of concurrent multisensory inputs. In principle, this can be confusing. For example, whenever we find ourselves at a social event such as a party or a busy restaurant, we are confronted with multiple people, represented by unique pairs of audible (A) and visible (V) perceptual attributes, speaking at the same time. If we wish to meaningfully communicate with any one of these people, we must first solve the multisensory cocktail party problem (MCPP). To do so, we must: (a) perceptually segregate the target person\u2019s A and V attributes, including their audiovisual (AV) speech utterances, from those of all the other people, (b) correctly integrate the specific A &amp; V attributes representing the target person, and (c) ignore (i.e., filter out) the A &amp; V attributes representing all the other people who can be heard and seen talking at the same time.\n\n \n\nThis project investigated the developmental emergence of the solution of the MCPP. Because its solution requires the operation of relatively sophisticated perceptual, attentional, and cognitive mechanisms, we expected that the MCPP would be challenging for infants and young children. To test this prediction, we created a novel experimental paradigm during which we presented composite AV events, either consisting of multiple moving/sounding objects or of multiple simultaneously talking faces, where the A event attribute corresponded to only one of the V event attributes. The participants\u2019 task was to find the bouncing object or to find the talking face with the assumption being that they would spontaneously selectively attend to the object or face that corresponded to the concurrent auditory stimulus. Using an eye tracker, we measured selective attention to each object or to each face as well as the eyes and mouth of each respective face. By identifying which object or face captured maximum attention, we were able to determine which specific attributes enabled participants to solve the MCPP. Furthermore, in the talking face experiments, measures of selective attention to the eyes and mouth enabled us to determine whether the social/deictic cues inherent in a talker\u2019s eyes and/or the integrated AV speech cues inherent in a talker\u2019s mouth drove the solution of the MCPP. \n\n \n\nAV temporal synchrony is arguably the most basic and fundamental multisensory perceptual attribute that enables us to integrate our multisensory world and, in the process, perceive unitary objects/events that we can segregate from all others when challenged with the MCPP. Therefore, we focused on when and how A and V temporal synchrony affects infants\u2019, children\u2019s, and adults\u2019 ability to perceptually segregate and integrate multiple moving/sounding objects and multiple talking faces, with a primary focus on the latter. We found that temporal A and V synchrony drives attention and responsiveness and, thus, that it is a powerful perceptual segregation and integration cue. As predicted, however, its power to drive attention and responsiveness only emerged gradually during development. Specifically, we found that even though infants are known to prefer temporally synchronized AV inputs in relatively simple AV displays, the infants in this project did not exhibit any preferences when they were confronted with the more complex and challenging AV event that more closely resembles the MCPP in the real world. We did find, however, that children did begin to exhibit a small preference for the temporally synchronized face by 3 years of age and that this preference grew considerably by 7 years of age. Despite this, the preference found by 7 years of age was significantly lower than that found in adults who exhibited a massive preference for the audiovisually integrated talking face (&gt;80%). Overall, these results indicate that the mechanisms essential for the solution of the MCPP emerge gradually during development. Furthermore, they suggest that efficient and correct identification of an AV target in a complex and dynamic AV scene emerges gradually over many years. Presumably, this critical component of an efficiently operating social communication system depends on neural maturation and extended experience with the everyday, multisensory world. \n\n \n\nBroadly speaking, the results from this project demonstrate that the mechanisms that enable successful social communication emerge slowly and gradually during childhood. This has important implications for understanding how neurotypical children learn from their interlocutors and social partners and suggests how we might maximize children\u2019s learning opportunities at home and in school. Our findings also provide novel insights into the reasons why neuroatypical infants and children (e.g., those on the autism spectrum) fail to acquire appropriate communication skills. Furthermore, they suggest ways to ameliorate these children\u2019s disabilities through specific intervention strategies that may maximize their selective attention skills, that may improve their impaired multisensory integration skills, and that ultimately may improve their ability to interact with and learn from their social partners.  \n\n \n\n\t\t\t\t\tLast Modified: 08/21/2023\n\n\t\t\t\t\tSubmitted by: David J Lewkowicz"
 }
}