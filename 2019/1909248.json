{
 "awd_id": "1909248",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: Improving Mobile Device Input for Users who are Blind or Low Vision",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 225663.0,
 "awd_amount": 225663.0,
 "awd_min_amd_letter_date": "2019-08-26",
 "awd_max_amd_letter_date": "2023-09-11",
 "awd_abstract_narration": "Smartphones are an essential part of everyday life. But for people with visual impairments, basic tasks like composing text messages or browsing the web can be prohibitively slow and difficult. The goal of this project is to develop accessible text entry methods that will enable people with visual impairments to enter text at rates comparable to sighted people. This project will design new algorithms and feedback methods for today's standard text entry approaches of tapping on individual keys, gesturing across keys, or dictating via speech. The project aims to help users avoid errors by enabling more accurate input via audio and tactile feedback, help users find errors by providing audio and visual annotation of uncertain portions of the text, and help users correct errors by combining the probabilistic information from the original input, the correction, and approximate information about an error's location. Improving text entry methods for people who are blind or have low vision will enable them to use their mobile devices more effectively for work and leisure. Thus, this project represents an important step to achieving equity for people with visual impairments. \r\n\r\nThis project will contribute novel interface designs to the accessibility and human-computer interaction literature. It will advance the state-of-the-art in mobile device accessibility by, first, studying text entry accessibility for low vision in addition to blind people. Next, the researchers will study and develop accessible gesture typing input methods.  Finally, the project will develop accessible speech input methods.  This project will produce design guidelines, feedback methods, input techniques, recognition algorithms, user study results, and software prototypes that will guide improvements to research and commercial input systems for users who are blind or low-vision. Further, the project's work on the error correction and revision process will improve the usability and performance of touchscreen and speech input methods for everyone.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Keith",
   "pi_last_name": "Vertanen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Keith Vertanen",
   "pi_email_addr": "vertanen@mtu.edu",
   "nsf_id": "000620484",
   "pi_start_date": "2019-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Michigan Technological University",
  "inst_street_address": "1400 TOWNSEND DR",
  "inst_street_address_2": "",
  "inst_city_name": "HOUGHTON",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "9064871885",
  "inst_zip_code": "499311200",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MI01",
  "org_lgl_bus_name": "MICHIGAN TECHNOLOGICAL UNIVERSITY",
  "org_prnt_uei_num": "GKMSN3DA6P91",
  "org_uei_num": "GKMSN3DA6P91"
 },
 "perf_inst": {
  "perf_inst_name": "Michigan Technological University",
  "perf_str_addr": "1400 Townsend Drive",
  "perf_city_name": "Houghton",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "499311295",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 225663.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-806e0820-7fff-3d1e-1177-f49199e88118\"> </span></p>\n<p dir=\"ltr\"><span>Mobile touchscreen devices can be difficult to use eyes-free (i.e. without visually attending to the device's screen). This is particularly problematic for users who are visually-impaired. Text input is a common task and can be slow and error-prone without visual feedback. This project investigated how to make text input more efficient when operating a mobile device eyes-free.</span></p>\n<p dir=\"ltr\"><span>During the project, we interviewed 12 smartphone users who were blind. We asked about their experiences with mobile text input and solicited their opinions on ways to improve the process. We found that speech dictation was their most common text input method followed by an onscreen keyboard with a screen reader. We identified three primary pain points: the poor accuracy of dictation, difficulty entering text in noisy environments, and difficulty correcting errors.&nbsp;</span></p>\n<p>Before errors can be corrected, the user must first detect that an error has occurred in the first place. Typical input methods such as speech dictation play a user's recognized text via text-to-speech (TTS). Detecting errors in TTS audio can be difficult. We conducted a user study with 48 participants that explored dynamically adjusting the TTS to better alert users to potential errors. Our approach used a speech recognizer's confidence in its result to slow the TTS playback rate or to add a beep. We found selectively slowing the audio when the recognizer had low confidence led to a relative increase of 12% in participants&rsquo; error detection ability compared to uniformly slowing the audio.</p>\n<p dir=\"ltr\"><span>A typical smartphone keyboard presents a small number of word predictions in an effort to help users correct errors or accelerate their writing. Providing eyes-free access to these predictions requires playing each sequentially via TTS which can be time consuming. In a user study with 24 participants, we compared playing predictions simultaneously via spatial audio. In a follow-up study with 96 participants, we compared different short delays between the simultaneous audio playback. We found a short delay of 0.25s allowed two predictions to be played simultaneously and in less time without sacrificing user accuracy.</span></p>\n<p dir=\"ltr\"><span>During the project, we reimagined eyes-free text input by designing FlexType. FlexType allows writing via a set of four location-independent gestures. Each gesture represents a set of possible characters (e.g. a tap with one finger represents A-E, a tap with two fingers represents F-M, etc). After the input of a sequence of such ambiguous gestures, our algorithm determines the most likely word given the gesture sequence and a user's previous text. We first conducted computational simulations to optimize ambiguous character groups. Next, we conducted a longitudinal study with 16 participants to validate the method and to compare two candidate character groupings. We then conducted a longitudinal study with 14 blind participants comparing FlexType with their usual text input method. While we found FlexType was significantly slower than Apple's onscreen Braille input, there was no significant difference in entry or error rate between FlexType and an onscreen keyboard coupled with a screen reader (Apple's VoiceOver feature). This was despite the participants having much more experience with the VoiceOver keyboard compared to FlexType. Overall, blind participants averaged 8 words per minute with a 7% character error rate after correction when writing with FlexType.</span></p>\n<p dir=\"ltr\"><span>This project added substantially to our knowledge on how to improve eyes-free text input on mobile devices. Our results show that simple changes to how we provide audio feedback to users can help them better detect errors and make better use of features such as word predictions. We also found blind users learned to write with our novel eyes-free input method at a speed comparable to the current standard input method of using an onscreen keyboard with a screen reader. Finally, this project provided research opportunities for two graduate students (both of whom went on to academic positions) and one undergraduate student.</span></p><br>\n<p>\n Last Modified: 10/30/2024<br>\nModified by: Keith&nbsp;Vertanen</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1909248/1909248_10637750_1730068393303_speech_detect_errors--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1909248/1909248_10637750_1730068393303_speech_detect_errors--rgov-800width.png\" title=\"The four parts of the interface. 1) Target sentence with a record and stop button. 2) An audio playback widget. 3) A radio button asking if the audio matched the target sentence or not. 4) Sequence of buttons for each word in the result along with plus buttons between each word.\"><img src=\"/por/images/Reports/POR/2024/1909248/1909248_10637750_1730068393303_speech_detect_errors--rgov-66x44.png\" alt=\"The four parts of the interface. 1) Target sentence with a record and stop button. 2) An audio playback widget. 3) A radio button asking if the audio matched the target sentence or not. 4) Sequence of buttons for each word in the result along with plus buttons between each word.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Interface for investigating detecting speech recognition errors via text-to-speech (TTS) audio feedback. 1) Users recorded themselves speaking a provided sentence. 2) Users played the recognition result via TTS. 3) Users decided if the result was correct. 4) Users mark incorrect or missing words in</div>\n<div class=\"imageCredit\">Sadia Nowrin</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Keith&nbsp;Vertanen\n<div class=\"imageTitle\">The four parts of the interface. 1) Target sentence with a record and stop button. 2) An audio playback widget. 3) A radio button asking if the audio matched the target sentence or not. 4) Sequence of buttons for each word in the result along with plus buttons between each word.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1909248/1909248_10637750_1730067259772_FlexType_example--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1909248/1909248_10637750_1730067259772_FlexType_example--rgov-800width.png\" title=\"A sequence of images showing three tap actions followed by two swipe actions that result in writing the word \"man\".\"><img src=\"/por/images/Reports/POR/2024/1909248/1909248_10637750_1730067259772_FlexType_example--rgov-66x44.png\" alt=\"A sequence of images showing three tap actions followed by two swipe actions that result in writing the word \"man\".\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Entering \"man\" using FlexType. From left to right, the user taps with two fingers, one finger, and then three fingers. The possible letters for each position are read after each tap. The user swipes right and the \"her\" is recognized as most likely. The user swipes up to change to \"man\".</div>\n<div class=\"imageCredit\">Dylan Gaines</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Keith&nbsp;Vertanen\n<div class=\"imageTitle\">A sequence of images showing three tap actions followed by two swipe actions that result in writing the word \"man\".</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nMobile touchscreen devices can be difficult to use eyes-free (i.e. without visually attending to the device's screen). This is particularly problematic for users who are visually-impaired. Text input is a common task and can be slow and error-prone without visual feedback. This project investigated how to make text input more efficient when operating a mobile device eyes-free.\n\n\nDuring the project, we interviewed 12 smartphone users who were blind. We asked about their experiences with mobile text input and solicited their opinions on ways to improve the process. We found that speech dictation was their most common text input method followed by an onscreen keyboard with a screen reader. We identified three primary pain points: the poor accuracy of dictation, difficulty entering text in noisy environments, and difficulty correcting errors.\n\n\nBefore errors can be corrected, the user must first detect that an error has occurred in the first place. Typical input methods such as speech dictation play a user's recognized text via text-to-speech (TTS). Detecting errors in TTS audio can be difficult. We conducted a user study with 48 participants that explored dynamically adjusting the TTS to better alert users to potential errors. Our approach used a speech recognizer's confidence in its result to slow the TTS playback rate or to add a beep. We found selectively slowing the audio when the recognizer had low confidence led to a relative increase of 12% in participants error detection ability compared to uniformly slowing the audio.\n\n\nA typical smartphone keyboard presents a small number of word predictions in an effort to help users correct errors or accelerate their writing. Providing eyes-free access to these predictions requires playing each sequentially via TTS which can be time consuming. In a user study with 24 participants, we compared playing predictions simultaneously via spatial audio. In a follow-up study with 96 participants, we compared different short delays between the simultaneous audio playback. We found a short delay of 0.25s allowed two predictions to be played simultaneously and in less time without sacrificing user accuracy.\n\n\nDuring the project, we reimagined eyes-free text input by designing FlexType. FlexType allows writing via a set of four location-independent gestures. Each gesture represents a set of possible characters (e.g. a tap with one finger represents A-E, a tap with two fingers represents F-M, etc). After the input of a sequence of such ambiguous gestures, our algorithm determines the most likely word given the gesture sequence and a user's previous text. We first conducted computational simulations to optimize ambiguous character groups. Next, we conducted a longitudinal study with 16 participants to validate the method and to compare two candidate character groupings. We then conducted a longitudinal study with 14 blind participants comparing FlexType with their usual text input method. While we found FlexType was significantly slower than Apple's onscreen Braille input, there was no significant difference in entry or error rate between FlexType and an onscreen keyboard coupled with a screen reader (Apple's VoiceOver feature). This was despite the participants having much more experience with the VoiceOver keyboard compared to FlexType. Overall, blind participants averaged 8 words per minute with a 7% character error rate after correction when writing with FlexType.\n\n\nThis project added substantially to our knowledge on how to improve eyes-free text input on mobile devices. Our results show that simple changes to how we provide audio feedback to users can help them better detect errors and make better use of features such as word predictions. We also found blind users learned to write with our novel eyes-free input method at a speed comparable to the current standard input method of using an onscreen keyboard with a screen reader. Finally, this project provided research opportunities for two graduate students (both of whom went on to academic positions) and one undergraduate student.\t\t\t\t\tLast Modified: 10/30/2024\n\n\t\t\t\t\tSubmitted by: KeithVertanen\n"
 }
}