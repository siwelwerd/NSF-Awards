{
 "awd_id": "1940091",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CoPe EAGER: Collaborative Research: A GeoAI Data-Fusion Framework for Real-Time Assessment of Flood Damage and Transportation Resilience by Integrating Complex Sensor Datasets",
 "cfda_num": "47.050",
 "org_code": "06010000",
 "po_phone": "7032924708",
 "po_email": "amadams@nsf.gov",
 "po_sign_block_name": "Manda S. Adams",
 "awd_eff_date": "2020-01-01",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 163151.0,
 "awd_amount": 163151.0,
 "awd_min_amd_letter_date": "2019-09-09",
 "awd_max_amd_letter_date": "2019-09-09",
 "awd_abstract_narration": "Traditional modeling approaches for flood damage assessment are often labor-intensive and time-consuming due to requirements for domain expertise, training data, and field surveys. Additionally, the lack of data and standard methodologies makes it more challenging to assess transportation network resilience in real-time during flood disasters. To address these challenges, this project aims to integrate novel data streams from both physical sensor networks (e.g., remotely-sensed data using unmanned aerial vehicles [UAVs]), and citizen sensor networks (e.g., crowdsourced traffic data, social media and community responsive teams connected through a developed mobile app). The goal is to develop a framework for real-time assessment of damage and the resilience of urban transportation infrastructures after coastal floods via the state-of-the-art computer vision, deep learning and data fusion technologies. The project will also advance Data Science through multi-disciplinary and multi-institutional collaborations. The project is expected to improve the sustainability, resilience, livability, and general well-being of coastal communities by having a direct impact on the effectiveness, capability, and potential of using both physical and social sensor data. This will in turn enable and transform damage assessments, and identify critical and vulnerable components in transportation networks in a more effective and efficient manner. The interdisciplinary research team, along with students and collaborators from different coastal regions, will facilitate the sharing of knowledge and technologies from different socio-environmental contexts and testing the transferability of the research outcomes.\r\n\r\nThe project will harmonize physical and citizen sensors within a geospatial artificial intelligence (GeoAI) data-fusion framework with a focus on three research thrusts: (1) unsupervised flood extent detection by integrating UAV images collected throughout this project with existing geospatial data (e.g., road networks and building footprints); (2) flood depth estimation using deep learning and computer vision techniques combined with crowdsourced photos and UAV imagery; and (3) assessment of the impact on and resilience of transportation networks based on near real-time flood and damage information. The innovative methodology will be demonstrated and deployed through collaborative efforts in response to future flood events as well as several historical storms. The project will produce open-source algorithms for future educational use, raw and processed datasets and associated processing software, a mobile app to engage community responsive science teams, and three research publications.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "GEO",
 "org_dir_long_name": "Directorate for Geosciences",
 "div_abbr": "RISE",
 "org_div_long_name": "Integrative and Collaborative Education and Research (ICER)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Qunying",
   "pi_last_name": "Huang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qunying Huang",
   "pi_email_addr": "qhuang46@wisc.edu",
   "nsf_id": "000672251",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Wright",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel B Wright",
   "pi_email_addr": "danielb.wright@wisc.edu",
   "nsf_id": "000731822",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Song",
   "pi_last_name": "Gao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Song Gao",
   "pi_email_addr": "song.gao@wisc.edu",
   "nsf_id": "000805179",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "550 North Park st",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061404",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "097Y00",
   "pgm_ele_name": "CoPe-Coastlines and People"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "4444",
   "pgm_ref_txt": "INTERDISCIPLINARY PROPOSALS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 163151.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Flooding is the most frequent natural disaster in the US, accounting for over 75% of federally-declared disasters. Flood impact assessment, including identification of both inundation extent and depth, is critical in both real-time and longer-term applications. This project integrates data from both <em>physical sensor networks</em> (e.g., UAVs), and <em>citizen sensor networks</em> (e.g., social media and citizen contributed data through a developed mobile app), with advanced artificial intelligence (AI), computer vision and spatial analysis techniques to assess flood damage.</p>\n<p>To delineate flooded areas (i.e., inundation extent) in heterogeneous coastal cities, we first developed Patch Similarity Convolutional Neural Network (PSNet) models that allow to mapping floods over densely populated urban areas <strong>with only a small number of training samples labeled in less than 1 hour by a geospatial expert </strong>(Figure 1). Experiments on the high resolution imagery before and after the urban flooding events (i.e., the 2017 Hurricane Harvey and the 2018 Hurricane Florence) showed that the developed PSNet models can produce urban flood maps with consistently high precision, recall, F1 score, and overall accuracy compared with baseline classification models including support vector machine, decision tree, random forest, and AdaBoost, which were often poor in either precision or recall (Figure 1).</p>\n<p>Next, these PSNet models were then enhanced <strong>with a self-supervised learning framework for object-based urban flood mapping, eliminating the requirement of labeled data</strong> (Figure 2). This is the pioneering research to map heterogeneous urban floods with high resolution using self-supervised object-based image analysis.</p>\n<p>Moving forward, we developed various approaches for <strong>real-time flood extent mapping at the pixel level through weakly supervised learning techniques without the need of human labelling efforts</strong>. Specifically, we developed a weakly-supervised pixel-wise flood mapping method by leveraging multi-temporal remote sensing imagery and image processing techniques (e.g., thresholding, k-means clustering, and edge detection) to create weakly-labeled data for training a bi-temporal U-Net model for flood detection. In addition, we also explored other auxiliary features and datasets, including the FEMA national flood hazard layer (NFHL), and Microsoft building footprints, to automatically sample flooded and non-flooded locations as weak labels for training ML models and mapping floods across the study area (Figure 3).</p>\n<p>To estimate flood inundation level (i.e., flood depth), we developed different methods to recover submerged structured ground reference objects (e.g., buildings, lampposts, vehicles, and humans) and their geometric information (e.g., diameters of car wheels, average human heights, and building heights), captured by widely-available crowdsourcing data (Figure 4). For example, to determine the proportion of human body below the water, we not only detect the 17 key points of the human body (e.g., head and knees), but also predict the height of each human object by inferring their age, gender and ethnicity based on facial recognition algorithms, and then using the average height of different groups as height of each human object. For leveraging buildings as reference objects, we automatically downloaded pairs of pre- and post-flood images from social media and Google Street View and used machine learning methods such as Mask R-CNN for building segmentation. As the compared images were taken at different angles and positions, we rectified them to the same angle using a series of image processing techniques to enable comparability.</p>\n<p>To engage citizens in contributing post-disaster photos and disaster-relevant datasets that can be used for flood inundation level detection (Figure 4), we have developed a mobile app prototype based on ESRI ArcGIS AppStudio for users to report and post hurricane relevant information (Figure 5).</p>\n<p>To date, the project has published seven peer-reviewed papers, with additional publications in progress. The scientific findings have been disseminated through conference presentations, workshops organized, invited talks and papers. The source code for the mobile app prototype, which enables users to report and post hurricane-related information, has been publicly released on the <a href=\"https://github.com/mwu233/QuickReport\">GitHub repository</a> (https://github.com/mwu233/QuickReport). The project has supported and trained four graduate students and seven undergraduate students in various fields, such as machine learning, computer vision, image processing, remote sensing, and mobile app development. Specifically, the project has resulted in a Ph.D. thesis on \"Machine Learning and Remote Sensing for Near Realtime Flood Mapping\" and an MS thesis on \"Physics-Informed Weakly Supervised Learning for Near Real-Time Flood Mapping\". Additionally, three undergraduate students have published their first paper as leading authors.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/29/2023<br>\n\t\t\t\t\tModified by: Qunying&nbsp;Huang</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682810899375_Figure1PSNet--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682810899375_Figure1PSNet--rgov-800width.jpg\" title=\"Figure 1. Architecture of the proposed PSNet models and performance tested on the Hurricane Harvey and Florence\"><img src=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682810899375_Figure1PSNet--rgov-66x44.jpg\" alt=\"Figure 1. Architecture of the proposed PSNet models and performance tested on the Hurricane Harvey and Florence\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 1. Architecture of the proposed PSNet models and performance tested on the Hurricane Harvey and Florence</div>\n<div class=\"imageCredit\">Bo Peng</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qunying&nbsp;Huang</div>\n<div class=\"imageTitle\">Figure 1. Architecture of the proposed PSNet models and performance tested on the Hurricane Harvey and Florence</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811033317_Figure2Self-supervised--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811033317_Figure2Self-supervised--rgov-800width.jpg\" title=\"Figure 2. The proposed self-supervised learning framework and model performance tested on the Hurricane Harvey and Florence\"><img src=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811033317_Figure2Self-supervised--rgov-66x44.jpg\" alt=\"Figure 2. The proposed self-supervised learning framework and model performance tested on the Hurricane Harvey and Florence\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 2. The proposed self-supervised learning framework and model performance tested on the Hurricane Harvey and Florence</div>\n<div class=\"imageCredit\">Bo Peng</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qunying&nbsp;Huang</div>\n<div class=\"imageTitle\">Figure 2. The proposed self-supervised learning framework and model performance tested on the Hurricane Harvey and Florence</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811096468_Figure3Weakly-supervised--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811096468_Figure3Weakly-supervised--rgov-800width.jpg\" title=\"Figure 3. The proposed Spatial-Temporal-Spectral knowledge informed weakly supervised learning for flood mapping and model performance tested on the Hurricane Harvey and Florence\"><img src=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811096468_Figure3Weakly-supervised--rgov-66x44.jpg\" alt=\"Figure 3. The proposed Spatial-Temporal-Spectral knowledge informed weakly supervised learning for flood mapping and model performance tested on the Hurricane Harvey and Florence\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 3. The proposed Spatial-Temporal-Spectral knowledge informed weakly supervised learning for flood mapping and model performance tested on the Hurricane Harvey and Florence</div>\n<div class=\"imageCredit\">Bo Peng</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qunying&nbsp;Huang</div>\n<div class=\"imageTitle\">Figure 3. The proposed Spatial-Temporal-Spectral knowledge informed weakly supervised learning for flood mapping and model performance tested on the Hurricane Harvey and Florence</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811722554_Figure4Flood_Depth_Estimation--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811722554_Figure4Flood_Depth_Estimation--rgov-800width.jpg\" title=\"Figure 4. Estimation of flood depth with human (a-d) and building (e-i) objects as reference\"><img src=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811722554_Figure4Flood_Depth_Estimation--rgov-66x44.jpg\" alt=\"Figure 4. Estimation of flood depth with human (a-d) and building (e-i) objects as reference\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 4. Estimation of flood depth with human (a-d) and building (e-i) objects as reference</div>\n<div class=\"imageCredit\">Zonglin Meng, Boyuan Zou, Qunying Huang</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qunying&nbsp;Huang</div>\n<div class=\"imageTitle\">Figure 4. Estimation of flood depth with human (a-d) and building (e-i) objects as reference</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811851762_Figure5MobileApp--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811851762_Figure5MobileApp--rgov-800width.jpg\" title=\"Figure 5. Use the developed mobile app to submit a report\"><img src=\"/por/images/Reports/POR/2023/1940091/1940091_10640724_1682811851762_Figure5MobileApp--rgov-66x44.jpg\" alt=\"Figure 5. Use the developed mobile app to submit a report\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Use the developed mobile app to submit a report</div>\n<div class=\"imageCredit\">Meiliu Wu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qunying&nbsp;Huang</div>\n<div class=\"imageTitle\">Figure 5. Use the developed mobile app to submit a report</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nFlooding is the most frequent natural disaster in the US, accounting for over 75% of federally-declared disasters. Flood impact assessment, including identification of both inundation extent and depth, is critical in both real-time and longer-term applications. This project integrates data from both physical sensor networks (e.g., UAVs), and citizen sensor networks (e.g., social media and citizen contributed data through a developed mobile app), with advanced artificial intelligence (AI), computer vision and spatial analysis techniques to assess flood damage.\n\nTo delineate flooded areas (i.e., inundation extent) in heterogeneous coastal cities, we first developed Patch Similarity Convolutional Neural Network (PSNet) models that allow to mapping floods over densely populated urban areas with only a small number of training samples labeled in less than 1 hour by a geospatial expert (Figure 1). Experiments on the high resolution imagery before and after the urban flooding events (i.e., the 2017 Hurricane Harvey and the 2018 Hurricane Florence) showed that the developed PSNet models can produce urban flood maps with consistently high precision, recall, F1 score, and overall accuracy compared with baseline classification models including support vector machine, decision tree, random forest, and AdaBoost, which were often poor in either precision or recall (Figure 1).\n\nNext, these PSNet models were then enhanced with a self-supervised learning framework for object-based urban flood mapping, eliminating the requirement of labeled data (Figure 2). This is the pioneering research to map heterogeneous urban floods with high resolution using self-supervised object-based image analysis.\n\nMoving forward, we developed various approaches for real-time flood extent mapping at the pixel level through weakly supervised learning techniques without the need of human labelling efforts. Specifically, we developed a weakly-supervised pixel-wise flood mapping method by leveraging multi-temporal remote sensing imagery and image processing techniques (e.g., thresholding, k-means clustering, and edge detection) to create weakly-labeled data for training a bi-temporal U-Net model for flood detection. In addition, we also explored other auxiliary features and datasets, including the FEMA national flood hazard layer (NFHL), and Microsoft building footprints, to automatically sample flooded and non-flooded locations as weak labels for training ML models and mapping floods across the study area (Figure 3).\n\nTo estimate flood inundation level (i.e., flood depth), we developed different methods to recover submerged structured ground reference objects (e.g., buildings, lampposts, vehicles, and humans) and their geometric information (e.g., diameters of car wheels, average human heights, and building heights), captured by widely-available crowdsourcing data (Figure 4). For example, to determine the proportion of human body below the water, we not only detect the 17 key points of the human body (e.g., head and knees), but also predict the height of each human object by inferring their age, gender and ethnicity based on facial recognition algorithms, and then using the average height of different groups as height of each human object. For leveraging buildings as reference objects, we automatically downloaded pairs of pre- and post-flood images from social media and Google Street View and used machine learning methods such as Mask R-CNN for building segmentation. As the compared images were taken at different angles and positions, we rectified them to the same angle using a series of image processing techniques to enable comparability.\n\nTo engage citizens in contributing post-disaster photos and disaster-relevant datasets that can be used for flood inundation level detection (Figure 4), we have developed a mobile app prototype based on ESRI ArcGIS AppStudio for users to report and post hurricane relevant information (Figure 5).\n\nTo date, the project has published seven peer-reviewed papers, with additional publications in progress. The scientific findings have been disseminated through conference presentations, workshops organized, invited talks and papers. The source code for the mobile app prototype, which enables users to report and post hurricane-related information, has been publicly released on the GitHub repository (https://github.com/mwu233/QuickReport). The project has supported and trained four graduate students and seven undergraduate students in various fields, such as machine learning, computer vision, image processing, remote sensing, and mobile app development. Specifically, the project has resulted in a Ph.D. thesis on \"Machine Learning and Remote Sensing for Near Realtime Flood Mapping\" and an MS thesis on \"Physics-Informed Weakly Supervised Learning for Near Real-Time Flood Mapping\". Additionally, three undergraduate students have published their first paper as leading authors.\n\n\t\t\t\t\tLast Modified: 04/29/2023\n\n\t\t\t\t\tSubmitted by: Qunying Huang"
 }
}