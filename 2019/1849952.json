{
 "awd_id": "1849952",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: Active Learning of Preferences for Human-Aware Autonomy",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Juan Wachs",
 "awd_eff_date": "2019-06-01",
 "awd_exp_date": "2022-11-30",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2019-05-20",
 "awd_max_amd_letter_date": "2019-05-20",
 "awd_abstract_narration": "Humans' preferences play a key role in specifying how robotics systems should act, i.e., how an assistive robot arm should move, or how an autonomous car should drive. The learned human preferences are an important element in planning for interactive autonomous systems, e.g., robots collaborating with different types of human teammates, or shared autonomy with a human to efficiently teleoperate a robot arm. One hopes that learning techniques can be used to learn reward functions representing humans' preferences for robotics applications. However, a significant part of the success of learning algorithms can be attributed to the availability of large amounts of labeled data. Unfortunately, collecting and labeling data can be costly and time-consuming in robotics applications. In addition, humans are not always capable of reliably assigning a success value (reward) to a given robot action, and their demonstrations are usually suboptimal due to the difficulty of operating robots with more than a few degrees of freedom. The proposed research develops foundational techniques to address the key challenges of using learning techniques in human-robot interaction. \r\n\r\nThe goal of this project is to develop efficient methods and algorithms to first better model and understand humans preferences while operating, interacting, and collaborating with robots. Furthermore, the investigator will design algorithms that plan for robots that are aware of such preferences and can initiate a safe and seamless interaction with humans. This project involves two main contributions: (1) Developing efficient and active algorithms to learn probabilistic mixture models for humans preferences about how a robot should operate based on comparisons and rankings., and (2) Developing planning algorithms for robots that leverage humans preferences to enable seamless shared autonomy and more efficient human-robot interaction. Preliminary results in the domain of autonomous driving suggest that one can learn driving preferences of humans and this approach can improve efficiency and safety of robotics systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dorsa",
   "pi_last_name": "Sadigh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dorsa Sadigh",
   "pi_email_addr": "dorsa@cs.stanford.edu",
   "nsf_id": "000769760",
   "pi_start_date": "2019-05-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "353 Serra Mall",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943055008",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-362f986b-7fff-dd5e-3561-705ce1004cc7\"> </span></p>\n<p dir=\"ltr\"><span>In this project, we studied the problem of learning human preferences by actively querying them for informative feedback. As part of this project, we have developed active preference-based learning techniques that learn a reward preference-function in robotics domains such as manipulation and navigation. Our work demonstrates how we can learn from pairwise comparisons, rankings, scaled feedback, or how to integrate few-shot and multi-task learning with active preference-based learning. Focusing on the robotics domain, we often are limited to the size of available datasets, so not only we can actively query people, we can also tap into non-traditional sources of data. In this project, we have demonstrated how to learn from offline suboptimal demonstrations as well as other sources such as unstructured play data. The preference reward or policies learned through our techniques can enable more efficient robot policies that can effectively perform complex tasks and coordinate and collaborate with people.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>This project has directly impacted important applications in assistive robotics and autonomous driving that directly impact human life. The algorithms developed as part of this project can enable learning human preferences when teleoperating assistive robotics arms using intuitive interfaces. In addition, the ideas developed in this project were incorporated in our undergraduate and graduate courses directly influencing the education material in interactive robotics lectures. Throughout the course of this project, we have also hosted numerous high school students, undergraduate students, and high school teachers in our lab who contributed to the work developed as part of this project, but also collaborated with us on developing outreach modules that could be used in K-12 education. We have further organized the CS mentoring program at Stanford connecting undergraduate students from underrepresented groups with graduate mentors. Finally, we have actively disseminated our work through lab visits, media coverage, and publishing at top tier robotics and machine learning conferences and journals.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/31/2023<br>\n\t\t\t\t\tModified by: Dorsa&nbsp;Sadigh</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nIn this project, we studied the problem of learning human preferences by actively querying them for informative feedback. As part of this project, we have developed active preference-based learning techniques that learn a reward preference-function in robotics domains such as manipulation and navigation. Our work demonstrates how we can learn from pairwise comparisons, rankings, scaled feedback, or how to integrate few-shot and multi-task learning with active preference-based learning. Focusing on the robotics domain, we often are limited to the size of available datasets, so not only we can actively query people, we can also tap into non-traditional sources of data. In this project, we have demonstrated how to learn from offline suboptimal demonstrations as well as other sources such as unstructured play data. The preference reward or policies learned through our techniques can enable more efficient robot policies that can effectively perform complex tasks and coordinate and collaborate with people.\n\n \nThis project has directly impacted important applications in assistive robotics and autonomous driving that directly impact human life. The algorithms developed as part of this project can enable learning human preferences when teleoperating assistive robotics arms using intuitive interfaces. In addition, the ideas developed in this project were incorporated in our undergraduate and graduate courses directly influencing the education material in interactive robotics lectures. Throughout the course of this project, we have also hosted numerous high school students, undergraduate students, and high school teachers in our lab who contributed to the work developed as part of this project, but also collaborated with us on developing outreach modules that could be used in K-12 education. We have further organized the CS mentoring program at Stanford connecting undergraduate students from underrepresented groups with graduate mentors. Finally, we have actively disseminated our work through lab visits, media coverage, and publishing at top tier robotics and machine learning conferences and journals.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 03/31/2023\n\n\t\t\t\t\tSubmitted by: Dorsa Sadigh"
 }
}