{
 "awd_id": "1902395",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "US-German Data Sharing Proposal: CRCNS Data Sharing: REvealing SPONtaneous Speech Processes in Electrocorticography (RESPONSE)",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jonathan Fritz",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 388060.0,
 "awd_amount": 388060.0,
 "awd_min_amd_letter_date": "2019-01-30",
 "awd_max_amd_letter_date": "2019-01-30",
 "awd_abstract_narration": "The uniquely human capability to produce speech enables swift communication of abstract and substantive information. Currently, nearly two million people in the United States, and far more worldwide, suffer from significant speech production deficits as a result of severe neuromuscular impairments due to injury or disease. In extreme cases, individuals may be unable to speak at all. These individuals would greatly benefit from a device that could alleviate speech deficits and enable them to communicate more naturally and effectively. This project will explore aspects of decoding a user's intended speech directly from the electrical activity of the brain and converting it to synthesized speech that could be played through a loudspeaker in real-time to emulate natural speaking from thought. In particular, this project will uniquely focus on decoding continuous, spontaneous speech processes to achieve more natural and practical communication device for the severely disabled.\r\n\r\nThe complex dynamics of brain activity and the fundamental processing units of continuous speech production and perception are largely unknown, and such dynamics make it challenging to investigate these speech processes with traditional neuroimaging techniques. Electrocorticography (ECoG) measures electrical activity directly from the brain surface and covers an area large enough to provide insights about widespread networks for speech production and understanding, while simultaneously providing localized information for decoding nuanced aspects of the underlying speech processes. Thus, ECoG is instrumental and unparalleled for investigating the detailed spatiotemporal dynamics of speech. The research team's prior work has shown for the first time the detailed spatiotemporal progression of brain activity during prompted continuous speech, and that the team's Brain-to-text system can model phonemes and decode words. However, in pursuit of the ultimate objective of developing a natural speech neuroprosthetic for the severely disabled, research must move beyond studying prompted and isolated aspects of speech. This project will extend the research team's prior experiments to investigate the neural processes of spontaneous and imagined speech production. In conjunction with in-depth analysis of the recorded neural signals, the researchers will apply customized ECoG-based automatic speech recognition (ASR) techniques to facilitate the analysis of the large amount of phones occurring in continuous speech. Ultimately, the project aims to define fundamental units of continuous speech production and understanding, illustrate functional differences between these units, and demonstrate that representations of spontaneous speech can be synthesized directly from the neural recordings. A companion project is being funded by the Federal Ministry of Education and Research, Germany (BMBF)",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dean",
   "pi_last_name": "Krusienski",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dean Krusienski",
   "pi_email_addr": "djkrusienski@vcu.edu",
   "nsf_id": "000500739",
   "pi_start_date": "2019-01-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Commonwealth University",
  "inst_street_address": "910 WEST FRANKLIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "RICHMOND",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "8048286772",
  "inst_zip_code": "232849005",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "VA04",
  "org_lgl_bus_name": "VIRGINIA COMMONWEALTH UNIVERSITY",
  "org_prnt_uei_num": "WXQLZ1PA6XP3",
  "org_uei_num": "MLQFL4JSSAA9"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Commonwealth University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "232980568",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "VA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "732700",
   "pgm_ele_name": "CRCNS-Computation Neuroscience"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7327",
   "pgm_ref_txt": "CRCNS"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 388058.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main objective of this research is to better understand the electrical activity of the brain when producing and imagining speech, and to use this understanding to develop algorithms that decode and synthesize speech directly from brain activity in real-time.&nbsp; This research represents an important step toward the creation of a natural speech neuroprosthetic for the severely disabled.</p>\n<p><strong>Intellectual Merit: </strong></p>\n<p>This work led to several major developments that advance the field of speech neuroprosthetics.&nbsp; A novel speech decoding model was developed, consisting of a deep convolutional neural network architecture that preserves the spatiotemporal structure of the neural signals to estimate the speech spectrogram.&nbsp; The spectrogram is then synthesized into an acoustic waveform using a vocoder.&nbsp; This approach was evaluated offline using data from patients with electrocorticographic electrodes implanted over speech production areas of the brain.&nbsp; The results show that it is possible to decode and synthesize high-quality speech directly from brain activity measured exclusively from these areas on the cortex.&nbsp; This finding is important for the development of real-time speech neuroprosthetics that must rely on causal speech production processes rather than noncausal perception processes.&nbsp; In order to reduce the computational requirements toward real-time implementation, an alternate decoding and synthesis approach was developed using a method known as unit selection.&nbsp; This method stores segments of fundamental speech recordings and the associated brain activity.&nbsp; The incoming brain activity is compared to the stored activity to determine the best match and the speech waveform is reconstructed directly from the corresponding speech segments.&nbsp; This method is capable of synthesizing speech with processing latency under a millisecond and synthesis quality on par with the previous approach.&nbsp; A subsequent study provided the first demonstration of real-time synthesis of imagined speech from intracranial brain activity.&nbsp; In a patient implanted with stereotactic depth electrodes, linear decoding models and a real-time vocoder were employed to synthesize imagined speech in a closed-loop experiment with a latency of approximately 7 ms.&nbsp; While the resulting synthesized speech is not yet intelligible, the method yields natural reconstructions with precise temporal gating of speech and non-speech output. The reconstruction quality is expected to reach the level of the offline approaches given better electrode coverage of relevant speech areas.&nbsp; The resulting decoding models also reveal that speech processes have similar representations when vocalized, whispered, or imagined. This finding is important for developing and training future closed-loop decoding schemes for individuals who have lost the ability to speak.&nbsp; These insights are vital for studying the co-adaptation between the user and the system to optimize the practical usability of the speech neuroprosthetic.</p>\n<p><strong>Broader Impacts: </strong></p>\n<p>The methods developed for real-time neural signal decoding are broadly applicable to other brain-computer interface and neuroprosthetic research domains.&nbsp; As a speech neuroprosthetic, the envisioned technology has the potential to enhance the quality of life for severely disabled patients, their families, and their caregivers by providing a more natural communication channel.&nbsp; It is also conceivable that such approaches can be extended for application to other neurological speech disorders.&nbsp; This successful international collaboration facilitated personnel exchanges and data/resource sharing that directly impacted the project outcomes.&nbsp; The multidisciplinary project involved and supported undergraduate and graduate students in biomedical engineering, electrical engineering, computer engineering, computer science, biology, and medicine.&nbsp; Participating students have developed and configured various hardware and software components, designed and conducted human-subject experiments, performed data analysis, coauthored papers, and given technical presentations.&nbsp; This project produced numerous presentations and peer-reviewed publications, as well as a unique, annotated dataset collected from 36 patients across several institutions.&nbsp; This dataset consists of simultaneous intracranial and acoustic speech recordings collected while patients performed actual, mouthed, whispered, and/or imagined speech as part of a battery of single word, sentence, free-response, and conversational speaking tasks.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/14/2021<br>\n\t\t\t\t\tModified by: Dean&nbsp;Krusienski</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe main objective of this research is to better understand the electrical activity of the brain when producing and imagining speech, and to use this understanding to develop algorithms that decode and synthesize speech directly from brain activity in real-time.  This research represents an important step toward the creation of a natural speech neuroprosthetic for the severely disabled.\n\nIntellectual Merit: \n\nThis work led to several major developments that advance the field of speech neuroprosthetics.  A novel speech decoding model was developed, consisting of a deep convolutional neural network architecture that preserves the spatiotemporal structure of the neural signals to estimate the speech spectrogram.  The spectrogram is then synthesized into an acoustic waveform using a vocoder.  This approach was evaluated offline using data from patients with electrocorticographic electrodes implanted over speech production areas of the brain.  The results show that it is possible to decode and synthesize high-quality speech directly from brain activity measured exclusively from these areas on the cortex.  This finding is important for the development of real-time speech neuroprosthetics that must rely on causal speech production processes rather than noncausal perception processes.  In order to reduce the computational requirements toward real-time implementation, an alternate decoding and synthesis approach was developed using a method known as unit selection.  This method stores segments of fundamental speech recordings and the associated brain activity.  The incoming brain activity is compared to the stored activity to determine the best match and the speech waveform is reconstructed directly from the corresponding speech segments.  This method is capable of synthesizing speech with processing latency under a millisecond and synthesis quality on par with the previous approach.  A subsequent study provided the first demonstration of real-time synthesis of imagined speech from intracranial brain activity.  In a patient implanted with stereotactic depth electrodes, linear decoding models and a real-time vocoder were employed to synthesize imagined speech in a closed-loop experiment with a latency of approximately 7 ms.  While the resulting synthesized speech is not yet intelligible, the method yields natural reconstructions with precise temporal gating of speech and non-speech output. The reconstruction quality is expected to reach the level of the offline approaches given better electrode coverage of relevant speech areas.  The resulting decoding models also reveal that speech processes have similar representations when vocalized, whispered, or imagined. This finding is important for developing and training future closed-loop decoding schemes for individuals who have lost the ability to speak.  These insights are vital for studying the co-adaptation between the user and the system to optimize the practical usability of the speech neuroprosthetic.\n\nBroader Impacts: \n\nThe methods developed for real-time neural signal decoding are broadly applicable to other brain-computer interface and neuroprosthetic research domains.  As a speech neuroprosthetic, the envisioned technology has the potential to enhance the quality of life for severely disabled patients, their families, and their caregivers by providing a more natural communication channel.  It is also conceivable that such approaches can be extended for application to other neurological speech disorders.  This successful international collaboration facilitated personnel exchanges and data/resource sharing that directly impacted the project outcomes.  The multidisciplinary project involved and supported undergraduate and graduate students in biomedical engineering, electrical engineering, computer engineering, computer science, biology, and medicine.  Participating students have developed and configured various hardware and software components, designed and conducted human-subject experiments, performed data analysis, coauthored papers, and given technical presentations.  This project produced numerous presentations and peer-reviewed publications, as well as a unique, annotated dataset collected from 36 patients across several institutions.  This dataset consists of simultaneous intracranial and acoustic speech recordings collected while patients performed actual, mouthed, whispered, and/or imagined speech as part of a battery of single word, sentence, free-response, and conversational speaking tasks.\n\n \n\n\t\t\t\t\tLast Modified: 10/14/2021\n\n\t\t\t\t\tSubmitted by: Dean Krusienski"
 }
}