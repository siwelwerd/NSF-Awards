{
 "awd_id": "1931537",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Frameworks: Designing Next-Generation MPI Libraries for Emerging Dense GPU Systems",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927116",
 "po_email": "sghafoor@nsf.gov",
 "po_sign_block_name": "Sheikh Ghafoor",
 "awd_eff_date": "2019-11-01",
 "awd_exp_date": "2023-10-31",
 "tot_intn_awd_amt": 1360534.0,
 "awd_amount": 1360534.0,
 "awd_min_amd_letter_date": "2019-07-23",
 "awd_max_amd_letter_date": "2019-07-23",
 "awd_abstract_narration": "The extremely high compute and communication capabilities offered by modern Graphics Processing Units (GPUs) and high-performance interconnects have led to the creation of High-Performance Computing (HPC) platforms with multiple GPUs and high-performance interconnects per node. Unfortunately, state-of-the-art production quality implementations of the popular Message Passing Interface (MPI) programming model do not have the appropriate support to deliver the best performance and scalability for applications on such dense GPU systems. These developments in High-End Computing (HEC) technologies and associated middleware issues lead to the following broad challenge: How can existing production quality MPI middleware be enhanced to take advantage of emerging networking technologies to deliver the best possible scale-up and scale-out for HPC and Deep Learning (DL) applications on emerging dense GPU systems? A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions. The proposed framework will be made available to collaborators and the broader scientific community to understand the impact of the proposed innovations on next-generation HPC and DL frameworks and applications in various science domains.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The proposed work will enable curriculum advancements via research in pedagogy for key courses in the new Data Science programs at OSU, SDSC and TACC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials and workshops will be organized at PEARC, SC and other conferences to share the research results and experience with the community. The project is aligned with the National Strategic Computing Initiative (NSCI) to advance US leadership in HPC and the recent initiative of the US Government to maintain leadership in Artificial Intelligence (AI.)\r\n\r\nThe proposed innovations include: 1) Designing high-performance and scalable point-to-point, and collective communication operations that fully utilize multiple network adapters and advanced in-network computing features for GPU and CPU buffers within and across nodes; 2) Designing novel datatype processing and unified memory management to improve application performance; 3) Designing CUDA-aware I/O subsystem to accelerate MPI I/O and checkpoint-restart for HPC and DL applications; 4) Designing support for containerized environments to better enable easy deployment of proposed solutions on modern cloud environments; and 5) Carry out integrated development and evaluation to ensure proper integration of proposed designs with the driving applications. The proposed designs will be integrated into the widely-used MVAPICH2 library and made available. The project team members will work closely with internal and external collaborators to facilitate wide deployment and adoption of released software. The proposed solutions will be targeted to enable scale-up and scale-out of the driving science domains (molecular dynamics, lattice QCD, seismology, image classification, and fusion research) on emerging dense GPU platforms. The transformative impact of the proposed development effort is to achieve scalability, performance, and portability out of HPC and DL frameworks and applications to take advantage of emerging dense GPU platforms and hence, leading to significant advancements in science and engineering.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dhabaleswar",
   "pi_last_name": "Panda",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Dhabaleswar K Panda",
   "pi_email_addr": "panda.2@osu.edu",
   "nsf_id": "000487085",
   "pi_start_date": "2019-07-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Karen",
   "pi_last_name": "Tomko",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Karen A Tomko",
   "pi_email_addr": "ktomko@osc.edu",
   "nsf_id": "000330142",
   "pi_start_date": "2019-07-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Hari",
   "pi_last_name": "Subramoni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hari Subramoni",
   "pi_email_addr": "subramoni.1@osu.edu",
   "nsf_id": "000704577",
   "pi_start_date": "2019-07-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Samuel",
   "pi_last_name": "Khuvis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Samuel Khuvis",
   "pi_email_addr": "skhuvis@osc.edu",
   "nsf_id": "000779446",
   "pi_start_date": "2019-07-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "077Z",
   "pgm_ref_txt": "CSSI-1: Cyberinfr for Sustained Scientif"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 1360534.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-ec3a4e83-7fff-18ca-b351-0605315fceb5\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">During recent years, there have been significant advances in computing, networking, and storage technologies for High-Performance Computing (HPC) systems.&nbsp; The frontiers of computation and communication are being driven by the technological advancements of many-cores architectures such as NVIDIA GPUs. The extremely high compute and communication capabilities offered by modern GPUs have led to the creation of HPC platforms with multiple GPUs. On the other hand, HPC and Deep Learning (DL) applications are typically computationally intensive and benefit greatly by an increase in computing power. With the increased availability of GPUs in high performance computing systems, several researchers have been taking advantage of GPUs to accelerate the computation phases of their HPC and DL applications.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The success of modern DL frameworks has been greatly accelerated by using GPUs that have become the de facto choice for training large complex DL systems. As the computational challenges for HPC and DL frameworks are being resolved by using advanced hardware architectures, these frameworks are heading towards the use of larger datasets using the capabilities offered by modern systems.&nbsp; To enable this, parallel programming models used by applications should be enhanced to achieve both scale-up and scale-out together with performance and portability on modern HPC systems.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The Message-Passing Interface (MPI) has dominated the supercomputing landscape, as being the primary parallel programming model of choice for large-scale parallelism because of its support for distributed memory communication. Through the reliance on MPI, traditional HPC applications in various scientific domains (e.g., molecular dynamics, weather prediction, etc.) are taking advantage of the advances in HPC technologies for several decades. With the increased popularity of DL applications and recent advances in high performance computing systems, the community is moving towards using HPC machines for machine learning, deep learning, and data analytics projects. This movement has resulted in popular DL frameworks such as TensorFlow moving to MPI to enable efficient scale-out on modern HPC platforms.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Although fundamental research has been done in the literature to take advantage of recent advances in high performance computing systems for DL applications, neither current generation communication middleware nor applications are able to fully take advantage of these advances due to the lack of availability of these solutions in state-of-the-art production quality middleware. This brings up this challenge: How can existing production quality MPI middleware be enhanced to take advantage of emerging state-of-the-art technologies to deliver the best possible scale-up and scale-out for HPC and DL applications on emerging dense GPU systems?</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In order to tackle the outlined challenges for this project, we have adopted a multi-year/multi-tiered approach that utilizes the underlying system architecture to improve scalability of traditional HPC and DL frameworks/applications on dense GPU systems with state-of-the-art interconnects. Challenges have been addressed along the following directions:</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1. Design efficient point-to-point communication operations on GPU-based clusters</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">2. Optimize collective communication operations on dense GPU-based clusters</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">3. Architect communication libraries for novel and emerging accelerator devices</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">4. Carrying out in-depth study of the new designs with a range of computing and networking technologies.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">5. Co-designing a set of applications (HPC and ML/DL) with the new runtimes and studied performance and scalability on a set of contemporary multi-petaflop systems.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">6. Deploying the new frameworks and runtimes on various HPC systems at Ohio Supercomputer Center (OSC), Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and carrying out continuous engagement with their users to improve and optimize the designs and deliver better performance and scalability for a large number of applications.</span></p>\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The results of this research (new designs, performance results, benchmarks, etc.) have been made available to the community through the MVAPICH2 MPI libraries. Multiple releases of these libraries have been made during the project period. More than 200,000 copies of the MVAPICH2 MPI libraries have been downloaded from the project's web site during this project period. In each of these releases, features, performance numbers and scalability information have been shared with the MVAPICH user community through mailing lists and the project's web site.&nbsp; In addition to the software distribution, the results have been presented at various conferences and journals and events through Keynote talks, invited talks, tutorials, and hands-on sessions.&nbsp; The research has also led to the thesis for several M.S. and Ph.D. students.</span></p><br>\n<p>\n Last Modified: 02/27/2024<br>\nModified by: Dhabaleswar&nbsp;K&nbsp;Panda</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nDuring recent years, there have been significant advances in computing, networking, and storage technologies for High-Performance Computing (HPC) systems. The frontiers of computation and communication are being driven by the technological advancements of many-cores architectures such as NVIDIA GPUs. The extremely high compute and communication capabilities offered by modern GPUs have led to the creation of HPC platforms with multiple GPUs. On the other hand, HPC and Deep Learning (DL) applications are typically computationally intensive and benefit greatly by an increase in computing power. With the increased availability of GPUs in high performance computing systems, several researchers have been taking advantage of GPUs to accelerate the computation phases of their HPC and DL applications.\n\n\nThe success of modern DL frameworks has been greatly accelerated by using GPUs that have become the de facto choice for training large complex DL systems. As the computational challenges for HPC and DL frameworks are being resolved by using advanced hardware architectures, these frameworks are heading towards the use of larger datasets using the capabilities offered by modern systems. To enable this, parallel programming models used by applications should be enhanced to achieve both scale-up and scale-out together with performance and portability on modern HPC systems.\n\n\nThe Message-Passing Interface (MPI) has dominated the supercomputing landscape, as being the primary parallel programming model of choice for large-scale parallelism because of its support for distributed memory communication. Through the reliance on MPI, traditional HPC applications in various scientific domains (e.g., molecular dynamics, weather prediction, etc.) are taking advantage of the advances in HPC technologies for several decades. With the increased popularity of DL applications and recent advances in high performance computing systems, the community is moving towards using HPC machines for machine learning, deep learning, and data analytics projects. This movement has resulted in popular DL frameworks such as TensorFlow moving to MPI to enable efficient scale-out on modern HPC platforms.\n\n\nAlthough fundamental research has been done in the literature to take advantage of recent advances in high performance computing systems for DL applications, neither current generation communication middleware nor applications are able to fully take advantage of these advances due to the lack of availability of these solutions in state-of-the-art production quality middleware. This brings up this challenge: How can existing production quality MPI middleware be enhanced to take advantage of emerging state-of-the-art technologies to deliver the best possible scale-up and scale-out for HPC and DL applications on emerging dense GPU systems?\n\n\nIn order to tackle the outlined challenges for this project, we have adopted a multi-year/multi-tiered approach that utilizes the underlying system architecture to improve scalability of traditional HPC and DL frameworks/applications on dense GPU systems with state-of-the-art interconnects. Challenges have been addressed along the following directions:\n\n\n1. Design efficient point-to-point communication operations on GPU-based clusters\n\n\n2. Optimize collective communication operations on dense GPU-based clusters\n\n\n3. Architect communication libraries for novel and emerging accelerator devices\n\n\n4. Carrying out in-depth study of the new designs with a range of computing and networking technologies.\n\n\n5. Co-designing a set of applications (HPC and ML/DL) with the new runtimes and studied performance and scalability on a set of contemporary multi-petaflop systems.\n\n\n6. Deploying the new frameworks and runtimes on various HPC systems at Ohio Supercomputer Center (OSC), Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and carrying out continuous engagement with their users to improve and optimize the designs and deliver better performance and scalability for a large number of applications.\n\n\nThe results of this research (new designs, performance results, benchmarks, etc.) have been made available to the community through the MVAPICH2 MPI libraries. Multiple releases of these libraries have been made during the project period. More than 200,000 copies of the MVAPICH2 MPI libraries have been downloaded from the project's web site during this project period. In each of these releases, features, performance numbers and scalability information have been shared with the MVAPICH user community through mailing lists and the project's web site. In addition to the software distribution, the results have been presented at various conferences and journals and events through Keynote talks, invited talks, tutorials, and hands-on sessions. The research has also led to the thesis for several M.S. and Ph.D. students.\t\t\t\t\tLast Modified: 02/27/2024\n\n\t\t\t\t\tSubmitted by: DhabaleswarKPanda\n"
 }
}