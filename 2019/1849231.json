{
 "awd_id": "1849231",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: FND: COLLAB: Learning from Stories: Practical Value Alignment and Taskability for Autonomous Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2019-06-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 291307.0,
 "awd_amount": 291307.0,
 "awd_min_amd_letter_date": "2019-05-17",
 "awd_max_amd_letter_date": "2019-05-17",
 "awd_abstract_narration": "In the near future we are likely to see increasingly-capable autonomous systems operating in proximity to humans and immersed in society. As these systems become more sophisticated, they will interact increasingly with humans. With this increased human-agent interaction comes an increased obligation to ensure that autonomous systems do not cause even unintentional harm to a human. Creating systems that cannot intentionally or unintentionally harm humans in not an easy task. This is because there are infinitely many undesirable outcomes that can be achieved in an open world, making it impossible to instruct these systems to avoid each one. If the desired behavior cannot be directly specified, then it must be learned. Past approaches to learn these types of behaviors have focused on learning from human examples, but these methods are unlikely to scale. This research uses natural language explanations of behavior as a scalable alternative for training autonomous agents for safe operation. Naturalistic descriptions contain vast amounts of information about sociocultural norms, which make them rich sources for such training. Enabling systems to better understand and learn from such descriptions will enable human operators to more naturally specify goals or tasks for the agent to complete.\r\n\r\nThis research explores the concept of learning via natural language descriptions of desired behavior. This technique uses procedural knowledge contained in natural language explanations to help train autonomous agents. Concretely, this approach learns utility functions that can be used to guide autonomous agents towards behaviors that are aligned with the description used for training. To accomplish this, researchers will create computational models capable of extracting both knowledge about sociocultural norms as well as procedural knowledge from naturally occurring corpora. These models will then be used to create behavior policies that are both aligned with sociocultural norms and procedurally plausible. To further ensure that these models can be practically deployed, researchers will enable their models to incorporate a \"human in the loop\" to provide online feedback about the quality of these learned behavior policies in terms of their social acceptability and appropriateness. Safeguards will also be investigated to protect the learned behavior policies against the effects of adversarial or malicious training examples.\r\n\r\nThis award is jointly funded by the Division of Information and Intelligent Systems in the Directorate for Computer & Information Science & Engineering and the Established Program to Stimulate Competitive Research (EPSCoR) in the Office of Integrative Activities.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brent",
   "pi_last_name": "Harrison",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brent Harrison",
   "pi_email_addr": "harrison@cs.uky.edu",
   "nsf_id": "000739149",
   "pi_start_date": "2019-05-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Kentucky Research Foundation",
  "inst_street_address": "500 S LIMESTONE",
  "inst_street_address_2": "109 KINKEAD HALL",
  "inst_city_name": "LEXINGTON",
  "inst_state_code": "KY",
  "inst_state_name": "Kentucky",
  "inst_phone_num": "8592579420",
  "inst_zip_code": "405260001",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "KY06",
  "org_lgl_bus_name": "UNIVERSITY OF KENTUCKY RESEARCH FOUNDATION, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "H1HYA8Z1NTM5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Kentucky Research Foundation",
  "perf_str_addr": "500 S Limestone 109 Kinkead Hall",
  "perf_city_name": "Lexington",
  "perf_st_code": "KY",
  "perf_st_name": "Kentucky",
  "perf_zip_code": "405260001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "KY06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 291307.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this work, we created a practical pipeline for using stories to train value aligned agents in virtual worlds. The first stage of the pipeline involves using transformer models to extract value information from stories. To show this, we trained various transformer models on a dataset containing Goofus and Gallant comic strips and showed that these models have the capability of learning value information from natural language text contained in the comics. We also showed that this knowledge can generalize to other domains by showing that our Goofus and Gallant value model can achieve a high zero-shot accuracy on two other datasets: one containing science fiction stories, and another containing stories from the book, Plotto.</p>\n<p>We then evaluated various methodologies for integrating this model into the reinforcement learning process. This, which was one of the first works to do so at the time, involved showing how the Goofus and Gallant model could be used to influence the reinforcement learning process during training. To that end, we developed two techniques. The first of these is based on Policy Shaping, which involves biasing the reinforcement learning search process to prefer states that align with the values encoded in the Goofus and Gallant Model. The second approach is a modification of the actor critic framework in which the Goofus and Gallant model is used as a third module that provides feedback on each state that is visited along with the critic. We showed that each of these techniques works to learn value aligned policies in a variety of text-based learning environments that we developed.</p>\n<p>Finally, we also developed techniques that enable us to learn fine-grained value information from naturally occurring stories. We augmented the transformer models used to learn the initial Goofus and Gallant model to work with multi-label value information. This information, which we called principles, gives a more nuanced view of a value, and provides more information about the event in question. To evaluate this, we extended the Goofus and Gallant dataset to contain principle information, and showed that our method could predict, with high accuracy, the values involved in Goofus and Gallant comics.</p>\n<p>This work has had many broader impacts. First and foremost, this work has provided other researchers with valuable test environments and datasets to aid in value alignment research. We believe that the Goofus and Gallant dataset, in particular, is unique in that it represents a dataset that is well curated, but is small in nature. This is very different from many similar datasets currently available. In addition, this project helped train a graduate researcher who was able to successfully receive their Ph.D. using the work done in this project. Thus, it has helped train the next generation of computer scientists and autonomous agents researchers.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/28/2023<br>\n\t\t\t\t\tModified by: Brent&nbsp;Harrison</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this work, we created a practical pipeline for using stories to train value aligned agents in virtual worlds. The first stage of the pipeline involves using transformer models to extract value information from stories. To show this, we trained various transformer models on a dataset containing Goofus and Gallant comic strips and showed that these models have the capability of learning value information from natural language text contained in the comics. We also showed that this knowledge can generalize to other domains by showing that our Goofus and Gallant value model can achieve a high zero-shot accuracy on two other datasets: one containing science fiction stories, and another containing stories from the book, Plotto.\n\nWe then evaluated various methodologies for integrating this model into the reinforcement learning process. This, which was one of the first works to do so at the time, involved showing how the Goofus and Gallant model could be used to influence the reinforcement learning process during training. To that end, we developed two techniques. The first of these is based on Policy Shaping, which involves biasing the reinforcement learning search process to prefer states that align with the values encoded in the Goofus and Gallant Model. The second approach is a modification of the actor critic framework in which the Goofus and Gallant model is used as a third module that provides feedback on each state that is visited along with the critic. We showed that each of these techniques works to learn value aligned policies in a variety of text-based learning environments that we developed.\n\nFinally, we also developed techniques that enable us to learn fine-grained value information from naturally occurring stories. We augmented the transformer models used to learn the initial Goofus and Gallant model to work with multi-label value information. This information, which we called principles, gives a more nuanced view of a value, and provides more information about the event in question. To evaluate this, we extended the Goofus and Gallant dataset to contain principle information, and showed that our method could predict, with high accuracy, the values involved in Goofus and Gallant comics.\n\nThis work has had many broader impacts. First and foremost, this work has provided other researchers with valuable test environments and datasets to aid in value alignment research. We believe that the Goofus and Gallant dataset, in particular, is unique in that it represents a dataset that is well curated, but is small in nature. This is very different from many similar datasets currently available. In addition, this project helped train a graduate researcher who was able to successfully receive their Ph.D. using the work done in this project. Thus, it has helped train the next generation of computer scientists and autonomous agents researchers.\n\n\t\t\t\t\tLast Modified: 09/28/2023\n\n\t\t\t\t\tSubmitted by: Brent Harrison"
 }
}