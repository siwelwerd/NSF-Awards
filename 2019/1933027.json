{
 "awd_id": "1933027",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Efficiently Distributing Optimization over Large-Scale Networks",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": "7032925394",
 "po_email": "rnash@nsf.gov",
 "po_sign_block_name": "Richard Nash",
 "awd_eff_date": "2019-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 299952.0,
 "awd_amount": 299952.0,
 "awd_min_amd_letter_date": "2019-08-12",
 "awd_max_amd_letter_date": "2019-08-12",
 "awd_abstract_narration": "This project will design new algorithms for distributed optimization which can work without any kind of central coordinator or processor server and whose asymptotic performance always improves in larger networks. The algorithms will run over communication networks based on peer-to-peer nearest neighbor with connectivity backbones that can vary with time. Our model will explicitly account for asynchrony, communication delays, message losses, and unpredictable downtime, which are common in real-world in distributed computing; nevertheless, despite all these phenomena, the asymptotic performance of our methods will be identical to the best centralized method with the same computational power as the entire distributed network. Because larger networks of identical processors have more total computational power, this will mean that performance is better in larger networks.  This is to be contrasted with the current state of the art, where the performance of existing algorithms typically gets more sluggish as the size of the network increases due to the difficulty of coordination across a large network. A variety of outreach activities related to the project are planned, including incorporation of the results into undergraduate and graduate education.\r\n\r\nWhile distributed optimization has been used in a plethora of applications in control and network science over the past decade, few of these applications have been large-scale in the sense of reaching into tens of thousands of nodes. In part this is because convergence times in distributed optimization tend to grow with the inverse spectral gap of the underlying network, and this can scale poorly with the number of nodes; as a result, large networks experience slowdowns in performance compared with smaller ones. For example, the inverse spectral gap of the Laplacian on the line network grows quadratically on the number of nodes; on a 2D grid, the same inverse spectral gap will grow linearly with the number of nodes. Any time such inverse spectral gaps appear in expressions for convergence times, they hide polynomial factors of the total number of nodes. This project will create techniques for overcoming this barrier. By putting together new analysis of inexact gradient oracles (which bound the performance of first-order optimization methods when the gradients can only be computed with error) with small-gain type arguments which interconnect chains of relations among variables in the system (effectively demonstrating that errors throughout the network have less of an effect with time), we will design new algorithms whose performance, after a transient, does not depend at all on the underlying network. This implies there is effectively no cost to distributing the method over the network, provided the method runs for long enough.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Olshevsky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander Olshevsky",
   "pi_email_addr": "alexols@bu.edu",
   "nsf_id": "000617353",
   "pi_start_date": "2019-08-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "881 COMMONWEALTH AVE",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151300",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760700",
   "pgm_ele_name": "EPCN-Energy-Power-Ctrl-Netwrks"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "092E",
   "pgm_ref_txt": "Control systems & applications"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 299952.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project focused on developing new algorithms for distributed optimization, a technique widely used in many applications throughout machine learning. Unlike traditional methods, which often rely on a central coordinator, this project aimed to create algorithms that operate efficiently in large, decentralized networks without a central processor.&nbsp;A significant challenge in distributed optimization is the performance drop in large-scale networks. Traditional methods typically experience slower convergence times as the network size increases, making them less effective for large-scale applications.</p>\n<p><br />The project successfully overcame this barrier. The team developed a method for stochastic gradient descent (a popular optimization technique) that works effectively in networks with time-varying connectivity, asynchronous communication, and even message losses. The performance of this method matches that of the best centralized approaches, meaning that it performs equally well compared to systems with a central processing unit handling the entire network's computation.&nbsp;The team's technical approach drew from advances in multi-agent control and involved sophisticated analysis of consensus processes &ndash; how agreement or coordination is reached among decentralized nodes or agents. &nbsp;Additionally, insights were provided into the transient period &ndash; the time it takes for the distributed method to reach optimal performance. They established that this duration is related to the network's structure and depends on specific network characteristics. This finding offers a clear understanding of how long it takes for optimal performance to be achieved in different network topologies.</p><br>\n<p>\n Last Modified: 12/08/2023<br>\nModified by: Alexander&nbsp;V&nbsp;Olshevsky</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1933027/1933027_10632587_1702075843419_Screen_Shot_2023_12_08_at_5.46.24_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1933027/1933027_10632587_1702075843419_Screen_Shot_2023_12_08_at_5.46.24_PM--rgov-800width.png\" title=\"Transient Time of Gradient Descent on Ring Graph\"><img src=\"/por/images/Reports/POR/2023/1933027/1933027_10632587_1702075843419_Screen_Shot_2023_12_08_at_5.46.24_PM--rgov-66x44.png\" alt=\"Transient Time of Gradient Descent on Ring Graph\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This plots of the transient time of stochastic gradient descent on the ring graph against a bound we have derived.</div>\n<div class=\"imageCredit\">A sharp estimate on the transient time of distributed stochastic gradient descent, Pu, Olshevsky, Paschalidis, IEEE Transactions on Automatic Control, 2021.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Alexander&nbsp;V&nbsp;Olshevsky\n<div class=\"imageTitle\">Transient Time of Gradient Descent on Ring Graph</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1933027/1933027_10632587_1702077135707_Screen_Shot_2023_12_08_at_5.46.30_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1933027/1933027_10632587_1702077135707_Screen_Shot_2023_12_08_at_5.46.30_PM--rgov-800width.png\" title=\"Transient Time of Distributed Stochastic Gradient Descent on the Grid Graph\"><img src=\"/por/images/Reports/POR/2023/1933027/1933027_10632587_1702077135707_Screen_Shot_2023_12_08_at_5.46.30_PM--rgov-66x44.png\" alt=\"Transient Time of Distributed Stochastic Gradient Descent on the Grid Graph\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Transient time of Distributed Stochastic Gradient Descent on the grid graph vs a bound we have derived. Here transient time is the time until the linear speedup phenomenon kicks in.</div>\n<div class=\"imageCredit\">A sharp estimate on the transient time of distributed stochastic gradient descent, Pu, Olshevsky, Paschalidis, IEEE Transactions on Automatic Control, 2021.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Alexander&nbsp;V&nbsp;Olshevsky\n<div class=\"imageTitle\">Transient Time of Distributed Stochastic Gradient Descent on the Grid Graph</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1933027/1933027_10632587_1702077473348_Screen_Shot_2023_12_08_at_5.47.08_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1933027/1933027_10632587_1702077473348_Screen_Shot_2023_12_08_at_5.47.08_PM--rgov-800width.png\" title=\"Network-Size vs Speedup\"><img src=\"/por/images/Reports/POR/2023/1933027/1933027_10632587_1702077473348_Screen_Shot_2023_12_08_at_5.47.08_PM--rgov-66x44.png\" alt=\"Network-Size vs Speedup\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Performance of one-shot averaging vs size of network. The graph shows the clear benefit of adding more nodes.</div>\n<div class=\"imageCredit\">Communication-efficient sgd: From local sgd to one-shot averaging, Spiridonoff, Olshevsky, Paschalidis, NeurIPS 2021</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Alexander&nbsp;V&nbsp;Olshevsky\n<div class=\"imageTitle\">Network-Size vs Speedup</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project focused on developing new algorithms for distributed optimization, a technique widely used in many applications throughout machine learning. Unlike traditional methods, which often rely on a central coordinator, this project aimed to create algorithms that operate efficiently in large, decentralized networks without a central processor.A significant challenge in distributed optimization is the performance drop in large-scale networks. Traditional methods typically experience slower convergence times as the network size increases, making them less effective for large-scale applications.\n\n\n\nThe project successfully overcame this barrier. The team developed a method for stochastic gradient descent (a popular optimization technique) that works effectively in networks with time-varying connectivity, asynchronous communication, and even message losses. The performance of this method matches that of the best centralized approaches, meaning that it performs equally well compared to systems with a central processing unit handling the entire network's computation.The team's technical approach drew from advances in multi-agent control and involved sophisticated analysis of consensus processes  how agreement or coordination is reached among decentralized nodes or agents. Additionally, insights were provided into the transient period  the time it takes for the distributed method to reach optimal performance. They established that this duration is related to the network's structure and depends on specific network characteristics. This finding offers a clear understanding of how long it takes for optimal performance to be achieved in different network topologies.\t\t\t\t\tLast Modified: 12/08/2023\n\n\t\t\t\t\tSubmitted by: AlexanderVOlshevsky\n"
 }
}