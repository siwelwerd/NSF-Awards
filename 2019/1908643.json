{
 "awd_id": "1908643",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FET: Small: DNA-based Neural Networks That Learn From Their Environment",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032928649",
 "po_email": "mbasu@nsf.gov",
 "po_sign_block_name": "Mitra Basu",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 499998.0,
 "awd_amount": 499998.0,
 "awd_min_amd_letter_date": "2019-07-05",
 "awd_max_amd_letter_date": "2019-07-05",
 "awd_abstract_narration": "The fundamental advantage of DNA circuits, in comparison to electronic circuits, is their capability to detect and act on information in a molecular environment. A significant challenge in engineered molecular systems, DNA circuits included, is embedded learning and adaptive behavior, which is proven to be powerful and pervasive in biology and is promised to open up many doors in molecular technologies. Currently, once built, a DNA circuit always has a fixed function for how to respond to the environment, which means the same input will always trigger the same output. Some DNA circuits are reconfigurable, for example DNA neural networks, but only in the sense that a human user can choose to mix different molecules with desired concentrations for making circuits that perform different tasks. Much effort has been devoted to the design of DNA circuits with embedded learning capabilities. However, successful experimental demonstration has so far been lacking. In this project, the team of researchers will establish new circuit architectures for self-reconfigurable DNA neural networks, and show that a molecular circuit can improve how well it performs a task in a test tube without human intervention. This kind of self-improvement through learning, both supervised and unsupervised, will allow synthetic molecular circuits to gain the adaptive power previously only seen in biology, laying out the foundation for future applications in smart medicine and materials. Moreover, the results will provide experimental evidence for a pure chemical system to spontaneously undergo self-improvement, which will support the hypothesis for learning to guide and effectively accelerate evolution at the origins of life. The scientific understanding will be incorporated into open online software tools, making it accessible to general public and promoting the applications of information-processing molecular circuits. Design principles and wet-lab constructions of DNA neural networks will be introduced into the classroom, and course materials will be shared outside of the researchers' home institution. Students and postdocs will be engaged in interdisciplinary research, with an emphasis to involve more women in science. Lab tours will be provided to local college students, including underrepresented groups. Communications with general public will be facilitated by public talks, news stories, and artistic illustrations and animations of the research.  \r\n\r\nThe function of winner-take-all DNA neural networks depends on the concentrations of the weight molecules, which encode the memories that an input pattern is compared with for classifying the pattern. In this project, the weight molecules are designed to be initially inactive, and an appropriate collection of them will become activated when a training pattern and a label strand (indicating which class the pattern is) are simultaneously present in supervised learning. The weight-activation process will be implemented using allosteric toehold strand displacement reactions. Over the course of learning, different sets of input strands representing different training patterns will be sequentially added to the test tube that contains a DNA neural network. Each set of input strands will trigger a response of the neural network to adjust its weights and thus improve its capability for recognizing similar patterns. In unsupervised learning, the DNA neural networks are capable of restoring desired concentrations of circuit components after each round of computation and adjusting the concentrations of active weight molecules based on the circuit output rather than a given class label. The DNA neural networks will be trained and tested using a well-defined and understood task, handwritten digit recognition, to evaluate the complexity and diversity of molecular patterns that a DNA neural network is capable of learning and processing. A software tool will be developed for the design and analysis of DNA neural networks with learning capabilities.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lulu",
   "pi_last_name": "Qian",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lulu Qian",
   "pi_email_addr": "luluqian@caltech.edu",
   "nsf_id": "000608153",
   "pi_start_date": "2019-07-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "California Institute of Technology",
  "inst_street_address": "1200 E CALIFORNIA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "PASADENA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6263956219",
  "inst_zip_code": "911250001",
  "inst_country_name": "United States",
  "cong_dist_code": "28",
  "st_cong_dist_code": "CA28",
  "org_lgl_bus_name": "CALIFORNIA INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "U2JMKHNS5TG4"
 },
 "perf_inst": {
  "perf_inst_name": "California Institute of Technology",
  "perf_str_addr": "1200 E. California Blvd.",
  "perf_city_name": "Pasadena",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "911250001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "28",
  "perf_st_cong_dist": "CA28",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "089Y00",
   "pgm_ele_name": "FET-Fndtns of Emerging Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7931",
   "pgm_ref_txt": "COMPUTATIONAL BIOLOGY"
  },
  {
   "pgm_ref_code": "7946",
   "pgm_ref_txt": "BIO COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we showed that a test tube of DNA molecules can be programmed to perform supervised learning in winner-take-all neural networks. We developed a new molecular circuit architecture for implementing adaptive memories: the molecular system was exposed to examples of what it may encounter and examples of the desired response; this information was used to improve its capability for handling similar situations in the future. We successfully demonstrated that the same set of molecules representing a blank memory can be trained to classify different sets of MNIST patterns. This work begins to address the question of how chemical systems could gain the capability to learn from an unknown environment, helps establish a deeper connection between natural algorithms and engineered molecular systems, and opens up the possibility of artificial molecular machines that can learn to perform different tasks when they are in action.</p>\n<p>Furthermore, we developed a general-purpose circuit architecture that uses heat to power different types of enzyme-free molecular circuits, enabling them to process information and make decisions in response to a changing molecular environment. We showed that several molecular building blocks can be designed to respond to a quick temperature ramp and hundreds of distinct molecules composing a complex system can spontaneously reconfigure to their original structures within minutes. We demonstrated reusable DNA logic circuits and neural networks that carried out up to 16 rounds of computation with distinct input combinations, while the circuit performance remained roughly constant over time. This work introduces a scalable approach for sustained operation of enzyme-free molecular circuits and serves as an essential step toward unsupervised learning in autonomous molecular machines.</p>\n<p>The investigation of implementing learning in DNA neural networks was much more challenging than implementing pattern recognition tasks alone. It was unclear whether the additional functions can be carried out by motifs as simple and robust as previously used in DNA neural networks. To explore this question in a more general context in theory, we asked whether arbitrary chemical reaction networks (CRNs) can be implemented using fuel molecules that each contains no more than two strands. We found that the answer is yes and showed such an implementation using a novel cooperative four-way strand displacement motif. We also showed that the implementation has a few other desired properties including physical reversibility and thus energy efficiency and is provably correct based on CRN bisimulation. On the experimental side, we developed a new type of DNA catalyst that we call a cooperative catalyst. Two signal species, interpreted as an activator and input, both exhibit catalytic control for output production. The design involves just a double-stranded gate species and a single-stranded fuel species, as simple as the previously developed DNA catalyst with no allosteric control. The simplicity and modularity of the design make the cooperative DNA catalyst a useful motif for supervised and unsupervised learning; it could also be used for general-purpose computation and dynamics.</p>\n<p>Expanding on the function of previously developed DNA neural networks, we developed a new circuit architecture for implementing loser-take-all neural networks. A loser-take-all function is defined as follows: an output signal is ON if and only if the corresponding input has the smallest analog value among all inputs. We experimentally demonstrated a three-input loser-take-all circuit with nine unique input combinations. Complementary to winner-take-all, loser-take-all DNA circuits could be used for recognition of molecular patterns based on their least similarities to a set of memories, allowing classification decisions for patterns that are extremely noisy and cannot be recognized as a single class. Moreover, the design principle of loser-take-all could be more generally applied in other DNA circuit implementations including k-winner-take-all, opening up possibilities for learning in more diverse DNA neural networks.</p>\n<p>This project has helped support one postdoctoral scholar, one graduate student, and two undergraduate students. All four of them have presented their work at international conferences. The PI has developed a new course incorporating the concepts of molecular neural networks. In this course, undergraduates in Bioengineering, Computer Science, and Computation and Neural Systems proposed and conducted open-ended research projects, resulting in first-author publications. Most of these undergraduates are women and underrepresented minorities and they have gone on to pursue molecular programming research in graduate school.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/05/2023<br>\n\t\t\t\t\tModified by: Lulu&nbsp;Qian</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this project, we showed that a test tube of DNA molecules can be programmed to perform supervised learning in winner-take-all neural networks. We developed a new molecular circuit architecture for implementing adaptive memories: the molecular system was exposed to examples of what it may encounter and examples of the desired response; this information was used to improve its capability for handling similar situations in the future. We successfully demonstrated that the same set of molecules representing a blank memory can be trained to classify different sets of MNIST patterns. This work begins to address the question of how chemical systems could gain the capability to learn from an unknown environment, helps establish a deeper connection between natural algorithms and engineered molecular systems, and opens up the possibility of artificial molecular machines that can learn to perform different tasks when they are in action.\n\nFurthermore, we developed a general-purpose circuit architecture that uses heat to power different types of enzyme-free molecular circuits, enabling them to process information and make decisions in response to a changing molecular environment. We showed that several molecular building blocks can be designed to respond to a quick temperature ramp and hundreds of distinct molecules composing a complex system can spontaneously reconfigure to their original structures within minutes. We demonstrated reusable DNA logic circuits and neural networks that carried out up to 16 rounds of computation with distinct input combinations, while the circuit performance remained roughly constant over time. This work introduces a scalable approach for sustained operation of enzyme-free molecular circuits and serves as an essential step toward unsupervised learning in autonomous molecular machines.\n\nThe investigation of implementing learning in DNA neural networks was much more challenging than implementing pattern recognition tasks alone. It was unclear whether the additional functions can be carried out by motifs as simple and robust as previously used in DNA neural networks. To explore this question in a more general context in theory, we asked whether arbitrary chemical reaction networks (CRNs) can be implemented using fuel molecules that each contains no more than two strands. We found that the answer is yes and showed such an implementation using a novel cooperative four-way strand displacement motif. We also showed that the implementation has a few other desired properties including physical reversibility and thus energy efficiency and is provably correct based on CRN bisimulation. On the experimental side, we developed a new type of DNA catalyst that we call a cooperative catalyst. Two signal species, interpreted as an activator and input, both exhibit catalytic control for output production. The design involves just a double-stranded gate species and a single-stranded fuel species, as simple as the previously developed DNA catalyst with no allosteric control. The simplicity and modularity of the design make the cooperative DNA catalyst a useful motif for supervised and unsupervised learning; it could also be used for general-purpose computation and dynamics.\n\nExpanding on the function of previously developed DNA neural networks, we developed a new circuit architecture for implementing loser-take-all neural networks. A loser-take-all function is defined as follows: an output signal is ON if and only if the corresponding input has the smallest analog value among all inputs. We experimentally demonstrated a three-input loser-take-all circuit with nine unique input combinations. Complementary to winner-take-all, loser-take-all DNA circuits could be used for recognition of molecular patterns based on their least similarities to a set of memories, allowing classification decisions for patterns that are extremely noisy and cannot be recognized as a single class. Moreover, the design principle of loser-take-all could be more generally applied in other DNA circuit implementations including k-winner-take-all, opening up possibilities for learning in more diverse DNA neural networks.\n\nThis project has helped support one postdoctoral scholar, one graduate student, and two undergraduate students. All four of them have presented their work at international conferences. The PI has developed a new course incorporating the concepts of molecular neural networks. In this course, undergraduates in Bioengineering, Computer Science, and Computation and Neural Systems proposed and conducted open-ended research projects, resulting in first-author publications. Most of these undergraduates are women and underrepresented minorities and they have gone on to pursue molecular programming research in graduate school.\n\n\t\t\t\t\tLast Modified: 03/05/2023\n\n\t\t\t\t\tSubmitted by: Lulu Qian"
 }
}