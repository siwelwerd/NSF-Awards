{
 "awd_id": "1909298",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Collaborative Research: Acceleration Algorithms for Large-scale Nonconvex Optimization",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032920000",
 "po_email": "ahero@nsf.gov",
 "po_sign_block_name": "Alfred Hero",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-01-31",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2019-08-05",
 "awd_max_amd_letter_date": "2019-08-05",
 "awd_abstract_narration": "Non-convex optimization problems are ubiquitous in data science, machine learning and artificial intelligence. Non-convexity, together with the high dimension of the model parameters and the large volume of uncertain data, presents significant challenges for solving these problems. Although popular methods have been proposed to speed up optimization algorithms for solving practical large-scale problems, these algorithms do not necessarily converge for non-convex problems, and some of them do not even converge in the convex setting. The primary goal of this project is to develop principled approaches for designing acceleration algorithms with provable theoretical convergence guarantees and superior practical performance for large-scale non-convex optimization. The developed algorithms will be applicable to big data problems in various domains, including deep learning, computer vision, medical image processing, social network learning, etc. \r\n\r\nThis project will design novel, fast, and scalable acceleration algorithms for solving a variety of large-scale non-convex problems including constrained, composite, and saddle point optimization problems. This will include the development of both novel direct acceleration methods inspired by Nesterov's approach, and of indirect acceleration methods via proximal point methods for different types of problems. The performance of these acceleration methods will be explored when combined with randomization methods in order to enhance their scalability with data dimension and volume. Comprehensive numerical validations will be conducted for application problems arising in large-scale data analysis. This project will contribute to a synthesis of optimization with data science, and will be incorporated into curriculum development, and in the training of students and future big data researchers and practitioners.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Guanghui",
   "pi_last_name": "Lan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Guanghui Lan",
   "pi_email_addr": "george.lan@isye.gatech.edu",
   "nsf_id": "000545078",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue, NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320420",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Nonconvex optimization problems are ubiquitous in data science, machine learning and artificial intelligence. The nature of nonconvexity, along with the high dimension of design variables and the large volume of uncertain data, has presented significant challenges to the design of efficient algorithms for solving these problems. Although popular methods have been proposed to speed up optimization algorithms for solving practical large-scale machine learning problems, these algorithms do not necessarily converge for nonconvex problems, and some of them do not even converge under the convex setting. It is thus imperative to develop principled approaches for designing acceleration algorithms with provable theoretical convergence guarantees for large-scale nonconvex optimization.&nbsp;</p>\n<p>This research program greatly advanced the development of novel, fast, and scalable acceleration algorithms for solving a variety of large-scale nonconvex problems including unconstrained, composite, and saddle point optimization problems. More specifically, with the support this NSF grant, the PI has made significant progresses along the following four research thrusts: a) development of novel direct acceleration and indirect acceleration via proximal point method for nonconvex optimization; b) design of new stochastic direct acceleration schemes with guaranteed global and local convergence rates; c) design of novel randomization schemes based on the indirect acceleration approach; and d) numerical validations of proposed acceleration algorithms.&nbsp;Furthermore, these generic algorithmic schemes have been specialized for structured nonconvex optimization arising from reinforcement learning.&nbsp;</p>\n<p>These research results will significantly enhance the availability of optimization techniques and tools for analyzing big data problems. Thus, the research will benefit a broad range of scientists and researchers in many research areas including computer vision, medical image processing, social network learning, public health, etc. Furthermore, the PI organized invited sessions and deliver tutorials and invited talks on the topic of large-scale nonconvex optimization in major conferences of optimization, machine learning, and signal processing, and will make all possible efforts into introducing the developed methodology to scientists beyond optimization, machine learning and statistics. This project helped to forge a synthesis of optimization and machine learning in curriculum development and training of students as well as future Big Data researchers.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/18/2023<br>\n\t\t\t\t\tModified by: Guanghui&nbsp;Lan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nNonconvex optimization problems are ubiquitous in data science, machine learning and artificial intelligence. The nature of nonconvexity, along with the high dimension of design variables and the large volume of uncertain data, has presented significant challenges to the design of efficient algorithms for solving these problems. Although popular methods have been proposed to speed up optimization algorithms for solving practical large-scale machine learning problems, these algorithms do not necessarily converge for nonconvex problems, and some of them do not even converge under the convex setting. It is thus imperative to develop principled approaches for designing acceleration algorithms with provable theoretical convergence guarantees for large-scale nonconvex optimization. \n\nThis research program greatly advanced the development of novel, fast, and scalable acceleration algorithms for solving a variety of large-scale nonconvex problems including unconstrained, composite, and saddle point optimization problems. More specifically, with the support this NSF grant, the PI has made significant progresses along the following four research thrusts: a) development of novel direct acceleration and indirect acceleration via proximal point method for nonconvex optimization; b) design of new stochastic direct acceleration schemes with guaranteed global and local convergence rates; c) design of novel randomization schemes based on the indirect acceleration approach; and d) numerical validations of proposed acceleration algorithms. Furthermore, these generic algorithmic schemes have been specialized for structured nonconvex optimization arising from reinforcement learning. \n\nThese research results will significantly enhance the availability of optimization techniques and tools for analyzing big data problems. Thus, the research will benefit a broad range of scientists and researchers in many research areas including computer vision, medical image processing, social network learning, public health, etc. Furthermore, the PI organized invited sessions and deliver tutorials and invited talks on the topic of large-scale nonconvex optimization in major conferences of optimization, machine learning, and signal processing, and will make all possible efforts into introducing the developed methodology to scientists beyond optimization, machine learning and statistics. This project helped to forge a synthesis of optimization and machine learning in curriculum development and training of students as well as future Big Data researchers. \n\n\t\t\t\t\tLast Modified: 05/18/2023\n\n\t\t\t\t\tSubmitted by: Guanghui Lan"
 }
}