{
 "awd_id": "1925044",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Knowledge-based Robot Sequential Decision Making under Uncertainty",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Juan Wachs",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 351374.0,
 "awd_amount": 367374.0,
 "awd_min_amd_letter_date": "2019-08-30",
 "awd_max_amd_letter_date": "2020-05-21",
 "awd_abstract_narration": "This project focuses on promoting the progress of science and technology development by developing computational frameworks needed to advance research on robot decision-making. The key activity is building intelligent agents that are able to perceive the environment through sensors and act upon that environment through actuators. This project aims to bring in computational methods of different modalities toward a generally applicable, robot decision-making framework that can significantly promote the development of intelligent agents in the real world. Example application domains include robotics (as mostly used in this proposal), finance, urban planning, healthcare, games, transportation, e-commerce, and many more. A key focus of this project is on incorporating robotic projects into K-16 education, including education of undergraduate students through the University Undergraduate Research (UUR) programs, which aim at exposing scientific research experiences to undergraduate students in early years, outreach education to regional high/middle schools, and education to the general public through public media. \r\n\r\nRobust robot decision-making in the real world is challenging for reasons such as imperfect perception capabilities, incomplete domain knowledge, non-deterministic action outcomes, and limited experience of interacting with the working environment. The focus of this proposed project is on a robot sequential decision-making framework that simultaneously supports learning to perceive the environment, reasoning about declarative contextual knowledge, and planning to actively collect information for task completion. Under the umbrella of artificial intelligence, there are at least three very different ways of realizing robot decision-making, namely supervised learning from robot experiences in the past, automated reasoning using declarative knowledge, and probabilistic planning toward accomplishing complex tasks that require more than one action. The research aims include developing algorithms that simultaneously allow logical-probabilistic reasoning and planning under uncertainty for robot decision-making in stochastic worlds; developing algorithms for simultaneous world state estimation via recurrent neural networks, representing and reasoning with declarative knowledge, and multi-modal perception control using decision-theoretic methods; and developing a principled integration of logical-probabilistic knowledge representation and reasoning (KRR) with reinforcement learning, enabling agents to simultaneously reason with declarative knowledge and learn from interaction experiences. This work has the potential to enable visionary robot decision-making while leveraging the extensive interaction experiences as well as contextual knowledge from people.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shiqi",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shiqi Zhang",
   "pi_email_addr": "zhangs@binghamton.edu",
   "nsf_id": "000753721",
   "pi_start_date": "2019-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Binghamton",
  "inst_street_address": "4400 VESTAL PKWY E",
  "inst_street_address_2": "",
  "inst_city_name": "BINGHAMTON",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6077776136",
  "inst_zip_code": "13902",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "L9ZDVULCHCV3",
  "org_uei_num": "NQMVAAQUFU53"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Binghamton",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "139026000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 351374.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-d3aa58f0-7fff-8503-6e4d-ae8d041fcc6c\">\n<p dir=\"ltr\"><span>In this project, we aimed to develop decision-making frameworks that leverage declarative knowledge in uncertain domains. In particular, we consider sequential decision-making, where the agents (including robots) must repeatedly select actions to achieve long-term, complex goals. Two computational paradigms of sequential decision-making methods were studied in this project, including planning under uncertainty, where a full description of the world is assumed available, and reinforcement learning, where the decision-making agent must learn from trial and error.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Our first major outcome is an algorithm and system that supports both learning from annotated data for state estimation and reasoning with commonsense knowledge for sequential decision making. The developed approach is called LCORPP (Learning, COmmonsense Reasoning, and Probabilistic Planning), and has been applied to a mobile robot that guides visitors in new environments. The robot was able to use computer vision techniques to capture human motion trajectory and estimate how likely the human is interested in an interaction, e.g., asking about how to get to the front desk. Here it relies on a dataset from the literature that includes human motion trajectories in shopping malls. The estimated human intention was then ported to a commonsense reasoner that incorporate domain knowledge into the reasoning process, e.g., students are less likely to ask for directions whereas student parents are more likely. The performance of LCORPP was shown to be better than existing methods that do not use common sense or existing motion trajectory data.&nbsp;&nbsp;</span></p>\n<p dir=\"ltr\"><span>Our second major outcome is an algorithm and system that enables mobile robots to use computer vision (360-degree vision in particular) to collect domain knowledge. The develop approach, called Scene Analysis for Robot Planning (SARP), has been demonstrated and evaluated on a mobile robot that works on indoor target search tasks. The robot dynamically constructs a scene graph that captures not only the objects but also their spatial relationship. Reasoning with such graph-based knowledge enables the robot to use locations of known objects to actively locate new objects.&nbsp;</span></p>\n<p dir=\"ltr\"><span>A third major outcome is an algorithm and system that leverages expert knowledge to guide reinforcement learning agents in exploring new domains. A human expert provides the knowledge, such as to go through a door one must stand in front of the door, and the knowledge is used for imagining interaction experience to give the learning agents (robots in our case) a warm up instead of letting the robot learn from mistakes in the real world. The developed approach was called Guided Dyna-Q (GDQ), and was applied to indoor navigation tasks of mobile robots. Results demonstrated that the robot tended to avoid navigating through small doors or long corridors even before it had any experience of interacting with them.&nbsp;</span></p>\n<p dir=\"ltr\"><span>There were four REU (Research Experiences for Undergraduates) students involved in this project. Their participation included algorithm design, software implementation, paper writing, data collection, and experiment conduction. One major outcome of this project is an article published in AI Magazine titled &ldquo;A Survey of Knowledge-based Sequential Decision Making under Uncertainty&rdquo; that was published in 2022. The article reviewed existing methods on knowledge-based decision-making, the theme of this project, and discussed the challenges and opportunities of this research area. The review article strengthened the broader impact of this project to the AI community.&nbsp; Finally, the developed algorithms and systems have been incorporated into the \"Introduction to Artificial Intelligence\" and &ldquo;Intelligent Mobile Robotics&rdquo; courses at the PI&rsquo;s home institution, where students work on related course projects and learn about the state of the art in agent decision making.&nbsp;</span></p>\n</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/17/2023<br>\nModified by: Shiqi&nbsp;Zhang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nIn this project, we aimed to develop decision-making frameworks that leverage declarative knowledge in uncertain domains. In particular, we consider sequential decision-making, where the agents (including robots) must repeatedly select actions to achieve long-term, complex goals. Two computational paradigms of sequential decision-making methods were studied in this project, including planning under uncertainty, where a full description of the world is assumed available, and reinforcement learning, where the decision-making agent must learn from trial and error.\n\n\nOur first major outcome is an algorithm and system that supports both learning from annotated data for state estimation and reasoning with commonsense knowledge for sequential decision making. The developed approach is called LCORPP (Learning, COmmonsense Reasoning, and Probabilistic Planning), and has been applied to a mobile robot that guides visitors in new environments. The robot was able to use computer vision techniques to capture human motion trajectory and estimate how likely the human is interested in an interaction, e.g., asking about how to get to the front desk. Here it relies on a dataset from the literature that includes human motion trajectories in shopping malls. The estimated human intention was then ported to a commonsense reasoner that incorporate domain knowledge into the reasoning process, e.g., students are less likely to ask for directions whereas student parents are more likely. The performance of LCORPP was shown to be better than existing methods that do not use common sense or existing motion trajectory data.\n\n\nOur second major outcome is an algorithm and system that enables mobile robots to use computer vision (360-degree vision in particular) to collect domain knowledge. The develop approach, called Scene Analysis for Robot Planning (SARP), has been demonstrated and evaluated on a mobile robot that works on indoor target search tasks. The robot dynamically constructs a scene graph that captures not only the objects but also their spatial relationship. Reasoning with such graph-based knowledge enables the robot to use locations of known objects to actively locate new objects.\n\n\nA third major outcome is an algorithm and system that leverages expert knowledge to guide reinforcement learning agents in exploring new domains. A human expert provides the knowledge, such as to go through a door one must stand in front of the door, and the knowledge is used for imagining interaction experience to give the learning agents (robots in our case) a warm up instead of letting the robot learn from mistakes in the real world. The developed approach was called Guided Dyna-Q (GDQ), and was applied to indoor navigation tasks of mobile robots. Results demonstrated that the robot tended to avoid navigating through small doors or long corridors even before it had any experience of interacting with them.\n\n\nThere were four REU (Research Experiences for Undergraduates) students involved in this project. Their participation included algorithm design, software implementation, paper writing, data collection, and experiment conduction. One major outcome of this project is an article published in AI Magazine titled A Survey of Knowledge-based Sequential Decision Making under Uncertainty that was published in 2022. The article reviewed existing methods on knowledge-based decision-making, the theme of this project, and discussed the challenges and opportunities of this research area. The review article strengthened the broader impact of this project to the AI community. Finally, the developed algorithms and systems have been incorporated into the \"Introduction to Artificial Intelligence\" and Intelligent Mobile Robotics courses at the PIs home institution, where students work on related course projects and learn about the state of the art in agent decision making.\n\n\n\n\t\t\t\t\tLast Modified: 12/17/2023\n\n\t\t\t\t\tSubmitted by: ShiqiZhang\n"
 }
}