{
 "awd_id": "1909900",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "OAC Core: SHF: Small: Enabling Rapid Design and Deployment of Deep Learning Models on Hardware Accelerators",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922625",
 "po_email": "jjli@nsf.gov",
 "po_sign_block_name": "Juan Li",
 "awd_eff_date": "2019-06-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2019-05-20",
 "awd_max_amd_letter_date": "2019-05-20",
 "awd_abstract_narration": "Machine Learning (ML) has rapidly emerged as one of the foundational technologies of this century. It is pervasive in our lives today, from allowing us to unlock smartphones, to powering recommendation engines for almost any human activities (dinning, movies, services etc). The applications of ML are expected to become even more transformative in the future, especially in healthcare, autonomous transport, robotics, agriculture, education, and space exploration. ML models are computationally expensive, need large amounts of memory to store the trained model, and have strict runtime requirements. They cannot be run efficiently on general-purpose processors, which has led to an explosive growth in custom hardware accelerators for ML. However, getting good performance and energy-efficiency from these accelerators is itself challenging, as it relies on three components: the ML model itself, the hardware parameters, and the scheduling of computations in the ML model onto the limited compute and memory resources on the accelerator. The proposed research will develop an open-source software cyberinfrastructure called MAESTRO that can be used to analytically determine the performance and energy-efficiency of ML models over target hardware platforms, prior to actually building the hardware and deploying the model. MAESTRO will be extremely useful for students, researchers, and industry practitioners alike to learn about, design, and deploy custom ML solutions. The project will also engage undergraduate and high-school students to teach them about ML through outreach activities involving hackathons and hardware building.\r\n\r\n\r\nMapping ML computations over finite compute elements within an accelerator, and understanding the corresponding data that needs to move across the memory hierarchy is a non-trivial problem; the space of all possible ways of slicing and dicing the model (known as \"dataflow\") is exponentially complex, and the benefits of any mapping vary across ML models and target accelerator. To address this, the PI will first develop a set of data-centric directives to directly describe the mapping of the ML model over the accelerator, which will enable precise calculations of data reuse opportunities across space and time to reduce overall data movement. Next, the PI will develop the MAESTRO analytical cost model framework to estimate reuse, end-to-end performance, and energy over the target hardware. Finally, a set of tools will be developed around MAESTRO to automatically search for and determine the optimal hardware/mapping/model given constraints of runtime, power, energy, or area.  The proposed framework will enable iterative innovation and co-design across the ML model, mapping and target hardware, and will therefore be highly valuable for ML model developers, compiler writers and computer architects. MAESTRO will be released and maintained on an open-source license, and the PI will run periodic tutorials to build an active user-base in the research community.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tushar",
   "pi_last_name": "Krishna",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tushar Krishna",
   "pi_email_addr": "tushar@ece.gatech.edu",
   "nsf_id": "000711282",
   "pi_start_date": "2019-05-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "North Avenue NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320420",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "090Y00",
   "pgm_ele_name": "OAC-Advanced Cyberinfrast Core"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">Artificial Intelligence (AI) and Machine Learning (ML) continues to demonstrate extremely promising results across diverse applications - from computer vision to speech recognition to recommendations to more recently text and image creation via generative AI. AI/ML has become a foundational technology across almost every industry including technology, finance, manufacturing, and health.&nbsp;</p>\n<p class=\"p1\">Deployment of AI solutions comprises of two phases: \"Training\" - which involves sifting through massive amounts of data to learn underlying patterns, followed by \"Inference\" - which involves recognizing/classifying/predicting some query when supplied with new data. One of the key enablers of the rise of AI has been the hardware platforms on which they are deployed. GPUs are the most accepted platform of choice for training. However, for inference (be it in the cloud or on the edge), custom hardware accelerators (which are typically a large array of processing elements (PEs)) have become the de-facto choice to get high energy-efficiency and throughput.&nbsp;</p>\n<p class=\"p1\">The runtime, throughput, and energy-efficiency of running a AI/ML models depends on three key factors - the deep neural network (DNN) model for the AI/ML task, the accelerator's microarchitecture, and the mapping of the DNN over the hardware. DNNs have millions of parameters (i.e., weights), and require billions of computations.&nbsp;&nbsp;Mapping the DNN computations over the finite PEs within an accelerator, and understanding the corresponding data that needs to move between PEs and across the memory hierarchy (i.e., ``dataflow\") is a non-trivial problem, as the space of all possible ways of slicing and dicing the DNN is exponentially complex. This makes the design-space of DNN accelerators extremely large and intractable due to this cross-dependence. Moreover, when this project started, there was a lack of any framework to enable systematic understanding and exploration of the design-space.</p>\n<p>Over the past few years, this project&nbsp;addressed the aforementioned challenges by developing MAESTRO. MAESTRO is a detailed analytical framework capturing the interactions between the DNN operators, mapping primitives, and hardware structures, enabling systematic and efficient design-space exploration of accelerator microarchitectures.</p>\n<p>On the technical front, this project made a suite of intellectual contributions: (i) a set of data-centric directives to precisely describe and characterize DNN mappings, (ii) an analytical engine to estimate runtime, throughput and energy metrics, (iii) multiple design-space exploration tools that call MAESTRO during the search process for both design-time and compile-time optimizations, (iv) exploration of accelerators for workloads beyond DNNs.</p>\n<p>The project also made several strong pedagogical contributions across the broader computing community. The MAESTRO tool itself, released open-source, has had hundreds of downloads and citations in papers. The learnings from the research from the tool led to a textbook on DNN accelerators and over ten research papers. The PI and his students also ran several tutorials on the tool across all the major computer architecture conferences. The MAESTRO tool has also been leveraged for use cases beyond its original intent - such as the study of high-performance computing (HPC) workloads by Department of Energy National Labs, Extended Reality workloads, DNN model-hardware co-design, and training AI models for hardware design.</p>\n<p>More up to date details on the MAESTRO project can be found here: https://maestro.ece.gatech.edu/</p><br>\n<p>\n Last Modified: 06/03/2024<br>\nModified by: Tushar&nbsp;Krishna</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1909900/1909900_10606125_1717450330373_Screenshot_2024_06_03_at_2.52.02_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1909900/1909900_10606125_1717450330373_Screenshot_2024_06_03_at_2.52.02_PM--rgov-800width.png\" title=\"MAESTRO Wiki Page\"><img src=\"/por/images/Reports/POR/2024/1909900/1909900_10606125_1717450330373_Screenshot_2024_06_03_at_2.52.02_PM--rgov-66x44.png\" alt=\"MAESTRO Wiki Page\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">MAESTRO Documentation on its webpage</div>\n<div class=\"imageCredit\">Tushar Krishna</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Tushar&nbsp;Krishna\n<div class=\"imageTitle\">MAESTRO Wiki Page</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nArtificial Intelligence (AI) and Machine Learning (ML) continues to demonstrate extremely promising results across diverse applications - from computer vision to speech recognition to recommendations to more recently text and image creation via generative AI. AI/ML has become a foundational technology across almost every industry including technology, finance, manufacturing, and health.\n\n\nDeployment of AI solutions comprises of two phases: \"Training\" - which involves sifting through massive amounts of data to learn underlying patterns, followed by \"Inference\" - which involves recognizing/classifying/predicting some query when supplied with new data. One of the key enablers of the rise of AI has been the hardware platforms on which they are deployed. GPUs are the most accepted platform of choice for training. However, for inference (be it in the cloud or on the edge), custom hardware accelerators (which are typically a large array of processing elements (PEs)) have become the de-facto choice to get high energy-efficiency and throughput.\n\n\nThe runtime, throughput, and energy-efficiency of running a AI/ML models depends on three key factors - the deep neural network (DNN) model for the AI/ML task, the accelerator's microarchitecture, and the mapping of the DNN over the hardware. DNNs have millions of parameters (i.e., weights), and require billions of computations.Mapping the DNN computations over the finite PEs within an accelerator, and understanding the corresponding data that needs to move between PEs and across the memory hierarchy (i.e., ``dataflow\") is a non-trivial problem, as the space of all possible ways of slicing and dicing the DNN is exponentially complex. This makes the design-space of DNN accelerators extremely large and intractable due to this cross-dependence. Moreover, when this project started, there was a lack of any framework to enable systematic understanding and exploration of the design-space.\n\n\nOver the past few years, this projectaddressed the aforementioned challenges by developing MAESTRO. MAESTRO is a detailed analytical framework capturing the interactions between the DNN operators, mapping primitives, and hardware structures, enabling systematic and efficient design-space exploration of accelerator microarchitectures.\n\n\nOn the technical front, this project made a suite of intellectual contributions: (i) a set of data-centric directives to precisely describe and characterize DNN mappings, (ii) an analytical engine to estimate runtime, throughput and energy metrics, (iii) multiple design-space exploration tools that call MAESTRO during the search process for both design-time and compile-time optimizations, (iv) exploration of accelerators for workloads beyond DNNs.\n\n\nThe project also made several strong pedagogical contributions across the broader computing community. The MAESTRO tool itself, released open-source, has had hundreds of downloads and citations in papers. The learnings from the research from the tool led to a textbook on DNN accelerators and over ten research papers. The PI and his students also ran several tutorials on the tool across all the major computer architecture conferences. The MAESTRO tool has also been leveraged for use cases beyond its original intent - such as the study of high-performance computing (HPC) workloads by Department of Energy National Labs, Extended Reality workloads, DNN model-hardware co-design, and training AI models for hardware design.\n\n\nMore up to date details on the MAESTRO project can be found here: https://maestro.ece.gatech.edu/\t\t\t\t\tLast Modified: 06/03/2024\n\n\t\t\t\t\tSubmitted by: TusharKrishna\n"
 }
}