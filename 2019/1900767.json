{
 "awd_id": "1900767",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Medium: Collaborative Research: Towards Effective Interpretation of Deep Learning: Prediction, Representation, Modeling and Utilization",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927978",
 "po_email": "racharya@nsf.gov",
 "po_sign_block_name": "Raj Acharya",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 216000.0,
 "awd_min_amd_letter_date": "2019-08-15",
 "awd_max_amd_letter_date": "2022-06-15",
 "awd_abstract_narration": "While deep learning has achieved unprecedented prediction capabilities, it is often criticized as a black box because of lacking interpretability, which is very important in real-world applications such as healthcare and cybersecurity. For example, healthcare professionals would appropriately trust and effectively manage prediction results only if they can understand why and how a patient is diagnosed with prediabetes. The project is to investigate the interpretability of deep learning by following the fundamental elements in a data mining practice from representation, modeling to prediction. The results of the project are expected to improve the usability of deep learning in important applications, positively boosting the overall value of the deep learning based information systems. The education program that integrates data science, industrial engineering, and visualization is to train students with data analytics technologies in industrial systems, to attract and mentor members of underrepresented groups pursuing careers in STEM.\r\n\r\nThe research goal of this project is to systematically explore interpretability of deep learning from a machine learning life cycle, i.e., representation, modeling and prediction, as well as the deployment of interpretability in various tasks. Specifically, this project aims to achieve the research goal by developing a series of interpretation algorithms and methods from the following aspects. It explores post-hoc interpretation methods to shed light on how deep learning models produce a specific prediction and generate a representation. It also investigates designing interpretable models from scratch, which aims to construct self-explanatory models and incorporate interpretability directly into the structure of a deep learning model. The aforementioned interpretation derived from a deep learning model is employed to promote the model performance. In addition, the applications of interpretability are utilized to debug model behaviors so as to ensure the model decision making process is consistent with human expert knowledge, as well as to promote model robustness when handling adversarial attacks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Ragan",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Eric D Ragan",
   "pi_email_addr": "eragan@ufl.edu",
   "nsf_id": "000690592",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Florida",
  "inst_street_address": "1523 UNION RD RM 207",
  "inst_street_address_2": "",
  "inst_city_name": "GAINESVILLE",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "3523923516",
  "inst_zip_code": "326111941",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "FL03",
  "org_lgl_bus_name": "UNIVERSITY OF FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NNFQH1JAPEP3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Florida",
  "perf_str_addr": "432 Newell Drive",
  "perf_city_name": "Gainesville",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "326115500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "FL03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 118562.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 81438.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>While capabilities and speed of artificial intelligence (AI) and deep learning technology has surpassed human abilities in various applications, AI is often criticized as an opaque black box model because of lacking interpretability, which is very important in high-stake applications such as healthcare recommendations, security monitoring, or financial decisions. For example, healthcare professionals would appropriately trust and effectively manage prediction results only if they can understand why and how a patient is diagnosed with prediabetes. This project investigated the interpretability of AI applications by following the fundamental elements AI for communication to regular people who need to understand the results of computer logic. The results of the project are expected to improve the usability of deep learning in important applications, positively boosting the overall value of AI-based information systems. The project also supports educational programs that integrates data science, industrial engineering, and visualization to train students with data analytics technologies in industrial systems, and to attract and mentor members of underrepresented groups pursuing careers in science, technology, engineering, and mathematics (STEM).</p>\n<p>In supporting human understanding of AI, the research produced new technological development that allows visual explanations of how computers work. For example, for AI that works with images, it can be effective to highlight the most important parts of the image that was used for the AI output. Alternatively, color-coded tables can summarize numerical information about system certainty or potential problems for a large number of details to help people see an overview. Using the developed software explanations, the research project included new scientific studies to better understand how people use visual explanations to interpret the AI system and make decisions. The research also considered how the nature of human trust and interpretation of AI depends on the personal background of the people using the system. For instance, the results of one such study in the project demonstrated how differences in the level of human domain expertise can influence differences in trust in AI and changes in trust over time due to seeing system errors. The collection of results from multiple studies and experiments with developed software have been published as research papers so that others may learn from findings and build on contributed scientific knowledge.</p>\n<p>The research project also included educational benefits at the University of Florida by involving undergraduate and graduate students. Relevant topics on human interpretation of AI have been merged into the Information Visualization course (CIS 6930) at the University of Florida, instructed by the PI. Three Ph.D. students acquired professional research training through participation in this project, and five undergraduate student received research mentorship during 2022-2024. Also notable is that the project period overlapped with a period during 2020-2021 when the challenges COVID-19 limited many alternative professional development opportunities, so the research project provided the student benefits of professional mentorship and research training at the University of Florida that may not otherwise have been possible for these students.</p><br>\n<p>\n Last Modified: 11/19/2024<br>\nModified by: Eric&nbsp;D&nbsp;Ragan</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1900767/1900767_10633987_1732042840556_trust_over_time--rgov-214x142.bmp\" original=\"/por/images/Reports/POR/2024/1900767/1900767_10633987_1732042840556_trust_over_time--rgov-800width.bmp\" title=\"Trust and expertise\"><img src=\"/por/images/Reports/POR/2024/1900767/1900767_10633987_1732042840556_trust_over_time--rgov-66x44.bmp\" alt=\"Trust and expertise\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Trends of human trust in AI over time can differ based on level of domain experience and when they observe system errors.</div>\n<div class=\"imageCredit\">Mahsan Nourani</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Eric&nbsp;D&nbsp;Ragan\n<div class=\"imageTitle\">Trust and expertise</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1900767/1900767_10633987_1732042519896_butterfly--rgov-214x142.bmp\" original=\"/por/images/Reports/POR/2024/1900767/1900767_10633987_1732042519896_butterfly--rgov-800width.bmp\" title=\"Visual image explanation\"><img src=\"/por/images/Reports/POR/2024/1900767/1900767_10633987_1732042519896_butterfly--rgov-66x44.bmp\" alt=\"Visual image explanation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An example of visual highlighting (b) for an AI model that recognizes the type of insect (c) from an image (a).</div>\n<div class=\"imageCredit\">Mahsan Nourani</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Eric&nbsp;D&nbsp;Ragan\n<div class=\"imageTitle\">Visual image explanation</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1900767/1900767_10633987_1732042752006_heatmap--rgov-214x142.bmp\" original=\"/por/images/Reports/POR/2024/1900767/1900767_10633987_1732042752006_heatmap--rgov-800width.bmp\" title=\"Heatmap table\"><img src=\"/por/images/Reports/POR/2024/1900767/1900767_10633987_1732042752006_heatmap--rgov-66x44.bmp\" alt=\"Heatmap table\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A color-coded table can summarize AI model confidence of many details to help people understand different options the computer is able to consider when generating output.</div>\n<div class=\"imageCredit\">Mahsan Nourani</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Eric&nbsp;D&nbsp;Ragan\n<div class=\"imageTitle\">Heatmap table</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nWhile capabilities and speed of artificial intelligence (AI) and deep learning technology has surpassed human abilities in various applications, AI is often criticized as an opaque black box model because of lacking interpretability, which is very important in high-stake applications such as healthcare recommendations, security monitoring, or financial decisions. For example, healthcare professionals would appropriately trust and effectively manage prediction results only if they can understand why and how a patient is diagnosed with prediabetes. This project investigated the interpretability of AI applications by following the fundamental elements AI for communication to regular people who need to understand the results of computer logic. The results of the project are expected to improve the usability of deep learning in important applications, positively boosting the overall value of AI-based information systems. The project also supports educational programs that integrates data science, industrial engineering, and visualization to train students with data analytics technologies in industrial systems, and to attract and mentor members of underrepresented groups pursuing careers in science, technology, engineering, and mathematics (STEM).\n\n\nIn supporting human understanding of AI, the research produced new technological development that allows visual explanations of how computers work. For example, for AI that works with images, it can be effective to highlight the most important parts of the image that was used for the AI output. Alternatively, color-coded tables can summarize numerical information about system certainty or potential problems for a large number of details to help people see an overview. Using the developed software explanations, the research project included new scientific studies to better understand how people use visual explanations to interpret the AI system and make decisions. The research also considered how the nature of human trust and interpretation of AI depends on the personal background of the people using the system. For instance, the results of one such study in the project demonstrated how differences in the level of human domain expertise can influence differences in trust in AI and changes in trust over time due to seeing system errors. The collection of results from multiple studies and experiments with developed software have been published as research papers so that others may learn from findings and build on contributed scientific knowledge.\n\n\nThe research project also included educational benefits at the University of Florida by involving undergraduate and graduate students. Relevant topics on human interpretation of AI have been merged into the Information Visualization course (CIS 6930) at the University of Florida, instructed by the PI. Three Ph.D. students acquired professional research training through participation in this project, and five undergraduate student received research mentorship during 2022-2024. Also notable is that the project period overlapped with a period during 2020-2021 when the challenges COVID-19 limited many alternative professional development opportunities, so the research project provided the student benefits of professional mentorship and research training at the University of Florida that may not otherwise have been possible for these students.\t\t\t\t\tLast Modified: 11/19/2024\n\n\t\t\t\t\tSubmitted by: EricDRagan\n"
 }
}