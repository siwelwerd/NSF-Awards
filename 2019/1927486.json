{
 "awd_id": "1927486",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AI-DCL: Fairness for the Allocation of Healthcare Resources",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 297928.0,
 "awd_amount": 297928.0,
 "awd_min_amd_letter_date": "2019-08-05",
 "awd_max_amd_letter_date": "2023-08-21",
 "awd_abstract_narration": "The goal of this research project is to develop machine learning techniques for the fair allocation of healthcare services such as those provided by Medicaid. Although such programs provide crucial services to vulnerable populations, many of the individuals who most need these services languish on waiting lists due to limited resources. Machine learning models can potentially improve this situation by predicting individuals' levels of need, which can then be used to prioritize the waiting lists. Providing care to those in need can prevent institutionalization for those individuals, which both improves quality of life and reduces overall costs. While the benefits of such an approach are clear, care must be taken to ensure that the prioritization process is fair. The researchers also plan to address this issue directly by developing fairness definitions and corresponding fair learning algorithms for the task of learning to rank. The proposed techniques for fair prioritization of healthcare have the potential to save lives, as well as taxpayer dollars. This project aims to lead to a deployed solution for Medicaid prioritization in the state of Maryland, where over 8,000 individuals have died on the Medicaid waitlist since the state's Medicaid expansion under the Affordable Care Act began, according to a 2018 report from the Foundation for Government Accountability.\r\n\r\nThis project will develop a machine learning intervention to the processes of ranking individuals in order of priority for receiving healthcare services. The researchers will apply their methods to Medicaid data, which they will access via their ongoing collaboration with colleagues from the Hilltop Institute, a nonpartisan research organization which is dedicated to community-oriented healthcare analytics; they will also evaluate their methods on a public dataset to facilitate research reproducibility. A key goal of the project is to promote fairness in the ranking. In meeting this goal, the project will extend the capabilities of fair machine learning definitions and algorithms to tasks that have not previously been addressed including survival and temporal modeling. To predict individuals' health status, the research team will use survival models to estimate the risk of future institutionalization, such as relocating to a nursing home. The team will use also Cox proportional hazard models; the multiplicative relationship between covariates and risk will serve to aid explainability. The fairness definitions and the corresponding fair learning algorithms for these models will yield risk scores that can then be used to prioritize waiting lists. For waitlists deployed in practice, it will be necessary to continually re-rank the list since individuals enter and leave the list (due to death or institutionalization, for example), and since covariates change for those who remain on the list; reranking should ensure that individuals who need care will eventually reach the front of the list. The proposed work crosses the boundaries of multiple disciplines (machine learning, fairness, health IT, feminism and civil rights) to solve an urgent real-world problem.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Foulds",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "James Foulds",
   "pi_email_addr": "jfoulds@umbc.edu",
   "nsf_id": "000762372",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shimei",
   "pi_last_name": "Pan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shimei Pan",
   "pi_email_addr": "shimei@umbc.edu",
   "nsf_id": "000677258",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ian",
   "pi_last_name": "Stockwell",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Ian J Stockwell",
   "pi_email_addr": "istockwell@hilltop.umbc.edu",
   "nsf_id": "000799266",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland Baltimore County",
  "inst_street_address": "1000 HILLTOP CIR",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4104553140",
  "inst_zip_code": "212500001",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND BALTIMORE COUNTY",
  "org_prnt_uei_num": "",
  "org_uei_num": "RNKYWXURFRL5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland Baltimore County",
  "perf_str_addr": "1000 Hilltop Circle",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212500001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 297928.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>With artificial intelligence (AI) technologies already widely deployed and affecting both common and consequential aspects of our lives, we must now reckon with how AI can have harmful consequences when deployed without due care. The goal of the project is to develop fair machine learning methods for prioritizing medical resources, which make accurate predictions while avoiding discriminatory bias. The research has primary application to the needs-based ranking of the Medicaid waitlist in the state of Maryland. This vital application motivates our fundamental research contributions regarding methods for achieving fairness in AI, particularly in machine learning models for event times and for ranking.</span></p>\r\n<p><span><br /></span><span>Medicaid provides crucial services to vulnerable populations via programs known as waivers. This project is particularly motivated by waivers that aim to help keep older adults or individuals with developmental or physical disabilities healthy enough to keep them out of institutional settings such as nursing homes. These programs are a win-win proposition, as this not only improves the health and quality of life of such an individual but also saves taxpayer dollars, as this is generally cheaper than institutionalization. The State of Maryland prioritizes this waiver partly as a first-in, first-out queue and partly as a needs-based ranking that is determined by an AI model. In more detail, a statistical model called a survival model, specifically a Cox proportional hazards (CPH) model, is used to predict the time that an individual would be admitted to a nursing home, and these predicted times are used to prioritize the waitlist. While such a model has clear benefits, due to well-known issues around fairness and bias in machine learning, there are concerns that the system might not behave in a fair manner. This project aims to develop the methods necessary to address these issues. Ultimately, the work aims to deploy a fair AI system for needs-based Medicaid prioritization in Maryland, thereby improving health and quality of life while making effective use of public funds.</span></p>\r\n<p><span><br /></span><span>Motivated by this application, the project developed fair survival models and associated AI algorithms. The project produced fair AI metrics and machine learning algorithms for the traditional CPH survival models, as well as their deep learning extensions. It further developed methods for directly producing rankings that are fair, and for studying the impacts of these methods on the Medicaid waitlist in Maryland.</span></p>\r\n<p><span><br /></span><span>An unexpected finding emerged from the project. While evaluating our methods, we discovered that it is frequently possible to achieve \"fairness for free,\" i.e., an improvement in the measured fairness of an AI system without a corresponding drop in predictive performance. This finding overturns the conventional wisdom that fairness must always come at the cost of prediction accuracy. It has profound practical implications, since this cost is frequently cited as a reason not to deploy fair AI methods. The project subsequently developed methods to automatically achieve the fairness-for-free property.</span></p>\r\n<p><span><br /></span><span>The project further led to applications in the related problems of fair AI methods for the AI-based filtering of job applicants' resumes and for career counseling, and to addressing challenges at the interface between humans and fair AI methods including the impact of humans' own biases, and how multiple stakeholders can navigate trade-offs between predictive performance and multiple competing notions of fairness.</span><span><br /></span></p>\r\n<p><span>Twelve peer-reviewed publications were produced by the project (many of which were at top-tier venues), as well as two journal special issues on responsible AI, with further articles expected to follow. The project has provided training, mentorship, and development for a number of student researchers, including three doctoral students, twelve master's students, two undergraduate student researchers, and two high school students. The principal investigator gave 27 oral presentations for the project, in addition to presentations for conference publications. The project was leveraged in the development of a new course on ethical and responsible AI, as well as advancing the curriculum on responsible AI topics for five other courses.</span></p><br>\n<p>\n Last Modified: 03/31/2025<br>\nModified by: James&nbsp;Foulds</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nWith artificial intelligence (AI) technologies already widely deployed and affecting both common and consequential aspects of our lives, we must now reckon with how AI can have harmful consequences when deployed without due care. The goal of the project is to develop fair machine learning methods for prioritizing medical resources, which make accurate predictions while avoiding discriminatory bias. The research has primary application to the needs-based ranking of the Medicaid waitlist in the state of Maryland. This vital application motivates our fundamental research contributions regarding methods for achieving fairness in AI, particularly in machine learning models for event times and for ranking.\r\n\n\n\nMedicaid provides crucial services to vulnerable populations via programs known as waivers. This project is particularly motivated by waivers that aim to help keep older adults or individuals with developmental or physical disabilities healthy enough to keep them out of institutional settings such as nursing homes. These programs are a win-win proposition, as this not only improves the health and quality of life of such an individual but also saves taxpayer dollars, as this is generally cheaper than institutionalization. The State of Maryland prioritizes this waiver partly as a first-in, first-out queue and partly as a needs-based ranking that is determined by an AI model. In more detail, a statistical model called a survival model, specifically a Cox proportional hazards (CPH) model, is used to predict the time that an individual would be admitted to a nursing home, and these predicted times are used to prioritize the waitlist. While such a model has clear benefits, due to well-known issues around fairness and bias in machine learning, there are concerns that the system might not behave in a fair manner. This project aims to develop the methods necessary to address these issues. Ultimately, the work aims to deploy a fair AI system for needs-based Medicaid prioritization in Maryland, thereby improving health and quality of life while making effective use of public funds.\r\n\n\n\nMotivated by this application, the project developed fair survival models and associated AI algorithms. The project produced fair AI metrics and machine learning algorithms for the traditional CPH survival models, as well as their deep learning extensions. It further developed methods for directly producing rankings that are fair, and for studying the impacts of these methods on the Medicaid waitlist in Maryland.\r\n\n\n\nAn unexpected finding emerged from the project. While evaluating our methods, we discovered that it is frequently possible to achieve \"fairness for free,\" i.e., an improvement in the measured fairness of an AI system without a corresponding drop in predictive performance. This finding overturns the conventional wisdom that fairness must always come at the cost of prediction accuracy. It has profound practical implications, since this cost is frequently cited as a reason not to deploy fair AI methods. The project subsequently developed methods to automatically achieve the fairness-for-free property.\r\n\n\n\nThe project further led to applications in the related problems of fair AI methods for the AI-based filtering of job applicants' resumes and for career counseling, and to addressing challenges at the interface between humans and fair AI methods including the impact of humans' own biases, and how multiple stakeholders can navigate trade-offs between predictive performance and multiple competing notions of fairness.\n\r\n\n\nTwelve peer-reviewed publications were produced by the project (many of which were at top-tier venues), as well as two journal special issues on responsible AI, with further articles expected to follow. The project has provided training, mentorship, and development for a number of student researchers, including three doctoral students, twelve master's students, two undergraduate student researchers, and two high school students. The principal investigator gave 27 oral presentations for the project, in addition to presentations for conference publications. The project was leveraged in the development of a new course on ethical and responsible AI, as well as advancing the curriculum on responsible AI topics for five other courses.\t\t\t\t\tLast Modified: 03/31/2025\n\n\t\t\t\t\tSubmitted by: JamesFoulds\n"
 }
}