{
 "awd_id": "1908711",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Exploring Rationale behind Visual Understanding: Combining Attention and Reasoning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2019-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 285029.0,
 "awd_amount": 285029.0,
 "awd_min_amd_letter_date": "2019-07-22",
 "awd_max_amd_letter_date": "2019-07-22",
 "awd_abstract_narration": "Recent progress in deep learning has resulted in models that show significant performance gains in computer vision tasks. This project aims to bridge the current gap between the increasing performance in intelligent systems and the lack of understanding in the complex task-solving process. With the overarching goal of understanding and modeling the process, this project studies two intertwined mechanisms heavily involved in task-solving -- attention and reasoning -- and develops a sound framework to integrate the two. It will serve as a critical step forward to untangling the process of solving a visual task and alleviating the black-box problem in machine learning. The research will build attention and reasoning capabilities into machines, thus empowering applications in a broad spectrum of artificial intelligence tasks including medical diagnosis and treatment, robotics, and education. The principal investigator will organize workshops and seminars, and make project results publicly available. The project also aims at integrated research and education with a focus on increased diversity, through K-12 outreach activities, student mentoring, and curriculum development.\r\n\r\nThis project focuses on both dataset and model development, as well as enabling new methods for network visualization, interpretation, and diagnosis. More specifically, the project develops: (1) a new dataset with human eye movements and textual explanations, to understand critical factors that contribute to task performance; (2) a framework where models devised in the framework make a first step to demonstrate the process of task-solving by showing attention and reasoning capabilities; and (3) a novel layer-wise network diagnosis method considering both performance and interpretability of each network layer. Addressing these questions will not only boost model performance but open the black-box of the decision-making process of a visual task as well as the structure of the deep neural networks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Qi",
   "pi_last_name": "Zhao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qi Zhao",
   "pi_email_addr": "qzhao@umn.edu",
   "nsf_id": "000753440",
   "pi_start_date": "2019-07-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota-Twin Cities",
  "perf_str_addr": "4-192, 200 Union Street SE",
  "perf_city_name": "Minneapolis",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554550169",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 285029.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-3b1882c0-7fff-d149-bef3-0a7515bef892\"> </span></p>\n<p><span id=\"docs-internal-guid-0dfd52d5-7fff-d701-ba70-4c9e12e8eb0c\"> </span></p>\n<p dir=\"ltr\"><span>Recent advancements in AI, powered by big data and deep neural networks, have led to significant breakthroughs in various domains. Despite their impressive performance, the black-box nature of AI systems limits their understanding of complex decision-making processes. This project aims to bridge the current gap between the increasing performance in intelligent systems and the lack of understanding in the complex task-solving process. With the overarching objective of visualizing and modeling this process, the project delves into two intertwined mechanisms crucial for task-solving&mdash;attention and reasoning&mdash;and formulates a robust framework to seamlessly integrate them. By enhancing attention and reasoning capacities in AI systems, the research has empowered machines to make intelligent and transparent decisions across diverse applications, addressing generalizability, interpretability, and trustworthy concerns in fields such as defense, business, healthcare, and autonomous vehicles.</span></p>\n<p dir=\"ltr\"><span>Throughout the project, we have made substantial progress in advancing the attention and reasoning capabilities of deep learning models.&nbsp; Our approach involves the creation of benchmark datasets that integrate attention, reasoning, and correctness labels, serving as foundational resources for attention and reasoning studies. We introduce a pioneering set of datasets, covering both images and 360-degree videos, to delve into attention, reasoning, and task performance in VQA. Additionally, a large-scale dataset focused on multi-modal explanations, incorporating grounding and reasoning, addresses key questions about model explanation and visualization. Our new explanation dataset for the first time encodes multi-modal information with a joint representation and explicitly considers the reasoning process.&nbsp;</span></p>\n<p dir=\"ltr\"><span>In the pursuit of enhancing performance, explainability, and generalizability, novel deep-learning models have been crafted to capitalize on progressive attention shifts within the reasoning process. These models demonstrate improved task performance and interpretability, as evaluated through attention and reasoning metrics. Through detailed visualizations, the models provide insights into the focal points of attention at different stages, shedding light on the decision-making processes. This not only boosts model transparency but also deepens comprehension of how attention evolves during reasoning, contributing to a more insightful interpretation of the model's functionality.</span></p>\n<p dir=\"ltr\"><span>We have expanded this effort by introducing a vision-language explanation framework explicitly defining a functional program for traversing the reasoning process, which empowers the construction of explanations and visualizations that articulate the decision-making process based on both textual explanation and visual grounding. A novel explanation model generates structured multimodal explanations concurrently with decision derivation. Interpreting the decision-making process through high-quality multimodal explanations and visualizations is a significant stride, advancing our understanding of reasoning processes and decision-making components, thus establishing trustworthy AI.</span></p>\n<p dir=\"ltr\"><span>Broadening our focus, we have further leveraged knowledge and seamlessly integrate it with attention and reasoning for model development. Unlike conventional deep neural networks constrained by their training data, our models showcase generalizability by accessing and utilizing knowledge as humans do. Specifically, our novel neural module network-based models dynamically allocate attention to relevant information and tightly integrate visual and external knowledge in intermediate reasoning steps during decision-making. This approach provides a dynamic perspective on the reasoning process, enhancing overall interpretability and generalizability.</span></p>\n<p dir=\"ltr\"><span>Furthermore, while previous trustworthiness models do not work well with real-life large-scale datasets, we have developed new machine learning methods to predict model trustworthiness in real-life complex situations. Notably, our approach addresses limitations in existing trustworthiness predictors by better separating the underlying distributions of positive and negative samples, enhancing prediction accuracy, and enabling a more nuanced tradeoff between them. Extensive experiments and analyses conducted on large-scale datasets validate the effectiveness and generalizability of our proposed trustworthiness predictor for complex data.&nbsp;</span></p>\n<p dir=\"ltr\"><span>By fostering the development of interpretable, generalizable, and trustworthy models, the project's outcomes have significantly impacted the broader landscape of artificial intelligence. This is increasingly crucial for the translation and deployment of AI, with implications both in the immediate future and for long-term developments. Breakthroughs from this project also offer potential life-changing improvements for the visually impaired population in the United States, granting them access to vital information from their surroundings. The emphasis on attention, reasoning, and ethical AI education set the project apart. Public accessibility of all datasets, analyses, and code promoted transparency and community engagement. The 21 publications, including prestigious conferences like CVPR, ECCV, and NeurIPS, signified the project's scholarly impact.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Over the reported period, ten students, spanning PhD, MSc, and undergraduate levels in computer science, data science, and robotics, actively contributed to this endeavor. The PI's close collaboration with these students not only advances their understanding of attention, reasoning, and trustworthy AI but also hones their research methodologies, scientific writing, and presentation skills. Notably, the project prioritized diversity, involving underrepresented students. The tangible outcomes extended beyond academia, with well-received research demos at summer camps targeting underrepresented groups. In essence, the project's influence has reached far and wide, contributing substantively to the ethical dimensions and trustworthiness of AI technologies.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/30/2023<br>\nModified by: Qi&nbsp;Zhao</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955893565_Slide4--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955893565_Slide4--rgov-800width.png\" title=\"REX: Reasoning-Aware and Grounded Explanation\"><img src=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955893565_Slide4--rgov-66x44.png\" alt=\"REX: Reasoning-Aware and Grounded Explanation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of the process for sequentially constructing multimodal explanation under our vision-language explanation framework. Partial explanations are shown to the right of the corresponding reasoning steps. The final explanation is obtained at the end.</div>\n<div class=\"imageCredit\">Qi Zhao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">REX: Reasoning-Aware and Grounded Explanation</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703956100086_Slide5--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703956100086_Slide5--rgov-800width.png\" title=\"Explicit Knowledge Incorporation for Visual Reasoning\"><img src=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703956100086_Slide5--rgov-66x44.png\" alt=\"Explicit Knowledge Incorporation for Visual Reasoning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Explicit visual reasoning methods often fail when the observation does not provide sufficient knowledge. Our method addresses this problem by generating scene graphs with explicit knowledge incorporation (e.g., suit-over-shirt) and inferring high-order relations (e.g., man-wearing-suit-over-shirt).</div>\n<div class=\"imageCredit\">Qi Zhao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">Explicit Knowledge Incorporation for Visual Reasoning</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955541308_Slide1--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955541308_Slide1--rgov-800width.png\" title=\"Progressive Attention and Reasoning for Decision Making\"><img src=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955541308_Slide1--rgov-66x44.png\" alt=\"Progressive Attention and Reasoning for Decision Making\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Attention and reasoning are two intertwined mechanisms underlying decision making. Answering questions correctly requires attention to the most relevant ROIs in the reasoning process (i.e., jeans, girl, and bag), while incorrect answers are caused by misdirected attention.</div>\n<div class=\"imageCredit\">Qi Zhao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">Progressive Attention and Reasoning for Decision Making</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703956217577_Slide6--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703956217577_Slide6--rgov-800width.png\" title=\"Learning to Predict Trustworthiness\"><img src=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703956217577_Slide6--rgov-66x44.png\" alt=\"Learning to Predict Trustworthiness\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustrations of trustworthiness prediction. In (a), the trustworthiness predictor acts as the oracle in the prediction process. In (b), it shows task difficulty with ImageNet. Predictions with confidence above (below) the positive (negative) threshold are classified as trustworthy (untrustworthy).</div>\n<div class=\"imageCredit\">Qi Zhao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">Learning to Predict Trustworthiness</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955621961_Slide2--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955621961_Slide2--rgov-800width.png\" title=\"Fantastic Answers and Where to Find Them: Immersive Question-Directed Visual Attention in 360-Degree Videos\"><img src=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955621961_Slide2--rgov-66x44.png\" alt=\"Fantastic Answers and Where to Find Them: Immersive Question-Directed Visual Attention in 360-Degree Videos\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Visual attention in 360-degree videos is driven by tasks. The correct attention (row 1) provides essential information for answering the question, while the incorrect attention (row 2) helps identify the distracting features to be avoided when designing intelligent visual systems.</div>\n<div class=\"imageCredit\">Qi Zhao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">Fantastic Answers and Where to Find Them: Immersive Question-Directed Visual Attention in 360-Degree Videos</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955701972_Slide3--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955701972_Slide3--rgov-800width.png\" title=\"AiR-E: Measuring the Alignment of Attention Maps with ROIs Relevant to Reasoning\"><img src=\"/por/images/Reports/POR/2023/1908711/1908711_10624040_1703955701972_Slide3--rgov-66x44.png\" alt=\"AiR-E: Measuring the Alignment of Attention Maps with ROIs Relevant to Reasoning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">AiR-E scores of Correct and Incorrect human attention maps, measuring their alignments with the bounding boxes of the ROIs.</div>\n<div class=\"imageCredit\">Qi Zhao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">AiR-E: Measuring the Alignment of Attention Maps with ROIs Relevant to Reasoning</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\n \n\n\nRecent advancements in AI, powered by big data and deep neural networks, have led to significant breakthroughs in various domains. Despite their impressive performance, the black-box nature of AI systems limits their understanding of complex decision-making processes. This project aims to bridge the current gap between the increasing performance in intelligent systems and the lack of understanding in the complex task-solving process. With the overarching objective of visualizing and modeling this process, the project delves into two intertwined mechanisms crucial for task-solvingattention and reasoningand formulates a robust framework to seamlessly integrate them. By enhancing attention and reasoning capacities in AI systems, the research has empowered machines to make intelligent and transparent decisions across diverse applications, addressing generalizability, interpretability, and trustworthy concerns in fields such as defense, business, healthcare, and autonomous vehicles.\n\n\nThroughout the project, we have made substantial progress in advancing the attention and reasoning capabilities of deep learning models. Our approach involves the creation of benchmark datasets that integrate attention, reasoning, and correctness labels, serving as foundational resources for attention and reasoning studies. We introduce a pioneering set of datasets, covering both images and 360-degree videos, to delve into attention, reasoning, and task performance in VQA. Additionally, a large-scale dataset focused on multi-modal explanations, incorporating grounding and reasoning, addresses key questions about model explanation and visualization. Our new explanation dataset for the first time encodes multi-modal information with a joint representation and explicitly considers the reasoning process.\n\n\nIn the pursuit of enhancing performance, explainability, and generalizability, novel deep-learning models have been crafted to capitalize on progressive attention shifts within the reasoning process. These models demonstrate improved task performance and interpretability, as evaluated through attention and reasoning metrics. Through detailed visualizations, the models provide insights into the focal points of attention at different stages, shedding light on the decision-making processes. This not only boosts model transparency but also deepens comprehension of how attention evolves during reasoning, contributing to a more insightful interpretation of the model's functionality.\n\n\nWe have expanded this effort by introducing a vision-language explanation framework explicitly defining a functional program for traversing the reasoning process, which empowers the construction of explanations and visualizations that articulate the decision-making process based on both textual explanation and visual grounding. A novel explanation model generates structured multimodal explanations concurrently with decision derivation. Interpreting the decision-making process through high-quality multimodal explanations and visualizations is a significant stride, advancing our understanding of reasoning processes and decision-making components, thus establishing trustworthy AI.\n\n\nBroadening our focus, we have further leveraged knowledge and seamlessly integrate it with attention and reasoning for model development. Unlike conventional deep neural networks constrained by their training data, our models showcase generalizability by accessing and utilizing knowledge as humans do. Specifically, our novel neural module network-based models dynamically allocate attention to relevant information and tightly integrate visual and external knowledge in intermediate reasoning steps during decision-making. This approach provides a dynamic perspective on the reasoning process, enhancing overall interpretability and generalizability.\n\n\nFurthermore, while previous trustworthiness models do not work well with real-life large-scale datasets, we have developed new machine learning methods to predict model trustworthiness in real-life complex situations. Notably, our approach addresses limitations in existing trustworthiness predictors by better separating the underlying distributions of positive and negative samples, enhancing prediction accuracy, and enabling a more nuanced tradeoff between them. Extensive experiments and analyses conducted on large-scale datasets validate the effectiveness and generalizability of our proposed trustworthiness predictor for complex data.\n\n\nBy fostering the development of interpretable, generalizable, and trustworthy models, the project's outcomes have significantly impacted the broader landscape of artificial intelligence. This is increasingly crucial for the translation and deployment of AI, with implications both in the immediate future and for long-term developments. Breakthroughs from this project also offer potential life-changing improvements for the visually impaired population in the United States, granting them access to vital information from their surroundings. The emphasis on attention, reasoning, and ethical AI education set the project apart. Public accessibility of all datasets, analyses, and code promoted transparency and community engagement. The 21 publications, including prestigious conferences like CVPR, ECCV, and NeurIPS, signified the project's scholarly impact.\n\n\nOver the reported period, ten students, spanning PhD, MSc, and undergraduate levels in computer science, data science, and robotics, actively contributed to this endeavor. The PI's close collaboration with these students not only advances their understanding of attention, reasoning, and trustworthy AI but also hones their research methodologies, scientific writing, and presentation skills. Notably, the project prioritized diversity, involving underrepresented students. The tangible outcomes extended beyond academia, with well-received research demos at summer camps targeting underrepresented groups. In essence, the project's influence has reached far and wide, contributing substantively to the ethical dimensions and trustworthiness of AI technologies.\n\n\n\t\t\t\t\tLast Modified: 12/30/2023\n\n\t\t\t\t\tSubmitted by: QiZhao\n"
 }
}