{
 "awd_id": "1925194",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: INT: FIngers See Things Differently (FIST-D): A Robotic Explosive Ordnance Disposal (EOD) based on Augmented Tactile Imaging",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928950",
 "po_email": "rwachter@nsf.gov",
 "po_sign_block_name": "Ralph Wachter",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 1499795.0,
 "awd_amount": 1499795.0,
 "awd_min_amd_letter_date": "2019-08-30",
 "awd_max_amd_letter_date": "2023-08-10",
 "awd_abstract_narration": "Explosive ordnance disposal is among the more hazardous occupations whether in the military and law enforcement when confronted with improvised explosive devices (IEDs), or after natural disasters when exposed explosive materials must be quickly, safely, and expeditiously disposed.  Explosive ordnance disposal (EOD) technicians respond to dangerous situations, wearing only protective suits, which may reduce the impact of a blast but do not provide complete protection.  Moreover, EOD technicians are subject to fatigue, heat stress, and reduced dexterity.  These can lead to a reduced ability by EOD technicians to deal with threats effectively and safely. \r\n\r\nRobots can greatly mitigate many of the risks, help reduce a technician's exposure and time-on-target, lessen the risk of otherwise very dangerous situations, and keep the technician better out of harm's way. Technician are still need albeit from afar by teleoperating the robots. A chronic problem in EOD is the limited perceptual information that such robotic systems can convey. Currently available robots depend heavily on vision and video telemetry, which can be drastically impaired when IEDs are buried or concealed. \r\n\r\nThis research addresses this scientific challenge by developing a multi-modal perceptual image comprised of tactile, optical, and force information, using a telerobot. Such a telerobot will: 1) deliver a multimodal image in a format that is easily interpretable by the operator; 2) learn from limited observations using principles of machine learning; 3) recognize objects from texture and weight signatures; and 4) recommend best strategies to approach and explore the objects. In addition to its main application, the theories and technologies involved in this proposal impact other fields in which dexterity and tactile feedback is key for successful task completion, such as tele-surgery. A unique feature of this project is a set of bimanual tool tips, equipped with multi-sensory devices for collecting tactile, force and chemical composition information for target characterization and action. As the exploration takes place, object profiles (based on sensed information) are built from discrete observations, which are accumulated and fitted to machine learning models based on transfer learning and zero/one-shot learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Juan",
   "pi_last_name": "Wachs",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Juan P Wachs",
   "pi_email_addr": "jpwachs@purdue.edu",
   "nsf_id": "000557038",
   "pi_start_date": "2019-08-30",
   "pi_end_date": "2021-05-18"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Wenzhuo",
   "pi_last_name": "Wu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wenzhuo Wu",
   "pi_email_addr": "wu966@purdue.edu",
   "nsf_id": "000706209",
   "pi_start_date": "2021-05-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Beaudoin",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen P Beaudoin",
   "pi_email_addr": "sbeaudoi@purdue.edu",
   "nsf_id": "000103903",
   "pi_start_date": "2019-08-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Hong",
   "pi_last_name": "Tan",
   "pi_mid_init": "Z",
   "pi_sufx_name": "",
   "pi_full_name": "Hong Z Tan",
   "pi_email_addr": "hongtan@purdue.edu",
   "nsf_id": "000107264",
   "pi_start_date": "2019-08-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Bryan",
   "pi_last_name": "Boudouris",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Bryan W Boudouris",
   "pi_email_addr": "boudouris@ua.edu",
   "nsf_id": "000598117",
   "pi_start_date": "2019-08-30",
   "pi_end_date": "2020-09-11"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Wenzhuo",
   "pi_last_name": "Wu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wenzhuo Wu",
   "pi_email_addr": "wu966@purdue.edu",
   "nsf_id": "000706209",
   "pi_start_date": "2019-08-30",
   "pi_end_date": "2021-05-18"
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "420 Central Drive",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072017",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 1499795.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>NSF Annual Project Report # 1925194</strong></p>\r\n<p>Outcomes</p>\r\n<p>Explosive ordnance disposal is one of the most hazardous occupations. Although various robots for explosive ordinance disposal (EOD) can help eliminate EOD and reduce the risk of otherwise deadly scenarios, most of them depend heavily on optical vision for telemetry. However, their performance can be drastically impaired when improvised explosive devices (IEDs) are concealed. In other words, the ability to deal with such threats depends on perceptual information rather than optical information. In this regard, the major goal of the project is to develop an EOD robot with multimodal perceptual abilities, including vision, chemical sensing, tactile, and force. The enhanced perception ability will be presented to the human operator for more accurate decision-making and teleoperation.&nbsp;</p>\r\n<p>&nbsp;The outcomes are summarized as follows: &nbsp;</p>\r\n<p>&nbsp;(1)&nbsp;&nbsp;The team developed a comprehensive suite of algorithms and technologies for advanced tactile exploration and object interaction. Key achievements include an object recognition and reconstruction algorithm that predicts object categories and completes shapes from partially observed point clouds, enabling feasible grasp generation. An autonomous tactile exploration algorithm was created to localize objects without vision. A novel whisker sensor designed to minimize intrusiveness enables reactive behaviors such as tapping, probing, and dragging for detailed object characterization. Inspired by caterpillar mechanics, a spring-based compressive continuum robot executes bending and compression motions to optimize whisker-based tactile exploration. Additionally, a whisker tactile array combining magnetic actuation with vision-based feedback facilitates precise tactile exploration and object grasping. These innovations have advanced tactile sensing and robotics, offering transformative tools for minimally invasive and adaptive object interactions.</p>\r\n<p>(2)&nbsp;&nbsp;This research investigated strategies for conveying information to human operators using haptic, visual, and auditory modalities. Significant accomplishments include conducting an experiment on selective attention to haptic information presented simultaneously across multiple body sites, and developing a subsequent experiment to evaluate the proposed haptic information displays in a virtual reality (VR) environment. These endeavors advance our understanding of multimodal information delivery and provide a foundation for designing effective haptic interfaces in immersive and interactive settings.</p>\r\n<p>(3)&nbsp;&nbsp;The team has developed innovative wearable triboelectric technologies for advanced tactile sensing and object recognition. A triboelectric device was created to convert contact events into distinct electrical signals, correlating the triboelectric signals with the surface texture or topography. An integrated pixelated triboelectric device enables multipoint sensing and mechanical contact imaging. Additionally, a whisker sensor was designed for minimally intrusive object interactions. This study culminated in a bioinspired triboelectric wearable stereognosis sensor (BEWARE) that mimics human stereognosis by combining proprioception and touch sensing via textile-integrated triboelectric units. An artificial neural network processes sensor data and recognizes diverse objects, including those that are visually indistinguishable. The BEWARE system also identifies unknown objects beyond its initial training set, demonstrating adaptability and versatility.</p>\r\n<p>(4)&nbsp;&nbsp;The team demonstrated the proof of concept for organic electrochemical transistor (OECT) sensors and developed prototype OECT devices. This study also quantified the approximate amount of explosive residue collected through surface swabbing, highlighting the potential of OECT sensors for sensitive and practical explosive detection applications.</p>\r\n<p>&nbsp;</p>\r\n<p>In addition to the above research outcomes, the team has also achieved the following key achievements:</p>\r\n<p>1)&nbsp;&nbsp;&nbsp;&nbsp;The object recognition algorithm and hardware using for the innovative whisker manipulator (COMPlacent), which is designed to accomplish tactile exploration with minimum intrusiveness has been accepted to IROS 2023 and is presented in that venue.</p>\r\n<p>2)&nbsp;&nbsp;&nbsp;&nbsp;The complete and integrate full scale system has been accepted for publication &ldquo; Sensing, Haptic Rendering, and Assistant Approach for a Telepresence Explosive Ordnance Disposal Robot\". (2023). IEEE Transactions on Robotics (TRO).</p>\r\n<p>3)&nbsp;&nbsp;&nbsp;&nbsp;Chenxi Xiao defended his thesis successfully by the beginning of Summer 2023. &ldquo;OBJECT EXPLORATION, CHARACTERIZATION, AND RECOGNITION BASED ON TACTILE SENSING&rdquo;.</p>\r\n<p>4)&nbsp;&nbsp;&nbsp;&nbsp;Shujia Xu defended his thesis successfully by the beginning of Spring 2024. &ldquo;SCALABLY MANUFACTURED SKIN-INTERFACED TRIBOELECTRIC SENSORS FOR HUMAN-ROBOTICS TEAMING&rdquo;.</p>\r\n<p>5)&nbsp;&nbsp;&nbsp;&nbsp;The spring-based compressive continuum robot has been accepted and presented at the post session of ICRA@40.</p>\r\n<p>6)&nbsp;&nbsp;&nbsp;&nbsp;The project has been providing the first-hand research training opportunity to the following students in the Purdue University: Aaron Benjamin Woeppel, Chenxi Xiao, Gina Marie Clepper, Janessa Elaine Schaefer, Min Wu, Shengjie Gao, Shujia Xu, Johannes Friedrich Rueschen, Diana Alejandra Narvaez Bernal, Zhixian Hu, Yi Cheng. From Dr. Wachs&rsquo; team, Chenxi Xiao and Diana Alejandra Narvaez Bernal have finished their studies. Dr. Xiao started a faculty position in Shanhai Tech, and Mrs. Bernal started a PhD with a different group. Recently Zhixian Hu has joined Dr. Wachs&rsquo; team as a new PhD student together with Prof. Yu She.</p>\r\n<p>7)&nbsp;&nbsp;&nbsp;&nbsp;The results have been disseminated to communities of interest through NSF NRI PI meeting. In addition, the research outcomes have been disseminated through presentations in academic conferences to communities of robotics and haptics. There are several presentations made by the research students in this year between 2023-2024, some including IROS and other robotics venues.</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/13/2025<br>\nModified by: Wenzhuo&nbsp;Wu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nNSF Annual Project Report # 1925194\r\n\n\nOutcomes\r\n\n\nExplosive ordnance disposal is one of the most hazardous occupations. Although various robots for explosive ordinance disposal (EOD) can help eliminate EOD and reduce the risk of otherwise deadly scenarios, most of them depend heavily on optical vision for telemetry. However, their performance can be drastically impaired when improvised explosive devices (IEDs) are concealed. In other words, the ability to deal with such threats depends on perceptual information rather than optical information. In this regard, the major goal of the project is to develop an EOD robot with multimodal perceptual abilities, including vision, chemical sensing, tactile, and force. The enhanced perception ability will be presented to the human operator for more accurate decision-making and teleoperation.\r\n\n\nThe outcomes are summarized as follows: \r\n\n\n(1)The team developed a comprehensive suite of algorithms and technologies for advanced tactile exploration and object interaction. Key achievements include an object recognition and reconstruction algorithm that predicts object categories and completes shapes from partially observed point clouds, enabling feasible grasp generation. An autonomous tactile exploration algorithm was created to localize objects without vision. A novel whisker sensor designed to minimize intrusiveness enables reactive behaviors such as tapping, probing, and dragging for detailed object characterization. Inspired by caterpillar mechanics, a spring-based compressive continuum robot executes bending and compression motions to optimize whisker-based tactile exploration. Additionally, a whisker tactile array combining magnetic actuation with vision-based feedback facilitates precise tactile exploration and object grasping. These innovations have advanced tactile sensing and robotics, offering transformative tools for minimally invasive and adaptive object interactions.\r\n\n\n(2)This research investigated strategies for conveying information to human operators using haptic, visual, and auditory modalities. Significant accomplishments include conducting an experiment on selective attention to haptic information presented simultaneously across multiple body sites, and developing a subsequent experiment to evaluate the proposed haptic information displays in a virtual reality (VR) environment. These endeavors advance our understanding of multimodal information delivery and provide a foundation for designing effective haptic interfaces in immersive and interactive settings.\r\n\n\n(3)The team has developed innovative wearable triboelectric technologies for advanced tactile sensing and object recognition. A triboelectric device was created to convert contact events into distinct electrical signals, correlating the triboelectric signals with the surface texture or topography. An integrated pixelated triboelectric device enables multipoint sensing and mechanical contact imaging. Additionally, a whisker sensor was designed for minimally intrusive object interactions. This study culminated in a bioinspired triboelectric wearable stereognosis sensor (BEWARE) that mimics human stereognosis by combining proprioception and touch sensing via textile-integrated triboelectric units. An artificial neural network processes sensor data and recognizes diverse objects, including those that are visually indistinguishable. The BEWARE system also identifies unknown objects beyond its initial training set, demonstrating adaptability and versatility.\r\n\n\n(4)The team demonstrated the proof of concept for organic electrochemical transistor (OECT) sensors and developed prototype OECT devices. This study also quantified the approximate amount of explosive residue collected through surface swabbing, highlighting the potential of OECT sensors for sensitive and practical explosive detection applications.\r\n\n\n\r\n\n\nIn addition to the above research outcomes, the team has also achieved the following key achievements:\r\n\n\n1)The object recognition algorithm and hardware using for the innovative whisker manipulator (COMPlacent), which is designed to accomplish tactile exploration with minimum intrusiveness has been accepted to IROS 2023 and is presented in that venue.\r\n\n\n2)The complete and integrate full scale system has been accepted for publication  Sensing, Haptic Rendering, and Assistant Approach for a Telepresence Explosive Ordnance Disposal Robot\". (2023). IEEE Transactions on Robotics (TRO).\r\n\n\n3)Chenxi Xiao defended his thesis successfully by the beginning of Summer 2023. OBJECT EXPLORATION, CHARACTERIZATION, AND RECOGNITION BASED ON TACTILE SENSING.\r\n\n\n4)Shujia Xu defended his thesis successfully by the beginning of Spring 2024. SCALABLY MANUFACTURED SKIN-INTERFACED TRIBOELECTRIC SENSORS FOR HUMAN-ROBOTICS TEAMING.\r\n\n\n5)The spring-based compressive continuum robot has been accepted and presented at the post session of ICRA@40.\r\n\n\n6)The project has been providing the first-hand research training opportunity to the following students in the Purdue University: Aaron Benjamin Woeppel, Chenxi Xiao, Gina Marie Clepper, Janessa Elaine Schaefer, Min Wu, Shengjie Gao, Shujia Xu, Johannes Friedrich Rueschen, Diana Alejandra Narvaez Bernal, Zhixian Hu, Yi Cheng. From Dr. Wachs team, Chenxi Xiao and Diana Alejandra Narvaez Bernal have finished their studies. Dr. Xiao started a faculty position in Shanhai Tech, and Mrs. Bernal started a PhD with a different group. Recently Zhixian Hu has joined Dr. Wachs team as a new PhD student together with Prof. Yu She.\r\n\n\n7)The results have been disseminated to communities of interest through NSF NRI PI meeting. In addition, the research outcomes have been disseminated through presentations in academic conferences to communities of robotics and haptics. There are several presentations made by the research students in this year between 2023-2024, some including IROS and other robotics venues.\r\n\n\n\r\n\n\n\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 01/13/2025\n\n\t\t\t\t\tSubmitted by: WenzhuoWu\n"
 }
}