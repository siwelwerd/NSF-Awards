{
 "awd_id": "1946752",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: Collaborative Research: Parallel Algorithm by Blocks - A Data-centric Compiler/runtime System for Productive Programming of Scalable Parallel Systems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Damian Dechev",
 "awd_eff_date": "2019-07-31",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 440033.0,
 "awd_amount": 440033.0,
 "awd_min_amd_letter_date": "2019-09-19",
 "awd_max_amd_letter_date": "2019-09-19",
 "awd_abstract_narration": "Achieving both high productivity and high performance on scalable parallel and heterogeneous computer systems is a challenging goal for application developers. Parallel programming with Message Passing Interface (MPI) is currently the most widely used and effective means of developing scalable parallel applications; however the productivity of application developers is lower than with programming models that offer a global shared view of data structures. In comparison, achieving high performance and scalability with global-address-space programming models is challenging. This project focuses on the development of a data-centric compiler/runtime framework, \"Parallel Algorithms by Blocks\" (PAbB), aimed at offering users the combined positive attributes of multiple parallel programming models without the disadvantages. The main novelty of this project is that it uses a combination of user insights, new compiler optimizations, and advanced runtime support to achieve both productivity and performance for an important class of computations that operate on matrices, tensors, and graphs. The main broader impact of the work is that it can significantly lower the barrier to entry for scientists from various domains who wish to develop new high-performance applications on large scale parallel systems, but presently find it too difficult with currently available parallel programming models. \r\n\r\nThis project brings together a team of investigators, with expertise across the software stack, to develop compiler tools and runtime systems for PAbB and demonstrate its use across a number of applications from computational science and data science. The PAbB model is intended to work in concert with MPI; that is, PAbB programs can execute in any standard MPI environment, interoperating with other native MPI code. The key idea behind the proposed approach is to offer the user a global-address view of the targeted data structures, requiring only (optionally in some cases) that they specify how data should be partitioned, but have the compiler/runtime handle the tedious aspects of the global-to-local re-indexing and inter-node data movement. In addition to the productivity benefit, a second significant benefit is in enabling system support for dynamic load balancing. The approach is being designed and demonstrated in the context of applications operating on dense and sparse matrices and tensors, and graphs.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ponnuswamy",
   "pi_last_name": "Sadayappan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ponnuswamy Sadayappan",
   "pi_email_addr": "saday@cs.utah.edu",
   "nsf_id": "000182536",
   "pi_start_date": "2019-09-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Utah",
  "inst_street_address": "201 PRESIDENTS CIR",
  "inst_street_address_2": "",
  "inst_city_name": "SALT LAKE CITY",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8015816903",
  "inst_zip_code": "841129049",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "UT01",
  "org_lgl_bus_name": "UNIVERSITY OF UTAH",
  "org_prnt_uei_num": "",
  "org_uei_num": "LL8GLEVH6MG3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Utah",
  "perf_str_addr": "75 S 2000 E",
  "perf_city_name": "SALT LAKE CITY",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "841128930",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "UT01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 440033.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The primary goal of the project was to develop data-block-centric  compile/runtime techniques for enabling high productivity and  performance in implementing parallel algorithms. The multi-institutional collaborative project collectievly developed parallel algorithms for several key data structures, including index-based data structures (Washington State University), graphs (Georgia Institute of Technology) and matrices/tensors (University of Utah).<br /><br />The main outcomes were:<br /><br />1)&nbsp;&nbsp;&nbsp;  Development of a novel method for compact capture of the impact of the  sparsity structure of a matrix on achievable data reuse for SpMM  (sparse-matrix dense-matrix multiplication) as a one-dimensional  signature and its use for efficient tile-size/blocking optimization for  parallel multicore SpMM.<br />2)&nbsp;&nbsp;&nbsp; Development of an analytical  characterization for data movement volume as a parametric function of  tile sizes for multi-level tiled CNN (Convolutional Neural Network) and  its use for synthesizing optimized multicore implementation of CNNs.<br />3)&nbsp;&nbsp;&nbsp;  Development of an analytical methodology for algorithm-architecture  co-design optimization for CNNs on coarse-grained spatial accelerators,  enabling energy and performance optimization.<br />4)&nbsp;&nbsp;&nbsp; Development of a  methodology for analysis and optimization of computation/data reuse in  Canonical Polyadic decomposition of sparse tensors.<br />5)&nbsp;&nbsp;&nbsp; Development  of a new approach to efficient distributed GNN (Graph Neural Network)  using redistribution of dense matrices between stages of the GNN.<br />6)&nbsp;&nbsp;&nbsp; Characterization of performance portability challenges for implementing tensor contractions on GPUs.</p><br>\n<p>\n Last Modified: 06/12/2024<br>\nModified by: Ponnuswamy&nbsp;Sadayappan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe primary goal of the project was to develop data-block-centric  compile/runtime techniques for enabling high productivity and  performance in implementing parallel algorithms. The multi-institutional collaborative project collectievly developed parallel algorithms for several key data structures, including index-based data structures (Washington State University), graphs (Georgia Institute of Technology) and matrices/tensors (University of Utah).\n\nThe main outcomes were:\n\n1)  Development of a novel method for compact capture of the impact of the  sparsity structure of a matrix on achievable data reuse for SpMM  (sparse-matrix dense-matrix multiplication) as a one-dimensional  signature and its use for efficient tile-size/blocking optimization for  parallel multicore SpMM.\n2) Development of an analytical  characterization for data movement volume as a parametric function of  tile sizes for multi-level tiled CNN (Convolutional Neural Network) and  its use for synthesizing optimized multicore implementation of CNNs.\n3)  Development of an analytical methodology for algorithm-architecture  co-design optimization for CNNs on coarse-grained spatial accelerators,  enabling energy and performance optimization.\n4) Development of a  methodology for analysis and optimization of computation/data reuse in  Canonical Polyadic decomposition of sparse tensors.\n5) Development  of a new approach to efficient distributed GNN (Graph Neural Network)  using redistribution of dense matrices between stages of the GNN.\n6) Characterization of performance portability challenges for implementing tensor contractions on GPUs.\t\t\t\t\tLast Modified: 06/12/2024\n\n\t\t\t\t\tSubmitted by: PonnuswamySadayappan\n"
 }
}