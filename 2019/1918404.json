{
 "awd_id": "1918404",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Doctoral Dissertation Research: Lexical Acoustics",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Mary Paster",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2023-01-31",
 "tot_intn_awd_amt": 9923.0,
 "awd_amount": 9923.0,
 "awd_min_amd_letter_date": "2019-08-04",
 "awd_max_amd_letter_date": "2019-08-04",
 "awd_abstract_narration": "This project will document which acoustic properties of the speech signal English listeners use to communicate by looking at patterns in both their correct recognition of words, and their confusions between words, when listening in a noisy environment. The project will enhance understanding of speech production and perception, particularly regarding how acoustic features are distributed throughout the words of a language to minimize misperceptions and therefore facilitate reliable communication. More broadly, this work is relevant for research on the relation between speech sounds and word knowledge in both adults and children, contributing to the identification of unique distinctions between languages that affect second language learners, and ultimately informing models focusing on various types of hearing impairment in everyday communication.\r\n\r\nTwo general models of the structure of the sound system of a language are contrasted. The first model, referred to as the \"inventory model\", is the canonical approach that forms the basis for most phonetic descriptions of sound systems in speech perception. Under the inventory model, the sound system is structured as a fixed set of contrastive consonants and vowels, where this set is treated as homogeneous in weight such that all elements play an equal role in defining the system and determining the relative importance of individual acoustic cues. The second model, referred to as the lexicon model, considers the system of contrastive sounds to be critically distributed over the lexicon, such that the relative weight of different cues now depends on the number and configuration of words those cues serve to distinguish. In contrasting these two approaches, two main questions are asked: (1) how well do the models agree in their estimates of cue weights, and what accounts for points of agreement and disagreement; and (2) what does each model entail for the stability of the system in the presence of background noise in the environment. The inventory model uses a database of controlled productions of syllables to predict prior-published confusion patterns between sounds, with cue weights then measured as the relative importance of each cue in the model predicting listener perception patterns. For the lexicon model, a database of nearly 27,000 words produced by a single speaker is used to predict listener word recognition patterns when these words are embedded in noise in a series of six experiments utilizing different tasks (open, fill-in-the-blank recognition, and closed choice between two similar-sounding words) and different stimuli (naturally produced, enhanced, and degraded). The cue weights from these two model fits are then compared to address the first question. To answer the second question, simulations of noise added to different cues, and the resulting change in predicted similarity between sounds (in the inventory case) and words (in the lexicon case), are used to study the stability and adaptive response of each system to uncertainty in the acoustic signal. As such, these experiments will be able to address key claims about the structure and organization of sound systems in language.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Allard",
   "pi_last_name": "Jongman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Allard Jongman",
   "pi_email_addr": "jongman@ku.edu",
   "nsf_id": "000396581",
   "pi_start_date": "2019-08-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "Redmon",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Charles H Redmon",
   "pi_email_addr": "redmon@ku.edu",
   "nsf_id": "000796095",
   "pi_start_date": "2019-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Kansas Center for Research Inc",
  "inst_street_address": "2385 IRVING HILL RD",
  "inst_street_address_2": "",
  "inst_city_name": "LAWRENCE",
  "inst_state_code": "KS",
  "inst_state_name": "Kansas",
  "inst_phone_num": "7858643441",
  "inst_zip_code": "660457563",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "KS01",
  "org_lgl_bus_name": "UNIVERSITY OF KANSAS CENTER FOR RESEARCH INC",
  "org_prnt_uei_num": "SSUJB3GSH8A5",
  "org_uei_num": "SSUJB3GSH8A5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Kansas Center for Research Inc",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "KS",
  "perf_st_name": "Kansas",
  "perf_zip_code": "660457568",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "KS01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "837400",
   "pgm_ele_name": "DDRI Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 9923.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p style=\"line-height: 100%; margin-bottom: 0cm;\">In communication, the process by which listeners take the sound wave that hits their ear drum and map it onto meaningful words and concepts in their brain remains poorly understood. One important aspect of this process is understanding which features in the acoustic signal listeners must attend to, and to what degree, in order to successfully receive the communicated message. Until now the predominant approach has been to study the individual sounds that make up words and are necessary to distinguish them from each other (e.g., [b] in 'bat' vs. [p] in 'pat'), and further to study this set of sounds in isolation from the complex set of words they distinguish in the mental lexicon (the dictionary of words in a speaker/listener's mind). This choice has been both for simplicity and convenience, but in the present study we aimed to test an alternative approach that properly contextualizes such sounds by directly studying the words that must be distinguished in communication. To this end we collected a large amount of data on English word recognition in noise (nearly 2000 words presented to 160 listeners), and then built computer models predicting listener perception patterns from different features of the acoustic signal. From these models we were able to identify the features that were most and least attended to by listeners in recognizing a wide and representative set of English words. We then compared these results with those of prior studies to identify which acoustic characteristics have been overlooked when not considered in the context of a more realistic word recognition process, and conversely those which may have been overemphasized when studied outside of this context. Further, we were able to identify a limited, efficient subset of properties of the sound wave that are most vital to the successful transmission of information in English communication. These results have several real-world implications, most notably in clinical support for hearing impairment and hearing loss, and in language education. In the area of hearing impairment/loss, our study demonstrates a new methodology for identifying the practical communicative consequences for degredation of sensitivity to different frequency ranges or loss of temporal acuity. That is, rather than estimating the impact of such changes on patients' ability to hear individual speech sounds, we can estimate the size and arrangement of words that would be most likely to be misperceived due to hearing impairment/loss. This information is not only more informative for patients' use of hearing in everyday communication, but is easier to explain as people often have a clearer understanding of words and which ones they confuse than they do of individual sounds. Thus we hope that this work can eventually be applied in the clinic and in the testing and calibration of hearing aids. Regarding language education, because our approach identifies features of the acoustic signal that are most critical in communication, we can use that information to better inform language learners (particularly in foreign language classrooms) of which properties they should attend to the most in developing their listening comprehension skills, and similarly what they should most practice in language production for successful communication. Ultimately, context is everything in communication, and our study both demonstrates that fact and provides a concrete methodology for better incorporating contextual information in our understanding of speech perception and production.</p>\n<!-- p { line-height: 115%; margin-bottom: 0.25cm; background: transparent } --><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/01/2023<br>\n\t\t\t\t\tModified by: Charles&nbsp;H&nbsp;Redmon</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "In communication, the process by which listeners take the sound wave that hits their ear drum and map it onto meaningful words and concepts in their brain remains poorly understood. One important aspect of this process is understanding which features in the acoustic signal listeners must attend to, and to what degree, in order to successfully receive the communicated message. Until now the predominant approach has been to study the individual sounds that make up words and are necessary to distinguish them from each other (e.g., [b] in 'bat' vs. [p] in 'pat'), and further to study this set of sounds in isolation from the complex set of words they distinguish in the mental lexicon (the dictionary of words in a speaker/listener's mind). This choice has been both for simplicity and convenience, but in the present study we aimed to test an alternative approach that properly contextualizes such sounds by directly studying the words that must be distinguished in communication. To this end we collected a large amount of data on English word recognition in noise (nearly 2000 words presented to 160 listeners), and then built computer models predicting listener perception patterns from different features of the acoustic signal. From these models we were able to identify the features that were most and least attended to by listeners in recognizing a wide and representative set of English words. We then compared these results with those of prior studies to identify which acoustic characteristics have been overlooked when not considered in the context of a more realistic word recognition process, and conversely those which may have been overemphasized when studied outside of this context. Further, we were able to identify a limited, efficient subset of properties of the sound wave that are most vital to the successful transmission of information in English communication. These results have several real-world implications, most notably in clinical support for hearing impairment and hearing loss, and in language education. In the area of hearing impairment/loss, our study demonstrates a new methodology for identifying the practical communicative consequences for degredation of sensitivity to different frequency ranges or loss of temporal acuity. That is, rather than estimating the impact of such changes on patients' ability to hear individual speech sounds, we can estimate the size and arrangement of words that would be most likely to be misperceived due to hearing impairment/loss. This information is not only more informative for patients' use of hearing in everyday communication, but is easier to explain as people often have a clearer understanding of words and which ones they confuse than they do of individual sounds. Thus we hope that this work can eventually be applied in the clinic and in the testing and calibration of hearing aids. Regarding language education, because our approach identifies features of the acoustic signal that are most critical in communication, we can use that information to better inform language learners (particularly in foreign language classrooms) of which properties they should attend to the most in developing their listening comprehension skills, and similarly what they should most practice in language production for successful communication. Ultimately, context is everything in communication, and our study both demonstrates that fact and provides a concrete methodology for better incorporating contextual information in our understanding of speech perception and production.\n\n\n\t\t\t\t\tLast Modified: 06/01/2023\n\n\t\t\t\t\tSubmitted by: Charles H Redmon"
 }
}