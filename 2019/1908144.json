{
 "awd_id": "1908144",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "OAC Core: Small: Collaborative Research: Scalable Run-Time for Highly Parallel, Heterogeneous Systems",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922625",
 "po_email": "jjli@nsf.gov",
 "po_sign_block_name": "Juan Li",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2019-06-19",
 "awd_max_amd_letter_date": "2019-06-19",
 "awd_abstract_narration": "Supercomputing has become an essential tool in many scientific fields, including advances in engineering and medicine, and contributes to national security. Progress in many areas depends on continued improvements in the performance of supercomputers and their usability. Communication between processes is a critical component of this effort and is the target of this project. This project departs from the traditional communication protocols. Rather, the project  focuses on providing middle ground solutions between hardware and software. This approach potentially  reduces communication overheads and better matches the functionality of the communication library to the capabilities of modern communication adapters and also improves the match between the requirements of modern parallel computing frameworks and applications. By improving the communication capabilities of computational platforms, this project will promote faster and more flexible communication capabilities and will improve the time to completion of scientific applications.  It, therefore, increases the scientific throughput of existing and future cyberinfrastructure platforms. The research and educational outcomes of this project are closely related, resulting in highly trained new generations of researchers and engineers leading to a more efficient and globally competent workforce. Therefore, this project aligns with the NSF's mission to promote the progress of science and to advance national prosperity and welfare through science, and serves the national interest.\r\n \r\nThis project brings together a multidisciplinary team and aims at breaking away from the limitation of standards such as Message Passing Interface and pointing the way for handling the needs of future computational frameworks and high-end systems. To this end the project (1) designs and implements a communication library with new communication primitives to enable fast coordination with no serial bottleneck, to manage irregular, fine grain communication, and to provide new efficient synchronization mechanisms; (2) demonstrates the value of this library by using it to accelerate multiple task-based runtimes (Legion, PaRSEC) and communication libraries (MPI and GasNET); (3) demonstrates the value of hardware support by porting key components to a programmable NIC; and (4) delivers improvements and extensions to mainstream communication libraries to provide the new functionality. This work puts a special emphasis on emerging programming models, such as Legion or PaRSEC, and on emerging application domains, such as graph analytics. It aims at an orthogonal design where different mechanisms for associating producer buffer with consumer buffer can be composed with different mechanisms for synchronizing producer and consumer; and where mechanisms can be specialized so as to allow efficient hardware support.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marc",
   "pi_last_name": "Snir",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Marc Snir",
   "pi_email_addr": "snir@illinois.edu",
   "nsf_id": "000165753",
   "pi_start_date": "2019-06-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S. Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "090Y00",
   "pgm_ele_name": "OAC-Advanced Cyberinfrast Core"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our project concerns the software that is used to run High-Performance Computing (HPC) platforms. Such systens can contain thousands of interconnected processors, each containing thousands of computing cores. Over the years, these systems have become larger and more heterogeneous, combining different types of compute cores and memories. Scientific applications are also becoming more complex as they use sparse data structures and adaptive algorithms in order to improve performance.</p>\n<p>HPC scientific applications have been traditionally written in a bulk-synchronous style, where all compute cores work in lock-step. This model is ill suited to heterogenous hardware and irregular, dynamic application codes. An alternative that has recently generated significant interest comes under the name of Asynchonous Multi-Task systems (AMTs) (previously known as hybrid dataflow). The basic concept is that a computation is described as a graph of tasks (simple, short sequential programs), and dependencies between tasks: A dependency exists when one task uses results produced by another task. The user specifies the task codes and the dependency graph, in some manner. The runtime system manages the movement of data from producing task to consuming task, and schedules tasks for execution when all their dependencies are satisfied.</p>\n<p>The execution of applications coded using an AMT generate communication patterns that are different than those occuring in more conventional HPC: Many threads may communicate concurrently, and messages are asynchronous. Executions generate a mix of long data messages and short control messages, and the communication pattern varies over time. Traditional communication libraries, such as MPI, do not handle such communication very efficiently. We developed a new communication library, the Lightweight Communication Library (LCI) that provides a better match to the needs of AMTs. LCI does so by supporting new mechanisms for signaling the completion of communications (the interface to the scheduler) and for managing memory required by communications (the interface to the memory manager). It is designed to reduce the need for coarse-grain locking that causes conflicts when many threads communicate concurrently; the implementation is also optimized to reduce the use of such locks.</p>\n<p>We replaced MPI with LCI as the communication mechanism under two current AMT systems: PaRSEC, developed at UTK, and HPX, developed at LSU. We tested both using specially designed micro-benchmarks, and by running two large apllication that were coded to use AMTs: HiCMA, a Tiled Low Rank Cholesky solver, for PeRCS; and Octo-Tiger, an astrophysics application, for HPX. LCI significantly improved execution time, for both. We hope that the lessons learned from this project will affect the design of future communication libraries, or improvements in the current one.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2023<br>\n\t\t\t\t\tModified by: Marc&nbsp;Snir</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur project concerns the software that is used to run High-Performance Computing (HPC) platforms. Such systens can contain thousands of interconnected processors, each containing thousands of computing cores. Over the years, these systems have become larger and more heterogeneous, combining different types of compute cores and memories. Scientific applications are also becoming more complex as they use sparse data structures and adaptive algorithms in order to improve performance.\n\nHPC scientific applications have been traditionally written in a bulk-synchronous style, where all compute cores work in lock-step. This model is ill suited to heterogenous hardware and irregular, dynamic application codes. An alternative that has recently generated significant interest comes under the name of Asynchonous Multi-Task systems (AMTs) (previously known as hybrid dataflow). The basic concept is that a computation is described as a graph of tasks (simple, short sequential programs), and dependencies between tasks: A dependency exists when one task uses results produced by another task. The user specifies the task codes and the dependency graph, in some manner. The runtime system manages the movement of data from producing task to consuming task, and schedules tasks for execution when all their dependencies are satisfied.\n\nThe execution of applications coded using an AMT generate communication patterns that are different than those occuring in more conventional HPC: Many threads may communicate concurrently, and messages are asynchronous. Executions generate a mix of long data messages and short control messages, and the communication pattern varies over time. Traditional communication libraries, such as MPI, do not handle such communication very efficiently. We developed a new communication library, the Lightweight Communication Library (LCI) that provides a better match to the needs of AMTs. LCI does so by supporting new mechanisms for signaling the completion of communications (the interface to the scheduler) and for managing memory required by communications (the interface to the memory manager). It is designed to reduce the need for coarse-grain locking that causes conflicts when many threads communicate concurrently; the implementation is also optimized to reduce the use of such locks.\n\nWe replaced MPI with LCI as the communication mechanism under two current AMT systems: PaRSEC, developed at UTK, and HPX, developed at LSU. We tested both using specially designed micro-benchmarks, and by running two large apllication that were coded to use AMTs: HiCMA, a Tiled Low Rank Cholesky solver, for PeRCS; and Octo-Tiger, an astrophysics application, for HPX. LCI significantly improved execution time, for both. We hope that the lessons learned from this project will affect the design of future communication libraries, or improvements in the current one.\n\n \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/30/2023\n\n\t\t\t\t\tSubmitted by: Marc Snir"
 }
}