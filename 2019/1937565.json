{
 "awd_id": "1937565",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Improved Situation Awareness of Unknown Environments through a Robotic Augmented Reality Virtual Window",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 149956.0,
 "awd_amount": 149956.0,
 "awd_min_amd_letter_date": "2019-08-16",
 "awd_max_amd_letter_date": "2022-09-23",
 "awd_abstract_narration": "When responding to a search warrant or 911 call, teams of police officers often have to enter and search a room.  The problem the police officers face is that the contents of the room are unknown and might be dangerous.  Therefore, the activity is stressful, and sometimes results in adverse outcomes.  This project addresses this problem by developing a robot that police officers can send into a room before they enter.  The robot uses cameras and other sensors to see the room, even in the dark.  The police officers wear augmented reality headsets that allow them to see the room through the robot's cameras.   In addition, the combination of augmented reality and the robot allows the police to see the room as though they had x-ray vision, giving them a virtual window through an otherwise solid wall.  Seeing the room through this virtual window would make searching the room more efficient, reduce uncertainty and stress, and result in avoided injuries and saved lives.  More broadly, the project will further the science of using robots and augmented reality with skilled teams performing dangerous work.  The lessons learned could advance efforts to incorporate robots into related activities, including additional policing tasks, firefighting, inspecting buildings and warehouses, responding to alarm activation, searching and rescuing, and military operations.\r\n\r\nThe objectives of this project are (1) to explore the use of an augmented reality virtual window to provide x-ray vision, where otherwise invisible objects can be seen, and (2) to use a robot to locate and identify objects in unknown, potentially dangerous, and stressful environments.  These objectives are pursued in the context of a common task -- the room-clearing scenario -- undertaken by teams of police officers.  This project will develop a non-weaponized robot, which team members will send into a simulated apartment with several rooms.  The robot will view each room from robot-mounted cameras and sensors, and will present the view in an augmented reality (AR) virtual window.  The cameras and sensors will see the room in multiple spectra, including visible, thermal, and laser, which will allow viewing in varying levels of illumination, including darkness.  The project will leverage the investigators' longstanding relationship with regional police SWAT teams (Special Weapons And Tactics), who have agreed to provide personnel to be subject matter experts, design partners, and evaluation subjects.  The project will be conducted in an existing tactical robotic testbed facility, and spatial understanding and situation awareness will be measured, using both standard and custom methods and surveys.  The work has the potential to advance the fields of augmented reality, robotics, and human-robot interaction, as well as to advance the day when robots can be successfully integrated into SWAT teams.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "J Edward",
   "pi_last_name": "Swan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "J Edward Swan",
   "pi_email_addr": "swan@cse.msstate.edu",
   "nsf_id": "000356075",
   "pi_start_date": "2019-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Cindy",
   "pi_last_name": "Bethel",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Cindy L Bethel",
   "pi_email_addr": "cbethel@cse.msstate.edu",
   "nsf_id": "000599389",
   "pi_start_date": "2019-08-16",
   "pi_end_date": "2022-09-23"
  }
 ],
 "inst": {
  "inst_name": "Mississippi State University",
  "inst_street_address": "245 BARR AVE",
  "inst_street_address_2": "",
  "inst_city_name": "MISSISSIPPI STATE",
  "inst_state_code": "MS",
  "inst_state_name": "Mississippi",
  "inst_phone_num": "6623257404",
  "inst_zip_code": "39762",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MS03",
  "org_lgl_bus_name": "MISSISSIPPI STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTXJM52SHKS7"
 },
 "perf_inst": {
  "perf_inst_name": "Mississippi State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MS",
  "perf_st_name": "Mississippi",
  "perf_zip_code": "397629637",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "MS03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 149956.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our goals for this project were (1) to develop an <em>augmented reality</em> (AR) <em>virtual window</em> that provides <em>x-ray vision</em>, allowing viewers to see otherwise invisible objects, and (2) to implement the window on a robot, which could be used for room clearing scenarios by teams of police officers. More generally, the robot could locate and identify objects in unknown, potentially dangerous, and stressful environments.</p>\n<p>The project resulted in a prototype version of the system. Taking images from a depth camera, the system transforms the depth images into the perspective of the viewer, who wears an AR display. The viewer sees these images through a virtual AR window. The window is located on a wall, and allows the viewer to see into a room beyond the wall. The depth camera is on a robot located in the room. Like looking through a real, physical window, the viewer can move their head to see different perspectives of the room. In addition, and unlike a real window, the viewer can move the location of the AR window to different positions on the wall.</p>\n<p>The project also sought to answer the research question \"When a viewer sees a room through the virtual AR window, can they achieve the same spatial understanding that they could if they were looking through a real, physical window?\" If the answer to this question is <em>yes</em>, then police teams may want to use the robotic AR virtual window system. Teams from other domains would find the system valuable as well. This question addresses a longstanding promise of AR: can it provide useful <em>x-ray vision</em>?</p>\n<p>The phenomenon of <em>spatial understanding</em> is complex, and has many dimensions. This project developed an experimental method that allows us to measure at least one component of spatial understanding: a viewer's perception of the distance between themselves and an object, such as a chair in a room seen through a window. Our method, called <em>blind triangulated walking</em>, is based on perceived distance methods from many years of research in the fields of virtual and augmented reality, and perceptual psychology. However, these methods have not previously measured perceived distance to objects located behind a physical wall. Most of the methods require the viewer to close their eyes and then walk, without vision, to the location where they perceive the object to be. In our method, viewers close their eyes and walk along a triangular path to the perceived location, which allows them to walk around a physical wall.</p>\n<p>In addition to these technical contributions, perhaps the most important impact of this project are the careers of the students who were supported. <em>Nate Phillips</em> completed his PhD degree during Summer 2022. His dissertation topic encompassed both developing the prototype system, and addressing the research question. He is now an entrepreneur, and is teaching computer science at Rhodes College in Memphis, Tennessee. <em>Brady Krause</em> completed his BS degree in computer science in December 2021. He also developed the prototype system. He is now a graduate student at Duke University. <em>Mohammed Arefin</em> completed his PhD degree during Fall 2022. His dissertation topic was related to the goals of this project, in that it involves virtual objects at specific depth positions, but distinct because it involves fonts for AR text labels. He is now an assistant professor at Colorado State University. <em>Farzana Khan</em> completed her MS degree during Summer 2022. Her thesis topic was related to the goal of this project, in that it involves measuring perceived location, but distinct because it addresses reaching distances. She has since had several technology-related industrial positions. The principal investigators remain very proud of these students and their accomplishments.</p>\n<p>To date, this project has resulted in 14 published papers and abstracts.</p><br>\n<p>\n Last Modified: 01/29/2024<br>\nModified by: John&nbsp;E&nbsp;Swan</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505379794_Image_2--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505379794_Image_2--rgov-800width.png\" title=\"Overview (a) and photos from the x-ray vision system for situation awareness in action space.\"><img src=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505379794_Image_2--rgov-66x44.png\" alt=\"Overview (a) and photos from the x-ray vision system for situation awareness in action space.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An x-ray vision system for situation awareness in action space.</div>\n<div class=\"imageCredit\">Created by the principal investigators.</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">John&nbsp;E&nbsp;Swan\n<div class=\"imageTitle\">Overview (a) and photos from the x-ray vision system for situation awareness in action space.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505533158_Image_3--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505533158_Image_3--rgov-800width.png\" title=\"Triangulated Walking Methodology.\"><img src=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505533158_Image_3--rgov-66x44.png\" alt=\"Triangulated Walking Methodology.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Measuring virtual object location with x-ray vision at action space distances.</div>\n<div class=\"imageCredit\">Created by the principal investigators.</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">John&nbsp;E&nbsp;Swan\n<div class=\"imageTitle\">Triangulated Walking Methodology.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505481319_Image_1--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505481319_Image_1--rgov-800width.png\" title=\"The concept of the robotic augmented reality (AR) virtual window.\"><img src=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505481319_Image_1--rgov-66x44.png\" alt=\"The concept of the robotic augmented reality (AR) virtual window.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">left: Illustration of the room-clearing scenario with the robotic augmented reality (AR) virtual window. right: Preliminary implementation of the AR virtual window concept: (top) A person sitting in a chair, visualized with a depth camera, (bottom) seen through an AR virtual window.</div>\n<div class=\"imageCredit\">Created by the principal investigators.</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">John&nbsp;E&nbsp;Swan\n<div class=\"imageTitle\">The concept of the robotic augmented reality (AR) virtual window.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505634555_Image_4--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505634555_Image_4--rgov-800width.png\" title=\"Experiment overview, showing the variables for both experiments.\"><img src=\"/por/images/Reports/POR/2024/1937565/1937565_10634802_1706505634555_Image_4--rgov-66x44.png\" alt=\"Experiment overview, showing the variables for both experiments.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Experiment overview of a method for measuring the perceived three-dimensional location of virtual objects in optical see-through augmented reality.</div>\n<div class=\"imageCredit\">Created by the principal investigators.</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">John&nbsp;E&nbsp;Swan\n<div class=\"imageTitle\">Experiment overview, showing the variables for both experiments.</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nOur goals for this project were (1) to develop an augmented reality (AR) virtual window that provides x-ray vision, allowing viewers to see otherwise invisible objects, and (2) to implement the window on a robot, which could be used for room clearing scenarios by teams of police officers. More generally, the robot could locate and identify objects in unknown, potentially dangerous, and stressful environments.\n\n\nThe project resulted in a prototype version of the system. Taking images from a depth camera, the system transforms the depth images into the perspective of the viewer, who wears an AR display. The viewer sees these images through a virtual AR window. The window is located on a wall, and allows the viewer to see into a room beyond the wall. The depth camera is on a robot located in the room. Like looking through a real, physical window, the viewer can move their head to see different perspectives of the room. In addition, and unlike a real window, the viewer can move the location of the AR window to different positions on the wall.\n\n\nThe project also sought to answer the research question \"When a viewer sees a room through the virtual AR window, can they achieve the same spatial understanding that they could if they were looking through a real, physical window?\" If the answer to this question is yes, then police teams may want to use the robotic AR virtual window system. Teams from other domains would find the system valuable as well. This question addresses a longstanding promise of AR: can it provide useful x-ray vision?\n\n\nThe phenomenon of spatial understanding is complex, and has many dimensions. This project developed an experimental method that allows us to measure at least one component of spatial understanding: a viewer's perception of the distance between themselves and an object, such as a chair in a room seen through a window. Our method, called blind triangulated walking, is based on perceived distance methods from many years of research in the fields of virtual and augmented reality, and perceptual psychology. However, these methods have not previously measured perceived distance to objects located behind a physical wall. Most of the methods require the viewer to close their eyes and then walk, without vision, to the location where they perceive the object to be. In our method, viewers close their eyes and walk along a triangular path to the perceived location, which allows them to walk around a physical wall.\n\n\nIn addition to these technical contributions, perhaps the most important impact of this project are the careers of the students who were supported. Nate Phillips completed his PhD degree during Summer 2022. His dissertation topic encompassed both developing the prototype system, and addressing the research question. He is now an entrepreneur, and is teaching computer science at Rhodes College in Memphis, Tennessee. Brady Krause completed his BS degree in computer science in December 2021. He also developed the prototype system. He is now a graduate student at Duke University. Mohammed Arefin completed his PhD degree during Fall 2022. His dissertation topic was related to the goals of this project, in that it involves virtual objects at specific depth positions, but distinct because it involves fonts for AR text labels. He is now an assistant professor at Colorado State University. Farzana Khan completed her MS degree during Summer 2022. Her thesis topic was related to the goal of this project, in that it involves measuring perceived location, but distinct because it addresses reaching distances. She has since had several technology-related industrial positions. The principal investigators remain very proud of these students and their accomplishments.\n\n\nTo date, this project has resulted in 14 published papers and abstracts.\t\t\t\t\tLast Modified: 01/29/2024\n\n\t\t\t\t\tSubmitted by: JohnESwan\n"
 }
}